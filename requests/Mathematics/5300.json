{
  "pages": [
    {
      "title": "Indicative conditional",
      "url": "https://en.wikipedia.org/wiki/Indicative_conditional",
      "text": "\nIn [[natural language]]s, an '''indicative conditional'''<ref>Stalnaker, R, Philosophia (1975)</ref><ref>Ellis, B, [[Australasian Journal of Philosophy]] (1984)</ref> is the [[logical connective|logical operation]] given by statements of the form \"If A then B\".  Unlike the [[material conditional]], an indicative conditional does not have a stipulated definition.  The philosophical literature on this operation is broad, and no clear consensus has been reached.\n\n==Distinctions from the material conditional==\n\nThe material conditional does not always function in accordance with everyday if-then reasoning. Therefore there are drawbacks with using the material conditional to represent if-then statements.\n\nOne problem is that the material conditional allows implications to be true even when the antecedent is irrelevant to the [[consequent]]. For example, it's commonly accepted that the sun is made of plasma, on one hand, and that 3 is a prime number, on the other. The standard definition of implication allows us to conclude that, if the sun is made of plasma, then 3 is a prime number. This is arguably synonymous to the following: the sun's being made of plasma renders 3 a prime number. Many people intuitively think that this is false, because the sun and the number three simply have nothing to do with one another. Logicians have tried to address this concern by developing alternative logics, e.g., [[relevance logic]].\n\nFor a related problem, see [[vacuous truth]].\n\nAnother issue is that the material conditional is not designed to deal with [[Counterfactual conditionals|counterfactuals]] and other cases that people often find in if-then reasoning. This has inspired people to develop [[modal logic]].\n\nA further problem is that the material conditional is such that (P AND ¬P) → Q, regardless of what Q is taken to mean. That is, a contradiction implies that absolutely everything is true. Logicians concerned with this have developed [[paraconsistent logic]]s.\n\nThe mentioned theories are not exclusive.\n\n==Psychology==\nMost behavioral experiments on conditionals in the psychology of reasoning have been carried out with indicative conditionals, causal conditionals,  and [[counterfactual conditionals]]. People readily make the [[modus ponens]] inference, that is,  given ''if A then B'', and given ''A'', they conclude ''B'', but only about half of participants in experiments make the [[modus tollens]] inference, that is,  given ''if A then B'', and given ''not-B'', only about half of participants conclude ''not-A'', the remainder say that nothing follows (Evans ''et al.'', 1993). When participants are given counterfactual conditionals, they make both the modus ponens and the modus tollens inferences (Byrne, 2005).\n\n==See also==\n{{Portal|Philosophy}}\n* [[Counterfactual conditional]]\n* [[Logical consequence]]\n* [[Material conditional]]\n* [[Strict conditional]]\n\n==References==\n{{reflist}}\n\n== Further reading ==\n* Byrne, R.M.J. (2005). ''The Rational Imagination: How People Create Counterfactual Alternatives to Reality.'' Cambridge, MA: MIT Press.\n* Edgington, Dorothy. (2006). \"Conditionals\". ''The Stanford Encyclopedia of Philosophy'', Edward Zalta (ed.). http://plato.stanford.edu/entries/conditionals/.\n* Evans, J. St. B. T.,  Newstead, S. and Byrne, R. M. J. (1993). ''Human Reasoning: The Psychology of Deduction.''  Hove, Psychology Press.\n\n[[Category:Conditionals]]\n[[Category:Logical connectives]]\n[[Category:Reasoning]]"
    },
    {
      "title": "Logical biconditional",
      "url": "https://en.wikipedia.org/wiki/Logical_biconditional",
      "text": "{{one source|date=June 2013}}\n\n[[File:Venn1001.svg|220px|thumb|[[Venn diagram]] of <math>P \\leftrightarrow Q</math><br>(true part in red)]]\n\nIn [[logic]] and [[mathematics]], the '''logical biconditional''' (sometimes known as the '''material biconditional''') is the [[logical connective]] of two statements asserting \"<math>P</math> [[if and only if]] <math>Q</math>\", where <math>P</math> is an ''[[antecedent (logic)|antecedent]]'' and <math>Q</math> is a ''[[consequent]]''.<ref>Handbook of Logic, page 81</ref> This is often abbreviated \"<math>P</math> iff <math>Q</math>\". The operator is denoted using a doubleheaded arrow (↔), a prefixed E \"E''pq''\" (in [[Polish_notation#Polish_notation_for_logic|Łukasiewicz notation]] or [[Józef_Maria_Bocheński|Bocheński notation]]), an equality sign (=), an equivalence sign (≡), or ''EQV''. It is logically equivalent to <math>(P \\rightarrow Q) \\land (Q \\rightarrow P)</math> and to <math>(P \\land Q) \\lor (\\neg P \\land \\neg Q)</math> (or the [[XNOR gate|XNOR]] (exclusive nor) [[Logical connective|boolean operator]]), meaning \"both or neither\".\n\nThe only difference from [[material conditional]] is the case when the hypothesis is false but the conclusion is true. In that case, in the conditional, the result is true, yet in the biconditional the result is false.\n\nIn the conceptual interpretation, <math>P = Q</math> means \"All <math>P</math>'s are <math>Q</math>'s and all '<math>Q</math>'s are <math>P</math>'s\"; in other words, the sets <math>P</math> and <math>Q</math> coincide: they are identical. This does not mean that the concepts have the same meaning. Examples: \"triangle\" and \"trilateral\", \"equiangular trilateral\" and \"equilateral triangle\". The antecedent is the ''subject'' and the consequent is the ''predicate'' of a [[universal affirmative]] proposition.\n\nIn the propositional interpretation, <math>P \\Leftrightarrow Q</math> (⇔) means that <math>P</math> implies <math>Q</math> and <math>Q</math> implies <math>P</math>; in other words, that the propositions are equivalent, that is to say, either true or false at the same time. This does not mean that they have the same meaning. [[Pons asinorum|Example]]: \"The triangle ABC has two equal sides\", and \"The triangle ABC has two equal angles\". The antecedent is the ''premise'' or the ''cause'' and the consequent is the ''consequence''. When an implication is translated by a ''hypothetical'' (or ''conditional'') judgment the antecedent is called the ''hypothesis'' (or the ''condition'') and the consequent is called the ''thesis''.\n\nA common way of demonstrating a biconditional is to use its equivalence to the conjunction of two converse [[Material conditional|conditional]]s, demonstrating these separately.\n\nWhen both members of the biconditional are propositions, it can be separated into two conditionals, of which one is called a ''theorem'' and the other its ''reciprocal''.{{Citation needed|date=August 2008}} Thus whenever a theorem and its reciprocal are true we have a biconditional. A simple theorem gives rise to an implication whose antecedent is the ''hypothesis'' and whose consequent is the ''thesis'' of the theorem.\n\nIt is often said that the hypothesis is the ''[[sufficient condition]]'' of the thesis, and the\nthesis the ''[[necessary condition]]'' of the hypothesis; that is to say, it is sufficient that the hypothesis be true for the thesis to be true; while it is necessary that the thesis be true for the hypothesis to be true also. When a theorem and its reciprocal are true we say that its hypothesis is the [[necessary and sufficient condition]] of the thesis; that is to say, that it is at the same time both cause and consequence.\n\n==Definition==\n\n[[Logical equality]] (also known as biconditional) is an [[logical operation|operation]] on two [[logical value]]s, typically the values of two [[proposition]]s, that produces a value of ''true'' if and only if both operands are false or both operands are true.\n\n===Truth table===\nThe truth table for <math>P \\leftrightarrow Q</math> (also written as <math>P \\equiv Q</math>, <math>P = Q</math>, or '''P EQ Q''') is as follows:\n\n{| class=\"wikitable\" style=\"text-align:center; background-color: #ddffdd;\"\n|- bgcolor=\"#ddeeff\"\n| <math> P </math> || <math> Q </math> || <math> P \\leftrightarrow Q </math>\n|-\n| T || T || T\n|-\n| T || F || F\n|-\n| F || T || F\n|-\n| F || F || T\n|}\n\n{{-}}\n\nMore than two statements combined by <math>~\\leftrightarrow~</math> are ambiguous:\n\n<math>~x_1 \\leftrightarrow x_2 \\leftrightarrow x_3 \\leftrightarrow ... \\leftrightarrow x_n</math> may be meant as <math>~(((x_1 \\leftrightarrow x_2) \\leftrightarrow x_3) \\leftrightarrow ...) \\leftrightarrow x_n</math>,\n\nor may be used to say that all <math>~x_i~</math> are ''together true or together false'': <math>(~x_1 \\land ... \\land x_n~)~\\lor~(\\neg x_1 \\land ... \\land \\neg x_n)</math>\n\nOnly for zero or two arguments this is the same.\n\nThe following truth tables show the same bit pattern only in the line with no argument and in the lines with two arguments:\n\n[[File:Multigrade operator XNOR.svg|thumb|left|220px|<math>~x_1 \\leftrightarrow ... \\leftrightarrow x_n</math><br>meant as equivalent to<br><math>\\neg~(\\neg x_1 \\oplus ... \\oplus \\neg x_n)</math><br><br>The central Venn diagram below,<br>and line ''(ABC&nbsp;&nbsp;)'' in this matrix<br>represent the same operation.]]\n\n[[File:Multigrade operator all or nothing.svg|thumb|right|220px|<math>~x_1 \\leftrightarrow ... \\leftrightarrow x_n</math><br>meant as shorthand for<br><math>(~x_1 \\land ... \\land x_n~)</math><br><math>\\lor~(\\neg x_1 \\land ... \\land \\neg x_n)</math><br><br>The Venn diagram directly below,<br>and line ''(ABC&nbsp;&nbsp;)'' in this matrix<br>represent the same operation.]]\n{{-}}\nThe left Venn diagram below, and the lines ''(AB&nbsp;&nbsp;&nbsp;&nbsp;)'' in these matrices represent the same operation.\n\n===Venn diagrams===\nRed areas stand for true (as in [[Image:Venn0001.svg|40px]] for ''[[Logical disjunction|and]]'').\n\n{| border=\"0\" style=\"width:100%\"\n| style=\"vertical-align:top;\"|<!--- START LEFT TABLE IN TABLE --->\n{|  style=\"background:#f9f9f9; border:1px solid #ccc; float:left;\"\n|-\n| [[Image:Venn1001.svg|220px]]\n|-\n| The biconditional of two statements<br />is the [[negation]] of the [[exclusive or]]:\n|- style=\"text-align:center;\"\n|<math>~A \\leftrightarrow B~~\\Leftrightarrow~~\\neg(A \\oplus B)</math>\n\n[[File:Venn1001.svg|40px]] <math>\\Leftrightarrow \\neg</math> [[Image:Venn0110.svg|40px]]\n|}<!--- END LEFT TABLE IN TABLE --->\n| style=\"width: 100px\"|\n| style=\"vertical-align:top;\"|<!--- START CENTRAL TABLE IN TABLE --->\n{|  style=\"background:#f9f9f9; border:1px solid #ccc; margin:auto;\"\n|-\n| [[File:Venn 0110 1001.svg|220px]]\n|-\n| The biconditional and the<br />exclusive or of three statements<br />give the same result:<br />\n<math>~A \\leftrightarrow B \\leftrightarrow C~~\\Leftrightarrow</math><br />\n<math>~A \\oplus B \\oplus C</math>\n\n[[File:Venn 1001 1001.svg|40px]] <math>\\leftrightarrow</math> [[File:Venn 0000 1111.svg|40px]]\n<math>~~\\Leftrightarrow~~</math>\n\n[[File:Venn 0110 0110.svg|40px]] <math>\\oplus</math> [[File:Venn 0000 1111.svg|40px]]\n<math>~~\\Leftrightarrow~~</math> [[File:Venn 0110 1001.svg|40px]]\n|}<!--- END CENTRAL TABLE IN TABLE --->\n| style=\"width: 100px\"|\n| style=\"vertical-align:top;\"|<!--- START RIGHT TABLE IN TABLE --->\n{|  style=\"background:#f9f9f9; border:1px solid #ccc; float:right;\"\n|-\n| [[File:Venn 1000 0001.svg|220px]]\n|-\n| But <math>~A \\leftrightarrow B \\leftrightarrow C</math><br />may also be used as an abbreviation<br />for <math>(A \\leftrightarrow B) \\land (B \\leftrightarrow C)</math>\n\n[[File:Venn 1001 1001.svg|40px]] <math>\\land</math> [[File:Venn 1100 0011.svg|40px]]\n<math>~~\\Leftrightarrow~~</math> [[File:Venn 1000 0001.svg|40px]]\n|}<!--- END RIGHT TABLE IN TABLE --->\n|}\n\n==Properties==\n'''[[commutativity]]: yes'''\n{| style=\"text-align:center; border:1px solid darkgrey;\"\n|-\n|<math>A \\leftrightarrow B</math>\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|<math>B \\leftrightarrow A</math>\n|-\n|[[File:Venn1001.svg|50px]]\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|[[File:Venn1001.svg|50px]]\n|}\n\n'''[[associativity]]: yes'''\n{| style=\"text-align:center; border:1px solid darkgrey;\"\n|-\n|<math>~A</math>\n|<math>~~~\\leftrightarrow~~~</math>\n|<math>(B \\leftrightarrow C)</math>\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|\n|\n|<math>(A \\leftrightarrow B)</math>\n|<math>~~~\\leftrightarrow~~~</math>\n|<math>~C</math>\n|-\n|[[File:Venn 0101 0101.svg|50px]]\n|<math>~~~\\leftrightarrow~~~</math>\n|[[File:Venn 1100 0011.svg|50px]]\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|[[File:Venn 0110 1001.svg|50px]]\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|[[File:Venn 1001 1001.svg|50px]]\n|<math>~~~\\leftrightarrow~~~</math>\n|[[File:Venn 0000 1111.svg|50px]]\n|}\n\n'''[[distributivity]]:'''  Biconditional doesn't distribute over any binary function (not even itself),<br>\nbut [[Logical disjunction#Properties|logical disjunction distributes]] over biconditional.\n\n'''[[idempotency]]: no'''<br>\n{| style=\"text-align:center; border:1px solid darkgrey;\"\n|-\n|<math>~A~</math>  \n|<math>~\\leftrightarrow~</math> \n|<math>~A~</math> \n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|<math>~1~</math> \n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\nLeftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|<math>~A~</math> \n|-\n|[[File:Venn01.svg|36px]] \n|<math>~\\leftrightarrow~</math> \n|[[File:Venn01.svg|36px]]\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|[[File:Venn11.svg|36px]]\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\nLeftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|[[File:Venn01.svg|36px]]\n|}\n\n'''[[Monotone boolean function|monotonicity]]: no'''\n{| style=\"text-align:center; border:1px solid darkgrey;\"\n|-\n|<math>A \\rightarrow B</math>\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\nRightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|\n|\n|<math>(A \\leftrightarrow C)</math>\n|<math>\\rightarrow</math>\n|<math>(B \\leftrightarrow C)</math>\n|-\n||[[File:Venn 1011 1011.svg|50px]]\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\nRightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n||[[File:Venn 1101 1011.svg|50px]]\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n||[[File:Venn 1010 0101.svg|50px]]\n|<math>\\rightarrow</math>\n||[[File:Venn 1100 0011.svg|50px]]\n|}\n\n'''truth-preserving: yes'''<br>\nWhen all inputs are true, the output is true.\n{| style=\"text-align:center; border:1px solid darkgrey;\"\n|-\n|<math>A \\land B</math>\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Rightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|<math>A \\leftrightarrow B</math>\n|-\n|[[File:Venn0001.svg|50px]]\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Rightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|[[File:Venn1001.svg|60px]]\n|}\n\n'''falsehood-preserving: no'''<br>\nWhen all inputs are false, the output is not false.\n{| style=\"text-align:center; border:1px solid darkgrey;\"\n|-\n|<math>A \\leftrightarrow B</math>\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\nRightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|<math>A \\lor B</math>\n|-\n|[[File:Venn1001.svg|60px]]\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\nRightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|[[File:Venn0111.svg|50px]]\n|}\n\n'''[[Hadamard transform|Walsh spectrum]]: (2,0,0,2)'''\n\n'''Non[[Linear#Boolean functions|linearity]]: 0''' (the function is linear)\n\n==Rules of inference==\n{{Main|Rules of inference}}\nLike all connectives in first-order logic, the biconditional has rules of inference that govern its use in formal proofs.\n\n===Biconditional introduction===\n{{Main|Biconditional introduction}}\nBiconditional introduction allows you to infer that, if B follows from A, and A follows from B, then A [[if and only if]] B.\n\nFor example, from the statements \"if I'm breathing, then I'm alive\" and \"if I'm alive, then I'm breathing\", it can be inferred that \"I'm breathing if and only if I'm alive\" or, equally inferrable, \"I'm alive if and only if I'm breathing.\"\n\n  B → A &nbsp;&nbsp;\n <u> A → B &nbsp;&nbsp;</u>\n  ∴  A ↔ B\n\n  B → A &nbsp;&nbsp;\n <u> A → B &nbsp;&nbsp;</u>\n  ∴  B ↔ A\n\n===Biconditional elimination===\n'''Biconditional elimination''' allows one to infer a [[Material conditional|conditional]] from a biconditional: if ( A <small>↔</small> B ) is true, then one may infer one direction of the biconditional, '''( A <small>→</small> B ) and ( B <small>→</small> A )'''.\n\nFor example, if it's true that I'm breathing [[if and only if]] I'm alive, then it's true that if I'm breathing, I'm alive; likewise, it's true that if I'm alive, I'm breathing.\n\nFormally:\n\n  <u>( A ↔ B )&nbsp;&nbsp;</u>\n  ∴ ( A → B )\n\nalso\n\n  <u>( A ↔ B )&nbsp;&nbsp;</u>\n  ∴ ( B → A )\n\n==Colloquial usage==\n\nOne unambiguous way of stating a biconditional in plain English is of the form \"''b'' if ''a''  and ''a'' if ''b''\". Another is \"''a'' if and only if ''b''\". Slightly more formally, one could say \"''b'' implies ''a'' and ''a'' implies ''b''\". The plain English \"if'\" may sometimes be used as a biconditional. One must weigh context heavily.\n\nFor example, \"I'll buy you a new wallet if you need one\" may be meant as a biconditional, since the speaker doesn't intend a valid outcome to be buying the wallet whether or not the wallet is needed (as in a conditional). However, \"it is cloudy if it is raining\" is not meant as a biconditional, since it can be cloudy while not raining.\n\n==See also==\n{{Portal|Thinking}}\n\n* [[If and only if]]\n* [[Logical equivalence]]\n* [[Logical equality]]\n* [[XNOR gate]]\n* [[Biconditional elimination]]\n* [[Biconditional introduction]]\n\n==Notes==\n{{reflist}}\n\n==References==\n*Brennan, Joseph G. [https://archive.org/stream/handbookoflogics012674mbp#page/n90/mode/1up Handbook of Logic, 2nd Edition]. [[Harper & Row]]. 1961\n\n{{Logical connectives}}\n{{PlanetMath attribution|id=484|title=Biconditional}}\n\n{{DEFAULTSORT:Logical Biconditional}}\n[[Category:Logical connectives]]\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "Logical conjunction",
      "url": "https://en.wikipedia.org/wiki/Logical_conjunction",
      "text": "{{Infobox logical connective\n| title        = {{PAGENAME}}\n| other titles = AND\n| Venn diagram = Venn0001.svg\n| definition   = <math>xy</math>\n| truth table  = <math>(0001)</math>\n| logic gate   = AND_ANSI.svg\n| DNF          = <math>xy</math>\n| CNF          = <math>xy</math>\n| Zhegalkin    = <math>xy</math>\n| 0-preserving = {{#switch:Да|Да|да=yes|Нет|нет=no}}\n| 1-preserving = {{#switch:Да|Да|да=yes|Нет|нет=no}}\n| monotone     = {{#switch:Нет|Да|да=yes|Нет|нет=no}}\n| affine       = {{#switch:Нет|Да|да=yes|Нет|нет=no}}\n| self-dual    = {{#switch:Да|Да|да=yes|Нет|нет=no}}\n}}\n\n{{distinguish|Caret {{!}} Circumflex Agent (^)|Lambda {{!}} Capital Lambda (Λ)|Turned V {{!}} Turned V (Λ)}}\n\n[[File:Venn 0000 0001.svg|220px|thumb|Venn diagram of <math>A \\land B \\land C</math>]]\n\nIn [[logic]], [[mathematics]] and [[linguistics]], And (∧) is the [[truth function|truth-functional]] operator of '''logical conjunction'''; the ''and'' of a set of operands is true if and only if ''all'' of its operands are true. The [[logical connective]] that represents this operator is typically written as {{math| ∧ }} or {{math| ⋅ }}. \n\n<math>A \\land B</math> is true only if <math>A</math> is true and <math>B</math> is true. \n\nAn operand of a conjunction is a '''conjunct'''.\n\nThe term \"logical conjunction\" is also used for the [[greatest lower bound]] in [[Lattice (order)|lattice theory]].\n\nRelated concepts in other fields are:\n\n* In [[natural language]], the [[Grammatical conjunction#Coordinating conjunctions|coordinating conjunction]] \"and\".\n* In programming languages, the [[short-circuit evaluation|short-circuit and]] [[control structure]].\n* In [[set theory]], [[intersection (set theory)|intersection]].\n* In [[predicate logic]], [[universal quantification]].\n\n==Notation==\n\n'''And''' is usually denoted by an infix operator: in mathematics and logic, it is denoted by '''{{math| ∧ }}''', '''{{math|&}}''' or '''{{math| × }}'''; in electronics, '''{{math| ⋅ }}'''; and in programming languages, '''<code>&amp;</code>''', '''<code>&amp;&amp;</code>''', or '''<code>and</code>'''.   In [[Jan Łukasiewicz]]'s [[Polish notation#Polish notation for logic|prefix notation for logic]], the operator is '''K''', for Polish ''koniunkcja''.<ref>[[Józef Maria Bocheński]] (1959), ''A Précis of Mathematical Logic'', translated by Otto Bird from the French and German editions, Dordrecht, North Holland:  D. Reidel, passim.</ref>\n\n==Definition==\n'''Logical conjunction''' is an [[logical operation|operation]] on two [[logical value]]s, typically the values of two [[proposition]]s, that produces a value of ''true'' [[Iff|if and only if]] both of its operands are true.\n\nThe conjunctive [[identity element|identity]] is 1, which is to say that AND-ing an expression with 1 will never change the value of the expression. In keeping with the concept of [[vacuous truth]], when conjunction is defined as an operator or function of arbitrary [[arity]], the empty conjunction (AND-ing over an empty set of operands) is often defined as having the result 1.\n\n===Truth table===\n[[File:Multigrade operator AND.svg|thumb|Conjunctions of the arguments on the left — The [[truth value|true]] [[bit]]<nowiki>s</nowiki> form a [[Sierpinski triangle]].]]\n\nThe [[truth table]] of <math>A \\land B</math>:\n\n{| class=\"wikitable\" style=\"text-align:center; background-color: #ddffdd;\"\n|- bgcolor=\"#ddeeff\"\n| <math>A</math> || <math>B</math> || <math>A \\wedge B</math>\n|-\n| T || T || T\n|-\n| T || F || F\n|-\n| F || T || F\n|-\n| F || F || F\n|}\n\n===Defined by other operators===\nIn systems where logical conjunction is not a primitive, it may be defined as<ref>{{Cite web |url=http://www.logicmatters.net/resources/pdfs/ProofSystems.pdf |title=Types of proof system |last=Smith |first=Peter |page=4}}</ref>\n:<math>A \\land B = \\neg(A \\to \\neg B) </math>\nor\n:<math>A \\land B = \\neg(\\neg A \\lor \\neg B).</math>\n\n==Introduction and elimination rules==\nAs a rule of inference, conjunction Introduction is a classically [[Validity (logic)|valid]], simple [[argument form]]. The argument form has two premises, ''A'' and ''B''. Intuitively, it permits the inference of their conjunction.\n\n:''A'',\n:''B''.\n:Therefore, ''A'' and ''B''.\n\nor in [[logical operator]] notation:\n:<math> A, </math>\n:<math> B </math>\n:<math> \\vdash A \\land B </math>\n\nHere is an example of an argument that fits the form ''[[conjunction introduction]]'':\n\n:Bob likes apples.\n:Bob likes oranges.\n:Therefore, Bob likes apples and oranges.\n\n[[Conjunction elimination]] is another classically [[Validity (logic)|valid]], simple [[argument form]]. Intuitively, it permits the inference from any conjunction of either element of that conjunction.\n\n:''A'' and ''B''.\n:Therefore, ''A''.\n\n...or alternatively,\n\n:''A'' and ''B''.\n:Therefore, ''B''.\n\nIn [[logical operator]] notation:\n:<math> A \\land B </math>\n:<math> \\vdash A </math>\n\n...or alternatively,\n\n:<math> A \\land B </math>\n:<math> \\vdash B </math>\n\n== Negation ==\n=== Definition ===\nA conjunction <math>A\\land B</math> is be proven false by establishing either <math>\\neg A</math> or <math>\\neg B</math>. \nIn terms of the object language, this reads\n\n:<math>\\neg A\\to\\neg(A\\land B)</math>\n\nThis formula can be seen as a special case of\n\n:<math>(A\\to C) \\to ( (A\\land B)\\to C )</math>\n\nwhen <math>C</math> is a false proposition.\n\n=== Other proof strategies ===\nIf <math>A</math> implies <math>\\neg B</math>, then both <math>\\neg A</math> as well as <math>A</math> prove the conjunction false:\n:<math>(A\\to\\neg{}B)\\to\\neg(A\\land B)</math>\nIn other words, a conjunction can actually be proven false just by knowing about the relation of its conjuncts and not necessary about their truth values.\n\nThis formula can be seen as a special case of\n:<math>(A\\to(B\\to C))\\to ( (A\\land B)\\to C ) </math>\n\nwhen <math>C</math> is a false proposition. \n\nEither of the above are constructively valid proofs by contradiction.\n\n==Properties==\n'''[[commutativity]]: yes'''\n{| style=\"text-align: center; border: 1px solid darkgray;\"\n|-\n|<math>A \\land B</math>\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|<math>B \\land A</math>\n|-\n|[[File:Venn0001.svg|50px]]\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|[[File:Venn0001.svg|50px]]\n|}\n\n'''[[associativity]]: yes'''\n{| style=\"text-align: center; border: 1px solid darkgray;\"\n|-\n|<math>~A</math>\n|<math>~~~\\land~~~</math>\n|<math>(B \\land C)</math>\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|\n|\n|<math>(A \\land B)</math>\n|<math>~~~\\land~~~</math>\n|<math>~C</math>\n|-\n|[[File:Venn 0101 0101.svg|50px]]\n|<math>~~~\\land~~~</math>\n|[[File:Venn 0000 0011.svg|50px]]\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|[[File:Venn 0000 0001.svg|50px]]\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|[[File:Venn 0001 0001.svg|50px]]\n|<math>~~~\\land~~~</math>\n|[[File:Venn 0000 1111.svg|50px]]\n|}\n\n'''[[distributivity]]:''' with various operations, especially with ''[[logical disjunction|or]]''\n{| style=\"text-align: center; border: 1px solid darkgray;\"\n|-\n|<math>~A</math>\n|<math>\\land</math>\n|<math>(B \\lor C)</math>\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|\n|\n|<math>(A \\land B)</math>\n|<math>\\lor</math>\n|<math>(A \\land C)</math>\n|-\n|-\n|[[File:Venn 0101 0101.svg|50px]]\n|<math>\\land</math>\n|[[File:Venn 0011 1111.svg|50px]]\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|[[File:Venn 0001 0101.svg|50px]]\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|[[File:Venn 0001 0001.svg|50px]]\n|<math>\\lor</math>\n|[[File:Venn 0000 0101.svg|50px]]\n|}\n\n{| class=\"collapsible collapsed\" style=\"width: 100%; border: 1px solid #aaaaaa;\"\n! bgcolor=\"#ccccff\"|others\n|-\n| \nwith [[exclusive or]]:\n{| style=\"text-align: center; border: 1px solid darkgray;\"\n|-\n|<math>~A</math>\n|<math>\\land</math>\n|<math>(B \\oplus C)</math>\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|\n|\n|<math>(A \\land B)</math>\n|<math>\\oplus</math>\n|<math>(A \\land C)</math>\n|-\n|-\n|[[File:Venn 0101 0101.svg|50px]]\n|<math>\\land</math>\n|[[File:Venn 0011 1100.svg|50px]]\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|[[File:Venn 0001 0100.svg|50px]]\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|[[File:Venn 0001 0001.svg|50px]]\n|<math>\\oplus</math>\n|[[File:Venn 0000 0101.svg|50px]]\n|}\n\nwith [[material nonimplication]]:\n{| style=\"text-align: center; border: 1px solid darkgray;\"\n|-\n|<math>~A</math>\n|<math>\\land</math>\n|<math>(B \\nrightarrow C)</math>\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|\n|\n|<math>(A \\land B)</math>\n|<math>\\nrightarrow</math>\n|<math>(A \\land C)</math>\n|-\n|-\n|[[File:Venn 0101 0101.svg|50px]]\n|<math>\\land</math>\n|[[File:Venn 0011 0000.svg|50px]]\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|[[File:Venn 0001 0000.svg|50px]]\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|[[File:Venn 0001 0001.svg|50px]]\n|<math>\\nrightarrow</math>\n|[[File:Venn 0000 0101.svg|50px]]\n|}\n\nwith itself:\n{| style=\"text-align: center; border: 1px solid darkgray;\"\n|-\n|<math>~A</math>\n|<math>\\land</math>\n|<math>(B \\land C)</math>\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|\n|\n|<math>(A \\land B)</math>\n|<math>\\land</math>\n|<math>(A \\land C)</math>\n|-\n|-\n|[[File:Venn 0101 0101.svg|50px]]\n|<math>\\land</math>\n|[[File:Venn 0000 0011.svg|50px]]\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|[[File:Venn 0000 0001.svg|50px]]\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|[[File:Venn 0001 0001.svg|50px]]\n|<math>\\land</math>\n|[[File:Venn 0000 0101.svg|50px]]\n|}\n|}\n\n'''[[idempotency]]: yes'''<br>\n{| style=\"text-align: center; border: 1px solid darkgray;\"\n|-\n|<math>~A~</math>\n|<math>~\\land~</math>\n|<math>~A~</math>\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|<math>A~</math>\n|-\n|[[File:Venn01.svg|36px]]\n|<math>~\\land~</math>\n|[[File:Venn01.svg|36px]]\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|[[File:Venn01.svg|36px]]\n|}\n\n'''[[Monotonic function#Boolean functions|monotonicity]]: yes'''\n{| style=\"text-align: center; border: 1px solid darkgray;\"\n|<math>A \\rightarrow B</math>\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Rightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|\n|\n|<math>(A \\land C)</math>\n|<math>\\rightarrow</math>\n|<math>(B \\land C)</math>\n|-\n||[[File:Venn 1011 1011.svg|50px]]\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Rightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n||[[File:Venn 1111 1011.svg|50px]]\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n||[[File:Venn 0000 0101.svg|50px]]\n|<math>\\rightarrow</math>\n||[[File:Venn 0000 0011.svg|50px]]\n|}\n\n'''truth-preserving: yes'''<br>\nWhen all inputs are true, the output is true.\n{| style=\"text-align: center; border: 1px solid darkgray;\"\n|-\n|<math>A \\land B</math>\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Rightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|<math>A \\land B</math>\n|-\n|[[File:Venn0001.svg|50px]]\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Rightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|[[File:Venn0001.svg|60px]]\n|-\n|\n|\n|{{small|(to be tested)}}\n|}\n\n'''falsehood-preserving: yes'''<br>\nWhen all inputs are false, the output is false.\n{| style=\"text-align: center; border: 1px solid darkgray;\"\n|-\n|<math>A \\land B</math>\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Rightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|<math>A \\lor B</math>\n|-\n|[[File:Venn0001.svg|60px]]\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Rightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|[[File:Venn0111.svg|50px]]\n|-\n|{{small|(to be tested)}}\n|\n|\n|}\n\n'''[[Hadamard transform|Walsh spectrum]]: (1,-1,-1,1)'''\n\n'''Non[[Linear#Boolean functions|linearity]]: 1''' (the function is [[bent function|bent]])\n\nIf using [[binary numeral system|binary]] values for true (1) and false (0), then ''logical conjunction'' works exactly like normal arithmetic [[multiplication]].\n\n==Applications in computer engineering{{anchor|software_AND}}==\n[[File:AND Gate diagram.svg|thumb|right|AND [[logic gate]]]]\nIn high-level computer programming and [[digital electronics]], logical conjunction is commonly represented by an infix operator, usually as a keyword such as \"<code>AND</code>\", an algebraic multiplication, or the ampersand symbol \"<code>&</code>\". Many languages also provide [[short-circuit evaluation|short-circuit]] control structures corresponding to logical conjunction.\n\nLogical conjunction is often used for bitwise operations, where <code>0</code> corresponds to false and <code>1</code> to true:\n\n* <code>0 AND 0</code> &nbsp;=&nbsp; <code>0</code>,\n* <code>0 AND 1</code> &nbsp;=&nbsp; <code>0</code>,\n* <code>1 AND 0</code> &nbsp;=&nbsp; <code>0</code>,\n* <code>1 AND 1</code> &nbsp;=&nbsp; <code>1</code>.\n\nThe operation can also be applied to two binary [[words]] viewed as [[bitstring]]s of equal length, by taking the bitwise AND of each pair of bits at corresponding positions. For example:\n\n* <code>11000110 AND 10100011</code> &nbsp;=&nbsp; <code>10000010</code>.\n\nThis can be used to select part of a bitstring using a [[Mask (computing)|bit mask]].  For example, <code>1001'''1'''101 AND 0000'''1'''000</code> &nbsp;=&nbsp; <code>0000'''1'''000</code> extracts the fifth bit of an 8-bit bitstring.\n\nIn [[computer networking]], bit masks are used to derive the network address of a [[subnetwork|subnet]] within an existing network from a given [[IP address]], by ANDing the IP address and the [[subnetwork#Binary subnet masks|subnet mask]].\n\nLogical conjunction \"<code>AND</code>\" is also used in [[SQL]] operations to form [[database]] queries.\n\nThe [[Curry–Howard correspondence]] relates logical conjunction to [[product type]]s.\n\n==Set-theoretic correspondence==\nThe  membership of an element of an [[intersection (set theory)|intersection set]] in [[set theory]] is defined in terms of a logical conjunction: ''x'' ∈ ''A'' ∩ ''B'' if and only if (''x'' ∈ ''A'') ∧ (''x'' ∈ ''B''). Through this correspondence, set-theoretic intersection shares several properties with logical conjunction, such as [[associativity]], [[commutativity]], and [[idempotence]].\n\n==Natural language==\nAs with other notions formalized in [[mathematical logic]], the logical conjunction ''and'' is related to, but not the same as, the [[grammatical conjunction]] ''and'' in natural languages.\n\nEnglish \"and\" has properties not captured by logical conjunction.  For example, \"and\" sometimes implies order.  For example, \"They got married and had a child\" in common discourse means that the marriage came before the child.  The word \"and\" can also imply a partition of a thing into parts, as \"The American flag is red, white, and blue.\"   Here it is not meant that the flag is ''at once'' red, white, and blue, but rather that it has a part of each color.\n\n==See also==\n{{colbegin|colwidth=20em}}\n* [[And-inverter graph]]\n* [[AND gate]]\n* [[Bitwise AND]]\n* [[Boolean algebra (logic)]]\n* [[Boolean algebra topics]]\n* [[Boolean conjunctive query]]\n* [[Boolean domain]]\n* [[Boolean function]]\n* [[Boolean-valued function]]\n* [[Conjunction introduction]]\n* [[Conjunction elimination]]\n* [[De Morgan's laws]]\n* [[First-order logic]]\n* [[Fréchet inequalities]]\n* [[Grammatical conjunction]]\n* [[Logical disjunction]]\n* [[Logical negation]]\n* [[Logical graph]]\n* [[Logical value]]\n* [[Operation (mathematics)|Operation]]\n* [[Peano–Russell notation]]\n* [[Propositional calculus]]\n{{colend}}\n\n==References==\n{{reflist}}\n\n==External links==\n* {{springer|title=Conjunction|id=p/c025080}}\n*[http://mathworld.wolfram.com/Conjunction.html Wolfram MathWorld: Conjunction]\n* {{cite web|url= http://www.math.hawaii.edu/~ramsey/Logic/And.html|title= \nProperty and truth table of AND propositions|archive-url= https://web.archive.org/web/20170506173821/http://www.math.hawaii.edu/~ramsey/Logic/And.html|archive-date= May 6, 2000|deadurl= }}\n\n{{Logical connectives}}\n\n[[Category:Logical connectives]]"
    },
    {
      "title": "Logical connective",
      "url": "https://en.wikipedia.org/wiki/Logical_connective",
      "text": "{{about|connectives in logical systems|connectors in natural languages|discourse connective|other logical symbols|List of logic symbols}}\n{{technical|date=April 2014}}\n\nIn [[Mathematical logic|logic]], a '''logical connective''' (also called a '''logical operator''', '''sentential connective''', or '''sentential operator''') is a [[symbol (formal)|symbol]] or [[word]] used to connect two or more [[sentence (linguistics)|sentences]] (of either a [[formal language|formal]] or a [[natural language]]) in a [[syntax (logic)|grammatically valid]] way, such that the value of the compound sentence produced depends only on that of the original sentences and on the meaning of the connective.\n\nThe most common logical connectives are '''[[Binary relation|binary]] connectives''' (also called '''[[dyadics|dyadic]] connectives''') which join two sentences which can be thought of as the function's [[operand]]s. Also commonly, [[negation]] is considered to be a '''[[Unary function|unary]] connective'''.\n\nLogical connectives along with [[Quantifier (logic)|quantifier]]s are the two main types of [[logical constant]]s used in [[formal system]]s such as [[propositional logic]] and [[predicate logic]]. Semantics of a logical connective is often, but not always, presented as a [[truth function]].\n\nA logical connective is similar to but not equivalent to a [[conditional operator]].<ref>{{cite web|last1=Cogwheel|title=What is the difference between logical and conditional /operator/|url=https://stackoverflow.com/questions/3154132/what-is-the-difference-between-logical-and-conditional-and-or-in-c|website=Stack Overflow|accessdate=9 April 2015}}</ref>\n\n==In language==\n\n===Natural language===\nIn the grammar of natural languages two sentences may be joined by a [[grammatical conjunction]] to form a ''grammatically'' [[compound sentence (linguistics)|compound sentence]]. Some but not all such grammatical conjunctions are [[truth function]]al. For example, consider the following sentences:\n\n{{ordered list\n | list-style-type = upper-alpha\n | Jack went up the hill.\n | Jill went up the hill.\n | Jack went up the hill ''and'' Jill went up the hill.\n | Jack went up the hill ''so'' Jill went up the hill.\n}}\n\nThe words ''and'' and ''so'' are ''grammatical'' conjunctions joining the sentences (A) and (B)  to form the compound sentences (C) and (D). The ''and'' in (C) is a ''logical'' connective, since the truth of (C) is completely determined by (A) and (B): it would make no sense to affirm (A) and (B) but deny (C). However, ''so'' in (D) is not a logical connective, since it would be quite reasonable to affirm (A) and (B) but deny (D): perhaps, after all, Jill went up the hill to fetch a pail of water, not because Jack had gone up the hill at all.\n\nVarious English words and word pairs express logical connectives, and some of them are synonymous. Examples are:\n\n{| class=\"wikitable sortable\"\n|-\n! Word !! Connective !! Symbol !! Logical gate\n|-\n| and || [[Logical conjunction|conjunction]] || \"∧\" || [[AND gate|AND]]\n|-\n| and then || [[Logical conjunction|conjunction]] || \"∧\" || [[AND gate|AND]]\n|-\n| and then within || [[Logical conjunction|conjunction]] || \"∧\" || [[AND gate|AND]]\n|-\n| or || [[Logical disjunction|disjunction]] || \"∨\" || [[OR gate|OR]]\n|-\n| either...or || [[Exclusive or|exclusive disjunction]] || \"⊻\" || [[XOR gate|XOR]]\n|-\n| either, but not both || [[Exclusive or|exclusive disjunction]] || \"⊻\" || [[XOR gate|XOR]]\n|-\n| implies || [[Material conditional|material implication]] || \"→\" ||[[IMPLY gate | IMPLY]]\n|-\n| is implied by || [[converse implication]] || \"←\" || \n|-\n| if...then || [[Material conditional|material implication]]  || \"→\" || [[IMPLY gate | IMPLY]]\n|-\n| ...if || [[converse implication]] || \"←\" ||\n|-\n| if and only if || [[logical biconditional|biconditional]] || \"↔\" || [[XNOR gate|XNOR]]\n|-\n| only if || [[converse implication]] || \"←\" || \n|-\n| just in case || [[logical biconditional|biconditional]] || \"↔\" || [[XNOR gate|XNOR]]\n|-\n| but || [[Logical conjunction|conjunction]] || \"∧\" || [[AND gate|AND]]\n|-\n| however || [[Logical conjunction|conjunction]] || \"∧\" || [[AND gate|AND]]\n|-\n| not both || [[Sheffer stroke|alternative denial]] || \"↑\" || [[NAND gate|NAND]]\n|-\n| neither...nor || [[Logical NOR|joint denial]] || \"↓\" || [[NOR gate|NOR]]\n|-\n| not, not that|| [[negation]] || \"¬\" || [[Inverter (logic gate)|NOT]]\n|-\n| it is false that || [[negation]] || \"¬\" || [[Inverter (logic gate)|NOT]]\n|-\n| it is not the case that || [[negation]] || \"¬\" || [[Inverter (logic gate)|NOT]]\n|-\n| although || [[Logical conjunction|conjunction]] || \"∧\" || [[AND gate|AND]]\n|-\n| even though || [[Logical conjunction|conjunction]] || \"∧\" || [[AND gate|AND]]\n|-\n| therefore ||  [[Material conditional|material implication]]  || \"→\" || [[IMPLY gate | IMPLY]]\n|-\n| so ||  [[Material conditional|material implication]]  || \"→\" || [[IMPLY gate | IMPLY]]\n|-\n| that is to say || [[logical biconditional|biconditional]] || \"↔\" || [[XNOR gate|XNOR]]\n|-\n| furthermore || [[Logical conjunction|conjunction]] || \"∧\" || [[AND gate|AND]]\n|-\n| but not || [[material nonimplication]] || \"↛\" || [[NIMPLY gate | NIMPLY]]\n|-\n| not...but || [[converse nonimplication]] || \"↚\" ||\n|-\n| no...without || [[Material conditional|material implication]] || \"→\" || [[IMPLY gate | IMPLY]]\n|-\n| without...there is no || [[converse implication]] || \"←\" || \n|}\n\n===Formal languages===\nIn formal languages, truth functions are represented by unambiguous symbols. These symbols are called ''logical connectives'', ''logical operators'', ''propositional operators'', or, in [[classical logic]], ''[[truth function|truth-functional]] connectives''. See [[well-formed formula]] for the rules which allow new well-formed formulas to be constructed by joining other well-formed formulas using truth-functional connectives.\n\nLogical connectives can be used to link more than two statements, so one can speak about ''[[arity|{{mvar|n}}-ary]] logical connective''.\n\n==Common logical connectives==\n{| class=\"infobox\" style=\"margin-left:2em; margin-bottom:1ex; text-align:center;\"\n ! colspan=2 | Symbol, name\n ! colspan=5 | Truth <br/>table\n !           | Venn <br/><small>diagram</small>\n |-\n ! colspan=8 | Unary connectives\n |-\n | colspan=2 |\n |           style=\"background-color:#ffff66;\" | {{mvar|P}}&nbsp;=\n | colspan=2 style=\"background-color:#ffff66;\" | 0\n | colspan=2 style=\"background-color:#ffff66;\" | 1\n |\n |-\n | ⊤ || style=\"text-align:left; | [[Truth]]/[[tautology (logic)|tautology]]\n |\n | colspan=2 | 1\n | colspan=2 | 1\n | [[Image:Venn11.svg|32px]]\n |-\n | || style=\"text-align:left; | Proposition {{mvar|P}}\n |\n | colspan=2 | 0\n | colspan=2 | 1\n | [[Image:Venn01.svg|32px]]\n |-\n | ⊥ || style=\"text-align:left; | [[False (logic)|Falsity]]/[[contradiction]]\n |\n | colspan=2 | 0\n | colspan=2 | 0\n | [[Image:Venn00.svg|32px]]\n |-\n | ¬ || style=\"text-align:left; | [[Negation]]\n |\n | colspan=2 | 1\n | colspan=2 | 0\n | [[Image:Venn10.svg|32px]]\n\n |-\n ! colspan=9 | Binary connectives\n |-\n ! colspan=2 rowspan=2 |\n |           style=\"background-color:#ffff66; text-align:right;\"  | {{mvar|P}}&nbsp;=\n | colspan=2 style=\"background-color:#ffff66; text-align:center;\" | 0\n | colspan=2 style=\"background-color:#ffff66; text-align:center;\" | 1\n |- style=\"background-color:#ffff66; text-align:center;\"\n | {{mvar|Q}}&nbsp;=\n | 0\n | 1\n | 0\n | 1\n |-\n | || style=\"text-align:left; | Proposition {{mvar|P}}\n | ||0||0||1||1|| [[Image:Venn0101.svg|40px]]\n |-\n | || style=\"text-align:left; | Proposition {{mvar|Q}}\n | ||0||1||0||1|| [[Image:Venn0011.svg|40px]]\n |-\n | ∧ || style=\"text-align:left; | [[Logical conjunction|Conjunction]]\n | ||0||0||0||1|| [[Image:Venn0001.svg|40px]]\n |-\n | ↑ || style=\"text-align:left; | [[Sheffer stroke|Alternative denial]]\n | ||1||1||1||0|| [[Image:Venn1110.svg|40px]]\n |-\n | ∨ || style=\"text-align:left; | [[Logical disjunction|Disjunction]]\n | ||0||1||1||1|| [[Image:Venn0111.svg|40px]]\n |-\n | ↓ || style=\"text-align:left; | [[Logical NOR|Joint denial]]\n | ||1||0||0||0|| [[Image:Venn1000.svg|40px]]\n |-\n | → || style=\"text-align:left; | [[Material conditional]]\n | ||1||1||0||1|| [[Image:Venn1011.svg|40px]]\n |-\n | <math>\\nleftrightarrow</math> || style=\"text-align:left; | [[Exclusive or]]\n | ||0||1||1||0|| [[Image:Venn0110.svg|40px]]\n |-\n | ↔ || style=\"text-align:left; | [[logical biconditional|Biconditional]]\n | ||1||0||0||1|| [[Image:Venn1001.svg|40px]]\n |-\n | ← || style=\"text-align:left; | [[Converse implication]]\n | ||1||0||1||1|| [[Image:Venn1101.svg|40px]]\n |-\n | colspan=8 style=\"text-align:center;\" | [[Truth function#Table of binary truth functions|More information]]\n |}\n\n===List of common logical connectives===\nCommonly used logical connectives include\n* [[negation|Negation (not)]]: ¬ , N (prefix), ~\n* [[logical conjunction|Conjunction (and)]]: &and; , K (prefix), & , ∙\n* [[logical disjunction|Disjunction (or)]]: &or;, A (prefix)\n* [[material conditional|Material implication (if...then)]]: &rarr; , C (prefix), &rArr; , &sup;\n* [[logical biconditional|Biconditional (if and only if)]]: &harr; , E (prefix), &equiv; , =\n\nAlternative names for biconditional are ''iff'', ''xnor'', and ''bi-implication''.\n\nFor example, the meaning of the statements ''it is raining'' and ''I am indoors'' is transformed when the two are combined with logical connectives. For statement ''P'' = ''It is raining'' and ''Q'' = ''I am indoors'':\n* It is '''not''' raining ({{not}}''P'')\n* It is raining '''and ''' I am indoors (<math>P \\wedge Q</math>)\n* It is raining '''or''' I am indoors (<math>P \\lor Q</math>)\n* '''If''' it is raining, '''then''' I am indoors (<math>P \\rightarrow Q</math>)\n* '''If''' I am indoors, '''then''' it is raining (<math>Q \\rightarrow P</math>)\n* I am indoors '''if and only if''' it is raining (<math>P \\leftrightarrow Q</math>)\n\nIt is also common to consider the ''always true'' formula and the ''always false'' formula to be connective:\n* [[Truth|True]] formula (⊤, 1, V [prefix], or T)\n* [[False (logic)|False]] formula (⊥, 0, O [prefix], or F)\n\n===History of notations===\n* Negation: the symbol ¬ appeared in [[Arend Heyting|Heyting]] in 1929.<ref name=\"autogenerated1929\">[[Heyting]] (1929) ''Die formalen Regeln der intuitionistischen Logik''.</ref><ref>Denis Roegel,;l,;l.';, (2002), ''[http://www.loria.fr/~roegel/cours/symboles-logiques.pdf Petit panorama des notations logiques du 20e siècle]'' (see chart on page 2).</ref> (compare to [[Gottlob Frege|Frege]]'s symbol ⫟ in his [[Begriffsschrift]]); the symbol ~ appeared in Russell in 1908;<ref name=\"autogenerated222\">[[Bertrand Russell|Russell]] (1908) ''Mathematical logic as based on the theory of types'' (American Journal of Mathematics 30, p222–262, also in From Frege to Gödel edited by van Heijenoort).</ref> an alternative notation is to add a horizontal line on top of the formula, as in <math>\\overline{P}</math>; another alternative notation is to use a [[prime (symbol)|prime symbol]] as in P'.\n* Conjunction: the symbol ∧ appeared in Heyting in 1929<ref name=\"autogenerated1929\"/> (compare to [[Giuseppe Peano|Peano]]'s use of the set-theoretic notation of [[intersection (set theory)|intersection]] ∩<ref>[[Giuseppe Peano|Peano]] (1889) ''[[Arithmetices principia, nova methodo exposita]]''.</ref>); & appeared at least in [[Moses Schönfinkel|Schönfinkel]] in 1924;<ref name=\"autogenerated1924\">[[Moses Schönfinkel|Schönfinkel]] (1924) '' Über die Bausteine der mathematischen Logik'', translated as ''On the building blocks of mathematical logic'' in From Frege to Gödel edited by van Heijenoort.</ref> '''.''' comes from [[George Boole|Boole]]'s interpretation of logic as an [[elementary algebra]].\n* Disjunction: the symbol ∨ appeared in [[Bertrand Russell|Russell]] in 1908<ref name=\"autogenerated222\"/> (compare to [[Giuseppe Peano|Peano]]'s use of the set-theoretic notation of [[union (set theory)|union]] ∪); the symbol + is also used, in spite of the ambiguity coming from the fact that the + of ordinary [[elementary algebra]] is an [[exclusive or]] when interpreted logically in a two-element [[Boolean ring|ring]]; punctually in the history a + together with a dot in the lower right corner has been used by [[Charles Sanders Peirce|Peirce]],<ref>[[Charles Sanders Peirce|Peirce]] (1867) ''On an improvement in Boole's calculus of logic.</ref>\n* Implication: the symbol → can be seen in [[David Hilbert|Hilbert]] in 1917;<ref>[[David Hilbert|Hilbert]] (1917/1918) ''Prinzipien der Mathematik'' (Bernays' course notes).</ref> ⊃ was used by Russell in 1908<ref name=\"autogenerated222\"/> (compare to Peano's inverted C notation); &rArr; was used in Vax.<ref>Vax (1982) ''Lexique logique'', Presses Universitaires de France.</ref>\n* Biconditional: the symbol ≡ was used at least by Russell in 1908;<ref name=\"autogenerated222\"/> ↔ was used at least by [[Alfred Tarski|Tarski]] in 1940;<ref>[[Alfred Tarski|Tarski]] (1940) ''Introduction to logic and to the methodology of deductive sciences''.</ref> ⇔ was used in Vax; other symbols appeared punctually in the history such as ⊃⊂ in [[Gerhard Gentzen|Gentzen]],<ref>[[Gerhard Gentzen|Gentzen]] (1934) ''Untersuchungen über das logische Schließen''.</ref> ~ in Schönfinkel<ref name=\"autogenerated1924\"/> or ⊂⊃ in Chazal.<ref>Chazal (1996) : Éléments de logique formelle.</ref>\n* True: the symbol 1 comes from [[George Boole|Boole]]'s interpretation of logic as an [[elementary algebra]] over the [[two-element Boolean algebra]]; other notations include <math>\\bigwedge</math> to be found in Peano.\n* False: the symbol 0 comes also from Boole's interpretation of logic as a ring; other notations include <math>\\bigvee</math> to be found in Peano.\n\nSome authors used letters for connectives at some time of the history: '''u.''' for conjunction (German's \"und\" for \"and\") and '''o.''' for disjunction (German's \"oder\" for \"or\") in earlier works by Hilbert (1904); '''N''p''''' for negation, '''K''pq''''' for conjunction, '''D''pq''''' for alternative denial, '''A''pq''''' for disjunction, '''X''pq''''' for joint denial, '''C''pq''''' for implication, '''E''pq''''' for biconditional in [[Jan Łukasiewicz|Łukasiewicz]] (1929);<ref>See Roegel</ref> cf. [[Polish notation#Polish notation for logic|Polish notation]].\n\n===Redundancy===\n\nSuch a logical connective as [[converse implication]] \"←\" is actually the same as [[material conditional]] with swapped arguments; thus, the symbol for converse implication is redundant. In some logical calculi (notably, in [[classical logic]]) certain essentially different compound statements are [[logical equivalence|logically equivalent]]. A less [[Triviality (mathematics)|trivial]] example of a redundancy is the classical equivalence between {{math|¬''P'' ∨ ''Q''}} and {{math|''P'' → ''Q''}}. Therefore, a classical-based logical system does not need the conditional operator \"→\" if \"¬\" (not) and \"∨\" (or) are already in use, or may use the \"→\" only as a [[syntactic sugar]] for a compound having one negation and one disjunction.\n\nThere are sixteen [[Boolean function]]s associating the input [[truth value]]s {{mvar|P}} and {{mvar|Q}} with four-digit [[binary numeral system|binary]] outputs.<ref>[[Józef Maria Bocheński|Bocheński]] (1959), ''A Précis of Mathematical Logic'', passim.</ref> These correspond to possible choices of binary logical connectives for [[classical logic]]. Different implementations of classical logic can choose different [[Functional completeness|functionally complete]] subsets of connectives.\n\nOne approach is to choose a ''minimal'' set, and define other connectives by some logical form, as in the example with the material conditional above.\nThe following are the [[Functional completeness#Minimal functionally complete operator sets|minimal functionally complete sets of operators]] in classical logic whose arities do not exceed 2:\n;One element: {↑}, {↓}.\n;Two elements: <math>\\{\\vee, \\neg\\}</math>, <math>\\{\\wedge, \\neg\\}</math>, <math>\\{\\to, \\neg\\}</math>, <math>\\{\\gets, \\neg\\}</math>, <math>\\{\\to, \\bot\\}</math>, <math>\\{\\gets, \\bot\\}</math>, <math>\\{\\to, \\nleftrightarrow\\}</math>, <math>\\{\\gets, \\nleftrightarrow\\}</math>, <math>\\{\\to, \\nrightarrow\\}</math>, <math>\\{\\to, \\nleftarrow\\}</math>, <math>\\{\\gets, \\nrightarrow\\}</math>, <math>\\{\\gets, \\nleftarrow\\}</math>, <math>\\{\\nrightarrow, \\neg\\}</math>, <math>\\{\\nleftarrow, \\neg\\}</math>, <math>\\{\\nrightarrow, \\top\\}</math>, <math>\\{\\nleftarrow, \\top\\}</math>, <math>\\{\\nrightarrow, \\leftrightarrow\\}</math>, <math>\\{\\nleftarrow, \\leftrightarrow\\}</math>.\n;Three elements: <math>\\{\\lor, \\leftrightarrow, \\bot\\}</math>, <math>\\{\\lor, \\leftrightarrow, \\nleftrightarrow\\}</math>, <math>\\{\\lor, \\nleftrightarrow, \\top\\}</math>, <math>\\{\\land, \\leftrightarrow, \\bot\\}</math>, <math>\\{\\land, \\leftrightarrow, \\nleftrightarrow\\}</math>, <math>\\{\\land, \\nleftrightarrow, \\top\\}</math>.\n\nAnother approach is to use with equal rights connectives of a certain convenient and functionally complete, but ''not minimal'' set. This approach requires more propositional [[axiom]]s, and each equivalence between logical forms must be either an axiom or provable as a theorem.\n\nThe situation, however, is more complicated in [[intuitionistic logic]].   Of its five connectives, {∧, ∨, →, ¬, ⊥}, only negation \"¬\" can be reduced to other connectives (see [[false (logic)#False, negation and contradiction|details]]). Neither conjunction, disjunction, nor material conditional has an equivalent form constructed of the other four logical connectives.\n\n==Properties==\nSome logical connectives possess properties which may be expressed in the theorems containing the connective. Some of those properties that a logical connective may have are:\n\n; [[Associativity]]: Within an expression containing two or more of the same associative connectives in a row, the order of the operations does not matter as long as the sequence of the operands is not changed.\n; [[Commutativity]]: The operands of the connective may be swapped preserving logical equivalence to the original expression.\n; [[Distributivity]]: A connective denoted by · distributes over another connective denoted by +, if {{math|1=''a'' · (''b'' + ''c'') = (''a'' · ''b'') + (''a'' · ''c'')}} for all operands {{mvar|a}}, {{mvar|b}}, {{mvar|c}}.\n; [[Idempotence]]: Whenever the operands of the operation are the same, the compound is logically equivalent to the operand.\n; [[Absorption Law|Absorption]]: A pair of connectives &and;, &or; satisfies the absorption law if <math>a\\land(a\\lor b)=a</math> for all operands {{mvar|a}}, {{mvar|b}}.\n; [[Monotonicity]]: If ''f''(''a''<sub>1</sub>, ..., ''a''<sub>''n''</sub>) ≤ ''f''(''b''<sub>1</sub>, ..., ''b''<sub>''n''</sub>) for all ''a''<sub>1</sub>, ..., ''a''<sub>''n''</sub>, ''b''<sub>1</sub>, ..., ''b''<sub>''n''</sub> ∈ {0,1} such that ''a''<sub>1</sub> ≤ ''b''<sub>1</sub>, ''a''<sub>2</sub> ≤ ''b''<sub>2</sub>, ..., ''a''<sub>''n''</sub> ≤ ''b''<sub>''n''</sub>. E.g., &or;, &and;, ⊤, ⊥.\n; [[Affine transformation|Affinity]]: Each variable always makes a difference in the truth-value of the operation or it never makes a difference.<!-- has this an appropriate generalization to non-classical logics? --> E.g., &not;, &harr;,  <math>\\nleftrightarrow</math>, ⊤, ⊥.\n; [[Duality (mathematics)|Duality]]: To read the truth-value assignments for the operation from top to bottom on its [[truth table]] is the same as taking the complement of reading the table of the same or another connective from bottom to top. Without resorting to truth tables it may be formulated as {{math|1=''g&#771;''(¬''a''<sub>1</sub>, ..., ¬''a''<sub>''n''</sub>) = ¬''g''(''a''<sub>1</sub>, ..., ''a''<sub>''n''</sub>)}}. E.g., &not;.\n; Truth-preserving: The compound all those argument are tautologies is a tautology itself. E.g., &or;, &and;, ⊤, &rarr;, &harr;, ⊂ (see [[Validity (logic)|validity]]).\n; Falsehood-preserving: The compound all those argument are [[contradiction]]s is a contradiction itself. E.g., &or;, &and;, <math>\\nleftrightarrow</math>, ⊥, ⊄, ⊅ (see [[Validity (logic)|validity]]).\n; [[Involution (mathematics)|Involutivity]] (for unary connectives): {{math|1=''f''(''f''(''a'')) = ''a''}}. E.g. negation in classical logic.\n\nFor classical and intuitionistic logic, the \"=\"<!-- BTW why not \"⇔\"? --> symbol means that corresponding implications \"…→…\" and \"…←…\" for logical compounds can be both proved as theorems, and the \"≤\"<!-- BTW why not \"⇒\"/\"→\"? --> symbol means that \"…→…\" for logical compounds is a consequence of corresponding \"…→…\" connectives for propositional variables. Some [[many-valued logic]]s may have incompatible definitions of equivalence and order (entailment).\n\nBoth conjunction and disjunction are associative, commutative and idempotent in classical logic, most varieties of many-valued logic and intuitionistic logic. The same is true about distributivity of conjunction over disjunction and disjunction over conjunction, as well as for the absorption law.\n\nIn classical logic and some varieties of many-valued logic, conjunction and disjunction are dual, and negation is self-dual, the latter is also self-dual in intuitionistic logic. <!-- I am not sure about ∧ and ∨. Aforementioned definition of duality does not imply that one connective is equivalent to a form with two-layer negation, so such intuitionistic duality is plausible. But one should carefully verify such additions, at least because intuitionistic negation is not an involution and hence the duality relation is not symmetric. -->\n\n{{expand section|date=March 2012}}\n\n==Order of precedence==\nAs a way of reducing the number of necessary parentheses, one may introduce [[precedence rule]]s: &not; has higher precedence than &and;, &and; higher than &or;, and &or; higher than &rarr;. So for example, <math>P \\vee Q \\wedge{\\neg R} \\rightarrow S</math> is short for <math>(P \\vee (Q \\wedge (\\neg R))) \\rightarrow S</math>.\n\nHere is a table that shows a commonly used precedence of logical operators.<ref>{{citation|title=Discrete Mathematics Using a Computer|first1=John|last1=O'Donnell|first2=Cordelia|last2=Hall|first3=Rex|last3=Page|publisher=Springer|year=2007|isbn=9781846285981|page=120|url=https://books.google.com/books?id=KKxyQQWQam4C&pg=PA120}}.</ref>\n\n:<math>\n\\begin{array}{c|c}\n\\text{Operator} & \\text{Precedence} \\\\\n\\hline\n\\neg & 1\n\\\\\n\\land & 2\n\\\\\n\\vee & 3\n\\\\\n\\to & 4\n\\\\\n\\leftrightarrow & 5\n\\end{array}\n</math>\n\nHowever, not all compilers use the same order; for instance, an ordering in which disjunction is lower precedence than implication or bi-implication has also been used.<ref>{{citation|title=Software Abstractions: Logic, Language, and Analysis|first=Daniel|last=Jackson|publisher=MIT Press|year=2012|isbn=9780262017152|page=263|url=https://books.google.com/books?id=DDv8Ie_jBUQC&pg=PA263}}.</ref> Sometimes precedence between conjunction and disjunction is unspecified requiring to provide it explicitly in given formula with parentheses. The order of precedence determines which connective is the \"main connective\" when interpreting a non-atomic formula.\n\n==Computer science==\n{{expand section|date=March 2012}}\n\nA truth-functional approach to logical operators is implemented as [[logic gate]]s in [[digital circuit]]s. Practically all digital circuits (the major exception is [[DRAM]]) are built up from [[logical nand|NAND]], [[logical nor|NOR]], [[negation|NOT]], and [[logic gate|transmission gate]]s; see more details in [[Truth function#Computer science|Truth function in computer science]]. Logical operators over [[bit array|bit vectors]] (corresponding to finite [[Boolean algebra (structure)|Boolean algebras]]) are [[bitwise operation]]s.\n\nBut not every usage of a logical connective in [[computer programming]] has a Boolean semantic. For example, [[lazy evaluation]] is sometimes implemented for {{math|''P'' ∧ ''Q''}} and {{math|''P'' ∨ ''Q''}}, so these connectives are not commutative if either or both of the expressions {{mvar|P}}, {{mvar|Q}} have [[side effect (computer science)|side effect]]s. Also, a [[conditional (programming)|conditional]], which in some sense corresponds to the [[material conditional]] connective, is essentially non-Boolean because for <code>if (P) then Q;</code> the consequent&nbsp;Q is not executed if the [[antecedent (logic)|antecedent]]&nbsp;P is false (although a compound as a whole is successful ≈ \"true\" in such case). This is closer to intuitionist and [[constructive mathematics|constructivist]] views on the material conditional, rather than to classical logic's ones.\n\n==See also==\n{{Col-begin}}\n{{Col-break}}\n* [[Boolean domain]]\n* [[Boolean function]]\n* [[Boolean logic]]\n* [[Boolean-valued function]]\n* [[List of Boolean algebra topics]]\n{{Col-break}}\n{{Portal|Logic|Thinking}}\n* [[Logical constant]]\n* [[Modal operator]]\n* [[Propositional calculus]]\n* [[Truth function]]\n* [[Truth table]]\n* [[Truth value]]s\n{{Col-end}}\n\n==Notes==\n{{Reflist}}\n\n==References==\n* [[Józef Maria Bocheński|Bocheński, Józef Maria]] (1959), ''A Précis of Mathematical Logic'', translated from the French and German editions by Otto Bird, D. Reidel, Dordrecht, South Holland.\n* {{Citation | last1=Enderton | first1=Herbert |author1-link=Herbert Enderton| title=A Mathematical Introduction to Logic | publisher=Academic Press | location=Boston, MA | edition=2nd | isbn=978-0-12-238452-3 | year=2001}}\n* {{citation|last=Gamut|first=L.T.F|authorlink=L. T. F. Gamut|title=Logic, Language and Meaning|publisher=University of Chicago Press|year=1991|volume=1|pages=54&ndash;64|contribution=Chapter 2|oclc=21372380}}\n* {{Citation|author=[[Wolfgang Rautenberg|Rautenberg, W.]]|doi=10.1007/978-1-4419-1221-3|title=A Concise Introduction to Mathematical Logic|url=http://www.springerlink.com/content/978-1-4419-1220-6/|publisher=[[Springer Science+Business Media]] |location=[[New York City|New York]]|edition=3rd|isbn=978-1-4419-1220-6|year=2010}}.\n\n==Further reading==\n* {{cite book|author=Lloyd Humberstone|title=The Connectives|year=2011|publisher=MIT Press|isbn=978-0-262-01654-4}}\n\n==External links==\n* {{springer|title=Propositional connective|id=p/p075490}}\n* Lloyd Humberstone (2010), \"[http://plato.stanford.edu/entries/connectives-logic/ Sentence Connectives in Formal Logic]\", [[Stanford Encyclopedia of Philosophy]] (An [[abstract algebraic logic]] approach to connectives.)\n* John MacFarlane (2005), \"[http://plato.stanford.edu/entries/logical-constants/ Logical constants]\", [[Stanford Encyclopedia of Philosophy]].\n\n{{Logical connectives}}\n{{Mathematical logic}}\n\n{{DEFAULTSORT:Logical Connective}}\n[[Category:Logical connectives]]\n[[Category:Logic symbols]]\n\n[[da:Logisk konnektiv]]\n[[he:קשר לוגי]]"
    },
    {
      "title": "Logical disjunction",
      "url": "https://en.wikipedia.org/wiki/Logical_disjunction",
      "text": "{{redirect|Disjunction|the logic gate|OR gate|separation of chromosomes|Meiosis|disjunctions in distribution|Disjunct distribution}}\n{{more footnotes|date=June 2016}}\n\n{{Infobox logical connective\n| title        = Logical disjunction\n| other titles = OR\n| Venn diagram = Venn0111.svg\n| definition   = <math>x+y</math>\n| truth table  = <math>(0111)</math>\n| logic gate   = OR_ANSI.svg\n| DNF          = <math>x+y</math>\n| CNF          = <math>x+y</math>\n| Zhegalkin    = <math>x \\oplus y \\oplus xy</math>\n| 0-preserving = yes\n| 1-preserving = yes\n| monotone     = yes\n| affine       = no\n| self-dual    = no\n}}\n[[File:Venn 0111 1111.svg|thumb|220px|Venn diagram of <math>\\scriptstyle A \\lor B \\lor C</math>]]\n\nIn [[logic]] and [[mathematics]], '''or''' is the [[truth function|truth-functional]] operator of ('''inclusive''') '''disjunction''', also known as '''alternation'''; the ''or'' of a set of operands is true [[if and only if]] ''one or more'' of its operands is true.  The [[logical connective]] that represents this operator is typically written as ∨ or +.\n\n<math>A \\lor B</math> is true if <math>A</math> is true, or if <math>B</math> is true, or if both <math>A</math> and <math>B</math> are true.\n\nIn logic, ''or'' by itself means the ''inclusive'' ''or'', distinguished from an [[exclusive or]], which is false when both of its arguments are true, while an \"or\" is true in that case.\n\nAn operand of a disjunction is called a '''disjunct'''.\n\nRelated concepts in other fields are:\n* In [[natural language]], the [[Grammatical conjunction#Coordinating conjunctions|coordinating conjunction]] \"or\".\n* In programming languages, the [[short-circuit evaluation|short-circuit or]] [[control structure]].\n* In [[set theory]], [[union (set theory)|union]].\n* In [[predicate logic]], [[existential quantification]].\n\n==Notation==\n'''Or''' is usually expressed with an infix operator: in mathematics and logic, '''∨'''; in electronics, '''+'''; and in most programming languages, '''|''', '''||''', or '''or'''.   In [[Jan Łukasiewicz]]'s [[Polish notation#Polish notation for logic|prefix notation for logic]], the operator is '''A''', for Polish ''alternatywa'' (English: alternative).<ref>[[Józef Maria Bocheński]] (1959), ''A Précis of Mathematical Logic'', translated by Otto Bird from the French and German editions, Dordrecht, North Holland:  D. Reidel, passim.</ref>\n\n==Definition==\n'''Logical disjunction''' is an [[logical operation|operation]] on two [[logical value]]s, typically the values of two [[proposition]]s, that has a value of ''false'' if and only if both of its operands are false. More generally, a disjunction is a logical formula that can have one or more [[literal (mathematical logic)|literal]]s separated only by 'or's. A single literal is often considered to be a degenerate disjunction.\n\nThe disjunctive [[identity element|identity]] is false, which is to say that the ''or'' of an expression with false has the same value as the original expression. In keeping with the concept of [[vacuous truth]], when disjunction is defined as an operator or function of arbitrary [[arity]], the empty disjunction (OR-ing over an empty set of operands) is generally defined as false.\n\n===Truth table===\n\nThe [[truth table]] of <math>A \\lor B</math>:\n\n{| class=\"wikitable\" style=\"text-align:center; background-color: #ddffdd;\"\n|- bgcolor=\"#ddeeff\"\n| <math>A</math> || <math>B</math> || <math>A \\lor B</math>\n|-\n| T || T || T\n|-\n| T || F || T\n|-\n| F || T || T\n|-\n| F || F || F\n|}\n\n==Properties==\n\nThe following properties apply to disjunction:\n\n*[[associativity]]: <math>a \\lor (b \\lor c) \\equiv (a \\lor b) \\lor c </math>\n*[[commutativity]]: <math>a \\lor  b \\equiv b \\lor a </math>\n*[[distributivity]]: <math>(a \\lor (b \\land c)) \\equiv ((a \\lor b) \\land (a \\lor c))</math>\n:::<math>(a \\lor (b \\lor c)) \\equiv ((a \\lor b) \\lor (a \\lor c))</math>\n:::<math>(a \\lor (b \\equiv c)) \\equiv ((a \\lor b) \\equiv (a \\lor c))</math>\n\n*[[idempotency]]: <math>a \\lor a \\equiv a </math>\n*[[monotonicity]]: <math>(a \\rightarrow b) \\rightarrow ((c \\lor a) \\rightarrow (c \\lor b))</math>\n:::<math>(a \\rightarrow b) \\rightarrow ((a \\lor c) \\rightarrow (b \\lor c))</math>\n\n*'''truth-preserving''': The interpretation under which all variables are assigned a [[truth value]] of 'true' produces a truth value of 'true' as a result of disjunction.\n*'''falsehood-preserving''': The interpretation under which all variables are assigned a [[truth value]] of 'false' produces a truth value of 'false' as a result of disjunction.\n\n==Symbol==\nThe mathematical symbol for logical disjunction varies in the literature. In addition to the word \"or\", and the formula \"A''pq''\", the symbol \"<math>\\lor</math>\", deriving from the Latin word ''[[wikt:en:vel#Latin|vel]]'' (“either”, “or”) is commonly used for disjunction. For example: \"''A'' <math>\\lor</math> ''B''&nbsp;\" is read as \"''A'' or ''B''&nbsp;\". Such a disjunction is false if both ''A'' and ''B'' are false. In all other cases it is true.\n\nAll of the following are disjunctions:\n: <math>A \\lor B</math>\n: <math>\\neg A \\lor B</math>\n: <math>A \\lor \\neg B \\lor \\neg C \\lor D \\lor \\neg E.</math>\n\nThe corresponding operation in set theory is the [[union (set theory)|set-theoretic union]].\n\n==Applications in computer science==\n[[File:Or-gate-en.svg|thumb|150px|OR [[logic gate]]]]\n[[Operator (programming)|Operators]] corresponding to logical disjunction exist in most [[programming language]]s.\n\n===Bitwise operation===\nDisjunction is often used for bitwise operations. Examples:\n* 0 or 0 = 0\n* 0 or 1 = 1\n* 1 or 0 = 1\n* 1 or 1 = 1\n* 1010 or 1100 = 1110\n\nThe <code>or</code> operator can be used to set bits in a [[bit field]] to 1, by <code>or</code>-ing the field with a constant field with the relevant bits set to 1. For example, <code>x = x | 0b00000001</code> will force the final bit to 1 while leaving other bits unchanged.\n\n===Logical operation===\nMany languages distinguish between bitwise and logical disjunction by providing two distinct operators; in languages following C, bitwise disjunction is performed with the single pipe (<code>|</code>) and logical disjunction with the double pipe (<code>||</code>) operators.\n\nLogical disjunction is usually [[Short-circuit evaluation|short-circuited]]; that is, if the first (left) operand evaluates to <code>true</code> then the second (right) operand is not evaluated. The logical disjunction operator thus usually constitutes a [[sequence point]].\n\n{{anchor|parallel-or}}\nIn a parallel (concurrent) language, it is possible to short-circuit both sides: they are evaluated in parallel,\nand if one terminates with value true, the other is interrupted. This operator is thus called the '''parallel or'''.\n\nAlthough in most languages the type of a logical disjunction expression is boolean and thus can only have the value <code>true</code> or <code>false</code>, in some (such as [[Python programming language|Python]] and [[JavaScript]]) the logical disjunction operator returns one of its operands: the first operand if it evaluates to a true value, and the second operand otherwise.\n\n===Constructive disjunction===\nThe [[Curry–Howard correspondence]] relates a [[constructivism (mathematics)|constructivist]] form of disjunction to [[tagged union]] types.\n\n==Union==\nThe [[Element (mathematics)|membership]] of an element of a [[union (set theory)|union set]] in [[set theory]] is defined in terms of a logical disjunction: ''x'' ∈ ''A'' ∪ ''B'' if and only if (''x'' ∈ ''A'') ∨ (''x'' ∈ ''B''). Because of this, logical disjunction satisfies many of the same identities as set-theoretic union, such as associativity, commutativity, distributivity, and [[de Morgan's laws]], identifying [[logical conjunction]] with [[set intersection]], [[logical negation]] with [[set complement]].\n\n==Natural language==\nAs with other notions formalized in [[mathematical logic]], the [[meaning (linguistics)|meaning]] of the natural-language [[Grammatical conjunction#Coordinating conjunctions|coordinating conjunction]] ''or'' is closely related to but different from the logical ''or''. For example, \"Please ring me or send an email\" likely means \"do one or the other, but not both\". On the other hand, \"Her grades are so good that either she's very bright or she studies hard\" does not exclude the possibility of both. In other words, in ordinary language \"or\" (even if used with \"either\") can mean either the inclusive \"or\" [inclusive-]or the exclusive \"or.\"\n\n==See also==\n{{col-begin}}\n{{col-break}}\n* [[Affirming a disjunct]]\n* [[Bitwise OR]]\n* [[Boolean algebra (logic)]]\n* [[Boolean algebra topics]]\n* [[Boolean domain]]\n{{col-break}}\n* [[Boolean function]]\n* [[Boolean-valued function]]\n* [[Disjunctive syllogism]]\n* [[Disjunction elimination]]\n* [[Disjunction introduction]]\n* [[First-order logic]]\n* [[Fréchet inequalities]]\n{{col-break}}\n* [[Logical graph]]\n* [[Logical value]]\n* [[Operation (mathematics)|Operation]]\n* [[Operator (programming)]]\n* [[OR gate]]\n* [[Propositional calculus]]\n{{col-end}}\n\n==Notes==\n* [[George Boole]], closely following analogy with ordinary mathematics, premised, as a necessary condition to the definition of \"x + y\", that x and y were mutually exclusive. [[William Stanley Jevons|Jevons]], and practically all mathematical logicians after him, advocated, on various grounds, the definition of \"logical addition\" in a form which does not necessitate mutual exclusiveness.\n\n==References==\n{{Reflist}}\n\n==External links==\n* {{springer|title=Disjunction|id=p/d033260}}\n* {{cite SEP |url-id=disjunction |title=Disjunction |last=Aloni |first=Maria}}\n* Eric W. Weisstein. [http://mathworld.wolfram.com/Disjunction.html  \"Disjunction.\"] From MathWorld—A Wolfram Web Resource\n\n{{Logical connectives}}\n\n[[Category:Logical connectives]]"
    },
    {
      "title": "Logical equality",
      "url": "https://en.wikipedia.org/wiki/Logical_equality",
      "text": "{{For|the corresponding concept in [[combinational logic]]|XNOR gate}}\n\n{{Infobox logical connective\n| title        = Logical equality\n| other titles = EQ, XNOR\n| Venn diagram = Venn1001.svg\n| definition   = <math>x = y</math>\n| truth table  = <math>(1001)</math>\n| logic gate   = XNOR_ANSI.svg\n| DNF          = <math>x \\cdot y + \\overline{x} \\cdot \\overline{y}</math>\n| CNF          = <math>(\\overline{x} + y ) \\cdot (x + \\overline{y})</math>\n| Zhegalkin    = <math>1 \\oplus x \\oplus y</math>\n| 0-preserving = {{#switch:Нет|Да|да=yes|Нет|нет=no}}\n| 1-preserving = {{#switch:Да|Да|да=yes|Нет|нет=no}}\n| monotone     = {{#switch:Нет|Да|да=yes|Нет|нет=no}}\n| affine       = {{#switch:Да|Да|да=yes|Нет|нет=no}}\n| self-dual    = {{#switch:Нет|Да|да=yes|Нет|нет=no}}\n}}\n\n'''Logical equality''' is a [[logical operator]] that corresponds to [[equality (mathematics)|equality]] in [[Boolean algebra (logic)|Boolean algebra]] and to the [[logical biconditional]] in [[propositional calculus]].  It gives the [[function (mathematics)|functional]] value ''[[Truth|true]]'' if both functional arguments have the same [[logical value]], and ''[[False (logic)|false]]'' if they are different.\n\nIt is customary practice in various applications, if not always technically precise, to indicate the operation of '''logical equality''' on the logical operands ''x'' and ''y'' by any of the following forms:\n\n:<math>\\begin{align}\n x &\\leftrightarrow y  &  x &\\Leftrightarrow y  &  Exy \\\\\n x &\\mathrm{~EQ~} y  &  x &= y\n\\end{align}</math><!-- should be \"\\mathrel{\\mathrm{EQ}}\", but it is broken [https://phabricator.wikimedia.org/T148304] -->\n\nSome logicians, however, draw a firm distinction between a ''functional form'', like those in the left column, which they interpret as an application of a function to a pair of arguments — and thus a mere indication that the value of the compound expression depends on the values of the component expressions — and an ''equational form'', like those in the right column, which they interpret as an assertion that the arguments have equal values, in other words, that the functional value of the compound expression is ''true''.\n\nIn [[mathematics]], the plus sign \"+\" almost invariably indicates an operation that satisfies the axioms assigned to addition in the type of [[algebraic structure]] that is known as a ''[[field (mathematics)|field]]''.  For boolean algebra, this means that the logical operation signified by \"+\" is not the same as the [[inclusive disjunction]] signified by \"∨\" but is actually equivalent to the logical inequality operator signified by \"≠\", or what amounts to the same thing, the [[exclusive disjunction]] signified by \"XOR\" or \"⊕\".  Naturally, these variations in usage have caused some failures to communicate between mathematicians and switching engineers over the years.  At any rate, one has the following array of corresponding forms for the symbols associated with logical inequality:\n\n:<math>\\begin{align}\n x &+ y  &  x &\\not\\equiv y  &  Jxy \\\\\n x &\\mathrm{~XOR~} y  & x &\\ne y\n\\end{align}</math><!-- should be \"\\mathrel{\\mathrm{XOR}}\", but it is broken [https://phabricator.wikimedia.org/T148304] -->\n\nThis explains why \"EQ\" is often called \"[[XNOR gate|XNOR]]\" in the [[combinational logic]] of circuit engineers, since it is the ''negation'' of the ''[[XOR]]'' operation; \"NXOR\" is a less commonly used alternative.<ref>{{citation|title=Using Java 2|first1=Brian|last1=Keeton|first2=Chuck|last2=Cavaness|first3=Geoff|last3=Friesen|publisher=Que Publishing|year=2001|isbn=9780789724687|page=112|url=https://books.google.com/books?id=yhFxiVyd1MgC&pg=PA112}}.</ref>  Another rationalization of the admittedly circuitous name \"XNOR\" is that one begins with the \"both false\" operator NOR and then adds the eXception \"or both true\".\n\n==Definition==\n\n'''Logical equality''' is an [[logical operation|operation]] on two [[logical value]]s, typically the values of two [[proposition]]s, that produces a value of ''true'' if and only if both operands are false or both operands are true.\n\nThe [[truth table]] of '''p EQ q''' (also written as '''p = q''', '''p ↔ q''', '''p ≡ q''', or '''p == q''') is as follows:\n\n[[File:Venn1001.svg|thumb|right|150px|The [[Venn diagram]] of A EQ B (red part is true)]]\n:{| class=\"wikitable\" style=\"text-align:center\"\n|+ Logical&nbsp;equality\n! p\n! q\n! p = q\n|-\n| 0 || 0 || 1\n|-\n| 0 || 1 || 0\n|-\n| 1 || 0 || 0\n|-\n| 1 || 1 || 1\n|}\n\n{{Clear}}\n\n==Alternative descriptions==\n\nThe form (''x'' = ''y'') is equivalent to the form (''x'' ∧ ''y'') ∨ (¬''x'' ∧ ¬''y'').\n\n<math>(x = y) = \\lnot(x \\oplus y) = \\lnot x \\oplus y = x \\oplus \\lnot y = (x \\land y) \\lor (\\lnot x \\land \\lnot y) = (\\lnot x \\lor y) \\land (x \\lor \\lnot y)</math>\n\nFor the operands ''x'' and ''y'', the [[truth table]] of the logical equality operator is as follows:\n\n:{| class=\"wikitable\"\n! colspan=\"2\" rowspan=\"2\" | <math>x \\leftrightarrow y </math> !! colspan=\"2\" | y\n|-\n! T !! F\n|-\n! rowspan=\"2\" | x !! T\n| style=\"padding: 1em;\" | T\n| style=\"padding: 1em;\" | F\n|-\n! F\n| style=\"padding: 1em;\" | F\n| style=\"padding: 1em;\" | T\n|}\n\n== See also ==\n{{Portal|Thinking}}\n* [[Boolean function]]\n* [[If and only if]]\n* [[Logical equivalence]]\n* [[Logical biconditional]]\n* [[Propositional calculus]]\n\n==References==\n{{reflist}}\n\n==External links==\n* Mathworld, [http://mathworld.wolfram.com/XNOR.html XNOR]\n\n{{Logical connectives}}\n\n{{DEFAULTSORT:Logical Equality}}\n[[Category:Logical connectives]]\n[[Category:Logic gates]]\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "Logical NOR",
      "url": "https://en.wikipedia.org/wiki/Logical_NOR",
      "text": "{{About|NOR in the logical sense|the electronic gate|NOR gate|other uses|Nor (disambiguation){{!}}Nor}}\n{{redirect-distinguish2|Peirce arrow|[[Pierce-Arrow]], an automobile manufacturer}}\n\n{{Infobox logical connective\n| title        = Logical NOR\n| other titles = NOR\n| Venn diagram = Venn1000.svg\n| definition   = <math>\\overline{x + y}</math>\n| truth table  = <math>(1000)</math>\n| logic gate   = NOR_ANSI.svg\n| DNF          = <math>\\overline{x} \\cdot \\overline{y}</math>\n| CNF          = <math>\\overline{x} \\cdot \\overline{y}</math>\n| Zhegalkin    = <math>1 \\oplus x \\oplus y \\oplus xy</math>\n| 0-preserving = {{#switch:Нет|Да|да=yes|Нет|нет=no}}\n| 1-preserving = {{#switch:Нет|Да|да=yes|Нет|нет=no}}\n| monotone     = {{#switch:Нет|Да|да=yes|Нет|нет=no}}\n| affine       = {{#switch:Нет|Да|да=yes|Нет|нет=no}}\n| self-dual    = {{#switch:Нет|Да|да=yes|Нет|нет=no}}\n}}\n\nIn [[boolean logic]], '''logical nor''' or '''joint denial''' is a truth-functional operator which produces a result that is the negation of [[logical disjunction|logical or]].  That is, a sentence of the form (''p'' NOR ''q'') is true precisely when neither ''p'' nor ''q'' is true—i.e. when both of ''p'' and ''q'' are ''false''. In [[grammar]], '''nor''' is a [[grammatical conjunction|coordinating conjunction]].\n\nThe NOR operator is also known as '''Peirce's arrow'''—[[Charles Sanders Peirce]] introduced the symbol ↓ for it,<ref name=\"BüningLettmann1999\">{{cite book|author1=Hans Kleine Büning|author2=Theodor Lettmann|title=Propositional logic: deduction and algorithms|url=https://books.google.com/books?id=3oJE9yczr3EC&pg=PA2|year=1999|publisher=Cambridge University Press|isbn=978-0-521-63017-7|page=2}}</ref> and demonstrated that the logical NOR is completely expressible: by combining uses of the logical NOR it is possible to express any logical operation on two variables. Thus, as with its [[duality (mathematics)|dual]], the [[logical nand|NAND operator]] (a.k.a. the [[Sheffer stroke]]—symbolized as either ↑, | or /), NOR can be used by itself, without any other logical operator, to constitute a logical [[formal system]] (making NOR [[functional completeness|functionally complete]]). It is also known as '''[[Willard Van Orman Quine|Quine]]'s dagger''' (his symbol was †), the '''{{visible anchor|ampheck}}''' (from Ancient Greek {{lang|grc|ἀμφήκης}}, {{transl|grc|amphēkēs}}, \"cutting both ways\") by Peirce,<ref>C.S. Peirce, [[Charles Sanders Peirce bibliography#CP|CP]] 4.264</ref> or '''neither-nor'''.\n\nOther ways of notating <math>P \\downarrow Q</math> include, P NOR Q, and \"X''pq''\" (in [[Józef_Maria_Bocheński|Bocheński notation]]).\nIt is logically equivalent to <math>\\neg(P \\lor Q)</math>, where the symbol <math>\\lor</math> signifies OR and <math>\\neg</math> signifies the [[negation]].\n\nThe [[computer]] used in the spacecraft that first carried humans to the [[moon]], the [[Apollo Guidance Computer]], was constructed entirely using NOR gates with three inputs.<ref name=\"Hall-1996\">{{Citation\n  | first = Eldon C.\n  | last = Hall\n  | author-link = Eldon C. Hall\n  | title = Journey to the Moon: The History of the Apollo Guidance Computer\n  | place = Reston, Virginia, USA\n  | publisher = [[American Institute of Aeronautics and Astronautics|AIAA]]\n  | year = 1996\n  | page = 196\n  | doi =\n  | isbn = 1-56347-185-X }}\n</ref>\n\n==Definition==\n\nThe '''NOR operation''' is a [[logical operation]] on two [[logical value]]s, typically the values of two [[proposition]]s, that produces a value of ''true'' if and only if both operands are false.  In other words, it produces a value of ''false'' if and only if at least one operand is true.\n\n===Truth table===\nThe [[truth table]] of <math>P \\downarrow Q</math> (also written as '''P NOR Q''') is as follows:\n\n{| class=\"wikitable\" style=\"text-align:center; background-color: #ddffdd;\"\n|- bgcolor=\"#ddeeff\"\n| <math> P </math> || <math> Q </math> || <math> P \\downarrow Q </math>\n|-\n| T || T || F\n|-\n| T || F || F\n|-\n| F || T || F\n|-\n| F || F || T\n|}\n\n===Logical Equivalences===\n\nThe logical NOR <math>\\downarrow</math> is the negation of the disjunction:\n\n{| style=\"text-align: center; border: 1px solid darkgray;\"\n|-\n|<math>P \\downarrow Q</math>\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|<math>\\neg (P \\lor Q)</math>\n|-\n|[[File:Venn1000.svg|50px]]\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|<math>\\neg</math> [[File:Venn0111.svg|50px]]\n|}\n\n==Properties==\nLogical NOR does not possess any of the five qualities (truth-preserving, false-preserving, [[Linear#Boolean functions|linear]], [[Monotonic#Boolean functions|monotonic]], self-dual) required to be absent from at least one member of a set of [[functional completeness|functionally complete]] operators. Thus, the set containing only NOR suffices as a complete set.\n\n==Other Boolean Operations in terms of the Logical NOR==\n\nNOR has the interesting feature that all other logical operators can be expressed by interlaced NOR operations.\nThe [[Sheffer stroke|logical NAND]] operator also has this ability.\n\nExpressed in terms of NOR <math>\\downarrow</math>, the usual operators of propositional logic are:\n\n{|\n|-\n|<!--- not --->\n{| style=\"text-align: center; border: 1px solid darkgray;\"\n|-\n|<math>\\neg P</math>\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|<math>P \\downarrow P</math>\n|-\n|<math>\\neg</math> [[File:Venn01.svg|36px]]\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|[[File:Venn10.svg|36px]]\n|}<!--- end not--->\n|&nbsp;&nbsp;&nbsp;\n|<!--- arrow --->\n{| style=\"text-align: center; border: 1px solid darkgray;\" \n|-\n|<math>P \\rightarrow Q</math>\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|<math>\\Big( (P \\downarrow P) \\downarrow Q \\Big)</math>\n|<math>\\downarrow</math>\n|<math>\\Big( (P \\downarrow P) \\downarrow Q \\Big)</math>\n|-\n|[[File:Venn1011.svg|50px]]\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|[[File:Venn0100.svg|50px]]\n|<math>\\downarrow</math>\n|[[File:Venn0100.svg|50px]]\n|}<!--- end arrow --->\n|-\n|&nbsp;\n|-\n|<!--- and --->\n{| style=\"text-align: center; border: 1px solid darkgray;\" \n|-\n|<math>P \\land Q</math>\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|<math>(P \\downarrow P)</math> \n|<math>\\downarrow</math> \n|<math>(Q \\downarrow Q)</math>\n|-\n|[[File:Venn0001.svg|50px]]\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|[[File:Venn1010.svg|50px]] \n|<math>\\downarrow</math> \n|[[File:Venn1100.svg|50px]]\n|}<!--- end and --->\n|&nbsp;&nbsp;&nbsp;\n|<!--- or --->\n{| style=\"text-align: center; border: 1px solid darkgray;\" \n|-\n|<math>P \\lor Q</math>\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|<math>(P \\downarrow Q)</math> \n|<math>\\downarrow</math> \n|<math>(P \\downarrow Q)</math> \n|-\n|[[File:Venn0111.svg|50px]]\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|[[File:Venn1000.svg|50px]] \n|<math>\\downarrow</math> \n|[[File:Venn1000.svg|50px]] \n|}<!--- end or --->\n|}\n\n==See also==\n\n{{col-begin}}\n{{col-break}}\n* [[Bitwise operation|Bitwise NOR]]\n* [[Boolean algebra (logic)|Boolean algebra]]\n* [[Boolean domain]]\n* [[Boolean function]]\n{{col-break}}\n* [[Functional completeness]]\n* [[NOR gate]]\n* [[Propositional logic]]\n* [[Sole sufficient operator]]\n{{col-end}}\n\n==References==\n{{reflist}}\n\n{{Logical connectives}}\n\n[[Category:Logical connectives]]\n\n[[de:Peirce-Funktion]]\n[[it:Algebra di Boole#NOR]]"
    },
    {
      "title": "Logical truth",
      "url": "https://en.wikipedia.org/wiki/Logical_truth",
      "text": "'''Logical truth''' is one of the most fundamental [[concept]]s in [[logic]], and there are different theories on its nature. A logical truth is a [[Statement (logic)|statement]] which is [[truth|true]], and remains true under all [[interpretation (logic)|reinterpretations]] of its components other than its [[logical constant]]s. It is a type of [[Analytic–synthetic distinction|analytic statement]]. All of [[philosophical logic]] can be thought of as providing accounts of the nature of logical truth, as well as [[logical consequence]].<ref>[[Willard Van Orman Quine|Quine, Willard Van Orman]], ''Philosophy of logic''</ref>\n\nLogical truths (including [[Tautology (logic)|tautologies]]) are truths which are considered to be '''necessarily true'''. This is to say that they are considered to be such that they could not be untrue and no situation could arise which would cause us to reject a logical truth.  It must be true in every sense of intuition, practices, and bodies of beliefs. However, it is not universally agreed that there are any statements which are ''necessarily'' true.\n\nA logical truth is considered by some philosophers to be a statement which is true in all [[possible world]]s. This is contrasted with [[fact]]s (which may also be referred to as ''contingent claims'' or ''synthetic claims'') which are true in ''this'' world, as it has historically unfolded, but which is not true in at least one possible world, as it might have unfolded. The [[proposition]] \"If p and q, then p\" and the proposition \"All married people are married\" are logical truths because they are true due to their inherent structure and not because of any facts of the world. \nLater, with the rise of formal logic a logical truth was considered to be a statement which is true under all possible interpretations.\n\nThe existence of logical truths has been put forward by [[rationalist]] philosophers as an objection to [[empiricism]] because they hold that it is impossible to account for our [[knowledge]] of logical truths on empiricist grounds. Empiricists commonly respond to this objection by arguing that logical truths (which they usually deem to be mere tautologies), are analytic and thus do not purport to describe the world.\n\n== Logical truths and analytic truths ==\n{{Main|Analytic–synthetic distinction}}\n\nLogical truths, being analytic statements, do not contain any information about any matters of [[fact]]. Other than logical truths, there is also a second class of analytic statements, typified by \"no bachelor is married\". The characteristic of such a statement is that it can be turned into a logical truth by substituting synonyms for synonyms ''[[salva veritate]]''. \"No bachelor is married\" can be turned into \"no unmarried man is married\" by substituting \"unmarried man\" for its synonym \"bachelor\".\n\nIn his essay [[Two Dogmas of Empiricism]], the philosopher [[Willard Van Orman Quine|W. V. O. Quine]] called into question the distinction between analytic and synthetic statements. It was this second class of analytic statements that caused him to note that the concept of analyticity itself stands in need of clarification, because it seems to depend on the concept of [[synonym]]y, which stands in need of clarification. In his conclusion, Quine rejects that logical truths are necessary truths. Instead he posits that the truth-value of any statement can be changed, including logical truths, given a re-evaluation of the truth-values of every other statement in one's complete theory.\n\n== Truth values and tautologies ==\n{{Main|Tautology (logic)}}\n\nConsidering different [[interpretation (logic)|interpretations]] of the same statement leads to the notion of [[truth value]]. The simplest approach to truth values means that the statement may be \"true\" in one case, but [[false (logic)|\"false\"]]<!-- not yet an article, but ought to be.  please, do not remove the link --> in another. In one sense of the term ''tautology'', it is any type of [[well-formed formula|formula]] or [[proposition]] which turns out to be true under any possible interpretation of its terms (may also be called a [[Valuation (logic)|valuation]] or assignment depending upon the context). This is synonymous to logical truth.\n\nHowever, the term ''tautology'' is also commonly used to refer to what could more specifically be called [[Truth function|truth-functional]] tautologies. Whereas a tautology or logical truth is true solely because of the logical terms it contains in general (e.g. \"[[Universal quantification|every]]\", \"[[Existential quantification|some]]\", and \"is\"), a truth-functional tautology is true because of the logical terms it contains which are [[logical connective]]s (e.g. \"[[logical disjunction|or]]\", \"[[logical conjunction|and]]\", and \"[[joint denial|nor]]\"). Not all logical truths are tautologies of such a kind.\n\n== Logical truth and logical constants ==\n{{Main|Logical constant}}\n\nLogical constants, including [[logical connective]]s and [[Quantifier (logic)|quantifiers]], can all be reduced conceptually to logical truth. For instance, two statements or more are [[Logical NAND|logically incompatible]] ''[[if and only if|if, and only if]]'' their [[Logical conjunction|conjunction]] is logically false. One statement [[Logical implication|logically implies]] another when it is logically incompatible with the [[negation]] of the other. A statement is logically true if, and only if its opposite is logically false.  The opposite statements must contradict one another.  In this way all logical connectives can be expressed in terms of preserving logical truth.  The logical form of a sentence is determined by its semantic or syntactic structure and by the placement of logical constants.  Logical constants determine whether a statement is a logical truth when they are combined with a language that limits its meaning.  Therefore, until it is determined how to make a distinction between all logical constants regardless of their language, it is impossible to know the complete truth of a statement or argument.<ref>{{Cite web|url=https://plato.stanford.edu/entries/logical-constants/|title=Logical Constants|last=MacFarlane|first=J.|date=May 16, 2005|website=|archive-url=|archive-date=|dead-url=|access-date=}}</ref>\n\n== Logical truth and rules of inference ==\n\nThe concept of logical truth is closely connected to the concept of a [[rule of inference]].<ref>[[A. J. Ayer|Alfred Ayer]], ''[[Language, Truth, and Logic]]''</ref>\n\n== Logical truth and logical positivism ==\n\n[[Logical positivism]] was a movement in the early 20th century that tried to reduce the reasoning processes of science to pure logic. Among other things, the logical positivists claimed that any proposition that is not empirically verifiable is neither true nor false, but nonsense. This movement faded out due to various problems with their approach among which was a growing understanding that science does not work in the way that the positivists described. Another problem was that one of the favorite slogans of the movement: \"any proposition that is not empirically verifiable is nonsense\" was itself not empirically verifiable, and therefore, by its own terms, nonsense.\n\n== Non-classical logics ==\n\n{{Main|Non-classical logic}}\n\nNon-classical logic is the name given to [[formal system]]s which differ in a significant way from standard logical systems such as [[Propositional logic|propositional]] and [[predicate logic]]. There are several ways in which this is done, including by way of extensions, deviations, and variations. The aim of these departures is to make it possible to construct different models of [[logical consequence]] and logical truth.<ref>[[Theodore Sider]], ''Logic for philosophy''</ref>\n\n== See also ==\n* [[Contradiction]]\n* [[False (logic)]]\n* [[Satisfiability]]\n* [[Tautology (logic)]] (for symbolism of logical truth)\n* [[Theorem]]\n* [[Validity (logic)|Validity]]\n\n== References ==\n{{reflist}}\n\n== External links ==\n* {{cite SEP |url-id=logical-truth |title=Logical Truth}}\n* {{InPho|taxonomy|2408}}\n* {{PhilPapers|category|logical-semantics-and-logical-truth/}}\n\n{{logic}}\n{{Logical connectives}}\n{{Logical truth}}\n\n[[Category:Philosophical logic]]\n[[Category:Necessity]]\n[[Category:Concepts in logic]]\n[[Category:Logical connectives]]\n[[Category:Logical truth]]\n[[Category:Truth]]\n[[Category:Philosophy of logic]]\n\n[[ca:Valor vertader]]"
    },
    {
      "title": "Material conditional",
      "url": "https://en.wikipedia.org/wiki/Material_conditional",
      "text": "{{Redirect|Logical conditional|other related meanings|Conditional statement (disambiguation){{!}}Conditional statement}}\n{{distinguish|Material inference}}\n\n{{Infobox logical connective\n| title        = Material conditional\n| other titles = IMPLY\n| Venn diagram = Venn1011.svg\n| definition   = <math>x \\rightarrow y</math>\n| truth table  = <math>(1011)</math>\n| logic gate   = IMPLY_ANSI.svg\n| DNF          = <math>\\overline{x} + y</math>\n| CNF          = <math>\\overline{x} + y</math>\n| Zhegalkin    = <math>1 \\oplus x \\oplus xy</math>\n| 0-preserving = no\n| 1-preserving = yes\n| monotone     = no\n| affine       = no\n| self-dual    = no\n}}\n\nThe '''material conditional''' (also known as '''[[Material implication (rule of inference)|material implication]]''', '''material consequence''', or simply '''implication''', '''implies''', or '''conditional''') is a [[logical connective]] (or a [[binary operator]]) that is often symbolized by a forward arrow \"→\". The material conditional is used to form [[statement (logic)|statements]] of the form ''p''&nbsp;→&nbsp;''q'' (termed a ''conditional statement'') which is read as \"if ''p'' then ''q''\". Unlike the English construction \"if...&nbsp;then...\", the material conditional statement '''''p''&nbsp;→&nbsp;''q''''' does not conventionally specify a [[causal relationship]] between ''p'' and ''q''; \"''p'' is the cause and ''q'' is the consequence from it\" is not a generally valid  [[interpretation (logic)|interpretation]] of '''''p''&nbsp;→&nbsp;''q'''''. It merely means '''\"if ''p'' is true, then ''q'' is also true\"''' such that the statement ''p''&nbsp;→&nbsp;''q'' is false only when both ''p'' is true and ''q'' is false.<ref>{{cite web|title=forallx: An Introduction to Formal Logic|url=http://www.fecundity.com/codex/forallx.pdf|author=Magnus, P.D|date=January 6, 2012|publisher=Creative Commons|page=25|accessdate=28 May 2013}}</ref> In a [[bivalence|bivalent]] truth table of ''p''&nbsp;→&nbsp;''q'', if ''p'' is false, then ''p''&nbsp;→&nbsp;''q'' is true regardless of whether ''q'' is true or false (Latin phrase: ''ex falso quod libet'') since (1) ''p''&nbsp;→&nbsp;''q'' is always true as long ''q'' is true and (2) ''p''&nbsp;→&nbsp;''q'' is true when both ''p'' and ''q'' are false. This truth table is useful to prove some mathematical theorems (e.g.: defining a [[subset]]).\n\nThe material conditional is also symbolized using:\n\n# 𝑝 ⊃ 𝑞 (Although this symbol may be used for the superset symbol in [[set theory]].);\n# 𝑝 ⇒ 𝑞 (Although this symbol is often used for [[logical consequence]] (''i.e.,'' logical implication) rather than for material conditional.)\n# C𝑝𝑞 (using [[Polish notation#Polish notation for logic|Łukasiewicz notation]] or [[Józef_Maria_Bocheński|Bocheński notation]])\n\nWith respect to the material conditionals above:\n* ''p'' is termed the '''[[antecedent (logic)|antecedent]]''' of the conditional, and\n* ''q'' is termed the '''[[consequent]]''' of the conditional.\n\nConditional statements may be nested such that either or both of the antecedent or the consequent may themselves be conditional statements. In the example {{gaps|\"(''p''|→|''q'')&nbsp;→&nbsp;(''r''|→|''s'')\"}}, meaning \"if the truth of ''p'' implies the truth of ''q'', then the truth of ''r'' implies the truth of ''s''), both the antecedent and the consequent are conditional statements.\n\nIn [[classical logic]] <math>p \\rightarrow q</math> is [[Logical equivalence|logically equivalent]] to <math>\\neg(p \\land \\neg q)</math> and by [[De Morgan's Law]] logically equivalent to <math>\\neg p \\lor q</math>.<ref>{{cite web|title=A Modern Formal Logic Primer: Sentence Logic Volume 1|url=http://tellerprimer.ucdavis.edu/pdf/1ch4.pdf|author=Teller, Paul|date=January 10, 1989|publisher=Prentice Hall|page=54|accessdate=28 May 2013}}</ref> Whereas, in [[minimal logic]] (and therefore also intuitionistic logic) <math>p \\rightarrow q</math> only [[Logical consequence|logically entails]] <math>\\neg(p \\land \\neg q)</math>; and in [[intuitionistic logic]] (but not minimal logic) <math>\\neg p \\lor q</math> entails <math>p \\rightarrow q</math>.\n\n==Definitions==\nLogicians have many different views on the nature of material implication and approaches to explain its sense.<ref>{{cite web | url=http://www.cs.cornell.edu/Info/People/gries/symposium/clarke.htm | title=A Comparison of Techniques for Introducing Material Implication | publisher=Cornell University | date=March 1996 | accessdate=March 4, 2012 | author=Clarke, Matthew C.}}</ref>\n\n===As a truth function===\nThe compound {{gaps|''p''|→|''q''}} is ''false'' [[if and only if]] ''p'' is true and ''q'' is false. By the same stroke, {{gaps|''p''|→|''q''}} is ''true'' if and only if either ''p'' is false or ''q'' is true (or both). The → symbol is a function that uses pairs of [[truth value]]s of the components ''p,'' ''q'' (e.g. ''p'' is True, q is True ... p is False, q is False) and maps it to the truth values of the compound {{gaps|''p''|→|''q''}}. The truth value of {{gaps|''p''|→|''q''}} is a function of the truth values of its components (''p,q''). Hence, this interpretation is called ''[[Truth function|truth-functional]]''. \n\nThe compound {{gaps|''p''|→|''q''}} is logically equivalent also to {{gaps|¬''p''|∨|''q''}} (either not ''p'', or ''q'' (or both)), and to {{gaps|¬''q''|→|¬''p''}} (if not ''q'' then not ''p''). It is, however, not equivalent to {{gaps|¬''p''|→|¬''q''}}, which is instead equivalent to {{gaps|''q''|→|''p''}}.\n\n====Truth table====\nThe [[truth table]] associated with the material conditional {{gaps|''p''|→|''q''}} is identical to that of {{gaps|¬''p''|∨|''q''}}. It is as follows:\n{|\n|\n{| class=\"wikitable center\" \n|- \n! <math> p </math>|| <math> q </math> || <math> p \\rightarrow q </math>\n|-\n| T || T || T\n|- \n| T ||style=\"background:papayawhip\" | F ||style=\"background:papayawhip\" | F\n|-\n|style=\"background:papayawhip\" | F || T || T\n|-\n| style=\"background:papayawhip\" |F ||style=\"background:papayawhip\" | F || T\n|}\n|}\n\nIt may also be useful to note that in [[Boolean algebra]], true and false can be denoted as 1 and 0 respectively with an equivalent table.\n\n===As a formal connective===\nThe material conditional can be considered as a symbol of a [[theory (mathematical logic)|formal theory]], taken as a set of sentences, satisfying all the classical inferences involving&nbsp;→, in particular the following characteristic rules:\n\n# [[Modus ponens]];\n# [[Conditional proof]];\n# [[contraposition|Classical contraposition]];\n# [[reductio ad absurdum|Classical reductio ad absurdum]].\n\nUnlike the truth-functional one, this approach to logical connectives permits the examination of structurally identical propositional forms in various [[Formal system|logical system]]s, where somewhat different properties may be demonstrated. For example, in [[intuitionistic logic]] which rejects proofs by contraposition as valid rules of inference, {{math|(''p''&nbsp;→&nbsp;''q'')&nbsp;⇒&nbsp;¬''p''&nbsp;∨&nbsp;''q''}} is not a propositional theorem, but [[False (logic)#False, negation and contradiction|the material conditional is used to define negation]].\n\n==Formal properties==\n\nWhen studying logic formally, the material conditional is distinguished from the [[Logical consequence#Semantic consequence|semantic consequence]] relation <math>\\models</math>. We say <math>A \\models B</math> if every interpretation that makes ''A'' true also makes ''B'' true. However, there is a close relationship between the two in most logics, including [[classical logic]]. For example, the following principles hold:\n\n* If <math>\\Gamma\\models\\psi</math> then <math>\\varnothing\\models(\\varphi_1\\land\\dots\\land\\varphi_n\\rightarrow\\psi)</math> for some <math>\\varphi_1,\\dots,\\varphi_n\\in\\Gamma</math>. (This is a particular form of the [[deduction theorem]]. In words, it says that if Γ models ''ψ'' this means that ''ψ'' can be deduced just from some subset of the theorems in Γ.)\n* The converse of the above\n* Both <math>\\rightarrow</math> and <math>\\models</math> are [[Monotonic function|monotonic]]; i.e., if <math>\\Gamma\\models\\psi</math> then <math>\\Delta\\cup\\Gamma\\models\\psi</math>, and if <math>\\varphi\\rightarrow\\psi</math> then <math>(\\varphi\\land\\alpha)\\rightarrow\\psi</math> for any ''α'', Δ. (In terms of structural rules, this is often referred to as [[weakening (logic)|weakening]] or ''thinning''.)\n\nThese principles do not hold in all logics, however. Obviously they do not hold in [[non-monotonic logic]]s, nor do they hold in [[relevance logic]]s.\n\nOther properties of implication (the following expressions are always true, for any logical values of variables):\n\n* [[distributivity]]: <math>(s \\rightarrow (p \\rightarrow q)) \\rightarrow ((s \\rightarrow p) \\rightarrow (s \\rightarrow q))</math>\n* [[transitive relation|transitivity]]: <math>((a \\rightarrow b) \\rightarrow (b \\rightarrow c)) \\rightarrow (a \\rightarrow c)</math>\n* [[reflexive relation|reflexivity]]: <math>a \\rightarrow a</math>\n* [[connex relation|totality]]: <math>(a \\rightarrow b) \\vee (b \\rightarrow a)</math>\n* truth preserving: The interpretation under which all variables are assigned a truth value of 'true' produces a truth value of 'true' as a result of material implication.\n* commutativity of antecedents: <math>(a \\rightarrow (b \\rightarrow c)) \\equiv (b \\rightarrow (a \\rightarrow c))</math>\n\nNote that <math>a \\rightarrow (b \\rightarrow c)</math> is [[Logical equivalence|logically equivalent]] to <math>(a \\land b) \\rightarrow c</math>; this property is sometimes called [[currying|un/currying]].  Because of these properties, it is convenient to adopt a [[right-associative]] notation for → where <math>a \\rightarrow b \\rightarrow c</math> denotes <math>a \\rightarrow (b \\rightarrow c)</math>.\n\nComparison of Boolean truth tables shows that <math>a \\rightarrow b</math> is equivalent to <math>\\neg a \\lor b</math>, and one is an equivalent replacement for the other in classical logic. See [[material implication (rule of inference)]].\n\n== Philosophical problems with material conditional ==\n\nOutside of mathematics, it is a matter of some controversy as to whether the [[truth function]] for [[material implication (rule of inference)|material implication]] provides an adequate treatment of conditional statements in a [[natural language]] such as English, i.e., [[indicative conditional]]s and [[counterfactual conditional]]s. An indicative conditional is a [[sentence (mathematical logic)|sentence]] in the [[indicative mood]] with a [[Conditional sentence|conditional clause]] attached. A counterfactual conditional is a false-to-fact sentence in the [[subjunctive mood]].<ref name=\"sep-conditionals\"/>  That is to say, critics argue that in some non-mathematical cases, the truth value of a compound statement, \"if ''p'' then ''q''\", is not adequately determined by the truth values of ''p'' and ''q''.<ref name=\"sep-conditionals\"/>  Examples of non-truth-functional statements include: \"''q'' because ''p''\", \"''p'' before ''q''\" and \"it is possible that ''p''\".<ref name=\"sep-conditionals\"/>\n\n\"[Of] the sixteen possible truth-functions of ''A'' and ''B'', material implication is the only serious candidate. First, it is uncontroversial that when ''A'' is true and ''B'' is false, \"If ''A'', ''B''\" is false. A basic rule of inference is [[modus ponens]]: from \"If ''A'', ''B''\" and ''A'', we can infer ''B''. If it were possible to have ''A'' true, ''B'' false and \"If ''A'', ''B''\" true, this inference would be invalid. Second, it is uncontroversial that \"If ''A'', ''B''\" is sometimes true when ''A'' and ''B'' are respectively (true, true), or (false, true), or (false, false)… Non-truth-functional accounts agree that \"If ''A'', ''B''\" is false when ''A'' is true and ''B'' is false; and they agree that the conditional is sometimes true for the other three combinations of truth-values for the components; but they deny that the conditional is always true in each of these three cases. Some agree with the truth-functionalist that when ''A'' and ''B'' are both true, \"If ''A'', ''B''\" must be true. Some do not, demanding a further relation between the facts that ''A'' and that ''B''.\"<ref name=\"sep-conditionals\">{{cite web |first=Dorothy |last=Edgington |editor=Edward N. Zalta |year=2008 |title=Conditionals |work=The Stanford Encyclopedia of Philosophy |edition=Winter 2008 |url=http://plato.stanford.edu/archives/win2008/entries/conditionals/}}</ref>\n\n{{quotation|The truth-functional theory of the conditional was integral to [[Gottlob Frege|Frege]]'s new logic (1879). It was taken up enthusiastically by [[Bertrand Russell|Russell]] (who called it \"material implication\"), [[Ludwig Wittgenstein|Wittgenstein]] in the ''[[Tractatus Logico-Philosophicus|Tractatus]]'', and the [[logical positivist]]s, and it is now found in every logic text. It is the first theory of conditionals which students encounter. Typically, it does not strike students as ''obviously'' correct. It is logic's first surprise. Yet, as the textbooks testify, it does a creditable job in many circumstances. And it has many defenders. It is a strikingly simple theory: \"If ''A'', ''B''\" is false when ''A'' is true and ''B'' is false. In all other cases, \"If ''A'', ''B''\" is true. It is thus equivalent to \"~(''A''&~''B'')\" and to \"~''A'' or ''B''\". \"''A'' ⊃ ''B''\" has, by stipulation, these truth conditions.|[[Dorothy Edgington]]|The Stanford Encyclopedia of Philosophy|\"Conditionals\"<ref name=\"sep-conditionals\"/>}}\n\nThe meaning of the material conditional can sometimes be used in the English \"if ''condition'' then ''consequence''\" construction (a kind of [[conditional sentence]]), where ''condition'' and ''consequence'' are to be filled with English sentences. However, this construction also implies a \"reasonable\" connection between the condition ([[Protasis (linguistics)|protasis]]) and consequence ([[Consequent|apodosis]]) (see [[Connexive logic]]).{{citation needed|date=February 2012}}\n\nThe material conditional can yield some unexpected truths when expressed in natural language. For example, any material conditional statement with a false antecedent is true (see [[vacuous truth]]). So the statement \"if 2 is odd then 2 is even\" is true. Similarly, any material conditional with a true consequent is true. So the statement \"if I have a penny in my pocket then Paris is in France\" is always true, regardless of whether or not there is a penny in my pocket. These problems are known as the [[paradoxes of material implication]], though they are not really paradoxes in the strict sense; that is, they do not elicit logical contradictions. These unexpected truths arise because speakers of English (and other natural languages) are tempted to [[equivocation|equivocate]] between the material conditional and the [[indicative conditional]], or other conditional statements, like the [[counterfactual conditional]] and the [[logical biconditional|material biconditional]].\n\nIt is not surprising that a rigorously defined truth-functional operator does not correspond exactly to all notions of implication or otherwise expressed by 'if ... then ...' sentences in natural languages. For an overview of some of the various analyses, formal and informal, of conditionals, see the \"References\" section below. [[Relevance logic]] attempts to capture these alternate concepts of implication that material implication glosses over.\n\n==See also==\n{{col-begin}}\n{{col-break}}\n* [[Boolean algebra]]\n* [[Boolean domain]]\n* [[Boolean function]]\n* [[Boolean logic]]\n{{col-break}}\n* [[Conditional quantifier]]\n* [[Implicational propositional calculus]]\n* [[Laws of Form]]\n* [[Logic gate]]\n* [[Logical graph]]\n{{col-break}}\n* [[Paradoxes of material implication]]\n* [[Peirce's law]]\n* [[Propositional logic]]\n* [[Sole sufficient operator]]\n{{col-end}}\n\n===Conditionals===\n* [[Counterfactual conditional]]\n* [[Indicative conditional]]\n* [[Corresponding conditional]]\n* [[Strict conditional]]\n\n==References==\n{{Reflist}}\n\n== Further reading ==\n* Brown, Frank Markham (2003), ''Boolean Reasoning:  The Logic of Boolean Equations'', 1st edition, [[Kluwer]] Academic Publishers, [[Norwell, Massachusetts|Norwell]], MA.  2nd edition, [[Dover Publications]], [[Mineola, New York|Mineola]], NY, 2003.\n* [[Dorothy Edgington|Edgington, Dorothy]] (2001), \"Conditionals\", in Lou Goble (ed.), ''The Blackwell Guide to Philosophical Logic'', [[Wiley-Blackwell|Blackwell]].\n* [[W. V. Quine|Quine, W.V.]] (1982), ''Methods of Logic'', (1st ed. 1950), (2nd ed. 1959), (3rd ed. 1972), 4th edition, [[Harvard University Press]], [[Cambridge, Massachusetts|Cambridge]], MA.\n* [[Robert Stalnaker|Stalnaker, Robert]], \"Indicative Conditionals\", ''[[Philosophia (journal)|Philosophia]]'', '''5''' (1975): 269–286.\n\n== External links ==\n* {{cite SEP |url-id=conditionals |title=Conditionals |last=Edgington |first=Dorothy}}\n\n{{Logical connectives}}\n\n[[Category:Logical connectives]]\n[[Category:Conditionals]]\n[[Category:Logical consequence]]"
    },
    {
      "title": "Material equivalence",
      "url": "https://en.wikipedia.org/wiki/Material_equivalence",
      "text": "#REDIRECT [[If and only if]]\n\n\n[[Category:Logical connectives]]"
    },
    {
      "title": "Material nonimplication",
      "url": "https://en.wikipedia.org/wiki/Material_nonimplication",
      "text": "[[File:Venn0100.svg|thumb|240px|[[Venn diagram]] of <math>P \\nrightarrow Q</math>]]\n\n'''Material nonimplication''' or '''abjunction''' ([[Latin]] ''ab'' = \"from\", ''junctio''&nbsp;=&ndash;\"joining\") is the [[negation]] of [[Material conditional|material implication]]. That is to say that for any two [[proposition]]s <math>P</math> and <math>Q</math>, the material nonimplication from <math>P</math> to <math>Q</math> is true [[if and only if]] the negation of the material implication from <math>P</math> to <math>Q</math> is true.  This is more naturally stated as that the material nonimplication from <math>P</math> to <math>Q</math> is true only if <math>P</math> is true and <math>Q</math> is false.\n\nIt may be written using logical notation as <math>P \\nrightarrow Q</math>, <math>P \\not \\supset Q</math>, or \"L''pq''\" (in [[Józef_Maria_Bocheński|Bocheński notation]]), and is logically equivalent to <math>\\neg (P \\rightarrow Q)</math>, and <math>P \\land \\neg Q</math>.\n\n==Definition==\n\n===Truth table===\n\n{| class=\"wikitable\" style=\"text-align:center; background-color: #ddffdd;\"\n|- bgcolor=\"#ddeeff\"\n| <math> P </math> || <math> Q </math> || <math> P \\nrightarrow Q </math>\n|-\n| T || T || F\n|-\n| T || F || T\n|-\n| F || T || F\n|-\n| F || F || F\n|}\n\n===Logical Equivalences===\n\nMaterial nonimplication may be defined as the negation of material implication.\n\n{| style=\"text-align: center; border: 1px solid darkgray;\"\n|-\n| <math>P \\nrightarrow Q</math>\n| &emsp;&emsp;<math>\\Leftrightarrow</math>&emsp;&emsp;\n| <math>\\neg (P \\rightarrow Q)</math>\n|-\n| [[File:Venn0100.svg|50px]]\n| &emsp;&emsp;<math>\\Leftrightarrow</math>&emsp;&emsp;\n| <math>\\neg</math> [[File:Venn1011.svg|50px]]\n|}\n\nIn [[classical logic]], it is also equivalent to the negation of the [[Logical_disjunction|disjunction]] of <math>\\neg P</math> and <math>Q</math>, and also the [[Logical_conjunction|conjunction]] of <math>P</math> and <math>\\neg Q</math>\n\n{| style=\"text-align: center; border: 1px solid darkgray;\"\n|-\n| <math>P \\nrightarrow Q</math>\n| &emsp;&emsp;<math>\\Leftrightarrow</math>&emsp;&emsp;\n| <math>\\neg(</math>\n| <math>\\neg P</math>\n| <math>\\lor</math>\n| <math>Q)</math>\n| &emsp;&emsp;<math>\\Leftrightarrow</math>&emsp;&emsp;\n| <math>P</math>\n| <math>\\land</math>\n| <math>\\neg Q</math>\n|-\n| [[File:Venn0100.svg|50px]]\n| &emsp;&emsp;<math>\\Leftrightarrow</math>&emsp;&emsp;\n| <math>\\neg(</math>\n| [[File:Venn1010.svg|50px]]\n| <math>\\lor</math>\n| [[File:Venn0011.svg|50px]]<math>)</math>\n| &emsp;&emsp;<math>\\Leftrightarrow</math>&emsp;&emsp;\n| [[File:Venn0101.svg|50px]]\n| <math>\\land</math>\n| [[File:Venn1100.svg|50px]]\n|}\n\n==Properties==\n\n'''falsehood-preserving''': The interpretation under which all variables are assigned a [[truth value]] of \"false\" produces a truth value of \"false\" as a result of material nonimplication.\n\n==Symbol==\nThe symbol for material nonimplication is simply a crossed-out material implication symbol. Its Unicode symbol is 8603 (decimal).\n\n==Natural language==\n\n===Grammatical===\n\n===Rhetorical===\n\"p but not q.\"\n\n==Computer science==\nBitwise operation: A&(~B) \n\nLogical operation: A&&(!B)\n\n==See also==\n* [[Material conditional|Implication]]\n* [[Boolean algebra]]\n\n== References ==\n{{reflist}}\n\n{{Unreferenced|date=June 2017}}\n\n{{Logical connectives}}\n\n[[Category:Logical connectives]]\n\n\n{{mathlogic-stub}}\n{{logic-stub}}"
    },
    {
      "title": "Modal operator",
      "url": "https://en.wikipedia.org/wiki/Modal_operator",
      "text": "{{Unreferenced|date=May 2019|bot=noref (GreenC bot)}}\nA '''modal connective''' (or '''modal operator''') is a [[logical connective]] for [[modal logic]]. It is an [[binary function|operator]] which forms [[proposition]]s from propositions. In general, a modal operator has the \"formal\" property of being non-[[truth function|truth-functional]] in the following sense: The truth-value of composite formulae sometimes depend on factors other than the actual truth-value of their components. In the case of alethic modal logic, a modal operator can be said to be truth-functional in another sense, namely, that of being sensitive only to the distribution of truth-values across possible worlds, actual or not. Finally, a modal operator is \"intuitively\" characterized by expressing a modal attitude (such as [[Logical truth|necessity]], [[Logical possibility|possibility]], [[belief]], or [[knowledge]]) about the proposition to which the operator is applied. \n\n== Modality interpreted ==\nThere are several ways to [[interpretation (logic)|interpret]] modal operators in modal logic, including:\n[[alethic modality|alethic]], [[deontic logic|deontic]], [[axiology|axiological]], [[epistemic modal logic|epistemic]], and [[doxastic logic|doxastic]].\n\n===Alethic===\n[[Alethic modality|Alethic]] modal operators (M-operators) determine the fundamental conditions of [[possible worlds]], especially [[causality]], time-space parameters, and the action capacity of persons. They indicate the [[logical possibility|possibility]], [[impossibility]] and [[Logical truth|necessity]] of actions, states of affairs, events, people, and qualities in the possible worlds.\n\n=== Deontic ===\n[[Deontic logic|Deontic]] modal operators (P-operators) influence the construction of possible worlds as proscriptive or prescriptive norms, i.e. they indicate what is prohibited, obligatory, or permitted.\n\n=== Axiological ===\n[[Axiology|Axiological]] modal operators (G-operators) transform the world's [[wikt:entity|entities]] into values and disvalues as seen by a social group, a culture, or a historical period. Axiological modalities are highly subjective categories: what is good for one person may be considered as bad by another one.\n\n=== Epistemic ===\n[[Epistemic logic|Epistemic]] modal operators (K-operators) reflect the level of knowledge, ignorance and belief in the possible world.\n\n=== Doxastic ===\n\n[[Doxastic logic|Doxastic]] modal operators express belief in statements. \n\n{{logic}}\n\n[[Category:Modal logic]]\n[[Category:Logic symbols]]\n[[Category:Logical connectives]]"
    },
    {
      "title": "Negation",
      "url": "https://en.wikipedia.org/wiki/Negation",
      "text": "{{for2|negation in linguistics|[[Affirmation and negation]]|other uses|[[Negation (disambiguation)]]}}\n{{more footnotes|date=March 2013}}\n{{Use dmy dates|date=September 2010}}\n\n{{Infobox logical connective\n| title        = Negation\n| other titles = NOT\n| Venn diagram = \n| definition   = <math>\\overline{x}</math>\n| truth table  = <math>(10)</math>\n| logic gate   = NOT_ANSI.svg\n| DNF          = <math>\\overline{x}</math>\n| CNF          = <math>\\overline{x}</math>\n| Zhegalkin    = <math>1 \\oplus x</math>\n| 0-preserving = {{#switch:Нет|Да|да=yes|Нет|нет=no}}\n| 1-preserving = {{#switch:Нет|Да|да=yes|Нет|нет=no}}\n| monotone     = {{#switch:Нет|Да|да=yes|Нет|нет=no}}\n| affine       = {{#switch:Да|Да|да=yes|Нет|нет=no}}\n| self-dual    = {{#switch:Да|Да|да=yes|Нет|нет=no}}\n}}\n\nIn [[logic]], '''negation''', also called the '''logical complement''', is an [[operation (mathematics)|operation]] that takes a [[proposition]] <math>P</math> to another proposition \"not <math>P</math>\", written <math>\\neg P</math>, which is interpreted intuitively as being true when <math>P</math> is false, and false when <math>P</math> is true. Negation is thus a unary (single-argument) [[logical connective]]. It may be applied as an operation on [[notion (philosophy)|notions]], [[proposition]]s, [[truth value]]s, or [[interpretation (logic)|semantic values]] more generally. In [[classical logic]], negation is normally identified with the [[truth function]] that takes ''truth'' to ''falsity'' and vice versa. In [[intuitionistic logic]], according to the [[Brouwer–Heyting–Kolmogorov interpretation]], the negation of a proposition <math>P</math> is the proposition whose proofs are the refutations of <math>P</math>.\n\n==Definition==\nNo agreement exists as to the possibility of defining negation, as to its logical status, function, and meaning, as to its field of applicability..., and as to the interpretation of the negative judgment, (F.H. Heinemann 1944).<ref name=Horn>{{cite book |last=Horn |first=Laurence R |year=2001 |title=A NATURAL HISTORY OF NEGATION |url=http://emilkirkegaard.dk/en/wp-content/uploads/A-natural-history-of-negation-Laurence-R.-Horn.pdf |location=Stanford University |chapter=Chapter 1 |page=1 |publisher=CLSI Publications |isbn=1-57586-336-7  |accessdate=29 Dec 2013 }}</ref>\n\n''Classical negation'' is an  [[logical operation|operation]] on one [[logical value]], typically the value of a [[proposition]], that produces a value of ''true'' when its operand is false and a value of ''false'' when its operand is true.  So, if statement <math>P</math> is true, then <math>\\neg P</math> (pronounced \"not P\") would therefore be false; and conversely, if <math>\\neg P</math> is false, then <math>P</math> would be true.\n\nThe [[truth table]] of <math>\\neg P</math> is as follows:\n\n{| class=\"wikitable\" style=\"text-align:center; background-color: #ddffdd;\"\n|- bgcolor=\"#ddeeff\"\n| <math> P </math> || <math> \\neg P </math>\n|-\n| True || False\n|-\n| False || True\n|}\n\nNegation can be defined in terms of other logical operations.  For example, <math>\\neg P</math> can be defined as <math>P \\rightarrow \\bot</math> (where <math>\\rightarrow</math> is [[logical consequence]] and <math>\\bot</math> is [[False_(logic)|absolute falsehood]]). Conversely, one can define <math>\\bot</math> as <math>Q \\land \\neg Q</math> for any proposition <math>Q</math> (where <math>\\land</math> is [[logical conjunction]]). The idea here is that any [[contradiction]] is false.  While these ideas work in both classical and intuitionistic logic, they do not work in [[paraconsistent logic]], where contradictions are not necessarily false. In classical logic, we also get a further identity, <math>P \\rightarrow Q</math> can be defined as <math>\\neg P \\lor Q</math>, where <math>\\lor</math> is [[logical disjunction]].\n\nAlgebraically, classical negation corresponds to [[Complement (order theory)|complementation]] in a [[Boolean algebra (structure)|Boolean algebra]], and intuitionistic negation to pseudocomplementation in a [[Heyting algebra]]. These algebras provide a [[algebraic semantics (mathematical logic)|semantics]] for classical and intuitionistic logic respectively.\n\n==Notation==\nThe negation of a proposition <math>p</math> is notated in different ways in various contexts of discussion and fields of application. Among these variants are the following:\n\n{| class=\"wikitable\"\n|- style=\"background:paleturquoise\"\n! style=\"text-align:center\" | Notation\n! Plain Text\n! Vocalization\n|-\n| style=\"text-align:center\" | <math>\\neg p</math>\n| ¬p\n| Not ''p''\n|-\n| style=\"text-align:center\" | <math>\\mathord{\\sim} p</math>\n| ~p\n| Not ''p''\n|-\n| style=\"text-align:center\" | <math>-p</math>\n| -p\n| Not ''p''\n|-\n| style=\"text-align:center\" | N''p''\n|\n| En ''p''\n|-\n| style=\"text-align:center\" | <math>p'</math>\n| p'\n| {{unbulleted list\n | ''p'' prime,\n | ''p'' complement\n }}\n|-\n| style=\"text-align:center\" | <math>\\overline{p}</math>\n| ̅p\n| {{unbulleted list\n | ''p'' bar,\n | Bar ''p''\n }}\n|-\n| style=\"text-align:center\" | <math>!p</math>\n| !p\n| {{unbulleted list\n | Bang ''p''\n | Not ''p''\n }}\n|-\n|}\n\nThe notation N''p'' is [[Polish_notation#Polish_notation_for_logic|Łukasiewicz notation]].\n\nIn [[Set theory#Basic concepts and notation|set theory]] <math>\\setminus</math> is also used to indicate 'not member of': <math>U \\setminus A</math> is the set of all members of <math>U</math> that are not members of <math>A</math>.\n\nNo matter how it is notated or [[List of logic symbols|symbolized]], the negation <math>\\neg P</math> can be read as \"it is not the case that <math>P</math>\", \"not that <math>P</math>\", or usually more simply as \"not <math>P</math>\".\n\n==Properties==\n\n===Double negation===\n\nWithin a system of [[classical logic]], double negation, that is, the negation of the negation of a proposition <math>P</math>, is [[logically equivalent]] to <math>P</math>.  Expressed in symbolic terms, <math>\\neg \\neg P \\equiv P</math>. In [[intuitionistic logic]], a proposition implies its double negation but not conversely. This marks one important difference between classical and intuitionistic negation. Algebraically, classical negation is called an [[involution (mathematics)|involution]] of period two.\n\nHowever, in [[intuitionistic logic]] we do have the equivalence of <math>\\neg \\neg \\neg P \\equiv \\neg P</math>. Moreover, in the propositional case, a sentence is classically provable if its double negation is intuitionistically provable. This result is known as [[Double-negation translation|Glivenko's theorem]].\n\n===Distributivity===\n\n[[De Morgan's laws]] provide a way of [[Distributive property|distributing]] negation over [[logical disjunction|disjunction]] and [[logical conjunction|conjunction]] :\n\n:<math>\\neg(P \\lor Q) \\equiv (\\neg P \\land \\neg Q)</math>,&nbsp; and\n:<math>\\neg(P \\land Q) \\equiv (\\neg P \\lor \\neg Q)</math>.\n\n===Linearity===\nLet <math>\\oplus</math> denote the logical [[Exclusive or|xor]] operation. In [[Boolean algebra (logic)|Boolean algebra]], a linear function is one such that:\n\nIf there exists <math>a_0, a_1, \\dots, a_n \\in \\{0,1\\}</math>,\n<math>f(b_1, b_2, \\dots, b_n) = a_0 \\oplus (a_1 \\land b_1) \\oplus \\dots \\oplus (a_n \\land b_n)</math>,\nfor all <math>b_1, b_2, \\dots, b_n \\in \\{0,1\\}</math>.\n\nAnother way to express this is that each variable always makes a difference in the [[truth-value]] of the operation or it never makes a difference. Negation is a linear logical operator.\n\n===Self dual===\nIn [[Boolean algebra (logic)|Boolean algebra]] a self dual function is one such that:\n\n<math>f(a_1, \\dots, a_n) = \\neg f(\\neg a_1, \\dots, \\neg a_n)</math> for all\n<math>a_1, \\dots, a_n \\in \\{0,1\\}</math>.\nNegation is a self dual logical operator.\n\n==Rules of inference==\n{{see also|double negation}}\nThere are a number of equivalent ways to formulate rules for negation. One usual way to formulate classical negation in a [[natural deduction]] setting is to take as primitive rules of inference ''negation introduction'' (from a derivation of <math>P</math> to both <math>Q</math> and <math>\\neg Q</math>, infer <math>\\neg P</math>; this rule also being called ''[[reductio ad absurdum]]''), ''negation elimination'' (from <math>P</math> and <math>\\neg P</math> infer <math>Q</math>; this rule also being called ''ex falso quodlibet''), and ''double negation elimination'' (from <math>\\neg \\neg P</math> infer <math>P</math>). One obtains the rules for intuitionistic negation the same way but by excluding double negation elimination.\n\nNegation introduction states that if an absurdity can be drawn as conclusion from <math>P</math> then <math>P</math> must not be the case (i.e. <math>P</math> is false (classically) or refutable (intuitionistically) or etc.). Negation elimination states that anything follows from an absurdity. Sometimes negation elimination is formulated using a primitive absurdity sign <math>\\bot</math>. In this case the rule says that from <math>P</math> and <math>\\neg P</math> follows an absurdity. Together with double negation elimination one may infer our originally formulated rule, namely that anything follows from an absurdity.\n\nTypically the intuitionistic negation <math>\\neg P</math> of <math>P</math> is defined as <math>P \\rightarrow \\bot</math>. Then negation introduction and elimination are just special cases of implication introduction ([[conditional proof]]) and elimination ([[modus ponens]]). In this case one must also add as a primitive rule ''ex falso quodlibet''.\n\n==Programming==\nAs in mathematics, negation is used in [[computer science]] to construct logical statements.\n<source lang=\"cpp\">\n    if (!(r == t))\n    {\n         /*...statements executed when r does NOT equal t...*/\n    }\n</source>\nThe \"<code>!</code>\" signifies logical NOT in [[B (programming language)|B]], [[C Programming Language|C]], and languages with a C-inspired syntax such as [[C++]], [[Java (programming language)|Java]], [[JavaScript]], [[Perl]], and [[PHP]]. \"<code>NOT</code>\" is the operator used in [[ALGOL 60]], [[BASIC programming language|BASIC]], and languages with an ALGOL- or BASIC-inspired syntax such as [[Pascal programming language|Pascal]], [[Ada programming language|Ada]], [[Eiffel (programming language)|Eiffel]] and [[Seed7]]. Some languages (C++, Perl, etc.) provide more than one operator for negation. A few languages like [[PL/I]] and [[Ratfor]] use <code>¬</code> for negation. Some modern computers and [[operating systems]] will display <code>¬</code> as <code>!</code> on files encoded in [[ASCII]]. Most modern languages allow the above statement to be shortened from <code>if (!(r == t))</code> to <code>if (r != t)</code>, which allows sometimes, when the compiler/interpreter is not able to optimize it, faster programs.\n\nIn computer science there is also ''bitwise negation''. This takes the value given and switches all the [[binary numeral system|binary]] 1s to 0s and 0s to 1s.  See [[bitwise operation]].  This is often used to create [[signed number representations|ones' complement]] or \"<code>~</code>\" in C or C++ and [[two's complement]] (just simplified to \"<code>-</code>\" or the negative sign since this is equivalent to taking the arithmetic negative value of the number) as it basically creates the opposite (negative value equivalent) or mathematical complement of the value (where both values are added together they create a whole).\n\nTo get the absolute (positive equivalent) value of a given integer the following would work as the \"<code>-</code>\" changes it from negative to positive (it is negative because \"<code>x < 0</code>\" yields true)\n\n<source lang=\"cpp\">\n    unsigned int abs(int x)\n    {\n        if (x < 0)\n            return -x;\n        else\n            return x;\n    }\n</source>\n\nTo demonstrate logical negation:\n\n<source lang=\"cpp\">\n    unsigned int abs(int x)\n    {\n        if (!(x < 0))\n            return x;\n        else\n            return -x;\n    }\n</source>\n\nInverting the condition and reversing the outcomes produces code that is logically equivalent to the original code, i.e. will have identical results for any input (note that depending on the compiler used, the actual instructions performed by the computer may differ).\n\nThis convention occasionally surfaces in written speech, as computer-related [[slang]] for ''not''.  The phrase <code>!voting</code>, for example, means \"not voting\".\n\n==Kripke semantics==\nIn [[Kripke semantics]] where the semantic values of formulae are sets of [[possible world]]s, negation can be taken to mean [[set-theoretic complement]]ation.{{citation needed|date=August 2012}} (See also [[possible world semantics]].)\n\n==See also==\n{{Div col|colwidth=20em}}\n* [[Affirmation and negation]] (grammatical polarity)\n* [[Ampheck]]\n* [[Apophasis]]\n* [[Binary opposition]]  \n* [[Bitwise operation#NOT|Bitwise NOT]]  \n* [[Cyclic negation]]\n* [[Logical conjunction]]  \n* [[Logical disjunction]]  \n* [[Negation as failure]]  \n* [[NOT gate]]  \n* [[Plato's beard]]\n* [[Square of opposition]]  \n* [[Truth function]]  \n* [[Truth table]]\n{{Div col end}}\n\n==References==\n{{reflist}}\n\n== Further reading ==\n* [[Dov Gabbay|Gabbay, Dov]], and Wansing, Heinrich, eds., 1999. ''What is Negation?'', [[Wolters Kluwer|Kluwer]].\n* [[Laurence R. Horn|Horn, L.]], 2001. ''A Natural History of Negation'', [[University of Chicago Press]].\n* [[G. H. von Wright]], 1953–59, \"On the Logic of Negation\", ''Commentationes Physico-Mathematicae 22''.\n* Wansing, Heinrich, 2001, \"Negation\", in Goble, Lou, ed., ''The Blackwell Guide to Philosophical Logic'', [[Wiley-Blackwell|Blackwell]].\n* {{cite journal | last1 = Tettamanti | first1 = Marco | last2 = Manenti | first2 = Rosa | last3 = Della Rosa | first3 = Pasquale A. | last4 = Falini | first4 = Andrea | last5 = Perani | first5 = Daniela | last6 = Cappa | first6 = Stefano F. | last7 = Moro | first7 = Andrea | year = 2008 | title = Negation in the brain:  Modulating action representation | doi = 10.1016/j.neuroimage.2008.08.004 | journal = NeuroImage | volume = 43 | issue = 2| pages = 358–367 | pmid=18771737}}\n\n==External links==\n*{{cite SEP |url-id=negation |title=Negation |first=Laurence R. |last=Horn |first2=Heinrich |last2=Wansing}}\n* {{springer|title=Negation|id=p/n066170}}\n* [http://mathworld.wolfram.com/NOT.html NOT], on [[MathWorld]]\n; [[Truth table|Tables of Truth]] of composite clauses\n* {{cite web|url= http://www.math.hawaii.edu/~ramsey/Logic/NotAnd.html|title= Table of truth for a NOT clause applied to an END sentence|archive-url= https://web.archive.org/web/20000301195359/http://www.math.hawaii.edu/~ramsey/Logic/NotAnd.html|archive-date= March 1, 2000|deadurl= no}}\n* {{cite web|url= http://www.math.hawaii.edu/~ramsey/Logic/NotAnd.html|title= NOT clause of an END sentence|archive-url= https://web.archive.org/web/20000301195359/http://www.math.hawaii.edu/~ramsey/Logic/NotAnd.html|archive-date= March 1, 2000|deadurl= no}}\n* {{cite web|url= http://www.math.hawaii.edu/~ramsey/Logic/NotOr.html|title= NOT clause of an OR sentence|archive-url= https://web.archive.org/web/20000117134708/http://www.math.hawaii.edu/~ramsey/Logic/NotOr.html|archive-date= Jan 17, 2000|deadurl= no}}\n* {{cite web|url= http://www.math.hawaii.edu/~ramsey/Logic/NotIfThen.html|title= NOT clause of an IF...THEN period|archive-url= https://web.archive.org/web/20000301223435/http://www.math.hawaii.edu/~ramsey/Logic/NotIfThen.html/|archive-date= March 1, 2000|deadurl= no}}\n\n\n{{Logical connectives}}\n{{Common logical symbols}}\n\n[[Category:Grammar]]\n[[Category:Semantics]]\n[[Category:Logical connectives]]\n[[Category:Negative concepts]]\n[[Category:Unary operations]]"
    },
    {
      "title": "Sheffer stroke",
      "url": "https://en.wikipedia.org/wiki/Sheffer_stroke",
      "text": "{{Infobox logical connective\n| title        = Sheffer stroke\n| other titles = NAND\n| Venn diagram = Venn1110.svg\n| definition   = <math>\\overline{x \\cdot y}</math>\n| truth table  = <math>(1110)</math>\n| logic gate   = NAND_ANSI.svg\n| DNF          = <math>\\overline{x} + \\overline{y}</math>\n| CNF          = <math>\\overline{x} + \\overline{y}</math>\n| Zhegalkin    = <math>1 \\oplus xy</math>\n| 0-preserving = no\n| 1-preserving = no\n| monotone     = no\n| affine       = no\n| self-dual    = no\n}}\n\nIn [[Boolean function]]s and [[propositional calculus]], the '''Sheffer stroke''' denotes a [[logical operation]] that is equivalent to the [[logical negation|negation]] of the [[logical conjunction|conjunction]] operation, expressed in ordinary language as \"not both\".  It is also called '''nand'''  (\"not and\") or the '''alternative denial''', since it says in effect that at least one of its operands is false.  In [[digital electronics]], it corresponds to the [[NAND gate]]. It is named after [[Henry M. Sheffer]] and written as ↑ or as | (but not as ||, often used to represent [[Logical disjunction|disjunction]]). In [[Józef_Maria_Bocheński|Bocheński notation]] it can be written as D''pq''.\n\nIts [[duality (mathematics)|dual]] is the [[logical NOR|NOR operator]] (also known as the [[Charles Sanders Peirce|Peirce]] arrow or [[Willard Van Orman Quine|Quine]] dagger). Like its dual, NAND can be used by itself, without any other logical operator, to constitute a logical [[formal system]] (making NAND [[functional completeness|functionally complete]]). This property makes the [[NAND gate]] crucial to modern [[digital electronics]], including its use in [[computer processor]] design.\n\n==Definition==\nThe '''NAND operation''' is a [[logical operation]] on two [[logical value]]s.  It produces a value of true, if — and only if — at least one of the [[proposition]]s is false.\n\n===Truth table===\nThe [[truth table]] of <math>P \\uparrow Q</math> (also written as <math>P \\mathop{|} Q</math>, or D''pq'') is as follows\n\n{| class=\"wikitable\" style=\"text-align:center; background-color: #ddffdd;\"\n|- bgcolor=\"#ddeeff\"\n| <math> P </math> || <math> Q </math> || <math> P \\uparrow Q </math>\n|-\n| T || T || F\n|-\n| T || F || T\n|-\n| F || T || T\n|-\n| F || F || T\n|}\n\n===Logical equivalences===\n\nThe Sheffer stroke of <math>P</math> and <math>Q</math> is the negation of their conjunction\n\n{| style=\"text-align: center; border: 1px solid darkgray;\"\n|-\n| <math>P \\uparrow Q</math>\n| &emsp;&emsp;<math>\\Leftrightarrow</math>&emsp;&emsp;\n| <math>\\neg (P \\land Q)</math>\n|-\n| [[File:Venn1110.svg|50px]]\n| &emsp;&emsp;<math>\\Leftrightarrow</math>&emsp;&emsp;\n| <math>\\neg</math> [[File:Venn0001.svg|50px]]\n|}\n\nBy [[De Morgan's Laws]], this is also equivalent to the disjunction of the negations of <math>P</math> and <math>Q</math>\n\n{| style=\"text-align: center; border: 1px solid darkgray;\"\n|-\n| <math>P \\uparrow Q</math>\n| &emsp;&emsp;<math>\\Leftrightarrow</math>&emsp;&emsp;\n| <math>\\neg P</math>\n| <math>\\lor</math>\n| <math>\\neg Q</math>\n|-\n| [[File:Venn1110.svg|50px]]\n| &emsp;&emsp;<math>\\Leftrightarrow</math>&emsp;&emsp;\n| [[File:Venn1010.svg|50px]]\n| <math>\\lor</math> \n| [[File:Venn1100.svg|50px]]\n|}\n\n==History==\nThe stroke is named after [[Henry M. Sheffer]], who in 1913 published a paper in the ''[[Transactions of the American Mathematical Society]]'' (Sheffer 1913) providing an axiomatization of [[Boolean algebra (structure)|Boolean algebra]]s using the stroke, and proved its equivalence to a standard formulation thereof by [[Edward Vermilye Huntington|Huntington]] employing the familiar operators of [[propositional logic]] ([[logical conjunction|and]], [[logical disjunction|or]],  [[negation|not]]). Because of self-[[duality (order theory)|duality]] of Boolean algebras, Sheffer's axioms are equally valid for either of the NAND or NOR operations in place of the stroke. Sheffer interpreted the stroke as a sign for nondisjunction ([[logical NOR|NOR]]) in his paper, mentioning non-conjunction only in a footnote and without a special sign for it.  It was [[Jean Nicod]] who first used the stroke as a sign for non-conjunction (NAND) in a paper of 1917 and which has since become current practice.<ref>{{Harvcoltxt|Church|1956|p=134}}</ref> Russell and Whitehead used the Sheffer stroke in the 1927 second edition of ''[[Principia Mathematica]]'' and suggested it as a replacement for the \"or\" and \"not\" operations of the first edition.\n\n[[Charles Sanders Peirce]] (1880) had discovered the [[functional completeness]] of NAND or NOR more than 30 years earlier, using the term ''[[ampheck]]'' (for 'cutting both ways'), but he never published his finding.\n\n==Properties==\nNAND does not possess any of the following five properties, each of which is required to be absent from, and the absence of all of which is sufficient for, at least one member of a set of [[functional completeness|functionally complete]] operators: truth-preservation, falsity-preservation, [[affine transformation|linearity]], [[monotonic]]ity, [[duality (mathematics)#Duality in logic and set theory|self-duality]]. (An operator is truth- (falsity-) preserving if its value is truth (falsity) whenever all of its arguments are truth (falsity).) Therefore {NAND} is a functionally complete set.\n\nThis can also be realized as follows: All three elements of the functionally complete set {AND, OR, NOT} can be [[#Introduction, elimination, and equivalencies|constructed using only NAND]]. Thus the set {NAND} must be functionally complete as well.\n\n==Other Boolean operations in terms of the Sheffer Stroke==\n\nExpressed in terms of NAND <math>\\uparrow</math>, the usual operators of propositional logic are:\n\n{|\n|-\n|<!--- not --->\n{| style=\"text-align: center; border: 1px solid darkgray;\"\n|-\n|<math>\\neg P</math>\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|<math>P</math>\n|<math>\\uparrow</math>\n|<math>P</math>\n|-\n|[[File:Venn10.svg|36px]]\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|[[File:Venn01.svg|36px]]\n|<math>\\uparrow</math>\n|[[File:Venn01.svg|36px]]\n|}<!--- end not--->\n\n|&nbsp;&nbsp;&nbsp;\n|<!--- arrow --->\n{| style=\"text-align: center; border: 1px solid darkgray;\"\n|-\n|<math>P \\rightarrow Q</math>\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|<math>~P</math>\n|<math>\\uparrow</math>\n|<math>(Q \\uparrow Q)</math>\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|<math>~P</math>\n|<math>\\uparrow</math>\n|<math>(P \\uparrow Q)</math>\n|-\n|[[File:Venn1011.svg|50px]]\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|[[File:Venn0101.svg|50px]]\n|<math>\\uparrow</math>\n|[[File:Venn1100.svg|50px]]\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|[[File:Venn0101.svg|50px]]\n|<math>\\uparrow</math>\n|[[File:Venn1110.svg|50px]]\n|}<!--- end arrow --->\n\n|&nbsp;&nbsp;&nbsp;\n|<!--- bi-arrow/equivalence --->\n{| style=\"text-align: center; border: 1px solid darkgray;\"\n|-\n|<math>P \\leftrightarrow Q</math>\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|<math>(P \\uparrow Q)</math>\n|<math>\\uparrow</math>\n|<math>((P \\uparrow P) \\uparrow (Q \\uparrow Q))</math>\n|-\n|[[File:Venn1001.svg|50px]]\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|[[File:Venn1110.svg|50px]]\n|<math>\\uparrow</math>\n|[[File:Venn0111.svg|50px]]\n|}<!--- end bi-arrow/equivalence --->\n\n|-\n|&nbsp;\n|-\n|<!--- and --->\n{| style=\"text-align: center; border: 1px solid darkgray;\"\n|-\n|<math>P \\land Q</math>\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|<math>(P \\uparrow Q)</math>\n|<math>\\uparrow</math>\n|<math>(P \\uparrow Q)</math>\n|-\n|[[File:Venn0001.svg|50px]]\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|[[File:Venn1110.svg|50px]]\n|<math>\\uparrow</math>\n|[[File:Venn1110.svg|50px]]\n|}<!--- end and --->\n|&nbsp;&nbsp;&nbsp;\n|<!--- or --->\n{| style=\"text-align: center; border: 1px solid darkgray;\"\n|-\n|<math>P \\lor Q</math>\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|<math>(P \\uparrow P)</math>\n|<math>\\uparrow</math>\n|<math>(Q \\uparrow Q)</math>\n|-\n|[[File:Venn0111.svg|50px]]\n|&nbsp;&nbsp;&nbsp;&nbsp;<math>\\Leftrightarrow</math>&nbsp;&nbsp;&nbsp;&nbsp;\n|[[File:Venn1010.svg|50px]]\n|<math>\\uparrow</math>\n|[[File:Venn1100.svg|50px]]\n|}<!--- end or --->\n|}\n\n==Formal system based on the Sheffer stroke==\nThe following is an example of a [[formal system]] based entirely on the Sheffer stroke, yet having the functional expressiveness of the [[propositional logic]]:\n\n===Symbols===\n''p<sub>n</sub>'' for natural numbers ''n'' <br>\n( | )\n\nThe Sheffer stroke commutes but does not associate (e.g., (T|T)|F = T, but T|(T|F) = F). Hence any formal system including the Sheffer stroke must also include a means of indicating grouping. We shall employ '(' and ')' to this effect.\n\nWe also write ''p'', ''q'', ''r'', … instead of ''p''<sub>0</sub>, ''p''<sub>1</sub>, ''p''<sub>2</sub>.\n\n===Syntax===\n'''Construction Rule I:''' For each natural number ''n'', the symbol ''p<sub>n</sub>'' is a [[well-formed formula]] (wff), called an atom.\n\n'''Construction Rule II:''' If ''X'' and ''Y'' are wffs, then (''X''|''Y'') is a wff.\n\n'''Closure Rule:''' Any formulae which cannot be constructed by means of the first two Construction Rules are not wffs.\n\nThe letters ''U'', ''V'', ''W'', ''X'', and ''Y'' are metavariables standing for wffs.\n\nA decision procedure for determining whether a formula is well-formed goes as follows: \"deconstruct\" the formula by applying the Construction Rules backwards, thereby breaking the formula into smaller subformulae. Then repeat this recursive deconstruction process to each of the subformulae. Eventually the formula should be reduced to its atoms, but if some subformula cannot be so reduced, then the formula is not a wff.\n\n===Calculus===\nAll wffs of the form\n:((''U''|(''V''|''W''))|((''Y''|(''Y''|''Y''))|((''X''|''V'')|((''U''|''X'')|(''U''|''X'')))))\nare axioms. Instances of\n:(''U''|(''V''|''W'')), ''U'' <math>\\vdash</math> ''W''\nare inference rules.\n\n===Simplification===\nSince the only connective of this logic is |, the symbol | could be discarded altogether, leaving only the parentheses to group the letters. A pair of parentheses must always enclose a pair of ''wff''s. Examples of theorems in this simplified notation are\n\n: (''p''(''p''(''q''(''q''((''pq'')(''pq'')))))),\n\n: (''p''(''p''((''qq'')(''pp'')))).\n\nThe notation can be simplified further, by letting\n: (''U'') := (''UU'')\n: ((''U'')) <math>\\equiv</math> ''U''\nfor any ''U''.  This simplification causes the need to change some rules:\n# More than two letters are allowed within parentheses.\n# Letters or wffs within parentheses are allowed to commute.\n# Repeated letters or wffs within a same set of parentheses can be eliminated.\nThe result is a parenthetical version of the Peirce [[existential graph]]s.\n\nAnother way to simplify the notation is to eliminate parentheses by using [[Polish Notation]]. For example, the earlier examples with only parentheses could be rewritten using only strokes as follows\n\n: (''p''(''p''(''q''(''q''((''pq'')(''pq'')))))) becomes\n: |''p''|''p''|''q''|''q''||''pq''|''pq'', and\n\n: (''p''(''p''((''qq'')(''pp'')))) becomes,\n: |''p''|''p''||''qq''|''pp''.\n\nThis follows the same rules as the parenthesis version, with the opening parenthesis replaced with a Sheffer stroke and the (redundant) closing parenthesis removed.\n\nOr one could omit both parentheses ''and'' strokes and allow the order of the arguments to determine the order of function application so that for example, applying the function from right to left (reverse Polish notation - any other unambiguous convention based on ordering would do)\n\n: ''pqr'' <math>\\equiv</math>  ( ''p'' | ( ''q'' | ''r'' ) ), whereas\n: ''rqp'' <math>\\equiv</math>  ( ''r'' | ( ''q'' | ''p'' ) ).\n\n==See also==\n* [[List of logic symbols]]\n{{Div col|colwidth=30em}}\n* [[AND gate]]\n* [[Boolean domain]]\n* [[CMOS]]\n* [[Gate equivalent|Gate equivalent (GE)]]\n* [[Laws of Form]]\n* [[Logic gate]]\n* [[Logical graph]]\n* [[Minimal axioms for Boolean algebra]]\n* NAND [[Flash Memory]]\n* [[NAND logic]]\n* [[NAND gate]]\n* [[NOR gate]]\n* [[NOT gate]]\n* [[OR gate]]\n* [[Peirce's law]]\n* [[Logical NOR|Peirce arrow = NOR]]\n* [[Propositional logic]]\n* [[Sole sufficient operator]]\n* [[XOR gate]]\n* [[Peirce arrow]]\n{{div col end}}\n\n==Notes==\n{{reflist}}\n\n==References==\n*[[Bocheński, Józef Maria]] (1960), ''Precis of Mathematical Logic'', rev., Albert Menne, edited and translated from the French and German editions by Otto Bird, [[Dordrecht]], [[South Holland]]:  [[D. Reidel]].\n*[[Alonzo Church|Church, Alonzo]], (1956) ''Introduction to mathematical logic'', Vol. 1, [[Princeton, New Jersey|Princeton]]:  [[Princeton University Press]].\n*{{cite journal | last1 = Nicod | first1 = Jean G. P. | authorlink = Jean Nicod | year = 1917 | title = A Reduction in the Number of Primitive Propositions of Logic | url = | journal = Proceedings of the Cambridge Philosophical Society | volume = 19 | issue = | pages = 32–41 }}\n* [[Charles Sanders Peirce]], 1880, \"A Boolian[sic] Algebra with One Constant\", in [[Charles Hartshorne|Hartshorne, C.]] and [[Paul Weiss (philosopher)|Weiss, P.]], eds., (1931–35) ''[[Charles Sanders Peirce bibliography#CP|Collected Papers of Charles Sanders Peirce]], Vol. 4'': 12–20, [[Cambridge]]:  [[Harvard University Press]].\n* {{citation|first=H. M.|last= Sheffer|year= 1913|title=A set of five independent postulates for Boolean algebras, with application to logical constants|journal=Transactions of the American Mathematical Society |volume=14|pages=481–488|jstor=1988701|doi=10.2307/1988701}}\n\n==External links==\n* [http://www.iep.utm.edu/Sheffer_stroke/ Sheffer Stroke] article in the ''[[Internet Encyclopedia of Philosophy]]''\n*http://hyperphysics.phy-astr.gsu.edu/hbase/electronic/nand.html\n*[http://www.sccs.swarthmore.edu/users/06/adem/engin/e77vlsi/lab3/ implementations of 2 and 4-input NAND gates]\n*[https://web.archive.org/web/20090526075041/http://projecteuclid.org/DPubS?verb=Display&version=1.0&service=UI&handle=euclid.pja/1195520940&page=record Proofs of some axioms by Stroke function by Yasuo Setô] @ [http://projecteuclid.org Project Euclid]\n\n{{Logical connectives}}\n{{Common logical symbols}}\n\n[[Category:Logic gates|NAND gate]]\n[[Category:Logical connectives]]\n[[Category:Logic symbols]]"
    },
    {
      "title": "Strict conditional",
      "url": "https://en.wikipedia.org/wiki/Strict_conditional",
      "text": "In [[logic]], a '''strict conditional''' (symbol: <math>\\Box</math>, or ⥽) is a conditional governed by a [[modal operator]], that is, a [[logical connective]] of [[modal logic]]. It is [[logical equivalence|logically equivalent]] to the [[material conditional]] of classical logic, combined with the [[Logical truth|necessity]] operator from [[modal logic]]. For any two [[proposition]]s ''p'' and ''q'', the [[well-formed formula|formula]] ''p'' → ''q'' says that ''p'' [[material conditional|materially implies]] ''q'' while <math>\\Box (p \\rightarrow q)</math> says that ''p'' [[logical consequence|strictly implies]] ''q''.<ref>Graham Priest, ''An Introduction to Non-Classical Logic: From if to is'', 2nd ed, Cambridge University Press, 2008, {{ISBN|0-521-85433-4}}, [https://books.google.com/books?id=rMXVbmAw3YwC&pg=PA72 p. 72.]</ref>  Strict conditionals are the result of [[Clarence Irving Lewis]]'s attempt to find a conditional for logic that can adequately express [[indicative conditional]]s in natural language.<ref>Nicholas Bunnin and Jiyuan Yu (eds), ''The Blackwell Dictionary of Western Philosophy'', Wiley, 2004, {{ISBN|1-4051-0679-4}}, \"strict implication,\" [https://books.google.com/books?id=OskKWI1YA7AC&pg=PA660 p. 660].</ref> They have also been used in studying [[Molinism|Molinist]] theology.<ref>Jonathan L. Kvanvig, \"Creation, Deliberation, and Molinism,\" in ''Destiny and Deliberation: Essays in Philosophical Theology'', Oxford University Press, 2011, {{ISBN|0-19-969657-8}}, [https://books.google.com/books?id=nQliRGPVpTwC&pg=PA127 p. 127–136].</ref>\n\n==Avoiding paradoxes==\nThe strict conditionals may avoid [[paradoxes of material implication]]. The following statement, for example, is not correctly formalized by material implication:\n\n: If Bill Gates had graduated in Medicine, then Elvis never died.\n\nThis condition should clearly be false: the degree of Bill Gates has nothing to do with whether Elvis is still alive. However, the direct encoding of this formula in [[classical logic]] using material implication leads to:\n\n: Bill Gates graduated in Medicine → Elvis never died.\n\nThis formula is true because whenever the antecedent ''A'' is false, a formula ''A'' → ''B'' is true. Hence, this formula is not an adequate translation of the original sentence. An encoding using the strict conditional is:\n\n: <math>\\Box</math> (Bill Gates graduated in Medicine → Elvis never died.)\n\nIn modal logic, this formula means (roughly) that, in every possible world in which Bill Gates graduated in Medicine, Elvis never died. Since one can easily imagine a world where Bill Gates is a Medicine graduate and Elvis is dead, this formula is false.  Hence, this formula seems to be a correct translation of the original sentence.\n\n==Problems==\nAlthough the strict conditional is much closer to being able to express natural language conditionals than the material conditional, it has its own problems with [[consequent]]s that are [[Logical truth|necessarily true]] (such as 2 + 2 = 4) or antecedents that are necessarily false.<ref>Roy A. Sorensen, ''A Brief History of the Paradox: Philosophy and the labyrinths of the mind'', Oxford University Press, 2003, {{ISBN|0-19-515903-9}}, [https://books.google.com/books?id=PB8I0kHeKy4C&pg=PA105 p. 105].</ref> The following sentence, for example, is not correctly formalized by a strict conditional:\n\n: If Bill Gates graduated in Medicine, then 2 + 2 = 4.\n\nUsing strict conditionals, this sentence is expressed as:\n\n: <math>\\Box</math> (Bill Gates graduated in Medicine → 2 + 2 = 4)\n\nIn modal logic, this formula means that, in every possible world where Bill Gates graduated in medicine, it holds that 2 + 2 = 4. Since 2 + 2 is equal to 4 in all possible worlds, this formula is true, although it does not seem that the original sentence should be. A similar situation arises with 2 + 2 = 5, which is necessarily false:\n\n: If 2 + 2 = 5, then Bill Gates graduated in Medicine.\n\nSome logicians view this situation as indicating that the strict conditional is still unsatisfactory. Others have noted that the strict conditional cannot adequately express [[counterfactual conditional]]s,<ref>Jens S. Allwood, Lars-Gunnar Andersson, and Östen Dahl, ''Logic in Linguistics'', Cambridge University Press, 1977, {{ISBN|0-521-29174-7}}, [https://books.google.com/books?id=hXIpFPttDjgC&pg=PA120 p. 120].</ref> and that it does not satisfy certain logical properties.<ref>Hans Rott and Vítezslav Horák, ''Possibility and Reality: Metaphysics and Logic'', ontos verlag, 2003, {{ISBN|3-937202-24-2}}, [https://books.google.com/books?id=ov9kN3HyltAC&pg=PA271 p. 271].</ref> In particular, the strict conditional is [[Transitive relation|transitive]], while the counterfactual conditional is not.<ref>John Bigelow and Robert Pargetter, ''Science and Necessity'', Cambridge University Press, 1990, {{ISBN|0-521-39027-3}}, [https://books.google.com/books?id=O-onBdR7TPAC&pg=PA116 p. 116].</ref>\n\nSome logicians, such as [[Paul Grice]], have used [[conversational implicature]] to argue that, despite apparent difficulties, the material conditional is just fine as a translation for the natural language 'if...then...'. Others still have turned to [[relevance logic]] to supply a connection between the antecedent and consequent of provable conditionals.\n\n==See also==\n* [[Corresponding conditional]]\n* [[Counterfactual conditional]]\n* [[Indicative conditional]]\n* [[Logical consequence]]\n* [[Material conditional]]\n\n==References==\n{{reflist}}\n\n==Bibliography==\n*Edgington, Dorothy, 2001, \"Conditionals,\" in Goble, Lou, ed., ''The Blackwell Guide to Philosophical Logic''. Blackwell.\n*For an introduction to non-classical logic as an attempt to find a better translation of the conditional, see:\n**[[Graham Priest|Priest, Graham]], 2001. ''An Introduction to Non-Classical Logic''. Cambridge Univ. Press.\n*For an extended philosophical discussion of the issues mentioned in this article, see:\n**[[Mark Sainsbury (philosopher)|Mark Sainsbury]], 2001. ''Logical Forms''. Blackwell Publishers.\n*[[Jonathan Bennett (philosopher)|Jonathan Bennett]], 2003. ''A Philosophical Guide to Conditionals''. Oxford Univ. Press.\n\n{{Logic}}\n\n[[Category:Conditionals]]\n[[Category:Logical connectives]]\n[[Category:Modal logic]]\n[[Category:Necessity]]"
    },
    {
      "title": "Beta normal form",
      "url": "https://en.wikipedia.org/wiki/Beta_normal_form",
      "text": "In the [[lambda calculus]], a term is in '''beta normal form''' if no ''[[lambda calculus#β-reduction|beta reduction]]'' is possible.<ref>{{cite web| url=http://encyclopedia2.thefreedictionary.com/Beta+normal+form | title=Beta normal form | work=Encyclopedia | publisher=[[TheFreeDictionary.com]] |accessdate=18 November 2013 }}</ref> A term is in '''beta-eta normal form''' if neither a beta reduction nor an ''[[lambda calculus#η-conversion|eta reduction]]'' is possible. A term is in '''head normal form''' if there is no ''beta-redex in head position''.\n\n==Beta reduction==\nIn the lambda calculus, a '''beta redex''' is a term of the form:\n\n:<math> (\\mathbf{\\lambda} x . A) M</math>.\n\nA redex <math>r</math> is in '''head position''' in a term <math>t</math>, if <math>t</math> has the following shape:\n\n:<math> \\lambda x_1 \\ldots \\lambda x_n . \\underbrace{(\\lambda x . A) M_1}_{\\text{the redex }r} M_2 \\ldots M_m </math>, where <math>n \\geq 0</math> and <math>m \\geq 1</math>.\n\nA '''beta reduction''' is an application of the following rewrite rule to a beta redex contained in a term:\n\n:<math> (\\mathbf{\\lambda} x . A) M \\longrightarrow A[x := M] </math>\n\nwhere <math>A[x := M]</math> is the result of substituting the term <math>M</math> for the variable <math>x</math> in the term <math>A</math>.\n\nA ''head'' beta reduction is a beta reduction applied in head position, that is, of the following form:\n\n:<math> \\lambda x_1 \\ldots \\lambda x_n . (\\lambda x . A) M_1 M_2 \\ldots M_m \\longrightarrow\n         \\lambda x_1 \\ldots \\lambda x_n . A[x := M_1] M_2 \\ldots M_m </math>, where <math>n \\geq 0</math> and <math>m \\geq 1</math>.\n\nAny other reduction is an ''internal'' beta reduction.\n\nA '''normal form''' is a term that does not contain any beta redex, ''i.e.'' that cannot be further reduced. A '''head normal form''' is a term that does not contain a beta redex in head position, ''i.e.'' that cannot be further reduced by a head reduction. When considering the simple lambda calculus, head normal forms are the terms of the following shape:\n\n:<math> \\lambda x_1 \\ldots \\lambda x_n . x M_1 M_2 \\ldots M_m </math>, where <math>x</math> is a variable, <math>n \\geq 0</math> and <math>m \\geq 0</math>.\n\nA head normal form is not always a normal form, because the applied arguments <math>M_j</math> need not be normal. However, the converse is true: any normal form is also a head normal form. In fact, the normal forms are exactly the head normal forms in which the subterms <math>M_j</math> are themselves normal forms. This gives an inductive syntactic description of normal forms.\n\n==Reduction strategies==\nIn general, a given term can contain several redexes, hence several different beta reductions could be applied. We may specify a [[reduction strategy (lambda calculus)|strategy]] to choose which redex to reduce.\n\n* '''Normal-order reduction''' is the strategy  in which one continually applies the rule for beta reduction in head position until no more such reductions are possible. At that point, the resulting term is in head normal form. One then continues applying head reduction in the subterms <math>M_j</math>, from left to right. Stated otherwise, normal‐order reduction is the strategy that always reduces the left‐most outer‐most redex first.\n* By contrast, in '''applicative order reduction''', one applies the internal reductions first, and then only applies the head reduction when no more internal reductions are possible.\n\nNormal-order reduction is complete, in the sense that if a term has a head normal form, then normal‐order reduction will eventually reach it. By the syntactic description of normal forms above, this entails the same statement for a “fully” normal form (this is the [[standardization theorem]]). By contrast, applicative order reduction may not terminate, even when the term has a normal form. For example, using applicative order reduction, the following sequence of reductions is possible:\n\n:<math>\\begin{align}\n &(\\mathbf{\\lambda} x . z) ((\\lambda w. w w w) (\\lambda w. w w w)) \\\\\n \\rightarrow &(\\lambda x . z) ((\\lambda w. w w w) (\\lambda w. w w w) (\\lambda w. w w w))\\\\\n \\rightarrow &(\\lambda x . z) ((\\lambda w. w w w) (\\lambda w. w w w) (\\lambda w. w w w) (\\lambda w. w w w))\\\\\n \\rightarrow &(\\lambda x . z) ((\\lambda w. w w w) (\\lambda w. w w w) (\\lambda w. w w w) (\\lambda w. w w w) (\\lambda w. w w w))\\\\\n &\\ldots\n\\end{align}</math>\n\nBut using normal-order reduction, the same starting point reduces quickly to normal form:\n\n:<math> (\\mathbf{\\lambda} x . z) ((\\lambda w. w w w) (\\lambda w. w w w)) </math>\n:<math> \\rightarrow z </math>\n\nSinot's [[director string]]s is one method by which the computational complexity of beta reduction can be optimized.\n\n==See also==\n* [[Lambda calculus]]\n* [[Normal form (disambiguation)]]\n\n==References==\n{{reflist}}\n\n{{DEFAULTSORT:Beta Normal Form}}\n[[Category:Lambda calculus]]\n[[Category:Normal forms (logic)]]"
    },
    {
      "title": "Blake canonical form",
      "url": "https://en.wikipedia.org/wiki/Blake_canonical_form",
      "text": "In [[Boolean logic]], a [[Formula (mathematical logic)|formula]] for a Boolean function  ''f'' is in '''Blake canonical form''' ('''BCF'''),<ref name=\"Brown_2012\"/> also called the '''complete sum of prime implicants''',<ref>{{cite book |author-first=Tsutomu |author-last=Sasao |chapter=Ternary Decision Diagrams and their Applications |editor-first1=Tsutomu |editor-last1=Sasao |editor-first2=Masahira |editor-last2=Fujita |title=Representations of Discrete Functions |isbn=978-0792397205 |date=1996 |page=278|doi=10.1007/978-1-4613-1385-4_12 }}</ref> the '''complete sum''',<ref name=\"kandel\">{{cite book |author-first=Abraham |author-last=Kandel |title=Foundations of Digital Logic Design |page=177|url=https://books.google.com/books?id=4sX9fTGRo7QC&printsec=frontcover#v=onepage&q=%22complete%20sum%22&f=false|isbn=9789810231101 |year=1998 }}</ref> or the '''disjunctive prime form''',<ref>[[Donald E. Knuth]], ''[[The Art of Computer Programming]]'' '''4A''': ''Combinatorial Algorithms, Part 1'', 2011, p. 54</ref> when it is a [[logical disjunction|disjunction]] of all the [[prime implicant]]s of ''f''.<ref name=\"Brown_2012\"/>\n\n==Relation to other forms==\nThe Blake canonical form is a special case of [[disjunctive normal form]].\n\nThe Blake canonical form is not necessarily [[Boolean minimization|minimal]], however all the terms of a minimal sum are contained in the Blake canonical form.<ref name=\"kandel\"/>\n\n==History==\nIt was introduced in 1937 by Archie Blake, who called it the \"simplified canonical form\";<ref name=\"Blake_1937\"/><ref name=\"Kinsey_1938\"/> it was named in honor of Blake by Frank Markham Brown in 1990.<ref name=\"Brown_2012\"/>\n\n==Methods for calculation==\nBlake discussed three methods for calculating the canonical form: exhaustion of implicants, iterated [[consensus (boolean algebra)|consensus]], and multiplication. The iterated consensus method was rediscovered by Samson and Mills, [[Willard van Orman Quine|Quine]], and Bing.<ref name=\"Brown_2012\"/>\n\n==See also==\n* [[Horn clause]]\n* [[Consensus theorem]]\n\n==References==\n{{reflist|refs=\n<ref name=\"Brown_2012\">{{cite book |title=Boolean Reasoning - The Logic of Boolean Equations |chapter=Chapter 3: The Blake Canonical Form |author-first=Frank Markham |author-last=Brown |edition=<!-- 2012 -->reissue of 2nd |publisher=[[Dover Publications, Inc.]] |location=Mineola, New York |date=2012 |orig-year=2003, 1990 |isbn=978-0-486-42785-0 |pages=77ff}} [<!-- 1st edition -->http://www2.fiit.stuba.sk/~kvasnicka/Free%20books/Brown_Boolean%20Reasoning.pdf<!-- https://web.archive.org/web/20170416231752/http://www2.fiit.stuba.sk/~kvasnicka/Free%20books/Brown_Boolean%20Reasoning.pdf -->]</ref>\n<ref name=\"Blake_1937\">{{cite book |title=Canonical expressions in Boolean algebra |author-first=Archie |author-last=Blake |type=Dissertation |publisher=[[University of Chicago Libraries]] |location=Department of Mathematics, [[University of Chicago]] |date=1937}}</ref>\n<ref name=\"Kinsey_1938\">{{cite journal |title=Blake, Archie. Canonical expressions in Boolean algebra, Department of Mathematics, University of Chicago, 1937 |type=Review |editor-first=J. C. C. |editor-last=McKinsey |editor-link=John Charles Chenoweth McKinsey |journal=[[The Journal of Symbolic Logic]] |volume=3 |pages=93 |number=2:93 |date=June 1938 |doi=10.2307/2267634 |jstor=2267634 |url=https://www.researchgate.net/publication/275744873|last1=McKinsey |first1=J. C. C. }}</ref>\n}}\n\n[[Category:Normal forms (logic)]]"
    },
    {
      "title": "Conjunctive normal form",
      "url": "https://en.wikipedia.org/wiki/Conjunctive_normal_form",
      "text": "In [[Boolean logic]], a [[Formula (mathematical logic)|formula]] is in '''conjunctive normal form''' ('''CNF''') or '''clausal normal form''' if it is a [[logical conjunction|conjunction]] of one or more [[clause (logic)|clauses]], where a clause is a [[logical disjunction|disjunction]] of [[literal (mathematical logic)|literal]]s; otherwise put, it is '''an AND of ORs'''. As a [[Canonical normal form|canonical normal form]], it is useful in [[automated theorem proving]] and [[circuit theory]].\n\nAll conjunctions of literals and all disjunctions of literals are in CNF, as they can be seen as conjunctions of one-literal clauses and conjunctions of a single clause, respectively. As in the [[disjunctive normal form]] (DNF), the only propositional connectives a formula in CNF can contain are [[logical conjunction|and]], [[logical disjunction|or]], and [[logical negation|not]]. The not operator can only be used as part of a literal, which means that it can only precede a [[propositional variable]] or a [[First-order logic#Formulas|predicate symbol]].\n\nIn automated theorem proving, the notion \"''clausal normal form''\" is often used in a narrower sense, meaning a particular representation of a CNF formula as a set of sets of literals.\n\n==Examples and non-examples==\nAll of the following formulas in the variables A, B, C, D, E, and F are in conjunctive normal form:\n\n* <math>(A \\lor \\neg B \\lor \\neg C) \\land (\\neg D \\lor E \\lor F)</math>\n* <math>(A \\lor B) \\land C</math>\n* <math>A \\lor B</math>\n* <math>A</math>\n\nThe third formula is in conjunctive normal form because it is viewed as a \"conjunction\" with just one conjunct, namely the clause <math>A \\lor B</math>.\nIncidentally, the last two formulas are also in [[disjunctive normal form]].\n\nThe following formulas are '''not''' in conjunctive normal form:\n* <math>\\neg (B \\lor C)</math>, since an OR is nested within a NOT\n* <math>(A \\land B) \\lor C</math>\n* <math>A \\land (B \\lor (D \\land E))</math>, since an AND is nested within an OR\n\nEvery formula can be equivalently written as a formula in conjunctive normal form.\nIn particular this is the case for the three non-examples just mentioned; they are respectively equivalent to the following three formulas, which are in conjunctive normal form:\n* <math>\\neg B \\land \\neg C</math>\n* <math>(A \\lor C) \\land (B \\lor C)</math>\n* <math>A \\land (B \\lor D) \\land (B \\lor E).</math>\n\n==Conversion into CNF==\n\nEvery [[propositional formula]] can be converted into an [[logical equivalence|equivalent]] formula that is in CNF. This transformation is based on rules about [[logical equivalence]]s: [[double negation elimination]], [[De Morgan's laws]], and the [[distributive law]].\n\nSince all propositional formulae can be converted into an equivalent formula in conjunctive normal form, proofs are often based on the assumption that all formulae are CNF. However, in some cases this conversion to CNF can lead to an exponential explosion of the formula. For example, translating the following non-CNF formula into CNF produces a formula with <math>2^n</math> clauses:\n\n:<math>(X_1 \\wedge Y_1) \\vee (X_2 \\wedge Y_2) \\vee \\dots \\vee (X_n \\wedge Y_n).</math>\n\nIn particular, the generated formula is:\n:<math>(X_1 \\vee X_2 \\vee \\cdots \\vee X_n) \\wedge (Y_1 \\vee X_2 \\vee \\cdots \\vee X_n) \\wedge (X_1 \\vee Y_2 \\vee \\cdots \\vee X_n) \\wedge (Y_1 \\vee Y_2 \\vee \\cdots \\vee X_n) \\wedge \\cdots \\wedge (Y_1 \\vee Y_2 \\vee \\cdots \\vee Y_n).</math>\n\nThis formula contains <math>2^n</math> clauses; each clause contains either <math>X_i</math> or <math>Y_i</math> for each <math>i</math>.\n\nThere exist transformations into CNF that avoid an exponential increase in size by preserving [[Boolean satisfiability problem|satisfiability]] rather than [[logical equivalence|equivalence]].<ref>Tseitin (1968)</ref><ref>Jackson and Sheridan (2004)</ref> These transformations are guaranteed to only linearly increase the size of the formula, but introduce new variables. For example, the above formula can be transformed into CNF by adding variables <math>Z_1,\\ldots,Z_n</math> as follows:\n\n:<math>(Z_1 \\vee \\cdots \\vee Z_n) \\wedge\n(\\neg Z_1 \\vee X_1) \\wedge (\\neg Z_1 \\vee Y_1) \\wedge\n\\cdots \\wedge \n(\\neg Z_n \\vee X_n) \\wedge (\\neg Z_n \\vee Y_n). </math>\n\nAn [[interpretation (logic)|interpretation]] satisfies this formula only if at least one of the new variables is true. If this variable is <math>Z_i</math>, then both <math>X_i</math> and <math>Y_i</math> are true as well. This means that every [[Model theory|model]] that satisfies this formula also satisfies the original one. On the other hand, only some of the models of the original formula satisfy this one: since the <math>Z_i</math> are not mentioned in the original formula, their values are irrelevant to satisfaction of it, which is not the case in the last formula. This means that the original formula and the result of the translation are [[Equisatisfiability|equisatisfiable]] but not [[logical equivalence|equivalent]].\n\nAn alternative translation, the [[Tseitin transformation]], includes also the clauses <math>Z_i \\vee \\neg X_i \\vee \\neg Y_i</math>. With these clauses, the formula implies <math>Z_i \\equiv X_i \\wedge Y_i</math>; this formula is often regarded to \"define\" <math>Z_i</math> to be a name for <math>X_i \\wedge Y_i</math>.\n\n==First-order logic==\n\nIn first order logic, conjunctive normal form can be taken further to yield the [[clausal normal form]] of a logical formula, which can be then used to perform [[Resolution (logic)#Resolution in first order logic|first-order resolution]].\nIn resolution-based automated theorem-proving, a CNF formula \n{| \n|-\n||\n|| <math>(</math>\n|| <math>l_{11}</math>\n|| <math>\\lor</math>\n|| <math>\\ldots</math>\n|| <math>\\lor</math>\n|| <math>l_{1n_1}</math>\n|| <math>)</math>\n|| <math>\\land</math>\n|| <math>\\ldots</math>\n|| <math>\\land</math>\n|| <math>(</math>\n|| <math>l_{m1}</math>\n|| <math>\\lor</math>\n|| <math>\\ldots</math>\n|| <math>\\lor</math>\n|| <math>l_{mn_m}</math>\n|| <math>)</math>\n||\n|| , with <math>l_{ij}</math> literals, is commonly represented as a set of sets\n|-\n|| <math>\\{</math>\n|| <math>\\{</math>\n|| <math>l_{11}</math>\n|| <math>,</math>\n|| <math>\\ldots</math>\n|| <math>,</math>\n|| <math>l_{1n_1}</math>\n|| <math>\\}</math>\n|| <math>,</math>\n|| <math>\\ldots</math>\n|| <math>,</math>\n|| <math>\\{</math>\n|| <math>l_{m1}</math>\n|| <math>,</math>\n|| <math>\\ldots</math>\n|| <math>,</math>\n|| <math>l_{mn_m}</math>\n|| <math>\\}</math>\n|| <math>\\}</math>\n|| .\n|}\nSee [[#Converting_from_first-order_logic|below]] for an example.\n\n==Computational complexity==\n\nAn important set of problems in [[computational complexity theory|computational complexity]] involves finding assignments to the variables of a boolean formula expressed in Conjunctive Normal Form, such that the formula is true. The ''k''-SAT problem is the problem of finding a satisfying assignment to a boolean formula expressed in CNF in which each disjunction contains at most ''k'' variables. [[Boolean satisfiability problem|3-SAT]] is [[NP-complete]] (like any other ''k''-SAT problem with ''k''>2) while [[2-satisfiability|2-SAT]] is known to have solutions in [[polynomial time]].\nAs a consequence,<ref>since one way to check a CNF for satisfiability is to convert it into a [[Disjunctive normal form|DNF]], the satisfiability of which can be checked in [[Time complexity#Linear time|linear time]]</ref> the task of converting a formula into a [[Disjunctive normal form|DNF]], preserving satisfiability, is [[NP-hard]]; [[Boolean algebra#Duality principle|dually]], converting into CNF, preserving [[Satisfiability and validity|validity]], is also NP-hard; hence equivalence-preserving conversion into DNF or CNF is again NP-hard.\n\nTypical problems in this case involve formulas in \"3CNF\": conjunctive normal form with no more than three variables per conjunct. Examples of such formulas encountered in practice can be very large, for example with 100,000 variables and 1,000,000 conjuncts.\n\nA formula in CNF can be converted into an equisatisfiable formula in \"''k''CNF\" (for ''k''&ge;3) by replacing each conjunct with more than ''k'' variables <math>X_1 \\vee \\cdots \\vee X_k \\vee \\cdots \\vee X_n</math> by two conjuncts <math>X_1 \\vee \\cdots \\vee X_{k-1} \\vee Z</math> and <math>\\neg Z \\vee X_k \\cdots \\vee X_n</math> with {{mvar|Z}} a new variable, and repeating as often as necessary.\n\n==Converting from first-order logic==\n\nTo convert [[first-order logic]] to CNF:<ref>[https://pdfs.semanticscholar.org/bef0/731f247a1d01c9e0ff52f2412007c143899d.pdf Artificial Intelligence: A modern Approach] {{Webarchive|url=https://web.archive.org/web/20170831090316/https://pdfs.semanticscholar.org/bef0/731f247a1d01c9e0ff52f2412007c143899d.pdf |date=2017-08-31 }} [1995...] Russell and Norvig</ref>\n#Convert to [[negation normal form]].\n## Eliminate implications and equivalences: repeatedly replace <math>P \\rightarrow Q</math> with <math>\\lnot P \\lor Q</math>; replace <math>P \\leftrightarrow Q</math> with <math>(P \\lor \\lnot Q) \\land (\\lnot P \\lor Q)</math>. Eventually, this will eliminate all occurrences of <math>\\rightarrow</math> and <math>\\leftrightarrow</math>.\n##Move NOTs inwards by repeatedly applying [[De Morgan's Law]]. Specifically, replace <math>\\lnot (P \\lor Q)</math> with <math>(\\lnot P) \\land (\\lnot Q)</math>; replace <math>\\lnot (P \\land Q)</math> with <math>(\\lnot P) \\lor (\\lnot Q)</math>; and replace <math>\\lnot\\lnot P</math> with <math>P</math>; replace <math>\\lnot (\\forall x P(x))</math> with <math>\\exists x \\lnot P(x)</math>; <math>\\lnot (\\exists x P(x))</math> with <math>\\forall x \\lnot P(x)</math>. After that, a <math>\\lnot</math> may occur only immediately before a predicate symbol.\n#Standardize variables\n##For sentences like <math>(\\forall x P(x)) \\lor (\\exists x Q(x))</math> which use the same variable name twice, change the name of one of the variables. This avoids confusion later when dropping quantifiers. For example, <math>\\forall x [\\exists y \\mathrm{Animal}(y) \\land \\lnot \\mathrm{Loves}(x, y)] \\lor [\\exists y \\mathrm{Loves}(y, x)]</math> is renamed to <math>\\forall x [\\exists y \\mathrm{Animal}(y) \\land \\lnot \\mathrm{Loves}(x, y)] \\lor [\\exists z \\mathrm{Loves}(z,x)]</math>.\n#[[Skolem normal form|Skolemize]] the statement\n##Move quantifiers outwards: repeatedly replace <math>P \\land (\\forall x Q(x))</math> with <math>\\forall x (P \\land Q(x))</math>; replace <math>P \\lor (\\forall x Q(x))</math> with <math>\\forall x (P \\lor Q(x))</math>; replace <math>P \\land (\\exists x Q(x))</math> with <math>\\exists x (P \\land Q(x))</math>; replace <math>P \\lor (\\exists x Q(x))</math> with <math>\\exists x (P \\lor Q(x))</math>. These replacements preserve equivalence, since the previous variable standardization step ensured that <math>x</math> doesn't occur in <math>P</math>. After these replacements, a quantifier may occur only in the initial prefix of the formula, but never inside a <math>\\lnot</math>, <math>\\land</math>, or <math>\\lor</math>. \n##Repeatedly replace <math>\\forall x_1 \\ldots \\forall x_n \\; \\exists y \\; P(y)</math> with <math>\\forall x_1 \\ldots \\forall x_n \\; P(f(x_1,\\ldots,x_n))</math>, where <math>f</math> is a new <math>n</math>-ary function symbol, a so-called \"[[Skolem normal form|skolem function]]\". This is the only step that preserves only satisfiability rather than equivalence. It eliminates all existential quantifiers.\n#Drop all universal quantifiers.\n#Distribute ORs inwards over ANDs: repeatedly replace <math>P \\lor (Q \\land R)</math> with <math>(P \\lor Q) \\land (P \\lor R)</math>.\n\nAs an example, the formula saying ''\"Anyone who loves all animals, is in turn loved by someone\"'' is converted into CNF (and subsequently into [[clause (logic)|clause]] form in the last line) as follows (highlighting replacement rule redices in <math>{\\color{red}{\\text{red}}}</math>):\n\n{|\n|-\n||<math>\\forall x</math>\n||\n||\n||\n||<math>(</math>\n||<math>\\forall y</math>\n||\n||\n||\n||\n||<math>\\mathrm{Animal}(</math>\n||<math>y</math>\n||<math>)</math>\n||<math>\\color{red}\\rightarrow</math>\n||\n||<math>\\mathrm{Loves}(x,</math>\n||<math>y</math>\n||<math>)</math>\n||\n||<math>)</math>\n||<math>\\rightarrow</math>\n||<math>(</math>\n||<math>\\exists</math>\n||<math>y</math>\n||<math>\\mathrm{Loves}(</math>\n||<math>y</math>\n||<math>,x)</math>\n||<math>)</math>\n||\n||\n||\n||\n||\n||\n||\n||\n||\n\n|-\n||<math>\\forall x</math>\n||\n||\n||\n||<math>(</math>\n||<math>\\forall y</math>\n||\n||\n||\n||<math>\\lnot</math>\n||<math>\\mathrm{Animal}(</math>\n||<math>y</math>\n||<math>)</math>\n||<math>\\lor</math>\n||\n||<math>\\mathrm{Loves}(x,</math>\n||<math>y</math>\n||<math>)</math>\n||\n||<math>)</math>\n||<math>\\color{red}\\rightarrow</math>\n||<math>(</math>\n||<math>\\exists</math>\n||<math>y</math>\n||<math>\\mathrm{Loves}(</math>\n||<math>y</math>\n||<math>,x)</math>\n||<math>)</math>\n||\n||\n||\n||\n||\n||\n||\n||\n||by 1.1\n\n|-\n||<math>\\forall x</math>\n||<math>\\color{red}\\lnot</math>\n||\n||\n||<math>(</math>\n||<math>{\\color{red}{\\forall y}}</math>\n||\n||\n||\n||<math>\\lnot</math>\n||<math>\\mathrm{Animal}(</math>\n||<math>y</math>\n||<math>)</math>\n||<math>\\lor</math>\n||\n||<math>\\mathrm{Loves}(x,</math>\n||<math>y</math>\n||<math>)</math>\n||\n||<math>)</math>\n||<math>\\lor</math>\n||<math>(</math>\n||<math>\\exists</math>\n||<math>y</math>\n||<math>\\mathrm{Loves}(</math>\n||<math>y</math>\n||<math>,x)</math>\n||<math>)</math>\n||\n||\n||\n||\n||\n||\n||\n||\n||by 1.1\n\n|-\n||<math>\\forall x</math>\n||\n||\n||\n||<math>(</math>\n||<math>\\exists y</math>\n||<math>\\color{red}\\lnot</math>\n||<math>(</math>\n||\n||<math>\\lnot</math>\n||<math>\\mathrm{Animal}(</math>\n||<math>y</math>\n||<math>)</math>\n||<math>\\color{red}\\lor</math>\n||\n||<math>\\mathrm{Loves}(x,</math>\n||<math>y</math>\n||<math>)</math>\n||<math>)</math>\n||<math>)</math>\n||<math>\\lor</math>\n||<math>(</math>\n||<math>\\exists</math>\n||<math>y</math>\n||<math>\\mathrm{Loves}(</math>\n||<math>y</math>\n||<math>,x)</math>\n||<math>)</math>\n||\n||\n||\n||\n||\n||\n||\n||\n||by 1.2\n\n|-\n||<math>\\forall x</math>\n||\n||\n||\n||<math>(</math>\n||<math>\\exists y</math>\n||\n||\n||<math>\\color{red}\\lnot</math>\n||<math>\\color{red}\\lnot</math>\n||<math>\\mathrm{Animal}(</math>\n||<math>y</math>\n||<math>)</math>\n||<math>\\land</math>\n||<math>\\lnot</math>\n||<math>\\mathrm{Loves}(x,</math>\n||<math>y</math>\n||<math>)</math>\n||\n||<math>)</math>\n||<math>\\lor</math>\n||<math>(</math>\n||<math>\\exists</math>\n||<math>y</math>\n||<math>\\mathrm{Loves}(</math>\n||<math>y</math>\n||<math>,x)</math>\n||<math>)</math>\n||\n||\n||\n||\n||\n||\n||\n||\n||by 1.2\n\n|-\n||<math>\\forall x</math>\n||\n||\n||\n||<math>(</math>\n||<math>{\\color{red}{\\exists y}}</math>\n||\n||\n||\n||\n||<math>\\mathrm{Animal}(</math>\n||<math>y</math>\n||<math>)</math>\n||<math>\\land</math>\n||<math>\\lnot</math>\n||<math>\\mathrm{Loves}(x,</math>\n||<math>y</math>\n||<math>)</math>\n||\n||<math>)</math>\n||<math>\\lor</math>\n||<math>(</math>\n||<math>\\color{red}\\exists</math>\n||<math>\\color{red}y</math>\n||<math>\\mathrm{Loves}(</math>\n||<math>y</math>\n||<math>,x)</math>\n||<math>)</math>\n||\n||\n||\n||\n||\n||\n||\n||\n||by 1.2\n\n|-\n||<math>\\forall x</math>\n||\n||\n||\n||<math>(</math>\n||<math>\\exists y</math>\n||\n||\n||\n||\n||<math>\\mathrm{Animal}(</math>\n||<math>y</math>\n||<math>)</math>\n||<math>\\land</math>\n||<math>\\lnot</math>\n||<math>\\mathrm{Loves}(x,</math>\n||<math>y</math>\n||<math>)</math>\n||\n||<math>)</math>\n||<math>\\color{red}\\lor</math>\n||<math>(</math>\n||<math>\\color{red}\\exists</math>\n||<math>\\color{red}z</math>\n||<math>\\mathrm{Loves}(</math>\n||<math>z</math>\n||<math>,x)</math>\n||<math>)</math>\n||\n||\n||\n||\n||\n||\n||\n||\n||by 2\n\n|-\n||<math>\\forall x</math>\n||<math>\\exists z</math>\n||\n||\n||<math>(</math>\n||<math>{\\color{red}{\\exists y}}</math>\n||\n||\n||\n||\n||<math>\\mathrm{Animal}(</math>\n||<math>y</math>\n||<math>)</math>\n||<math>\\land</math>\n||<math>\\lnot</math>\n||<math>\\mathrm{Loves}(x,</math>\n||<math>y</math>\n||<math>)</math>\n||\n||<math>)</math>\n||<math>\\color{red}\\lor</math>\n||\n||\n||\n||<math>\\mathrm{Loves}(</math>\n||<math>z</math>\n||<math>,x)</math>\n||\n||\n||\n||\n||\n||\n||\n||\n||\n||by 3.1\n\n|-\n||<math>\\forall x</math>\n||<math>{\\color{red}{\\exists z}}</math>\n||\n||\n||\n||<math>\\exists y</math>\n||\n||<math>(</math>\n||\n||\n||<math>\\mathrm{Animal}(</math>\n||<math>y</math>\n||<math>)</math>\n||<math>\\land</math>\n||<math>\\lnot</math>\n||<math>\\mathrm{Loves}(x,</math>\n||<math>y</math>\n||<math>)</math>\n||<math>)</math>\n||\n||<math>\\lor</math>\n||\n||\n||\n||<math>\\mathrm{Loves}(</math>\n||<math>z</math>\n||<math>,x)</math>\n||\n||\n||\n||\n||\n||\n||\n||\n||\n||by 3.1\n\n|-\n||<math>\\forall x</math>\n||\n||\n||\n||\n||<math>{\\color{red}{\\exists y}}</math>\n||\n||<math>(</math>\n||\n||\n||<math>\\mathrm{Animal}(</math>\n||<math>y</math>\n||<math>)</math>\n||<math>\\land</math>\n||<math>\\lnot</math>\n||<math>\\mathrm{Loves}(x,</math>\n||<math>y</math>\n||<math>)</math>\n||<math>)</math>\n||\n||<math>\\lor</math>\n||\n||\n||\n||<math>\\mathrm{Loves}(</math>\n||<math>g(x)</math>\n||<math>,x)</math>\n||\n||\n||\n||\n||\n||\n||\n||\n||\n||by 3.2\n\n|-\n||\n||\n||\n||\n||\n||\n||\n||<math>(</math>\n||\n||\n||<math>\\mathrm{Animal}(</math>\n||<math>f(x)</math>\n||<math>)</math>\n||<math>\\color{red}\\land</math>\n||<math>\\lnot</math>\n||<math>\\mathrm{Loves}(x,</math>\n||<math>f(x)</math>\n||<math>)</math>\n||<math>)</math>\n||\n||<math>\\color{red}\\lor</math>\n||\n||\n||\n||<math>\\mathrm{Loves}(</math>\n||<math>g(x)</math>\n||<math>,x)</math>\n||\n||\n||\n||\n||\n||\n||\n||\n||\n||by 4\n\n|-\n||\n||\n||\n||<math>(</math>\n||\n||\n||\n||\n||\n||\n||<math>\\mathrm{Animal}(</math>\n||<math>f(x)</math>\n||<math>)</math>\n||\n||\n||\n||\n||\n||\n||\n||<math>\\color{red}\\lor</math>\n||\n||\n||\n||<math>\\mathrm{Loves}(</math>\n||<math>g(x)</math>\n||<math>,x)</math>\n||\n||<math>)</math>\n||<math>\\color{red}\\land</math>\n||<math>(</math>\n||<math>\\lnot \\mathrm{Loves}(x,f(x))</math>\n||<math>\\color{red}\\lor</math>\n||<math>\\mathrm{Loves}(g(x),x)</math>\n||<math>)</math>\n||\n||by 5\n\n|-\n||\n||\n||<math>\\{</math>\n||<math>\\{</math>\n||\n||\n||\n||\n||\n||\n||<math>\\mathrm{Animal}(</math>\n||<math>f(x)</math>\n||<math>)</math>\n||\n||\n||\n||\n||\n||\n||\n||<math>,</math>\n||\n||\n||\n||<math>\\mathrm{Loves}(</math>\n||<math>g(x)</math>\n||<math>,x)</math>\n||\n||<math>\\}</math>\n||<math>,</math>\n||<math>\\{</math>\n||<math>\\lnot \\mathrm{Loves}(x,f(x))</math>\n||<math>,</math>\n||<math>\\mathrm{Loves}(g(x),x)</math>\n||<math>\\}</math>\n||<math>\\}</math>\n||([[clause (logic)|clause]] representation)\n\n|}\n\nInformally, the skolem function <math>g(x)</math> can be thought of as yielding the person by whom <math>x</math> is loved, while <math>f(x)</math> yields the animal (if any) that <math>x</math> doesn't love. The 3rd last line from below then reads as ''\"<math>x</math> doesn't love the animal <math>f(x)</math>, or else <math>x</math> is loved by <math>g(x)</math>\"''.\n\nThe 2nd last line from above, <math>(\\mathrm{Animal}(f(x)) \\lor \\mathrm{Loves}(g(x), x)) \\land (\\lnot \\mathrm{Loves}(x, f(x)) \\lor \\mathrm{Loves}(g(x), x))</math>, is the CNF.\n\n=== Notes ===\n<references/>\n\n==See also==\n* [[Algebraic normal form]]\n* [[Disjunctive normal form]]\n* [[Horn clause]]\n* [[Quine&ndash;McCluskey algorithm]]\n\n==References==\n* {{cite book|author=J. Eldon Whitesitt|title=Boolean Algebra and Its Applications|url=https://books.google.com/books?id=20Un1T78GlMC&printsec=frontcover&dq=%22conjunctive+normal+form%22&hl=en&sa=X&ved=0ahUKEwiWn-C22_HiAhXmslQKHTDOCvEQ6AEIKjAA#v=onepage&q=%22conjunctive%20normal%20form%22&f=false|date=24 May 2012|publisher=Courier Corporation|isbn=978-0-486-15816-7}}\n* {{cite book|author1=Hans Kleine Büning|author2=Theodor Lettmann|title=Propositional Logic: Deduction and Algorithms|url=https://books.google.com/books?id=3oJE9yczr3EC&printsec=frontcover#v=onepage&q=%22conjunctive%20normal%20form%22&f=false|date=28 August 1999|publisher=Cambridge University Press|isbn=978-0-521-63017-7}}\n* Paul Jackson, Daniel Sheridan: [http://homepages.inf.ed.ac.uk/pbj/papers/sat04-bc-conv.pdf Clause Form Conversions for Boolean Circuits]. In: Holger H. Hoos, David G. Mitchell (Eds.): Theory and Applications of Satisfiability Testing, 7th International Conference, SAT 2004, Vancouver, BC, Canada, May 10&ndash;13, 2004, Revised Selected Papers. Lecture Notes in Computer Science 3542, Springer 2005, pp.&nbsp;183&ndash;198\n* G.S. Tseitin: [http://www.decision-procedures.org/handouts/Tseitin70.pdf On the complexity of derivation in propositional calculus]. In: Slisenko, A.O. (ed.) Structures in Constructive Mathematics and Mathematical Logic, Part II, Seminars in Mathematics (translated from Russian), pp.&nbsp;115&ndash;125. Steklov Mathematical Institute (1968)\n\n==External links==\n* {{springer|title=Conjunctive normal form|id=p/c025090}}\n* [https://archive.is/20121208184549/http://www.izyt.com/BooleanLogic/applet.php Java applet for converting to CNF and DNF, showing laws used] \n\n[[Category:Normal forms (logic)]]"
    },
    {
      "title": "Disjunctive normal form",
      "url": "https://en.wikipedia.org/wiki/Disjunctive_normal_form",
      "text": "{{multiple|\n{{refimprove|date=November 2010}}\n{{No footnotes|date=November 2010}}\n}}\nIn [[boolean logic]], a '''disjunctive normal form''' ('''DNF''') is a [[Canonical normal form|canonical normal form]] of a logical formula consisting of a disjunction of conjunctive [[clause (logic)|clauses]]; it can also be described as an '''OR of ANDs''', a [[sum of products]], or (in philosophical logic) a ''cluster concept''.  As a [[Normal form (abstract rewriting)|normal form]], it is useful in [[automated theorem proving]].\n\n==Definition==\nA logical formula is considered to be in DNF [[iff|if and only if]] it is a [[logical disjunction|disjunction]] of one or more [[logical conjunction|conjunctions]] of one or more [[literal (mathematical logic)|literals]].<ref name=\"Davey.Priestley.1990\">{{cite book | author=B.A. Davey and H.A. Priestley | title=Introduction to Lattices and Order | publisher=Cambridge University Press | series=Cambridge Mathematical Textbooks | year=1990 }}</ref>{{rp|153}}  A DNF formula is in '''full disjunctive normal form''' if each of its variables appears exactly once in every conjunction. As in [[conjunctive normal form]] (CNF), the only propositional operators in DNF are [[logical conjunction|and]], [[logical disjunction|or]], and [[negation|not]].  The ''not'' (¬) operator can only be used as part of a literal, which means that it can only precede a [[propositional variable]]. \n\nThe following is a [[formal grammar]] for DNF:\n# ''disjunction'' → (''conjunction'' ∨ ''disjunction'')\n# ''disjunction'' → ''conjunction''\n# ''conjunction'' → (''literal'' ∧ ''conjunction'')\n# ''conjunction'' → ''literal''\n# ''literal'' → ¬''variable''\n# ''literal'' → ''variable''\nWhere ''variable'' is any variable.\n\nFor example, all of the following formulas are in DNF:\n\n*<math>(A \\land \\neg B \\land \\neg C) \\lor (\\neg D \\land E \\land F)</math>\n*<math>(A \\land B) \\lor C</math>\n*<math>A \\land B</math>\n*<math>A</math>\n\nHowever, the following formulas are '''not''' in DNF:\n*<math>\\neg(A \\lor B)</math>, since an OR is nested within a NOT\n*<math>A \\lor (B \\land (C \\lor D))</math>, since an OR is nested within an AND\n\n==Conversion to DNF==\n[[File:Karnaugh map KV 4mal4 18.svg|thumb|[[Karnaugh map]] of the disjunctive normal form {{color|#800000|(¬''A''∧¬''B''∧¬''D'')}} ∨ {{color|#000080|(¬''A''∧''B''∧''C'')}} ∨ {{color|#008000|(''A''∧''B''∧''D'')}} ∨ {{color|#800080|(''A''∧¬''B''∧¬''C'')}}]]\n[[File:Karnaugh map KV 4mal4 19.svg|thumb|Karnaugh map of the disjunctive normal form {{color|#800080|(¬''A''∧''C''∧¬''D'')}} ∨ {{color|#008000|(''B''∧''C''∧''D'')}} ∨ {{color|#000080|(''A''∧¬''C''∧''D'')}} ∨ {{color|#800000|(¬''B''∧¬''C''∧¬''D'')}}. Despite the different grouping, the same fields contain a \"1\" as in the previous map.]]\nConverting a formula to DNF involves using [[logical equivalence]]s, such as [[double negation elimination]], [[De Morgan's laws]], and the [[distributivity|distributive law]].\n\nAll logical formulas can be converted into an equivalent disjunctive normal form.<ref name=\"Davey.Priestley.1990\"/>{{rp|152-153}}\nHowever, in some cases conversion to DNF can lead to an exponential explosion of the formula. For example, the DNF of a logical formula of the following form has 2<sup>''n''</sup> terms:\n\n:<math>(X_1 \\lor Y_1) \\land (X_2 \\lor Y_2) \\land \\dots \\land (X_n \\lor Y_n)</math>\n\nAny particular Boolean function can be represented by one and only one<ref group=note>Ignoring variations based on associativity and commutativity of AND and OR.</ref> ''full'' disjunctive normal form, one of the [[canonical form (Boolean algebra)|canonical form]]s. In contrast, two different ''plain'' disjunctive normal forms may denote the same Boolean function, see pictures.\n\n==Complexity issues==\nAn important variation used in the study of [[Analysis of algorithms|computational complexity]] is ''k-DNF''. A formula is in ''k-DNF'' if it is in DNF and each clause contains at most k literals.\n\n==See also==\n\n* [[Algebraic normal form]]\n* [[Boolean function]]\n* [[Boolean-valued function]]\n* [[Conjunctive normal form]]\n* [[Horn clause]]\n* [[Karnaugh map]]\n* [[Logical graph]]\n* [[Propositional logic]]\n* [[Quine–McCluskey algorithm]]\n* [[Truth table]]\n\n==Notes==\n{{reflist|group=note}}\n\n==References==\n{{reflist}}\n*{{cite book|author1=David Hilbert|author2=Wilhelm Ackermann|title=Principles of Mathematical Logic|url=https://books.google.com/books?id=45ZGMjV9vfcC&printsec=frontcover#v=onepage&q=%22disjunctive%20normal%20form%22&f=false|year=1999|publisher=American Mathematical Soc.|isbn=978-0-8218-2024-7}}\n*{{cite book|author=J. Eldon Whitesitt|title=Boolean Algebra and Its Applications|url=https://books.google.com/books?id=20Un1T78GlMC&printsec=frontcover&dq=%22disjunctive+normal+form%22&hl=en&sa=X&ved=0ahUKEwiWn-C22_HiAhXmslQKHTDOCvEQ6AEIKjAA#v=snippet&q=%22disjunctive%20normal%20form%22&f=false|date=24 May 2012|publisher=Courier Corporation|isbn=978-0-486-15816-7}}\n*{{cite book|author=Colin Howson|title=Logic with Trees: An Introduction to Symbolic Logic|url=https://books.google.com/books?id=Y4WGAgAAQBAJ&printsec=frontcover#v=onepage&q=%22disjunctive%20normal%20form%22&f=false|date=11 October 2005|publisher=Routledge|isbn=978-1-134-78550-6}}\n*{{cite book|author1=David Gries|author2=Fred B. Schneider|title=A Logical Approach to Discrete Math|url=https://books.google.com/books?id=ZWTDQ6H6gsUC&pg=PA67&dq=%22disjunctive+normal+form%22&hl=en&sa=X&ved=0ahUKEwiF6p2i3fHiAhWrjVQKHaNqChIQ6AEIMDAB#v=onepage&q=%22disjunctive%20normal%20form%22&f=false|date=22 October 1993|publisher=Springer Science & Business Media|isbn=978-0-387-94115-8|pages=67–}}\n\n==External links==\n* {{springer|title=Disjunctive normal form|id=p/d033300}}\n\n[[Category:Normal forms (logic)]]"
    },
    {
      "title": "Head normal form",
      "url": "https://en.wikipedia.org/wiki/Head_normal_form",
      "text": "#REDIRECT [[Beta normal form]]\n\n[[Category:Normal forms (logic)]]"
    },
    {
      "title": "Herbrand normal form",
      "url": "https://en.wikipedia.org/wiki/Herbrand_normal_form",
      "text": "#REDIRECT [[Herbrandization]]\n\n[[Category:Normal forms (logic)]]"
    },
    {
      "title": "Herbrandization",
      "url": "https://en.wikipedia.org/wiki/Herbrandization",
      "text": "The '''Herbrandization''' of a logical formula (named after [[Jacques Herbrand]]) is a construction that is [[Duality (mathematics)|dual]] to the [[Skolemization]] of a formula.  [[Thoralf Skolem]] had considered the Skolemizations of formulas in [[prenex form]] as part of his proof of the [[Löwenheim–Skolem theorem]] (Skolem 1920).  Herbrand worked with this dual notion of Herbrandization, generalized to apply to non-prenex formulas as well, in order to prove [[Herbrand's theorem (proof theory)|Herbrand's theorem]] (Herbrand 1930).\n\nThe resulting formula is not necessarily [[logical equivalence|equivalent]] to the original one. As with Skolemization, which only preserves [[satisfiability]], Herbrandization being Skolemization's dual preserves [[Validity (logic)|validity]]: the resulting formula is valid if and only if the original one is.\n\n==Definition and examples==\nLet <math>F</math> be a formula in the language of [[first-order logic]].  We may assume that <math>F</math> contains no variable that is bound by two different quantifier occurrences, and that no variable occurs both bound and free.  (That is, <math>F</math> could be relettered to ensure these conditions, in such a way that the result is an equivalent formula).\n\nThe ''Herbrandization'' of <math>F</math> is then obtained as follows:\n\n* First, replace any free variables in <math>F</math> by constant symbols.\n* Second, delete all quantifiers on variables that are either (1) universally quantified and within an even number of negation signs, or (2) existentially quantified and within an odd number of negation signs.\n* Finally, replace each such variable <math>v</math> with a function symbol <math>f_v(x_1,\\dots,x_k)</math>, where <math>x_1,\\dots,x_k</math> are the variables that are still quantified, and whose quantifiers govern <math>v</math>.\n\nFor instance, consider the formula <math>F := \\forall y \\exists x [R(y,x) \\wedge \\neg\\exists z S(x,z)]</math>.  There are no free variables to replace.  The variables <math>y,z</math> are the kind we consider for the second step, so we delete the quantifiers <math>\\forall y</math> and <math>\\exists z</math>.  Finally, we then replace <math>y</math> with a constant <math>c_y</math> (since there were no other quantifiers governing <math>y</math>), and we replace <math>z</math> with a function symbol <math>f_z(x)</math>:\n\n: <math> F^H = \\exists x [R(c_y,x) \\wedge \\neg S(x,f_z(x))]. </math>\n\nThe ''Skolemization'' of a formula is obtained similarly, except that in the second step above, we would delete quantifiers on variables that are either (1) existentially quantified and within an even number of negations, or (2) universally quantified and within an odd number of negations.  Thus, considering the same <math>F</math> from above, its Skolemization would be:\n\n: <math> F^S = \\forall y [R(y,f_x(y)) \\wedge \\neg\\exists z S(f_x(y),z)]. </math>\n\nTo understand the significance of these constructions, see [[Herbrand's theorem (proof theory)|Herbrand's theorem]] or the [[Löwenheim–Skolem theorem]].\n\n== See also ==\n* [[Predicate functor logic]]\n\n==References==\n\n* Skolem, T.  \"Logico-combinatorial investigations in the satisfiability or provability of mathematical propositions: A simplified proof of a theorem by L. Löwenheim and generalizations of the theorem\".  (In van Heijenoort 1967, 252-63.)\n* Herbrand, J.  \"Investigations in proof theory: The properties of true propositions\".  (In van Heijenoort 1967, 525-81.)\n* van Heijenoort, J.  ''From Frege to Gödel: A Source Book in Mathematical Logic, 1879-1931''.  Harvard University Press, 1967.\n\n[[Category:Logic]]\n[[Category:Normal forms (logic)]]"
    },
    {
      "title": "Horn clause",
      "url": "https://en.wikipedia.org/wiki/Horn_clause",
      "text": "In [[mathematical logic]] and [[logic programming]], a '''Horn clause''' is a logical formula of a particular rule-like form which gives it useful properties for use in logic programming, [[formal specification]], and [[model theory]]. Horn clauses are named for the logician [[Alfred Horn]], who first pointed out their significance in 1951.<ref name=\"onsentences\">{{cite journal\n|authorlink=Alfred Horn\n|year=1951\n|last=Horn\n|first=Alfred\n|title=On sentences which are true of direct unions of algebras\n|journal=[[Journal of Symbolic Logic]]\n|pages=14–21\n|volume=16\n|number=1\n|doi=10.2307/2268661|jstor=2268661\n}}</ref>\n\n== Definition ==\n\nA Horn clause is a [[clause (logic)|clause]] (a [[disjunction]] of [[literal (mathematical logic)|literals]]) with at most one positive, i.e. [[negation|unnegated]], literal. \n\nConversely, a disjunction of literals with at most one negated literal is called a '''dual-Horn clause'''. \n\nA Horn clause with exactly one positive literal is a '''definite clause''' or a '''strict Horn clause'''<ref name=\"makowsky\">{{cite journal\n|last1=Makowsky\n|url=https://core.ac.uk/download/pdf/82190596.pdf\n|title=Why Horn Formulas Matter in Computer Science: Initial Structures and Generic Examples|journal=Journal of Computer and System Sciences |volume=34 |number= |pages=266&ndash;292|month= |year=1987 }}</ref>; a definite clause with no negative literals is sometimes called a '''unit clause'''<ref>{{cite book | isbn=978-0-444-89840-1 | issn=0049-237X | editor = Samuel R. Buss | title=Handbook of Proof Theory | location= | publisher=Elsevier B.V | series=Studies in Logic and the Foundations of Mathematics | volume=137 | edition= | month= | year=1998 | last1=Buss |first1=Samuel R. | contribution=An Introduction to Proof Theory | pages=1&ndash;78 | contributionurl=http://www.sciencedirect.com/science/article/pii/S0049237X98800165}}</ref>, and a unit clause without variables is sometimes called a '''fact'''<ref>{{cite journal |last1=Lau & Ornaghi |title=Specifying Compositional Units for Correct Program Development in Computational Logic. |journal=Lecture Notes in Computer Science |pages=1–29 |doi=10.1007/978-3-540-25951-0_1}}</ref>; and a Horn clause without a positive literal is sometimes called a '''goal clause''' (note that the empty clause consisting of no literals is a goal clause). These three kinds of Horn clauses are illustrated in the following [[Propositional formula|propositional]] example:\n{|class=\"wikitable\"\n|-\n!                   \n!Disjunction form\n![[Material conditional|Implication]] form\n!Read intuitively as\n|- align=\"center\"\n!Definite clause\n|¬''p'' ∨ ¬''q'' ∨ ... ∨ ¬''t'' ∨ ''u''\n|| ''u'' ← ''p'' ∧ ''q'' ∧ ... ∧ ''t'' \n|| assume that,<BR>if ''p'' and ''q'' and ... and ''t'' all hold, then also ''u'' holds\n|-align=\"center\"\n!Fact     \n|''u'' \n||''u''               \n||assume that<BR>''u'' holds\n|-align=\"center\"\n!Goal clause\n| ¬''p'' ∨ ¬''q'' ∨ ... ∨ ¬''t''\n|| ''false'' ← ''p'' ∧ ''q'' ∧ ... ∧ ''t'' \n|| show   that<BR>''p'' and ''q'' and ... and ''t'' all hold <ref group=note>Like in [[Resolution_(logic)#A_resolution_technique|resolution theorem proving]], intuitive meanings \"show φ\" and \"assume ¬φ\" are synonymous ([[indirect proof]]); they both correspond to the same formula, viz. ¬φ. This way, a mechanical proving tool needs to maintain only one set of formulas (assumptions), rather than two sets (assumptions and (sub)goals).</ref>\n|}\n\nIn the [[first-order logic|non-propositional]] case, \nall variables<ref group=note>Formula constituent names differ between [[Propositional calculus|Propositional logic]] and [[First-order logic]]. An atomic formula is just a ''propositional variable'' in the former, while in the latter it is composed of a predicate symbol and appropriately many [[term (logic)|terms]], each of which may contain ''domain variables''. Domain variables are meant here.</ref> in a clause are implicitly [[Universal quantification|universally quantified]] with the scope being the entire clause. Thus, for example:\n\n:¬ ''human''(''X'') ∨ ''mortal''(''X'')\n\nstands for:\n\n:∀X( ¬ ''human''(''X'') ∨ ''mortal''(''X'') )\n\nwhich is logically equivalent to:\n\n:∀X ( ''human''(''X'') → ''mortal''(''X'') )\n\n===Significance===\nHorn clauses play a basic role in [[constructive logic]] and [[computational logic]]. They are important in [[automated theorem proving]] by [[first-order resolution]], because the [[Resolution (logic)|resolvent]] of two Horn clauses is itself a Horn clause, and the resolvent of a goal clause and a definite clause is a goal clause. These properties of Horn clauses can lead to greater efficiencies in proving a theorem (represented as the negation of a goal clause).\n\nPropositional Horn clauses are also of interest in [[Computational complexity theory|computational complexity]]. The problem of finding truth value assignments to make a conjunction of propositional Horn clauses true is a [[P-complete]] problem, solvable in [[linear time]],<ref name=\"dowling\">{{cite journal\n|last1=Dowling\n|first1=William F.\n|last2=Gallier\n|first2=Jean H.|author2-link= Jean Gallier\n|year=1984\n|title=Linear-time algorithms for testing the satisfiability of propositional Horn formulae\n|journal=[[Journal of Logic Programming]]\n|volume=1\n|number=3\n|pages=267–284\n|doi=10.1016/0743-1066(84)90014-1}}</ref> and sometimes called [[Horn-satisfiability|HORNSAT]]. (The unrestricted [[Boolean satisfiability problem]] is an [[NP-complete]] problem however.) Satisfiability of first-order Horn clauses is [[Undecidable problem|undecidable]].{{Citation needed|date=December 2015}}\n\n==Logic programming==\n\nHorn clauses are also the basis of [[logic programming]], where it is common to write definite clauses in the form of an [[Material conditional|implication]]:\n\n:(''p'' ∧ ''q'' ∧ ... ∧ ''t'') → ''u''\n\nIn fact, the resolution of a goal clause with a definite clause to produce a new goal clause is the basis of the [[SLD resolution]] inference rule, used to implement [[logic programming]] in the programming language [[Prolog]].\n\nIn logic programming a definite clause behaves as a goal-reduction procedure. For example, the Horn clause written above behaves as the procedure:\n\n:to show ''u'', show ''p'' and show ''q'' and ... and show ''t''.\n\nTo emphasize this reverse use of the clause, it is often written in the reverse form:\n\n:''u'' ← (''p'' ∧ ''q'' ∧ ... ∧ ''t'')\n\nIn [[Prolog]] this is written as:\n\n u :- p, q, ..., t.\n\nIn [[logic programming]] and [[datalog]], computation and query evaluation are performed by representing the negation of a problem to be solved as a goal clause. For example, the problem of solving the existentially quantified conjunction of positive literals:\n\n:∃''X'' (''p'' ∧ ''q'' ∧ ... ∧ ''t'')\n\nis represented by negating the problem (denying that it has a solution), and representing it in the logically equivalent form of a goal clause:\n\n:∀''X'' (''false'' ← ''p'' ∧ ''q'' ∧ ... ∧ ''t'')\n\nIn [[Prolog]] this is written as:\n\n :- p, q, ..., t.\n\nSolving the problem amounts to deriving a contradiction, which is represented by the empty clause (or \"false\"). The solution of the problem is a substitution of terms for the variables in the goal clause, which can be extracted from the proof of contradiction. Used in this way, goal clauses are similar to [[conjunctive query|conjunctive queries]] in relational databases, and Horn clause logic is equivalent in computational power to a [[universal Turing machine]]. \n\nThe Prolog notation is actually ambiguous, and the term “goal clause” is sometimes also used ambiguously.  The variables in a goal clause can be read as universally or existentially quantified, and deriving “false” can be interpreted either as deriving a contradiction or as deriving a successful solution of the problem to be solved.\n\nVan Emden and Kowalski (1976) investigated the model theoretic properties of Horn clauses in the context of logic programming, showing that every set of definite clauses '''D''' has a unique minimal model '''M'''. An atomic formula '''A''' is logically implied by '''D''' if and only if '''A''' is true in '''M'''. It follows that a problem '''P''' represented by an existentially quantified conjunction of positive literals is logically implied by '''D''' if and only if '''P''' is true in '''M'''. The minimal model semantics of Horn clauses is the basis for the [[stable model semantics]] of logic programs.<ref name=\"emdenkowalski\">{{cite journal\n|last1=van Emden\n|first1=M. H.\n|authorlink2=Robert Kowalski\n|last2=Kowalski\n|first2=R. A.\n|year=1976\n|url=http://www.doc.ic.ac.uk/~rak/papers/kowalski-van_emden.pdf\n|title=The semantics of predicate logic as a programming language\n|journal=[[Journal of the ACM]]\n|volume=23\n|number=4\n|pages=733–742\n|doi=10.1145/321978.321991|citeseerx=10.1.1.64.9246\n}}</ref>\n\n== Notes==\n{{reflist|group=note}}\n\n== References ==\n{{reflist}}\n\n{{DEFAULTSORT:Horn clause}}\n[[Category:Logic in computer science]]\n[[Category:Normal forms (logic)]]"
    },
    {
      "title": "Negation normal form",
      "url": "https://en.wikipedia.org/wiki/Negation_normal_form",
      "text": "In [[mathematical logic]], a formula is in '''negation normal form''' if the [[negation]] operator (<math>\\lnot</math>, {{smallcaps|not}}) is only applied to variables and the only other allowed [[Boolean algebra|Boolean operators]] are [[logical conjunction|conjunction]] (<math>\\land</math>, {{smallcaps|and}}) and [[logical disjunction|disjunction]] (<math>\\lor</math>, {{smallcaps|or}}). \n\nNegation normal form is not a canonical form: for example, <math>a \\land (b\\lor \\lnot c)</math> and <math>(a \\land b) \\lor (a \\land \\lnot c)</math> are equivalent, and are both in negation normal form.\n\nIn classical logic and many [[modal logic]]s, every formula can be brought into this form by replacing implications and equivalences by their definitions, using [[De Morgan's laws]] to push negation inwards, and eliminating double negations.  This process can be represented using the following [[rewrite rule]]s ([[Handbook of Automated Reasoning]] 1, p. 204):\n\n:<math>A \\Rightarrow B \\to \\lnot A \\lor B</math>\n:<math>\\lnot (A \\lor B) \\to \\lnot A \\land \\lnot B</math>\n:<math>\\lnot (A \\land B) \\to \\lnot A \\lor \\lnot B</math>\n:<math>\\lnot \\lnot A \\to A</math>\n:<math>\\lnot \\exists x A \\to \\forall x \\lnot A</math>\n:<math>\\lnot \\forall x A \\to \\exists x \\lnot A</math>\n\n[In these rules, the <math>\\Rightarrow</math> symbol indicates logical implication in the formula being rewritten, and <math>\\to</math> is the rewriting operation.]\n\nA formula in negation normal form can be put into the stronger [[conjunctive normal form]] or [[disjunctive normal form]] by applying [[Distributive property|distributivity]].\n\n==Examples and counterexamples==\nThe following formulae are all in negation normal form:\n:<math>(A \\vee B) \\wedge C</math>\n:<math>(A \\wedge (\\lnot B \\vee C) \\wedge \\lnot C) \\vee D</math>\n:<math>A \\vee \\lnot B</math>\n:<math>A \\wedge \\lnot B</math>\n\nThe first example is also in [[conjunctive normal form]] and the last two are in both [[conjunctive normal form]] and [[disjunctive normal form]], but the second example is in neither.\n\nThe following formulae are not in negation normal form:\n:<math>A \\Rightarrow B</math>\n:<math>\\lnot (A \\vee B)</math>\n:<math>\\lnot (A \\wedge B)</math>\n:<math>\\lnot (A \\vee \\lnot C)</math>\n\nThey are however respectively equivalent to the following formulae in negation normal form:\n:<math>\\lnot A \\vee B</math>\n:<math>\\lnot A \\wedge \\lnot B</math>\n:<math>\\lnot A \\vee \\lnot B</math>\n:<math>\\lnot A \\wedge C</math>\n\n==References==\n\n* Alan J.A. Robinson and Andrei Voronkov, ''Handbook of Automated Reasoning'' '''1''':203''ff''  (2001) {{isbn|0444829490}}.\n\n==External links==\n* [http://www.izyt.com/BooleanLogic/applet.php Java applet for converting logical formula to Negation Normal Form, showing laws used]\n\n[[Category:Propositional calculus]]\n[[Category:Normal forms (logic)]]"
    },
    {
      "title": "Prenex normal form",
      "url": "https://en.wikipedia.org/wiki/Prenex_normal_form",
      "text": "A [[formula (mathematical logic)|formula]] of the [[predicate calculus]] is in '''prenex<ref>The term 'prenex' comes from the [[Latin]] ''praenexus'' \"tied or bound up in front\", past participle of ''praenectere'' [http://cs.nyu.edu/pipermail/fom/2007-November/012328.html] (archived as of May 27, 2011 at [https://web.archive.org/web/20110527102347/http://cs.nyu.edu/pipermail/fom/2007-November/012328.html])</ref> [[normal form (mathematics)|normal form]]''' ('''PNF''') if it is written as a string of [[quantifier (logic)|quantifiers]] and [[bound variable|bound variables]], called the '''prefix''', followed by a quantifier-free part, called the '''matrix'''.<ref>Hinman, P. (2005), p. 110</ref> \n\nEvery formula in [[classical logic]] is equivalent to a formula in prenex normal form. For example, if <math>\\phi(y)</math>, <math>\\psi(z)</math>, and <math>\\rho(x)</math> are quantifier-free formulas with the free variables shown then\n:<math>\\forall x \\exists y \\forall z (\\phi(y) \\lor (\\psi(z) \\rightarrow \\rho(x)))</math>\nis in prenex normal form with matrix <math>\\phi(y) \\lor (\\psi(z) \\rightarrow \\rho(x))</math>, while\n:<math>\\forall x ((\\exists y \\phi(y)) \\lor ((\\exists z \\psi(z) ) \\rightarrow \\rho(x)))</math>\nis logically equivalent but not in prenex normal form.\n\n== Conversion to prenex form ==\n{{refimprove section|date=August 2018}}\nEvery [[first-order predicate calculus|first-order]] formula is [[logically equivalent]] (in classical logic) to some formula in prenex normal form.<ref>Hinman, P. (2005), p. 111</ref>  There are several conversion rules that can be recursively applied to convert a formula to prenex normal form.  The rules depend on which [[logical connective]]s appear in the formula. \n\n=== Conjunction and disjunction ===\n\nThe rules for [[logical conjunction|conjunction]] and [[logical disjunction|disjunction]] say that\n:<math>(\\forall x \\phi) \\land \\psi</math> is equivalent to <math>\\forall x ( \\phi \\land \\psi)</math> under (mild) additional condition <math>\\exists x \\top</math>, or, equivalently, <math>\\lnot\\forall x \\bot</math> (meaning that at least one individual exists),\n:<math>(\\forall x \\phi) \\lor \\psi</math> is equivalent to <math>\\forall x ( \\phi \\lor \\psi)</math>;\nand \n:<math>(\\exists x \\phi) \\land \\psi</math> is equivalent to <math>\\exists x (\\phi \\land \\psi)</math>,\n:<math>(\\exists x \\phi) \\lor \\psi</math> is equivalent to <math>\\exists x (\\phi \\lor \\psi)</math> under additional condition <math>\\exists x \\top</math>.\nThe equivalences are valid when <math>x</math> does not appear as a [[free variable]] of <math>\\psi</math>; if <math>x</math> does appear free in <math>\\psi</math>, one can rename the bound <math>x</math> in <math>(\\exists x \\phi)</math> and obtain the equivalent <math>(\\exists x' \\phi[x/x'])</math>.\n\nFor example, in the language of [[ring (mathematics)|rings]],\n:<math>(\\exists x (x^2 = 1)) \\land (0 = y)</math> is equivalent to  <math>\\exists x ( x^2 = 1 \\land 0 = y)</math>,\nbut\n:<math>(\\exists x (x^2 = 1)) \\land (0 = x)</math> is not equivalent to <math>\\exists x ( x^2 = 1 \\land 0 = x)</math>\nbecause the formula on the left is true in any ring when the free variable ''x'' is equal to 0, while the formula on the right has no free variables and is false in any nontrivial ring. So <math>(\\exists x (x^2 = 1)) \\land (0 = x)</math> will be first rewritten as <math>(\\exists x' (x'^2 = 1)) \\land (0 = x)</math> and then put in prenex normal form <math>\\exists x' ( x'^2 = 1 \\land 0 = x)</math>.\n\n=== Negation ===\n\nThe rules for negation say that \n:<math>\\lnot \\exists x \\phi</math> is equivalent to <math>\\forall x \\lnot \\phi</math> \nand \n:<math>\\lnot \\forall x \\phi</math> is equivalent to <math>\\exists x \\lnot \\phi</math>.\n\n=== Implication ===\n\nThere are four rules for implication: two that remove quantifiers from the antecedent and two that remove quantifiers from the consequent. These rules can be derived by rewriting the implication\n<math>\\phi \\rightarrow \\psi</math> as <math>\\lnot \\phi \\lor \\psi</math> and applying the rules for disjunction above.  As with the rules for disjunction, these rules require that the variable quantified in one subformula does not appear free in the other subformula. \n\nThe rules for removing quantifiers from the antecedent are (note the change of quantifiers):\n:<math>(\\forall x \\phi ) \\rightarrow \\psi</math> is equivalent to <math>\\exists x (\\phi \\rightarrow \\psi)</math>,\n:<math>(\\exists x \\phi ) \\rightarrow \\psi</math> is equivalent to <math>\\forall x (\\phi \\rightarrow \\psi)</math>.\nThe rules for removing quantifiers from the consequent are:\n:<math>\\phi \\rightarrow (\\exists x \\psi)</math> is equivalent to <math>\\exists x (\\phi \\rightarrow \\psi)</math>,\n:<math>\\phi \\rightarrow (\\forall x \\psi)</math> is equivalent to <math>\\forall x (\\phi \\rightarrow \\psi)</math>.\n\n=== Example ===\n\nSuppose that <math>\\phi</math>, <math>\\psi</math>, and <math>\\rho</math> are quantifier-free formulas and no two of these formulas share any free variable.   Consider the formula\n:<math> (\\phi \\lor \\exists x \\psi) \\rightarrow \\forall z \\rho</math>.\nBy recursively applying the rules starting at the innermost subformulas, the following sequence of logically equivalent formulas can be obtained:\n:<math> (\\phi \\lor \\exists x \\psi) \\rightarrow \\forall z \\rho</math>.\n:<math> ( \\exists x (\\phi \\lor \\psi) ) \\rightarrow \\forall z \\rho</math>,\n:<math> \\neg( \\exists x (\\phi \\lor \\psi) ) \\lor \\forall z \\rho</math>,\n:<math> (\\forall x \\neg(\\phi \\lor \\psi)) \\lor \\forall z \\rho</math>,\n:<math> \\forall x (\\neg(\\phi \\lor \\psi) \\lor \\forall z \\rho)</math>,\n:<math> \\forall x ( ( \\phi \\lor \\psi) \\rightarrow \\forall z \\rho )</math>,\n:<math> \\forall x ( \\forall z (( \\phi \\lor \\psi) \\rightarrow \\rho ))</math>,\n:<math> \\forall x \\forall z ( ( \\phi \\lor \\psi) \\rightarrow \\rho )</math>.\nThis is not the only prenex form equivalent to the original formula.  For example, by dealing with the consequent before the antecedent in the example above, the prenex form\n:<math>\\forall z \\forall x ( ( \\phi \\lor \\psi) \\rightarrow \\rho)</math>\ncan be obtained: \n:<math> \\forall z (  (\\phi \\lor \\exists x \\psi) \\rightarrow \\rho )</math>\n:<math> \\forall z (  (\\exists x (\\phi \\lor \\psi) ) \\rightarrow \\rho )</math>,\n:<math> \\forall z (  \\forall x ( (\\phi \\lor \\psi) \\rightarrow \\rho ) )</math>,\n:<math> \\forall z \\forall x ( (\\phi \\lor \\psi) \\rightarrow \\rho )</math>.\n\n=== Intuitionistic logic ===\n\nThe rules for converting a formula to prenex form make heavy use of classical logic.  In [[intuitionistic logic]], it is not true that every formula is logically equivalent to a prenex formula.  The negation connective is one obstacle, but not the only one.  The implication operator is also treated differently in intuitionistic logic than classical logic; in intuitionistic logic, it is not definable using disjunction and negation.  \n\nThe [[BHK interpretation]] illustrates why some formulas have no intuitionistically-equivalent prenex form. In this interpretation, a proof of \n:<math>(\\exists x \\phi) \\rightarrow \\exists y \\psi \\qquad (1)</math>\nis a function which, given a concrete ''x'' and a proof of <math>\\phi (x)</math>, produces a concrete ''y'' and a proof of ψ(''y'').  In this case it is allowable for the value of ''y'' to be computed from the given value of ''x''.  A proof of \n:<math>\\exists y ( \\exists x \\phi \\rightarrow \\psi), \\qquad  (2)</math>\non the other hand, produces a single concrete value of ''y'' and a function that converts any proof of <math>\\exists x \\phi</math> into a proof of ψ(''y'').   If each ''x''  satisfying φ can be used to construct a ''y'' satisfying ψ but no such ''y'' can be constructed without knowledge of such an ''x'' then formula (1) will not be equivalent to formula (2).\n\nThe rules for converting a formula to prenex form that do ''fail'' in intuitionistic logic are:\n:(1) <math>\\forall x (\\phi \\lor \\psi)</math> implies <math>(\\forall x \\phi) \\lor \\psi</math>,\n:(2) <math>\\forall x (\\phi \\lor \\psi)</math> implies <math>\\phi \\lor (\\forall x \\psi)</math>,\n:(3) <math>(\\forall x \\phi) \\rightarrow \\psi</math> implies <math>\\exists x (\\phi \\rightarrow \\psi)</math>,\n:(4) <math>\\phi \\rightarrow (\\exists x \\psi)</math> implies <math>\\exists x (\\phi \\rightarrow \\psi)</math>,\n:(5) <math>\\lnot \\forall x \\phi</math> implies <math>\\exists x \\lnot \\phi</math>,\n(''x'' does not appear as a free variable of <math>\\,\\psi</math> in (1) and (3); ''x'' does not appear as a free variable of <math>\\,\\phi</math> in (2) and (4)).\n\n== Use of prenex form ==\n\nSome [[proof calculus|proof calculi]] will only deal with a theory whose formulae are written in prenex normal form. The concept is essential for developing the [[arithmetical hierarchy]] and the [[analytical hierarchy]].\n\n[[Gödel]]'s proof of his [[Gödel's completeness theorem|completeness theorem]] for [[first-order logic]] presupposes that all formulae have been recast in prenex normal form.\n\n[[Tarski's axioms]] for geometry is a logical system whose sentences can ''all'' be written in '''universal-existential form''', a special case of the prenex normal form that has every [[universal quantification|universal quantifier]] preceding any [[existential quantification|existential quantifier]], so that all sentences can be rewritten in the form <math>\\forall u</math>&nbsp;<math>\\forall v</math>&nbsp;<math>\\ldots</math>&nbsp;<math>\\exists a</math>&nbsp;<math>\\exists b</math>&nbsp;<math>\\phi</math>, where <math>\\phi</math> is a sentence that does not contain any quantifier. This fact allowed [[Alfred Tarski|Tarski]] to prove that Euclidean geometry is [[decidability (logic)|decidable]].\n\n==See also==\n{{Wiktionary|prenex}}\n*[[Herbrandization]]\n*[[Skolemization]]\n*[[Arithmetical hierarchy]]\n\n==Notes==\n{{reflist}}\n\n==References==\n* {{cite book|author=Richard L. Epstein|title=Classical Mathematical Logic: The Semantic Foundations of Logic|url=https://books.google.com/books?id=-7HpkQRvQhoC&pg=PA108&dq=%22prenex+normal+form%22&hl=en&sa=X&ved=0ahUKEwiFjYqn0PHiAhVRQ6wKHZh0DqQQ6AEIMzAC#v=onepage&q=%22prenex%20normal%20form%22&f=false|date=18 December 2011|publisher=Princeton University Press|isbn=1-4008-4155-0|pages=108–}}\n* {{cite book|author=Peter B. Andrews|title=An Introduction to Mathematical Logic and Type Theory: To Truth Through Proof|url=https://books.google.com/books?id=UaPuCAAAQBAJ&pg=PA111&dq=%22prenex+normal+form%22&hl=en&sa=X&ved=0ahUKEwiTwe6M6PHiAhUQx58KHa9ZAGQQ6AEILjAB#v=snippet&q=%22prenex%20normal%20form%22&f=false|date=17 April 2013|publisher=Springer Science & Business Media|isbn=978-94-015-9934-4|pages=111–}}\n* {{cite book|author=Elliott Mendelson|title=Introduction to Mathematical Logic, Fourth Edition|url=https://books.google.com/books?id=ZO1p4QGspoYC&pg=PA109&dq=%22prenex+normal+form%22&hl=en&sa=X&ved=0ahUKEwjU4pWA9PPiAhVeIDQIHTH2AYUQ6AEIUjAH#v=snippet&q=%22prenex%20normal%20form%22&f=false|date=1 June 1997|publisher=CRC Press|isbn=978-0-412-80830-2|pages=109–}}\n* {{Citation | last1=Hinman | first1=P. | title=Fundamentals of Mathematical Logic | publisher=[[A K Peters]] | isbn=978-1-56881-262-5 | year=2005}}\n\n[[Category:Normal forms (logic)]]"
    },
    {
      "title": "Skolem normal form",
      "url": "https://en.wikipedia.org/wiki/Skolem_normal_form",
      "text": "In [[mathematical logic]], a [[Well-formed_formula|formula]] of [[first-order logic]] is in '''Skolem normal form''' if it is in [[prenex normal form]] with only [[Universal quantification|universal first-order quantifiers]]. \n\nEvery first-order [[Well-formed formula|formula]] may be converted into Skolem normal form while not changing its [[satisfiability]] via a process called '''Skolemization''' (sometimes spelled '''Skolemnization'''). The resulting formula is not necessarily [[logical equivalence|equivalent]] to the original one, but is [[equisatisfiable]] with it: it is satisfiable if and only if the original one is satisfiable.<ref>{{cite web|title=Normal Forms and Skolemization|url=http://www.mpi-inf.mpg.de/departments/rg1/teaching/autrea-ss10/script/lecture10.pdf|publisher=max planck institut informatik|accessdate=15 December 2012}}</ref>\n\nReduction to Skolem normal form is a method for removing [[existential quantification|existential quantifiers]] from [[formal logic]] statements, often performed as the first step in an [[automated theorem prover]]. \n \n== Examples ==\n\nThe simplest form of Skolemization is for existentially quantified variables which are not inside the [[scope (logic)|scope]] of a universal quantifier. These may be replaced simply by creating new constants. For example, <math>\\exists x P(x)</math> may be changed to <math>P(c)</math>, where <math>c</math> is a new constant (does not occur anywhere else in the formula).\n\nMore generally, Skolemization is performed by replacing every existentially quantified variable <math>y</math> with a term <math>f(x_1,\\ldots,x_n)</math> whose function symbol <math>f</math> is new. The variables of this term are as follows. If the formula is in [[prenex normal form]], <math>x_1,\\ldots,x_n</math> are the variables that are universally quantified and whose quantifiers precede that of <math>y</math>. In general, they are the variables that are quantified universally {{clarify|reason=If the formula is not in prenex form, also some existentially quantified variables may belong to the x, e.g. the formula ¬∃x ¬∃y. p(x,y) should be skolemized to is equivalent to ¬∃x. ¬ p(x,f(x)).|date=August 2013}} and such that <math>\\exists y</math> occurs in the scope of their quantifiers. The function <math>f</math> introduced in this process is called a '''Skolem function''' (or '''Skolem constant''' if it is of zero [[arity]]) and the term is called a '''Skolem term'''.\n \nAs an example, the formula <math>\\forall x \\exists y \\forall z. P(x,y,z)</math> is not in Skolem normal form because it contains the existential quantifier <math>\\exists y</math>. Skolemization replaces <math>y</math> with <math>f(x)</math>, where <math>f</math> is a new function symbol, and removes the quantification over {{nowrap|<math>y</math>.}} The resulting formula is <math>\\forall x \\forall z . P(x,f(x),z)</math>. The Skolem term <math>f(x)</math> contains <math>x</math>, but not <math>z</math>, because the quantifier to be removed <math>\\exists y</math> is in the scope of <math>\\forall x</math>, but not in that of <math>\\forall z</math>; since this formula is in prenex normal form, this is equivalent to saying that, in the list of quantifiers, <math>x</math> precedes <math>y</math> while <math>z</math> does not. The formula obtained by this transformation is satisfiable if and only if the original formula is.\n\n==How Skolemization works==\n\nSkolemization works by applying a [[second-order logic|second-order]] equivalence in conjunction to the definition of first-order satisfiability. The equivalence provides a way for \"moving\" an existential quantifier before a universal one. \n\n:<math>\\forall x \\Big( R(g(x)) \\vee \\exists y  R(x,y) \\Big) \\iff \\exists f \\forall x \\Big( R(g(x)) \\vee R(x,f(x)) \\Big)</math>\nwhere\n:<math>f(x)</math> is a function that maps <math>x</math> to <math>y</math>.\n\nIntuitively, the sentence \"for every <math>x</math> there exists a <math>y</math> such that <math>R(x,y)</math>\" is converted into the equivalent form \"there exists a function <math>f</math> mapping every <math>x</math> into a <math>y</math> such that, for every <math>x</math> it holds <math>R(x,f(x))</math>\".\n\nThis equivalence is useful because the definition of first-order satisfiability implicitly existentially quantifies over the evaluation of function symbols. In particular, a first-order formula <math>\\Phi</math> is satisfiable if there exists a model <math>M</math> and an evaluation <math>\\mu</math> of the free variables of the formula that evaluate the formula to ''true''. The model contains the evaluation of all function symbols; therefore, Skolem functions are implicitly existentially quantified. In the example above, <math>\\forall x . R(x,f(x))</math> is satisfiable if and only if there exists a model <math>M</math>, which contains an evaluation for <math>f</math>, such that <math>\\forall x . R(x,f(x))</math> is true for some evaluation of its free variables (none in this case). This may be expressed in second order as <math>\\exists f \\forall x . R(x,f(x))</math>. By the above equivalence, this is the same as the satisfiability of <math>\\forall x \\exists y . R(x,y)</math>.\n\nAt the meta-level, [[First-order logic#Semantics|first-order satisfiability]] of a formula <math>\\Phi</math> may be written with a little abuse of notation as <math>\\exists M \\exists \\mu ~.~ ( M,\\mu \\models \\Phi)</math>, where <math>M</math> is a model, <math>\\mu</math> is an evaluation of the free variables, and <math>\\models</math> means that <math>\\Phi</math> is true in <math>M</math> under <math>\\mu</math>. Since first-order models contain the evaluation of all function symbols, any Skolem function <math>\\Phi</math> contains is implicitly existentially quantified by <math>\\exists M</math>. As a result, after replacing an existential quantifier over variables into an existential quantifiers over functions at the front of the formula, the formula still may be treated as a first-order one by removing these existential quantifiers. This final step of treating <math>\\exists f \\forall x . R(x,f(x))</math> as <math>\\forall x . R(x,f(x))</math> may be completed because functions are implicitly existentially quantified by <math>\\exists M</math> in the definition of first-order satisfiability.\n\nCorrectness of Skolemization may be shown on the example formula <math>F_1 = \\forall x_1 \\dots \\forall x_n \\exists y R(x_1,\\dots,x_n,y)</math> as follows. This formula is satisfied by a [[model (abstract)|model]] <math>M</math> if and only if, for each possible value for <math>x_1,\\dots,x_n</math> in the domain of the model, there exists a value for <math>y</math> in the domain of the model that makes <math>R(x_1,\\dots,x_n,y)</math> true. By the [[axiom of choice]], there exists a function <math>f</math> such that <math>y = f(x_1,\\dots,x_n)</math>. As a result, the formula <math>F_2 = \\forall x_1 \\dots \\forall x_n R(x_1,\\dots,x_n,f(x_1,\\dots,x_n))</math> is satisfiable, because it has the model obtained by adding the evaluation of <math>f</math> to <math>M</math>. This shows that <math>F_1</math> is satisfiable only if <math>F_2</math> is satisfiable as well. In the other way around, if <math>F_2</math> is satisfiable, then there exists a model <math>M'</math> that satisfies it; this model includes an evaluation for the function <math>f</math> such that, for every value of <math>x_1,\\dots,x_n</math>, the formula <math>R(x_1,\\dots,x_n,f(x_1,\\dots,x_n))</math> holds. As a result, <math>F_1</math> is satisfied by the same model because one may choose, for every value of <math>x_1,\\ldots,x_n</math>, the value <math>y=f(x_1,\\dots,x_n)</math>, where <math>f</math> is evaluated according to <math>M'</math>.\n\n==Uses of Skolemization==\nOne of the uses of Skolemization is [[automated theorem proving]]. For example, in the [[method of analytic tableaux]], whenever a formula whose leading quantifier is existential occurs, the formula obtained by removing that quantifier via Skolemization may be generated. For example, if <math>\\exists x . \\Phi(x,y_1,\\ldots,y_n)</math> occurs in a tableau, where <math>x,y_1,\\ldots,y_n</math> are the free variables of <math>\\Phi(x,y_1,\\ldots,y_n)</math>, then <math>\\Phi(f(y_1,\\ldots,y_n),y_1,\\ldots,y_n)</math> may be added to the same branch of the tableau. This addition does not alter the satisfiability of the tableau: every model of the old formula may be extended, by adding a suitable evaluation of <math>f</math>, to a model of the new formula.\n\nThis form of Skolemization is an improvement over \"classical\" Skolemization in that only variables that are free in the formula are placed in the Skolem term. This is an improvement because the semantics of tableau may implicitly place the formula in the [[Scope (programming)|scope]] of some universally quantified variables that are not in the formula itself; these variables are not in the Skolem term, while they would be there according to the original definition of Skolemization. Another improvement that may be used is applying the same Skolem function symbol for formulae that are identical [[up to]] variable renaming.<ref>R. Hähnle. Tableaux and related methods. [[Handbook of Automated Reasoning]].</ref>\n\nAnother use is in the [[Resolution_(logic)#Resolution_in_first_order_logic|resolution method for first order logic]], where formulas are represented as sets of [[clause (logic)|clause]]s understood to be universally quantified. (For an example see [[drinker paradox]].)\n\n==Skolem theories==\nIn general, if <math>T</math> is a [[theory (mathematical logic)|theory]] and for each formula <math>F</math> with [[free variable]]s <math>x_1, \\dots, x_n, y</math> there is a Skolem function, then <math>T</math> is called a '''Skolem theory'''.<ref>[http://www.math.uu.nl/people/jvoosten/syllabi/logicasyllmoeder.pdf \"Sets, Models and Proofs\" (3.3) by I. Moerdijk and J. van Oosten]</ref> For example, by the above, [[arithmetic]] with the axiom of choice is a Skolem theory.\n\nEvery Skolem theory is [[model complete theory|model complete]], i.e. every [[substructure (mathematics)|substructure]] of a model is an [[elementary equivalence|elementary substructure]]. Given a model ''M'' of a Skolem theory ''T'', the smallest substructure containing a certain set ''A'' is called the '''Skolem hull''' of ''A''. The Skolem hull of ''A'' is an [[atomic model (mathematical logic)|atomic]] [[prime model]] over ''A''.\n\n== History ==\nSkolem normal form is named after the late Norwegian mathematician [[Thoralf Skolem]], who died in 1963.\n\n==See also==\n* [[Herbrandization]], the dual of Skolemization \n* [[Predicate functor logic]]\n\n==Notes==\n<references />\n\n==References==\n* {{Citation | last=Hodges | first=Wilfrid | authorlink=Wilfrid Hodges | title=A Shorter Model Theory | publisher=[[Cambridge University Press]] | isbn=978-0-521-58713-6 | year=1997}}\n\n==External links==\n* {{springer|title=Skolem function|id=p/s085740}}\n* [http://planetmath.org/skolemization Skolemization on PlanetMath.org]\n* [http://demonstrations.wolfram.com/Skolemization/ Skolemization] by Hector Zenil, [[The Wolfram Demonstrations Project]].\n* {{MathWorld |title=SkolemizedForm |urlname=SkolemizedForm}}\n\n{{DEFAULTSORT:Skolem Normal Form}}\n[[Category:Normal forms (logic)]]\n[[Category:Model theory]]"
    },
    {
      "title": "Binary phase",
      "url": "https://en.wikipedia.org/wiki/Binary_phase",
      "text": "{{confuse|Two-phase (disambiguation)}}\n[[File:NaCl polyhedra.png|thumb|Sodium chloride is a famous binary phase.  It features two elements: Na and Cl.]]\nIn [[materials chemistry]], a '''binary phase''' is a [[chemical compound]] containing two different elements. Some binary phases compounds are molecular, e.g. [[carbon tetrachloride]] (CCl<sub>4</sub>). More typically binary phase refers to extended solids.  Famous examples are the two polymorphs of [[zinc sulfide]].<ref name=earnshaw>{{Greenwood&Earnshaw2nd}}</ref>\n\nPhases with higher degrees of complexity feature more elements, e.g. three elements in [[ternary phase]]s, four elements in [[quaternary phase]]s, \n==References==\n{{reflist}}\n\n{{DEFAULTSORT:Binary Compound}}\n[[Category:Chemical compounds]]\n[[Category:Binary compounds| ]]\n\n\n{{chem-stub}}"
    },
    {
      "title": "Binary alloy",
      "url": "https://en.wikipedia.org/wiki/Binary_alloy",
      "text": "#REDIRECT [[Alloy]]\n\n[[Category:Binary compounds]]"
    },
    {
      "title": "Binary compounds of hydrogen",
      "url": "https://en.wikipedia.org/wiki/Binary_compounds_of_hydrogen",
      "text": "'''Binary compounds of hydrogen''' are [[binary compound|binary]] [[chemical compounds]] containing just [[hydrogen]] and one other [[chemical element]]. By convention all binary hydrogen compounds are called [[hydride]]s even when the hydrogen atom in it is not an [[anion]].<ref>''Concise Inorganic Chemistry'' J.D. Lee</ref><ref>''Main Group Chemistry'', 2nd Edition, A. G. Massey</ref><ref>''Advanced Inorganic Chemistry'' [[F. Albert Cotton]], Geoffrey Wilkinson</ref><ref>''Inorganic chemistry'', Catherine E. Housecroft, A. G. Sharpe</ref> These hydrogen compounds can be grouped into several types.\n\n==Overview==\nBinary hydrogen compounds in [[group 1 element|group 1]] are the ionic hydrides (also called saline hydrides) wherein hydrogen is bound electrostatically. Because hydrogen is located somewhat centrally in an electronegative sense, it is necessary for the counterion to be exceptionally electropositive for the hydride to possibly be accurately described as truly behaving ionic. Therefore, this category of hydrides contains only a few members.\n\nHydrides in [[group 2 element|group 2]] are polymeric covalent hydrides. In these, hydrogen forms bridging covalent bonds, usually possessing mediocre degrees of ionic character, which make them difficult to be accurately described as either covalent or ionic. The one exception is [[beryllium hydride]], which has definitively covalent properties.\n\nHydrides in the [[transition metal]]s and [[lanthanides]] are also typically polymeric covalent hydrides. However, they usually possess only weak degrees of ionic character. Usually, these hydrides rapidly decompose into their component elements at ambient conditions. The results consist of metallic matrices with dissolved, often stoichiometric or near so, concentrations of hydrogen, ranging from negligible to substantial. Such a solid can be thought of as a [[solid solution]] and is alternately termed a metallic- or interstitial hydride. These decomposed solids are identifiable by their ability to conduct electricity and their magnetic properties (the presence of hydrogen is coupled with the delocalisation of the valence electrons of the metal), and their lowered density compared to the metal. Both the saline hydrides and the polymeric covalent hydrides typically react strongly with water and air.\n\nIt is possible to produce a metallic hydride without requiring decomposition as a necessary step. If a sample of bulk metal is subjected to any one of numerous hydrogen absorption techniques, the characteristics, such as luster and hardness of the metal is often retained to a large degree. Bulk [[actinoid]] hydrides are only known in this form. The affinity for hydrogen for most of the [[d-block]] elements are low. Therefore, elements in this block do not form hydrides (the '''hydride gap''') under [[standard temperature and pressure]] with the notable exception of [[palladium]].<ref>''Inorganic Chemistry'' Gary Wulfsberg 2000</ref> Palladium can absorb up to 900 times its own volume of hydrogen and is therefore actively researched in the field [[hydrogen storage]].\n\nElements in group 13 to 17 ([[p-block]]) form [[covalent bond|covalent]] hydrides (or '''nonmetal hydrides'''). In [[Group 12 element|group 12]] [[zinc hydride]] is a common chemical reagent but [[cadmium hydride]] and [[mercury(I) hydride|mercury hydride]] are very unstable and esoteric. In group 13 [[boron]] hydrides exist as a highly reactive monomer BH<sub>3</sub>, as an adduct for example [[ammonia borane]] or as dimeric [[diborane]] and as a whole group of BH cluster compounds. Alane (AlH<sub>3</sub>) is a polymer. [[Gallium]] exists as the dimer [[digallane]]. [[Indium hydride]] is only stable below {{Convert|−90|C}}. Not much is known about the final [[group 13 hydride]], [[thallium hydride]].\n\nDue to the total number of possible binary saturated compounds with [[carbon]] of the type C<sub>n</sub>H<sub>2n+2</sub> being very large, there are many [[group 14 hydride]]s. Going down the group the number of binary [[silicon]] compounds ([[silanes]]) is small (straight or branched but rarely cyclic) for example [[disilane]] and [[trisilane]]. For [[germanium]] only 5 linear chain binary compounds are known as gases or volatile liquids. Examples are n-pentagermane, isopentagermane and neopentagermane. Of tin only the distannane is known. [[Plumbane]] is an unstable gas.\n\nThe [[hydrogen halide]]s, [[hydrogen chalcogenide]]s and [[pnictogen hydride]]s also form compounds with hydrogen, whose lightest members show many anomalous properties due to [[hydrogen bonding]].\n\nNon-classical hydrides are those in which extra hydrogen molecules are coordinated as a ligand on the central atoms.  These are very unstable but some have been shown to exist.\n\n[[Polyhydride]]s or superhydrides are compounds in which the number of hydrogen atoms exceed the valency of the combining atom. These may only be stable under extreme pressure, but may be [[high temperature superconductor]]s, such as H<sub>3</sub>S, superconducting at up to 203&nbsp;K. Polyhydrides are actively studied with the hope of discovering a [[room temperature superconductor]].\n\n== The periodic table of the stable binary hydrides ==\nThe relative stability of binary hydrogen compounds and alloys at [[standard temperature and pressure]] can be inferred from their [[standard enthalpy of formation]] values.<ref>Data in KJ/mole gas-phase source: ''Modern Inorganic Chemistry'' W.L. Jolly</ref>\n\n{| class=\"wikitable\" style=\"margin-left:auto; margin-right:auto; margin-bottom:0; margin-top:0; text-align:top; background-color:white; border:0px\"\n|- <!-- ##### PERIODE 1 ##### -->\n| style=\"background-color:#99bbff\" | [[Hydrogen|H<sub>2</sub>]]  <small>0</small>\n| colspan=1 style=\"border:none\"|\n| colspan=10 style=\"border:none\" rowspan=3|\n| colspan=5 style=\"border:none\"|\n| style=\"background:mistyrose\" |He\n|- <!-- ##### PERIODE 2 ##### -->\n| style=\"background:#F0DC82\" | [[Lithium hydride|LiH]]  <small>-91</small>\n| style=\"background:pink\" | [[Beryllium hydride|BeH<sub>2</sub>]]  <small>negative</small>\n| style=\"background:#99bbff\" | [[Borane|BH<sub>3</sub>]]  <small>41</small>\n| style=\"background:#99bbff\" | [[Methane|CH<sub>4</sub>]]  <small>-74.8</small>\n| style=\"background:#99bbff\" | [[Ammonia|NH<sub>3</sub>]]  <small>-46.8</small>\n| style=\"background:#99bbff\" | [[Water|H<sub>2</sub>O]]  <small>-243</small>\n| style=\"background:#99bbff\" | [[Hydrogen fluoride|HF]]  <small>-272</small>\n| style=\"background:mistyrose\" | Ne\n|- <!-- ##### PERIODE 3 ##### -->\n| style=\"background:#F0DC82\" | [[Sodium hydride|NaH]]  <small>-57</small>\n| style=\"background:#F0DC82\" | [[Magnesium hydride|MgH<sub>2</sub>]] <small>-75</small>\n| style=\"background:pink\" | [[Aluminum hydride|AlH<sub>3</sub>]] <small>-46</small>\n| style=\"background:#99bbff\" | [[Silane|SiH<sub>4</sub>]]  <small>31</small>\n| style=\"background:#99bbff\" | [[Phosphine|PH<sub>3</sub>]]  <small>5.4</small>\n| style=\"background:#99bbff\" | [[Hydrogen sulfide|H<sub>2</sub>S]] <small>-20.7</small>\n| style=\"background:#99bbff\" | [[Hydrochloric acid|HCl]]  <small>-93</small>\n| style=\"background:mistyrose\" | Ar\n|- <!-- ##### PERIODE 4 ##### -->\n| style=\"background:#F0DC82\" | [[Potassium hydride|KH]]  <small>-58</small>\n| style=\"background:#F0DC82\" | [[Calcium hydride|CaH<sub>2</sub>]]  <small>-174</small>\n| style=\"background:#98FF98\" | [[Scandium hydride|ScH<sub>2</sub>]]\n| style=\"background:#98FF98\" | [[Titanium hydride|TiH<sub>2</sub>]]\n| style=\"background:#98FF98\" | [[Vanadium hydride|VH]]\n| style=\"background:#98FF98\" | [[Chromium hydride|CrH]]\n| style=\"background:#ddd\" | Mn\n| style=\"background:#ddd\" | Fe\n| style=\"background:#ddd\" | Co\n| style=\"background:#ddd\" | Ni\n| style=\"background:pink\" | [[Copper hydride|CuH]]\n| style=\"background:pink\" | [[Zinc hydride|ZnH<sub>2</sub>]]\n| style=\"background:#99bbff\" | [[Gallane|GaH<sub>3</sub>]]\n| style=\"background:#99bbff\" | [[Germane|GeH<sub>4</sub>]]  <small>92</small>\n| style=\"background:#99bbff\" | [[Arsine|AsH<sub>3</sub>]]  <small>67</small>\n| style=\"background:#99bbff\" | [[Hydrogen selenide|H<sub>2</sub>Se]]  <small>30</small>\n| style=\"background:#99bbff\" | [[Hydrobromic acid|HBr]]  <small>-36.5</small>\n| style=\"background:mistyrose\" | Kr\n|- <!-- ##### PERIODE 5 ##### -->\n| style=\"background:#F0DC82\" | [[Rubidium hydride|RbH]]  <small>-47</small>\n| style=\"background:#F0DC82\" | [[Strontium hydride|SrH<sub>2</sub>]] <small>-177</small>\n| style=\"background:#98FF98\" | [[Yttrium hydride|YH<sub>2</sub>]]\n| style=\"background:#98FF98\" | [[Zirconium hydride|ZrH<sub>2</sub>]]\n| style=\"background:#98FF98\" | [[Niobium hydride|NbH]]\n| style=\"background:#ddd\" | Mo\n| style=\"background:#ddd\" | Tc\n| style=\"background:#ddd\" | Ru\n| style=\"background:#ddd\" | Rh\n| style=\"background:#98FF98\" | [[Palladium hydride|PdH]]\n| style=\"background:#ddd\" | Ag\n| style=\"background:pink\" | [[Cadmium hydride|CdH<sub>2</sub>]]\n| style=\"background:pink\" | [[Indium hydride|InH<sub>3</sub>]]\n| style=\"background:#99bbff\" | [[Stannane|SnH<sub>4</sub>]]  <small>163</small>\n| style=\"background:#99bbff\" | [[Stibine|SbH<sub>3</sub>]]  <small>146</small>\n| style=\"background:#99bbff\" | [[Hydrogen telluride|H<sub>2</sub>Te]] <small>100</small>\n| style=\"background:#99bbff\" | [[Hydroiodic acid|HI]]  <small>26.6</small>\n| style=\"background:mistyrose\" | Xe\n|- <!-- ##### PERIODE 6 ##### -->\n| style=\"background:#F0DC82\" | [[Caesium hydride|CsH]]  <small>-50</small>\n| style=\"background:#F0DC82\" | [[Barium hydride|BaH<sub>2</sub>]]  <small>-172</small>\n| style=\"background:white\" | \n| style=\"background:#98FF98\" | [[Hafnium hydride|HfH<sub>2</sub>]]\n| style=\"background:#98FF98\" | [[Tantalum hydride|TaH]]\n| style=\"background:#ddd\" | W\n| style=\"background:#ddd\" | Re\n| style=\"background:#ddd\" | Os\n| style=\"background:#ddd\" | Ir\n| style=\"background:#ddd\" | Pt\n| style=\"background:#ddd\" | Au\n| style=\"background:#ddd\" | Hg\n| style=\"background:#ddd\" | Tl\n| style=\"background:#99bbff\" | [[Plumbane|PbH<sub>4</sub>]]  <small>252</small>\n| style=\"background:#99bbff\" | [[Bismuthine|BiH<sub>3</sub>]]  <small>247</small>\n| style=\"background:#99bbff\" | [[Hydrogen polonide|H<sub>2</sub>Po]]  <small>167</small>\n| style=\"background:#99bbff\" | [[Hydrogen astatide|HAt]]  <small>positive</small>\n| style=\"background:mistyrose\" | Rn\n|-<!-- ##### PERIODE 7 ##### -->style=\"background-color:mistyrose\"\n| Fr\n| style=\"background:mistyrose\" | Ra\n| style=\"background:white\" | \n| Rf\n| Db\n| Sg\n| Bh\n| Hs\n| Mt\n| Ds\n| Rg\n| Cn\n| Nh\n| Fl\n| Mc\n| Lv\n| Ts\n| Og\n|-\n| colspan=2 style=\"border:none\"|\n| style=\"border:none\"| ↓\n| colspan=14 style=\"border:none\"|\n|- <!-- #### LANTHANIDES #### -->style=\"background-color:mistyrose\"\n| colspan=2 style=\"border:none; background-color:white\"| \n| style=\"background:#98FF98\"|[[Lanthanum hydride|LaH<sub>2</sub>]]\n| style=\"background:#98FF98\"|[[Cerium hydride|CeH<sub>2</sub>]]\n| style=\"background:#98FF98\"|[[Praseodymium hydride|PrH<sub>2</sub>]]\n| style=\"background:#98FF98\"|[[Neodymium hydride|NdH<sub>2</sub>]]\n| style=\"background:#98FF98\"|[[Promethium hydride|PmH<sub>2</sub>]]\n| style=\"background:#98FF98\"|[[Samarium hydride|SmH<sub>2</sub>]]\n| style=\"background:#F0DC82\"|[[Europium hydride|EuH<sub>2</sub>]]\n| style=\"background:#98FF98\"|[[Gadolinium hydride|GdH<sub>2</sub>]]\n| style=\"background:#98FF98\"|[[Terbium hydride|TbH<sub>2</sub>]]\n| style=\"background:#98FF98\"|[[Dysprosium hydride|DyH<sub>2</sub>]]\n| style=\"background:#98FF98\"|[[Holmium hydride|HoH<sub>2</sub>]]\n| style=\"background:#98FF98\"|[[Erbium hydride|ErH<sub>2</sub>]]\n| style=\"background:#98FF98\"|[[Thulium hydride|TmH<sub>2</sub>]]\n| style=\"background:#F0DC82\"|[[Ytterbium hydride|YbH<sub>2</sub>]]\n| style=\"background:#98FF98\"|[[Lutetium hydride|LuH<sub>2</sub>]]\n|- <!-- #### ACTIONIDES #### --> style=\"background-color:mistyrose\"\n| colspan=2 style=\"border:none; background-color:white\"|\n| style=\"background:#98FF98\"|Ac\n| style=\"background:#98FF98\"|Th\n| style=\"background:#98FF98\"|Pa\n| style=\"background:#98FF98\"|U\n| style=\"background:#98FF98\"|Np\n| style=\"background:#98FF98\"|Pu\n| style=\"background:#98FF98\"|Am\n| style=\"background:#98FF98\"|Cm\n| style=\"background:#98FF98\"|Bk\n| style=\"background:#98FF98\"|Cf\n| style=\"background:#98FF98\"|Es\n| style=\"background:#98FF98\"|Fm\n| style=\"background:#98FF98\"|Md\n| style=\"background:#98FF98\"|No\n| style=\"background:#98FF98\"|Lr\n| style=\"border:none; background-color:white\"|\n|}\n{| border=2 cellpadding=4 style=\"margin-left:auto; margin-right:auto; text-align:center; background:silver; border:1px solid gray; border-collapse:collapse; width:50%; font-size:100%; \"\n|-\n|+ '''Binary compounds of [[hydrogen]]'''\n|-\n| style=\"background:#99bbff\" | Covalent hydrides\n| style=\"background:#98FF98\" | metallic hydrides\n\n|-\n| style=\"background:#F0DC82\" | Ionic hydrides\n| style=\"background:pink\" | Intermediate hydrides\n|-\n| style=\"background:#ddd\" | Do not exist\n| style=\"background:mistyrose\" | Not assessed\n|}\n\n== Molecular hydrides ==\nThe isolation of monomeric molecular hydrides usually require extremely mild conditions, which are partial pressure and cryogenic temperature. The reason for this is threefold - firstly, most molecular hydrides are thermodynamically unstable toward decomposition into their elements; secondly, many molecular hydrides are also thermodynamically unstable toward polymerisation; and thirdly, most molecular hydrides are also kinetically unstable toward these types of reactions due to low activation energy barriers.\n\nInstability toward decomposition is generally attributable to poor contribution from the orbitals of the heavier elements to the molecular bonding orbitals. Instability toward polymerisation is a consequence of the electron-deficiency of the monomers relative to the polymers. Relativistic effects play an important role in determining the energy levels of molecular orbitals formed by the heavier elements. As a consequence, these molecular hydrides are commonly less electron-deficient than otherwise expected. For example, based on its position in the 12th column of the periodic table alone, mercury(II) hydride would be expected to be rather deficient. However, it is in fact satiated, with the monomeric form being much more energetically favourable than any oligomeric form.\n\nThe table below shows the monomeric hydride for each element that is closest to, but not surpassing its heuristic valence. A heuristic valence is the valence of an element that strictly obeys the octet, duodectet, and sexdectet valence rules. Elements may be prevented from reaching their heuristic valence by various steric and electronic effects. In the case of chromium, for example, stearic hindrance ensures that both the octahedral and trigonal prismatic molecular geometries for {{Chem|CrH|6}} are thermodynamically unstable to rearranging to a [[Kubas complex]] structural isomer.\n\nWhere available, both the enthalpy of formation for each monomer and the enthalpy of formation for the hydride in its standard state is shown (in brackets) to give a rough indication of which monomers tend to undergo aggregation to lower enthalpic states. For example, monomeric lithium hydride has an enthalpy of formation of 139 kJ mol<sup>−1</sup>, whereas solid lithium hydride has an enthalpy of −91 kJ mol<sup>−1</sup>. This means that it is energetically favourable for a mole of monomeric LiH to aggregate into the ionic solid, losing 230 kJ as a consequence. Aggregation can occur as a chemical association, such as polymerisation, or it can occur as an electrostatic association, such as the formation of hydrogen-bonding in water.\n\n=== Classical hydrides ===\n{| class = \"wikitable\" style = \"margin-left:auto; margin-right:auto; margin-bottom:0; margin-top:0; text-align:top; background-color:white; border:0px\"\n|+ Classical hydrides\n! 1\n! 2\n! 3\n! 4\n! 5\n! 6\n! 5\n! 4\n! 3\n! 2\n! 1\n! 2\n! 3\n! 4\n! 3\n! 2\n! 1\n| colspan = 1 style = \"border:none\" rowspan = 8 |\n|-\n| style = \"background:#99bbff\" | '''{{Chem|H|2}}'''  <small>0</small>\n|-\n| style = \"background:pink\" | [[lithium hydride|'''LiH''']]<ref name=Wang2007July>{{cite journal|last1=Wang|first1=Xuefeng|last2=Andrews|first2=Lester|title=Infrared spectra and theoretical calculations of lithium hydride clusters in solid hydrogen, neon, and argon|journal=The Journal of Physical Chemistry A|date=12 July 2007|volume=111|issue=27|pages=6008–6019|doi=10.1021/jp071251y|pmid=17547379|bibcode=2007JPCA..111.6008W}}</ref>  {{Sup sub|139|(−91)}}\n| style = \"background:#F0DC82\" | [[beryllium hydride#molecular beh2 (dihydridoberyllium)|'''{{Chem|BeH|2}}''']]<ref name=Tague1993>{{cite journal|last1=Tague Jr.|first1=Thomas J.|last2=Andrews|first2=Lester|title=Reactions of beryllium atoms with hydrogen. Matrix infrared spectra of novel product molecules|journal=Journal of the American Chemical Society|date=December 1993|volume=115|issue=25|pages=12111–12116|doi=10.1021/ja00078a057|type = PDF}}</ref>  <small>123</small>\n| colspan = 10 style = \"border:none\" rowspan = 2 |\n| style = \"background:#98FF98\" | [[borane|'''{{Chem|BH|3}}''']]<ref name=\"Tague1994\">{{Cite journal|last=Tague Jr.|first=Thomas J.|author2=Andrews, Lester|title=Reactions of pulsed-laser evaporated boron atoms with hydrogen. Infrared spectra of boron hydride intermediate species in solid argon|journal=Journal of the American Chemical Society|date=June 1994|volume=116|issue=11|pages=4970–4976|doi=10.1021/ja00090a048}}</ref>  {{Sup sub|107|(41)}}\n| style = \"background:#99bbff\" | [[methane|'''{{Chem|CH|4}}''']]  <small>−75</small>\n| style = \"background:#99bbff\" | [[ammonia|'''{{Chem|NH|3}}''']]  <small>−46</small>\n| style = \"background:#99bbff\" | [[properties of water|'''{{Chem|H|2|O}}''']]  {{Sup sub|−242|(−286)}}\n| style = \"background:#99bbff\" | [[hydrogen fluoride|'''HF''']]  <small>−273</small>\n|-\n| style = \"background:pink\" | [[sodium hydride|'''NaH''']]<ref name=Wang2007August>{{cite journal|last1=Wang|first1=Xuefeng|last2=Andrews|first2=Lester|title=Sodium hydride clusters in solid hydrogen and neon: infrared spectra and theoretical calculations|journal=The Journal of Physical Chemistry A|date=2 August 2007|volume=111|issue=30|pages=7098–7104|doi=10.1021/jp0727852|pmid=17602543|bibcode=2007JPCA..111.7098W}}</ref>  {{Sup sub|140|(−56)}}\n| style = \"background:pink\" | [[magnesium hydride#structure and bonding|'''{{Chem|MgH|2}}''']]  {{Sup sub|142|(−76)}}\n| style = \"background:#F0DC82\" | [[aluminium hydride#molecular forms of alane|'''{{Chem|AlH|3}}''']]<ref name=Chertihin1993>{{Cite journal|last=Chertihin|first=George V.|author2=Andrews, Lester|title=Reactions of pulsed-laser ablated aluminum atoms with hydrogen: Infrared spectra of aluminum hydride (AlH, AlH2, AlH3, and Al2H2) species|journal=The Journal of Physical Chemistry|date=October 1993|volume=97|issue=40|pages=10295–10300|doi=10.1021/j100142a007}}</ref>  {{Sup sub|123|(−46)}}\n| style = \"background:#99bbff\" | [[silane|'''{{Chem|SiH|4}}''']]  <small>34</small>\n| style = \"background:#99bbff\" | [[phosphine|'''{{Chem|PH|3}}''']]  <small>5</small>\n| style = \"background:#99bbff\" | [[hydrogen sulfide|'''{{Chem|H|2|S}}''']]  <small>−21</small>\n| style = \"background:#99bbff\" | [[hydrogen chloride|'''HCl''']]  <small>−92</small>\n|-\n| style = \"background:pink\" | [[potassium hydride|'''KH''']]  {{Sup sub|132|(−58)}}\n| style = \"background:pink\" | [[calcium hydride|'''{{Chem|CaH|2}}''']]  {{Sup sub|192|(−174)}}\n| style = \"background:#F0DC82\" | [[scandium(III) hydride|'''{{Chem|ScH|3}}''']]\n| style = \"background:#ddd\" | [[titanium(IV) hydride|'''{{Chem|TiH|4}}''']]\n| style = \"background:#FBEC5D\" | '''{{Chem|VH|2}}'''<ref name=Wang2011/>\n| style = \"background:#FBEC5D\" | [[chromium(II) hydride#mononuclear form|'''{{Chem|CrH|2}}''']]<ref name=Wang2003January>{{Cite journal|last=Wang|first=Xuefeng|author2=Andrews, Lester|title=Chromium hydrides and dihydrogen complexes in solid neon, argon, and hydrogen: Matrix infrared spectra and quantum chemical calculations|journal=The Journal of Physical Chemistry A|date=1 January 2003|volume=107|issue=4|pages=570–578|doi=10.1021/jp026930h|bibcode=2003JPCA..107..570W}}</ref>\n| style = \"background:#ddd\" | '''{{Chem|MnH|2}}'''<ref name=Wang2003April>{{Cite journal|last=Wang|first=Xuefeng|author2=Andrews, Lester|title=Matrix infrared spectra and density functional theory calculations of manganese and rhenium hydrides|journal=The Journal of Physical Chemistry A|date=30 April 2003|volume=107|issue=20|pages=4081–4091|doi=10.1021/jp034392i|bibcode=2003JPCA..107.4081W}}</ref> <small>(−12)</small>\n| style = \"background:#FBEC5D\" | [[iron(II) hydride|'''{{Chem|FeH|2}}''']]<ref name=Wang2008December>{{Cite journal|last=Wang|first=Xuefeng|author2=Andrews, Lester|title=Infrared Spectra and Theoretical Calculations for Fe, Ru, and Os Metal Hydrides and Dihydrogen Complexes|journal=The Journal of Physical Chemistry A|date=18 December 2008|volume=113|issue=3|pages=551–563|doi=10.1021/jp806845h|bibcode=2009JPCA..113..551W}}</ref>  <small>324</small>\n| style = \"background:#ddd\" | '''{{Chem|CoH|2}}'''<ref name=Billups>{{cite journal|last1=Billups|first1=W. E.|last2=Chang|first2=Sou-Chan|last3=Hauge|first3=Robert H.|last4=Margrave|first4=John L.|title=Low-Temperature Reactions of Atomic Cobalt with {{Chem|CH|2|N|2}}, {{Chem|CH|4}}, {{Chem|CH|3|D}}, {{Chem|CH|2|D|2}}, {{Chem|CHD|3}}, {{Chem|CD|4}}, {{Chem|H|2}}, {{Chem|D|2}}, and HD|journal=Journal of the American Chemical Society|date=February 1995|volume=117|issue=4|pages=1387–1392|doi=10.1021/ja00109a024}}</ref>\n| style = \"background:#FBEC5D\" | '''{{Chem|NiH|2}}'''<ref name=Li1997>{{Cite journal|last1=Li|first1=S.|last2=van Zee|first2=R. J.|last3=Weltner Jr.|first3=W.|last4=Cory|first4=M. G.|last5=Zerner|first5=M. C.|title=Magneto-Infrared Spectra of Matrix-Isolated NiH and {{Chem|NiH|2}} Molecules and Theoretical Calculations of the Lowest Electronic States of {{Chem|NiH|2}}|journal=The Journal of Chemical Physics|date=8 February 1997|volume=106|issue=6|pages=2055–2059|doi=10.1063/1.473342|bibcode=1997JChPh.106.2055L}}</ref>  <small>168</small>\n| style = \"background:#F0DC82\" | [[copper hydride#molecular form|'''CuH''']]<ref name=Wang2003September>{{Cite journal|last=Wang|first=Xuefeng|author2=Andrews, Lester|title=Infrared spectra and DFT calculations for the coinage metal hydrides MH, {{Chem{{!}}(H|2|)MH}}, {{Chem|MH|2}}, {{Chem|M|2|H}}, {{Chem|M|2|H|-}}, and {{Chem|(H|2|)CuHCu}} in solid argon, neon, and hydrogen|journal=The Journal of Physical Chemistry A|date=13 September 2003|volume=107|issue=41|pages=8492–8505|doi=10.1021/jp0354346|bibcode=2003JPCA..107.8492W}}</ref>  {{Sup sub|278|(28)}}\n| style = \"background:#F0DC82\" | [[zinc(II) hydride#molecular form|'''{{Chem|ZnH|2}}''']]<ref name=Greene1995>{{Cite journal|last=Greene|first=Tim M.|author2=Brown, Wendy |author3=Andrews, Lester |author4=Downs, Anthony J. |author5=Chertihin, George V. |author6=Runeberg , Nino |author7= Pyykko, Pekka |title=Matrix infrared spectroscopic and ab initio studies of ZnH2, CdH2, and related metal hydride species|journal=The Journal of Physical Chemistry|date=May 1995|volume=99|issue=20|pages=7925–7934|doi=10.1021/j100020a014}}</ref>  <small>162</small>\n| style = \"background:#98FF98\" | [[gallane|'''{{Chem|GaH|3}}''']]<ref name=Wang2003December>{{Cite journal|last=Wang|first=Xuefeng|author2=Andrews, Lester|title=Infrared spectra of gallium hydrides in solid hydrogen: {{Chem{{!}}Ga|H|1,2,3}}, {{Chem|Ga|2|H|2,4,6}}, and the {{Chem|GaH|2,4|-}} anions|journal=The Journal of Physical Chemistry A|date=2 December 2003|volume=107|issue=51|pages=11371–11379|doi=10.1021/jp035393d|bibcode=2003JPCA..10711371W}}</ref>  <small>151</small>\n| style = \"background:#99bbff\" | [[germane|'''{{Chem|GeH|4}}''']]  <small>92</small>\n| style = \"background:#99bbff\" | [[arsine|'''{{Chem|AsH|3}}''']]  <small>67</small>\n| style = \"background:#99bbff\" | [[hydrogen selenide|'''{{Chem|H|2|Se}}''']]  <small>30</small>\n| style = \"background:#99bbff\" | [[hydrogen bromide|'''HBr''']]  <small>−36</small>\n|-\n| style = \"background:pink\" | [[rubidium hydride|'''RbH''']]  {{Sup sub|132|(−47)}}\n| style = \"background:pink\" | '''{{Chem|SrH|2}}'''  {{Sup sub|201|(−177)}}\n| style = \"background:#F0DC82\" | '''{{Chem|YH|3}}'''\n| style = \"background:#ddd\" | '''{{Chem|ZrH|4}}'''\n| style = \"background:#ddd\" | {{Chem|NbH|4}}<ref name=Wang2011/>\n| style = \"background:#ddd\" | '''{{Chem|MoH|6}}'''<ref name=Wang2005>{{Cite journal|last=Wang|first=Xuefeng|author2=Andrews, Lester|title=Matrix infrared spectra and density functional theory calculations of molybdenum hydrides|journal=The Journal of Physical Chemistry A|date=17 September 2005|volume=109|issue=40|pages=9021–9027|doi=10.1021/jp053591u|pmid=16332007|bibcode=2005JPCA..109.9021W}}</ref>\n| style = \"background:silver\" | Tc\n| style = \"background:#ddd\" | '''{{Chem|RuH|2}}'''<ref name=Wang2008December />\n| style = \"background:#ddd\" | {{Chem|RhH|2}}<ref name=Wang2002March>{{Cite journal|last=Wang|first=Xuefeng|author2=Andrews, Lester|title=Infrared spectra of rhodium hydrides in solid argon, neon, and deuterium with supporting density functional calculations|journal=The Journal of Physical Chemistry A|date=19 March 2002|volume=106|issue=15|pages=3706–3713|doi=10.1021/jp013624f|bibcode=2002JPCA..106.3706W}}</ref>\n| style = \"background:#FBEC5D\" | '''PdH'''<ref name=Andrews2001March>{{Cite journal|last=Andrews|first=Lester|last2=Wang |first2=Xuefeng |last3=Alikhani |first3=Mohammad Esmaïl |last4=Manceron |first4=Laurent|title=Observed and calculated infrared spectra of {{Chem{{!}}Pd(H|2|)|1,2,3}} complexes and palladium hydrides in solid argon and neon|journal=The Journal of Physical Chemistry A|date=6 March 2001|volume=15|issue=13|pages=3052–3063|doi=10.1021/jp003721t|bibcode=2001JPCA..105.3052A}}</ref>  <small>361</small>\n| style = \"background:#ddd\" | '''AgH'''<ref name=Wang2003September />  <small>288</small>\n| style = \"background:#F0DC82\" | [[cadmium hydride#molecular cdh2|'''{{Chem|CdH|2}}''']]<ref name=Greene1995 />  <small>183</small>\n| style = \"background:#F0DC82\" | [[indium hydride#indigane|'''{{Chem|InH|3}}''']]<ref name=Wang2004April>{{Cite journal|last=Wang|first=Xuefeng|author2=Andrews, Lester|title=Infrared spectra of indium hydrides in solid hydrogen and neon|journal=The Journal of Physical Chemistry A|date=24 April 2004|volume=108|issue=20|pages=4440–4448|doi=10.1021/jp037942l|bibcode=2004JPCA..108.4440W}}</ref>  <small>222</small>\n| style = \"background:#99bbff\" | [[stannane|'''{{Chem|SnH|4}}''']]  <small>163</small>\n| style = \"background:#99bbff\" | [[stibine|'''{{Chem|SbH|3}}''']]  <small>146</small>\n| style = \"background:#99bbff\" | [[hydrogen telluride|'''{{Chem|H|2|Te}}''']]  <small>100</small>\n| style = \"background:#99bbff\" | [[hydrogen iodide|'''HI''']]  <small>27</small>\n|-\n| style = \"background:pink\" | [[caesium hydride|'''CsH''']]  {{Sup sub|119|(−50)}}\n| style = \"background:pink\" | '''{{Chem|BaH|2}}'''  {{Sup sub|213|(−177)}}\n| colspan = 1 style = \"border:none\" rowspan = 2 |\n| style = \"background:#ddd\" | '''{{Chem|HfH|4}}'''\n| style = \"background:#ddd\" | {{Chem|TaH|4}}<ref name=Wang2011>{{Cite journal|last1=Wang|first1=Xuefeng|last2=Andrews|first2=Lester|title=Tetrahydrometalate Species {{Chem|VH|2|(H|2|)}}, {{Chem|NbH|4}}, and {{Chem|TaH|4}}: Matrix Infrared Spectra and Quantum Chemical Calculations|journal=The Journal of Physical Chemistry A|date=15 December 2011|volume=115|issue=49|pages=14175–14183|doi=10.1021/jp2076148|bibcode=2011JPCA..11514175W}}</ref>\n| style = \"background:#F0DC82\" | '''{{Chem|WH|6}}'''<ref name=Wang2002June>{{Cite journal|last=Wang|first=Xuefeng|author2=Andrews, Lester|title=Neon Matrix Infrared Spectra and DFT Calculations of Tungsten Hydrides {{Chem|WH|''x''}} (''x'' = 1−4, 6)|journal=The Journal of Physical Chemistry A|date=29 June 2002|volume=106|issue=29|pages=6720–6729|doi=10.1021/jp025920d|bibcode=2002JPCA..106.6720W}}</ref>  <small>586</small>\n| style = \"background:#ddd\" | {{Chem|ReH|4}}<ref name=Wang2003April />\n| style = \"background:silver\" | Os\n| style = \"background:silver\" | Ir\n| style = \"background:#ddd\" | '''{{Chem|PtH|2}}'''<ref name=Andrews2001January>{{Cite journal|last1=Andrews|first1=Lester|last2=Wang|first2=Xeufeng|last3=Manceron|first3=Laurent|title=Infrared Spectra and Density Functional Calculations of Platinum Hydrides|journal=The Journal of Chemical Physics|date=22 January 2001|volume=114|issue=4|pages=1559|doi=10.1063/1.1333020|bibcode=2001JChPh.114.1559A}}</ref>\n| style = \"background:#ddd\" | '''AuH'''<ref name=Wang2003September />  <small>295</small>\n| style = \"background:#99bbff\" | [[mercury(II) hydride|'''{{Chem|HgH|2}}''']]<ref name=Wang2004October>{{Cite journal|last=Wang|first=Xuefeng|author2=Andrews, Lester|title=Solid Mercury Dihydride: Mercurophilic Bonding in Molecular {{Chem|HgH|2}} Polymers|journal=Inorganic Chemistry|date=2 October 2004|volume=43|issue=22|pages=7146–7150|doi=10.1021/ic049100m|pmid=15500353}}</ref>  <small>101</small>\n| style = \"background:#ddd\" | [[thallium hydride|'''{{Chem|TlH|3}}''']]<ref name=Wang2004March>{{Cite journal|last=Wang|first=Xuefeng|author2=Andrews, Lester|title=Infrared Spectra of Thallium Hydrides in Solid Neon, Hydrogen, and Argon|journal=The Journal of Physical Chemistry A|date=19 March 2004|volume=108|issue=16|pages=3396–3402|doi=10.1021/jp0498973|bibcode=2004JPCA..108.3396W}}</ref>  <small>293</small>\n| style = \"background:#99bbff\" | [[plumbane|'''{{Chem|PbH|4}}''']]  <small>252</small>\n| style = \"background:#99bbff\" | [[bismuthine|'''{{Chem|BiH|3}}''']]  <small>247</small>\n| style = \"background:#99bbff\" | [[polonium hydride|'''{{Chem|H|2|Po}}''']]  <small>167</small>\n| style = \"background:#ddd\" | [[hydrogen astatide|'''HAt''']]  <small>88</small>\n|-\n| style = \"background:silver\" | Fr\n| style = \"background:silver\" | Ra\n| style = \"background:silver\" | Rf\n| style = \"background:silver\" | Db\n| style = \"background:silver\" | Sg\n| style = \"background:silver\" | Bh\n| style = \"background:silver\" | Hs\n| style = \"background:silver\" | Mt\n| style = \"background:silver\" | Ds\n| style = \"background:silver\" | Rg\n| style = \"background:silver\" | Cn\n| style = \"background:silver\" | Nh\n| style = \"background:silver\" | Fl\n| style = \"background:silver\" | Mc\n| style = \"background:silver\" | Lv\n| style = \"background:silver\" | Ts\n|-\n| colspan = 2 style = \"border:none\" rowspan = 4 |\n| style = \"border:none\" | ↓\n|-\n! 3\n! 4\n! 5\n! 6\n! 7\n! 8\n! 7\n! 6\n! 5\n! 4\n! 3\n! 2\n! 1\n! 2\n! 3\n|-\n| style = \"background:#F0DC82\" | '''{{Chem|LaH|3}}'''\n| style = \"background:#ddd\" | '''{{Chem|CeH|4}}'''\n| style = \"background:#ddd\" | {{Chem|PrH|3}}\n| style = \"background:#ddd\" | {{Chem|NdH|4}}\n| style = \"background:silver\" | Pm\n| style = \"background:#ddd\" | {{Chem|SmH|4}}\n| style = \"background:#ddd\" | {{Chem|EuH|3}}<ref name=\"matsuoka2011\">{{Cite journal|last1=Matsuoka|first1=T.|last2=Fujihisa|first2=H.|last3=Hirao|first3=N.|last4=Ohishi|first4=Y.|last5=Mitsui|first5=T.|last6=Masuda|first6=R.|last7=Seto|first7=M.|last8=Yoda|first8=Y.|last9=Shimizu|first9=K.|last10=Machida|first10=A.|last11=Aoki|first11=K.|title=Structural and valence changes of europium hydride induced by application of high-pressure {{Chem|H|2}}|journal=Physical Review Letters|date=5 July 2011|volume=107|issue=2|doi=10.1103/PhysRevLett.107.025501|bibcode=2011PhRvL.107b5501M|pmid=21797616|pages=025501}}</ref>\n| style = \"background:#F0DC82\" | {{Chem|GdH|3}}\n| style = \"background:#F0DC82\" | {{Chem|TbH|3}}\n| style = \"background:#ddd\" | '''{{Chem|DyH|4}}'''\n| style = \"background:#ddd\" | '''{{Chem|HoH|3}}'''\n| style = \"background:#ddd\" | '''{{Chem|ErH|2}}'''\n| style = \"background:#ddd\" | '''TmH'''\n| style = \"background:#F0DC82\" | '''{{Chem|YbH|2}}'''\n| style = \"background:#ddd\" | '''{{Chem|LuH|3}}'''\n|-\n| style = \"background:silver\" | Ac\n| style = \"background:#ddd\" | '''{{Chem|ThH|4}}'''<ref name=Wang2008February>{{Cite journal|last1=Wang|first1=Xuefeng|last2=Andrews|first2=Lester|last3=Gagliardi|first3=Laura|title=Infrared Spectra of {{Chem|ThH|2}}, {{Chem|ThH|4}}, and the Hydride Bridging {{Chem|ThH|4|(H|2|)|''x''}} (''x'' = 1−4) Complexes in Solid Neon and Hydrogen|journal=The Journal of Physical Chemistry A|date=28 February 2008|volume=112|issue=8|pages=1754–1761|doi=10.1021/jp710326k|pmid=18251527|bibcode=2008JPCA..112.1754W|url=http://archive-ouverte.unige.ch/unige:71}}</ref>\n| style = \"background:silver\" | Pa\n| style = \"background:#ddd\" | [[uranium(IV) hydride|'''{{Chem|UH|4}}''']]<ref name=Raab2007>{{Cite journal|last1=Raab|first1=Juraj|last2=Lindh|first2=Roland H.|last3=Wang|first3=Xuefeng|last4=Andrews|first4=Lester|last5=Gagliardi|first5=Laura|title=A Combined Experimental and Theoretical Study of Uranium Polyhydrides with New Evidence for the Large Complex {{Chem|UH|4|(H|2|)|6}}|journal=The Journal of Physical Chemistry A|date=19 May 2007|volume=111|issue=28|pages=6383–6387|doi=10.1021/jp0713007|pmid=17530832|bibcode=2007JPCA..111.6383R|url=http://archive-ouverte.unige.ch/unige:3194}}</ref>\n| style = \"background:silver\" | Np\n| style = \"background:silver\" | Pu\n| style = \"background:silver\" | Am\n| style = \"background:silver\" | Cm\n| style = \"background:silver\" | Bk\n| style = \"background:silver\" | Cf\n| style = \"background:silver\" | Es\n| style = \"background:silver\" | Fm\n| style = \"background:silver\" | Md\n| style = \"background:silver\" | No\n| style = \"background:silver\" | Lr\n|}\n\n{| border = 2 cellpadding = 4 style = \"margin-left:auto;margin-right:auto;text-align:center;border:1px solid gray;border-collapse:collapse;width:50%;font-size:100%;\"\n|-\n|+ '''Legend'''\n|-\n| style = \"background:#99bbff\" | Monomeric covalent\n| [[File:Methane-CRC-MW-3D-balls.png|100px]]\n| style = \"background:#98FF98\" | Oligomeric covalent\n| [[File:Diborane-3D-balls-A.png|100px]]\n|-\n| style = \"background:#F0DC82\" | Polymeric covalent\n| rowspan = \"2\" | [[File:Beryllium-hydride-3D-balls.png|100px]]\n| rowspan = \"2\" style = \"background:pink\" | Ionic\n| rowspan = \"2\" | [[File:Lithium-hydride-3D-vdW.png|100px]]\n|-\n| style = \"background:#FBEC5D\" | Polymeric delocalised covalent\n|-\n| style = \"background:#ddd\" | Unknown solid structure\n| [[File:Question mark alternate.svg|50px]]\n| colspan = 2 style = \"background:silver\" | Not assessed\n|}\n\nThis table includes the thermally unstable dihydrogen complexes for the sake of completeness. As with the above table, only the complexes with the most complete valence is shown, to the negligence of the most stable complex.\n\n=== Non-classical covalent hydrides ===\nA molecular hydride may be able to bind to hydrogen molecules acting as a ligand. The complexes are termed non-classical covalent hydrides. These complexes contain more hydrogen than the classical covalent hydrides, but are only stable at very low temperatures.  They may be isolated in inert gas matrix, or as a cryogenic gas. Others have only been predicted using computational chemistry.\n{| class=\"wikitable\" style=\"margin-left:auto;margin-right:auto;margin-bottom:0;margin-top:0;text-align:top;background-color:white;border:0px\"\n|+ Non-classical covalent hydrides\n|-\n! colspan = 2 | 8\n! colspan = 8 | 18\n! colspan = 3 | 8\n|-\n| style = \"background:#99bbff\" | {{Chem|LiH|(H|2|)|2}}<ref name=Wang2007July />\n| style = \"background:#ddd\" | Be\n| colspan = 10 style = \"border:none\" rowspan = 2 |\n| style = \"background:#99bbff\" | '''{{Chem|BH|3|(H|2|)}}'''\n|-\n| style = \"background:#ddd\" | Na\n| style = \"background:#ddd\" | {{Chem|MgH|2|(H|2|)|n}}<ref name=\"Wang2004\">{{cite journal|last=Wang|first=Xuefeng|author2=Lester Andrews |year=2004|title=Infrared Spectra of Magnesium Hydride Molecules, Complexes, and Solid Magnesium Dihydride|journal=The Journal of Physical Chemistry A|volume=108|issue=52|pages=11511–11520|issn=1089-5639|doi=10.1021/jp046410h|bibcode=2004JPCA..10811511W}}</ref>\n| style = \"background:#99bbff\" | '''{{Chem|AlH|3|(H|2|)}}'''\n|-\n| style = \"background:#ddd\" | K\n| style = \"background:#ddd\" | Ca<ref name=wang004>{{cite journal|last1=Wang|first1=Xuefeng|last2=Andrews|first2=Lester|title=Metal Dihydride (MH 2 ) and Dimer (M H2 ) Structures in Solid Argon, Neon, and Hydrogen (M = Ca, Sr, and Ba): Infrared Spectra and Theoretical Calculations|journal=The Journal of Physical Chemistry A|date=December 2004|volume=108|issue=52|pages=11500–11510|doi=10.1021/jp046046m|bibcode=2004JPCA..10811500W}}</ref>\n| style = \"background:#99bbff\" | {{Chem|ScH|3|(H|2|)|6}}<ref name=zhao>{{cite web|last1=Zhao|first1=Yufeng|last2=Kim|first2=Yong-Hyun|last3=Dillon|first3=Anne C.|last4=Heben|first4=Michael J.|last5=Zhang|first5=Shengbai|title=Towards High wt%, Room Temperature Reversible, Carbon-Based Hydrogen Adsorbents|url=https://www.researchgate.net/publication/260320124|website=ResearchGate|accessdate=30 November 2015|date=4 August 2014}} Scandium has many empty orbitals to accommodate dihydrogen</ref><ref name=zhao005>{{cite journal|last1=Zhao|first1=Yufeng|last2=Kim|first2=Yong-Hyun|last3=Dillon|first3=A. C.|last4=Heben|first4=M. J.|last5=Zhang|first5=S. B.|title=Hydrogen Storage in Novel Organometallic Buckyballs|journal=Physical Review Letters|date=22 April 2005|volume=94|issue=15|doi=10.1103/PhysRevLett.94.155504|bibcode=2005PhRvL..94o5504Z|pmid=15904160|page=155504}}</ref>\n| style = \"background:#ddd\" | {{Chem|TiH|2|(H|2|)}}<ref>{{cite journal|last1=Ma|first1=Buyong|last2=Collins|first2=Charlene L.|last3=Schaefer|first3=Henry F.|title=Periodic Trends for Transition Metal Dihydrides MH , Dihydride Dihydrogen Complexes MH 2 ·H2 , and Tetrahydrides MH4 (M = Ti, V, and Cr)|journal=Journal of the American Chemical Society|date=January 1996|volume=118|issue=4|pages=870–879|doi=10.1021/ja951376t}}</ref>\n| style = \"background:#99bbff\" | {{Chem|VH|2|(H|2|)}}<ref name=Wang2011/>\n| style = \"background:#99bbff\" | CrH<sub>2</sub>(H<sub>2</sub>)<sub>2</sub><ref>{{cite journal|last1=Wang|first1=Xuefeng|last2=Andrews|first2=Lester|title=Chromium Hydrides and Dihydrogen Complexes in Solid Neon, Argon, and Hydrogen: Matrix Infrared Spectra and Quantum Chemical Calculations|journal=The Journal of Physical Chemistry A|date=January 2003|volume=107|issue=4|pages=570–578|doi=10.1021/jp026930h|bibcode=2003JPCA..107..570W}}</ref>\n| style = \"background:#ddd\" | Mn\n| style = \"background:#99bbff\" | {{Chem|FeH|2|(H|2|)|3}}<ref name=Wang2008>{{Cite journal|last=Wang|first=Xuefeng|author2=Andrews, Lester|title=Infrared spectra and theoretical calculations for Fe, Ru, and Os metal hydrides and dihydrogen complexes|journal=The Journal of Physical Chemistry A|date=18 December 2008|volume=113|issue=3|pages=551–563|doi=10.1021/jp806845h|pmid=19099441|bibcode=2009JPCA..113..551W}}</ref>\n| style = \"background:#99bbff\" | {{Chem|CoH(H|2|)}}\n| style = \"background:#99bbff\" | '''{{Chem|Ni(H|2|)|4}}'''\n| style = \"background:#99bbff\" | CuH(H<sub>2</sub>)\n| style = \"background:#99bbff\" | {{Chem|ZnH|2|(H|2|)}}\n| style = \"background:#99bbff\" | {{Chem|GaH|3|(H|2|)}}\n|-\n| style = \"background:#ddd\" | Rb\n| style = \"background:#ddd\" | Sr<ref name=wang004/>\n| style = \"background:#99bbff\" | {{Chem|YH|2|(H|2|)|3}}\n| style = \"background:#ddd\" | Zr\n| style = \"background:#99bbff\" | {{Chem|NbH|4|(H|2|)|4}}<ref>{{cite journal|last1=Gao|first1=Guoying|last2=Hoffmann|first2=Roald|last3=Ashcroft|first3=N. W.|last4=Liu|first4=Hanyu|last5=Bergara|first5=Aitor|last6=Ma|first6=Yanming|title=Theoretical study of the ground-state structures and properties of niobium hydrides under pressure|journal=Physical Review B|date=12 November 2013|volume=88|issue=18|pages=184104|doi=10.1103/PhysRevB.88.184104|bibcode=2013PhRvB..88r4104G|url=https://digital.csic.es/bitstream/10261/102456/1/Theoretical%20study%20of%20the%20ground-state.pdf|hdl=10261/102456}}</ref>\n| style = \"background:#ddd\" | Mo\n| style = \"background:#ddd\" | Tc\n| style = \"background:#99bbff\" | '''{{Chem|RuH|2|(H|2|)|4}}'''<ref name=Wang2008September>{{cite journal|last1=Wang|first1=Xuefeng|last2=Andrews|first2=Lester|title=Infrared spectrum of the {{Chem|RuH|2|(H|2|)|4}} complex in solid hydrogen|journal=Organometallics|date=13 August 2008|volume=27|issue=17|pages=4273–4276|doi=10.1021/om800507u}}</ref>\n| style = \"background:#99bbff\" | RhH<sub>3</sub>(H<sub>2</sub>)\n| style = \"background:#99bbff\" | {{Chem|Pd(H|2|)|3}}\n| style = \"background:#99bbff\" | AgH(H<sub>2</sub>)\n| style = \"background:#99bbff\" | {{Chem|CdH(H|2|)}}\n| style = \"background:#99bbff\" | {{Chem|InH|3|(H|2|)}}<ref>{{cite journal|last1=Wang|first1=Xuefeng|last2=Andrews|first2=Lester|title=Infrared Spectra of Indium Hydrides in Solid Hydrogen and Neon|journal=The Journal of Physical Chemistry A|date=May 2004|volume=108|issue=20|pages=4440–4448|doi=10.1021/jp037942l|bibcode=2004JPCA..108.4440W}}</ref>\n|-\n| style = \"background:#ddd\" | Cs\n| style = \"background:#ddd\" | Ba<ref name=wang004/>\n| colspan = 1 style = \"border:none\" rowspan = 2 |\n| style = \"background:#ddd\" | Hf\n| style = \"background:#99bbff\" | {{Chem|TaH|4|(H|2|)|4}}<ref name=Wang2011/>\n| style = \"background:#99bbff\" | '''{{Chem|WH|4|(H|2|)|4}}'''<ref>{{cite journal|last1=Wang|first1=Xuefeng|last2=Andrews|first2=Lester|last3=Infante|first3=Ivan|last4=Gagliardi|first4=Laura|title=Infrared Spectra of the WH4(H2) 4 Complex in Solid Hydrogen|journal=Journal of the American Chemical Society|date=February 2008|volume=130|issue=6|pages=1972–1978|doi=10.1021/ja077322o|pmid=18211070|url=http://archive-ouverte.unige.ch/unige:69}}</ref>\n| style = \"background:#ddd\" | Re\n| style = \"background:#ddd\" | Os\n| style = \"background:#ddd\" | Ir\n| style = \"background:#99bbff\" | {{Chem|PtH(H|2|)}}\n| style = \"background:#99bbff\" | {{Chem|AuH|3|(H|2|)}}\n| style = \"background:#ddd\" | Hg\n| style = \"background:#ddd\" | Tl\n|-\n| style = \"background:#ddd\" | Fr\n| style = \"background:#ddd\" | Ra\n| style = \"background:#ddd\" | Rf\n| style = \"background:#ddd\" | Db\n| style = \"background:#ddd\" | Sg\n| style = \"background:#ddd\" | Bh\n| style = \"background:#ddd\" | Hs\n| style = \"background:#ddd\" | Mt\n| style = \"background:#ddd\" | Ds\n| style = \"background:#ddd\" | Rg\n| style = \"background:#ddd\" | Cn\n| style = \"background:#ddd\" | Nh\n|-\n| colspan = 2 style = \"border:none\" rowspan = 4 |\n| style = \"border:none\" | ↓\n|-\n! colspan = 12 | 32\n! colspan = 3 | 18\n|-\n| style = \"background:#99bbff\" | {{Chem|LaH|2|(H|2|)|2}}\n| style = \"background:#99bbff\" | {{Chem|CeH|2|(H|2|)}}\n| style = \"background:#99bbff\" | {{Chem|PrH|2|(H|2|)|2}}\n| style = \"background:#ddd\" | Nd\n| style = \"background:#ddd\" | Pm\n| style = \"background:#ddd\" | Sm\n| style = \"background:#ddd\" | Eu\n| style = \"background:#99bbff\" | {{Chem|GdH|2|(H|2|)}}\n| style = \"background:#ddd\" | Tb\n| style = \"background:#ddd\" | Dy\n| style = \"background:#ddd\" | Ho\n| style = \"background:#ddd\" | Er\n| style = \"background:#ddd\" | Tm\n| style = \"background:#ddd\" | Yb\n| style = \"background:#ddd\" | Lu\n|-\n| style = \"background:#ddd\" | Ac\n| style = \"background:#99bbff\" | ThH<sub>4</sub>(H<sub>2</sub>)<sub>4</sub><ref>{{cite journal|last1=Wang|first1=Xuefeng|last2=Andrews|first2=Lester|last3=Gagliardi|first3=Laura|title=Infrared Spectra of ThH2, ThH4, and the Hydride Bridging ThH4(H2) x(x= 1−4) Complexes in Solid Neon and Hydrogen|journal=The Journal of Physical Chemistry A|date=February 2008|volume=112|issue=8|pages=1754–1761|doi=10.1021/jp710326k|pmid=18251527|bibcode=2008JPCA..112.1754W|url=http://archive-ouverte.unige.ch/unige:71}}</ref>\n| style = \"background:#ddd\" | Pa\n| style = \"background:#99bbff\" | '''{{Chem|UH|4|(H|2|)|6}}'''<ref name=Raab2007 />\n| style = \"background:#ddd\" | Np\n| style = \"background:#ddd\" | Pu\n| style = \"background:#ddd\" | Am\n| style = \"background:#ddd\" | Cm\n| style = \"background:#ddd\" | Bk\n| style = \"background:#ddd\" | Cf\n| style = \"background:#ddd\" | Es\n| style = \"background:#ddd\" | Fm\n| style = \"background:#ddd\" | Md\n| style = \"background:#ddd\" | No\n| style = \"background:#ddd\" | Lr\n|}\n\n{| border=2 cellpadding=4 style=\"margin-left:auto;margin-right:auto;text-align:center;background:silver;border:1px solid gray;border-collapse:collapse;width:50%;font-size:100%;\"\n|-\n|+ '''Legend'''\n|-\n| style = \"background:#99bbff\" | Assessed{{By whom|date=November 2015}}\n| style = \"background:#ddd\" | Not assessed\n|-\n|}\n\n== Hydrogen solutions ==\nHydrogen has a highly variable solubility in the elements. When the continuous phase of the solution is a metal, it is called a ''metallic hydride'' or ''interstitial hydride'', on account of the position of the hydrogen within the crystal structure of the metal. In solution, hydrogen can occur in either the atomic or molecular form. For some elements, when hydrogen content exceeds its solubility, the excess precipitates out as a stoichiometric compound. The table below shows the solubility of hydrogen in each element as a molar ratio at {{Convert|25|C|F}} and 100 kPa.\n\n{| class = \"wikitable\" style = \"margin-left:auto; margin-right:auto; margin-bottom:0; margin-top:0; text-align:top; background-color:white; border:0px\"\n|+\n|-\n| colspan=17 style=\"border:none\"|\n| style = \"background:#99bbff\" | He\n|-\n| style = \"background:#ddd\" | {{Chem|LiH|<nowiki><</nowiki>1{{E|-4}}}}<br/><ref group=nb>Upper limit imposed by phase diagram, taken at 454 K.</ref><ref name=Songster1993>{{Cite journal|last1=Songster|first1=J.|last2=Pélton|first2=A. D.|title=The H-Li (Hydrogen-Lithium) System|journal=Journal of Phase Equilibria|date=1 June 1993|volume=14|issue=3|pages=373–381|doi=10.1007/BF02668238}}</ref>\n| style = \"background:silver\" | Be\n| colspan = 10 style = \"border:none\" rowspan = 2 |\n| style = \"background:silver\" | B\n| style = \"background:silver\" | C\n| style = \"background:#99bbff\" | N\n| style = \"background:#99bbff\" | O\n| style = \"background:#99bbff\" | F\n| style = \"background:#99bbff\" | Ne\n|-\n| style = \"background:#ddd\" | {{Chem|NaH|<nowiki><</nowiki>8{{E|-7}}}}<br/><ref group=nb>Upper limit imposed by phase diagram, taken at 383 K.</ref><ref name=San-Martin1990June>{{Cite journal|last1=San-Martin|first1=A.|last2=Manchester|first2=F. D.|title=The H-Na (Hydrogen-Sodium) System|journal=Bulletin of Alloy Phase Diagrams|date=1 June 1990|volume=11|issue=3|pages=287–294|doi=10.1007/BF03029300}}</ref>\n| style = \"background:#ddd\" | {{Chem|MgH|<nowiki><</nowiki>0.010}}<br/><ref group=nb>Upper limit imposed by phase diagram, taken at 650 K and 25 MPa.</ref><ref name=San-Martin1987October>{{Cite journal|last1=San-Martin|first1=A.|last2=Manchester|first2=F. D.|title=The H−Mg (Hydrogen-Magnesium) System|journal=Journal of Phase Equilibria|date=1 October 1987|volume=8|issue=5|pages=431–437|doi=10.1007/BF02893152}}</ref>\n| style = \"background:#ddd\" | {{Chem|AlH|<nowiki><</nowiki>2.5{{E|-8}}}}<br/><ref group=nb>Upper limit imposed by phase diagram, taken at 556 K.</ref><ref name=Qiu2004>{{cite journal|last1=Qiu|first1=Caian|last2=Olson|first2=Gregory B.|last3=Opalka|first3=Susanne M.|last4=Anton|first4=Donald L.|title=Thermodynamic evaluation of the Al-H system|journal=Journal of Phase Equilibria and Diffusion|date=1 November 2004|volume=25|issue=6|pages=520–527|doi=10.1007/s11669-004-0065-1|issn=1863-7345}}</ref>\n| style = \"background:silver\" | Si\n| style = \"background:silver\" | P\n| style = \"background:silver\" | S\n| style = \"background:#99bbff\" | Cl\n| style = \"background:#99bbff\" | Ar\n|-\n| style = \"background:#ddd\" | {{Chem|KH|<nowiki><<</nowiki>0.01}}<br/><ref group=nb>Upper limit imposed by phase diagram.</ref><ref name=Sangster1997>{{Cite journal|last1=Sangster|first1=J.|last2=Pelton|first2=A. D.|title=The H-K (Hydrogen-Potassium) System|journal=Journal of Phase Equilibria|date=1 August 1997|volume=18|issue=4|pages=387–389|doi=10.1007/s11669-997-0066-y}}</ref>\n| style = \"background:#ddd\" | {{Chem|CaH|<nowiki><<</nowiki>0.01}}<br/><ref group=nb>Upper limit imposed by phase diagram, taken at 500 K.</ref><ref name=Predel1993>{{Cite book|last1=Predel|first1=B.|editor1-last=Madelung|editor1-first=O.|title=Ca-Cd – Co-Zr|date=1993|publisher=Springer Berlin Heidelberg|isbn=978-3-540-47411-1|pages=1–3|chapter=Ca-H (Calcium-Hydrogen)|doi=10.1007/10086082_696|series=Landolt-Börnstein - Group IV Physical Chemistry}}</ref>\n| style = \"background:#ddd\" | {{Chem|ScH|≥1.86}}<br/><ref group=nb>Lower limit imposed by phase diagram.</ref><ref name=Manchester1997April>{{Cite journal|last1=Manchester|first1=F. D.|last2=Pitre|first2=J. M.|title=The H-Sc (Hydrogen-Scandium) System|journal=Journal of Phase Equilibria|date=1 April 1997|volume=18|issue=2|pages=194–205|doi=10.1007/BF02665706}}</ref>\n| style = \"background:#ddd\" | {{Chem|TiH|2.00}}<br/><ref group=nb>Limit imposed by phase diagram.</ref><ref name=San-Martin1987February>{{Cite journal|last1=San-Martin|first1=A.|last2=Manchester|first2=F. D.|title=The H−Ti (Hydrogen-Titanium) System|journal=Bulletin of Alloy Phase Diagrams|date=1 February 1987|volume=8|issue=1|pages=30–42|doi=10.1007/BF02868888}}</ref>\n| style = \"background:#ddd\" | {{Chem|VH|1.00}}<br/><ref group=nb>Limit imposed by phase diagram.</ref><ref name=Predel1996>{{cite book|last1=Predel|first1=B.|editor1-last=Madelung|editor1-first=O.|title=Ga-Gd – Hf-Zr|date=1996|publisher=Springer Berlin Heidelberg|isbn=978-3-540-44996-6|pages=1–5|chapter=H-V (Hydrogen-Vanadium)|doi=10.1007/10501684_1565|series=Landolt-Börnstein - Group IV Physical Chemistry}}</ref>\n| style = \"background:silver\" | Cr\n| style = \"background:#ddd\" | {{Chem|MnH|<nowiki><</nowiki>5{{E|-6}}}}<br/><ref group=nb>Upper limit imposed by phase diagram, taken at 500 K.</ref><ref name=San-Martin1995>{{Cite journal|last1=San-Martin|first1=A.|last2=Manchester|first2=F. D.|title=The H-Mn (Hydrogen-Manganese) System|journal=Journal of Phase Equilibria|date=1 June 1995|volume=16|issue=3|pages=255–262|doi=10.1007/BF02667311}}</ref>\n| style = \"background:#ddd\" | {{Chem|FeH|3{{E|-8}}}}<br/><ref name=San-Martin1990April>{{Cite journal|last1=San-Martin|first1=A.|last2=Manchester|first2=F. D.|title=The Fe-H (Iron-Hydrogen) System|journal=Bulletin of Alloy Phase Diagrams|date=1 April 1990|volume=11|issue=2|pages=173–184|doi=10.1007/BF02841704}}</ref>\n| style = \"background:silver\" | Co\n| style = \"background:#ddd\" | {{Chem|NiH|3{{E|-5}}}}<br/><ref name=Wayman1989>{{Cite journal|last1=Wayman|first1=M. L.|last2=Weatherly|first2=G. C.|title=The H−Ni (Hydrogen-Nickel) System|journal=Bulletin of Alloy Phase Diagrams|date=1 October 1989|volume=10|issue=5|pages=569–580|doi=10.1007/BF02882416}}</ref>\n| style = \"background:#ddd\" | {{Chem|CuH|<nowiki><</nowiki>1{{E|-7}}}}<br/><ref group=nb>Upper limit imposed by phase diagram, taken at 1000 K.</ref><ref name=Predel1994>{{Cite book|last1=Predel|first1=B.|editor1-last=Madelung|editor1-first=O.|title=Cr-Cs – Cu-Zr|date=1994|publisher=Springer Berlin Heidelberg|isbn=978-3-540-47417-3|pages=1–3|chapter=Cu-H (Copper-Hydrogen)}}</ref>\n| style = \"background:#ddd\" | {{Chem|ZnH|<nowiki><</nowiki>3{{E|-7}}}}<br/><ref group=nb>Upper limit at 500 K.</ref><ref name=San-Martin1989>{{Cite journal|last1=San-Martin|first1=A.|last2=Manchester|first2=F. D.|title=The H-Zn (Hydrogen-Zinc) System|journal=Bulletin of Alloy Phase Diagrams|date=1 December 1989|volume=10|issue=6|pages=664–666|doi=10.1007/BF02877640}}</ref>\n| style = \"background:silver\" | Ga\n| style = \"background:silver\" | Ge\n| style = \"background:silver\" | As\n| style = \"background:silver\" | Se\n| style = \"background:silver\" | Br\n| style = \"background:#99bbff\" | Kr\n|-\n| style = \"background:#ddd\" | {{Chem|RbH|<<0.01}}<br/><ref group=nb>Upper limit imposed by phase diagram.</ref><ref name=Sangster1994>{{Cite journal|last1=Sangster|first1=J.|last2=Pelton|first2=A. D.|title=The H-Rb (Hydrogen-Rubidium) System|journal=Journal of Phase Equilibria|date=1 February 1994|volume=15|issue=1|pages=87–89|doi=10.1007/BF02667687}}</ref>\n| style = \"background:silver\" | Sr\n| style = \"background:#ddd\" | {{Chem|YH|≥2.85}}<br/><ref group=nb>Lower limit imposed by phase diagram.</ref><ref name=Khatamian1988>{{cite journal|last1=Khatamian|first1=D.|last2=Manchester|first2=F. D.|title=The H−Y (Hydrogen-Yttrium) System|journal=Bulletin of Alloy Phase Diagrams|date=1 June 1988|volume=9|issue=3|pages=252–260|doi=10.1007/BF02881276}}</ref>\n| style = \"background:#ddd\" | {{Chem|ZrH|≥1.70}}<br/><ref group=nb>Lower limit imposed by phase diagram.</ref><ref name=Zuzek1990>{{Cite journal|last1=Zuzek|first1=E.|last2=Abriata|first2=J. P.|last3=San-Martin|first3=A.|last4=Manchester|first4=F. D.|title=The H-Zr (Hydrogen-Zirconium) System|journal=Bulletin of Alloy Phase Diagrams|date=1 August 1990|volume=11|issue=4|pages=385–395|doi=10.1007/BF02843318}}</ref>\n| style = \"background:#ddd\" | {{Chem|NbH|1.1}}<br/><ref group=nb>Limit imposed by phase diagram.</ref><ref name=Okamoto2012>{{Cite journal|last1=Okamoto|first1=H.|title=H-Nb (Hydrogen-Niobium)|journal=Journal of Phase Equilibria and Diffusion|date=1 April 2013|volume=34|issue=2|pages=163–164|doi=10.1007/s11669-012-0165-2}}</ref>\n| style = \"background:silver\" | Mo\n| style = \"background:silver\" | Tc\n| style = \"background:silver\" | Ru\n| style = \"background:silver\" | Rh\n| style = \"background:#ddd\" | {{Chem|PdH|0.724}}<br/><ref name=MSIT2006>{{Cite book|author1=Materials Science International Team|editor1-last=Effenberg|editor1-first=G.|editor2-last=Ilyenko|editor2-first=S.|title=Noble Metal Systems. Selected Systems from Ag-Al-Zn to Rh-Ru-Sc|volume=11B|date=2006|publisher=Springer Berlin Heidelberg|location=Berlin|isbn=978-3-540-46994-0|pages=1–8|chapter=Au-H-Pd (Gold - Hydrogen - Palladium)|doi=10.1007/10916070_26|series=Landolt-Börnstein - Group IV Physical Chemistry}}</ref>\n| style = \"background:#ddd\" | {{Chem|AgH|3.84{{E|-14}}}}<br/><ref name=Subramanian1991>{{Cite journal|last1=Subramanian|first1=P.R|title=The Ag-H (Silver-Hydrogen) System|journal=Journal of Phase Equilibria|date=1 December 1991|volume=12|issue=6|pages=649–651|doi=10.1007/BF02645164}}</ref>\n| style = \"background:silver\" | Cd\n| style = \"background:silver\" | In\n| style = \"background:silver\" | Sn\n| style = \"background:silver\" | Sb\n| style = \"background:silver\" | Te\n| style = \"background:silver\" | I\n| style = \"background:#99bbff\" | Xe\n|-\n| style = \"background:#ddd\" | {{Chem|CsH|<<0.01}}<br/><ref group=nb>Upper limit imposed by phase diagram.</ref><ref name=Songster1994>{{cite journal|last1=Songster|first1=J.|last2=Pelton|first2=A. D.|title=The H-Cs (Hydrogen-Cesium) System|journal=Journal of Phase Equilibria|date=1 February 1994|volume=15|issue=1|pages=84–86|doi=10.1007/BF02667686}}</ref>\n| style = \"background:silver\" | Ba\n| colspan = 1 style = \"border:none\" rowspan = 2 |\n| style = \"background:silver\" | Hf\n| style = \"background:#ddd\" | {{Chem|TaH|0.79}}<br/><ref group=nb>Limit imposed by phase diagram.</ref><ref name=San-Martin1991>{{Cite journal|last1=San-Martin|first1=A.|last2=Manchester|first2=F. D.|title=The H-Ta (Hydrogen-Tantalum) System|journal=Journal of Phase Equilibria|date=1 June 1991|volume=12|issue=3|pages=332–343|doi=10.1007/BF02649922}}</ref>\n| style = \"background:silver\" | W\n| style = \"background:silver\" | Re\n| style = \"background:silver\" | Os\n| style = \"background:silver\" | Ir\n| style = \"background:silver\" | Pt\n| style = \"background:#ddd\" | {{Chem|AuH|3.06{{E|-9}}}}<br/><ref name=MSIT2006 />\n| style = \"background:#ddd\" | {{Chem|HgH|5{{E|-7}}}}<br/><ref name=Guminski2002>{{Cite journal|last1=Guminski|first1=C.|title=The H-Hg (Hydrogen-Mercury) System|journal=Journal of Phase Equilibria|date=1 October 2002|volume=23|issue=5|pages=448–450|doi=10.1361/105497102770331460}}</ref>\n| style = \"background:silver\" | Tl\n| style = \"background:silver\" | Pb\n| style = \"background:silver\" | Bi\n| style = \"background:silver\" | Po\n| style = \"background:silver\" | At\n| style = \"background:#99bbff\" | Rn\n|-\n| style = \"background:silver\" | Fr\n| style = \"background:silver\" | Ra\n| style = \"background:silver\" | Rf\n| style = \"background:silver\" | Db\n| style = \"background:silver\" | Sg\n| style = \"background:silver\" | Bh\n| style = \"background:silver\" | Hs\n| style = \"background:silver\" | Mt\n| style = \"background:silver\" | Ds\n| style = \"background:silver\" | Rg\n| style = \"background:silver\" | Cn\n| style = \"background:silver\" | Nh\n| style = \"background:silver\" | Fl\n| style = \"background:silver\" | Mc\n| style = \"background:silver\" | Lv\n| style = \"background:silver\" | Ts\n| style = \"background:silver\" | Og\n|-\n| colspan = 2 style = \"border:none\" rowspan = 3 |\n| style = \"border:none\" | ↓\n|-\n| style = \"background:#ddd\" | {{Chem|LaH|≥2.03}}<br/><ref group=nb>Lower limit imposed by phase diagram.</ref><ref name=Khatamian1990>{{Cite journal|last1=Khatamian|first1=D.|last2=Manchester|first2=F. D.|title=The H-La (Hydrogen-Lanthanum) System|journal=Bulletin of Alloy Phase Diagrams|date=1 February 1990|volume=11|issue=1|pages=90–99|doi=10.1007/BF02841589}}</ref>\n| style = \"background:#ddd\" | {{Chem|CeH|≥2.5}}<br/><ref group=nb>Lower limit imposed by phase diagram.</ref><ref name=Manchester1997February>{{Cite journal|last1=Manchester|first1=F. D.|last2=Pitre|first2=J. M.|title=The Ce-H (Cerium-Hydrogen) system|journal=Journal of Phase Equilibria|date=1 February 1997|volume=18|issue=1|pages=63–77|doi=10.1007/BF02646759}}</ref>\n| style = \"background:silver\" | Pr\n| style = \"background:silver\" | Nd\n| style = \"background:silver\" | Pm\n| style = \"background:#ddd\" | {{Chem|SmH|3.00}}<br/><ref name=Zinkevich2002>{{Cite journal|last1=Zinkevich|first1=M.|last2=Mattern|first2=N.|last3=Handstein|first3=A.|last4=Gutfleisch|first4=O.|title=Thermodynamics of Fe–Sm, Fe–H, and H–Sm Systems and its Application to the Hydrogen–Disproportionation–Desorption–Recombination (HDDR) Process for the System {{Chem|Fe|17|Sm|2|–H|2}}|journal=Journal of Alloys and Compounds|date=13 June 2002|volume=339|issue=1–2|pages=118–139|doi=10.1016/S0925-8388(01)01990-9}}</ref>\n| style = \"background:silver\" | Eu\n| style = \"background:silver\" | Gd\n| style = \"background:silver\" | Tb\n| style = \"background:silver\" | Dy\n| style = \"background:silver\" | Ho\n| style = \"background:silver\" | Er\n| style = \"background:silver\" | Tm\n| style = \"background:silver\" | Yb\n| style = \"background:silver\" | Lu\n|-\n| style = \"background:silver\" | Ac\n| style = \"background:silver\" | Th\n| style = \"background:silver\" | Pa\n| style = \"background:#ddd\" | {{Chem|UH|≥3.00}}<br/><ref group=nb>Lower limit imposed by phase diagram.</ref><ref name=Manchester1995>{{Cite journal|last1=Manchester|first1=F. D.|last2=San-Martin|first2=A.|title=The H-U (Hydrogen-Uranium) System|journal=Journal of Phase Equilibria|date=1 June 1995|volume=16|issue=3|pages=263–275|doi=10.1007/BF02667312}}</ref>\n| style = \"background:silver\" | Np\n| style = \"background:silver\" | Pu\n| style = \"background:silver\" | Am\n| style = \"background:silver\" | Cm\n| style = \"background:silver\" | Bk\n| style = \"background:silver\" | Cf\n| style = \"background:silver\" | Es\n| style = \"background:silver\" | FM\n| style = \"background:silver\" | Md\n| style = \"background:silver\" | No\n| style = \"background:silver\" | Lr\n|}\n\n{| border=2 cellpadding=4 style=\"margin-left:auto;margin-right:auto;text-align:center;background:silver;border:1px solid gray;border-collapse:collapse;width:50%;font-size:100%;\"\n|-\n|+ '''Legend'''\n|-\n| style = \"background:#99bbff\" | Miscible\n| style = \"background:silver\" | Undetermined\n|-\n|}\n\n== Notes ==\n{{Reflist|group=nb|2}}\n\n== References ==\n{{Reflist|2}}\n{{Hydrides by group}}\n\n{{DEFAULTSORT:Binary Compounds Of Hydrogen}}\n[[Category:Hydrogen compounds]]\n[[Category:Binary compounds]]"
    },
    {
      "title": "Binary compounds of silicon",
      "url": "https://en.wikipedia.org/wiki/Binary_compounds_of_silicon",
      "text": "[[File:Diagramme binaire Fe Si analyse thermique 30.svg|frame|right|Experimental iron-silicon phase diagram]]\n'''Binary compounds of silicon''' are [[binary compound|binary]] [[chemical compounds]] containing [[silicon]] and one other [[chemical element]].<ref>''Inorganic chemistry'',  Egon Wiberg, Nils Wiberg, Arnold Frederick Holleman</ref> Technically the term [[silicide]] is reserved for any compounds containing silicon bonded to a more [[Electronegativity#Electropositivity|electropositive]] element. Binary silicon compounds can be grouped into several classes.  [[Salt (chemistry)|Salt]]like silicides are formed with the electropositive s-block metals. Covalent silicides and silicon compounds occur with hydrogen and the elements in groups 10 to 17.\n\n[[Transition metal]]s form metallic silicides with some exceptions: [[silver]], [[gold]] and the [[group 12 elements]]. The general composition is M<sub>n</sub>Si or MSi<sub>n</sub> with n ranging from 1 to 6 and M standing for metal. Examples are M<sub>5</sub>Si,  M<sub>3</sub>Si (Cu, V, Cr, Mo, Mn, Fe, Pt, U), M<sub>2</sub>Si (Zr, Hf, Ta, Ir, Ru, Rh, Co, Ni, Ce), M<sub>3</sub>Si<sub>2</sub> (Hf, Th, U), MSi (Ti, Zr, Hf, Fe, Ce, Th, Pu) and MSi<sub>2</sub> (Ti, V, Nb, Ta, Cr, Mo, W, Re).\n\nThe [[Kopp–Neumann law]] applies as:\n\nCp(M,Si,) = xCp(M) + yCp(Si)\n\nAs a general rule nonstochiometry implies instability. These [[intermetallics]] are in general resistant to hydrolysis, brittle,  and melt at a lower temperature than the corresponding [[carbide]]s or [[boride]]s. They are electrical conductors.  However, some, such as CrSi<sub>2</sub>, Mg<sub>2</sub>Si, β-FeSi<sub>2</sub> and  MnSi<sub>1.7</sub>, are [[semiconductors]]. Since [[degenerate semiconductor]]s exhibit some metallic properties, such as luster and electrical conductivity which decreases with temperature, some silicides classified as metals may be semiconductors.\n\n==Group 1==\nSilicides of [[group 1 elements]] are saltlike silicides, except for [[silane]] (SiH<sub>4</sub>) whose bonds to hydrogen are covalent. Higher silane homologues are [[disilane]] and [[trisilane]]. [[Polysilicon hydride]] is a two-dimensional [[polymer network]].\nFor '''lithium silicide''' many cluster compounds are known for example Li<sub>13</sub>Si<sub>4</sub>, Li<sub>22</sub>Si<sub>5</sub>, Li<sub>7</sub>Si<sub>3</sub> and Li<sub>12</sub>Si<sub>7</sub>.<ref>''The Li-Si (Lithium-Silicon)'' system H. Okamoto Journal of Phase Equilibria Volume 11, Number 3, 306-312, {{DOI|10.1007/BF03029305}}</ref> Li<sub>4.4</sub>Si is prepared from silicon and lithium metal in high-energy [[Ball mill]] process.<ref>''Solid state ionics for batteries'', Tsutomu Minami, Masahiro Tatsumisago</ref> Potential uses include electrodes in lithium batteries.  Li<sub>12</sub>Si<sub>7</sub> has a [[Zintl phase]] with planar Si<sub>5</sub><sup>6−</sup> rings. [[Li NMR spectroscopy]] suggests these rings are [[aromatic]].<ref>Sven Dupke, Thorsten Langer, Rainer Pöttgen, Martin Winter, Hellmut Eckert (2012), Structural and dynamic characterization of Li12Si7 and Li12Ge7 using solid state NMR. Solid State Nuclear Magnetic Resonance, Volume 42, Pages 17-25. {{doi|10.1016/j.ssnmr.2011.09.002}}</ref>\n\nOther group 1 elements also form clusters: ''sodium silicide'' can be represented by NaSi, NaSi<sub>2</sub> and Na<sub>11</sub>Si<sub>36</sub><ref>''The na-si (sodium-silicon) system'' J Songster and A.D Pelton Journal of Phase Equilibria Volume 13, Number 1, 67-69, {{DOI|10.1007/BF02645381}}</ref> and '''potassium silicide''' by K<sub>8</sub>Si<sub>46</sub>. Group 1 silicides are in general high melting, metallic grey, with moderate to poor electrical conductance and prepared by heating the elements. Superconducting properties have been reported for  Ba<sub>8</sub>Si<sub>46</sub>.<ref>High-Pressure Synthesis of a New Silicon Clathrate Superconductor, Ba8Si46 Shoji Yamanaka, Eiji Enishi, Hiroshi Fukuoka, and Masahiro Yasukawa Inorg. Chem., 2000, 39 (1), pp 56–58 {{DOI|10.1021/ic990778p}}</ref> Several silicon [[Zintl ion]]s (Si<sub>4</sub><sup>4−</sup> Si<sub>9</sub><sup>4−</sup>, Si<sub>5</sub><sup>2−</sup>) are known with group 1 counter ions.<ref>Scharfe, S., Kraus, F., Stegmaier, S., Schier, A. and Fässler, T. F. (2011), Zintl Ions, Cage Compounds, and Intermetalloid Clusters of Group 14 and Group 15 Elements. Angewandte Chemie International Edition, 50: 3630–3670. {{doi|10.1002/anie.201001630}}</ref>\n\n==Group 2==\nSilicides of [[group 2 elements]] are also saltlike silicides except for [[beryllium]] whose phase diagram with silicon is a simple eutectic (1085&nbsp;°C @ 60% by weight silicon).<ref>''Be-Si (Beryllium-Silicon)'' H. Okamoto Journal of Phase Equilibria and Diffusion Volume 30, Number 1, 115, {{DOI|10.1007/s11669-008-9433-6}}</ref> Again there is variation in composition: [[magnesium silicide]] is represented by Mg<sub>2</sub>Si,<ref>''The Mg−Si (Magnesium-Silicon) system'' A. A. Nayeb-Hashemi and J. B. Clark Journal of Phase Equilibria Volume 5, Number 6, 584-592, {{DOI|10.1007/BF02868321}}</ref> [[Calcium disilicide|calcium silicide]] can be represented by Ca<sub>2</sub>Si, CaSi, CaSi<sub>2</sub>, Ca<sub>5</sub>Si<sub>3</sub> and by Ca<sub>14</sub>Si<sub>19</sub>,<ref>''Ca14Si19 – a Zintl Phase with a Novel Twodimensional Silicon Framework'' Zeitschrift für anorganische und allgemeine Chemie Volume 622, Issue 3, März 1996, Pages: 501–508, Antonio Currao, Steffen Wengert, Reinhard Nesper, Jan Curda and H. Hillebrecht {{DOI|10.1002/zaac.19966220319}}</ref> strontium silicide can be represented by  Sr<sub>2</sub>Si, SrSi<sub>2</sub> and Sr<sub>5</sub>Si<sub>3</sub><ref>''The Si-Sr (Silicon-Strontium) system'' V. P. Itkin and C. B. Alcock Journal of Phase Equilibria Volume 10, Number 6, 630-634, {{DOI|10.1007/BF02877630}}</ref> and barium silicide can be represented by  Ba<sub>2</sub>Si, BaSi<sub>2</sub>, Ba<sub>5</sub>Si<sub>3</sub> and Ba<sub>3</sub>Si<sub>4</sub>.<ref>''The Metallic Zintl Phase Ba3Si4 – Synthesis, Crystal Structure, Chemical Bonding, and Physical Properties'' Zeitschrift für anorganische und allgemeine Chemie Volume 634, Issue 10, August 2008, Pages: 1651–1661, Umut Aydemir, Alim Ormeci, Horst Borrmann, Bodo Böhme, Fabio Zürcher, Burcu Uslu, Thorsten Goebel, Walter Schnelle, Paul Simon, Wilder Carrillo-Cabrera, Frank Haarmann, Michael Baitinger, Reinhard Nesper, Hans Georg von Schnering and Yuri Grin {{DOI|10.1002/zaac.200800116}}</ref> Mg<sub>2</sub>Si, and its [[solid solution]]s with Mg<sub>2</sub>Ge and Mg<sub>2</sub>Sn, are good [[Thermoelectric materials#Magnesium group IV compounds|thermoelectric material]]s and their [[Thermoelectric materials#Figure of merit|figure of merit]] values are comparable with those of established materials.\n\n==Transition metals==\nThe [[transition metal]]s form a wide range of silicon [[intermetallics]] with at least one binary crystalline phase. Some exceptions exist. [[Gold]] forms a [[eutectic]] at 363&nbsp;°C with 2.3% silicon by weight (18% atom percent) without mutual solubility in the solid state.<ref>Constitution of Binary Alloys, second edition,  Max Hansen and Kurt Anderko, McGraw-Hill Book Co., (NY NY 1958)  p. 232 and EG Heath, J. of Electro Control, 11, 1961, pp 13-15 as summarized in Constitution of Binary Alloys, First Supplement, Elliott, McGraw-Hill Book Inc., (NY NY 1965)  p. 103</ref> [[Silver]] forms another eutectic at 835&nbsp;°C with 11% silicon by weight, again with negligible mutual solid state solubility. In [[group 12 element|group 12]] all elements form a eutectic close to the metal melting point without mutual solid-state solubility: zinc at 419&nbsp;°C and > 99 atom percent zinc and cadmium at 320&nbsp;°C (< 99% Cd).\n\nCommercially relevant intermetallics are [[group 6 element|group 6]] [[molybdenum disilicide]], a commercial ceramic mostly used as an heating element. [[Tungsten disilicide]] is also a commercially available ceramic with uses in microelectronics. [[Platinum silicide]] is a semiconductor material. [[Ferrosilicon]] is an iron alloy that also contains some calcium and aluminium.\n\nMnSi known as [[brownleeite]] can be found in outer space. Several Mn silicides form a [[Nowotny phase]]. Nanowires based on silicon and manganese are also known. They can be synthesised from Mn(CO)<sub>5</sub>SiCl<sub>3</sub> forming nanowired based on Mn<sub>19</sub>Si<sub>33</sub>.<ref>''Higher Manganese Silicide Nanowires of Nowotny Chimney Ladder Phase'' Jeremy M. Higgins, Andrew L. Schmitt, Ilia A. Guzei and Song Jin [[J. Am. Chem. Soc.]], 2008, 130 (47), pp 16086–16094 {{DOI|10.1021/ja8065122}}</ref> or grown on a silicon surface<ref>''Formation of manganese silicide nanowires on Si(111) surfaces by the reactive epitaxy method.'' Dan Wang and Zhi-Qiang Zou 2009 Nanotechnology 20 275607 {{doi|10.1088/0957-4484/20/27/275607}}\n</ref><ref>''Ostwald ripening of manganese silicide islands on Si(001)'' M. R. Krause, A. Stollenwerk, M. Licurse, and V. P. LaBella J. Vac. Sci. Technol. A 24, 1480 (2006); {{doi|10.1116/1.2167070}}</ref><ref>''Preparation of manganese silicide thin films by solid phase reaction'' Jinliang Wang, Masaaki Hirai, Masahiko Kusaka and Motohiro Iwami Applied Surface Science\nVolumes 113-114, April 1997, Pages 53-56 {{doi|10.1016/S0169-4332(96)00823-9}}</ref> MnSi<sub>1.73</sub> was investigated as [[thermoelectric materials|thermoelectric material]]<ref>''Synthesis of Thermoelectric Manganese Silicide by Mechanical Alloying and Pulse Discharge Sintering'' Takashi Itoh and Masataka Yamada Journal of Electronic Materials Volume 38, Number 7, 925-929, {{DOI|10.1007/s11664-009-0697-3}}</ref> and as an optoelectronic thin film.<ref>''The potential of higher manganese silicide as an optoelectronic thin film material'' John E. Mahan Thin Solid Films Volume 461, Issue 1, 2 August 2004, Pages 152-159 {{doi|10.1016/j.tsf.2004.02.090}}</ref> Single-crystal MnSi<sub>1.73</sub> can form from a tin-lead melt<ref>''Crystallization of highest manganese silicide MnSi1.71–1.75 from tin-lead solution-melt'' F. Yu. Solomkin, V. K. Zaitsev, N. F. Kartenko, A. S. Kolosova, A. Yu. Samunin and G. N. Isachenko Technical Physics Volume 53, Number 12, 1636-1637, {{DOI|10.1134/S1063784208120190}}</ref>\n\nIn the frontiers of technological research, iron disilicide is becoming more and more relevant to [[optoelectronics]], specially in its crystalline form β-FeSi<sub>2</sub>.<ref>Wetzig, Klaus; Schneider, Claus Michael (eds.). [https://books.google.com/books?id=0LgYYj3Q1pIC&printsec=frontcover#v=onepage&q&f=false ''Metal based thin films for electronics.''] Wiley-VCH, 2006 (2nd edition), p. 64. {{ISBN|3-527-40650-6}}</ref><ref>[http://www.nature.com/nature/journal/v387/n6634/full/387686a0.html ''A silicon/iron-disilicide light-emitting diode operating at a wavelength of 1.5 &mu;m.''] D. Leong, M. Harry, K. J. Reeson and K. P. Homewood. Nature 387, 686-688, 12 June 1997.</ref> They are used as thin films or as nanoparticles, obtained by means of epitaxial growth on a silicon substrate.<ref>''Heteroepitaxy of &beta;-FeSi<sub>2</sub> on Si by gas-source MBE.'' A. Rizzi, B. N. E. Rösen, D. Freundt, C. Dieker, H. Lüth and\nD. Gerthsen. Physical Review B, Volume 51, Issue 24, 17780–17794 (1995). {{DOI|10.1103/PhysRevB.51.17780}}</ref><ref>''Surface electron‐diffraction patterns of &beta;‐FeSi<sub>2</sub> films epitaxially grown on silicon.'' J. E. Mahan, V. L. Thanh, J. Chevrier, I. Berberzier, J. Derrien,\nand R. G. Long. Journal of Applied Physics, Volume 74, Issue 3, 1747 (1993).{{DOI|10.1063/1.354804}}</ref>\n\n{| class=\"wikitable sortable collapsible\" border=\"1\"\n|+ List of reported silicon intermetallics\n|-\n! [[Atomic number]]\n! Name\n! [[Chemical symbol|Symbol]]\n! [[Group (periodic table)|Group]]\n! [[Period (periodic table)|Period]]\n! [[Periodic table block|Block]]\n! Phases\n! Element type\n|-\n| 21\n| [[Scandium]]\n| Sc\n| 3\n| 4\n| d\n| Sc<sub>5</sub>Si<sub>3</sub>, ScSi, Sc<sub>2</sub>Si<sub>3</sub>,<ref name=Schlesinger /> Sc<sub>5</sub>Si<sub>4</sub><ref>''Phases in rapidly cooled scandium-silicon samples'' V. Kotroczo and I.J. McColm Journal of Alloys and Compounds\nVolume 203, 4 January 1994, Pages 259-265 {{doi|10.1016/0925-8388(94)90744-7}}</ref><ref>''Comment on Sc-Si (Scandium-Silicon)'' H. Okamoto Journal of Phase Equilibria Volume 16, Number 5, 477, {{DOI|10.1007/BF02645365}}</ref><ref>Sc-Si (Scandium-Silicon) H. Okamoto Journal of Phase Equilibria Volume 13, Number 6, 679-681, {{DOI|10.1007/BF02667229}}</ref>\n| Transition metal\n|-\n| 22\n| [[Titanium]]\n| Ti\n| 4\n| 4\n| d\n|  Ti<sub>5</sub>Si<sub>3</sub>, TiSi, TiSi<sub>2</sub>, TiSi<sub>3</sub>,  Ti<sub>6</sub>Si<sub>4</sub><ref name=Schlesinger />\n| Transition metal\n|-\n| 23\n| [[Vanadium]]\n| V\n| 5\n| 4\n| d\n|  V<sub>3</sub>Si, V<sub>5</sub>Si<sub>3</sub>, V<sub>6</sub>Si<sub>5</sub>, VSi<sub>2</sub>, V<sub>6</sub>Si<sub>5</sub><ref name=Schlesinger /><ref>''The Si−V (Silicon-Vanadium) system: Addendum'' J. F. Smith Journal of Phase Equilibria Volume 6, Number 3, 266-271, {{DOI|10.1007/BF02880413}}</ref>\n| Transition metal\n|-\n| 24\n| [[Chromium]]\n| Cr\n| 6\n| 4\n| d\n|  Cr<sub>3</sub>Si, Cr<sub>5</sub>Si<sub>3</sub>, CrSi, CrSi<sub>2</sub><ref name=Schlesinger /><ref>''The Cr−Si (Chromium-Silicon) system'' A. B. Gokhale and G. J. Abbaschian Journal of Phase Equilibria Volume 8, Number 5, 474-484, {{DOI|10.1007/BF02893156}}</ref>\n| Transition metal\n|-\n| 25\n| [[Manganese]]\n| Mn\n| 7\n| 4\n| d\n|  MnSi, Mn<sub>9</sub>Si<sub>2</sub>, Mn<sub>3</sub>Si, Mn<sub>5</sub>Si<sub>3</sub>, Mn<sub>11</sub>Si<sub>9</sub><ref name=Schlesinger />\n| Transition metal\n|-\n| 26\n| [[Iron]]\n| Fe\n| 8\n| 4\n| d\n|  Fe<sub>3</sub>Si, FeSi ('''ferrosilicon'''),<ref>Acta Crystallogr. (1948). 1, 212-216   ''The nature of the bonds in the iron silicide, FeSi, and related crystals'' L. Pauling and A. M. Soldate {{doi|10.1107/S0365110X48000570}}</ref><ref>Acta Crystallogr. (1999). B55, 484-493  ''Crystal structure, compressibility and possible phase transitions in FeSi studied by first-principles pseudopotential calculations'' L. Vocadlo, G. D. Price and I. G. Wood {{doi|10.1107/S0108768199001214}}</ref> FeSi<sub>2</sub>\n| Transition metal\n|-\n| 27\n| [[Cobalt]]\n| Co\n| 9\n| 4\n| d\n|  CoSi, CoSi<sub>2</sub>, Co<sub>2</sub>Si, Co<sub>2</sub>Si, Co<sub>3</sub>Si<ref>''Synthesis and Characterization of Cobalt Monosilicide (CoSi) with CsCl Structure Stabilized by a β-SiC Matrix'' Zeitschrift für anorganische und allgemeine Chemie Volume 631, Issue 6-7, May 2005, Pages: 1285–1288, Dirk Walter and I W. Karyasa {{DOI|10.1002/zaac.200500050}}</ref><ref>''The Co-Si (Cobalt-Silicon) system'' K Ishida, T Nishizawa and M. E Schlesinger Journal of Phase Equilibria Volume 12, Number 5, 578-586, {{DOI|10.1007/BF02645074}}</ref>\n| Transition metal\n|-\n| 28\n| [[Nickel]]\n| Ni\n| 10\n| 4\n| d\n|  Ni<sub>3</sub>Si, Ni<sub>31</sub>Si<sub>12</sub>, Ni<sub>2</sub>Si, Ni<sub>3</sub>Si<sub>2</sub>, NiSi, NiSi<sub>2</sub><ref name=Schlesinger /><ref>''The Ni−Si (Nickel-Silicon) system'' P. Nash and A. Nash Journal of Phase Equilibria Volume 8, Number 1, 6-14, {{DOI|10.1007/BF02868885}}</ref>\n| Transition metal\n|-\n| 29\n| [[Copper]]\n| Cu\n| 11\n| 4\n| d\n| Cu<sub>17</sub>Si<sub>3</sub>, Cu<sub>56</sub>Si<sub>11</sub>,Cu<sub>5</sub>Si, Cu<sub>33</sub>Si<sub>7</sub>, Cu<sub>4</sub>Si, Cu<sub>19</sub>Si<sub>6</sub>,Cu<sub>3</sub>Si,Cu<sub>87</sub>Si<sub>13</sub><ref name=Schlesinger>''Thermodynamics of solid transition-metal silicides'' Mark E. Schlesinger [[Chem. Rev.]], 1990, 90 (4), pp 607–628 {{doi|10.1021/cr00102a003}}</ref><ref>''Cu-Si (copper-silicon)'' H. Okamoto Journal of Phase Equilibria Volume 23, Number 3, 281-282, {{DOI|10.1361/105497102770331857}}</ref>\n| Transition metal\n|-\n| 30\n| [[Zinc]]\n| Zn\n| 12\n| 4\n| d\n| eutectic<ref>''The Si-Zn (Silicon-Zinc) system'' R. W. Olesinski and G. J. Abbaschian Journal of Phase Equilibria Volume 6, Number 6, 545-548, {{DOI|10.1007/BF02887156}}</ref>\n| Transition metal\n|-\n| 39\n| [[Yttrium]]\n| Y\n| 3\n| 4\n| d\n| Y<sub>5</sub>Si<sub>3</sub>,  Y<sub>5</sub>Si<sub>4</sub>, YSi,  Y<sub>3</sub>Si<sub>5</sub>,<ref>''The Si−Y (Silicon-Yttrium) system'' A. B. Gokhale and G. J. Abbaschian  Journal of Phase Equilibria Volume 7, Number 5, 485-489, {{DOI|10.1007/BF02867814}}</ref><ref>''The Binary Silicides Eu5Si3 and Yb3Si5 – Synthesis, Crystal Structure, and Chemical Bonding'' Zeitschrift für anorganische und allgemeine Chemie Volume 624, Issue 6, June 1998, Pages: 945–951, Rainer Pöttgen, Rolf-Dieter Hoffmann and Dirk Kußmann {{DOI|10.1002/(SICI)1521-3749(199806)624:6<945::AID-ZAAC945>3.0.CO;2-D}}</ref> YSi<sub>1.4</sub>.<ref>''The Real Structure of YbSi1.4 - Commensurately and Incommensurately Modulated Silicon Substructures'' Zeitschrift für anorganische und allgemeine Chemie Volume 631, Issue 2-3, February 2005, Pages: 546–555, Christof Kubata, Frank Krumeich, Michael Wörle and Reinhard Nesper {{DOI|10.1002/zaac.200400423}}</ref>\n| Transition metal\n|-\n| 40\n| [[Zirconium]]\n| Zr\n| 4\n| 5\n| d\n| Zr<sub>5</sub>Si<sub>3</sub>, Zr<sub>5</sub>Si<sub>4</sub>, ZrSi, ZrSi<sub>2</sub>,<ref name=Schlesinger /> Zr<sub>3</sub>Si<sub>2</sub>, Zr<sub>2</sub>Si, Zr<sub>3</sub>Si<ref>''The Si-Zr (Silicon-Zirconium) system'' H. Okamoto Journal of Phase Equilibria Volume 11, Number 5, 513-519, {{DOI|10.1007/BF02898272}}</ref>\n| Transition metal\n|-\n| 41\n| [[Niobium]]\n| Nb\n| 5\n| 5\n| d\n| Nb<sub>5</sub>Si<sub>3</sub>, Nb<sub>4</sub>Si<ref name=Schlesinger />\n| Transition metal\n|-\n| 42\n| [[Molybdenum]]\n| Mo\n| 6\n| 5\n| d\n|  Mo<sub>3</sub>Si, Mo<sub>5</sub>Si<sub>3</sub>, MoSi<sub>2</sub><ref name=Schlesinger />\n| Transition metal\n|-\n| 43\n| [[Technetium]]\n| Tc\n| 7\n| 5\n| d\n| Tc<sub>4</sub>Si<sub>7</sub> (proposed)<ref>Ein Aufbaumodell für „Chimney-Ladder“-Strukturen Juri N. Grin Monatshefte für Chemie / Chemical Monthly Volume 117, Numbers 8-9, 921-932, {{DOI|10.1007/BF00811261}}</ref>\n| Transition metal\n|-\n| 44\n| [[Ruthenium]]\n| Ru\n| 8\n| 5\n| d\n| Ru<sub>2</sub>Si, Ru<sub>4</sub>Si<sub>3</sub>, RuSi,  Ru<sub>2</sub>Si<sub>3</sub><ref>''The Ruthenium–Silicon system'' L. Perringa, b, F. Bussyc, J. C. Gachonb, * and P. Feschottea Journal of Alloys and Compounds Volume 284, Issues 1-2, 4 March 1999, Pages 198-205 {{DOI|10.1016/S0925-8388(98)00911-6}}</ref><ref>''Ru-Si (Ruthenium-Silicon)'' H. Okamoto Journal of Phase Equilibria\nVolume 21, Number 5, 498, {{DOI|10.1361/105497100770339806}}</ref>\n| Transition metal\n|-\n| 45\n| [[Rhodium]]\n| Rh\n| 9\n| 5\n| d\n| RhSi,<ref>Acta Crystallogr. (1954). 7, 441-443 {{doi|10.1107/S0365110X54001314}} The crystal structure of rhodium silicide, RhSi S. Geller and E. A. Wood</ref> Rh<sub>2</sub>Si,  Rh<sub>5</sub>Si<sub>3</sub>, Rh<sub>3</sub>Si<sub>2</sub>, Rh<sub>20</sub>Si<sub>13</sub><ref>''The rh-si (rhodium-silicon) system'' M.E Schlesinger Journal of Phase Equilibria Volume 13, Number 1, 54-59, {{DOI|10.1007/BF02645377}}</ref>\n| Transition metal\n|-\n| 46\n| [[Palladium]]\n| Pd\n| 10\n| 5\n| d\n| Pd<sub>5</sub>Si, Pd<sub>9</sub>Si<sub>2</sub>, Pd<sub>3</sub>Si, Pd<sub>2</sub>Si, PdSi<ref>''The pdsi (palladiumsilicon) system'' H. C. Baxi and T. B. Massalski Journal of Phase Equilibria Volume 12, Number 3, 349-356, {{DOI|10.1007/BF02649925}}</ref>  \n| Transition metal\n|-\n| 47\n| [[Silver]]\n| Ag\n| 11\n| 5\n| d\n| eutectic<ref>''The Ag-Si (Silver-Silicon) system'' R. W. Olesinski, A. B. Gokhale and G. J. Abbaschian Journal of Phase Equilibria Volume 10, Number 6, 635-640, {{DOI|10.1007/BF02877631}}</ref>\n| Transition metal\n|-\n| 48\n| [[Cadmium]]\n| Cd\n| 12\n| 5\n| d\n| eutectic<ref>''The Cd-Si (Cadmium-Silicon) system'' R. W. Olesinski and G. J. Abbaschian Journal of Phase Equilibria Volume 6, Number 6, 534-536, {{DOI|10.1007/BF02887152}}</ref>\n| Transition metal\n|-\n| 57\n| [[Lanthanum]]\n| La\n| 3\n| 6\n| d\n| La<sub>5</sub>Si<sub>3</sub>, La<sub>3</sub>Si<sub>2</sub>, La<sub>5</sub>Si<sub>4</sub>, LaSi, LaSi<sub>2</sub><ref>''La-Si (Lanthanum-Silicon)'' H. Okamoto Journal of Phase Equilibria and Diffusion Volume 28, Number 6, 585, {{DOI|10.1007/s11669-007-9204-9}}</ref>\n| Lanthanide\n|-\n| 58\n| [[Cerium]]\n| Ce\n| 3\n| 6\n| f\n| Ce<sub>5</sub>Si<sub>3</sub>, Ce<sub>3</sub>Si<sub>2</sub>, Ce<sub>5</sub>Si<sub>4</sub>, CeSi,<ref>''Cerium–silicon system'' M.V. Bulanova, P.N. Zheltov, K.A. Meleshevich, P.A. Saltykov and G. Effenberg Journal of Alloys and Compounds Volume 345, Issues 1-2, 28 October 2002, Pages 110-115 {{doi|10.1016/S0925-8388(02)00409-7}}</ref>  Ce<sub>3</sub>Si<sub>5</sub>, CeSi<sub>2</sub><ref>''The Ce-Si (Cerium-Silicon) system'' A. Munitz, A. B. Gokhale and G. J. Abbaschian Journal of Phase Equilibria Volume 10, Number 1, 73-78, {{DOI|10.1007/BF02882179}}</ref>\n| Lanthanide\n|-\n| 59\n| [[Praseodymium]]\n| Pr\n| 3\n| 6\n| f\n| Pr<sub>5</sub>Si<sub>3</sub>, Pr<sub>3</sub>Si<sub>2</sub>, Pr<sub>5</sub>Si<sub>4</sub>, PrSi, PrSi<sub>2</sub><ref>''Thermodynamic properties of praseodymium silicides in the temperature range 298.15-2257'' K N. P. Gorbachuk, A. S. Bolgar and A. V. Blinder Powder Metallurgy and Metal Ceramics Volume 36, Numbers 9-10, 498-501, {{DOI|10.1007/BF02680501}}</ref>\n| Lanthanide\n|-\n| 60\n| [[Neodymium]]\n| Nd\n| 3\n| 6\n| f\n| Nd<sub>5</sub>Si<sub>3</sub>, Nd<sub>5</sub>Si<sub>4</sub>, Nd<sub>5</sub>Si<sub>3</sub>,NdSi, Nd<sub>3</sub>Si<sub>4</sub>, Nd<sub>2</sub>Si<sub>3</sub>, NdSi<sub>x</sub><ref>''The Nd-Si (Neodymium-Silicon) system'' A. B. Gokhale, A. Munitz and G. J. Abbaschian Journal of Phase Equilibria Volume 10, Number 3, 246-251, {{DOI|10.1007/BF02877504}}</ref>\n| Lanthanide\n|-\n| 61\n| [[Promethium]]\n| Pm\n| 3\n| 6\n| f\n| \n| Lanthanide\n|-\n| 62\n| [[Samarium]]\n| Sm\n| 3\n| 6\n| f\n| Sm<sub>5</sub>Si<sub>4</sub>, Sm<sub>5</sub>Si<sub>3</sub>, SmSi, Sm<sub>3</sub>Si<sub>5</sub>, SmSi<sub>2</sub><ref>''The Si-Sm (Silicon-Samarium) system''  A. B. Gokhale and G. J. Abbaschian Journal of Phase Equilibria Volume 9, Number 5, 582-585, {{DOI|10.1007/BF02881960}}</ref><ref>''The Si-Sm (Silicon-Samarium) system'' A. B. Gokhale and G. J. Abbaschian Journal of Phase Equilibria Volume 9, Number 5, 582-585, {{DOI|10.1007/BF02881960}}</ref>\n| Lanthanide\n|-\n| 63\n| [[Europium]]\n| Eu\n| 3\n| 6\n| f\n| \n| Lanthanide\n|-\n| 64\n| [[Gadolinium]]\n| Gd\n| 3\n| 6\n| f\n| Gd<sub>5</sub>Si<sub>3</sub>, Gd<sub>5</sub>Si<sub>4</sub>, gdSi, GdSi<sub>2</sub><ref>''The Gd−Si (Gadolinium-Silicon) system'' A. B. Gokhale and G. J. Abbaschian Journal of Phase Equilibria Volume 9, Number 5, 574-578, {{DOI|10.1007/BF02881958}}</ref>\n| Lanthanide\n|-\n| 65\n| [[Terbium]]\n| Tb\n| 3\n| 6\n| f\n| Si<sub>2</sub>Tb ([[terbium silicide]]), SiTb, Si<sub>4</sub>Tb<sub>5</sub>,  Si<sub>3</sub>Tb<sub>5</sub><ref>Si-Tb (Silicon-Terbium) H. Okamoto Journal of Phase Equilibria Volume 21, Number 5, 500, {{DOI|10.1361/105497100770339824}}</ref>\n| Lanthanide\n|-\n| 66\n| [[Dysprosium]]\n| Dy\n| 3\n| 6\n| f\n| Dy<sub>5</sub>Si<sub>5</sub>, DySi, DySi<sub>2</sub><ref>''The Enthalpies of DySi2 and HoSi1.67 at 298.15-2007 K. Phase Transformation Enthalpies'' Nikolai P. Gorbachuk and Alexander S. Bolgar Powder Metallurgy and Metal Ceramics Volume 41, Numbers 3-4, 173-176, {{DOI|10.1023/A:1019891128273}}</ref>\n| Lanthanide\n|-\n| 67\n| [[Holmium]]\n| Ho\n| 3\n| 6\n| f\n| Ho<sub>5</sub>Si<sub>3</sub>,Ho<sub>5</sub>Si<sub>4</sub>,HoSi,Ho<sub>4</sub>Si<sub>5</sub>,HoSi<sub>2</sub><ref>''Ho-Si (holmium-silicon)'' H. Okamoto Journal of Phase Equilibria Volume 17, Number 4, 370-371, {{DOI|10.1007/BF02665570}}</ref>\n| Lanthanide\n|-\n| 68\n| [[Erbium]]\n| Er\n| 3\n| 6\n| f\n| Er<sub>5</sub>Si<sub>3</sub>, Er<sub>5</sub>Si<sub>4</sub>, ErSi, ErSi<sub>2</sub><ref>Er-Si (erbium-silicon) H. Okamoto Journal of Phase Equilibria Volume 18, Number 4, 403, {{DOI|10.1007/s11669-997-0073-z}}</ref>\n| Lanthanide\n|-\n| 69\n| [[Thulium]]\n| Tm\n| 3\n| 6\n| f\n| \n| Lanthanide\n|-\n| 70\n| [[Ytterbium]]\n| Yb\n| 3\n| 6\n| f\n| Si<sub>1.8</sub>Yb,Si<sub>5</sub>Yb<sub>3</sub>,Si<sub>4</sub>Yb<sub>3</sub>, SiYb, Si<sub>4</sub>Yb<sub>5</sub>, Si<sub>3</sub>Yb<sub>5</sub><ref>''Si-Yb (Silicon-Ytterbium'') H. Okamoto Journal of Phase Equilibria Volume 24, Number 6, 583, {{DOI|10.1361/105497103772084787}}</ref>\n| Lanthanide\n|-\n| 71\n| [[Lutetium]]\n| Lu\n| 3\n| 6\n| f\n| Lu<sub>5</sub>Si<sub>3</sub><ref>Standard enthalpies of formation of Me5Si3 (Me triple bond; length as m-dash Y, Lu, Zr) and of Hf3Si2 L. Topor,  and O.J. Kleppa Journal of the Less Common Metals Volume 167, Issue 1, December 1990, Pages 91-99 {{doi|10.1016/0022-5088(90)90292-R}}</ref>\n| Lanthanide\n|-\n| 72\n| [[Hafnium]]\n| Hf\n| 4\n| 6\n| d\n|  Hf<sub>2</sub>Si, Hf<sub>3</sub>Si<sub>2</sub>,  HfSi, Hf<sub>5</sub>Si<sub>4</sub>, HfSi<sub>2</sub><ref name=Schlesinger /><ref>''The Hf-Si (hafnium-silicon) system'' A. B. Gokhale and G. J. Abbaschian Journal of Phase Equilibria Volume 10, Number 4, 390-393, {{DOI|10.1007/BF02877595}}</ref>\n| Transition metal\n|-\n| 73\n| [[Tantalum]]\n| Ta\n| 5\n| 6\n| d \n| Ta<sub>9</sub>Si<sub>2</sub>, Ta<sub>3</sub>Si, Ta<sub>5</sub>Si<sub>3</sub><ref name=Schlesinger />\n| Transition metal\n|-\n| 74\n| [[Tungsten]]\n| W\n| 6\n| 6\n| d\n|  W<sub>5</sub>Si<sub>3</sub>, WSi<sub>2</sub><ref>''Tungsten: Properties, Chemistry, Technology of the Element, Alloys, and Chemical Compounds'' Lassner, Erik, Schubert, Wolf-Dieter 1999</ref>\n| Transition metal\n|-\n| 75\n| [[Rhenium]]\n| Re\n| 7\n| 6\n| d\n| Re<sub>2</sub>Si, ReSi, ReSi<sub>1.8</sub><ref>''The Re-Si system (rhenium-silicon)'' A. B. Gokhale and R. Abbaschian Journal of Phase Equilibria Volume 17, Number 5, 451-454, {{DOI|10.1007/BF02667640}}</ref> Re<sub>5</sub>Si<sub>3</sub><ref name=Schlesinger />\n| Transition metal\n|-\n| 76\n| [[Osmium]]\n| Os\n| 8\n| 6\n| d\n| OsSi, Os<sub>2</sub>Si<sub>3</sub>, OsSi<sub>2</sub><ref>''Os-Si (Osmium-Silicon)'' H. Okamoto Journal of Phase Equilibria and Diffusion Volume 28, Number 4, 410, {{DOI|10.1007/s11669-007-9121-y}}</ref>\n| Transition metal\n|-\n| 77\n| [[Iridium]]\n| Ir\n| 9\n| 6\n| d\n| IrSi, Ir<sub>4</sub>Si<sub>5</sub>, Ir<sub>3</sub>Si<sub>4</sub>, Ir<sub>3</sub>Si<sub>5</sub>, IrSi<sub>3</sub>.  Ir<sub>2</sub>Si<sub>3</sub>, Ir<sub>4</sub>Si<sub>7</sub>, IrSi<sub>2</sub><ref>''Phase diagram and electrical behavior of silicon-rich iridium silicide compounds'' Journal of Alloys and Compounds, Volume 200, Issues 1-2, 8 October 1993, Pages 99-105 C.E. Allevato, Cronin B. Vining {{doi|10.1016/0925-8388(93)90478-6}}</ref><ref>Acta Crystallogr. (1967). 22, 417-430 {{doi|10.1107/S0365110X67000799}} The crystal structure of Rh17Ga22, an example of a new kind of electron compound W. Jeitschko and E. Parthé</ref>\n| Transition metal\n|-\n| 78\n| [[Platinum]]\n| Pt\n| 10\n| 6\n| d\n| Pt<sub>25</sub>Si<sub>7</sub>, Pt<sub>17</sub>Si<sub>8</sub>,  Pt<sub>6</sub>Si<sub>5</sub>,  Pt<sub>5</sub>Si<sub>2</sub>, Pt<sub>3</sub>Si, Pt<sub>2</sub>Si, PtSi<ref>''Pt-Si (Platinum-Silicon)'' H. Okamoto Journal of Phase Equilibria\nVolume 16, Number 3, 286-287, {{DOI|10.1007/BF02667320}}</ref>\n| Transition metal\n|-\n| 79\n| [[Gold]]\n| Au\n| 11\n| 6\n| d\n| [[Eutectic bonding|Eutectic]] diagram at link<ref name=\"AuSi\">''The Au−Si (Gold-Silicon) system'' H. Okamoto and T. B. Massalski Journal of Phase Equilibria Volume 4, Number 2, 190-198, {{DOI|10.1007/BF02884878}}</ref>\n| Transition metal\n|-\n| 80\n| [[Mercury (element)|Mercury]]\n| Hg\n| 12\n| 6\n| d\n| eutectic<ref>''The Hg-Si system (mercury-silicon)'' C. Guminski Journal of Phase Equilibria Volume 22, Number 6, 682-683, {{DOI|10.1007/s11669-001-0041-y}}</ref>\n| Transition metal\n|-\n| 89\n| [[Actinium]]\n| Ac\n| 3\n| 7\n| d\n| \n| Actinide\n|-\n| 90\n| [[Thorium]]\n| Th\n| 3\n| 7\n| f\n|  Th<sub>3</sub>Si<sub>2</sub>, ThSi, Th<sub>3</sub>Si<sub>5</sub>, and ThSi<sub>2−x</sub><ref>as summarized in Constitution of Binary Alloys, Second Supplement, Francis A. Shunk, McGraw-Hill Book Inc., (NY NY 1969)  p. 681-82.</ref>\n| Actinide\n|-\n| 91\n| [[Protactinium]]\n| Pa\n| 3\n| 7\n| f\n| \n| Actinide\n|-\n| 92\n| [[Uranium]]\n| U\n| 3\n| 7\n| f\n| U<sub>3</sub>Si, U<sub>3</sub>Si<sub>2</sub>, USi, U<sub>3</sub>Si<sub>5</sub>, USi<sub>2−x</sub>, USi<sub>2</sub> and USi<sub>3</sub><ref>http://www.rertr.anl.gov/Web1999/PDF/18suripto.pdf</ref>\n| Actinide\n|-\n| 93\n| [[Neptunium]]\n| Np\n| 3\n| 7\n| f\n| NpSi<sub>3</sub>, Np<sub>3</sub>Si<sub>2</sub>, and NpSi<ref>''Structural chemistry of the neptunium–siliconnext term binary system '' Pascal Boulet, Daniel Bouëxière, Jean Rebizant and Franck Wastin Journal of Alloys and Compounds Volume 349, Issues 1-2, 3 February 2003, Pages 172-179  {{doi|10.1016/S0925-8388(02)00918-0}}</ref>\n| Actinide\n|-\n| 94\n| [[Plutonium]]\n| Pu\n| 3\n| 7\n| f\n| Pu<sub>5</sub>Si<sub>3</sub>, Pu<sub>3</sub>Si<sub>2</sub>, PuSi, Pu<sub>3</sub>Si<sub>5</sub>  and PuSi<sub>2</sub><ref>The ''plutonium-silicon system'' C.C. Land, K.A. Johnson and F.H. Ellinger  Journal of Nuclear Materials Volume 15, Issue 1, 1965, Pages 23-32 {{doi|10.1016/0022-3115(65)90105-4}}</ref>\n| Actinide\n|-\n| 95\n| [[Americium]]\n| Am\n| 3\n| 7\n| f\n| AmSi, AmSi<sub>2</sub><ref>''Americium monosilicide and “disilicide”'' F. Weigel, F.D. Wittmann and R. Marquart Journal of the Less Common Metals Volume 56, Issue 1, November 1977, Pages 47-53 {{doi|10.1016/0022-5088(77)90217-X}}</ref>\n\n| Actinide\n|-\n| 96\n| [[Curium]]\n| Cm\n| 3\n| 7\n| f\n| CmSi, Cm<sub>2</sub>Si<sub>3</sub>, CmSi<sub>2</sub><ref>''Preparation and properties of some curium silicides'' F. Weigel and R. Marquart Journal of the Less Common Metals Volume 90, Issue 2, April 1983, Pages 283-290 {{doi|10.1016/0022-5088(83)90077-2}}</ref>\n\n| Actinide\n|-\n| 97\n| [[Berkelium]]\n| Bk\n| 3\n| 7\n| f\n|\n| Actinide\n|-\n| 98\n| [[Californium]]\n| Cf\n| 3\n| 7\n| f\n|\n| Actinide\n|-\n| 99\n| [[Einsteinium]]\n| Es\n| 3\n| 7\n| f\n|\n| Actinide\n|-\n| 100\n| [[Fermium]]\n| Fm\n| 3\n| 7\n| f\n|\n| Actinide\n|-\n| 101\n| [[Mendelevium]]\n| Md\n| 3\n| 7\n| f\n|\n| Actinide\n|-\n| 102\n| [[Nobelium]]\n| No\n| 3\n| 7\n| f\n|\n| Actinide\n|-\n| 103\n| [[Lawrencium]]\n| Lr\n| 3\n| 7\n| f\n|\n| Actinide\n|}\n\n==Group 13==\nIn [[group 13 elements|group 13]] boron (a [[metalloid]]) forms several binary crystalline [[silicon boride]] compounds:  SiB<sub>3</sub>, SiB<sub>6</sub>, SiB<sub>n</sub>.<ref>''The B−Si (Boron-Silicon) system'' R. W. Olesinski and G. J. Abbaschian Journal of Phase Equilibria Volume 5, Number 5, 478-484, {{DOI|10.1007/BF02872900}}</ref> With [[aluminium]], a [[post-transition metal]], a eutectic is formed (577&nbsp;°C @ 12.2 atom % Al) with maximum solubility of silicon in solid aluminium of 1.5%. Commercially relevant [[aluminium alloy]]s containing silicon have at least element added.<ref>''The Al-Si (Aluminum-Silicon) system'' J. L. Murray and A. J. McAlister Journal of Phase Equilibria Volume 5, Number 1, 74-84, {{DOI|10.1007/BF02868729}}</ref> [[Gallium]], also a  [[post-transition metal]], forms a eutectic at 29&nbsp;°C with 99.99% Ga without mutual solid-state solubility;<ref>''The Ga−Si (Gallium-Silicon) system'' R. W. Olesinski, N. Kanani and G. J. Abbaschian Journal of Phase Equilibria Volume 6, Number 4, 362-364, {{DOI|10.1007/BF02880523}}</ref> [[indium]]<ref>''The In−Si (Indium-Silicon) system'' R. W. Olesinski, N. Kanani and G. J. Abbaschian Journal of Phase Equilibria Volume 6, Number 2, 128-130, {{DOI|10.1007/BF02869223}}</ref> and thallium<ref>''The Si-Zn (Silicon-Thallium) system'' R. W. Olesinski and G. J. Abbaschian Journal of Phase Equilibria Volume 6, Number 6, 543-544, {{DOI|10.1007/BF02887155}}</ref> behave similarly.\n\n==Group 14==\n[[Silicon carbide]] (SiC) is widely used as a ceramic or example in car brakes and bulletproof vests. It is also used in semiconductor electronics. It is manufactured from [[silicon dioxide]] and carbon in an [[Acheson furnace]] between 1600 and 2500&nbsp;°C. There are 250 known crystalline forms with alpha silicon carbide the most common. Silicon itself is an important semiconductor material used in microchips. It is produced commercially from [[silica]] and [[carbon]] at 1900&nbsp;°C and crystallizes in a diamond cubic crystal structure. [[Germanium silicide]] forms a [[solid solution]] and is again a commercially used semiconductor material.<ref>''The Ge−Si (Germanium-Silicon) system'' R. W. Olesinski and G. J. Abbaschian Journal of Phase Equilibria Volume 5, Number 2, 180-183, {{DOI|10.1007/BF02868957}}</ref> The [[tin]]–silicon phase diagram is a eutectic<ref>''The Si−Sn (Silicon−Tin) system'' R. W. Olesinski and G. J. Abbaschian Journal of Phase Equilibria Volume 5, Number 3, 273-276, {{DOI|10.1007/BF02868552}}</ref> and the [[lead]]–silicon phase diagram shows a [[monotectic]] transition and a small eutectic transition but no solid solubility.<ref>''The Pb−Si (Lead−Silicon) system'' R. W. Olesinski and G. J. Abbaschian Journal of Phase Equilibria Volume 5, Number 3, 271-273, {{DOI|10.1007/BF02868551}}</ref>\n\n==Group 15==\n[[Silicon nitride]] (Si<sub>3</sub>N<sub>4</sub>) is a ceramic with many commercial high-temperature applications such as engine parts. It can be synthesized from the elements at temperatures between 1300 and 1400&nbsp;°C. Three different crystallographic forms exist.  Other binary silicon nitrogen compounds have been proposed (SiN, Si<sub>2</sub>N<sub>3</sub>, Si<sub>3</sub>N)<ref>''The N-Si (Nitrogen-Silicon) system'' O. N. Carlson Journal of Phase Equilibria Volume 11, Number 6, 569-573, {{DOI|10.1007/BF02841719}}</ref> and other SiN compounds have been investigated at cryogenic temperatures (SiN<sub>2</sub>, Si(N<sub>2</sub>)<sub>2</sub>, SiNNSi).<ref>''Reactions of Silicon Atoms with Nitrogen:  A Combined Matrix Spectroscopic and Density Functional Theory Study'' Günther Maier, Hans Peter Reisenauer, and Jörg Glatthaar Organometallics, 2000, 19 (23), pp 4775–4783 {{DOI|10.1021/om000234r}}</ref> [[Silicon tetraazide]] is an unstable compound that easily detonates.\n\nThe phase diagram with [[phosphorus]] shows SiP and SiP<sub>2</sub>.<ref>''The P−Si (Phosphorus-Silicon) system'' R. W. Olesinski, N. Kanani and G. J. Abbaschian Journal of Phase Equilibria Volume 6, Number 2, 130-133, {{DOI|10.1007/BF02869224}}</ref> A reported '''silicon phosphide'''  is Si<sub>12</sub>P<sub>5</sub> (no practical applications),<ref>''A new silicon phosphide, Si12P5: Formation conditions, structure, and properties'' J. R. A. Carlsson, L. D. Madsen, M. P. Johansson, L. Hultman, X.-H. Li,b) and H. T. G. Hentzell, L. R. Wallenberg J. Vac. Sci. Technol. A 15(2), Mar/Apr 1997 {{doi|10.1116/1.580497}}</ref><ref>''Further study on structural and electronic properties of silicon phosphide compounds with 3:4 stoichiometry'' M. Huanga and Y.P. Feng Computational Materials Science\nVolume 30, Issues 3-4, August 2004, Pages 371-375 {{doi|10.1016/j.commatsci.2004.02.031}}</ref> formed by [[annealing (metallurgy)|annealing]] an amorphous Si-P alloy.\n\nThe [[arsenic]]–silicon phase diagram  measured at 40 Bar has two phases: SiAs and SiAs<sub>2</sub>.<ref>''The As−Si (Arsenic-Silicon) system'' R. W. Olesinski and G. J. Abbaschian Journal of Phase Equilibria Volume 6, Number 3, 254-258, {{DOI|10.1007/BF02880410}}</ref> The [[antimony]]–silicon system comprises a single eutectic close to the melting point of Sb.<ref>''The Sb-Si (Antimony-Silicon) system'' R. W. Olesinski and G. J. Abbaschian Journal of Phase Equilibria Volume 6, Number 5, 445-448, {{DOI|10.1007/BF02869508}}</ref> The [[bismuth]] system is a monotectic.<ref>''The Bi−Si (Bismuth-Silicon) system'' R. W. Olesinski and G. J. Abbaschian Journal of Phase Equilibria Volume 6, Number 4, 359-361, {{DOI|10.1007/BF02880522}}</ref>\n\n==Group 16==\nIn group 16 [[silicon dioxide]] is a very common compound that widely occurs as sand or quartz. SiO<sub>2</sub> is tetrahedral with each silicon atom surrounded by 4 oxygen atoms. Numerous crystalline forms exist with the tetrahedra linked to form a polymeric chain. Examples are [[tridymite]] and [[cristobalite]]. A less common oxide is [[silicon monoxide]] that can be found in outer space. Unconfirmed reports exist for nonequilibrium Si<sub>2</sub>O, Si<sub>3</sub>O<sub>2</sub>, Si<sub>3</sub>O<sub>4</sub>, Si<sub>2</sub>O<sub>3</sub> and Si<sub>3</sub>O<sub>5</sub>.<ref>''The O-Si (Oxygen-Silicon) system'' H. A. Wrledt Journal of Phase Equilibria Volume 11, Number 1, 43-61, {{DOI|10.1007/BF02841583}}</ref> [[Silicon sulfide]] is also a chain compound. Cyclic SiS<sub>2</sub> has been reported to exist in the gas phase.<ref>Mück, L. A., Lattanzi, V., Thorwirth, S., McCarthy, M. C. and Gauss, J. (2012), ''Cyclic SiS2: A New Perspective on the Walsh Rules.'' Angew. Chem. Int. Ed., 51: 3695–3698. {{doi|10.1002/anie.201108982}}</ref> The phase diagram of silicon with [[selenium]] has two phases: SiSe<sub>2</sub> and SiSe.<ref>''Se-Si (Selenium-Silicon'') H. Okamoto Journal of Phase Equilibria Volume 21, Number 5, 499, {{DOI|10.1361/105497100770339815}}</ref> Tellurium silicide is a semiconductor with formula TeSi<sub>2</sub> or Te<sub>2</sub>Si<sub>3</sub>.<ref>''A note on the Si-Te phase diagram'' T. G. Davey and E. H. Baker Journal of Materials Science Volume 15, Number 6, 1601-1602, {{DOI|10.1007/BF00752149}}</ref>\n\n==Group 17==\nBinary silicon compounds in group 17 are stable compounds ranging from gaseous  [[silicon fluoride]] (SiF<sub>4</sub>) to the liquids [[silicon chloride]] (SiCl<sub>4</sub> and [[silicon bromide]] SiBr<sub>4</sub>) to the solid [[silicon iodide]] (SiI<sub>4</sub>). The [[molecular geometry]] in these compounds is tetrahedral and the bonding mode covalent. Other known stable fluorides in this group are Si<sub>2</sub>F<sub>6</sub>, Si<sub>3</sub>F<sub>8</sub> (liquid) and polymeric solids  known as [[Polysilicon halides|polysilicon fluorides]] (SiF<sub>2</sub>)<sub>x</sub> and (SiF)<sub>x</sub>. The other halides form similar binary silicon compounds.<ref>''Inorganic chemistry'', Egon Wiberg, Nils Wiberg, Arnold Frederick Holleman 2001</ref>\n\n==The periodic table of the binary silicon compounds==\n\n{| class=\"wikitable\" style=\"margin-left:auto; margin-right:auto; margin-bottom:0; margin-top:0; text-align:top; background-color:white; border:0px\"\n|- <!-- ##### PERIODE 1 ##### -->\n| style=\"background:#99bbff\" | [[Silane|SiH<sub>4</sub>]]\n| colspan=1 style=\"border:none\"|\n| colspan=10 style=\"border:none\" rowspan=3|\n| colspan=5 style=\"border:none\"|\n| style=\"background:white\" |He\n|- <!-- ##### PERIODE 2 ##### -->\n| style=\"background:#F0DC82\" | '''LiSi'''\n| style=\"background:#ddd\" |Be\n| style=\"background:#99bbff\" | [[Silicon boride|SiB<sub>3</sub>]]\n| style=\"background:#99bbff\" | [[Silicon carbide|SiC]]\n| style=\"background:#99bbff\"| [[Silicon nitride|Si<sub>3</sub>N<sub>4</sub>]]\n| style=\"background:#99bbff\" | [[Silicon oxide|SiO<sub>2</sub>]]  \n| style=\"background:#99bbff\" | [[Silicon tetrafluoride|SiF<sub>4</sub>]]  \n| style=\"background:white\" | Ne\n|- <!-- ##### PERIODE 3 ##### -->\n| style=\"background:#F0DC82\" | '''NaSi'''\n| style=\"background:#F0DC82\" | [[Magnesium silicide|Mg<sub>2</sub>Si]] \n| style=\"background:#ddd\" | Al\n| style=\"background:#99bbff\" | [[silicon|Si]]\n| style=\"background:#99bbff\" | '''SiP'''\n| style=\"background:#99bbff\" | [[Silicon sulfide|SiS<sub>2</sub>]]\n| style=\"background:#99bbff\" | [[Tetrachlorosilane|SiCl<sub>4</sub>]]  \n| style=\"background:white\" | Ar\n|- <!-- ##### PERIODE 4 ##### -->\n| style=\"background:#F0DC82\" | '''KSi'''  \n| style=\"background:#F0DC82\" | [[Calcium disilicide|CaSi<sub>2</sub>]]  \n| style=\"background:#98FF98\" | '''ScSi'''\n| style=\"background:#98FF98\" | '''TiSi'''\n| style=\"background:#98FF98\" | '''V<sub>5</sub>Si<sub>3</sub>'''\n| style=\"background:#98FF98\" | '''Cr<sub>5</sub>Si<sub>3</sub>'''\n| style=\"background:#98FF98\" | [[MnSi]]\n| style=\"background:#98FF98\" | '''FeSi'''\n| style=\"background:#98FF98\" | '''CoSi'''\n| style=\"background:#98FF98\" |'''NiSi'''\n| style=\"background:#98FF98\" | [[Copper silicide|Cu<sub>5</sub>Si]]\n| style=\"background:#ddd\" | Zn\n| style=\"background:#ddd\" | Ga\n| style=\"background:#ddd\" | [[Germanium silicide|Si<sub>1−x</sub>Ge<sub>x</sub>]]  \n| style=\"background:#99bbff\" | '''SiAs'''\n| style=\"background:#99bbff\" | '''SiSe<sub>2</sub>'''\n| style=\"background:#99bbff\" | [[Tetrabromosilane|SiBr<sub>4</sub>]]  \n| style=\"background:white\" | Kr\n|- <!-- ##### PERIODE 5 ##### -->\n| style=\"background:#F0DC82\" | '''RbSi''' \n| style=\"background:#F0DC82\" | '''Sr<sub>2</sub>Si'''\n| style=\"background:#98FF98\" | '''YSi'''\n| style=\"background:#98FF98\" | '''ZrSi'''\n| style=\"background:#98FF98\" | '''Nb<sub>5</sub>Si<sub>3</sub>'''\n| style=\"background:#98FF98\" | '''Mo<sub>5</sub>Si<sub>3</sub>'''\n| style=\"background:mistyrose\" |Tc\n| style=\"background:#98FF98\" | '''RuSi'''\n| style=\"background:#98FF98\" | '''RhSi'''\n| style=\"background:#98FF98\" | '''PdSi'''\n| style=\"background:#ddd\" | Ag\n| style=\"background:#ddd\" | Cd\n| style=\"background:#ddd\" |In\n| style=\"background:#ddd\" | Sn\n| style=\"background:#ddd\" | Sb\n| style=\"background:#99bbff\" | '''TeSi<sub>2</sub>'''\n| style=\"background:#99bbff\" | [[silicon tetraiodide|SiI<sub>4</sub>]] \n| style=\"background:white\" | Xe\n|- <!-- ##### PERIODE 6 ##### -->\n| style=\"background:#F0DC82\" | '''CsSi''' \n| style=\"background:#F0DC82\" | '''Ba<sub>2</sub>Si''' \n| style=\"background:mistyrose\" | \n| style=\"background:#98FF98\" | '''HfSi'''\n| style=\"background:#98FF98\" | '''Ta<sub>5</sub>Si<sub>3</sub>'''\n| style=\"background:#98FF98\" | '''W<sub>5</sub>Si<sub>3</sub>'''\n| style=\"background:#98FF98\" |'''ReSi<sub>2</sub>'''\n| style=\"background:#98FF98\" | '''OsSi'''\n| style=\"background:#98FF98\" | '''IrSi'''\n| style=\"background:#98FF98\" | [[Platinum silicide|PtSi]]\n| style=\"background:#ddd\" | Au\n| style=\"background:#ddd\" | Hg\n| style=\"background:#ddd\" | Tl\n| style=\"background:#ddd\" |Pb\n| style=\"background:#ddd\" | Bi\n| style=\"background:mistyrose\" | Po\n| style=\"background:mistyrose\" | At\n| style=\"background:white\" | Rn\n|-<!-- ##### PERIODE 7 ##### -->style=\"background-color:mistyrose\"\n| Fr\n| style=\"background:mistyrose\" | Ra\n|  style=\"background:white\" |\n| Rf\n| Db\n| Sg\n| Bh\n| Hs\n| Mt\n| Ds\n| Rg\n| Cn\n| style=\"background:mistyrose\" | Nh\n| style=\"background:mistyrose\" | Fl\n| style=\"background:mistyrose\" | Mc\n| style=\"background:mistyrose\" | Lv\n| style=\"background:mistyrose\" | Ts\n| style=\"background:white\" | Og\n|-\n| colspan=2 style=\"border:none\"|\n| style=\"border:none\"| ↓\n| colspan=14 style=\"border:none\"|\n|- <!-- #### LANTHANIDES #### -->style=\"background-color:mistyrose\"\n| colspan=2 style=\"border:none; background-color:white\"| \n| style=\"background-color:#98FF98\"|'''LaSi'''\n| style=\"background-color:#98FF98\"|'''CeSi'''\n| style=\"background-color:#98FF98\"|'''PrSi'''\n| style=\"background-color:#98FF98\"|'''NdSi'''\n| style=\"background-color:mistyrose\"|Pm\n| style=\"background-color:#98FF98\"|'''SmSi'''\n| style=\"background-color:#98FF98\"|'''EuSi'''\n| style=\"background-color:#98FF98\"|'''GdSi'''\n| style=\"background-color:#98FF98\"|'''TbSi'''\n| style=\"background-color:#98FF98\"|'''DySi'''\n| style=\"background-color:#98FF98\"|'''HoSi'''\n| style=\"background-color:#98FF98\"|'''ErSi'''\n| style=\"background-color:mistyrose\"|Tm\n| style=\"background-color:#98FF98\"|'''YbSi'''\n| style=\"background-color:#98FF98\"|'''LuSi'''\n|- <!-- #### ACTINIDES #### --> style=\"background-color:mistyrose\"\n| colspan=2 style=\"border:none; background-color:white\"|\n| style=\"background-color:mistyrose\"|Ac\n| style=\"background-color:#98FF98\"|'''ThSi '''\n| style=\"background-color:mistyrose\"|Pa\n| style=\"background-color:#98FF98\"|'''USi '''\n| style=\"background-color:#98FF98\"|'''NpSi'''\n| style=\"background-color:#98FF98\"|'''PuSi'''\n| style=\"background-color:#98FF98\"|'''AmSi'''\n| style=\"background-color:#98FF98\"|'''CmSi'''\n| style=\"background-color:mistyrose\"|Bk\n| style=\"background-color:mistyrose\"|Cf\n| style=\"background-color:mistyrose\"|Es\n| style=\"background-color:mistyrose\"|Fm\n| style=\"background-color:mistyrose\"|Md\n| style=\"background-color:mistyrose\"|No\n| style=\"background-color:mistyrose\"|Lr\n| style=\"border:none; background-color:white\"|\n|}\n{| border=2 cellpadding=4 style=\"margin-left:auto; margin-right:auto; text-align:center; background:silver; border:1px solid gray; border-collapse:collapse; width:50%; font-size:100%;\"\n|-\n|+ '''Binary compounds of [[silicon]]'''\n|-\n| style=\"background:#99bbff\" | Covalent silicon compounds\n| style=\"background:#98FF98\" | metallic silicides.\n\n|-\n| style=\"background:#F0DC82\" | Ionic silicides\n| style=\"background:white\" | Do not exist\n|-\n| style=\"background:#ddd\" | Eutectic / monotectic / solid solution\n| style=\"background:mistyrose\" | Unknown / Not assessed\n|}>\n\n==References==\n{{Reflist|2}}\n\n[[Category:Binary compounds]]\n[[Category:Silicon compounds]]\n{{silicon compounds}}"
    },
    {
      "title": "Bismuth antimonide",
      "url": "https://en.wikipedia.org/wiki/Bismuth_antimonide",
      "text": "{{chembox\n|Section1 = {{Chembox Identifiers\n| ChemSpiderID = 11201349\n| InChI = AEMQIQQWIVNHAU-UHFFFAOYSA-N\n| PubChem = 6914523}}|Section2 = {{Chembox Properties|Formula = BiSb|MolarMass = 330.74 g/mol|Appearance = Faint-grey to dark-grey powder|Density = 8.31 g/cm<sup>3</sup>|SolubleOther = insoluble}}|Section3 = {{Chembox Structure\n|CrystalStruct = [[Hexagonal]], [[Strukturbericht Symbol|A7]], SpaceGroup =  R-3m, No. 166|LattConst_a = 4.546A|LattConst_c = 11.860A|LattConst_ref = <ref name=\":3\"/>\n}}|Section7 = {{Chembox Hazards\n|ExternalSDS = [http://www.sigmaaldrich.com/MSDS/MSDS/DisplayMSDSPage.do?country=US&language=en&productNumber=745804&brand=ALDRICH&PageToGoToURL=http%3A%2F%2Fwww.sigmaaldrich.com%2Fcatalog%2Fproduct%2Faldrich%2F745804%3Flang%3Den]|NFPA-H = 2|NFPA-F = 0|NFPA-R = 0\n}}\n}}\n'''Bismuth antimonides''', '''Bismuth-antimonys''', or '''Bismuth-antimony alloys''', (Bi<sub>1−x</sub>Sb<sub>x</sub>) are binary alloys of [[bismuth]] and [[antimony]] in various ratios.\n\nSome, in particular Bi<sub>0.9</sub>Sb<sub>0.1</sub>, were the first experimentally-observed three-dimensional [[topological insulator]]s, materials that have conducting surface states but have an insulating interior.<ref name=\":0\">{{Cite journal|title = A topological Dirac insulator in a quantum spin Hall phase|url = http://www.nature.com/nature/journal/v452/n7190/full/nature06843.html|journal = Nature|date = 2008-04-24|issn = 0028-0836|pages = 970–974|volume = 452|issue = 7190|doi = 10.1038/nature06843|first = D.|last = Hsieh|first2 = D.|last2 = Qian|first3 = L.|last3 = Wray|first4 = Y.|last4 = Xia|first5 = Y. S.|last5 = Hor|first6 = R. J.|last6 = Cava|first7 = M. Z.|last7 = Hasan|pmid=18432240|arxiv = 0902.1356}}</ref>\n\nVarious BiSb alloys also [[Superconductivity|superconduct]] at low temperatures,<ref name=\":1\">{{Cite journal|title = Fluctuation Heat Capacity in Superconducting Thin Films of Amorphous BiSb |journal = Physical Review Letters|pages = 1710–1712|volume = 27|issue = 25|doi = 10.1103/physrevlett.27.1710|first = G. D.|last = Zally|first2 = J. M.|last2 = Mochel|year = 1971 }}</ref> are [[semiconductor]]s,<ref name=\":3\">{{Cite journal|title = Temperature Dependence of the Electrical Properties of Bismuth-Antimony Alloys|journal = Physical Review|pages = 1518–1528|volume = 114|issue = 6|doi = 10.1103/physrev.114.1518|first = A. L.|last = Jain|year = 1959}}</ref> and are used in [[Thermoelectric effect|thermoelectric]] devices.<ref name=\":2\">{{Cite journal|title = Thermoelectric Properties of Bismuth‐Antimony Alloys|journal = Journal of Applied Physics|date = 1962-03-01|issn = 0021-8979|pages = 841–846|volume = 33|issue = 3|doi = 10.1063/1.1777178|first = G. E.|last = Smith|first2 = R.|last2 = Wolfe}}</ref>\n\n'''Bismuth antimonide''' itself (see box to right) is sometimes described as Bi<sub>2</sub>Sb<sub>2</sub>.<ref>[https://pubchem.ncbi.nlm.nih.gov/compound/6914523 Bismuth Antimonide]</ref>\n\n== Synthesis ==\n\nCrystals of bismuth antimonides are synthesized by melting bismuth and antimony together under inert gas or vacuum. [[Zone melting]] is used to decrease the concentration of impurities.<ref name=\":2\" /> When synthesizing single crystals of bismuth antimonides, it is important that impurities are removed from the samples, as oxidation occurring at the impurities leads to polycrystalline growth.<ref name=\":3\" />\n\n== Properties ==\n\n=== Topological Insulator ===\n\nPure bismuth is a [[semimetal]], containing a small band gap, which leads to it having a relatively high conductivity (7.7*10<sup>5</sup> S/m at 20&nbsp;°C). When the bismuth is doped with antimony, the conduction band decreases in energy and the valence band increases in energy. At an Sb concentration of 4%, the two bands intersect, forming a Dirac point<ref name=\":0\" /> (which is defined as a point where the conduction and valence bands intersect). Further increases in the concentration of antimony result in a band inversion, in which the energy of the valence band becomes greater than that of the conduction band at specific momenta.  Between Sb concentrations of 7 and 22%, the bands no longer intersect, and the Bi<sub>1−x</sub>Sb<sub>x</sub> becomes an inverted-band insulator.<ref>{{Cite journal|title =Phase transition between the quantum spin Hall and insulator phases in 3D: emergence of a topological gapless phase  |doi = 10.1088/1367-2630/9/9/356 | volume=9 |issue =9  |journal=New Journal of Physics |page=356|year =2007  |last1 =Shuichi Murakami  }}</ref> It is at these higher concentrations of Sb that the band gap in the surface states vanishes, and the material thus conducts at its surface.<ref name=\":0\" />\n\n=== Superconductor ===\n\nThe highest temperatures at which Bi<sub>.4</sub>Sb<sub>.6</sub> thin film of thicknesses 150-1350A superconduct, the critical temperature T<sub>c</sub>, is approximately 2K.<ref name=\":1\" /> Single crystal Bi<sub>.935</sub>Sb<sub>.065</sub> can superconduct at slightly higher temperatures, and at 4.2K, its critical magnetic field B<sub>c</sub> (the maximum magnetic field that the superconductor can expel) of 1.6T at 4.2K.<ref>{{Cite journal|title = Anomalous Proximity Effect in the Nb-BiSb-Nb Junctions|journal = Physical Review Letters|pages = 3029–3032|volume = 77|issue = 14|doi = 10.1103/physrevlett.77.3029|first = A. Yu.|last = Kasumov|first2 = O. V.|last2 = Kononenko|first3 = V. N.|last3 = Matveev|first4 = T. B.|last4 = Borsenko|first5 = V. A.|last5 = Tulin|first6 = E. E.|last6 = Vdovin|first7 = I. I.|last7 = Khodos|pmid=10062113|year = 1996}}</ref>\n\n=== Semiconductor ===\n\n[[Electron mobility]] is one important parameter describing semiconductors because it describes the rate at which electrons can travel through the semiconductor. At 40K, electron mobility ranged from 0.49*10<sup>6</sup> cm<sup>2</sup>/Vs at an Sb concentration of 0 to .24*10<sup>6</sup> cm<sup>2</sup>/Vs at a Sb concentration of 7.2%.<ref name=\":3\" /> This is much greater than the electron mobility of other common semiconductors like Si, which is 1400&nbsp;cm<sup>2</sup>/Vs at room temperature.<ref>{{Cite web|title = Electrical properties of Silicon (Si)|url = http://www.ioffe.rssi.ru/SVA/NSM/Semicond/Si/electric.html|website = www.ioffe.rssi.ru|accessdate = 2015-12-11}}</ref>\n\nAnother important parameter of Bi<sub>1−x</sub>Sb<sub>x</sub> is the [[effective electron mass]] (EEM), a measure of the ratio of the acceleration of an electron to the force applied to an electron. The effective electron mass is .002m<sub>e</sub> for x=.11 and .0009m<sub>e</sub> at x=.06.<ref name=\":0\" /> This is much less than the electron effective mass in many common semiconductors (1.09 in Si at 300K, .55 in Ge, and .067 in GaAs). A low EEM is good for [[Thermophotovoltaic]] applications.\n\n=== Thermoelectric ===\n\nBismuth antimonides are used as the n-type legs in many [[thermoelectric]] devices below room temperature. The thermoelectric efficiency, given by its figure of merit zT = σS2T/λ, where S is the [[Seebeck coefficient]], λ is the thermal conductivity, and σ is the electrical conductivity, describes the ratio of the energy provided by the thermoelectric to the heat absorbed by the device. At 80K, the figure of merit (zT) for Bi<sub>1−x</sub>Sb<sub>x</sub> peaks at 6.5*10<sup>−3</sup>/K when x = 15%.<ref name=\":2\" /> Also, The Seebeck coefficient (the ratio of the potential difference between ends of a material to the temperature difference between the sides) at 80K of Bi<sub>.9</sub>Sb<sub>.1</sub> is -140μV/K, much smaller than the Seebeck coefficient of pure bismuth, -50μV/K.<ref>{{Cite journal|title = Bismuth–antimony alloys|journal = Physica Status Solidi A|date = 1970-01-16|issn = 1521-396X|pages = 7–28|volume = 1|issue = 1|doi = 10.1002/pssa.19700010102|first = H. J.|last = Goldsmid}}</ref>\n\n==References==\n{{Reflist|2}}\n\n[[Category:Antimony]]\n[[Category:Binary compounds]]\n[[Category:Bismuth compounds]]\n[[Category:Semiconductors]]"
    },
    {
      "title": "Disodium helide",
      "url": "https://en.wikipedia.org/wiki/Disodium_helide",
      "text": "{{one source|date=February 2018}}\n'''Disodium helide'''{{citation needed|date=February 2018}} (Na<sub>2</sub>He) is a compound of [[helium]] and [[sodium]] that is stable at high pressures above {{convert|113|GPa|bar}}. It was first [[crystal structure prediction|predicted]]<ref>{{cite journal |last1=Saleh |first1=Gabriele |last2=Dong |first2=Xiao |last3=Oganov |first3=Artem |last4=Gatti |first4=Carlo |last5=Qian |first5=Guang-rui |last6=Zhu |first6=Qiang |last7=Zhou |first7=Xiang-Feng |last8=Wang |first8=Hiu-tian |title=Stable Compound of Helium and Sodium at High Pressure |journal=Acta Crystallographica Section A |date=5 August 2014 |volume=70 |issue=a1 |pages=C617–C617 |doi=10.1107/S2053273314093826}}</ref> using USPEX code and then synthesised in 2016.<ref name=dong/>\n\nNa<sub>2</sub>He was predicted to be [[thermodynamically stable]] over 160&nbsp;GPa and dynamically stable over 100&nbsp;GPa. This means it should be possible to form at the higher pressure and then decompress to 100&nbsp;GPa, but below that it would decompose. Compared with other [[binary compound]]s of other elements and helium, it was predicted to be stable at the lowest pressure of any such combination. So that for example a helium-potassium compound is predicted to require much higher pressures of the order of terapascals.\n\nDisodium helide has a [[Cubic crystal system|cubic crystal structure]], resembling [[fluorite]]. At 300&nbsp;GPa the edge of a [[unit cell]] of the crystal has {{nowrap|1=''a'' = 3.95 [[Ångström|Å]]}}. Each unit cell contains four helium atoms on the centre of the cube faces and corners, and eight sodium atoms at coordinates halfway between the center and each corner. Double electrons (2e<sup>−</sup>) are positioned on each edge and the centre of the unit cell.<ref name=note1 group=note/> Each pair of electrons is spin paired. The presence of these isolated electrons makes this an [[electride]]. The helium atoms do not participate in any bonding. However the electron pairs can be considered as an eight-centre two-electron [[chemical bond|bond]].\n\nThe material was synthesised in a [[diamond anvil cell]] at 130&nbsp;GPa heated to 1,500&nbsp;K with a laser.<ref name=dong/> Disodium helide is predicted to be an insulator and transparent.<ref name=dong/> At 200 GPa the sodium atoms have a [[Bader charge analysis|Bader charge]] of +0.599, the helium charge is -0.174 and the two-electron spots are each near −0.511.<ref name=dong/> So this phase could be called disodium helium electride. The solid is an electrical insulator and is predicted to be transparent. Disodium helide melts at a high temperature near 1,500&nbsp;K, much higher than the melting point of sodium. When decompressed, it can keep its form as low as 113&nbsp;GPa.<ref name=dong/> As pressure increases, the sodium is predicted to gain more positive charge, the helium to lose negative charge and the free electron density to increase. Energy is compensated by the relative shrinking of the helium atoms and the space for electrons.<ref>{{cite journal |last1=Wang |first1=Hui-Tian |last2=Boldyrev |first2=Alexander I. |last3=Popov |first3=Ivan A. |last4=Konôpková |first4=Zuzana |last5=Prakapenka |first5=Vitali B. |last6=Zhou |first6=Xiang-Feng |last7=Dronskowski |first7=Richard |last8=Deringer |first8=Volker L. |last9=Gatti |first9=Carlo |last10=Zhu |first10=Qiang |last11=Qian |first11=Guang-Rui |last12=Saleh |first12=Gabriele |last13=Lobanov |first13=Sergey |last14=Stavrou |first14=Elissaios |last15=Goncharov |first15=Alexander F. |last16=Oganov |first16=Artem R. |last17=Dong |first17=Xiao |title=A stable compound of helium and sodium at high pressure - Supplementary Information table 5 |journal=Nature Chemistry |date=May 2017 |volume=9 |issue=5 |pages=440–445 |doi=10.1038/nchem.2716 |url=https://media.nature.com/original/nature-assets/nchem/journal/v9/n5/extref/nchem.2716-s1.pdf |language=en |issn=1755-4349}}</ref>\n\n==References==\n{{reflist|refs=\n\n<ref name=dong>{{cite journal|last1=Dong|first1=Xiao|last2=Oganov|first2=Artem R.|last3=Goncharov|first3=Alexander F.|last4=Stavrou|first4=Elissaios|last5=Lobanov|first5=Sergey|last6=Saleh|first6=Gabriele|last7=Qian|first7=Guang-Rui|last8=Zhu|first8=Qiang|last9=Gatti|first9=Carlo|last10=Deringer|first10=Volker L.|last11=Dronskowski|first11=Richard|last12=Zhou|first12=Xiang-Feng|last13=Prakapenka|first13=Vitali B.|last14=Konôpková|first14=Zuzana|last15=Popov|first15=Ivan A.|last16=Boldyrev|first16=Alexander I.|last17=Wang|first17=Hui-Tian|title=A stable compound of helium and sodium at high pressure|journal=[[Nature Chemistry]]|volume=9|issue=5|pages=440|date=6 February 2017|doi=10.1038/nchem.2716|pmid=28430195|bibcode=2017NatCh...9..440D|arxiv=1309.3827}}</ref>\n\n}}\n\n==Footnotes==\n{{reflist|group=note|refs=\n\n<ref name=note1 group=note>Each face is shared by two cells, each edge is shared by four cells, and each corner is shared by eight cells.</ref>\n\n}}\n\n{{Noble gas compounds}}\n{{sodium compounds}}\n\n[[Category:Sodium compounds]]\n[[Category:Helium compounds]]\n[[Category:Binary compounds]]\n[[Category:Substances discovered in 2010s]]"
    },
    {
      "title": "Hydrogen ditelluride",
      "url": "https://en.wikipedia.org/wiki/Hydrogen_ditelluride",
      "text": "{{Chembox\n| OtherNames = ditellane<br>Dihydrogen ditellanide\n|Section1 ={{Chembox Identifiers\n| CASNo = 55207-82--4\n| CASNo_Ref = <ref name=mac>{{cite book|last1=Macintyre|first1=Jane E.|title=Dictionary of Inorganic Compounds, Supplement 3|date=1995|publisher=CRC Press|isbn=9780412491108|page=287|url=https://books.google.com.au/books?id=YY2N7nOhBoIC&pg=PA287|language=en}}</ref>\n| PubChem = 123292\n| ChEBI = 50478\n| Gmelin = 239518 \n| ChemSpiderID = 109898\n| StdInChI=1S/H2Te2/c1-2/h1-2H\n| StdInChIKey = JVCDLODDVKFSTM-UHFFFAOYSA-N\n| SMILES = [TeH][TeH]\n}}\n|Section2={{Chembox Properties\n|Te=2|H=2\n}}\n}}\n'''Hydrogen ditelluride''' or '''ditellane''' is an unstable [[Hydrogen chalcogenide#Dihydrogen dichalcogenides|hydrogen dichalcogenide]] containing two [[tellurium]] atoms per molecule, with structure HTeTeH. Hydrogen ditelluride is interesting to theorists because its molecule is simple yet asymmetric (with no [[centre of symmetry]]) and is predicted to be one of the easiest to detect [[parity violation]], in which the left handed molecule has differing properties to the right handed one.\n\n==Production==\nHydrogen ditelluride can possibly be formed at the tellurium cathode in electrolysis in acid.<ref>{{cite journal|last1=Awad|first1=S. A.|title=POISONING EFFECT OF TELLURIDE IONS ON HYDROGEN EVOLUTION AND CATHODIC FORMATION OF HYDROGEN DITELLURIDE|journal=The Journal of Physical Chemistry|date=May 1962|volume=66|issue=5|pages=890–894|doi=10.1021/j100811a031}}</ref> When electrolysed in alkaline solutions, a tellurium cathode produces ditelluride Te<sub>2</sub><sup>2−</sup> ions, as well as Te<sup>2−</sup> and a red polytelluride. The greatest amount of ditelluride is made when pH is over 12.<ref>{{cite journal|last1=Alekperov|first1=A I|title=Electrochemistry of Selenium and Tellurium|journal=Russian Chemical Reviews|date=30 April 1974|volume=43|issue=4|pages=235–250|doi=10.1070/RC1974v043n04ABEH001803}}</ref>\n\nApart from its speculative detection in electrolysis, ditellane has been detected in the gas phase produced from di-''sec''-butylditellane.<ref name=mac>{{cite book|last1=Macintyre|first1=Jane E.|title=Dictionary of Inorganic Compounds, Supplement 3|date=1995|publisher=CRC Press|isbn=9780412491108|page=287|url=https://books.google.com.au/books?id=YY2N7nOhBoIC&pg=PA287|language=en}}</ref><ref>{{cite journal|last1=Hop|first1=Cornelis E. C. A.|last2=Medina|first2=Marco A.|title=H2Te2 Is Stable in the Gas Phase|journal=Journal of the American Chemical Society|date=April 1994|volume=116|issue=7|pages=3163–3164|doi=10.1021/ja00086a072}}</ref>\n\n==Properties==\nHydrogen ditelluride has been investigated theoretically, with various properties predicted. The molecule is twisted with a [[C2 symmetry|''C''<sub>2</sub> symmetry]]. There are two [[enantiomer]]s. Hydrogen ditelluride is equal simplest in its unsymmetrical molecule, any simpler molecule will not have the required low symmetry. The equilibrium geometry (not counting zero point energy or vibrational energy) has the molecule with 2.879&nbsp;Å between the tellurium atoms, and 1.678&nbsp;Å between hydrogen and tellurium. The H-Te-Te angle is 94.93°. The angle of lowest energy between the two H-Te bonds (dihedral angle) is 89.32°. The ''trans'' cofiguration is higher in energy (3.71 kcal/mol), and the ''cis'' would be even higher (4.69 kcal/mol).<ref>{{cite journal|last1=BelBruno|first1=Joseph J.|title=Ab Initio Calculations of the Rotational Barriers in H<sub>2</sub>Te<sub>2</sub> and (CH<sub>3</sub>)<sub>2</sub>Te<sub>2</sub>|journal=Heteroatom Chemistry|date=1997|volume=8|issue=3|pages=199–202|doi=10.1002/(SICI)1098-1071(1997)8:3<199::AID-HC1>3.0.CO;2-8}}</ref> \n\nBeing chiral the molecule is predicted show evidence of [[parity violation]], however this may get interference from [[stereomutation tunneling]], where the P enantiomer and M enantiomer spontaneously convert into each other by quantum tunneling. The parity violation effect on energy comes about from virual [[Z boson]] exchanges between the nucleus and electrons.<ref>{{cite book|last1=Senami|first1=Masato|last2=Inada|first2=Ken|last3=Soga|first3=Kota|last4=Fukuda|first4=Masahiro|last5=Tachibana|first5=Akitomo|title=Concepts, Methods and Applications of Quantum Systems in Chemistry and Physics|date=2018|publisher=Springer, Cham|isbn=9783319745817|pages=95–106|language=en|chapter=Difference of Chirality of the Electron Between Enantiomers of H$$_$$X$$_$$|doi=10.1007/978-3-319-74582-4_6}}</ref> It is proportional to the cube of the atomic number, so is stronger in tellurium molecules than others higher up in the periodic table (eg O, S, Se). Because of parity violation, the energy of the two enantiomers differs, and is likely to be higher in this molecule than most moleculess. So an effort is underway to observe this so-far undetected effect. The tunneling effect is reduced by higher masses, so that the deuterium form, D<sub>2</sub>Te<sub>2</sub> will show less tunneling. In a tortional vibrational mode the molecule can twist back and forward storing energy. Seven different quantum vibration levels are predicted below the energy to jump to the other enantiomer. The levels are numbered ''v''<sub>t</sub>=0 up to 6. The sixth level is predicted to be split into two energy levels because of quantum tunneling.<ref name=gott>{{cite journal|last1=Gottselig|first1=Michael|last2=Quack|first2=Martin|last3=Stohner|first3=Jürgen|last4=Willeke|first4=Martin|title=Mode-selective stereomutation tunneling and parity violation in HOClH+ and H2Te2 isotopomers|journal=International Journal of Mass Spectrometry|date=April 2004|volume=233|issue=1–3|pages=373–384|doi=10.1016/j.ijms.2004.01.014}}</ref> The parity violation energy is calculated as 3×10<sup>−9</sup>&nbsp;cm<sup>−1</sup> or 90 Hz.<ref name=gott/>\n\nThe different vibrational modes for H<sub>2</sub>Te are symmetrical stretch of H-Te, symmetrical bend of ∠H-Te-Te, torsion, stretch Te-Te, asymmetrical stretch H-Te, asymmetrical bend of ∠H-Te-Te.<ref name=gott/> The time to tunnel between enantiomers is only 0.6&nbsp;ms for H<sub>2</sub>Te<sub>2</sub>, but is 66000 seconds for T<sub>2</sub>Te<sub>2</sub>.<ref name=gott/>\n\n==Related==\nThere are organic derivatives, in which the hydrogen is replaced by organic groups. One example is bis-(2,4,6-tributylphenyl)ditellane.<ref>{{cite journal|last1=Lickiss|first1=P. D.|title=Chapter 9. Organometallic chemistry. Part (ii) Main-group elements|journal=Annual Reports Section \"B\" (Organic Chemistry)|date=1988|volume=85|page=263|doi=10.1039/OC9888500241}}</ref> Others are diphenylditellane and 1,2-bis(cyclohexylmethyl)ditellane.\n\n==References==\n{{Reflist}}\n\n{{Hydrides by group}}\n[[Category:Tellurium compounds]]\n[[Category:Binary compounds]]\n[[Category:Hydrogen compounds]]"
    },
    {
      "title": "Indium chalcogenides",
      "url": "https://en.wikipedia.org/wiki/Indium_chalcogenides",
      "text": "The '''indium chalcogenides''' include all [[chemical compound|compounds]] of [[indium]] with the [[chalcogen]] elements, [[oxygen]], [[sulfur]], [[selenium]] and [[tellurium]]. ([[Polonium]] is excluded as little is known about its compounds with indium). The best-characterised compounds are the In(III) and In(II) chalcogenides e.g. the sulfides [[indium(III) sulfide|In<sub>2</sub>S<sub>3</sub>]] and [[indium(II) sulfide|InS]].<br/>\nThis group of compounds has attracted a lot of research attention because they include [[semiconductor]]s, [[photovoltaics]] and [[phase-change materials]]. In many applications indium chalcogenides are used as the basis of ternary and quaternary compounds such as [[indium tin oxide]], ITO and [[copper indium gallium selenide]], CIGS.\n\nSome compounds that were reported and have found their way into text books have not been substantiated by later researchers. The list of compounds below shows compounds that have been reported, and those compounds that have not had their structure determined, or whose existence has not been confirmed by the latest structural investigations, are in italics.\n\n{| class=\"wikitable\"\n|-\n! oxide\n! sulfide\n! selenide\n! telluride\n|-\n|''In<sub>2</sub>O''\n|\n|''In<sub>2</sub>Se''\n|\n|-\n|\n|''In<sub>4</sub>S<sub>3</sub>''\n|'''In<sub>4</sub>Se<sub>3</sub>'''\n|'''In<sub>4</sub>Te<sub>3</sub>'''\n|-\n|\n| ''In<sub>5</sub>S<sub>4</sub>''\n|\n|\n|-\n|\n| '''InS'''\n| '''InSe'''\n| '''InTe'''\n|-\n|\n| '''In<sub>6</sub>S<sub>7</sub>''' \n| '''In<sub>6</sub>Se<sub>7</sub>'''\n|\n|-\n|\n| ''In<sub>3</sub>S<sub>4</sub>''\n|\n| '''In<sub>3</sub>Te<sub>4</sub>'''\n|- \n|\n|\n|\n|'''In<sub>7</sub>Te<sub>10</sub>''' \n|-\n|'''In<sub>2</sub>O<sub>3</sub>'''\n|'''In<sub>2</sub>S<sub>3</sub>'''\n|'''In<sub>2</sub>Se<sub>3</sub>'''\n|'''In<sub>2</sub>Te<sub>3</sub>'''\n|-\n|\n|\n|\n|''In<sub>3</sub>Te<sub>5</sub>''\n|-\n|\n|\n|\n| '''In<sub>2</sub>Te<sub>5</sub>'''\n|-\n|}\n\nThere are a lot of compounds, the reason for this being that indium can be present as \n*In<sup>3+</sup>, [[oxidation state]] +3\n*In<sup>+</sup>, [[oxidation state]] +1\n*In<sub>2</sub><sup>4+</sup> units, [[oxidation state]] of +2, also found in some [[indium halides]], e.g. In<sub>2</sub>Br<sub>3</sub>.\n*non linear In<sub>3</sub><sup>5+</sup> units [[isoelectronicity|isoelectronic]] with Hg<sub>3</sub><sup>2+</sup>.\nThe compound In<sub>2</sub>Te<sub>5</sub> is a [[polytelluride]] containing the Te<sub>3</sub><sup>2−</sup> unit.<br />\nNone of the indium chalcogenides can be described simply as ionic in nature, they all involve a degree of covalent bonding. However, in spite of this it is useful to formulate the compounds in ionic terms to get an insight into how the structures are built up. Compounds almost invariably have multiple polymorphs, that is they can crystallise in slightly different forms depending on either the method of production, or the substrate upon which they are deposited. Many of the compounds are made up of layers, and it is the different ways that the layers are stacked that is a cause of polymorphism.\n\n==In<sub>2</sub>O, In<sub>2</sub>Se==\n\n:In<sub>2</sub>O is well documented. It exists in the gaseous phase and there are numerous reports of small amounts detected in the solid phase but no definitive structure has been published. It is now believed that the compound described as In<sub>2</sub>Se was actually a sample of In<sub>4</sub>Se<sub>3</sub>.<ref>{{Cite journal | doi = 10.1107/S0567740873005108| title = The crystal structure of tetraindium triselenide| journal = Acta Crystallographica Section B| volume = 29| issue = 8| pages = 1590| year = 1973| last1 = Hogg | first1 = J. H. C.| last2 = Sutherland | first2 = H. H.| last3 = Williams | first3 = D. J.}}</ref>\n\n==In<sub>4</sub>S<sub>3</sub>, In<sub>4</sub>Se<sub>3</sub>, In<sub>4</sub>Te<sub>3</sub>==\n\n:In<sub>4</sub>S<sub>3</sub> had been reported but has more recently been re-investigated and is now believed not to exist. Both In<sub>4</sub>Se<sub>3</sub> and In<sub>4</sub>Te<sub>3</sub> are similar black crystalline solids  and have been formulated to contain a non linear In<sub>3</sub><sup>5+</sup> unit that is isoelectronic with Hg<sub>3</sub><sup>2+</sup>. For example the selenide  is formulated as In<sup>+</sup> In<sub>3</sub><sup>5+</sup> 3Se<sup>2−</sup>.<ref>{{Cite journal | doi = 10.1524/zkri.1995.210.5.342| title = In<sub>4</sub>Te<sub>3</sub> und In<sub>4</sub>Se<sub>3</sub>: Neubestimmung der Kristallstrukturen, druckabhängiges Verhalten und eine Bemerkung zur Nichtexistenz von In<sub>4</sub>S<sub>3</sub>| journal = Zeitschrift für Kristallographie| volume = 210| issue = 5| pages = 342| year = 1995| last1 = Schwarz | first1 = U.| last2 = Hillebrecht | first2 = H.| last3 = Deiseroth | first3 = H. J.| last4 = Walther | first4 = R.| bibcode = 1995ZK....210..342S}}</ref>\n\n==In<sub>5</sub>S<sub>4</sub>==\n\n:A reinvestigation showed that the original sample was actually SnIn<sub>4</sub>S<sub>4</sub>.<ref>{{Cite journal | doi = 10.1524/zkri.1991.196.14.197| title = In<sub>5</sub>S<sub>4</sub> = SnIn<sub>4</sub>S<sub>4</sub> : Eine Korrektur!| journal = Zeitschrift für Kristallographie - Crystalline Materials| volume = 196| year = 1991| last1 = Pfeifer | first1 = H.| last2 = Deiseroth | first2 = H. J.}}</ref>\n\n==InS, InSe, InTe==\n\n;InS, InSe\n:InS and InSe are similar, both contain In<sub>2</sub><sup>4+</sup> and have a layer structure. InS for instance can be formulated In<sub>2</sub><sup>4+</sup> 2S<sup>2−</sup>. InSe has two crystal forms β-InSe and γ-InSe that differ only in the way that the layers are stacked. InSe is a [[semiconductor]] and a phase change material and has potential as an optical recording medium.<ref>{{Cite journal | doi = 10.1063/1.1856690| title = Phase-change recording medium that enables ultrahigh-density electron-beam data storage| journal = Applied Physics Letters| volume = 86| issue = 5| pages = 051902| year = 2005| last1 = Gibson | first1 = G. A.| last2 = Chaiken | first2 = A.| last3 = Nauka | first3 = K.| last4 = Yang | first4 = C. C.| last5 = Davidson | first5 = R.| last6 = Holden | first6 = A.| last7 = Bicknell | first7 = R.| last8 = Yeh | first8 = B. S. | last9 = Chen | first9 = J.| last10 = Liao | first10 = H.| last11 = Subramanian | first11 = S.| last12 = Schut | first12 = D.| last13 = Jasinski | first13 = J.| last14 = Liliental-Weber | first14 = Z.| bibcode = 2005ApPhL..86e1902G}}</ref>\n;InTe\n:InTe in contrast to InS and InSe is a mixed valence indium compound containing In<sup>+</sup> and In<sup>3+</sup> and can be formulated as In<sup>+</sup> In<sup>3+</sup> 2Te<sup>2−</sup>. It is similar to TlSe and has tetrahedral InTe<sub>4</sub> units that share edges. It has potential for use in photovoltaic devices.<ref>{{cite journal|url=http://www.redalyc.org/articulo.oa?id=94201318|title=Grown of InTe films by close spaced vapor transport|author= Zapata-Torres, M. |journal=Superficies y Vacío |volume=13|pages= 69–71|year=2001}}</ref>\n\n==In<sub>6</sub>S<sub>7</sub>, In<sub>6</sub>Se<sub>7</sub>==\n\n:These compounds are isostructural, and have been formulated with indium in 3 different oxidation states, +1, +2 and +3. They have been formulated as e.g. In<sup>+</sup> In<sub>2</sub><sup>4+</sup>   3In<sup>3+</sup> 7S<sup>2−</sup>.   The indium – indium bond length in the In<sub>2</sub> units are 2.741 A (sulfide), 2.760 (selenide).\n<ref>{{Cite journal | doi = 10.1107/S056774087100445X| title = The crystal structure of In<sub>6</sub>Se<sub>7</sub>| journal = Acta Crystallographica Section B| volume = 27| issue = 8| pages = 1630| year = 1971| last1 = Hogg | first1 = J. H. C.| url = http://journals.iucr.org/b/issues/1971/08/00/a08284/a08284.pdf}}</ref><ref>{{Cite journal | doi = 10.1107/S0365110X6700221X| title = The crystal structure of In<sub>6</sub>S<sub>7</sub>| journal = Acta Crystallographica| volume = 23| pages = 111| year = 1967| last1 = Hogg | first1 = J. H. C.| last2 = Duffin | first2 = W. J.}}</ref> In<sub>6</sub>S<sub>7</sub> is an n-type semiconductor.<ref>{{Cite journal | doi = 10.1002/crat.2170320517| title = On the conduction mechanism and thermoelectric phenomena in In<sub>6</sub>S<sub>7</sub> layer crystals| journal = Crystal Research and Technology| volume = 32| issue = 5| pages = 723| year = 1997| last1 = Gamal | first1 = G. A.}}</ref>\n\n==In<sub>3</sub>Te<sub>4</sub>==\n\n:This compound has been reported as a superconductor.<ref>{{Cite journal | doi = 10.1103/PhysRevLett.13.127|bibcode=1964PhRvL..13..127G| title = Superconductivity of Intermetallic Compounds with NaCl-Type and Related Structures| journal = Physical Review Letters| volume = 13| issue = 4| pages = 127| year = 1964| last1 = Geller | first1 = S.| last2 = Hull | first2 = G.}}</ref> An unusual structure has been proposed <ref>{{Cite journal | doi = 10.1107/S0567739478000224|bibcode=1978AcCrA..34..123K| title = The ordered state of In<sub>3</sub>Te<sub>4</sub>| journal = Acta Crystallographica Section A| volume = 34| pages = 123| year = 1978| last1 = Karakostas | first1 = T.| last2 = Flevaris | first2 = N. F.| last3 = Vlachavas | first3 = N.| last4 = Bleris | first4 = G. L.| last5 = Economou | first5 = N. A.}}</ref> that is effectively In<sub>4</sub>Te<sub>4</sub>  but with one quarter of the indium positions vacant. There seems to be no short indium-indium distance that would indicate an In-In unit.\n\n==In<sub>7</sub>Te<sub>10</sub>==\n\n:This is formulated as In<sub>2</sub><sup>4+</sup> 12In<sup>3+</sup> 20Te<sup>2−</sup>. The In-In distance is 276.3pm. It has a similar structure to Ga<sub>7</sub>Te<sub>10</sub> and Al<sub>7</sub>Te<sub>10</sub>\n<ref>{{Cite journal | doi = 10.1524/zkri.1995.210.1.57| title = Crystal structures of heptagallium decatelluride, Ga<sub>7</sub>Te<sub>10</sub> and heptaindium decatelluride, In<sub>7</sub>Te<sub>10</sub>| journal = Zeitschrift für Kristallographie| volume = 210| pages = 57| year = 1995| last1 = Deiseroth | first1 = H. J.| last2 = Müller | first2 = H. -D. | bibcode = 1995ZK....210...57D}}</ref>\n\n==In<sub>2</sub>S<sub>3</sub>, In<sub>2</sub>Se<sub>3</sub>, In<sub>2</sub>Te<sub>3</sub>==\n\n;In<sub>2</sub>S<sub>3</sub>\n:[[Indium(III) sulfide]] is a yellow or red high melting solid. It is an [[n-type semiconductor]].\n;In<sub>2</sub>Se<sub>3</sub>\n:[[Indium(III) selenide]] is a black compound with potential optical applications.\n\n;In<sub>2</sub>Te<sub>3</sub>\n:[[Indium(III) telluride]] is a black high melting solid with applications as a semiconductor and in optical material. It has two crystalline forms, α- and β-.\n\n==In<sub>3</sub>Te<sub>5</sub>==\n\n:This was reported  in phase studies in 1964 but its structure has not been confirmed.\n\n==In<sub>2</sub>Te<sub>5</sub>==\n\n:This is a [[polytelluride]] compound and the structure is made up of layers that in turn are made up of chains of linked  InTe<sub>4</sub> tetrahedra where three of the Te atoms are bridging.  There are Te atoms separate from the chains. The compound has been formulated as (2In<sup>3+</sup> Te<sup>2−</sup>Te<sub>3</sub><sup>2−</sup>)<sub>n</sub> counterbalanced with separate Te<sup>2−</sup> ions. The structure is similar to Al<sub>2</sub>Te<sub>5</sub>.<ref>{{Cite journal | doi = 10.1002/zaac.19966220611| title = Die Pentatelluride M<sub>2</sub>Te<sub>5</sub> (M=Al, Ga, In) Polymorphie, Strukturbeziehungen und Homogenitätsbereiche| journal = Zeitschrift für anorganische und allgemeine Chemie| volume = 622| issue = 6| pages = 985| year = 1996| last1 = Deiseroth | first1 = H. J.| last2 = Amann | first2 = P.| last3 = Thurn | first3 = H.}}</ref>\n\n== References==\n{{reflist|30em}}\n\n== Further reading ==\n* [http://www.webelements.com/ WebElements]\n*{{Greenwood&Earnshaw}}\n\n{{DEFAULTSORT:Indium Chalcogenides}}\n[[Category:Indium compounds]]\n[[Category:Chalcogenides]]\n[[Category:Binary compounds|I]]"
    },
    {
      "title": "Iron hydride",
      "url": "https://en.wikipedia.org/wiki/Iron_hydride",
      "text": "{{Use dmy dates|date=June 2013}}\n[[File:FeH Molecule.png|right|thumb|120px|Ball-and-stick diagram of the iron(I) hydride (FeH) free molecule.]]\n\nAn '''iron hydride''' is a chemical system which contains [[iron]] and [[hydrogen]] in some associated form.<ref>{{Greenwood&Earnshaw2nd}}</ref><ref name=Badding/>\n\nBecause of the common occurrence of those two [[element (chemistry)|elements]] in the universe, possible compounds of hydrogen and iron have attracted attention.  A few molecular compounds have been detected in extreme environments (such as [[stellar atmosphere]]s) or detected in small amounts at very low temperatures. The two elements form a metallic [[alloy]] above 35000 atmospheres of pressure, that has been advanced as a possible explanation for the low density of [[inner core|Earth's \"iron\" core]].<ref name=Badding/><ref name=Saxena/> However those compounds are unstable when brought to ambient conditions, and eventually decompose into the separate elements.\n\nSmall amounts of hydrogen (up to about 0.08% by weight) are absorbed into iron as it solidifies from molten state.<ref name=Mikhay/> Although the H<sub>2</sub> is simply an impurity, its presence can affect the mechanical properties of the material.\n\nDespite the fleeting nature of binary iron hydrides, there are many fairly stable [[hydride complex|complexes]] containing iron-hydrogen bonds (and other elements).<ref name=Nakazawa/><ref name=\"Hieber31\"/>\n\n== Overview ==\n\n=== Solid solutions ===\n\nIron and iron-based alloys can form [[iron–hydrogen alloy|solid solutions with hydrogen]], which under extreme pressure may reach stoichiometric proportions, remaining stable even at high temperatures and that is reported to survive for a while under ambient pressure, at temperatures below 150K.<ref name=Antonov/>\n\n=== Binary compounds ===\n\n==== Molecular compounds ====\n\n* [[iron(I) hydride#hydridoiron|Hydridoiron]] (FeH). This molecule has been detected in the atmosphere of the [[Sun]] and some [[red dwarf]] stars. It is stable only as a gas, above the boiling point of iron, or as traces in frozen [[noble gas]]es below 30 [[kelvin|K]] (where it may form complexes with molecular hydrogen, such as FeH·{{chem|H|2}}).<ref name=\"Wang2009\"/>\n* [[iron(II) hydride#dihydridoiron|Dihydridoiron]] ({{chem|FeH|2}}). This compound has been obtained only in very rarefied gases or trapped in frozen gases below 30 [[kelvin|K]], and decomposes into the elements on warming.<ref name=\"Körsgen1996\"/><ref name=Chertihin/> It may form a dimer {{chem|Fe|2|H|4}} and complexes with molecular hydrogen, such as FeH<sub>2</sub>(H<sub>2</sub>)<sub>2</sub> and FeH<sub>2</sub>(H<sub>2</sub>)<sub>3</sub>.<ref name=\"Wang2009\"/><ref name=Andrews2004/>\n* What was once believed to be trihydridoiron ({{chem|FeH|3}}) was later shown to be FeH bound to molecular hydrogen H<sub>2</sub>.<ref name=Andrews2004/>\n\n==== Polymeric network compounds ====\n\n* [[Iron(I) hydride]]. It is stable at pressures exceeding 3.5&nbsp;GPa.\n* [[Iron(II) hydride]] or ferrous hydride. It is stable at pressures between 45 and 75&nbsp;GPa.\n* Iron(III) hydride or ferric hydride. It is stable at pressures exceeding 65&nbsp;GPa.\n* [[Iron pentahydride]] FeH<sub>5</sub> is a [[polyhydride]], where there is more hydrogen than expected by valence rules. It is stable under pressures over 85&nbsp;Gpa. It contains alternating sheets of FeH<sub>3</sub> and atomic hydrogen.<ref>{{cite journal|last1=Pépin|first1=C. M.|last2=Geneste|first2=G.|last3=Dewaele|first3=A.|last4=Mezouar|first4=M.|last5=Loubeyre|first5=P.|title=Synthesis of FeH5 : A layered structure with atomic hydrogen slabs|journal=Science|date=27 July 2017|volume=357|issue=6349|pages=382–385|doi=10.1126/science.aan0961|pmid=28751605|bibcode=2017Sci...357..382P}}</ref>\n\n=== Iron-hydrogen complexes ===\n\nComplexes displaying iron–hydrogen bonds include, for example:\n* [[iron tetracarbonyl hydride]] FeH<sub>2</sub>(CO)<sub>4</sub>, the first such compound to be synthesised (1931).<ref name=\"Hieber31\"/>\n* FeH<sub>2</sub>(CO)<sub>2</sub>[P(OPh)<sub>3</sub>]<sub>2</sub>.\n* Salts of the {{chem|FeH|6|2-}} anion, such as [[magnesium iron hexahydride]], {{chem|MgFeH|6}}, produced by treating mixtures of magnesium and iron powders with high pressures of H<sub>2</sub>.\n* Di- and polyiron hydrides, e.g. [HFe<sub>2</sub>(CO)<sub>8</sub>]<sup>−</sup> and the cluster [HFe<sub>3</sub>(CO)<sub>11</sub>]<sup>−</sup>.\n\nComplexes are also known with molecular hydrogen ({{chem|H|2}}) ligands.\n\n==Biological occurrence==\n[[Methanogens]], [[archaea]], bacteria and some [[cell (biology)|unicellular]] [[eukaryote]]s contain [[hydrogenase]] enzymes that [[catalysis|catalyse]] [[metabolism|metabolic]] reactions involving free hydrogen, whose active site is an iron atom with Fe–H bonds as well as other [[ligand]]s.<ref name=Font/>\n\n==See also==\n* [[Iron–hydrogen alloy]]\n\n== References ==\n<references>\n\n<ref name=Badding>\nJ.V. Badding, R.J. Hemley, and H.K. Mao (1991), \"High-pressure chemistry of hydrogen in metals: in situ study of iron hydride.\"  ''Science'', American Association for the Advancement of Science, volume 253, issue 5018, pages 421–424 {{doi|10.1126/science.253.5018.421}}</ref>\n\n<ref name=Nakazawa>\nHiroshi Nakazawa, Masumi Itazaki \"Fe–H Complexes in Catalysis\" Topics in Organometallic Chemistry (2011) 33: 27–81.\n{{DOI|10.1007/978-3-642-14670-1_2}}\n</ref>\n\n<ref name=\"Hieber31\">\n{{cite journal|last=Hieber|first=W.|author2=F. Leutert|year=1931|journal=Naturwissenschaften|volume=19|issue=17|pages=360–361|doi=10.1007/BF01522286|title=Zur Kenntnis des koordinativ gebundenen Kohlenoxyds: Bildung von Eisencarbonylwasserstoff|bibcode=1931NW.....19..360H}}\n</ref>\n\n<ref name=\"Wang2009\">{{cite journal | last1 = Wang | first1 = Xuefeng | last2 = Andrews | first2 = Lester | year = 2009 | title = Infrared Spectra and Theoretical Calculations for Fe, Ru, and Os Metal Hydrides and Dihydrogen Complexes | url = | journal = The Journal of Physical Chemistry A | volume = 113 | issue = 3| pages = 551–563 | doi = 10.1021/jp806845h | bibcode = 2009JPCA..113..551W }}</ref>\n\n<ref name=\"Körsgen1996\">\nHelga Körsgen, Petra Mürtz, Klaus Lipus, Wolfgang Urban, Jonathan P. Towle, John M. Brown (1996), \"The identification of the {{chem|FeH|2}} radical in the gas phase by infrared spectroscopy\". ''The Journal of Chemical Physics'' volume 104(12) page 4859  {{doi|10.1063/1.471180}}\n</ref>\n\n<ref name=Chertihin>George V. Chertihin and Lester Andrews (1995), \"Infrared spectra of FeH, {{chem|FeH|2}}, and {{chem|FeH|3}} in solid argon\" ''Journal of Physical Chemistry'' volume 99, issue 32, pages 12131–12134 {{doi|10.1021/j100032a013}}\n</ref>\n\n<ref name=Font>{{cite journal | last1 = Fontecilla-Camps | first1 = J. C. | last2 = Amara | first2 = P. | last3 = Cavazza | first3 = C. | last4 = Nicolet | first4 = Y. | last5 = Volbeda | first5 = A. | year = 2009 | title = Structure-function relationships of anaerobic gas-processing metalloenzymes | url = | journal = Nature | volume = 460 | issue = 7257| pages = 814–822 | doi = 10.1038/nature08299 | pmid=19675641| bibcode = 2009Natur.460..814F }}</ref>\n\n<ref name=Mikhay>\nA. S. Mikhaylushkin, N. V. Skorodumova, R. Ahuja, B. Johansson (2006), [http://proceedings.aip.org/resource/2/apcpcs/837/1/161_1 \"Structural and magnetic properties of FeH<sub>x</sub> (x=0.25; 0.50;0.75)\"]. In: ''Hydrogen in Matter: A Collection from the Papers Presented at the Second International Symposium on Hydrogen in Matter (ISOHIM)'', AIP Conference Proceedings, volume 837, pages 161–167  {{doi|10.1063/1.2213072}}\n</ref>\n\n<ref name=Saxena>{{cite journal | last1 = Saxena | first1 = Surendra K. | last2 = Liermann | first2 = Hanns-Peter | last3 = Shen | first3 = Guoyin | year = 2004 | title = Formation of iron hydride and high-magnetite at high pressure and temperature | url = | journal = Physics of the Earth and Planetary Interiors | volume = 146 | issue = 1–2| pages = 313–317 | doi = 10.1016/j.pepi.2003.07.030 | bibcode = 2004PEPI..146..313S }}</ref>\n\n<ref name=Antonov>{{cite journal | last1 = Antonov | first1 = V. E. | last2 = Cornell | first2 = K. | last3 = Fedotov | first3 = V.K. | last4 = Ponyatovsky | first4 = A. I. Kolesnikov E.G. | last5 = Shiryaev | first5 = V.I. | last6 = Wipf | first6 = H. | year = 1998 | title = Neutron diffraction investigation of the dhcp and hcp iron hydrides and deuterides | url = http://issp3.issp.ac.ru/lhpp/PapersAntonov/114.pdf | format = PDF | journal = Journal of Alloys and Compounds | volume = 264 | issue = 1–2| pages = 214–222 | doi = 10.1016/S0925-8388(97)00298-3 }}</ref>\n\n<ref name=Andrews2004>{{cite journal | last1 = Andrews | first1 = Lester | year = 2004 | title = Matrix infrared spectra and density functional calculations of transition metal hydrides and dihydrogen complexes | url = | journal = Chemical Society Reviews | volume = 33 | issue = 2| pages = 123–132 | doi = 10.1039/B210547K | pmid=14767507}}</ref>\n\n</references>\n\n{{Set index article}}\n\n[[Category:Metal hydrides]]\n[[Category:Ferrous alloys]]\n[[Category:Binary compounds|I]]"
    },
    {
      "title": "ANFO",
      "url": "https://en.wikipedia.org/wiki/ANFO",
      "text": "{{More citations needed|date=November 2011}}\n{{Use dmy dates|date=April 2012}}\n[[File:Ammonium nitrate-fuel oil (ANFO) explosive.jpg|thumb|300px|Ammonium nitrate prills used in ANFO at a [[potash|potash mine]].]]\n[[File:Anfoa.jpg|thumb|300px|{{convert|25|kg|abbr=on}} sacks containing ANFO]]\n\n'''ANFO''' (or '''AN/FO''', for '''ammonium nitrate/fuel oil''') is a widely used bulk industrial [[explosive]]. Its name is commonly pronounced as \"an-fo\"<ref>{{Cite web|url=https://www.britannica.com/technology/ANFO|title=ANFO {{!}} explosive|website=Encyclopedia Britannica|language=en|access-date=2019-05-04}}</ref>.\n\nIt consists of 94% porous [[prill]]ed [[ammonium nitrate]] (NH<sub>4</sub>NO<sub>3</sub>) (AN), which acts as the oxidizing agent and absorbent for the fuel, and 6% number 2 [[fuel oil]] (FO).<ref>{{cite book |last=Cook |first=Melvin A. |title=The Science of Industrial Explosives |publisher=IRECO Chemicals |year=1974 |page=1 |asin=B0000EGDJT}}</ref>\n\nANFO has found wide use in [[coal mining]], [[quarrying]], metal [[mining]], and civil construction in applications where its low cost and ease of use may outweigh the benefits of other explosives, such as water resistance, oxygen balance, higher detonation velocity, or performance in small-diameter columns. ANFO is also widely used in avalanche hazard mitigation.<ref>{{cite book |last=Cook |first=Melvin A. |title=The Science of Industrial Explosives |publisher=IRECO Chemicals |year=1974 |page=2 |asin=B0000EGDJT}}</ref>\n\nIt accounts for an estimated 80% of the {{convert|6|e9lb|kg|abbr=on|order=flip}} of explosives used annually in North America.<ref>{{cite journal|author=Edward M. Green|date=June 2006|title=Explosives regulation in the USA|url=http://www.crowell.com/documents/DOCASSOCFKTYPE_ARTICLES_408.pdf|journal=Industrial Materials|issue=465|page=78|accessdate=3 March 2013}}<!--This isn't a peer-reviewed journal, but a trade journal; is {{cite journal}} correct?--></ref>\n\nThe press and other media have used the term ANFO loosely and imprecisely in describing [[improvised explosive device]]s (IEDs), in cases of '''fertilizer bombs''' (see [[#Malicious use|Malicious use]] below).<ref>{{cite news |author = Jo Thomas |title = Jury to Be Picked in 2d Oklahoma Bomb Trial |work = [[The New York Times]] |date = 29 September 1997 |url=https://www.nytimes.com/1997/09/29/us/jury-to-be-picked-in-2d-oklahoma-bomb-trial.html |accessdate=3 March 2013}}</ref>\n\nThe use of ANFO originated in the 1950s.<ref>[http://www.britannica.com/EBchecked/topic/198577/explosive/82378/Ammonium-nitrate-fuel-oil-mixtures Encyclopædia Britannica]</ref>\n\n==Chemistry==\nThe chemistry of ANFO detonation is the reaction of ammonium nitrate with a long-chain [[alkane]] (C<sub>n</sub>H<sub>2n+2</sub>) to form [[nitrogen]], [[carbon dioxide]], and [[water]]. In an ideal [[stoichiometric]]ally balanced reaction, ANFO is composed of about 94.3% AN and 5.7% FO by weight. In practice, a slight excess of fuel oil is added, as underdosing results in reduced performance while overdosing merely results in more post-blast fumes.<ref name=\"pothole\" /> When detonation conditions are optimal, the aforementioned gases are the only products. In practical use, such conditions are impossible to attain, and blasts produce moderate amounts of toxic gases such as [[carbon monoxide]] and [[nitrogen oxide]]s ([[NOx|NO<sub>x</sub>]]).\n\nThe fuel component of ANFO is typically diesel, but [[kerosene]], coal dust, racing fuel, or even [[molasses]] have been used instead. Finely powdered aluminium in the mixture will sensitise it to detonate more readily.<ref>{{cite book|last=Singh|first=R. D. |title=Principles and Practices of Modern Coal Mining|year=2005|publisher=New Age International|isbn=9788122409741|page=532}}</ref>\n\nANFO is classified as a blasting agent, meaning that it decomposes through [[detonation]] rather than [[deflagration]] at a velocity higher than the speed of sound in the material but cannot be detonated with a No. 8 blasting cap without a sensitizer. ANFO has a moderate [[velocity of detonation|velocity]] compared to other industrial explosives, measuring 3,200&nbsp;m/s in {{convert|5|in|mm| abbr=on|order=flip}} diameter, unconfined, at ambient temperature.\n\nANFO is a [[tertiary explosive]], meaning that it cannot be set off by the small quantity of primary explosive in a typical blasting cap. A larger quantity of secondary explosive, known as a primer or a [[explosive booster|booster]], must be used.<ref>{{cite book |publisher=E. I. du Pont de Nemours & Company |title=Blasters' Handbook 15th Edition |year=1969 |pages=64–68 |asin=B000JM3SD0}}</ref> One or two sticks of [[dynamite]] were historically used; current practice is to use [[Tovex]] or [[cast booster]]s of [[pentolite]] (TNT/[[PETN]] or similar compositions).<ref>{{cite web |url=http://www.globalsecurity.org/military/systems/munitions/explosives-anfo.htm |title=Explosives - ANFO (Ammonium Nitrate - Fuel Oil) |publisher=GlobalSecurity.org |accessdate=3 March 2013}}</ref>\n\n==Industrial use==\n[[File:Charging with anfo.jpg|thumb|Charging a hole with ANFO for rock blasting]]\n\nIn the mining industry, the term ANFO specifically describes a mixture of solid ammonium nitrate prills and diesel fuel. Other explosives based on the ANFO chemistry exist; the most commonly used are [[emulsion]]s. They differ from ANFO in the physical form the reactants take. The most notable properties of emulsions are water resistance and higher bulk density.\n\nWhile the density of pure crystalline ammonium nitrate is 1700&nbsp;kg/m<sup>3</sup>, individual prills of explosive-grade AN measure approximately 1300&nbsp;kg/m<sup>3</sup>. Their lower density is due to the presence of a small spherical air pocket within each prill: this is the primary difference between AN sold for blasting and that sold for agricultural use. These voids are necessary to sensitize ANFO: they create so-called \"hot spots\".<ref>It was found by the IRA, in response to using low-[[brisance]] AN fertilizers, that \"hot spots\" can be created by blending powdered sugar into the ANFO mixture, effectively sensitizing the mixture to mining-standard [[prilled]] ammonium nitrate effectiveness in which the interaction of the detonation front with a spherical void concentrates energy. Blasting-grade AN prills are typically between 0.9 and 3.0&nbsp;mm in diameter.</ref> Finely powdered [[aluminium]] can be added to ANFO to increase both sensitivity and energy; however, this has fallen out of favor due to cost.\n\nANFO has a [[bulk density]] of about 840&nbsp;kg/m<sup>3</sup>. In surface mining applications, it is typically augured into boreholes by dedicated trucks that mix the AN and FO components immediately before the product is dispensed. In underground mining applications, ANFO is typically blow-loaded.\n\nAN is highly [[hygroscopic]], readily absorbing water from air. In humid environments, absorbed water interferes with its explosive function.{{citation needed|date=September 2013}} AN is fully water-soluble; as such, it cannot be loaded into boreholes that contain standing water. When used in wet mining conditions, considerable effort must be taken to remove standing water and install a liner in the borehole; it is generally more productive to instead use a water-resistant explosive such as emulsion.\n\n==Regulatory treatment==\nIn most jurisdictions, ammonium nitrate need not be classified as an explosive for transport purposes; it is merely an [[oxidizer]]. Mines typically prepare ANFO on-site using the same [[diesel fuel]] that powers their vehicles. While many fuels can theoretically be used, the low volatility and cost of diesel make it ideal.\n\nANFO under most conditions is [[blasting cap]]–insensitive, so it is classified as a [[blasting agent]]<ref>{{cite book |last=Cook |first=Melvin A. |title=The Science of Industrial Explosives |publisher=IRECO Chemicals |year=1974 |page=16 |asin=B0000EGDJT}}</ref> and not a [[high explosive]].<ref>{{cite web |url=http://www.osha.gov/pls/oshaweb/owadisp.show_document?p_id=9755&p_table=STANDARDS |title=Explosives and blasting agents |publisher=Occupation Safety & Health Administration |accessdate= March 3, 2013}}</ref>\n\nAmmonium nitrate is widely used as a [[fertilizer]] in the [[agricultural industry]]. It is also found in [[instant cold pack]]s. In many countries, its purchase and use are restricted to buyers who have obtained the proper license.\n\n==In popular culture==\n{{unreferenced section|date=October 2017}}\nThe Discovery Channel show ''[[MythBusters]]'' commonly used ANFO (with the help of detonation professionals), especially in [[MythBusters (2005 season)|episode 26: \"Salsa Escape, Cement Removal\"]] and [[MythBusters (2009 season)|episode 125: \"Knock Your Socks Off\"]] as well as the series finale when they not only blew up a recreational vehicle, but recreated the most iconic explosion in the show's history when they used 5,001 pounds of ANFO to blow up a cement truck.\n\nIt was used by [[Jim Caviezel]]'s character in the 2006 film ''[[Déjà Vu (2006 film)|Déjà Vu]]'' in a domestic terrorism attack.\n\nIn the book ''The Third Day, The Frost'' by [[John Marsden (writer)|John Marsden]], ANFO is used to blow up Cobblers Bay.\n\nIn a sub-plot of the book ''[[Executive Orders]]'' by [[Tom Clancy]], anarchists make a [[truck bomb]] filled with ANFO but are caught before reaching their target.\n\nIn the book ''[[Battle Royale (novel)|Battle Royale]]'' by [[Koshun Takami]], students who attempt to bomb a school use ANFO.\n\nIn the movie ''[[The Dark Knight (film)|The Dark Knight]]'' by [[Christopher Nolan]], [[The Joker (The Dark Knight)|The Joker]] uses the compound to hold two ferries hostage, and tries to make the people on each ferry detonate the other ferry.\n\nIn [[Stephen King]]'s novel ''[[Desperation (novel)|Desperation]]'', novelist Johnny Marinville prevents the rest of the group from going forward, and proceeds to blow up the Pit and the ''ini'' inside with the ANFO, sacrificing himself.\n\nIt is often mentioned and used by the protagonist Vic in ''[[NOS4A2]]'' by [[Joe Hill (writer)|Joe Hill]].\n\n==Disasters==\n{{main|Ammonium nitrate disasters}}\nUnmixed ammonium nitrate can decompose explosively and has been responsible for several industrial disasters, including the 1922 [[Oppau explosion]] in Germany, the 1947 [[Texas City disaster]] in [[Texas City, Texas]], the 2004 [[Ryongchon disaster]] in [[North Korea]], and the 2013 [[West Fertilizer Company explosion]] in [[West, Texas]]. Environmental hazards include [[eutrophication]] in confined waters and nitrate/gas oil contamination of ground or surface water.<ref>P. Cosgrove. Ammogex Material Safety Data Sheet, Document No: HS-MSDS-03, [[Irish Industrial Explosives Ltd]]</ref>\n\n==Malicious use==\nANFO was first used maliciously in 1970 when protests by students became violent at the [[University of Wisconsin–Madison]], who learned how to make and use ANFO from a Wisconsin Conservation Department booklet entitled ''Pothole Blasting for Wildlife'',<ref name=\"pothole\">{{cite book |author=Mathiak, Harold A. |title=Pothole Blasting for Wildlife |page=11 |publisher=Wisconsin Conservation Department, Madison, Wisconsin 53701 |year=1965}}<!--Not in LOC catalog--></ref><ref>{{cite book |author=Mike Davis |title=Buda's Wagon: A Brief History of the Car Bomb |publisher=Verso |location=New York |year=2007 |page=53 |lccn=2007274127 |isbn=1844671321}}.</ref> resulting in the [[Sterling Hall bombing]].\n\nANFO used to be widely used by the FLNC ([[National Liberation Front of Corsica]]) in Corsica, along with f15 explosive, in order to fight against French colonists and symbols. Five containers of 500 kilos each were used to blow up the Tax Office building in Bastia on 28 February 1987.\n\nThe ANFO [[car bomb]] was adopted by the [[Provisional IRA]] in 1972 and, by 1973, [[the Troubles]] were consuming 47,000&nbsp;lb of ammonium nitrate for the majority of bombs.<ref>{{cite news |author=Henry Stanhope |title=The will to blow the lid off Ulster still remains strong |newspaper=The Times |location=London |<!--url: Archive starts in 2000--> |date=8 November 1974}}</ref> The [[Ulster Volunteer Force]] (UVF) also made use of ANFO bombs, often mixing in [[gelignite]] as a booster.<ref>https://balaclavastreet.wordpress.com/tag/bombs/</ref> The IRA [[Bishopsgate bombing|detonated an ANFO truck bomb on Bishopsgate]] in London in 1993, killing one and causing £350 million in damage. It has also seen use by groups such as the [[Revolutionary Armed Forces of Colombia]] and [[ETA (separatist group)|ETA]]. In 1992, [[Shining Path]] perpetrated the [[Tarata bombing]] in [[Lima, Peru]], using two ANFO truck bombs.\n\nA more sophisticated variant of ANFO (ammonium nitrate with [[nitromethane]] as the fuel, called ANNM) was used in the 1995 [[Oklahoma City bombing]].\n\nThe [[Shijiazhuang bombings]] (Chinese: 靳如超爆炸案 or 石家庄\"3·16\"特大爆炸案) rocked the city of Shijiazhuang, China, on 16 March 2001. A total of 108 people were killed, and 38 others injured when, within a short time, several ANFO bombs exploded near four apartment buildings.<ref>{{cite news |author=<!--Staff writer(s); no by-line.--> |date=3 April 2001 |title=石家庄九名制贩爆炸物的嫌犯被刑事拘留 |trans-title=Shijiazhuang nine suspects of the sale of explosives were detained in criminal detention |url=http://news.sina.com.cn/c/222078.html&usg=ALkJrhgOJfTK3ZhEpPY9V8vwNL_TK_SjRA |language=Chinese |location=Beijing |access-date=12 August 2017 }}{{Dead link|date=September 2018 |bot=InternetArchiveBot |fix-attempted=yes }}</ref>\n\nImprovised bombs made with agricultural-grade AN are less sensitive and less efficient than the explosive-grade variety. In November 2009, a ban on [[ammonium sulfate]], ammonium nitrate, and [[calcium ammonium nitrate]] fertilizers was imposed in the former [[Malakand Division]] – comprising the [[Upper Dir]], [[Lower Dir]], [[Swat, Pakistan|Swat]], [[Chitral]] and [[Malakand District|Malakand]] districts of the [[Khyber Pakhtunkhwa|North West Frontier Province]] (NWFP) of [[Pakistan]] – by the NWFP government, following reports that those chemicals were used by militants to make explosives.\n\nIn April 2010, police in Greece confiscated 180&nbsp;kg of ANFO and other related material stashed in a hideaway in the Athens suburb of Kareas. The material was believed to be linked to attacks previously carried out by the \"Revolutionary Struggle\" terrorist group.\n\nIn January 2010, President [[Hamid Karzai]] of [[Afghanistan]] also issued a decree banning the use, production, storage, purchase, or sale of ammonium nitrate, after an investigation showed militants in the [[Taliban insurgency]] had used the substance in bomb attacks.<ref>{{cite news|url=http://www.timesunion.com/ASPStories/story.asp?StoryID=720948 |newspaper=Times Union |location=Albany, N.Y. |title=Afghanistan bans chemical used to make bombs; protesters denounce killings |deadurl=yes |archiveurl=https://web.archive.org/web/20100607130249/http://www.timesunion.com/AspStories/story.asp?storyID=720948 |archivedate=7 June 2010 }}<!--There are several different stories on archive.org matching this URL but none of them have anything close to this title--></ref><ref>{{cite news |url=https://www.theguardian.com/world/feedarticle/8910984 |agency=AP Foreign |date=22 January 2010 |newspaper=The Guardian | title=Afghanistan bans chemical used to make bombs |accessdate=3 March 2013}}</ref><ref>{{cite news |author=Dexter Filkins |url=https://www.nytimes.com/2009/11/11/world/asia/11afghan.html |title=Bomb Material Cache Uncovered in Afghanistan |newspaper=The New York Times |date=11 November 2009 |access-date=3 March 2013}}</ref>\n\nOn 22 July 2011, an aluminium powder-enriched ANNM explosive, with total size of 950&nbsp;kg (150&nbsp;kg of aluminium powder), increasing demolition power by 10–30% over plain ANFO, was used in the [[2011 Norway attacks|Oslo bombing]].<ref name=nrk950kg>{{cite news |author=Stina Åshildsdatter Grolid |author2=Unni Eikeseth |url=http://www.nrk.no/nyheter/1.7726208 | title=Slik virket trykkbølgen etter bomben |language=Norwegian |trans-title=Such seemed the shock wave after the bomb |date=25 July 2011| publisher = NRK| access-date=28 July 2011 }}</ref><ref>{{cite news | url=https://www.bloomberg.com/news/2011-07-27/norway-police-spreads-breivik-terror-probe-europe-wide-after-twin-attacks.html | work=Bloomberg | first1=Marianne | last1=Stigset | first2=Josiane | last2=Kremer | first3=Stephen | last3=Treloar | title=Police in Norway Extend Terror Probe Across Europe After Breivik Attacks | date=27 July 2011}}</ref>\n\nOn 13 April 2016, two suspected [[Irish Republican Army|IRA]] members were stopped in Dublin with 67&nbsp;kg of ANFO.<ref>{{cite news |author=Daniel Hickey |title=Two men appear in court charged with possession of 150kg of homemade explosives |newspaper=Irish Independent |location=Dublin|url=http://www.independent.ie/irish-news/courts/two-men-appear-in-court-charged-with-possession-of-150kg-of-homemade-explosives-34631186.html|access-date=16 April 2016 |date=13 April 2016}}</ref>\n\nOn 6 March 2018, 8 members of the extreme right [[Neo-Nazism|neo-Nazi]] group «[[Combat 18]] Hellas» arrested in Athens, Greece accused for multiple attacks on immigrants and activists with 50&nbsp;kg of ANFO in their possession.<ref>http://news247.gr/eidiseis/koinonia/o-tsamp-ths-combat-18-kai-o-lukos-ths-xryshs-ayghs-epivevaiwnoyn-th-logikh-twn-sygkoinwnountwn-doxeiwn.5111561.html</ref>\n\n==ANNM==\nAmmonium nitrate and [[nitromethane]] (ANNM) is one of the most powerful improvised types of AN-based explosives. The [[relative effectiveness factor]] of ANNM varies depending on the mix, but does not exceed 1.0 (ANNM+AL = R.E.F. of 1.0–1.1). ANNM usually contains a 60:40 (kinepak) mix of AN and NM (60% ammonium nitrate, 40% nitromethane by mass), though this results in a wet slurry. Sometimes, more AN is added to reduce liquidity and make it easier to store and handle, as well as providing an oxygen-balanced mix. ANNM is also more sensitive to shock than standard ANFO and is therefore easier to detonate. When ANNM [[detonate]]s, the primary products are H<sub>2</sub>O, CO<sub>2</sub> and N<sub>2</sub>, but NO<sub>x</sub> and other toxic gases are inevitably formed because of a negative oxygen balance. The balanced equation is as follows:\n\n3NH<sub>4</sub>NO<sub>3</sub> + 2CH<sub>3</sub>NO<sub>2</sub> -> 4N<sub>2</sub> + 2CO<sub>2</sub> + 9H<sub>2</sub>O\n\nDepending on the detonation impetus (for example a #6 versus a #10 [[detonator]]), the products of the detonation can be decidedly [[stoichiometric|unstoichiometric]].\n\n==References==\n{{Reflist|30em}}\n\n==External links==\n{{Commons category}}\n* [http://www.liveleak.com/view?i=71c_1182663536 Video of ANFO being used at an open pit mine]\n* [https://www.youtube.com/watch?v=ccQ9vkLrvfo Video showing detonation of a 5 kg ANFO charge]\n* [https://purl.fdlp.gov/GPO/gpo37048 Securing Ammonium Nitrate: Using Lessons Learned in Afghanistan to Protect the Homeland from IEDs: Hearing before the Subcommittee on Cybersecurity, Infrastructure Protection, and Security Technologies of the Committee on Homeland Security, House of Representatives, One Hundred Twelfth Congress, Second Session, 12 July 2012]\n\n{{DEFAULTSORT:Anfo}}\n[[Category:Binary explosives]]\n[[Category:Explosives]]"
    },
    {
      "title": "Binary explosive",
      "url": "https://en.wikipedia.org/wiki/Binary_explosive",
      "text": "A '''binary explosive''' or '''two-component explosive''' is an [[explosive]] consisting of two components, neither of which is explosive by itself, which have to be mixed in order to become explosive. Examples of common binary explosives include [[Oxyliquit]] (liquid oxygen/combustible powder), [[ANFO]] (ammonium nitrate/fuel oil), [[Kinestik]] ([[ammonium nitrate]]/[[nitromethane]]), [[Tannerite]] and [[ammonal]] (ammonium nitrate/aluminum), and FIXOR (nitroethane/physical sensitizer).\n\nBinary explosives are often used in commercial applications because of their greater handling safety.{{cn|date=December 2018}} \n\n==United States law==\n\nIn the United States, [[Bureau of Alcohol, Tobacco, Firearms and Explosives|ATF]] regulations allow the components of some binary explosives to be legally purchased, when neither one is an explosive by itself.<ref>{{cite web|url= http://www.atf.gov/explarson/fedexplolaw/2007edition/index.htm|date= November 2007|accessdate  = March 11, 2009|publisher= [[Bureau of Alcohol, Tobacco, Firearms and Explosives]]|page        = 4|title= Federal Explosives Law and Regulations, Questions and Answers|deadurl= yes|archiveurl= https://web.archive.org/web/20091008040315/http://www.atf.gov/explarson/fedexplolaw/2007edition/index.htm|archivedate= October 8, 2009|df= mdy-all}}</ref> ATF advises: \"Persons manufacturing explosives for their own personal, non-business use only (e.g., personal target practice) are not required to have a Federal explosives license or permit.\" A prohibited person (a person barred by federal law from buying or owning a firearm) cannot legally possess mixed explosives. Explosives for lawful target practice must be used once mixed: any transport, storage or commercial use of mixed explosives falls under federal explosives laws,<ref name=ATF1>[https://www.atf.gov/explosives/binary-explosives Bureau of Alcohol, Tobacco, Firearms and Explosives, \"Binary Explosives\"], atf.gov, 22 Sep 2016.</ref> and cannot be transported in mixed form without following strict regulations including insurance, packaging, signage on the transport vehicle, storage magazines, etc.\n\nVarious regulations also govern the storage of unmixed explosives. As oxidizers and combustibles, the unmixed components still have some shipping restrictions in the United States.<ref name=USPS1>[https://pe.usps.com/text/pub52/pub52c3_022.htm 344: Flammable Solids (Hazard Class 4) - USPS]</ref><ref name=USPS2>[https://pe.usps.com/text/pub52/pub52c3_023.htm 345: Oxidizing Substances, Organic Peroxides (Hazard Class 5) - USPS]</ref>\n\nA Maryland law intended specifically to ban the sale or ownership of consumer products containing binary explosive components (such as [[Tannerite]] brand rifle targets) became effective on October 1, 2012, and expanded the definition of an explosive to include, in addition to \"bombs and destructive devices designed to operate by chemical, mechanical, or explosive action\", \"two or more components that are advertised and sold together with instructions on how to combine the components to create an explosive\".<ref>Maryland House Bill [http://legiscan.com/MD/bill/HB875/2012 875] (May 22, 2012)</ref>\n\nOn August 5, 2013, the [[United States Forest Service]] (USFS) and the [[United States Attorney|U.S. Attorney's office]] in [[Denver]] announced that the USFS is implementing a closure order to prohibit the use of unpermitted explosives, particularly exploding targets, on all USFS lands in the Rocky Mountain Region. This region includes [[United States National Forest|national forests]] and [[United States National Grassland|grasslands]] in the states of [[Colorado]], [[Wyoming]], [[Kansas]], [[Nebraska]], and [[South Dakota]]. According to the USFS, at least 16 [[wildfire]]s in the Western states had been associated with exploding targets. It cost more than $33 million to extinguish the fires.<ref>{{cite news|author=Mike M. Ahlers and Rene Marsh|url=http://www.cnn.com/2013/09/06/us/guns-exploding-targets/ |title= Exploding targets: shooting aid or a 'bomb kit for dummies?' |publisher=CNN |date=September 6, 2013 |accessdate= September 8, 2013}}</ref> Such a ban has already been implemented by the USFS in [[Washington (U.S. state)|Washington]], [[Oregon]] and [[Montana]]. The [[Bureau of Land Management]] has banned the use of all exploding targets on BLM land in Utah.<ref>{{cite web |url=https://www.blm.gov/style/medialib/blm/ut/salt_lake_fo/recreation/target_shooting0.Par.94915.File.dat/FPO_BLM_Nov2014.pdf |title=FIRE PREVENTION ORDER – UTAH BLM LANDS | archiveurl=https://web.archive.org/web/20160304060221/https://www.blm.gov/style/medialib/blm/ut/salt_lake_fo/recreation/target_shooting0.Par.94915.File.dat/FPO_BLM_Nov2014.pdf|archive-date= March 4, 2017 |author=<!--Staff writer(s); no by-line.--> |date= November 18, 2013|website= |format=PDF|publisher=United States Department of the Interior |access-date= September 18, 2016|quote=Acts prohibited under this order include the following<nowiki>:</nowiki>The non-commercial use/discharge of explosives of any kind, incendiary or chemical devices, pyrotechnic devices or exploding targets.}}</ref>\n\n== See also ==\n* [[Binary (chemical weapon)]]\n* [[:Category:Binary explosives]]\n\n==References==\n<references />\n\n== External links ==\n* [https://web.archive.org/web/20041104205112/http://wrc.navair-rdte.navy.mil/warfighter_enc/weapons/ordnance/types.htm Types of Explosives] ([https://web.archive.org/web/20041105105408/https://wrc.navair-rdte.navy.mil/warfighter_enc/weapons/ordnance/types.htm Archived page])\n* [http://www.FIXOR.com FIXOR, a commercial binary explosive sold for mine clearance]\n* [https://web.archive.org/web/20060821193210/http://siri.uvm.edu/ppt/blast1/tsld014.htm Binary/Two Component Explosives], from a presentation by the N. C. Dept. of Transportation\n\n[[Category:Binary explosives]]\n\n\n{{explosive-stub}}"
    },
    {
      "title": "Miedziankit",
      "url": "https://en.wikipedia.org/wiki/Miedziankit",
      "text": "{{unreferenced|date=April 2010}}\n\n'''Miedziankit''' is a Polish [[explosive]]. It consists of [[Potassium chlorate|Potassium Chlorate]], [[Kerosene]] and optionally [[Aluminium]] Powder with traces of [[iron oxide]].\n\nThe Kerosene content must not exceed 9% but can be close to that. It is a [[Sprengel explosive|Sprengel Class Explosive]] and is mixed just before use.\n\n[[Category:Binary explosives]]"
    },
    {
      "title": "PLX",
      "url": "https://en.wikipedia.org/wiki/PLX",
      "text": "{{about|the binary explosive|the semiconductor company|PLX Technology}}\n{{More references|date=July 2008}}\n'''PLX''', or '''Picatinny Liquid Explosive''', is a liquid binary [[explosive]], a mixture of 95% [[nitromethane]] (NM) along with 5% [[ethylene diamine]] (EDA) as a sensitizer. Other [[amine]] compounds can be used instead of ethylene diamine, such as [[triethylene tetramine]] or [[ethanolamine]], but EDA has been found to be most effective. PLX is a fairly powerful [[high explosive]], marginally exceeding the destructive yield of [[trinitrotoluene|TNT]].\n\n==Properties==\nPLX, when mixed, is a transparent liquid with a yellowish tint. Both ethylene diamine and nitromethane are very volatile, requiring the contents to be sealed if any storage is intended. Generally, for safety purposes, the contents are transported separately and mixed on site. PLX is known to have a [[velocity of detonation]] (VoD) of anywhere between 6,000 and 7,000&nbsp;m/s, depending on diameter. Although greatly sensitized by the addition of EDA, PLX still requires a powerful [[blasting cap]] or a small booster charge to successfully detonate.\n\n==Uses and discovery==\nPLX was invented during [[World War II]] by the [[Picatinny Arsenal]] in New Jersey. It was originally designed to clear [[minefield]]s by being spread via plane over the targeted area or poured from a safe distance and detonated by troops on the ground.\n\nThis explosive can also be [[gel]]led through the addition of [[nitrocellulose]], [[erythritol tetranitrate|ETN]], or any number of soluble [[nitrate|nitrate esters]] or [[gelling agent]]s. This allows for powdered metals, such as [[aluminum]] or [[magnesium]], to be suspended in the mixture. The metal powders act as [[fuel]], increasing heat and energy output but lowering the [[brisance]] and VoD. The result is a  more sustained blast wave and a \"push and heave\" effect, desirable for [[thermobaric]] purposes. Trzciński{{who?|date=April 2017}} reports that 200 grams of a mixture of NM with PMMA as gelling agent and AlMg (45:55, mean particle size = 63 microns) as fuel, in a ratio of 67.2/2.8/30 by mass, has a peak overpressure of 120&nbsp;kPa 2 m from the(open air) blast site, a 1.65 TNT equivalency in peak pressure, and a 1.62 equivalency in shockwave impulse.<ref>https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0067618</ref> This means that more than 50 % of eardrums will be perforated in case of 200 g PLX/magensium-aluminium alloy detonation at a distance of 2 m. 104 kPa is widely regarded as a pressure where 50 % of eardrums fail.<ref>https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0067618</ref> It's still 3 - 5 less than needed to induce 50 % of fatalities via pulmonary injury as per Bass/Bowen equations (standing adult, facing any direction).<ref>https://www.ffi.no/no/Rapporter/2012%20-%2000539.pdf</ref>\n\nPLX has been implicated as one of the materials capable of being used in catastrophic terrorism, as most steel core columns can not withstand the detonation of 10 - 30 kg PLX in direct contact (explosive on bare steel). Nitromethane and its gelling agents are freely sold to the public in the US, though. Its sale to the public has been banned in the EU in september 2014.<ref>https://ec.europa.eu/home-affairs/sites/homeaffairs/files/what-we-do/policies/crisis-and-terrorism/explosives/explosives-precursors/docs/guidelines_on_the_marketing_and_use_of_explosives_precursors_en.pdf</ref>\n\n==Sensitizing of nitromethane==\n\nIt was the supposed explosive used in the film ''[[Die Hard with a Vengeance]]''.{{Fact|date=October 2007}} However, the film grossly exaggerated the sensitivity of this explosive mixture. \n\nPLX was one of the explosives used to down [[Korean Air Flight 858]] along with [[C-4 (explosive)|C-4]].\n\n==References==\n<references />\n\n[[Category:Binary explosives]]\n[[Category:Liquid explosives]]"
    },
    {
      "title": "PNNM",
      "url": "https://en.wikipedia.org/wiki/PNNM",
      "text": "{{Multiple issues|\n{{unreferenced|date=April 2010}}\n{{orphan|date=December 2010}}\n}}\n\n'''PNNM''' is a similar explosive to [[ANFO#ANNM|ANNM]], it consists of [[potassium nitrate]] and [[nitromethane]] in the ratio of 60:40 w/w respectively.  It can be [[Detonation|detonated]] with a grade 8 cap and often has [[aluminium]] powder added.\n\n[[Category:Binary explosives]]\n[[Category:Explosives]]\n\n\n{{explosive-stub}}"
    },
    {
      "title": "Tannerite",
      "url": "https://en.wikipedia.org/wiki/Tannerite",
      "text": "{{Distinguish|Tenorite}}\n{{Use mdy dates|date=May 2015}}\n\n'''Tannerite''' is a brand of [[binary explosive]] targets used for firearms practice and sold in kit form.<ref name=patE1>{{Citation|title=Binary exploding target, package process and product|date=2013-07-18|url=https://patents.google.com/patent/USRE45440E1/en|accessdate=2018-04-23}}</ref><ref name=\"Forbes\">{{cite web |url=https://www.forbes.com/sites/samlemonick/2016/09/19/the-science-of-tannerite-the-explosive-possibly-used-in-the-chelsea-bombs/#6726070c2739 |title= The Science Of Tannerite, The Explosive Possibly Used In The Chelsea Bombs|last=Lemonick |first=Sam |date= September 19, 2016|work=[[Forbes]] |access-date=April 25, 2018 |quote=}}</ref> The targets comprise a combination of [[oxidizer]]s and a fuel – primarily [[aluminum powder]] – that is supplied as two separate components that are mixed by the user. The combination is relatively stable when subjected to forces less severe than a high-velocity bullet impact. A hammer blow, being dropped, or impact from a low-velocity bullet or shotgun blast will not initiate a reaction. It is also designed to be non-flammable<ref name=tannerite1>{{cite web|url=https://tannerite.com/about-tannerite-sports/|title=About Tannerite Sports, LLC |website=Tannerite Sports, LLC ''(official website)'' |access-date=November 30, 2018}}</ref> (the reaction cannot be triggered by a burning fuse or electricity), although its explosion can cause other flammable material to ignite.\n\nBecause it is sold as two separate components, it can be transported and sold in many places without the legal restrictions that would otherwise apply to explosives. The term ''tannerite'' is often used to refer to the mixture itself, and other reactive targets and combination explosives are often [[generic trademark|generically referred to]] as tannerite.<ref name=CNN>{{cite web |first1=Mike M. |last1=Ahlers |first2=Rene |last2=Marsh |url=http://www.cnn.com/2013/09/06/us/guns-exploding-targets/ |title=Exploding targets: shooting aid or a 'bomb kit for dummies?' |website=[[CNN]] |date=September 6, 2013 |access-date=November 27, 2018}}</ref><ref name=\"Forbes\" />\n\n==Uses==\nTannerite brand targets explode when shot by a [[muzzle velocity|high-velocity bullet]]. Low-velocity bullets and shotgun ammunition will not initiate a reaction.{{r|patE1}}\n\nThe explosive reaction, once initiated, occurs at a very high velocity, producing a large vapor cloud and a loud report. It is marketed as a target designation that is useful for long-range target practice: the shooter does not need to walk down-range to see if the target has been hit, as the target will react and serve as a highly visible indicator.\n\nBinary explosives like Tannerite are also used in some business applications, including commercial blasting, product testing, and special effects.<ref name=ATF1>{{cite web |url=https://www.atf.gov/explosives/binary-explosives |title=Binary Explosives |website=[[U.S. Bureau of Alcohol, Tobacco, Firearms and Explosives]] |access-date=September 22, 2016}}</ref>\n\nFor safety reasons, Tannerite Sports recommends using no more than {{convert|1|lb|kg|adj=off}} of the mixed composition at once, and will sell its largest targets with a size of {{convert|2|lb|kg|adj=off}} to professionals only.<ref name=tannerite2>{{cite web |url=https://tannerite.com/binary-target-faq/#7|title=FAQ's: Got Questions? |website=Tannerite Sports, LLC ''(official website)'' |access-date=November 29, 2018}}</ref>\n\n==Target composition and sale==\nTannerite targets are sold in pre-sized quantities. The package includes two containers. An oxidizer composition is contained within one of the containers and a catalyst composition is contained within the other.\n\nThe product, developed by Daniel Jeremy Tanner, and initially formulated in 1996,<ref name=tannerite1/> consists of two components: a fuel mixed with a catalyst or sensitizer, and a bulk material or oxidizer. The fuel/catalyst mixture is 90% 600-mesh dark flake [[aluminium|aluminum]] powder, combined with the catalyst that is a mixture of 5% 325-mesh [[titanium]] sponge and 5% 200-mesh [[zirconium hydride]]{{r|patE1}} (with another patent document<ref name=Pat952>[https://www.google.com/patents/US20030033952 US patent 20030033952] (image)</ref> listing 5% [[zirconium hydroxide]]). The oxidizer is a mixture of 85% 200-mesh [[ammonium nitrate]] and 15% [[ammonium perchlorate]].<ref name=patE1/> The patents on these formulations were applied for on August 20, 2001.{{r|patE1}}<ref name=Pat952/>\n\n== United States law ==\n{{See also|Binary explosive#United States law}}\nIn the United States, the [[Bureau of Alcohol, Tobacco, Firearms and Explosives]] advises: \"Persons manufacturing explosives for their own personal, non-business use only (e.g., personal target practice) are not required to have a Federal explosives license or permit.\"{{r|ATF1}} However, \"persons falling into certain categories are prohibited from possessing explosive materials\".{{r|ATF1}} Those prohibited from possessing explosives include most non-citizens, unlawful drug users and addicts, those convicted or indicted for serious crimes, fugitives, and those who have been officially declared mentally defective or have been committed to a mental institution.{{r|ATF1}} Restrictions imposed at the state and local level also apply.<ref name=tannerite1/>{{r|ATF1}} In [[California]] in particular, a permit may be required to use or possess the product.<ref name=tannerite2/>\n\nVarious regulations also govern the storage of unmixed explosives. As oxidizers and combustibles, the unmixed components still have some shipping restrictions in the United States.<ref name=USPS1>{{cite web|url=https://pe.usps.com/text/pub52/pub52c3_022.htm|title=344 Flammable Solids (Hazard Class 4) |website=[[U.S. Postal Service]] Postal Explorer}}</ref><ref name=USPS2>{{cite web|url=https://pe.usps.com/text/pub52/pub52c3_023.htm|title=345 Oxidizing Substances, Organic Peroxides (Hazard Class 5) |website=U.S. Postal Service Postal Explorer}}</ref>\n\n==Notable incidents==\nA Minnesota man was fined $2,583 and sentenced to three years' probation<ref>{{cite news|title=Welch man gets probation for explosion|newspaper=Rochester Post-Bulletin|date=October 10, 2009}}</ref> on charges of detonating an explosive device and unlawful possession of components for explosives after he detonated {{convert|100|lb|abbr=on}} of Tannerite inside the bed of a dump truck by shooting it with a rifle chambered in [[.50 BMG]] from {{convert|300|yd}} away on January 14, 2008, in [[Red Wing, Minnesota]]. The man was on probation when he mixed and shot the Tannerite and was not allowed to possess firearms or explosives.<ref>\"[http://www.hastingsstargazette.com/news/1078101-blast-near-red-wing-brings-felony-charges Blast near Red Wing brings felony charges]\" ''Hastings Star Gazette'' January 16, 2008</ref><ref>{{cite web|url=http://www.wdio.com/kstpImages/childs_criminal_complaint.pdf |title=State of Minnesota Criminal Complaint |accessdate=March 17, 2009 |archiveurl=https://web.archive.org/web/20141222032916/http://www.wdio.com/kstpImages/childs_criminal_complaint.pdf |archive-date=December 22, 2014|last=Barringer |first=Glen |date=January 15, 2008 |format=PDF }}</ref>\n\nA 20-year-old man in [[Busti, New York]], shot {{convert|18|lb|abbr=on}} of Tannerite on January 13, 2013, that sent a particularly \"loud boom\" through much of southern [[Chautauqua County, New York]], and extending as far south as Pennsylvania, at least 3 miles away. Multiple other sounds of explosions were also reported in the incident. The explosive noise caused numerous phone calls to the Chautauqua County Sheriff's Office, the New York State Police, and other law enforcement in the area.<ref>Eric Tichy, \"[http://www.post-journal.com/news/page-one/2013/01/boom-caused-by-shooting-explosives-ban-considered-in-county/ Boom Caused By Shooting Explosives; Ban Considered In County]\" ''Post Journal'' January 15, 2013.</ref>\n\nThe [[2016 New York and New Jersey bombings|September 2016 New York and New Jersey bombings]] involved [[improvised explosive device]]s that contained \"a compound similar to a commercial explosive known as Tannerite\",{{r|NYT3}} set off by a small charge of unstable [[hexamethylene triperoxide diamine]], which served as a [[detonator]]<ref name=NYT3>[https://www.nytimes.com/2016/09/20/nyregion/nyc-nj-explosions-ahmad-khan-rahami.html Ahmad Khan Rahami Is Arrested in Manhattan and New Jersey Bombings] - The New York Times</ref><ref>{{cite web |url=http://www.scientificamerican.com/article/chemicals-could-be-a-key-in-investigating-the-new-york-and-new-jersey-bombings/ |title=Chemicals Could Be a Key in Investigating the New York and New Jersey Bombings |journal=[[Scientific American]] |first=Larry |last=Greenemeier |date=September 19, 2016 |access-date=November 27, 2018}}</ref> for the highly stable [[ammonal]]-type secondary charge.\n\nOn April 23, 2017, Dennis Dickey, an off-duty [[U.S. Border Patrol]] agent, shot a Tannerite target in a [[gender reveal]] celebration on state trust land south of [[Tucson, Arizona]], which accidentally ignited the nearby dry brush and started a {{convert|46000|acre|ha|adj=on}} fire known as the [[Sawmill Fire, Arizona|Sawmill Fire]].  At the time, winds were gusting up to {{convert|40|mi|km}} per hour and the [[National Weather Service]] had issued a fire watch in the area. By the time the wildfire was mostly contained one week later, it had jumped over the [[Santa Rita Mountains]] and crossed Arizona Highway 83, spreading into the historic [[Empire Ranch]] and the surrounding {{convert|42000|acre|ha|adj=on}} [[Las Cienegas National Conservation Area]]. The estimated damage caused by the blaze was $8.19 million.<ref>{{cite web |url=https://www.washingtonpost.com/news/morning-mix/wp/2018/10/01/a-border-patrol-agent-threw-a-gender-reveal-party-he-ended-up-starting-a-47000-acre-wildfire/ |title=Exploding target pegged as trigger for 46,000-acre Sawmill Fire |first=Antonia |last=Noori Farzan |date= October 1, 2018 |newspaper= [[Washington Post]] |access-date=October 1, 2018 |quote=}}</ref> Dickey pleaded guilty in September 2018 to a misdemeanor violation of [[U.S. Forest Service]] regulations and was sentenced to five years' probation. He also was ordered to pay restitution, with an initial payment of $100,000 (taken from his retirement fund) and monthly payments of $500 per month thereafter for 20 years unless his income changes significantly.<ref name=CNNignited>{{cite web | last=Diaz | first=Andrea | title=Officials release video from gender reveal party that ignited a 47,000-acre wildfire | website=CNN | date=November 28, 2018 | url=https://www.cnn.com/2018/11/27/us/arizona-gender-reveal-party-sawmill-wildfire-trnd/index.html | access-date=November 28, 2018}}</ref> The payments will total $220,000 over the 20 years, after which the case will return to a judge to make a decision about future restitution.<ref name=\"GVhook\">{{cite web | last=Smith | first=Kim | title=Updated: BP agent on hook for $8.2 million in Sawmill Fire | newspaper=[[Green Valley News]] | date=September 29, 2018 | url=https://www.gvnews.com/news/updated-bp-agent-on-hook-for-million-in-sawmill-fire/article_894b83ea-c413-11e8-8f16-7782974f21e7.html | access-date=November 30, 2018}}</ref> The eventual restitution payments could hypothetically be up to $8,188,069.<ref name=CNNignited/><ref name=GVhook/>\n\n==References==\n{{Reflist|30em}}\n\n[[Category:Explosives]]\n[[Category:Binary explosives]]"
    },
    {
      "title": "Binary operation",
      "url": "https://en.wikipedia.org/wiki/Binary_operation",
      "text": "{{short description|Mathematical operation that combines two elements for producing a third one}}\n\n{{distinguish|Bitwise operation}}\n[[File:Binary operations as black box.svg|thumb|A binary operation <math>\\circ</math> is a calculation that combines the arguments {{mvar|x}} and {{mvar|y}} to <math>x\\circ y</math>]]\nIn [[mathematics]], a '''binary operation''' or '''dyadic operation''' is a calculation that combines two elements (called [[operands]]) to produce another element. More formally, a binary operation is an [[Operation (mathematics)|operation]] of [[arity]] two. \n\nMore specifically, a binary operation ''on a [[Set (mathematics)|set]]'' is a binary operation whose two [[Domain of a function|domains]] and the [[codomain]] are the same set.  Examples include the familiar [[arithmetic operations]] of [[addition]], [[subtraction]], [[multiplication]].  Other examples are readily found in different areas of mathematics, such as [[vector addition]], [[matrix multiplication]] and [[Conjugation (group theory)|conjugation in groups]].\n\nHowever, a binary operation may also involve several sets. For example, [[scalar multiplication]] of [[vector spaces]] takes a scalar and a vector to produce a vector, and [[scalar product]] takes two vectors to produce a scalar. \n\nBinary operations are the keystone of most [[algebraic structure]]s, that are studied in [[algebra]], and used in all mathematics, such as [[field (mathematics)|fields]], [[group (mathematics)|groups]], [[monoid]]s, [[ring (algebra)|ring]]s, [[Algebra over a field|algebras]], and many more.\n\n==Terminology==\nMore precisely, a binary operation on a [[Set (mathematics)|set]] ''S'' is a [[Map (mathematics)|map]] which sends elements of the [[Cartesian product]] {{math|size=100%|''S'' × ''S''}} to ''S'':<ref>{{harvnb|Rotman|1973|loc=pg. 1}}</ref><ref>{{harvnb|Hardy|Walker|2002|loc=pg. 176, Definition 67}}</ref><ref>{{harvnb|Fraleigh|1976|loc= pg. 10}}</ref>\n:<math>\\,f \\colon S \\times S \\rightarrow S.</math>\nBecause the result of performing the operation on a pair of elements of ''S'' is again an element of ''S'', the operation is called a '''closed''' binary operation on ''S'' (or sometimes expressed as having the property of [[closure (mathematics)|closure]]).<ref>{{harvnb|Hall, Jr.|1959|loc=pg. 1}}</ref>  If ''f'' is not a [[Function (mathematics)|function]], but is instead a [[partial function]], it is called a '''partial binary operation'''.  For instance, division of [[real numbers]] is a partial binary operation, because one can't [[Division by zero|divide by zero]]: ''a''/0 is not defined for any real ''a''.  However, both in [[universal algebra]] and [[model theory]] the binary operations considered are defined on all of {{math|size=100%|''S'' × ''S''}}.\n\nSometimes, especially in [[computer science]], the term is used for any [[binary function]].\n\n== Properties and examples ==\nTypical examples of binary operations are the [[addition]] (+) and [[multiplication]] (&times;) of [[number]]s and [[matrix (mathematics)|matrices]] as well as [[composition of functions]] on a single set.\nFor instance,\n* On the set of real numbers '''R''', {{math|size=100%|1=''f''(''a'', ''b'') = ''a'' + ''b''}} is a binary operation since the sum of two real numbers is a real number.\n* On the set of natural numbers '''N''', {{math|size=100%|1=''f''(''a'', ''b'') = ''a'' + ''b''}} is a binary operation since the sum of two natural numbers is a natural number.  This is a different binary operation than the previous one since the sets are different.\n* On the set M(2,'''R''') of {{math|size=100%|2 × 2}} matrices with real entries, {{math|size=100%|1=''f''(''A'', ''B'') = ''A'' + ''B''}} is a binary operation since the sum of two such matrices is a {{math|size=100%|2 × 2}} matrix.\n* On the set M(2,'''R''') of {{math|size=100%|2 × 2}} matrices with real entries, {{math|size=100%|1=''f''(''A'', ''B'') = ''AB''}} is a binary operation since the product of two such matrices is a {{math|size=100%|2 × 2}} matrix.\n* For a given set ''C'', let ''S'' be the set of all functions {{math|size=100%|''h'' : ''C'' → ''C''}}.  Define {{math|size=100%|''f'' : ''S'' × ''S'' → ''S''}} by {{math|size=100%|1=''f''(''h''{{sub|1}}, ''h''{{sub|2}})(''c'') = (''h''{{sub|1}} ∘ ''h''{{sub|2}}) (''c'') = ''h''{{sub|1}}(''h''{{sub|2}}(''c''))}} for all {{math|size=100%|''c'' ∈ ''C''}}, the composition of the two functions ''h''{{sub|1}} and ''h''{{sub|2}} in ''S''. Then ''f'' is a binary operation since the composition of the two functions is again a function on the set ''C'' (that is, a member of ''S'').\n\nMany binary operations of interest in both algebra and formal logic are [[commutative]], satisfying {{math|size=100%|1=''f''(''a'', ''b'') = ''f''(''b'', ''a'')}} for all elements ''a'' and ''b'' in ''S'', or [[associative]], satisfying {{math|size=100%|1=''f''(''f''(''a'', ''b''), ''c'') = ''f''(''a'', ''f''(''b'', ''c''))}} for all ''a'', ''b'' and ''c'' in ''S''.  Many also have [[identity element]]s and [[inverse element]]s.\n\nThe first three examples above are commutative and all of the above examples are associative.\n\nOn the set of real numbers '''R''', [[subtraction]], that is, {{math|size=100%|1=''f''(''a'', ''b'') = ''a'' − ''b''}}, is a binary operation which is not commutative since, in general, {{math|size=100%|''a'' − ''b'' ≠ ''b'' − ''a''}}.  It is also not associative, since, in general, {{math|size=100%|''a'' − (''b'' − ''c'') ≠ (''a'' − ''b'') − ''c''}}; for instance, {{math|size=100%|1=1 − (2 − 3) = 2}} but {{math|size=100%|1=(1 − 2) − 3 = −4}}.\n\nOn the set of natural numbers '''N''', the binary operation [[exponentiation]], {{math|size=100%|1=''f''(''a'',''b'') = ''a''<sup>''b''</sup>}}, is not commutative since,  {{math|size=100%|''a''<sup>''b''</sup> ≠ ''b''<sup>''a''</sup>}} (cf. [[Equation xʸ = yˣ]]), and is also not associative since {{math|size=100%|''f''(''f''(''a'', ''b''), ''c'') ≠ ''f''(''a'', ''f''(''b'', ''c''))}}.  For instance, with {{math|size=100%|1=''a'' = 2}}, {{math|size=100%|1=''b'' = 3}} and {{math|size=100%|1=''c'' = 2}}, {{math|size=100%|1=''f''(2<sup>3</sup>,2) = ''f''(8,2) = 8<sup>2</sup> = 64}}, but {{math|size=100%|1=''f''(2,3<sup>2</sup>) = ''f''(2,9) = 2<sup>9</sup> = 512}}.  By changing the set '''N''' to the set of integers '''Z''', this binary operation becomes a partial binary operation since it is now undefined when {{math|size=100%|1=''a'' = 0}} and ''b'' is any negative integer.  For either set, this operation has a ''right identity'' (which is 1) since {{math|size=100%|1=''f''(''a'', 1) = ''a''}} for all ''a'' in the set, which is not an ''identity'' (two sided identity) since {{math|size=100%|''f''(1, ''b'') ≠ ''b''}} in general.\n\n[[division (mathematics)|Division]] (/), a partial binary operation on the set of real or rational numbers, is not commutative or associative.  [[Tetration]] (↑↑), as a binary operation on the natural numbers, is not commutative or associative and has no identity element.\n\n==Notation==\nBinary operations are often written using [[infix notation]] such as {{math|size=100%|''a'' ∗ ''b''}}, {{math|size=100%|''a'' + ''b''}}, {{math|size=100%|''a'' · ''b''}} or (by [[wikt:juxtaposition|juxtaposition]] with no symbol) ''ab'' rather than by functional notation of the form {{math|size=100%|''f''(''a'', ''b'')}}.  Powers are usually also written without operator, but with the second argument as [[superscript]].\n\nBinary operations sometimes use prefix or (probably more often) postfix notation, both of which dispense with parentheses.  They are also called, respectively, [[Polish notation]] and [[reverse Polish notation]].\n\n==Pair and tuple==\nA binary operation, ''ab'', depends on the [[ordered pair]] (''a, b'') and so (''ab'')''c'' (where the parentheses here mean first operate on the ordered pair (''a'', ''b'') and then operate on the result of that using the ordered pair ((''ab''), ''c'')) depends in general on the ordered pair ((''a'', ''b''), ''c'').  Thus, for the general, non-associative case, binary operations can be represented with [[binary tree]]s.\n\nHowever:\n*If the operation is associative, (''ab'')''c'' = ''a''(''bc''), then the value of (''ab'')''c'' depends only on the [[tuple]] (''a'', ''b'', ''c'').\n*If the operation is commutative, ''ab'' = ''ba'', then the value of (''ab'')''c'' depends only on { {''a'', ''b''}, ''c''}, where braces indicate  [[multiset]]s.\n*If the operation is both associative and commutative then the value of (''ab'')''c'' depends only on the multiset {''a'', ''b'', ''c''}.\n*If the operation is associative, commutative and [[idempotent]], ''aa'' = ''a'', then the value of (''ab'')''c''  depends only on the [[Set (mathematics)|set]] {''a'', ''b'', ''c''}.\n\n== Binary operations as ternary relations ==\n\nA binary operation ''f'' on a set ''S'' may be viewed as a ''ternary'' [[Finitary relation|relation]] on ''S'', that is, the set of triples (''a'', ''b'', ''f''(''a,b'')) in ''S'' × ''S'' × ''S'' for all ''a'' and ''b'' in ''S''.\n\n== External binary operations ==\nAn '''[[external (mathematics)|external]] binary operation''' is a binary function from ''K'' &times; ''S'' to ''S''.  This differs from a ''binary operation on a set'' in the sense in that ''K'' need not be ''S''; its elements come from ''outside''.\n\nAn example of an [[external (mathematics)|external]] binary operation is [[scalar multiplication]] in [[linear algebra]].  Here ''K'' is a [[field (mathematics)|field]] and ''S'' is a [[vector space]] over that field.\n\nAn [[external (mathematics)|external]] binary operation may alternatively be viewed as an [[Group action (mathematics)|action]]; ''K'' is acting on ''S''.\n\nThe [[dot product]] of two vectors maps from ''S'' &times; ''S'' to ''K'', where ''K'' is a field and ''S'' is a vector space over ''K''. It depends on authors whether it is considered as a binary operation.\n\n==See also==\n* [[Truth table#Binary operations]]\n* [[Iterated binary operation]]\n* [[Operator (programming)]]\n* [[Ternary operation]]\n* [[Unary operation]]\n\n== Notes==\n{{reflist}}\n\n== References==\n* {{citation|last=Fraleigh|first=John B.|title=A First Course in Abstract Algebra|edition=2nd|publisher=Addison-Wesley|place=Reading|year=1976|isbn=0-201-01984-1}}\n* {{citation|last=Hall, Jr.|first= Marshall|title=The Theory of Groups|publisher=Macmillan|place=New York|year=1959}}\n* {{citation|last1=Hardy|first1=Darel W.|last2=Walker|first2=Carol L.|title=Applied Algebra: Codes, Ciphers and Discrete Algorithms|publisher=Prentice-Hall|place=Upper Saddle River, NJ|year=2002|isbn=0-13-067464-8}}\n* {{citation|last=Rotman|first=Joseph J.|title=The Theory of Groups: An Introduction|publisher=Allyn and Bacon|place=Boston|year=1973|edition=2nd}}\n\n== External links ==\n* {{MathWorld|title=Binary Operation|urlname=BinaryOperation}}\n\n{{Mathematical logic}}\n\n{{DEFAULTSORT:Binary Operation}}\n[[Category:Binary operations| ]]"
    },
    {
      "title": "Absorbing element",
      "url": "https://en.wikipedia.org/wiki/Absorbing_element",
      "text": "In [[mathematics]], an '''absorbing element''' (or, '''annihilating element''') is a special type of element of a [[Set (mathematics)|set]] with respect to a [[binary operation]] on that set.  The result of combining an absorbing element with any element of the set is the absorbing element itself. In [[semigroup]] theory, the absorbing element is called a '''[[Semigroup#Identity and zero|zero element]]'''<ref>J.M. Howie, pp. 2–3</ref><ref name=kkm>M. Kilp, U. Knauer, A.V. Mikhalev pp. 14–15</ref> because there is no risk of confusion with [[Zero element|other notions of zero]]. In this article the two notions are synonymous.\n\n== Definition ==\nFormally, let {{nowrap|(''S'', •)}} be a set ''S'' with a closed binary operation • on it (known as a [[magma (algebra)|magma]]). A '''zero element''' is an element ''z'' such that for all ''s'' in ''S'', {{nowrap|1=''z'' • ''s'' = ''s'' • ''z'' = ''z''}}. A refinement<ref name=kkm/> are the notions of '''left zero''', where one requires only that {{nowrap|1=''z'' • ''s'' = ''z''}}, and '''right zero''', where {{nowrap|1=''s'' • ''z'' = ''z''}}.\n\nAbsorbing elements are particularly interesting for [[semigroup]]s, especially the multiplicative semigroup of a [[semiring]]. In the case of a semiring with 0, the definition of an absorbing element is sometimes relaxed so that it is not required to absorb 0; otherwise, 0 would be the only absorbing element.<ref>J.S. Golan p. 67</ref>\n\n==Properties==\n* If a magma has both a left zero ''z'' and a right zero ''z''′, then it has a zero, since {{nowrap|1=''z'' = ''z'' • ''z''′ = ''z''′}}.\n* A magma can have at most one zero element.\n\n==Examples==\n*The most well known example of an absorbing element comes from elementary algebra, where any number multiplied by zero equals zero.  Zero is thus an absorbing element.\n*The zero of any [[Ring (mathematics)|ring]] is also an absorbing element. For an element ''r'' of a ring ''R'', ''r=r(1+0)=r+r0'', so ''r0=0'', as zero is the unique element ''a'' for which ''r+a=r'' for any ''r'' in the ring ''R''.\n* [[Floating point]] arithmetics as defined in IEEE-754 standard contains a special value called Not-a-Number (\"NaN\"). It is an absorbing element for every operation; i.e., {{nowrap|1=''x'' + NaN = NaN + ''x'' = NaN}}, {{nowrap|1=''x'' − NaN = NaN − ''x'' = NaN}}, etc.\n* The set of [[binary relations]] over a set ''X'', together with the [[composition of relations]] forms a [[monoid]] with zero, where the zero element is the [[empty relation]] ([[empty set]]).\n* The closed interval {{nowrap|1=''H'' = [0, 1]}} with {{nowrap|1=''x'' • ''y'' = min(''x'', ''y'')}} is also a monoid with zero, and the zero element is&nbsp;0.\n* More examples:\n{| class=\"wikitable\" style=\"margin: 1em auto 1em auto\"\n! Domain\n! colspan=2 | Operation\n! colspan=2 | Absorber\n|-\n| [[Real number]]s\n| ⋅ || Multiplication\n| [[0 (number)|0]] ||\n|-\n| [[Integer]]s\n|   || [[Greatest common divisor]]\n| 1 ||\n|-\n| ''n''-by-''n'' square [[matrix (mathematics)|matrices]]\n| || [[Matrix multiplication]]\n| || [[zero matrix|Matrix of all zeroes]]\n|-\n| rowspan=2 | [[Extended real number line|Extended real number]]s\n|    || Minimum/infimum\n| −∞ ||\n|-\n|    || Maximum/supremum\n| +∞ ||\n|-\n| [[Set (mathematics)|Set]]s\n| ∩ || Intersection\n| ∅ || [[Empty set]]\n|-\n| Subsets of a set ''M''\n| ∪     || Union\n| ''M'' ||\n|-\n| rowspan=2 | [[Boolean algebra|Boolean logic]]\n| ∧ || [[Logical conjunction|Logical and]]\n| ⊥ || Falsity\n|-\n| ∨ || [[Logical disjunction|Logical or]]\n| ⊤ || Truth\n|}\n\n==See also==\n*[[Idempotent (ring theory)]]{{snd}}an element ''x'' of a ring such that ''x''<sup>2</sup> = ''x''\n*[[Identity element]]\n*[[Null semigroup]]\n\n==Notes==\n{{reflist|2}}\n\n==References==\n*{{cite book|last= Howie|first= John M.|title=Fundamentals of Semigroup Theory|year=1995|publisher=[[Clarendon Press]]|isbn=0-19-851194-9}}\n* M. Kilp, U. Knauer, A.V. Mikhalev, ''Monoids, Acts and Categories with Applications to Wreath Products and Graphs'', De Gruyter Expositions in Mathematics vol. 29, Walter de Gruyter, 2000, {{ISBN|3-11-015248-7}}.\n* {{cite book |title=Semirings and Their Applications |first=Jonathan S. |last=Golan |year=1999 |publisher=Springer |isbn=0-7923-5786-8}}\n\n==External links==\n* [http://planetmath.org/absorbingelement Absorbing element] at PlanetMath\n\n[[Category:Semigroup theory]]\n[[Category:Binary operations|*Absorbing element]]"
    },
    {
      "title": "Addition",
      "url": "https://en.wikipedia.org/wiki/Addition",
      "text": "{{short description|Arithmetic operation}}\n{{other uses}}\n{{redirect|Add||ADD (disambiguation)}}\n{{good article}}\n[[File:Addition01.svg|right|thumb|120px|3 + 2 = 5 with [[apple]]s, a popular choice in textbooks<ref>From Enderton (p. 138): \"...select two sets ''K'' and ''L'' with card ''K'' = 2 and card ''L'' = 3. Sets of fingers are handy; sets of apples are preferred by textbooks.\"</ref>]]\n'''Addition''' (often signified by the [[plus symbol]] \"+\") is one of the four basic [[Operation (mathematics)|operations]] of [[arithmetic]]; the others are [[subtraction]], [[multiplication]] and [[Division (mathematics)|division]]. The addition of two [[Natural number|whole numbers]] is the total amount of those values combined. For example, in the adjacent picture, there is a combination of three apples and two apples together, making a total of five apples. This observation is equivalent to the [[Expression (mathematics)|mathematical expression]] {{nowrap|1=\"3 + 2 = 5\"}} i.e., \"3 ''add'' 2 is [[Equality (mathematics)|equal]] to 5\".\n\nBesides counting items, addition can also be defined on other types of numbers, such as [[integer]]s, [[real number]]s and [[complex number]]s. This is part of [[arithmetic]], a branch of mathematics. In [[algebra]], another area of mathematics, addition can be performed on abstract objects such as [[Euclidean vector|vectors]] and [[Matrix (mathematics)|matrices]].\n\nAddition has several important properties. It is [[commutative property|commutative]], meaning that order does not matter, and it is [[associativity|associative]], meaning that when one adds more than two numbers, the order in which addition is performed does not matter (see ''[[Summation]]''). Repeated addition of {{num|1}} is the same as [[counting]]; addition of {{num|0}} does not change a number. Addition also obeys predictable rules concerning related operations such as [[subtraction]] and [[multiplication]].\n\nPerforming addition is one of the simplest numerical tasks. Addition of very small numbers is accessible to toddlers; the most basic task, {{nowrap|1 + 1}}, can be performed by infants as young as five months and even some members of other animal species. In [[primary education]], students are taught to add numbers in the [[decimal]] system, starting with single digits and progressively tackling more difficult problems. Mechanical aids range from the ancient [[abacus]] to the modern [[computer]], where research on the most efficient implementations of addition continues to this day.\n\n==Notation and terminology==\n[[File:PlusCM128.svg|right|120px|thumb|The plus sign]]\nAddition is written using the [[Plus and minus signs|plus sign]] \"+\" between the terms; that is, in [[infix notation]]. The result is expressed with an [[equals sign]]. For example,\n:<math>1 + 1 = 2</math> (\"one plus one equals two\")\n:<math>2 + 2 = 4</math> (\"two plus two equals four\")\n:<math>1 + 2 = 3</math> (\"one plus two equals three\")\n:<math>5 + 4 + 2 = 11</math> (see \"associativity\" [[#Associativity|below]])\n:<math>3 + 3 + 3 + 3 = 12</math> (see \"multiplication\" [[#Related operations|below]])\n\n[[File:AdditionVertical.svg|right|thumb|Columnar addition – the numbers in the column are to be added, with the sum written below the [[underline]]d number.]]\nThere are also situations where addition is \"understood\" even though no symbol appears:\n* A whole number followed immediately by a [[fraction (mathematics)|fraction]] indicates the sum of the two, called a ''mixed number''.<ref>Devine et al. p. 263</ref> For example,<br />{{spaces|6}}3½ = 3 + ½ = 3.5.<br />This notation can cause confusion since in most other contexts [[wikt:juxtaposition|juxtaposition]] denotes [[multiplication]] instead.<ref>Mazur, Joseph. ''Enlightening Symbols: A Short History of Mathematical Notation and Its Hidden Powers''. Princeton University Press, 2014. p. 161</ref>\n\nThe sum of a [[series (mathematics)|series]] of related numbers can be expressed through [[capital sigma notation]], which compactly denotes [[iteration]]. For example,\n:<math>\\sum_{k=1}^5 k^2 = 1^2 + 2^2 + 3^2 + 4^2 + 5^2 = 55.</math>\n\n{{anchor|summand}}\nThe numbers or the objects to be added in general addition are collectively referred to as the '''terms''',<ref>Department of the Army (1961) Army Technical Manual TM 11-684: Principles and Applications of Mathematics for Communications-Electronics. Section 5.1</ref> the '''{{vanchor|addend}}s'''<ref name=\"Shmerko\">{{cite book |last1=Shmerko |first1=V.P. |last2=Yanushkevich [Ânuškevič] |first2=Svetlana N. [Svitlana N.] |last3=Lyshevski |first3=S.E. |date=2009 |title=Computer arithmetics for nanoelectronics |publisher=[[CRC Press]] |page=80}}</ref><ref name=\"Schmid_1974\"/> or the '''summands''';<ref>Hosch, W.L. (Ed.). (2010). The Britannica Guide to Numbers and Measurement. The Rosen Publishing Group. p. 38</ref>\nthis terminology carries over to the summation of multiple terms.\nThis is to be distinguished from ''factors'', which are [[multiplication|multiplied]].\nSome authors call the first addend the ''augend''.<ref name=\"Shmerko\"/><ref name=\"Schmid_1974\">{{cite book |title=Decimal Computation |first=Hermann |last=Schmid|author-link=Hermann Schmid (computer scientist) |date=1974 |edition=1st |publisher=[[John Wiley & Sons]] |location=Binghamton, NY |isbn=0-471-76180-X}} and {{cite book |title=Decimal Computation |first=Hermann |last=Schmid|author-link=Hermann Schmid (computer scientist) |orig-year=1974 |date=1983 |edition=reprint of 1st|publisher=Robert E. Krieger Publishing Company |location=Malabar, FL|isbn=978-0-89874-318-0}}</ref> In fact, during the [[Renaissance]], many authors did not consider the first addend an \"addend\" at all. Today, due to the [[commutative property]] of addition, \"augend\" is rarely used, and both terms are generally called addends.<ref name=\"Schwartzman p. 19\">Schwartzman p. 19</ref>\n\nAll of the above terminology derives from [[Latin]]. \"[[wikt:addition|Addition]]\" and \"[[wikt:add|add]]\" are [[English language|English]] words derived from the Latin [[verb]] ''addere'', which is in turn a [[compound (linguistics)|compound]] of ''ad'' \"to\" and ''dare'' \"to give\", from the [[Proto-Indo-European root]] {{PIE|''*deh₃-''}} \"to give\"; thus to ''add'' is to ''give to''.<ref name=\"Schwartzman p. 19\"/> Using the [[gerundive]] [[Affix|suffix]] ''-nd'' results in \"addend\", \"thing to be added\".<ref group=lower-alpha>\"Addend\" is not a Latin word; in Latin it must be further conjugated, as in ''numerus addendus'' \"the number to be added\".</ref> Likewise from ''augere'' \"to increase\", one gets \"augend\", \"thing to be increased\".\n\n[[File:AdditionNombryng.svg|left|thumb|Redrawn illustration from ''The Art of Nombryng'', one of the first English arithmetic texts, in the 15th century.<ref>Karpinski pp. 56–57, reproduced on p. 104</ref>]]\n\"Sum\" and \"summand\" derive from the Latin [[noun]] ''summa'' \"the highest, the top\" and associated verb ''summare''. This is appropriate not only because the sum of two positive numbers is greater than either, but because it was common for the [[Ancient Greece|ancient Greeks]] and [[Ancient Rome|Romans]] to add upward, contrary to the modern practice of adding downward, so that a sum was literally higher than the addends.<ref>Schwartzman (p. 212) attributes adding upwards to the [[Ancient Greece|Greeks]] and [[Ancient Rome|Romans]], saying it was about as common as adding downwards. On the other hand, Karpinski (p. 103) writes that [[Leonard of Pisa]] \"introduces the novelty of writing the sum above the addends\"; it is unclear whether Karpinski is claiming this as an original invention or simply the introduction of the practice to Europe.</ref>\n''Addere'' and ''summare'' date back at least to [[Anicius Manlius Severinus Boethius|Boethius]], if not to earlier Roman writers such as [[Vitruvius]] and [[Sextus Julius Frontinus|Frontinus]]; Boethius also used several other terms for the addition operation. The later [[Middle English]] terms \"adden\" and \"adding\" were popularized by [[Geoffrey Chaucer|Chaucer]].<ref>Karpinski pp. 150–153</ref>\n\nThe [[plus sign]] \"+\" ([[Unicode]]:U+002B; [[ASCII]]: <code>&amp;#43;</code>) is an abbreviation of the Latin word ''et'', meaning \"and\".<ref>{{cite book |last=Cajori |first=Florian |title=A History of Mathematical Notations, Vol. 1 |year=1928 |publisher=The Open Court Company, Publishers |chapter=Origin and meanings of the signs + and -}}</ref> It appears in mathematical works dating back to at least 1489.<ref name=\"OED\">{{OED|plus}}</ref>\n\n==Interpretations==\nAddition is used to model many physical processes. Even for the simple case of adding [[natural number]]s, there are many possible interpretations and even more visual representations.\n\n===Combining sets===\n[[File:AdditionShapes.svg|right|200px|thumb]]\nPossibly the most fundamental interpretation of addition lies in combining sets:\n* When two or more disjoint collections are combined into a single collection, the number of objects in the single collection is the sum of the numbers of objects in the original collections.\n\nThis interpretation is easy to visualize, with little danger of ambiguity. It is also useful in higher mathematics; for the rigorous definition it inspires, see ''[[#Natural numbers|Natural numbers]]'' below. However, it is not obvious how one should extend this version of addition to include fractional numbers or negative numbers.<ref>See Viro 2001 for an example of the sophistication involved in adding with sets of \"fractional cardinality\".</ref>\n\nOne possible fix is to consider collections of objects that can be easily divided, such as pies or, still better, segmented rods.<ref>''Adding it up'' (p. 73) compares adding measuring rods to adding sets of cats: \"For example, inches can be subdivided into parts, which are hard to tell from the wholes, except that they are shorter; whereas it is painful to cats to divide them into parts, and it seriously changes their nature.\"</ref> Rather than just combining collections of segments, rods can be joined end-to-end, which illustrates another conception of addition: adding not the rods but the lengths of the rods.\n\n===Extending a length===\n[[File:AdditionLineAlgebraic.svg|right|frame|A number-line visualization of the algebraic addition 2 + 4 = 6. A translation by 2 followed by a translation by 4 is the same as a translation by 6.]]\n[[File:AdditionLineUnary.svg|right|frame|A number-line visualization of the unary addition 2 + 4 = 6. A translation by 4 is equivalent to four translations by 1.]]\nA second interpretation of addition comes from extending an initial length by a given length:\n* When an original length is extended by a given amount, the final length is the sum of the original length and the length of the extension.<ref>Mosley, F. (2001). ''Using number lines with 5–8 year olds''. Nelson Thornes. p. 8</ref>\n\nThe sum ''a'' + ''b'' can be interpreted as a [[binary operation]] that combines ''a'' and ''b'', in an algebraic sense, or it can be interpreted as the addition of ''b'' more units to ''a''. Under the latter interpretation, the parts of a sum {{nowrap|''a'' + ''b''}} play asymmetric roles, and the operation {{nowrap|''a'' + ''b''}} is viewed as applying the [[unary operation]] +''b'' to ''a''.<ref>Li, Y., & [[Glenda Lappan|Lappan, G.]] (2014). ''Mathematics curriculum in school education''. Springer. p. 204</ref> Instead of calling both ''a'' and ''b'' addends, it is more appropriate to call ''a'' the '''augend''' in this case, since ''a'' plays a passive role. The unary view is also useful when discussing [[subtraction]], because each unary addition operation has an inverse unary subtraction operation, and ''vice versa''.\n\n==Properties==\n===Commutativity===\n[[File:AdditionComm01.svg|right|113px|thumb|4 + 2 = 2 + 4 with blocks]]\nAddition is [[commutative]]: one can change the order of the terms in a sum, and the result is the same. Symbolically, if ''a'' and ''b'' are any two numbers, then\n:''a'' + ''b'' = ''b'' + ''a''.\nThe fact that addition is commutative is known as the \"commutative law of addition\". Some other [[binary operation]]s are commutative, such as multiplication, but many others are not, such as subtraction and division.\n\n===Associativity===\n[[File:AdditionAsc.svg|left|100px|thumb|2 + (1 + 3) = (2 + 1) + 3 with segmented rods]]\nAddition is [[associativity|associative]]: when adding three or more numbers, the [[order of operations]] does not matter.\n\nAs an example, should the expression ''a'' + ''b'' + ''c'' be defined to mean (''a'' + ''b'') + ''c'' or ''a'' + (''b'' + ''c'')? Given that addition is associative, the choice of definition is irrelevant. For any three numbers ''a'', ''b'', and ''c'', it is true that {{nowrap|1=(''a'' + ''b'') + ''c'' = ''a'' + (''b'' + ''c'')}}. For example, {{nowrap|1=(1 + 2) + 3 = 3 + 3 = 6 = 1 + 5 = 1 + (2 + 3)}}.\n\nWhen addition is used together with other operations, the [[order of operations]] becomes important. In the standard order of operations, addition is a lower priority than [[exponentiation]], [[nth root]]s, multiplication and division, but is given equal priority to subtraction.<ref>{{cite book |title=Taschenbuch der Mathematik |author-first1=Ilja Nikolaevič<!-- Nikolajewitsch --> |author-last1=Bronstein<!-- 1903–1976 --> |author-first2=Konstantin Adolfovič<!-- Adolfowitsch --> |author-last2=Semendjajew<!-- 1908–1988 --> |editor-first1=Günter |editor-last1=Grosche |editor-first2=Viktor |editor-last2=Ziegler<!-- 1922–1980--> |editor-first3=Dorothea |editor-last3=Ziegler |others=Weiß, Jürgen<!-- lector --> |translator-first=Viktor |translator-last=Ziegler |volume=1 |date=1987 |edition=23 |orig-year=1945 |publisher=[[Verlag Harri Deutsch]] (and [[B.G. Teubner Verlagsgesellschaft]], Leipzig) |location=Thun and Frankfurt am Main |language=German |chapter=2.4.1.1. |pages=115–120 |isbn=978-3-87144-492-0 |title-link=Bronstein and Semendjajew}}</ref>\n\n===Identity element===\n[[File:AdditionZero.svg|right|70px|thumb|5 + 0 = 5 with bags of dots]]\nWhen adding [[0 (number)|zero]] to any number, the quantity does not change; zero is the [[identity element]] for addition, also known as the [[additive identity]]. In symbols, for any ''a'',\n:''a'' + 0 = 0 + ''a'' = ''a''.\nThis law was first identified in [[Brahmagupta]]'s ''[[Brahmasphutasiddhanta]]'' in 628&nbsp;AD, although he wrote it as three separate laws, depending on whether ''a'' is negative, positive, or zero itself, and he used words rather than algebraic symbols. Later [[Indian mathematicians]] refined the concept; around the year 830, [[Mahavira (mathematician)|Mahavira]] wrote, \"zero becomes the same as what is added to it\", corresponding to the unary statement {{nowrap|1=0 + ''a'' = ''a''}}. In the 12th&nbsp;century, [[Bhāskara II|Bhaskara]] wrote, \"In the addition of cipher, or subtraction of it, the quantity, positive or negative, remains the same\", corresponding to the unary statement {{nowrap|1=''a'' + 0 = ''a''}}.<ref>Kaplan pp. 69–71</ref>\n\n===Successor===\nWithin the context of integers, addition of [[1 (number)|one]] also plays a special role: for any integer ''a'', the integer {{nowrap|(''a'' + 1)}} is the least integer greater than ''a'', also known as the [[successor function|successor]] of ''a''.<ref>Hempel, C.G. (2001). The philosophy of Carl G. Hempel: studies in science, explanation, and rationality. p. 7</ref> For instance, 3 is the successor of 2 and 7 is the successor of 6. Because of this succession, the value of {{nowrap|''a'' + ''b''}} can also be seen as the ''b''th successor of ''a'', making addition iterated succession. For examples, {{nowrap|6 + 2}} is 8, because 8 is the successor of 7, which is the successor of 6, making 8 the 2nd successor of 6.\n\n===Units===\nTo numerically add physical quantities with [[units of measurement|units]], they must be expressed with common units.<ref>R. Fierro (2012) ''Mathematics for Elementary School Teachers''. Cengage Learning. Sec 2.3</ref> For example, adding 50&nbsp;milliliters to 150&nbsp;milliliters gives 200&nbsp;milliliters. However, if a measure of 5&nbsp;feet is extended by 2&nbsp;inches, the sum is 62&nbsp;inches, since 60&nbsp;inches is synonymous with 5&nbsp;feet. On the other hand, it is usually meaningless to try to add 3&nbsp;meters and 4&nbsp;square meters, since those units are incomparable; this sort of consideration is fundamental in [[dimensional analysis]].\n\n==Performing addition==\n===Innate ability===\nStudies on mathematical development starting around the 1980s have exploited the phenomenon of [[habituation]]: [[infant]]s look longer at situations that are unexpected.<ref>Wynn p. 5</ref> A seminal experiment by [[Karen Wynn]] in 1992 involving [[Mickey Mouse]] dolls manipulated behind a screen demonstrated that five-month-old infants ''expect'' {{nowrap|1 + 1}} to be 2, and they are comparatively surprised when a physical situation seems to imply that {{nowrap|1 + 1}} is either 1 or 3. This finding has since been affirmed by a variety of laboratories using different methodologies.<ref>Wynn p. 15</ref> Another 1992 experiment with older [[toddler]]s, between 18 and 35&nbsp;months, exploited their development of motor control by allowing them to retrieve [[ping-pong]] balls from a box; the youngest responded well for small numbers, while older subjects were able to compute sums up to 5.<ref>Wynn p. 17</ref>\n\nEven some nonhuman animals show a limited ability to add, particularly [[primate]]s. In a 1995 experiment imitating Wynn's 1992 result (but using [[eggplant]]s instead of dolls), [[rhesus macaque]] and [[cottontop tamarin]] monkeys performed similarly to human infants. More dramatically, after being taught the meanings of the [[Arabic numerals]] 0 through 4, one [[Common Chimpanzee|chimpanzee]] was able to compute the sum of two numerals without further training.<ref>Wynn p. 19</ref> More recently, [[Asian elephant]]s have demonstrated an ability to perform basic arithmetic.<ref>{{cite news |newspaper=The Guardian |last=Randerson |first=James |url=https://www.theguardian.com/science/2008/aug/21/elephants.arithmetic |title=Elephants have a head for figures |date=21 August 2008 |accessdate=29 March 2015}}</ref>\n\n===Learning addition as children===\nTypically, children first master [[counting]]. When given a problem that requires that two items and three items be combined, young children model the situation with physical objects, often fingers or a drawing, and then count the total. As they gain experience, they learn or discover the strategy of \"counting-on\": asked to find two plus three, children count three past two, saying \"three, four, ''five''\" (usually ticking off fingers), and arriving at five. This strategy seems almost universal; children can easily pick it up from peers or teachers.<ref>F. Smith p. 130</ref> Most discover it independently. With additional experience, children learn to add more quickly by exploiting the commutativity of addition by counting up from the larger number, in this case starting with three and counting \"four, ''five''.\" Eventually children begin to recall certain addition facts (\"[[number bond]]s\"), either through experience or rote memorization. Once some facts are committed to memory, children begin to derive unknown facts from known ones. For example, a child asked to add six and seven may know that {{nowrap|1=6 + 6 = 12}} and then reason that {{nowrap|6 + 7}} is one more, or 13.<ref>{{Cite book |last=Carpenter |first=Thomas |author2=Fennema, Elizabeth |author3=Franke, Megan Loef |author4=Levi, Linda |author5=Empson, Susan |title=Children's mathematics: Cognitively guided instruction |publisher=Heinemann |year=1999 |location=Portsmouth, NH |isbn=978-0-325-00137-1}}</ref> Such derived facts can be found very quickly and most elementary school students eventually rely on a mixture of memorized and derived facts to add fluently.<ref name=Henry>{{Cite journal |last=Henry |first=Valerie J. |author2=Brown, Richard S. |title=First-grade basic facts: An investigation into teaching and learning of an accelerated, high-demand memorization standard |journal=Journal for Research in Mathematics Education |volume=39 |issue=2 |pages=153–183 |year=2008 |doi=10.2307/30034895|jstor=30034895 }}</ref>\n\nDifferent nations introduce whole numbers and arithmetic at different ages, with many countries teaching addition in pre-school.<ref>\nBeckmann, S. (2014). The twenty-third ICMI study: primary mathematics study on whole numbers. International Journal of STEM Education, 1(1), 1-8.\nChicago\n</ref> However, throughout the world, addition is taught by the end of the first year of elementary school.<ref>Schmidt, W., Houang, R., & Cogan, L. (2002). \"A coherent curriculum\". ''American Educator'', 26(2), 1–18.</ref>\n\n====Addition table====\nChildren are often presented with the addition table of pairs of numbers from 1 to 10 to memorize. Knowing this, one can perform any addition.\n{{Addition table}}\n\n===Decimal system===\nThe prerequisite to addition in the [[decimal]] system is the fluent recall or derivation of the 100 single-digit \"addition facts\". One could [[memorize]] all the facts by [[rote learning|rote]], but pattern-based strategies are more enlightening and, for most people, more efficient:<ref name=\"FosnotDolk99\">Fosnot and Dolk p. 99</ref>\n* ''Commutative property'': Mentioned above, using the pattern ''a + b = b + a'' reduces the number of \"addition facts\" from 100 to 55.\n* ''One or two more'': Adding 1 or 2 is a basic task, and it can be accomplished through counting on or, ultimately, [[intuition (knowledge)|intuition]].<ref name=\"FosnotDolk99\"/>\n* ''Zero'': Since zero is the additive identity, adding zero is trivial. Nonetheless, in the teaching of arithmetic, some students are introduced to addition as a process that always increases the addends; [[word problem (mathematics education)|word problems]] may help rationalize the \"exception\" of zero.<ref name=\"FosnotDolk99\"/>\n* ''Doubles'': Adding a number to itself is related to counting by two and to [[multiplication]]. Doubles facts form a backbone for many related facts, and students find them relatively easy to grasp.<ref name=\"FosnotDolk99\"/>\n* ''Near-doubles'': Sums such as 6 + 7 = 13 can be quickly derived from the doubles fact {{nowrap|1=6 + 6 = 12}} by adding one more, or from {{nowrap|1=7 + 7 = 14}} but subtracting one.<ref name=\"FosnotDolk99\"/>\n* ''Five and ten'': Sums of the form 5 + {{mvar|x}} and 10 + {{mvar|x}} are usually memorized early and can be used for deriving other facts. For example, {{nowrap|1=6 + 7 = 13}} can be derived from {{nowrap|1=5 + 7 = 12}} by adding one more.<ref name=\"FosnotDolk99\"/>\n* ''Making ten'': An advanced strategy uses 10 as an intermediate for sums involving 8 or 9; for example, {{nowrap|1=8 + 6 = 8 + 2 + 4 =}} {{nowrap|1=10 + 4 = 14}}.<ref name=\"FosnotDolk99\"/>\nAs students grow older, they commit more facts to memory, and learn to derive other facts rapidly and fluently. Many students never commit all the facts to memory, but can still find any basic fact quickly.<ref name=Henry/>\n\n====Carry====\n{{main|Carry (arithmetic)}}\nThe standard algorithm for adding multidigit numbers is to align the addends vertically and add the columns, starting from the ones column on the right. If a column exceeds nine, the extra digit is \"[[carry (arithmetic)|carried]]\" into the next column. For example, in the addition {{nowrap|27 + 59}}\n\n   ¹\n   27\n + 59\n ————\n   86\n\n7 + 9 = 16, and the digit 1 is the carry.<ref group=lower-alpha>Some authors think that \"carry\" may be inappropriate for education; Van de Walle (p. 211) calls it \"obsolete and conceptually misleading\", preferring the word \"trade\". However, \"carry\" remains the standard term.</ref> An alternate strategy starts adding from the most significant digit on the left; this route makes carrying a little clumsier, but it is faster at getting a rough estimate of the sum. There are many alternative methods.\n\n====Addition of decimal fractions====\n[[Decimal fractions]] can be added by a simple modification of the above process.<ref>Rebecca Wingard-Nelson (2014) ''Decimals and Fractions: It's Easy'' Enslow Publishers, Inc.</ref> One aligns two decimal fractions above each other, with the decimal point in the same location. If necessary, one can add trailing zeros to a shorter decimal to make it the same length as the longer decimal. Finally, one performs the same addition process as above, except the decimal point is placed in the answer, exactly where it was placed in the summands.\n\nAs an example, 45.1 + 4.34 can be solved as follows:\n    4 5 . 1 0\n +  0 4 . 3 4\n ————————————\n    4 9 . 4 4\n\n====Scientific notation====\n{{main|Scientific notation#Basic operations}}\nIn [[scientific notation]], numbers are written in the form <math>x=a\\times10^{b}</math>, where <math>a</math> is the significand and <math>10^{b}</math> is the exponential part. Addition requires two numbers in scientific notation to be represented using the same exponential part, so that the two significands can simply be added.\n\nFor example:\n:<math>2.34\\times10^{-5} + 5.67\\times10^{-6} = 2.34\\times10^{-5} + 0.567\\times10^{-5} = 2.907\\times10^{-5}</math>\n\n===Addition in other bases===\n{{main|Binary addition}}\nAddition in other bases is very similar to decimal addition. As an example, one can consider addition in binary.<ref>Dale R. Patrick, Stephen W. Fardo, Vigyan Chandra (2008) ''Electronic Digital System Fundamentals'' The Fairmont Press, Inc. p. 155</ref> Adding two single-digit binary numbers is relatively simple, using a form of carrying:\n:0 + 0 → 0\n:0 + 1 → 1\n:1 + 0 → 1\n:1 + 1 → 0, carry 1 (since 1 + 1 = 2 = 0 + (1 × 2<sup>1</sup>))\nAdding two \"1\" digits produces a digit \"0\", while 1 must be added to the next column. This is similar to what happens in decimal when certain single-digit numbers are added together; if the result equals or exceeds the value of the radix (10), the digit to the left is incremented:\n:5 + 5 → 0, carry 1 (since 5 + 5 = 10 = 0 + (1 × 10<sup>1</sup>))\n:7 + 9 → 6, carry 1 (since 7 + 9 = 16 = 6 + (1 × 10<sup>1</sup>))\n\nThis is known as ''carrying''.<ref>P.E. Bates Bothman (1837) ''The common school arithmetic''. Henry Benton. p. 31</ref> When the result of an addition exceeds the value of a digit, the procedure is to \"carry\" the excess amount divided by the radix (that is, 10/10) to the left, adding it to the next positional value. This is correct since the next position has a weight that is higher by a factor equal to the radix. Carrying works the same way in binary:\n\n   {{brown|1 1 1 1 1    (carried digits)}}\n     0 1 1 0 1\n +   1 0 1 1 1\n —————————————\n   1 0 0 1 0 0 = 36\n\nIn this example, two numerals are being added together: 01101<sub>2</sub> (13<sub>10</sub>) and 10111<sub>2</sub> (23<sub>10</sub>). The top row shows the carry bits used. Starting in the rightmost column, {{nowrap|1=1 + 1 = 10<sub>2</sub>}}. The 1 is carried to the left, and the 0 is written at the bottom of the rightmost column. The second column from the right is added: {{nowrap|1=1 + 0 + 1 = 10<sub>2</sub>}} again; the 1 is carried, and 0 is written at the bottom. The third column: {{nowrap|1=1 + 1 + 1 = 11<sub>2</sub>}}. This time, a 1 is carried, and a 1 is written in the bottom row. Proceeding like this gives the final answer 100100<sub>2</sub> (36<sub>10</sub>).\n\n===Computers===\n[[File:Opampsumming2.svg|right|frame|Addition with an op-amp. See [[Operational amplifier applications#Summing amplifier|Summing amplifier]] for details.]]\n[[Analog computer]]s work directly with physical quantities, so their addition mechanisms depend on the form of the addends. A mechanical adder might represent two addends as the positions of sliding blocks, in which case they can be added with an [[arithmetic mean|averaging]] [[lever]]. If the addends are the rotation speeds of two [[axle|shafts]], they can be added with a [[differential (mechanics)|differential]]. A hydraulic adder can add the [[pressure]]s in two chambers by exploiting [[Newton's laws of motion|Newton's second law]] to balance forces on an assembly of [[piston]]s. The most common situation for a general-purpose analog computer is to add two [[voltage]]s (referenced to [[ground (electricity)|ground]]); this can be accomplished roughly with a [[resistor]] [[Electronic circuit|network]], but a better design exploits an [[operational amplifier]].<ref>Truitt and Rogers pp. 1;44–49 and pp. 2;77–78</ref>\n\nAddition is also fundamental to the operation of [[computer|digital computers]], where the efficiency of addition, in particular the [[carry (arithmetic)|carry]] mechanism, is an important limitation to overall performance.\n\n[[File:BabbageDifferenceEngine.jpg|left|thumb|Part of Charles Babbage's [[Difference Engine]] including the addition and carry mechanisms.]]\nThe [[abacus]], also called a counting frame, is a calculating tool that was in use centuries before the adoption of the written modern numeral system and is still widely used by merchants, traders and clerks in [[Asia]], [[Africa]], and elsewhere; it dates back to at least 2700–2300&nbsp;BC, when it was used in [[Sumer]].<ref>{{cite book |last=Ifrah |first=Georges |year=2001 |title=The Universal History of Computing: From the Abacus to the Quantum Computer |publisher=John Wiley & Sons, Inc. |location=New York |isbn=978-0-471-39671-0}} p. 11</ref>\n\n[[Blaise Pascal]] invented the mechanical calculator in 1642;<ref name=inventor>[[#MARG|Jean Marguin]], p. 48 (1994) ; Quoting [[#TATON63|René Taton]] (1963)</ref> it was the first operational [[adding machine]]. It made use of a gravity-assisted carry mechanism. It was the only operational mechanical calculator in the 17th&nbsp;century<ref>See [[Pascal's calculator#Competing designs|Competing designs]] in Pascal's calculator article</ref> and the earliest automatic, digital computer. [[Pascal's calculator]] was limited by its carry mechanism, which forced its wheels to only turn one way so it could add. To subtract, the operator had to use the [[method of complements|Pascal's calculator's complement]], which required as many steps as an addition. [[Giovanni Poleni]] followed Pascal, building the second functional mechanical calculator in 1709, a calculating clock made of wood that, once setup, could multiply two numbers automatically.\n\n[[File:Full-adder.svg|thumb|\"[[Adder (electronics)|Full adder]]\" logic circuit that adds two binary digits, ''A'' and ''B'', along with a carry input ''C<sub>in</sub>'', producing the sum bit, ''S'', and a carry output, ''C<sub>out</sub>''.]]\n[[Adder (electronics)|Adders]] execute integer addition in electronic digital computers, usually using [[binary arithmetic]]. The simplest architecture is the ripple carry adder, which follows the standard multi-digit algorithm. One slight improvement is the [[Carry bypass adder|carry skip]] design, again following human intuition; one does not perform all the carries in computing {{nowrap|999 + 1}}, but one bypasses the group of 9s and skips to the answer.<ref>Flynn and Overman pp. 2, 8</ref>\n\nIn practice, computational addition may be achieved via [[Exclusive or|XOR]] and [[Bitwise operation#AND|AND]] bitwise logical operations in conjunction with bitshift operations as shown in the pseudocode below. Both XOR and AND gates are straightforward to realize in digital logic allowing the realization of [[Adder (electronics)|full adder]] circuits which in turn may be combined into more complex logical operations. In modern digital computers, integer addition is typically the fastest arithmetic instruction, yet it has the largest impact on performance, since it underlies all [[floating point|floating-point]] operations as well as such basic tasks as [[memory address|address]] generation during [[memory (computers)|memory]] access and fetching [[instruction (computer science)|instructions]] during [[control flow|branching]]. To increase speed, modern designs calculate digits in [[parallel algorithm|parallel]]; these schemes go by such names as carry select, [[carry lookahead adder|carry lookahead]], and the [[Ling adder|Ling]] pseudocarry. Many implementations are, in fact, hybrids of these last three designs.<ref>Flynn and Overman pp. 1–9</ref><ref>Yeo, Sang-Soo, et al., eds. ''Algorithms and Architectures for Parallel Processing: 10th International Conference, ICA3PP 2010, Busan, Korea, May 21–23, 2010''. Proceedings. Vol. 1. Springer, 2010. p. 194</ref> Unlike addition on paper, addition on a computer often changes the addends. On the ancient [[abacus]] and adding board, both addends are destroyed, leaving only the sum. The influence of the abacus on mathematical thinking was strong enough that early [[Latin]] texts often claimed that in the process of adding \"a number to a number\", both numbers vanish.<ref>Karpinski pp. 102–103</ref> In modern times, the ADD instruction of a [[microprocessor]] often replaces the augend with the sum but preserves the addend.<ref>The identity of the augend and addend varies with architecture. For ADD in [[x86]] see Horowitz and Hill p. 679; for ADD in [[68k]] see p. 767.</ref> In a [[high-level programming language]], evaluating {{nowrap|''a'' + ''b''}} does not change either ''a'' or ''b''; if the goal is to replace ''a'' with the sum this must be explicitly requested, typically with the statement {{nowrap|1=''a'' = ''a'' + ''b''}}. Some languages such as [[C (programming language)|C]] or [[C++]] allow this to be abbreviated as {{nowrap|1=''a'' += ''b''}}.\n\n<source lang=\"c\">\n// Iterative Algorithm\nint add(int x, int y){\n    int carry = 0;\n    while (y != 0){      \n       carry = AND(x, y);   // Logical AND\n       x     = XOR(x, y);   // Logical XOR\n       y     = carry << 1;  // left bitshift carry by one\n   }\n   return x; \n}\n// Recursive Algorithm\nint add(int x, int y){\n   return x if (y == 0) else add(XOR(x, y) , AND(x, y) << 1);\n}\n</source>\n\nOn a computer, if the result of an addition is too large to store, an [[arithmetic overflow]] occurs, resulting in an incorrect answer. Unanticipated arithmetic overflow is a fairly common cause of [[software bug|program errors]]. Such overflow bugs may be hard to discover and diagnose because they may manifest themselves only for very large input data sets, which are less likely to be used in validation tests.<ref>Joshua Bloch, [http://googleresearch.blogspot.com/2006/06/extra-extra-read-all-about-it-nearly.html \"Extra, Extra – Read All About It: Nearly All Binary Searches and Mergesorts are Broken\"]. Official Google Research Blog, June 2, 2006.</ref> The [[Year 2000 problem]] was a series of bugs where overflow errors occurred due to use of a 2-digit format for years.<ref>{{cite web |url=http://catless.ncl.ac.uk/Risks/4.45.html |title=The Risks Digest Volume 4: Issue 45 |work=The Risks Digest}}</ref>\n\n==Addition of numbers==\nTo prove the usual properties of addition, one must first define addition for the context in question. Addition is first defined on the [[natural number]]s. In [[set theory]], addition is then extended to progressively larger sets that include the natural numbers: the [[integer]]s, the [[rational number]]s, and the [[real number]]s.<ref>[[Herbert Enderton|Enderton]] chapters 4 and 5, for example, follow this development.</ref> (In [[mathematics education]],<ref>According to a survey of the nations with highest TIMSS mathematics test scores; see Schmidt, W., Houang, R., & Cogan, L. (2002). ''A coherent curriculum''. American educator, 26(2), p. 4.</ref> positive fractions are added before negative numbers are even considered; this is also the historical route.<ref>Baez (p. 37) explains the historical development, in \"stark contrast\" with the set theory presentation: \"Apparently, half an apple is easier to understand than a negative apple!\"</ref>)\n\n===Natural numbers===\n{{further|Natural number}}\nThere are two popular ways to define the sum of two natural numbers ''a'' and ''b''. If one defines natural numbers to be the [[Cardinal number|cardinalities]] of finite sets, (the cardinality of a set is the number of elements in the set), then it is appropriate to define their sum as follows:\n* Let N(''S'') be the cardinality of a set ''S''. Take two disjoint sets ''A'' and ''B'', with {{nowrap|1=N(''A'') = ''a''}} and {{nowrap|1=N(''B'') = ''b''}}. Then {{nowrap|''a'' + ''b''}} is defined as <math> N(A \\cup B)</math>.<ref>Begle p. 49, Johnson p. 120, Devine et al. p. 75</ref>\nHere, {{nowrap|1=''A'' ∪ ''B''}} is the [[union (set theory)|union]] of ''A'' and ''B''. An alternate version of this definition allows ''A'' and ''B'' to possibly overlap and then takes their [[disjoint union]], a mechanism that allows common elements to be separated out and therefore counted twice.\n\nThe other popular definition is recursive:\n* Let ''n''<sup>+</sup> be the [[Peano axioms#Binary operations and ordering|successor]] of ''n'', that is the number following ''n'' in the natural numbers, so 0<sup>+</sup>=1, 1<sup>+</sup>=2. Define {{nowrap|1=''a'' + 0 = ''a''}}. Define the general sum recursively by {{nowrap|1=''a'' + (''b''<sup>+</sup>) = (''a'' + ''b'')<sup>+</sup>}}. Hence {{nowrap|1=1 + 1 = 1 + 0<sup>+</sup> = (1 + 0)<sup>+</sup> =}} {{nowrap|1=1<sup>+</sup> = 2}}.<ref>Enderton p. 79</ref>\nAgain, there are minor variations upon this definition in the literature. Taken literally, the above definition is an application of the [[recursion|Recursion Theorem]] on the [[partially ordered set]] '''N'''<sup>2</sup>.<ref>For a version that applies to any poset with the [[descending chain condition]], see Bergman p. 100.</ref> On the other hand, some sources prefer to use a restricted Recursion Theorem that applies only to the set of natural numbers. One then considers ''a'' to be temporarily \"fixed\", applies recursion on ''b'' to define a function \"''a''&nbsp;+\", and pastes these unary operations for all ''a'' together to form the full binary operation.<ref>Enderton (p. 79) observes, \"But we want one binary operation +, not all these little one-place functions.\"</ref>\n\nThis recursive formulation of addition was developed by Dedekind as early as 1854, and he would expand upon it in the following decades.<ref>Ferreirós p. 223</ref> He proved the associative and commutative properties, among others, through [[mathematical induction]].\n\n===Integers===\n{{further|Integer}}\nThe simplest conception of an integer is that it consists of an [[absolute value]] (which is a natural number) and a [[sign (mathematics)|sign]] (generally either [[positive number|positive]] or [[negative numbers|negative]]). The integer zero is a special third case, being neither positive nor negative. The corresponding definition of addition must proceed by cases:\n* For an integer ''n'', let |''n''| be its absolute value. Let ''a'' and ''b'' be integers. If either ''a'' or ''b'' is zero, treat it as an identity. If ''a'' and ''b'' are both positive, define {{nowrap|1=''a'' + ''b'' = {{!}}''a''{{!}} + {{!}}''b''{{!}}}}. If ''a'' and ''b'' are both negative, define {{nowrap|1=''a'' + ''b'' = −({{!}}''a''{{!}} + {{!}}''b''{{!}})}}. If ''a'' and ''b'' have different signs, define {{nowrap|''a'' + ''b''}} to be the difference between |''a''| and |''b''|, with the sign of the term whose absolute value is larger.<ref>K. Smith p. 234, Sparks and Rees p. 66</ref> As an example, {{nowrap|1=−6 + 4 = −2}}; because −6 and 4 have different signs, their absolute values are subtracted, and since the absolute value of the negative term is larger, the answer is negative.\n\nAlthough this definition can be useful for concrete problems, the number of cases to consider complicates proofs unnecessarily. So the following method is commonly used for defining integers. It is based on the remark that every integer is the difference of two natural integers and that two such differences, {{math|''a'' – ''b''}} and {{math|''c'' – ''d''}} are equal if and only if {{math|1=''a'' + ''d'' = ''b'' + ''c''}}.\nSo, one can define formally the integers as the [[equivalence class]]es of [[ordered pair]]s of natural numbers under the [[equivalence relation]]\n:{{math|(''a'', ''b'') ~ (''c'', ''d'')}} if and only if {{math|1=''a'' + ''d'' = ''b'' + ''c''}}.\nThe equivalence class of {{math|(''a'', ''b'')}} contains either {{math|(''a'' – ''b'', 0)}} if {{math|''a'' ≥ ''b''}}, or {{math|(0, ''b'' – ''a'')}} otherwise. If {{mvar|n}} is a natural number, one can denote {{math|+''n''}} the equivalence class of {{math|(''n'', 0)}}, and by {{math|–''n''}} the equivalence class of {{math|(0, ''n'')}}. This allows identifying the natural number {{mvar|n}} with the equivalence class {{math|+''n''}}.\n\nAddition of ordered pairs is done component-wise:\n:<math>\n(a, b)+(c, d)=(a+c,b+d).</math>\nA straightforward computation shows that the equivalence class of the result depends only on the equivalences classes of the summands, and thus that this defines an addition of equivalence classes, that is integers.<ref>Enderton p. 92</ref> Another straightforward computation shows that this addition is the same as the above case definition.\n\nThis way of defining integers as equivalence classes of pairs of natural numbers, can be used to embed into a [[group (mathematics)|group]] any commutative [[semigroup]] with [[cancellation property]]. Here, the semigroup is formed by the natural numbers and the group is the additive group of integers. The rational numbers are constructed similarly, by taking as semigroup the nonzero integers with multiplication. \n\nThis construction has been also generalized under the name of [[Grothendieck group]] to the case of any commutative semigroup. Without the cancellation property the [[semigroup homomorphism]] from the semigroup into the group may be non-injective. Originally, the ''Grothendieck group'' was, more specifically,  the result of this construction applied to the equivalences classes under isomorphisms of the objects of an [[abelian category]], with the [[direct sum]] as semigroup operation.\n\n===Rational numbers (fractions)===\nAddition of [[rational number]]s can be computed using the [[least common denominator]], but a conceptually simpler definition involves only integer addition and multiplication:\n* Define <math>\\frac ab + \\frac cd = \\frac{ad+bc}{bd}.</math>\n\nAs an example, the sum <math>\\frac 34 + \\frac 18 = \\frac{3 \\times 8+4 \\times 1}{4 \\times 8} = \\frac{24 + 4}{32} = \\frac{28}{32} = \\frac78</math>.\n\nAddition of fractions is much simpler when the [[denominator]]s are the same; in this case, one can simply add the numerators while leaving the denominator the same: <math>\\frac ac + \\frac bc = \\frac{a + b}{c}</math>, so <math>\\frac 14 + \\frac 24 = \\frac{1 + 2}{4} = \\frac 34</math>.<ref>Schyrlet Cameron, and Carolyn Craig (2013)''Adding and Subtracting Fractions, Grades 5–8'' Mark Twain, Inc.</ref>\n\nThe commutativity and associativity of rational addition is an easy consequence of the laws of integer arithmetic.<ref>The verifications are carried out in Enderton p. 104 and sketched for a general field of fractions over a commutative ring in Dummit and Foote p. 263.</ref> For a more rigorous and general discussion, see ''[[field of fractions]]''.\n\n===Real numbers===\n[[File:AdditionRealDedekind.svg|right|250px|thumb|Adding π<sup>2</sup>/6 and ''e'' using Dedekind cuts of rationals.]]\n{{further|Construction of the real numbers}}\nA common construction of the set of real numbers is the Dedekind completion of the set of rational numbers. A real number is defined to be a [[Dedekind cut]] of rationals: a [[non-empty set]] of rationals that is closed downward and has no [[greatest element]]. The sum of real numbers ''a'' and ''b'' is defined element by element:\n* Define <math>a+b = \\{q+r \\mid q\\in a, r\\in b\\}.</math><ref>Enderton p. 114</ref>\nThis definition was first published, in a slightly modified form, by [[Richard Dedekind]] in 1872.<ref>Ferreirós p. 135; see section 6 of ''[http://www.ru.nl/w-en-s/gmfw/bronnen/dedekind2.html Stetigkeit und irrationale Zahlen] {{webarchive |url=https://web.archive.org/web/20051031071536/http://www.ru.nl/w-en-s/gmfw/bronnen/dedekind2.html |date=2005-10-31 }}''.</ref>\nThe commutativity and associativity of real addition are immediate; defining the real number 0 to be the set of negative rationals, it is easily seen to be the additive identity. Probably the trickiest part of this construction pertaining to addition is the definition of additive inverses.<ref>The intuitive approach, inverting every element of a cut and taking its complement, works only for irrational numbers; see Enderton p. 117 for details.</ref>\n\n[[File:AdditionRealCauchy.svg|right|250px|thumb|Adding π<sup>2</sup>/6 and ''e'' using Cauchy sequences of rationals.]]\nUnfortunately, dealing with multiplication of Dedekind cuts is a time-consuming case-by-case process similar to the addition of signed integers.<ref>Schubert, E. Thomas, Phillip J. Windley, and James Alves-Foss. \"Higher Order Logic Theorem Proving and Its Applications: Proceedings of the 8th International Workshop, volume 971 of.\" ''Lecture Notes in Computer Science'' (1995).</ref> Another approach is the metric completion of the rational numbers. A real number is essentially defined to be the limit of a [[Cauchy sequence]] of rationals, lim&nbsp;''a''<sub>''n''</sub>. Addition is defined term by term:\n* Define <math>\\lim_na_n+\\lim_nb_n = \\lim_n(a_n+b_n).</math><ref>Textbook constructions are usually not so cavalier with the \"lim\" symbol; see Burrill (p. 138) for a more careful, drawn-out development of addition with Cauchy sequences.</ref>\nThis definition was first published by [[Georg Cantor]], also in 1872, although his formalism was slightly different.<ref>Ferreirós p. 128</ref>\nOne must prove that this operation is well-defined, dealing with co-Cauchy sequences. Once that task is done, all the properties of real addition follow immediately from the properties of rational numbers. Furthermore, the other arithmetic operations, including multiplication, have straightforward, analogous definitions.<ref>Burrill p. 140</ref>\n\n===Complex numbers===\n[[File:Vector Addition.svg|200px|right|thumb|Addition of two complex numbers can be done geometrically by constructing a parallelogram.]]\nComplex numbers are added by adding the real and imaginary parts of the summands.<ref>{{Citation |last=Conway |first=John B. |title=Functions of One Complex Variable I |year=1986 |publisher=Springer |isbn=978-0-387-90328-6}}</ref><ref>{{Citation |last1=Joshi |first1=Kapil D. |title=Foundations of Discrete Mathematics |publisher=[[John Wiley & Sons]] |location=New York |isbn=978-0-470-21152-6|year=1989}}</ref> That is to say:\n:<math>(a+bi) + (c+di) = (a+c) + (b+d)i.</math>\nUsing the visualization of complex numbers in the complex plane, the addition has the following geometric interpretation: the sum of two complex numbers ''A'' and ''B'', interpreted as points of the complex plane, is the point ''X'' obtained by building a [[parallelogram]] three of whose vertices are ''O'', ''A'' and ''B''. Equivalently, ''X'' is the point such that the [[triangle]]s with vertices ''O'', ''A'', ''B'', and ''X'', ''B'', ''A'', are [[Congruence (geometry)|congruent]].\n\n==Generalizations==\nThere are many binary operations that can be viewed as generalizations of the addition operation on the real numbers. The field of [[abstract algebra]] is centrally concerned with such generalized operations, and they also appear in [[set theory]] and [[category theory]].\n\n===Addition in abstract algebra===\n====Vector addition====\n{{main|Vector addition}}\nIn [[linear algebra]], a [[vector space]] is an algebraic structure that allows for adding any two [[coordinate vector|vectors]] and for scaling vectors. A familiar vector space is the set of all ordered pairs of real numbers; the ordered pair (''a'',''b'') is interpreted as a vector from the origin in the Euclidean plane to the point (''a'',''b'') in the plane. The sum of two vectors is obtained by adding their individual coordinates:\n:(''a'',''b'') + (''c'',''d'') = (''a''+''c'',''b''+''d'').\nThis addition operation is central to [[classical mechanics]], in which vectors are interpreted as [[force]]s.\n\n====Matrix addition====\n{{main|Matrix addition}}\nMatrix addition is defined for two matrices of the same dimensions. The sum of two ''m'' × ''n'' (pronounced \"m by n\") matrices '''A''' and '''B''', denoted by {{nowrap|'''A''' + '''B'''}}, is again an {{nowrap|''m'' × ''n''}} matrix computed by adding corresponding elements:<ref>Lipschutz, S., & Lipson, M. (2001). Schaum's outline of theory and problems of linear algebra. Erlangga.</ref><ref>{{cite book |title=Mathematical methods for physics and engineering |first1=K.F. |last1=Riley |first2=M.P.|last2=Hobson |first3=S.J. |last3=Bence |publisher=Cambridge University Press |year=2010 |isbn=978-0-521-86153-3}}</ref>\n\n:<math>\\begin{align}\n\\mathbf{A}+\\mathbf{B} & = \\begin{bmatrix}\n a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n \\vdots & \\vdots & \\ddots & \\vdots \\\\\n a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\\n\\end{bmatrix} +\n\n\\begin{bmatrix}\n b_{11} & b_{12} & \\cdots & b_{1n} \\\\\n b_{21} & b_{22} & \\cdots & b_{2n} \\\\\n \\vdots & \\vdots & \\ddots & \\vdots \\\\\n b_{m1} & b_{m2} & \\cdots & b_{mn} \\\\\n\\end{bmatrix} \\\\\n& = \\begin{bmatrix}\n a_{11} + b_{11} & a_{12} + b_{12} & \\cdots & a_{1n} + b_{1n} \\\\\n a_{21} + b_{21} & a_{22} + b_{22} & \\cdots & a_{2n} + b_{2n} \\\\\n \\vdots & \\vdots & \\ddots & \\vdots \\\\\n a_{m1} + b_{m1} & a_{m2} + b_{m2} & \\cdots & a_{mn} + b_{mn} \\\\\n\\end{bmatrix} \\\\\n\n\\end{align}</math>\n\nFor example:\n:<math>\n  \\begin{bmatrix}\n    1 & 3 \\\\\n    1 & 0 \\\\\n    1 & 2\n  \\end{bmatrix}\n+\n  \\begin{bmatrix}\n    0 & 0 \\\\\n    7 & 5 \\\\\n    2 & 1\n  \\end{bmatrix}\n=\n  \\begin{bmatrix}\n    1+0 & 3+0 \\\\\n    1+7 & 0+5 \\\\\n    1+2 & 2+1\n  \\end{bmatrix}\n=\n  \\begin{bmatrix}\n    1 & 3 \\\\\n    8 & 5 \\\\\n    3 & 3\n  \\end{bmatrix}\n</math>\n\n====Modular arithmetic====\n{{main|Modular arithmetic}}\nIn [[modular arithmetic]], the set of integers modulo 12 has twelve elements; it inherits an addition operation from the integers that is central to [[set theory (music)|musical set theory]]. The set of integers modulo&nbsp;2 has just two elements; the addition operation it inherits is known in [[Boolean logic]] as the \"[[exclusive or]]\" function. In [[geometry]], the sum of two [[angle|angle measures]] is often taken to be their sum as real numbers modulo&nbsp;2π. This amounts to an addition operation on the [[circle]], which in turn generalizes to addition operations on many-dimensional [[torus|tori]].\n\n====General addition====\nThe general theory of abstract algebra allows an \"addition\" operation to be any [[associative]] and [[commutative]] operation on a set. Basic [[algebraic structure]]s with such an addition operation include [[commutative monoid]]s and [[abelian group]]s.\n\n===Addition in set theory and category theory===\nA far-reaching generalization of addition of natural numbers is the addition of [[ordinal number]]s and [[cardinal number]]s in set theory. These give two different generalizations of addition of natural numbers to the [[transfinite number|transfinite]]. Unlike most addition operations, addition of ordinal numbers is not commutative. Addition of cardinal numbers, however, is a commutative operation closely related to the [[disjoint union]] operation.\n\nIn [[category theory]], disjoint union is seen as a particular case of the [[coproduct]] operation, and general coproducts are perhaps the most abstract of all the generalizations of addition. Some coproducts, such as ''[[Direct sum]]'' and ''[[Wedge sum]]'', are named to evoke their connection with addition.\n\n==Related operations==\nAddition, along with subtraction, multiplication and division, is considered one of the basic operations and is used in elementary arithmetic.\n\n===Arithmetic===\n[[Subtraction]] can be thought of as a kind of addition—that is, the addition of an [[additive inverse]]. Subtraction is itself a sort of inverse to addition, in that adding {{mvar|x}} and subtracting {{mvar|x}} are [[inverse function]]s.\n\nGiven a set with an addition operation, one cannot always define a corresponding subtraction operation on that set; the set of natural numbers is a simple example. On the other hand, a subtraction operation uniquely determines an addition operation, an additive inverse operation, and an additive identity; for this reason, an additive group can be described as a set that is closed under subtraction.<ref>The set still must be nonempty. Dummit and Foote (p. 48) discuss this criterion written multiplicatively.</ref>\n\n[[Multiplication]] can be thought of as [[Multiplication and repeated addition|repeated addition]]. If a single term {{mvar|x}} appears in a sum ''n'' times, then the sum is the product of ''n'' and {{mvar|x}}. If ''n'' is not a [[natural number]], the product may still make sense; for example, multiplication by {{num|−1}} yields the [[additive inverse]] of a number.\n\n[[File:Csl.JPG|left|thumb|A circular slide rule]]\nIn the real and complex numbers, addition and multiplication can be interchanged by the [[exponential function]]:\n:''e''<sup>''a'' + ''b''</sup> = ''e''<sup>''a''</sup> ''e''<sup>''b''</sup>.<ref>Rudin p. 178</ref>\nThis identity allows multiplication to be carried out by consulting a [[mathematical table|table]] of [[logarithm]]s and computing addition by hand; it also enables multiplication on a [[slide rule]]. The formula is still a good first-order approximation in the broad context of [[Lie group]]s, where it relates multiplication of infinitesimal group elements with addition of vectors in the associated [[Lie algebra]].<ref>Lee p. 526, Proposition 20.9</ref>\n\nThere are even more generalizations of multiplication than addition.<ref>Linderholm (p. 49) observes, \"By ''multiplication'', properly speaking, a mathematician may mean practically anything. By ''addition'' he may mean a great variety of things, but not so great a variety as he will mean by 'multiplication'.\"</ref> In general, multiplication operations always [[distributivity|distribute]] over addition; this requirement is formalized in the definition of a [[ring (mathematics)|ring]]. In some contexts, such as the integers, distributivity over addition and the existence of a multiplicative identity is enough to uniquely determine the multiplication operation. The distributive property also provides information about addition; by expanding the product {{nowrap|(1 + 1)(''a'' + ''b'')}} in both ways, one concludes that addition is forced to be commutative. For this reason, ring addition is commutative in general.<ref>Dummit and Foote p. 224. For this argument to work, one still must assume that addition is a group operation and that multiplication has an identity.</ref>\n\n[[Division (mathematics)|Division]] is an arithmetic operation remotely related to addition. Since {{nowrap|1=''a''/''b'' = ''a''(''b''<sup>−1</sup>)}}, division is right distributive over addition: {{nowrap|1=(''a'' + ''b'') / ''c'' = ''a''/''c'' + ''b''/''c''}}.<ref>For an example of left and right distributivity, see Loday, especially p. 15.</ref> However, division is not left distributive over addition; {{nowrap|1 / (2 + 2)}} is not the same as {{nowrap|1/2 + 1/2}}.\n\n===Ordering===\n[[File:XPlusOne.svg|right|thumb|[[Log-log plot]] of {{nowrap|1={{mvar|x}} + 1}} and {{nowrap|1=max ({{mvar|x}}, 1)}} from {{mvar|x}} = 0.001 to 1000<ref>Compare Viro Figure 1 (p. 2)</ref>]]\nThe maximum operation \"max (''a'', ''b'')\" is a binary operation similar to addition. In fact, if two nonnegative numbers ''a'' and ''b'' are of different [[orders of magnitude]], then their sum is approximately equal to their maximum. This approximation is extremely useful in the applications of mathematics, for example in truncating [[Taylor series]]. However, it presents a perpetual difficulty in [[numerical analysis]], essentially since \"max\" is not invertible. If ''b'' is much greater than ''a'', then a straightforward calculation of {{nowrap|(''a'' + ''b'') − ''b''}} can accumulate an unacceptable [[round-off error]], perhaps even returning zero. See also ''[[Loss of significance]]''.\n\nThe approximation becomes exact in a kind of infinite limit; if either ''a'' or ''b'' is an infinite [[cardinal number]], their cardinal sum is exactly equal to the greater of the two.<ref>Enderton calls this statement the \"Absorption Law of Cardinal Arithmetic\"; it depends on the comparability of cardinals and therefore on the [[Axiom of Choice]].</ref> Accordingly, there is no subtraction operation for infinite cardinals.<ref>Enderton p. 164</ref>\n\nMaximization is commutative and associative, like addition. Furthermore, since addition preserves the ordering of real numbers, addition distributes over \"max\" in the same way that multiplication distributes over addition:\n:''a'' + max (''b'', ''c'') = max (''a'' + ''b'', ''a'' + ''c'').\nFor these reasons, in [[tropical geometry]] one replaces multiplication with addition and addition with maximization. In this context, addition is called \"tropical multiplication\", maximization is called \"tropical addition\", and the tropical \"additive identity\" is [[extended real number line|negative infinity]].<ref>Mikhalkin p. 1</ref> Some authors prefer to replace addition with minimization; then the additive identity is positive infinity.<ref>Akian et al. p. 4</ref>\n\nTying these observations together, tropical addition is approximately related to regular addition through the [[logarithm]]:\n:log (''a'' + ''b'') ≈ max (log ''a'', log ''b''),\nwhich becomes more accurate as the base of the logarithm increases.<ref>Mikhalkin p. 2</ref> The approximation can be made exact by extracting a constant ''h'', named by analogy with [[Planck's constant]] from [[quantum mechanics]],<ref>Litvinov et al. p. 3</ref> and taking the \"[[classical limit]]\" as ''h'' tends to zero:\n:<math>\\max(a,b) = \\lim_{h\\to 0}h\\log(e^{a/h}+e^{b/h}).</math>\nIn this sense, the maximum operation is a ''dequantized'' version of addition.<ref>Viro p. 4</ref>\n\n===Other ways to add===\nIncrementation, also known as the [[Primitive recursive function|successor]] operation, is the addition of {{num|1}} to a number.\n\n[[Summation]] describes the addition of arbitrarily many numbers, usually more than just two. It includes the idea of the sum of a single number, which is itself, and the [[empty sum]], which is [[0 (number)|zero]].<ref>Martin p. 49</ref> An infinite summation is a delicate procedure known as a [[series (mathematics)|series]].<ref>Stewart p. 8</ref>\n\n[[Counting]] a finite set is equivalent to summing 1 over the set.\n\n[[Integral|Integration]] is a kind of \"summation\" over a [[Continuum (set theory)|continuum]], or more precisely and generally, over a [[differentiable manifold|differentiable]] [[manifold]]. Integration over a zero-dimensional manifold reduces to summation.\n\n[[Linear combination]]s combine multiplication and summation; they are sums in which each term has a multiplier, usually a [[real numbers|real]] or [[complex numbers|complex]] number. Linear combinations are especially useful in contexts where straightforward addition would violate some normalization rule, such as [[mixed strategy|mixing]] of [[strategy (game theory)|strategies]] in [[game theory]] or [[quantum superposition|superposition]] of [[quantum state|states]] in [[quantum mechanics]].\n\n[[Convolution]] is used to add two independent [[random variable]]s defined by [[probability distribution|distribution functions]]. Its usual definition combines integration, subtraction, and multiplication. In general, convolution is useful as a kind of domain-side addition; by contrast, vector addition is a kind of range-side addition.\n\n==Notes==\n{{notelist}}\n\n==Footnotes==\n{{reflist}}\n\n==References==\n{{refbegin}}\n'''History'''\n* {{cite book |first=José |last=Ferreirós |title=Labyrinth of Thought: A History of Set Theory and Its Role in Modern Mathematics |publisher=Birkhäuser |year=1999 |isbn=978-0-8176-5749-9}}\n* {{cite book |first=Louis |last=Karpinski |authorlink=Louis Charles Karpinski |title=The History of Arithmetic |publisher=Rand McNally |year=1925 |id={{LCC|QA21.K3}}}}\n* {{cite book |first=Steven |last=Schwartzman |title=The Words of Mathematics: An Etymological Dictionary of Mathematical Terms Used in English |publisher=[[Mathematical Association of America|MAA]] |year=1994 |isbn=978-0-88385-511-9}}\n* {{cite book |first=Michael |last=Williams |title=A History of Computing Technology |publisher=Prentice-Hall |year=1985 |isbn=978-0-13-389917-7}}\n\n'''Elementary mathematics'''\n* {{cite book |author1=Sparks, F. |author2=Rees C. |title=A Survey of Basic Mathematics |publisher=McGraw-Hill |year=1979 |isbn=978-0-07-059902-4}}\n\n'''Education'''\n* {{cite book |first=Edward |last=Begle |title=The Mathematics of the Elementary School |publisher=[[McGraw-Hill]] |year=1975 |isbn=978-0-07-004325-1}}\n* [https://web.archive.org/web/20051228115904/http://www.cde.ca.gov/be/st/ss/mthmain.asp California State Board of Education mathematics content standards] Adopted December 1997, accessed December 2005.\n* {{cite book |author1=Devine, D. |author2=Olson, J. |author3=Olson, M. |title=Elementary Mathematics for Teachers |edition=2e |publisher=[[John Wiley & Sons|Wiley]] |year=1991 |isbn=978-0-471-85947-5}}\n* {{cite book |author=National Research Council |title=Adding It Up: Helping Children Learn Mathematics |publisher=[[United States National Academies|National Academy Press]] |year=2001 |isbn=978-0-309-06995-3|authorlink=United States National Research Council |url=http://www.nap.edu/books/0309069955/html/index.html|doi=10.17226/9822 }}\n* {{cite book |first=John |last=Van de Walle |title=Elementary and Middle School Mathematics: Teaching developmentally |edition=5e |publisher=Pearson |year=2004 |isbn=978-0-205-38689-5}}\n\n'''Cognitive science'''\n* {{cite book |last1=Fosnot |first1=Catherine T. |last2=Dolk |first2=Maarten |title=Young Mathematicians at Work: Constructing Number Sense, Addition, and Subtraction |publisher=Heinemann |year=2001 |isbn=978-0-325-00353-5}}\n* {{cite conference |first=Karen |last=Wynn |booktitle=The Development of Mathematical Skills. |title=Numerical competence in infants |publisher=Taylor & Francis |year=1998 |pages= |isbn=0-86377-816-X}}\n\n'''Mathematical exposition'''\n* {{cite web |author=Bogomolny, Alexander |year=1996 |title=Addition |work=Interactive Mathematics Miscellany and Puzzles (cut-the-knot.org) |url=http://www.cut-the-knot.org/do_you_know/addition.shtml |accessdate=3 February 2006 |archiveurl=https://web.archive.org/web/20060426110928/http://www.cut-the-knot.org/do_you_know/addition.shtml |archivedate=April 26, 2006<!--DASHBot--> |deadurl=no}}\n* {{cite book |first=William |last=Dunham |title=The Mathematical Universe |publisher=Wiley |year=1994 |isbn=978-0-471-53656-7}}\n* {{cite book |first=Paul |last=Johnson |title=From Sticks and Stones: Personal Adventures in Mathematics |publisher=Science Research Associates |year=1975 |isbn=978-0-574-19115-1}}\n* {{cite book |first=Carl |last=Linderholm |year=1971 |title=Mathematics Made Difficult |publisher=Wolfe |isbn=978-0-7234-0415-6|title-link=Mathematics Made Difficult }}\n* {{cite book |first=Frank |last=Smith |title=The Glass Wall: Why Mathematics Can Seem Difficult |publisher=Teachers College Press |year=2002 |isbn=978-0-8077-4242-6}}\n* {{cite book |first=Karl |last=Smith |title=The Nature of Modern Mathematics |edition=3rd |publisher=Wadsworth |year=1980 |isbn=978-0-8185-0352-8}}\n\n'''Advanced mathematics'''\n* {{cite book |first=George |last=Bergman |title=An Invitation to General Algebra and Universal Constructions |edition=2.3 |publisher=General Printing |year=2005 |isbn=978-0-9655211-4-7 |url=http://math.berkeley.edu/~gbergman/245/index.html}}\n* {{cite book |first=Claude |last=Burrill |title=Foundations of Real Numbers |publisher=McGraw-Hill |year=1967 |id={{LCC|QA248.B95}}}}\n* {{cite book |author1=Dummit, D. |author2=Foote, R. |title=Abstract Algebra |edition=2 |publisher=Wiley |year=1999 |isbn=978-0-471-36857-1}}\n* {{cite book |first=Herbert |last=Enderton |title=Elements of Set Theory |publisher=[[Academic Press]] |year=1977 |isbn=978-0-12-238440-0}}\n* {{cite book |first=John |last=Lee |title=Introduction to Smooth Manifolds |publisher=Springer |year=2003 |isbn=978-0-387-95448-6}}\n* {{cite book |first=John |last=Martin |title=Introduction to Languages and the Theory of Computation |publisher=McGraw-Hill |edition=3 |year=2003 |isbn=978-0-07-232200-2}}\n* {{cite book |first=Walter |last=Rudin |title=Principles of Mathematical Analysis |edition=3 |publisher=McGraw-Hill |year=1976 |isbn=978-0-07-054235-8}}\n* {{cite book |first=James |last=Stewart |title=Calculus: Early Transcendentals |edition=4 |publisher=Brooks/Cole |year=1999 |isbn=978-0-534-36298-0}}\n\n'''Mathematical research'''\n* {{cite journal |author1=Akian, Marianne |author2=Bapat, Ravindra |author3=Gaubert, Stephane |title=Min-plus methods in eigenvalue perturbation theory and generalised Lidskii-Vishik-Ljusternik theorem |journal=INRIA Reports |year=2005 |arxiv=math.SP/0402090|bibcode=2004math......2090A }}\n* {{cite conference |author=[[John C. Baez|Baez, J.]] |author2=Dolan, J. |booktitle=Mathematics Unlimited – 2001 and Beyond. From Finite Sets to Feynman Diagrams |year=2001 |page=29 |arxiv=math.QA/0004133 |isbn=3-540-66913-2}}\n* Litvinov, Grigory; Maslov, Victor; Sobolevskii, Andreii (1999). [https://arxiv.org/abs/math.SC/9911126 Idempotent mathematics and interval analysis]. ''[http://www.springerlink.com/openurl.asp?genre=article&eissn=1573-1340&volume=7&issue=5&spage=353 Reliable Computing]'', Kluwer.\n* {{cite journal |first=Jean-Louis |last=Loday |title= Arithmetree |journal=Journal of Algebra |year=2002 |arxiv=math/0112034 |doi=10.1016/S0021-8693(02)00510-0 |volume=258 |page=275}}\n* {{cite book |last=Mikhalkin |first=Grigory |year=2006 |arxiv=math.AG/0601041 |zbl=1103.14034 |editor1-last=Sanz-Solé |editor1-first=Marta |title=Proceedings of the International Congress of Mathematicians (ICM), Madrid, Spain, August 22–30, 2006. Volume II: Invited lectures. Tropical Geometry and its Applications |location=Zürich |publisher=[[European Mathematical Society]] |isbn=978-3-03719-022-7 |pages=827–852}}\n* {{Cite book |last=Viro |first=Oleg |year=2001 |url=http://www.math.uu.se/~oleg/dequant/dequantH1.html |title=European Congress of Mathematics: Barcelona, July 10–14, 2000, Volume I. Dequantization of Real Algebraic Geometry on Logarithmic Paper |editor1-first=Carles |editor1-last=Cascuberta |editor2-first=Rosa Maria |editor2-last=Miró-Roig |editor3-first=Joan |editor3-last=Verdera |editor4-first=Sebastià |editor4-last=Xambó-Descamps |publisher=Birkhäuser |location=Basel |isbn=978-3-7643-6417-5 |series=Progress in Mathematics |volume=201 |pages=135–146 |arxiv=math/0005163 |zbl=1024.14026 |bibcode=2000math......5163V }}\n\n'''Computing'''\n* {{cite book |author1=Flynn, M. |author2=Oberman, S. |title=Advanced Computer Arithmetic Design |publisher=Wiley |year=2001 |isbn=978-0-471-41209-0}}\n* {{cite book |author1=Horowitz, P. |author2=Hill, W. |title=The Art of Electronics |edition=2 |publisher=Cambridge UP |year=2001 |isbn=978-0-521-37095-0}}\n* {{cite book |first=Albert |last=Jackson |title=Analog Computation |publisher=McGraw-Hill |year=1960 |id={{LCC|QA76.4|J3}}}}\n* {{cite book |author1=Truitt, T. |author2=Rogers, A. |title=Basics of Analog Computers |publisher=John F. Rider |year=1960 |id={{LCC|QA76.4|T7}}}}\n* {{cite book |ref=MARG |language=fr |title=Histoire des Instruments et Machines à Calculer, Trois Siècles de Mécanique Pensante 1642–1942 |first=Jean |last=Marguin |year=1994 |publisher=Hermann |location= |isbn=978-2-7056-6166-3}}\n* {{cite book |ref=TATON63 |language=fr |title=Le Calcul Mécanique. Que Sais-Je ? n° 367 |first=René |last=Taton |year=1963 |pages=20–28 |publisher=Presses universitaires de France |location= |isbn=}}\n{{refend}}\n\n==Further reading==\n* {{cite conference |last1=Baroody |first1=Arthur |last2=Tiilikainen |first2=Sirpa |booktitle=The Development of Arithmetic Concepts and Skills. Two perspectives on addition development |year=2003 |page=75 |isbn=0-8058-3155-X |publisher=Routledge}}\n* {{cite book |last1=Davison |first1=David M. |last2=Landau |first2=Marsha S. |last3=McCracken |first3=Leah |last4=Thompson |first4=Linda |title=Mathematics: Explorations & Applications |edition=TE |publisher=Prentice Hall |year=1999 |isbn=978-0-13-435817-8}}\n* {{cite book |first1=Lucas N.H. |last1=Bunt |first2=Phillip S. |last2=Jones |first3=Jack D. |last3=Bedient |title=The Historical roots of Elementary Mathematics |publisher=Prentice-Hall |year=1976 |isbn=978-0-13-389015-0}}\n* {{cite journal |last=Poonen |first=Bjorn |year=2010 |title=Addition |url=http://www.girlsangle.org/page/bulletin.php |journal=Girls' Angle Bulletin  |volume=3 |issue=3–5 |issn=2151-5743}}\n* {{cite conference |first=J. Fred |last=Weaver |booktitle=Addition and Subtraction: A Cognitive Perspective. Interpretations of Number Operations and Symbolic Representations of Addition and Subtraction |year=1982 |page=60 |isbn=0-89859-171-6 |publisher=Taylor & Francis}}\n\n{{Elementary arithmetic}}\n{{Hyperoperations}}\n\n[[Category:Elementary arithmetic]]\n[[Category:Binary operations]]\n[[Category:Mathematical notation]]\n[[Category:Addition| ]]"
    },
    {
      "title": "Band sum",
      "url": "https://en.wikipedia.org/wiki/Band_sum",
      "text": "In [[geometric topology]], a '''band sum''' of two ''n''-dimensional knots ''K''<sub>1</sub> and ''K''<sub>2</sub> along an (''n''&nbsp;+&nbsp;1)-dimensional 1-handle ''h'' called a ''band''  is an ''n''-dimensional knot ''K'' such that:\n\n* There is an (''n''&nbsp;+&nbsp;1)-dimensional 1-handle ''h'' connected to (''K''<sub>1</sub>,&nbsp;''K''<sub>2</sub>) embedded in ''S''<sup>''n''+2</sup>.\n* There are points <math>p_1\\in K_1</math> and <math>p_2\\in K_2</math> such that <math>h</math> is attached to <math>K_1\\sqcup K_2</math> along <math>p_1\\sqcup p_2</math>.\n\n''K'' is the ''n''-dimensional knot obtained by this surgery.\n\nA band sum is thus a generalization of the usual [[connected sum]] of knots.\n\n==See also==\n*[[Manifold decomposition]]\n\n==References==\n*{{citation|title=Knots and Links|first=Peter R.|last=Cromwell|publisher=Cambridge University Press|year=2004|isbn=9780521548311|page=90|url=https://books.google.com/books?id=djvbTNR2dCwC&pg=PA90}}.\n*{{citation|title=Survey on Knot Theory|first=Akio|last=Kawauchi|publisher=Springer|year=1996|isbn=9783764351243|page=31|url=https://books.google.com/books?id=gWbyJn7c5G0C&pg=PA31}}.\n\n[[Category:Topology]]\n[[Category:Differential topology]]\n[[Category:Knot theory]]\n[[Category:Binary operations]]\n\n\n{{knottheory-stub}}"
    },
    {
      "title": "Box topology",
      "url": "https://en.wikipedia.org/wiki/Box_topology",
      "text": "In [[topology]], the [[cartesian product]] of [[topological space]]s can be given several different topologies. One of the more obvious choices is the '''box topology''', where a [[Base (topology)|base]] is given by the Cartesian products of open sets in the component spaces.<ref>Willard, 8.2 pp. 52&ndash;53,</ref> Another possibility is the [[product topology]], where a base is given by the Cartesian products of open sets in the component spaces, only finitely many of which can be not equal to the entire component space.\n\nWhile the box topology has a somewhat more intuitive definition than the product topology, it satisfies fewer desirable properties. In particular, if all the component spaces are [[compact space|compact]], the box topology on their Cartesian product will not necessarily be compact, although the product topology on their Cartesian product will always be compact. In general, the box topology is [[finer topology|finer]] than the product topology, although the two agree in the case of [[wiktionary:finite|finite]] direct products (or when all but finitely many of the factors are [[trivial topology|trivial]]).\n\n==Definition==\nGiven <math>X</math> such that\n\n:<math>X := \\prod_{i \\in I} X_i,</math>\n\nor the (possibly infinite) Cartesian product of the topological spaces <math>X_i</math>, [[index set|indexed]] by <math>i \\in I</math>, the '''box topology''' on <math>X</math> is generated by the [[basis (topology)|base]]\n\n:<math>B = \\left\\{ \\left.\\prod_{i \\in I} U_i \\ \\right| U_i \\text{ open in } X_i \\right\\}.</math>\n\nThe name ''box'' comes from  the case of '''R'''<sup>''n''</sup>, the basis sets look like boxes or unions thereof.\n\n==Properties==\n\nBox topology on '''R'''<sup>''ω''</sup>:<ref>Steen, Seebach, 109. pp. 128&ndash;129.</ref>\n\n* The box topology is [[completely regular]]\n* The box topology is neither [[compact space|compact]] nor [[Connected space|connected]]\n* The box topology is not [[first countable]] (hence not [[metrizable]])\n* The box topology is not [[separable space|separable]]\n* The box topology is [[paracompact]] (and hence normal and completely regular) if the [[continuum hypothesis]] is true\n \n===Example - Failure at continuity===\nThe following example is based on the [[Hilbert cube]]. Let '''R'''<sup>''ω''</sup> denote the countable cartesian product of '''R''' with itself, i.e. the set of all [[sequence]]s in '''R'''. Equip '''R''' with the [[Real line#As a topological space|standard topology]] and '''R'''<sup>''ω''</sup> with the box topology. Define:\n\n:<math>\\begin{cases} f : \\mathbf{R} \\to \\mathbf{R}^\\omega \\\\ x \\mapsto (x,x,x, \\ldots) \\end{cases}</math>\n\nSo all the component functions are the identity and hence continuous, however we will show ''f'' is not continuous. To see this, consider the open set \n\n:<math> U = \\prod_{n=1}^{\\infty} \\left ( -\\tfrac{1}{n}, \\tfrac{1}{n} \\right ).</math>\n\nSuppose ''f'' were continuous. Then, since:\n\n:<math>f(0) = (0,0,0, \\ldots ) \\in U,</math>\n\nthere should exist <math>\\varepsilon > 0</math> such that <math>(-\\varepsilon, \\varepsilon) \\subset f^{-1}(U).</math> But this would imply that \n\n:<math> f\\left (\\tfrac{\\varepsilon}{2} \\right ) = \\left ( \\tfrac{\\varepsilon}{2}, \\tfrac{\\varepsilon}{2}, \\tfrac{\\varepsilon}{2}, \\ldots \\right ) \\in U,</math>\n\nwhich is false since <math>\\tfrac{\\varepsilon}{2} > \\tfrac{1}{n}</math> for <math>n > \\tfrac{2}{\\varepsilon}.</math> Thus ''f'' is not continuous even though all its component functions are.\n\n===Example - Failure at compactness===\nConsider the countable product <math>X = \\prod X_i</math> where for each ''i'', <math>X_i = \\{0,1\\}</math> with the discrete topology. The box topology on <math>X</math> will also be the discrete topology. Consider the sequence <math>\\{x_n\\}_{n=1}^\\infty</math> given by \n:<math>(x_n)_m=\\begin{cases}\n  0 & m < n \\\\\n  1 & m \\ge n \n\\end{cases}</math>\nSince no two points in the sequence are the same, the sequence has no limit point, and therefore <math>X</math> is not compact, even though its component spaces are.\n\n===Convergence in the box topology===\nTopologies are often best understood by describing how sequences converge. In general, a Cartesian product of a space <math>X</math> with itself over an [[index set|indexing set]] <math>S</math> is precisely the space of functions from <math>S</math> to <math>X</math>'','' denoted <math display=\"inline\">\\prod_{s \\in S} X = X^S</math>. The product topology yields the topology of [[pointwise convergence]]; sequences of functions converge if and only if they converge at every point of <math>S</math>.\n\nBecause the box topology is finer than the product topology, convergence of a sequence in the box topology is a more stringent condition.  Assuming <math>X</math> is Hausdorff, a sequence <math>(f_n)_n</math> of functions in <math>X^S</math> converges in the box topology to a function <math>f\\in X^S</math> if and only if it converges pointwise to <math>f</math> and \nthere is a finite subset <math>S_0\\subset S</math> and there is an <math>N</math> such that for all <math>n>N</math> the sequence <math>(f_n(s))_n</math> in <math>X</math> is constant for all <math>s\\in S\\setminus S_0</math>.  In other words, the sequence <math>(f_n(s))_n</math> is eventually constant for nearly all <math>s</math> and in a uniform way.<ref>{{cite web|last1=Scott|first1=Brian M.|title=Difference between the behavior of a sequence and a function in product and box topology on same set|url=https://math.stackexchange.com/q/448575|website=math.stackexchange.com}}</ref>\n\n==Comparison with product topology==\n\nThe basis sets in the product topology have almost the same definition as the above, ''except'' with the qualification that ''all but finitely many'' ''U<sub>i</sub>'' are equal to the component space ''X<sub>i</sub>''.  The product topology satisfies a very desirable property for maps ''f<sub>i</sub>'' : ''Y'' → ''X<sub>i</sub>'' into the component spaces: the product map  ''f'': ''Y'' → ''X'' defined by the component functions ''f'' is [[continuous function (topology)|continuous]] if and only if all the ''f<sub>i</sub>'' are continuous.  As shown above, this does not always hold in the box topology. This actually makes the box topology very useful for providing [[counterexample]]s&mdash;many qualities such as [[compact space|compactness]], [[connected space|connectedness]], metrizability, etc., if possessed by the factor spaces, are not in general preserved in the product with this topology. <!-- (''More specific examples here would be useful...'') -->\n\n==See also==\n* [[Cylinder set]]\n\n==Notes==\n{{reflist}}\n\n==References==\n* [[Lynn Arthur Steen|Steen, Lynn A.]] and [[J. Arthur Seebach, Jr.|Seebach, J. Arthur Jr.]]; ''[[Counterexamples in Topology]]'', Holt, Rinehart and Winston (1970). {{ISBN|0030794854}}.\n*{{cite book | author=Willard, Stephen | title=General Topology | publisher=Dover Publications | year=2004 | isbn=0-486-43479-6}}\n\n==External links==\n* {{planetmath reference|id=3095|title=Box topology}}\n\n{{DEFAULTSORT:Box Topology}}\n[[Category:Topological spaces]]\n[[Category:Binary operations]]"
    },
    {
      "title": "Cap product",
      "url": "https://en.wikipedia.org/wiki/Cap_product",
      "text": "{{No footnotes|date=September 2010}}\n\nIn [[algebraic topology]] the '''cap product''' is a method of adjoining a [[chain (algebraic topology)|chain]] of degree ''p'' with a [[cochain]] of degree ''q'', such that ''q'' ≤ ''p'', to form a composite chain of degree ''p'' − ''q''.   It was introduced by [[Eduard Čech]] in 1936, and independently by [[Hassler Whitney]] in 1938.\n\n==Definition==\nLet ''X'' be a [[topological space]] and ''R'' a coefficient ring. The cap product is a [[bilinear map]] on [[singular homology]] and [[cohomology]]\n:<math>\\frown\\;: H_p(X;R)\\times H^q(X;R) \\rightarrow H_{p-q}(X;R).</math>\n\ndefined by contracting a [[singular chain]] <math>\\sigma : \\Delta\\ ^p \\rightarrow\\ X</math> with a singular [[cochain]] <math> \\psi \\in C^q(X;R), </math> by the formula :\n\n:<math> \\sigma \\frown \\psi = \\psi(\\sigma|_{[v_0, \\ldots, v_q]}) \\sigma|_{[v_q, \\ldots, v_p]}.</math>\n\nHere, the notation <math>\\sigma|_{[v_0, \\ldots, v_q]}</math> indicates the restriction of the simplicial map <math>\\sigma</math> to its face spanned by the vectors of the base, see [[Simplex]].\n\n==Interpretation==\nIn analogy with the interpretation of the [[cup product]] in terms of the [[Künneth formula]], we can explain the existence of the cap product by considering the composition\n\n<math> C_\\bullet(X) \\otimes C^\\bullet(X) \\overset{\\Delta_* \\otimes \\mathrm{Id}}{\\longrightarrow} C_\\bullet(X) \\otimes C_\\bullet(X) \\otimes C^\\bullet(X) \\overset{\\mathrm{Id} \\otimes \\varepsilon}{\\longrightarrow} C_\\bullet(X) </math>\n\nin terms of the [[chain complex|chain]] and cochain complexes of <math>X</math>, where we are taking [[Künneth theorem|tensor products of chain complexes]], <math> \\Delta \\colon X \\to X \\times X</math> is the [[diagonal functor|diagonal map]] which induces the map <math>\\Delta_*</math> on the chain complex, and <math>\\varepsilon \\colon C_p(X) \\otimes C^q(X) \\to \\mathbb{Z}</math> is the [[evaluation map]] (always 0 except for <math>p=q</math>).\n\nThis composition then passes to the quotient to define the cap product <math> \\frown \\colon H_\\bullet(X) \\times H^\\bullet(X) \\to H_\\bullet(X)</math>, and looking carefully at the above composition shows that it indeed takes the form of maps <math> \\frown \\colon H_p(X) \\times H^q(X) \\to H_{p-q}(X)</math>, which is always zero for <math>p < q</math>.\n\n==The slant product==\n\nThe above discussion indicates that the same operation can be defined on [[cartesian product]]s <math>X\\times Y</math> yielding a product\n:<math>\\backslash\\;: H_p(X;R)\\otimes H^q(X\\times Y;R) \\rightarrow H^{q-p}(Y;R).</math>\n\nIn case ''X = Y'', the two products are related by the diagonal map.\n\n==Equations==\nThe boundary of a cap product is given by :\n\n:<math>\\partial(\\sigma \\frown \\psi) = (-1)^q(\\partial \\sigma \\frown \\psi - \\sigma \\frown \\delta \\psi). </math>\n\nGiven a map ''f'' the induced maps satisfy :\n\n:<math> f_*( \\sigma ) \\frown \\psi = f_*(\\sigma \\frown f^* (\\psi)). </math>\n\nThe cap and [[cup product]] are related by :\n\n:<math> \\psi(\\sigma \\frown \\varphi) = (\\varphi \\smile \\psi)(\\sigma)</math>\n\nwhere\n\n:<math>\\sigma : \\Delta ^{p+q} \\rightarrow X</math> ,  <math> \\psi \\in C^q(X;R)</math> and <math> \\varphi \\in C^p(X;R). </math>\n\nAn interesting consequence of the last equation is that it makes <math>H_{\\ast}(X;R)</math> into a right <math>H^{\\ast}(X;R)-</math> [[module (mathematics)|module]].\n\n==See also==\n*[[cup product]]\n*[[Poincaré duality]]\n*[[singular homology]]\n*[[homology theory]]\n\n==References==\n*[[Allen Hatcher|Hatcher, A.]], ''[http://www.math.cornell.edu/~hatcher/AT/ATchapters.html Algebraic Topology],'' [[Cambridge University Press]] (2002) {{ISBN|0-521-79540-0}}. Detailed discussion of homology theories for simplicial complexes and manifolds, singular homology, etc.\n*{{nlab|id=slant+product|title=slant product}}\n\n{{DEFAULTSORT:Cap Product}}\n[[Category:Homology theory]]\n[[Category:Algebraic topology]]\n[[Category:Binary operations]]"
    },
    {
      "title": "Cartesian product",
      "url": "https://en.wikipedia.org/wiki/Cartesian_product",
      "text": "{{Use mdy dates|date=August 2017}}\n{{Redirect|Cartesian square|Cartesian squares in category theory|Cartesian square (category theory)}}\n[[File:Cartesian Product qtl1.svg|thumb|Cartesian product <math>\\scriptstyle A \\times B</math> of the sets <math>\\scriptstyle A=\\{x,y,z\\}</math> and <math>\\scriptstyle B=\\{1,2,3\\}</math>]]\n\nIn [[set theory]] (and, usually, in other parts of [[mathematics]]), a '''Cartesian product''' is a [[mathematical operation]] that returns a [[set (mathematics)|set]] (or '''product set''' or simply '''product''') from multiple sets. That is, for sets ''A'' and ''B'', the Cartesian product {{nowrap|''A'' × ''B''}} is the set of all [[ordered pair]]s {{nowrap|(''a'', ''b'')}} where {{nowrap|''a'' ∈ ''A''}} and {{nowrap|''b'' ∈ ''B''}}. Products can be specified using [[set-builder notation]], e.g.\n: <math>A\\times B = \\{\\,(a,b)\\mid a\\in A \\ \\mbox{ and } \\ b\\in B\\,\\}.</math><ref>{{cite book|last=Warner|first=S.|title=Modern Algebra|page=6|publisher=[[Dover Publications]]|date=1990}}</ref>\n\nA table can be created by taking the Cartesian product of a set of rows and a set of columns. If the Cartesian product {{nowrap|''rows'' × ''columns''}} is taken, the cells of the table contain ordered pairs of the form {{nowrap|(row value, column value)}}.\n\nMore generally, a Cartesian product of ''n'' sets, also known as an '''''n''-fold Cartesian product''', can be represented by an array of ''n'' dimensions, where each element is an ''n''-[[tuple]]. An ordered pair is a [[Tuple#Names for tuples of specific lengths|2-tuple or couple]].\n\nThe Cartesian product is named after [[René Descartes]],<ref>{{cite web|title=Cartesian|date=2009|website=Merriam-Webster.com|accessdate=December 1, 2009|url=http://www.merriam-webster.com/dictionary/cartesian}}</ref> whose formulation of [[analytic geometry]] gave rise to the concept, which is further generalized in terms of [[direct product]].\n\n== Examples ==\n\n=== A deck of cards ===\n[[File:Piatnikcards.jpg|thumb|Standard 52-card deck]]\n\nAn illustrative example is the [[standard 52-card deck]]. The [[Playing cards#Anglo-American|standard playing card]] ranks {A, K, Q, J, 10, 9, 8, 7, 6, 5, 4, 3, 2} form a 13-element set. The card suits {{nowrap|{♠, {{color|#c00000|♥}}, {{color|#c00000|♦}}, ♣} }} form a four-element set. The Cartesian product of these sets returns a 52-element set consisting of 52 [[ordered pairs]], which correspond to all 52 possible playing cards. \n\n{{nowrap|''Ranks'' × ''Suits''}} returns a set of the form {(A, ♠), (A, {{color|#c00000|♥}}), (A, {{color|#c00000|♦}}), (A, ♣), (K, ♠), ..., (3, ♣), (2, ♠), (2, {{color|#c00000|♥}}), (2, {{color|#c00000|♦}}), (2, ♣)}.\n\n{{nowrap|''Suits'' × ''Ranks''}} returns a set of the form {(♠, A), (♠, K), (♠, Q), (♠, J), (♠, 10), ..., (♣, 6), (♣, 5), (♣, 4), (♣, 3), (♣, 2)}.\n\nBoth sets are distinct, even disjoint.\n\n=== A two-dimensional coordinate system ===\n[[File:Cartesian-coordinate-system.svg|thumb|Cartesian coordinates of example points]]\n\nThe main historical example is the [[Cartesian plane]] in [[analytic geometry]]. In order to represent geometrical shapes in a numerical way and extract numerical information from shapes' numerical representations, [[René Descartes]] assigned to each point in the plane a pair of [[real number]]s, called its coordinates. Usually, such a pair's first and second components are called its ''x'' and ''y'' coordinates, respectively (see picture). The set of all such pairs (i.e. the Cartesian product {{nowrap|ℝ×ℝ}} with ℝ denoting the real numbers) is thus assigned to the set of all points in the plane.\n\n== Most common implementation (set theory) ==\n{{Main article|Implementation of mathematics in set theory}}\n\nA formal definition of the Cartesian product from [[set theory|set-theoretical]] principles follows from a definition of [[ordered pair]]. The most common definition of ordered pairs, the [[Ordered pair#Kuratowski definition|Kuratowski definition]], is <math>(x, y) = \\{\\{x\\},\\{x, y\\}\\}</math>. Under this definition, <math>(x, y)</math> is an element of <math>\\mathcal{P}(\\mathcal{P}(X \\cup Y))</math>, and <math>X\\times Y</math> is a subset of that set, where <math>\\mathcal{P}</math> represents the [[power set]] operator. Therefore, the existence of the Cartesian product of any two sets in [[ZFC]] follows from the axioms of [[axiom of pairing|pairing]], [[axiom of union|union]], [[axiom of power set|power set]], and [[axiom schema of specification|specification]]. Since [[function (mathematics)|functions]] are usually defined as a special case of [[relation (mathematics)|relations]], and relations are usually defined as subsets of the Cartesian product, the definition of the two-set Cartesian product is necessarily prior to most other definitions.\n\n=== Non-commutativity and non-associativity ===\nLet ''A'', ''B'', ''C'', and ''D'' be sets.\n\nThe Cartesian product {{nowrap|''A'' × ''B''}} is not [[commutative]],\n: <math>A \\times B \\neq B \\times A,</math>\nbecause the [[ordered pair]]s are reversed unless at least one of the following conditions is satisfied:<ref name=\"cnx\"/>\n* ''A'' is equal to ''B'', or\n* ''A'' or ''B'' is the [[empty set]].\n\nFor example:\n: ''A'' = {1,2}; ''B'' = {3,4}\n:: ''A'' × ''B'' = {1,2} × {3,4} = {(1,3), (1,4), (2,3), (2,4)}\n:: ''B'' × ''A'' = {3,4} × {1,2} = {(3,1), (3,2), (4,1), (4,2)}\n\n: ''A'' = ''B'' = {1,2}\n:: ''A'' × ''B'' = ''B'' × ''A'' = {1,2} × {1,2} = {(1,1), (1,2), (2,1), (2,2)}\n\n: ''A'' = {1,2}; ''B'' = ∅\n:: ''A'' × ''B'' = {1,2} × ∅ = ∅\n:: ''B'' × ''A'' = ∅ × {1,2} = ∅\n\n\nStrictly speaking, the Cartesian product is not [[associative]] (unless one of the involved sets is empty).\n: <math>(A\\times B)\\times C \\neq A \\times (B \\times C)</math>\nIf for example ''A'' = {1}, then (''A'' × ''A'') × ''A'' = { ((1,1),1) } ≠ { (1,(1,1)) } = ''A'' × (''A'' × ''A'').\n\n=== Intersections, unions, and subsets ===\n{{multiple image\n|align=center\n| total_width = 750\n|image1=CartDistr_svg.svg\n|caption1=Example sets<br>\n{{color|#0000c0|''A''}}={''y''∈[[real numbers|ℝ]]:1≤''y''≤4}, <br />\n{{color|#c00000|''B''}}={''x''∈ℝ:2≤''x''≤5}, and {{color|#00c000|''C''}}={''x''∈ℝ:4≤''x''≤7}, demonstrating <br />\n''A''×(''B''∩''C'') = ({{highlight|''A''×''B''|#FCC6C6}})∩({{highlight|''A''×''C''|#C6FCC6}}), <br />\n''A''×(''B''∪''C'') = ({{highlight|''A''×''B''|#FCC6C6}})∪({{highlight|''A''×''C''|#C6FCC6}}), and <br />\n''A''×(''B''{{tsp}}\\{{hsp}}''C'') = ({{highlight|''A''×''B''|#FCC6C6}}){{tsp}}\\{{hsp}}({{highlight|''A''×''C''|#C6FCC6}})\n|image2=CartInts_svg.svg\n|caption2=Example sets <br />\n{{color|#c00000|''A''}}={''x''∈ℝ:2≤''x''≤5}, {{color|#00c000|''B''}}={''x''∈ℝ:3≤''x''≤7}, <br />\n{{color|#c00000|''C''}}={''y''∈ℝ:1≤''y''≤3}, {{color|#00c000|''D''}}={''y''∈ℝ:2≤''y''≤4}, demonstrating <br />\n{{highlight|(''A''∩''B'')×(''C''∩''D'')|#FCFCC6}} = {{highlight|(''A''×''C'')∩''(B''×''D'')|#FCFCC6}}.\n|image3=CartUnion_svg.svg\n|caption3={{highlight|(''A''∪''B'')×(''C''∪''D'')|#E0E0FC}} ≠ {{highlight|(''A''×''C'')|#FCC6C6}}∪{{highlight|(''B''×''D'')|#C6FCC6}} can be seen from the same example.\n}}\nThe Cartesian product behaves nicely with respect to [[Intersection (set theory)|intersections]] (see leftmost picture).\n: <math>(A \\cap B) \\times (C \\cap D) = (A \\times C) \\cap (B \\times D)</math><ref name=\"planetmath\">{{planetmath reference|id=359|title=CartesianProduct}}</ref>\n\nIn most cases the above statement is not true if we replace intersection with [[Union (set theory)|union]] (see middle picture).\n: <math>(A \\cup B) \\times (C \\cup D) \\neq (A \\times C) \\cup (B \\times D)</math>\n\nIn fact, we have that:\n: <math>(A \\times C) \\cup (B \\times D) = [(A \\setminus B) \\times C] \\cup [(A \\cap B) \\times (C \\cup D)] \\cup [(B \\setminus A) \\times D]</math>\n\nFor the set difference we also have the following identity:\n: <math>(A \\times C) \\setminus (B \\times D) = [A \\times (C \\setminus D)] \\cup [(A \\setminus B) \\times C] </math>\n\nHere are some rules demonstrating distributivity with other operators (see rightmost picture):<ref name=\"cnx\">Singh, S. (August 27, 2009). ''Cartesian product''. Retrieved from the Connexions Web site: http://cnx.org/content/m15207/1.5/</ref>\n: <math>A \\times (B \\cap C) = (A \\times B) \\cap (A \\times C),</math>\n: <math>A \\times (B \\cup C) = (A \\times B) \\cup (A \\times C),</math>\n: <math>A \\times (B \\setminus C) = (A \\times B) \\setminus (A \\times C),</math>\n: <math>(A \\times B)^\\complement = (A^\\complement \\times B^\\complement) \\cup (A^\\complement \\times B) \\cup (A \\times B^\\complement),</math><ref name=\"planetmath\"/>\nwhere <math>A^\\complement</math> denotes the [[absolute complement]] of ''A''.\n\nOther properties related with [[subset]]s are:\n: <math>\\text{if } A \\subseteq B \\text{ then } A \\times C \\subseteq B \\times C,</math>\n: <math>\\text{if both } A,B \\neq \\emptyset \\text{ then } A \\times B \\subseteq C \\times D \\iff A \\subseteq C\\text{ and } B \\subseteq D.</math><ref>Cartesian Product of Subsets. (February 15, 2011). ''ProofWiki''. Retrieved 05:06, August 1, 2011 from https://proofwiki.org/w/index.php?title=Cartesian_Product_of_Subsets&oldid=45868</ref>\n\n=== Cardinality ===\n{{See also|Cardinal arithmetic}}\n\nThe [[cardinality]] of a set is the number of elements of the set. For example, defining two sets: {{nowrap|1=''A'' = {a, b}}} and {{nowrap|1=''B'' = {5, 6}.}} Both set ''A'' and set ''B'' consist of two elements each. Their Cartesian product, written as {{nowrap|''A'' × ''B''}}, results in a new set which has the following elements:\n: ''A'' × ''B'' = {(a,5), (a,6), (b,5), (b,6)}.\n\nEach element of ''A'' is paired with each element of ''B''. Each pair makes up one element of the output set.\nThe number of values in each element of the resulting set is equal to the number of sets whose cartesian product is being taken; 2 in this case.\nThe cardinality of the output set is equal to the product of the cardinalities of all the input sets. That is,\n: |''A'' × ''B''| = |''A''| · |''B''|.\nIn this case, |''A'' × ''B''| = 4\n\nSimilarly\n: |''A'' × ''B'' × ''C''| = |''A''| · |''B''| · |''C''|\nand so on.\n\nThe set {{nowrap|''A'' × ''B''}} is [[infinite set|infinite]] if either ''A'' or ''B'' is infinite and the other set is not the empty set.<ref>Peter S. (1998). A Crash Course in the Mathematics of Infinite Sets. ''St. John's Review, 44''(2), 35–59. Retrieved August 1, 2011, from http://www.mathpath.org/concepts/infinity.htm</ref>\n\n== Cartesian products of several sets ==\n\n=== <var>n</var>-ary Cartesian product ===\nThe Cartesian product can be generalized to the '''''n''-ary Cartesian product''' over ''n'' sets ''X''<sub>1</sub>, ..., ''X<sub>n</sub>'' as the set\n\n: <math>X_1\\times\\cdots\\times X_n = \\{(x_1, \\ldots, x_n) \\mid x_i \\in X_i \\ \\text{for every} \\ i \\in \\{1, \\ldots, n\\} \\}.</math>\n\nof [[tuple|''n''-tuple]]s. If tuples are defined as [[Tuple#Tuples_as_nested_ordered_pairs|nested ordered pairs]], it can be identified with {{nowrap|(''X''<sub>1</sub> × ... × ''X<sub>n−1</sub>'') × ''X<sub>n</sub>''}}. If a tuple is defined as a function on {{nowrap|{1, 2, ..., ''n''} }} that takes its value at ''i'' to be the ''i''th element of the tuple, then the Cartesian product ''X''<sub>1</sub>×...×''X''<sub>n</sub> is the set of functions\n\n: <math>\\{ x:\\{1,\\ldots,n\\}\\to X_1\\cup\\ldots\\cup X_n \\ | \\ x(i)\\in X_i \\ \\text{for every} \\ i \\in \\{1, \\ldots, n\\} \\}.</math>\n\n=== <var>n</var>-ary Cartesian power ===\nThe '''Cartesian square''' of a set ''X'' is the Cartesian product {{nowrap|1=''X''<sup>2</sup> = ''X'' × ''X''}}.\nAn example is the 2-dimensional [[plane (mathematics)|plane]] {{nowrap|1='''R'''<sup>2</sup> = '''R''' × '''R'''}} where '''R''' is the set of [[real number]]s: '''R'''<sup>2</sup> is the set of all points {{nowrap|(''x'',''y'')}} where ''x'' and ''y'' are real numbers (see the [[Cartesian coordinate system]]).\n\nThe ''' ''n''-ary Cartesian power''' of a set ''X'' can be defined as\n\n: <math> X^n = \\underbrace{ X \\times X \\times \\cdots \\times X }_{n}= \\{ (x_1,\\ldots,x_n) \\ | \\ x_i \\in X \\ \\text{for every} \\ i \\in \\{1, \\ldots, n\\} \\}.</math>\n\nAn example of this is {{nowrap|1='''R'''<sup>3</sup> = '''R''' × '''R''' × '''R'''}}, with '''R''' again the set of real numbers, and more generally '''R'''<sup>''n''</sup>.\n\nThe ''n''-ary cartesian power of a set ''X'' is [[isomorphism|isomorphic]] to the space of functions from an ''n''-element set to ''X''.  As a special case, the 0-ary cartesian power of ''X'' may be taken to be a [[singleton set]], corresponding to the [[empty function]] with [[codomain]] ''X''.\n\n=== Infinite Cartesian products ===\nIt is possible to define the Cartesian product of an arbitrary (possibly [[Infinity|infinite]]) [[indexed family]] of sets. If ''I'' is any [[index set]], and <math>\\{X_i\\}_{i\\in I}</math> is a family of sets indexed by ''I'', then the Cartesian product of the sets in ''X'' is defined to be \n\n: <math>\\prod_{i \\in I} X_i = \\left\\{\\left. f: I \\to \\bigcup_{i \\in I} X_i\\ \\right|\\ (\\forall i)(f(i) \\in X_i)\\right\\},</math> \n\nthat is, the set of all functions defined on the [[index set]] such that the value of the function at a particular index ''i'' is an element of ''X<sub>i</sub>''.  Even if each of the ''X<sub>i</sub>'' is nonempty, the Cartesian product may be empty if the [[axiom of choice]] (which is equivalent to the statement that every such product is nonempty) is not assumed.\n\nFor each ''j'' in ''I'', the function \n: <math>  \\pi_{j}: \\prod_{i \\in I} X_i \\to X_{j},</math>\ndefined by <math>\\pi_{j}(f) = f(j)</math> is called the '''''j''th [[Projection (mathematics)|projection map]]'''.\n\n'''Cartesian power''' is a Cartesian product where all the factors ''X<sub>i</sub>'' are the same set ''X''. In this case, \n: <math>\\prod_{i \\in I} X_i = \\prod_{i \\in I} X</math> \nis the set of all functions from ''I'' to ''X'', and is frequently denoted ''X<sup>I</sup>''. This case is important in the study of [[cardinal exponentiation]]. An important special case is when the index set is <math>\\mathbb{N}</math>, the [[natural numbers]]: this Cartesian product is the set of all infinite sequences with the ''i''th term in its corresponding set ''X<sub>i</sub>''. For example, each element of \n: <math>\\prod_{n = 1}^\\infty \\mathbb R = \\mathbb R \\times \\mathbb R \\times \\cdots</math>\ncan be visualized as a [[Euclidean vector|vector]] with countably infinite real number components. This set is frequently denoted <math>\\mathbb{R}^\\omega</math>, or <math>\\mathbb{R}^{\\mathbb{N}}</math>.\n\n== Other forms ==\n\n=== Abbreviated form ===\nIf several sets are being multiplied together, e.g. ''X''<sub>1</sub>, ''X''<sub>2</sub>, ''X''<sub>3</sub>, …, then some authors<ref>Osborne, M., and Rubinstein, A., 1994. ''A Course in Game Theory''. MIT Press.</ref> choose to abbreviate the Cartesian product as simply <big>&times;</big>''X''<sub>''i''</sub>.\n\n=== Cartesian product of functions ===\nIf ''f'' is a function from ''A'' to ''B'' and ''g'' is a function from ''X'' to ''Y'', their Cartesian product {{nowrap|''f'' × ''g''}} is a function from {{nowrap|''A'' × ''X''}} to {{nowrap|''B'' × ''Y''}} with\n: <math>(f\\times g)(a, x) = (f(a), g(x)).</math>\n\nThis can be extended to [[tuple]]s and infinite collections of functions.\nThis is different from the standard cartesian product of functions considered as sets.\n\n=== Cylinder ===\nLet <math>A</math> be a set and <math>B \\subseteq A</math>. Then the ''cylinder'' of <math>B</math> with respect to <math>A</math> is the Cartesian product <math>B \\times A</math> of <math>B</math> and <math>A</math>. \n\nNormally, <math>A</math> is considered to be the universe of the context and is left away. For example, if <math>B</math> is a subset of the natural numbers <math>\\mathbb{N}</math>, then the cylinder of <math>B</math> is <math>B \\times \\mathbb{N}</math>.\n\n== Definitions outside set theory ==\n\n===Category theory===\nAlthough the Cartesian product is traditionally applied to sets, [[category theory]] provides a more general interpretation of the [[product (category theory)|product]] of mathematical structures. This is distinct from, although related to, the notion of a [[Cartesian square (category theory)|Cartesian square]] in category theory, which is a generalization of the [[fiber product]].\n\n[[Exponential object|Exponentiation]] is the [[right adjoint]] of the Cartesian product; thus any category with a Cartesian product (and a [[final object]]) is a [[Cartesian closed category]].\n\n===Graph theory===\nIn [[graph theory]] the [[Cartesian product of graphs|Cartesian product of two graphs]] ''G'' and ''H'' is the graph denoted by {{nowrap|''G'' × ''H''}} whose [[vertex (graph theory)|vertex]] set is the (ordinary) Cartesian product {{nowrap|''V''(''G'') × ''V''(''H'')}} and such that two vertices (''u'',''v'') and (''u''′,''v''′) are adjacent in {{nowrap|''G'' × ''H''}} if and only if {{nowrap|1=''u'' = ''u''′}} and ''v'' is adjacent with ''v''′ in ''H'', ''or'' {{nowrap|1=''v'' = ''v''′}} and ''u'' is adjacent with ''u''′ in ''G''.  The Cartesian product of graphs is not a [[product (category theory)|product]] in the sense of category theory.  Instead, the categorical product is known as the [[tensor product of graphs]].\n\n==See also==\n* [[Binary relation]]\n* [[Concatenation#Concatenation of sets of strings|Concatenation of sets of strings]]\n* [[Coproduct]]\n* [[Empty product]]\n* [[Euclidean space]]\n* [[Exponential object]]\n* [[Finitary relation]]\n* [[Join (SQL)#Cross join|Join (SQL) § Cross join]]\n* [[Total order#Orders on the Cartesian product of totally ordered sets|Orders on the Cartesian product of totally ordered sets]]\n* [[Product (category theory)]]\n* [[Product topology]]\n* [[Product type]]\n* [[Ultraproduct]]\n\n== References ==\n{{Reflist}}\n\n==External links==\n* [http://www.apronus.com/provenmath/cartesian.htm Cartesian Product at ProvenMath]\n* {{springer|title=Direct product|id=p/d032730}}\n* [http://education-portal.com/academy/lesson/how-to-find-the-cartesian-product.html How to find the Cartesian Product, Education Portal Academy]\n\n{{Set theory}}\n\n[[Category:Axiom of choice]]\n[[Category:Basic concepts in set theory]]\n[[Category:Binary operations]]"
    },
    {
      "title": "Circular convolution",
      "url": "https://en.wikipedia.org/wiki/Circular_convolution",
      "text": "The '''circular convolution''', also known as '''cyclic convolution''',  of two aperiodic functions (i.e. [[Schwartz functions]]) occurs when one of them is [[convolution|convolved in the normal way]] with a [[periodic summation]] of the other function. That situation arises in the context of the [[Discrete Fourier transform#Circular convolution theorem and cross-correlation theorem|circular convolution theorem]]. The identical operation can also be expressed in terms of the periodic summations of <u>both</u> functions, if the infinite integration interval is reduced to just one period. &nbsp;That situation arises in the context of the [[discrete-time Fourier transform]] (DTFT) and is also called '''periodic convolution'''. &nbsp;In particular, the DTFT of the product of two discrete sequences is the periodic convolution of the DTFTs of the individual sequences.<ref>If a sequence, ''x''[''n''], represents samples of a continuous function, ''x''(''t''), with Fourier transform ''X''(ƒ), its DTFT is a periodic summation of ''X''(ƒ). &nbsp;(see [[Discrete-time Fourier transform#Relationship to sampling]])</ref>\n\nLet ''x'' be a function with a well-defined periodic summation, ''x''<sub>''T''</sub>, where:\n\n:<math> x_T(t) \\ \\triangleq \\ \\sum_{k=-\\infty}^\\infty  x(t - kT) = \\sum_{k=-\\infty}^\\infty  x(t + kT).</math>\n\nIf ''h'' is any other function for which the convolution ''x''<sub>''T''</sub> ∗ ''h'' exists, then the convolution ''x''<sub>''T''</sub> ∗ ''h''  is periodic and identical to''':'''\n\n:<math>\n\\begin{align}\n(x_T * h)(t)\\quad &\\triangleq \\ \\int_{-\\infty}^\\infty h(\\tau)\\cdot x_T(t - \\tau)\\,d\\tau \\\\\n&\\equiv \\int_{t_o}^{t_o+T} h_T(\\tau)\\cdot x_T(t - \\tau)\\,d\\tau,\n\\end{align}\n</math><ref>Proof''':'''\n\n:<math>\\int_{-\\infty}^\\infty  h(\\tau)\\cdot x_T(t - \\tau)\\,d\\tau</math>\n:::<math>\n\\begin{align}\n&= \\sum_{k=-\\infty}^\\infty  \\left[\\int_{t_o+kT}^{t_o+(k+1)T} h(\\tau)\\cdot x_T(t - \\tau)\\ d\\tau\\right] \\\\\n&\\stackrel{\\tau \\rightarrow \\tau+kT}{=}\\  \\sum_{k=-\\infty}^\\infty \\left[\\int_{t_o}^{t_o+T} h(\\tau+kT)\\cdot x_T(t - \\tau -kT)\\ d\\tau\\right] \\\\\n&= \\int_{t_o}^{t_o+T} \\left[\\sum_{k=-\\infty}^\\infty h(\\tau+kT)\\cdot \\underbrace{x_T(t - \\tau-kT)}_{X_T(t - \\tau), \\text{ by periodicity}}\\right]\\ d\\tau\\\\\n&= \\int_{t_o}^{t_o+T} \\underbrace{\\left[\\sum_{k=-\\infty}^\\infty  h(\\tau+kT)\\right]}_{\\triangleq \\ h_T(\\tau)}\\cdot x_T(t - \\tau)\\ d\\tau \\quad \\quad \\scriptstyle{(QED)}\n\\end{align}\n</math>\n</ref>\n\nwhere ''t''<sub>o</sub> is an arbitrary parameter and ''h''<sub>''T''</sub> is a [[periodic summation]] of ''h''.\n\nThe second integral is called the '''periodic convolution'''<ref>Jeruchim 2000, pp 73-74.</ref><ref name=\"Uday\">Udayashankara 2010, p 189.</ref> of functions ''x''<sub>''T''</sub> and ''h''<sub>''T''</sub> and is sometimes normalized by 1/''T''.<ref>Oppenheim, pp 388-389</ref> When ''x''<sub>''T''</sub> is expressed as the [[periodic summation]] of another function, ''x'', the same operation may also be referred to as a '''circular convolution'''<ref name=\"Uday\"/><ref>Priemer 1991, pp 286-289.</ref> of functions ''h'' and ''x''.\n\n== Discrete sequences ==\nSimilarly, for discrete sequences, and a parameter '''N''', we can write a '''circular convolution''' of aperiodic functions <math>h</math> and <math>x</math>  as''':'''\n\n:<math>\n(x_N * h)[n] \\ \\triangleq \\ \\sum_{m=-\\infty}^\\infty  h[m] \\cdot \\underbrace{x_N[n-m]}_{\\sum_{k=-\\infty}^\\infty x[n -m -kN]}\n</math>\n\nThis function is '''N'''-periodic.  It has at most '''N''' unique values.  For the special case that the non-zero extent of both ''x'' and ''h'' are ''≤ N'', it is reducible to [[matrix multiplication]] where the kernel of the integral transform is a [[circulant matrix]].\n\n== Example ==\n[[Image:Circular convolution example.svg|thumb|400px|Circular convolution can be expedited by the FFT algorithm, so it is often used with an FIR filter to efficiently compute linear convolutions. These graphs illustrate how that is possible.]]\nA case of great practical interest is illustrated in the figure.  The duration of the '''x''' sequence is '''N''' (or less), and the duration of the '''h''' sequence is significantly less.  Then many of the values of the circular convolution are identical to values of '''x∗h''',&nbsp; which is actually the desired result when the '''h''' sequence is a [[finite impulse response]] (FIR) filter.  Furthermore, the circular convolution is very efficient to compute, using a [[fast Fourier transform]] (FFT) algorithm and the [[Discrete Fourier transform#Circular convolution theorem and cross-correlation theorem|circular convolution theorem]].\n\nThere are also methods for dealing with an '''x''' sequence that is longer than a practical value for '''N'''.  The sequence is divided into segments (''blocks'') and processed piecewise.  Then the filtered segments are carefully pieced back together.  Edge effects are eliminated by <u>overlapping</u> either the input blocks or the output blocks.  To help explain and compare the methods, we discuss them both in the context of an '''h''' sequence of length 201 and an FFT size of&nbsp;''N''&nbsp;=&nbsp;1024.\n\n'''Overlapping input blocks'''\n\nThis method uses a block size equal to the FFT size (1024).  We describe it first in terms of normal or ''linear'' convolution.  When a normal convolution is performed on each block, there are start-up and decay transients at the block edges, due to the filter ''latency'' (200-samples).  Only 824 of the convolution outputs are unaffected by edge effects.  The others are discarded, or simply not computed.  That would cause gaps in the output if the input blocks are contiguous.  The gaps are avoided by overlapping the input blocks by 200 samples.  In a sense, 200 elements from each input block are \"saved\" and carried over to the next block.  This method is referred to as '''[[Overlap-save method|overlap-save]]''',<ref>Rabiner 1975, pp 65–67.</ref> although the method we describe next requires a similar \"save\" with the output samples.\n\nWhen an FFT is used to compute the 824 unaffected DFT samples, we don't have the option of not computing the affected samples, but the leading and trailing edge-effects are overlapped and added because of circular convolution.  Consequently, the 1024-point inverse FFT (IFFT) output contains only 200 samples of edge effects (which are discarded) and the 824 unaffected samples (which are kept).  To illustrate this, the fourth frame of the figure at right depicts a block that has been periodically (or \"circularly\") extended, and the fifth frame depicts the individual components of a linear convolution performed on the entire sequence.  The edge effects are where the contributions from the extended blocks overlap the contributions from the original block.  The last frame is the composite output, and the section colored green represents the unaffected portion.\n\n'''Overlapping output blocks'''\n\nThis method is known as '''[[Overlap-add method|overlap-add]]'''.<ref>Rabiner 1975, pp 63–65.</ref>  In our example, it uses contiguous input blocks of size 824 and pads each one with 200 zero-valued samples.  Then it overlaps and adds the 1024-element output blocks.  Nothing is discarded, but 200 values of each output block must be \"saved\" for the addition with the next block.  Both methods advance only 824 samples per 1024-point IFFT, but overlap-save avoids the initial zero-padding and final addition.\n\n== See also ==\n*[[Hilbert transform#Discrete Hilbert transform|Discrete Hilbert transform]]\n*[[Circulant matrix]]\n\n== Notes ==\n{{Reflist}}\n\n== References ==\n*Rabiner, Lawrence R.; Gold, Bernard (1975). ''Theory and application of digital signal processing''. Englewood Cliffs, N.J.: Prentice-Hall. pp 63–67. {{ISBN|0139141014}}\n*Oppenheim, Alan V.; Schafer, Ronald W.; Buck, John A. (1999). ''Discrete-time signal processing''. Upper Saddle River, N.J.: Prentice Hall. {{ISBN|0137549202}}.\n*Priemer, Roland (July 1991). ''Introductory Signal Processing (Advanced Series in Electrical and Computer Engineering) (v. 6)''. Teaneck, N.J.: World Scientific Pub Co Inc. [https://books.google.com/books?id=QBT7nP7zTLgC&printsec=frontcover&dq=Priemer,+Roland&hl=en&sa=X&ei=J2owUZzANIb_ygGex4HAAg&ved=0CC8Q6AEwAA {{isbn|9971509199}}.\n*Jeruchim, Michel C.; Philip Balaban, K. Sam Shanmugan (October 2000). ''Simulation of Communication Systems: Modeling, Methodology and Techniques'' (2nd ed.). New York: Kluwer Academic Publishers. {{isbn|0306462672}}.\n*Udayashankara, V. (June 2010). ''Real Time Digital Signal Processing''. India: Prentice-Hall. {{isbn|8120340493}}.\n*{{cite book |author1=Oppenheim, Alan V. |author2=Willsky, with S. Hamid | title=Signals and Systems | publisher=Pearson Education | year=1998 | isbn=0-13-814757-4}}.\n\n[[Category:Functional analysis]]\n[[Category:Image processing]]\n[[Category:Binary operations]]"
    },
    {
      "title": "Complement (set theory)",
      "url": "https://en.wikipedia.org/wiki/Complement_%28set_theory%29",
      "text": "{{multiple image\n| align = right\n| image1 = Venn01.svg\n| width1 = 250\n| alt1 = \n| caption1 = If {{math|''A''}} is the area colored red in this image...\n| image2 = Venn10.svg\n| width2 = 250\n| alt2 = \n| caption2 = ... then the complement of {{math|''A''}} is everything else.\n| footer = \n}}\n\nIn [[set theory]], the '''complement''' of a [[Set (mathematics)|set]] {{math|''A''}} refers to [[Element (mathematics)|elements]] not in {{math|''A''}}.\n\nWhen all sets under consideration are considered to be [[subset]]s of a given set {{math|''U''}}, the '''absolute complement''' of {{math|''A''}} is the set of elements in {{math|''U''}} but not in {{math|''A''}}.\n\nThe '''relative complement'''  of {{math|''A''}} with respect to a set {{math|''B''}}, also termed the '''difference''' of sets {{math|''A''}} and {{math|''B''}}, written {{math|''B'' ∖ ''A''}}, is the set of elements in {{math|''B''}} but not in {{math|''A''}}.\n\n==Absolute complement==<!-- This section is linked from [[Bayes' theorem]] and [[absolute set complement]] -->\n\n[[File:Venn1010.svg|250px|thumb|The '''absolute complement''' of {{mvar|A}} (left circle) in ''U'':\n<math>A^c = U \\setminus A</math>.]]\n\n===Definition===\nIf {{math|''A''}} is a set, then the '''absolute complement''' of {{math|''A''}} (or simply the '''complement of {{math|''A''}}''') is the set of elements not in {{math|''A''}}. In other words, if {{math|''U''}} is the [[universe (mathematics)|universe]] that contains all the elements under study, and there is no need to mention it because it is obvious and unique, then the absolute complement of {{math|''A''}} is the relative complement of {{math|''A''}} in {{math|''U''}}:<ref>The set other than {{math|''A''}} is thus implicitly mentioned in an absolute complement, and explicitly mentioned in a relative complement.</ref>\n\n:<math>A^c = U \\setminus A</math>.\n\nFormally:\n\n:<math>A^c = \\{ x\\in U \\mid x \\notin A \\}.</math>\n\nThe absolute complement of {{math|''A''}} is usually denoted by <math>A^c</math>. Other notations include <math>\\overline A</math>, <math>A'</math>, <math>\\complement_U A</math>, and <math>\\complement A</math>.<ref name=\"Bou\">{{harvnb|Bourbaki|1970|p=E II.6}}.</ref>\n\n===Examples===\n* Assume that the universe is the set of [[integer]]s. If {{math|''A''}} is the set of odd numbers, then the complement of {{math|''A''}} is the set of even numbers. If {{math|''B''}} is the set of [[Multiple (mathematics)|multiples]] of 3, then the complement of {{math|''B''}} is the set of numbers [[Modular arithmetic|congruent]] to 1 or 2 modulo 3 (or, in simpler terms, the integers that are not multiples of 3).\n* Assume that the universe is the [[standard 52-card deck]]. If the set {{math|''A''}} is the suit of spades, then the complement of {{math|''A''}} is the [[Union (set theory)|union]] of the suits of clubs, diamonds, and hearts. If the set {{math|''B''}} is the union of the suits of clubs and diamonds, then the complement of {{math|''B''}} is the union of the suits of hearts and spades.\n\n===Properties===\nLet {{math|''A''}} and {{math|''B''}} be two sets in a universe {{math|''U''}}. The following identities capture important properties of absolute complements:\n\n: [[De Morgan's laws]]:<ref name=\"Halmos-1960\" />\n\n::* <math>\\left(A \\cup B \\right)^c=A^c \\cap B^c.</math>\n::* <math>\\left(A \\cap B \\right)^c=A^c\\cup B^c.</math>\n\n: Complement laws:<ref name=\"Halmos-1960\" />\n\n::* <math>A \\cup A^c = U .</math>\n::* <math>A \\cap A^c =\\varnothing .</math>\n::* <math>\\varnothing^c =U.</math>\n::* <math> U^c =\\varnothing.</math>\n::* <math>\\text{If }A\\subseteq B\\text{, then }B^c\\subseteq A^c.</math>\n::*: (this follows from the equivalence of a conditional with its [[contrapositive]]).\n\n: [[involution (mathematics)|Involution]] or double complement law:\n\n::* <math>(A^c)^c=A.</math>\n\n: Relationships between relative and absolute complements:\n\n::* <math>A \\setminus B = A \\cap B^c.</math>\n::* <math>(A \\setminus B)^c = A^c \\cup B.</math>\n\n: Relationship with set difference:\n::* <math> A^c \\setminus B^c = B \\setminus A. </math>\n\nThe first two complement laws above show that if {{math|''A''}} is a non-empty, [[proper subset]] of {{math|''U''}}, then {{math|{''A'', ''A''<sup>∁</sup>}{{null}}}} is a [[partition of a set|partition]] of {{math|''U''}}.\n\n== Relative complement ==\n\n<!-- Many links redirect to this section:\n[[difference (set theory)]], [[difference of two sets]], [[relative complement]], [[set-theoretic difference]], [[set difference]], [[set minus]], [[set subtraction]], [[set theoretic difference]], [[setminus]] -->\n\n===Definition===\nIf {{math|''A''}} and {{math|''B''}} are sets, then the '''relative complement''' of {{math|''A''}} in {{math|''B''}},<ref name=\"Halmos-1960\">{{harvnb|Halmos|1960|p=17}}.</ref> also termed the '''set difference''' of {{math|''B''}} and {{math|''A''}},<ref>{{harvnb|Devlin|1979|p=6}}.</ref> is the set of elements in {{math|''B''}} but not in {{math|''A''}}.\n\n[[File:Venn0010.svg|250px|thumb|The '''relative complement''' of {{math|''A''}} (left circle) in {{math|''B''}} (right circle):\n<math>B \\cap A^c = B \\setminus A</math>]]\n\nThe relative complement of {{math|''A''}} in {{math|''B''}} is denoted {{nowrap|{{math|''B'' ∖ ''A''}}}} according to the [[ISO 31-11#Sets|ISO 31-11 standard]]. It is sometimes written {{nowrap|{{math|''B'' − ''A''}}}}, but this notation is ambiguous, as in some contexts it can be interpreted as the set of all elements {{nowrap|{{math|''b'' − ''a''}}}}, where {{math|''b''}} is taken from {{math|''B''}} and {{math|''a''}} from {{math|''A''}}.\n\nFormally:\n\n: <math>B \\setminus A = \\{ x\\in B \\mid x \\notin A \\}. </math>\n\n===Examples===\n* <math>\\{ 1, 2, 3\\} \\setminus \\{ 2,3,4\\} = \\{ 1 \\}</math>.\n* <math>\\{ 2, 3, 4 \\} \\setminus \\{ 1,2,3 \\} = \\{ 4 \\} </math>.\n* If <math>\\mathbb{R}</math> is the set of [[real number]]s and <math>\\mathbb{Q}</math> is the set of [[rational number]]s, then <math>\\mathbb{R}\\setminus\\mathbb{Q}</math> is the set of [[irrational number]]s.\n\n===Properties===\nLet {{math|''A''}}, {{math|''B''}}, and {{math|''C''}} be three sets. The following [[identity (mathematics)|identities]] capture notable properties of relative complements:\n\n:* <math>C \\setminus (A \\cap B) = (C \\setminus A) \\cup (C \\setminus B)</math>.\n:* <math>C \\setminus (A \\cup B) = (C \\setminus A) \\cap (C \\setminus B)</math>.\n:* <math>C \\setminus (B \\setminus A) = (C \\cap A) \\cup (C \\setminus B)</math>,\n:*:with the important special case <math>C \\setminus (C \\setminus A) = (C \\cap A)</math> demonstrating that intersection can be expressed using only the relative complement operation.\n:* <math>(B \\setminus A) \\cap C = (B \\cap C) \\setminus A = B \\cap (C \\setminus A)</math>.\n:* <math>(B \\setminus A) \\cup C = (B \\cup C) \\setminus (A \\setminus C)</math>.\n:* <math>A \\setminus A = \\empty</math>.\n:* <math>\\empty \\setminus A = \\empty</math>.\n:* <math>A \\setminus \\empty = A</math>.\n:* <math>A \\setminus U = \\empty</math>.\n\n==Complementary relation==\nA [[binary relation]] ''R'' is defined as a subset of a [[product of sets]] ''X'' × ''Y''. The '''complementary relation''' <math>\\bar{R}</math> is the set complement of ''R'' in ''X'' × ''Y''. The complement of relation ''R'' can be written\n:<math>\\bar{R} \\ = \\ (X \\times Y) \\backslash R .</math>\nOften ''R'' is viewed as a [[logical matrix]] with rows representing the elements of ''X'', and columns elements of ''Y''. The truth of ''aRb'' corresponds to 1 in row ''a'', column ''b''. Producing the complementary relation to ''R'' then corresponds to switching all 1s to 0s and 0s to 1s for the logical matrix of the complement.\n\nTogether with [[composition of relations]] and [[converse relation]]s, complementary relations and the [[algebra of sets]] are the elementary [[operation (mathematics)|operation]]s of the [[calculus of relations]].\n\n== LaTeX notation ==\n\nIn the [[LaTeX]] typesetting language, the command <code>\\setminus</code><ref name=\"The Comprehensive LaTeX Symbol List\">[http://ctan.unsw.edu.au/info/symbols/comprehensive/symbols-a4.pdf] The Comprehensive LaTeX Symbol List</ref> is usually used for rendering a set difference symbol, which is similar to a [[backslash]] symbol. When rendered, the <code>\\setminus</code> command looks identical to <code>\\backslash</code> except that it has a little more space in front and behind the slash, akin to the LaTeX sequence <code>\\mathbin{\\backslash}</code>. A variant <code>\\smallsetminus</code> is available in the amssymb package.\n\n== In programming languages ==\n\nSome [[programming language]]s have [[set (computer science)|sets]] among their builtin [[data structure]]s. Such a data structure behaves as a [[finite set]], that is it consists of a finite number of data, that are not specifically ordered, and may thus be considered as the elements of a set. In some cases, the elements are not necessary distinct, and the data structure codes [[multiset]]s rather than sets. These programming languages have operators or functions for computing the complement and the set differences. \n\nThese operators may generally be applied also to data structures that are not really mathematical sets, such as [[list (data structure)|ordered list]]s or [[array data structure|arrays]]. It follows that some programming languages may have a function called \"set_difference\" even if they have not any data structure for sets.\n\n== See also ==\n\n* [[Algebra of sets]]\n* [[Naive set theory]]\n* [[Symmetric difference]]\n\n==Notes==\n<references/>\n\n== References ==\n\n*{{cite book \n | last = Bourbaki\n | first = N.\n | author-link = Nicolas Bourbaki\n | title = Théorie des ensembles\n | publisher = Hermann\n | place = Paris\n | year = 1970\n | isbn = 978-3-540-34034-8\n | language = fr\n | ref = harv\n}}\n\n*{{cite book\n | last = Devlin\n | first = Keith J.\n | author-link = Keith Devlin\n | title = Fundamentals of contemporary set theory\n | series = Universitext\n | publisher = [[Springer-Verlag|Springer]]\n | year = 1979\n | isbn = 0-387-90441-7\n | zbl = 0407.04003\n | ref = harv\n}}\n\n*{{cite book\n | last = Halmos\n | first = Paul R.\n | author-link = Paul Halmos\n | title = Naive set theory\n | series = The University Series in Undergraduate Mathematics\n | publisher = van Nostrand Company\n | year = 1960\n | zbl = 0087.04403\n | ref = harv\n}}\n\n== External links ==\n\n* {{MathWorld |title=Complement |id=Complement }}\n* {{MathWorld |title=Complement Set |id=ComplementSet }}\n\n{{Set theory}}\n\n{{DEFAULTSORT:Complement (set theory)}}\n[[Category:Basic concepts in set theory]]\n[[Category:Binary operations]]"
    },
    {
      "title": "Connected sum",
      "url": "https://en.wikipedia.org/wiki/Connected_sum",
      "text": "In [[mathematics]], specifically in [[topology]], the operation of '''connected sum''' is a geometric modification on [[manifold]]s. Its effect is to join two given manifolds together near a chosen point on each. This construction plays a key role in the [[Surface (topology)#Classification of closed surfaces|classification of closed surfaces]].\n\nMore generally, one can also join manifolds together along identical submanifolds; this generalization is often called the '''fiber sum'''. There is also a closely related notion of a connected sum on [[knot (mathematics)|knot]]s, called the '''knot sum''' or '''composition''' of knots.\n\n[[Image:Connected sum.svg|right|thumb|220px|Illustration of connected sum.]]\n\n== Connected sum at a point ==\n\nA '''connected sum''' of two ''m''-dimensional [[manifold]]s is a manifold formed by deleting a [[ball (mathematics)|ball]] inside each manifold and [[adjunction space|gluing together]] the resulting boundary [[sphere]]s.\n\nIf both manifolds are [[oriented]], there is a unique connected sum defined by having the gluing map reverse orientation. Although the construction uses the choice of the balls, the result is unique up to [[homeomorphism]]. One can also make this operation work in the [[smooth function|smooth]] [[category (mathematics)|category]], and then the result is unique up to [[diffeomorphism]]. There are subtle problems in the smooth case: not every diffeomorphism between the boundaries of the spheres gives the same composite manifold, even if the orientations are chosen correctly. For example, Milnor showed that two 7-cells can be glued along their boundary so that the result is an [[exotic sphere]] homeomorphic but not diffeomorphic to a 7-sphere.\n\nHowever, there is a canonical way to choose the gluing of <math>M_1</math> and <math>M_2</math> which gives a unique well defined connected sum.<ref>Kervaire and Milnor, Groups of Homotopy Spheres I, Annals of Mathematics Vol 77 No 3 May 1963</ref> Choose embeddings <math>i_1 : D_n \\rightarrow M_1</math> and <math>i_2 : D_n \\rightarrow M_2</math> so that <math>i_1</math> preserves orientation and <math>i_2</math> reverses orientation. Now obtain <math>M_1 \\# M_2</math> from the disjoint sum\n\n:<math> (M_1 - i_1(0)) \\sqcup (M_2 - i_2(0))</math>\nby identifying <math>i_1(tu)</math> with <math>i_2((1-t)u)</math> for each unit vector <math>u \\in S^{n-1}</math> and each <math>0 < t < 1</math>. Choose the orientation for <math>M_1 \\# M_2</math> which is compatible with <math>M_1</math> and <math>M_2</math>. The fact that this construction is well-defined depends crucially on the [[disc theorem]], which is not at all obvious. For further details, see <ref>Kosinski, Differential Manifolds, Academic Press Inc (1992).</ref>\n\nThe operation of connected sum is denoted by <math>\\#</math>; for example <math>A \\# B</math> denotes the connected sum of <math>A</math> and <math>B</math>.\n\nThe operation of connected sum has the sphere <math>S^m</math> as an [[identity (mathematics)|identity]]; that is, <math>M \\# S^m</math> is homeomorphic (or diffeomorphic) to <math>M</math>.\n\nThe classification of closed surfaces, a foundational and historically significant result in topology, states that any closed surface can be expressed as the connected sum of a sphere with some number <math>g</math> of [[torus|tori]] and some number <math>k</math> of [[real projective plane]]s.\n\n== Connected sum along a submanifold ==\n\nLet <math>M_1</math> and <math>M_2</math> be two smooth, oriented manifolds of equal dimension and <math>V</math> a smooth, closed, oriented manifold, embedded as a submanifold into both <math>M_1</math> and <math>M_2</math>. Suppose furthermore that there exists an isomorphism of [[normal bundle]]s\n\n:<math>\\psi: N_{M_1} V \\to N_{M_2} V</math>\n\nthat reverses the orientation on each fiber. Then <math>\\psi</math> induces an orientation-preserving diffeomorphism\n\n:<math>N_1 \\setminus V \\cong N_{M_1} V \\setminus V \\to N_{M_2} V \\setminus V \\cong N_2 \\setminus V,</math>\n\nwhere each normal bundle <math>N_{M_i} V</math> is diffeomorphically identified with a neighborhood <math>N_i</math> of <math>V</math> in <math>M_i</math>, and the map\n\n:<math>N_{M_2} V \\setminus V \\to N_{M_2} V \\setminus V</math>\n\nis the orientation-reversing diffeomorphic involution\n\n:<math>v \\mapsto v / |v|^2</math>\n\non [[normal vector]]s. The '''connected sum''' of <math>M_1</math> and <math>M_2</math> along <math>V</math> is then the space\n\n:<math>(M_1 \\setminus V) \\bigcup_{N_1 \\setminus V = N_2 \\setminus V} (M_2 \\setminus V)</math>\n\nobtained by gluing the deleted neighborhoods together by the orientation-preserving diffeomorphism. The sum is often denoted\n\n:<math>(M_1, V) \\# (M_2, V).</math>\n\nIts diffeomorphism type depends on the choice of the two embeddings of <math>V</math> and on the choice of <math>\\psi</math>.\n\nLoosely speaking, each normal fiber of the submanifold <math>V</math> contains a single point of <math>V</math>, and the connected sum along <math>V</math> is simply the connected sum as described in the preceding section, performed along each fiber. For this reason, the connected sum along <math>V</math> is often called the '''fiber sum'''.\n\nThe special case of <math>V</math> a point recovers the connected sum of the preceding section.\n\n== Connected sum along a codimension-two submanifold ==\n\nAnother important special case occurs when the dimension of <math>V</math> is two less than that of the <math>M_i</math>. Then the isomorphism <math>\\psi</math> of normal bundles exists whenever their [[Euler class]]es are opposite:\n\n:<math>e(N_{M_1} V) = -e(N_{M_2} V).</math>\n\nFurthermore, in this case the [[structure group]] of the normal bundles is the [[circle group]] <math>SO(2)</math>; it follows that the choice of embeddings can be canonically identified with the group of [[homotopy]] classes of maps from <math>V</math> to the circle, which in turn equals the first integral [[cohomology]] group <math>H^1(V)</math>. So the diffeomorphism type of the sum depends on the choice of <math>\\psi</math> and a choice of element from <math>H^1(V)</math>.\n\nA connected sum along a codimension-two <math>V</math> can also be carried out in the category of [[symplectic manifold]]s; this elaboration is called the [[symplectic sum]].\n\n== Local operation ==\n\nThe connected sum is a local operation on manifolds, meaning that it alters the summands only in a [[neighborhood (mathematics)|neighborhood]] of <math>V</math>. This implies, for example, that the sum can be carried out on a single manifold <math>M</math> containing two [[Disjoint sets|disjoint]] copies of <math>V</math>, with the effect of gluing <math>M</math> to itself. For example, the connected sum of a two-sphere at two distinct points of the sphere produces the two-torus.\n\n== Connected sum of knots ==\n\nThere is a closely related notion of the connected sum of two knots. In fact, if one regards a knot merely as a one-manifold, then the connected sum of two knots is just their connected sum as a one-dimensional manifold. However, the essential property of a knot is not its manifold structure (under which every knot is equivalent to a circle) but rather its [[embedding]] into the [[ambient space]]. So the connected sum of knots has a more elaborate definition that produces a well-defined embedding, as follows.\n\n[[Image:Sum_of_knots.png|300px|center|thumb| Consider disjoint planar projections of each knot.]][[Image:Sum_of_knots2.png|thumb|center|300px|Find a rectangle in the plane where one pair of sides are arcs along each knot but is otherwise disjoint from the knots.]][[Image:Sum_of_knots3.svg|thumb|center|300px|Now join the two knots together by deleting these arcs from the knots and adding the arcs that form the other pair of sides of the rectangle.]]\n\nThis procedure results in the projection of a new knot, a '''connected sum''' (or '''knot sum''', or '''composition''') of the original knots.  For the connected sum of knots to be well defined, one has to consider '''oriented knots''' in 3-space.  To define the connected sum for two oriented knots:\n\n# Consider a planar projection of each knot and suppose these projections are disjoint.\n# Find a rectangle in the plane where one pair of sides are arcs along each knot but is otherwise disjoint from the knots '''and''' so that the arcs of the knots on the sides of the rectangle are oriented around the boundary of the rectangle in the '''same direction'''.\n# Now join the two knots together by deleting these arcs from the knots and adding the arcs that form the other pair of sides of the rectangle.\n\nThe resulting connected sum knot inherits an orientation consistent with the orientations of the two original knots, and the oriented ambient isotopy class of the result is well-defined, depending only on the oriented ambient isotopy classes of the original two knots.\n\nUnder this operation, oriented knots in 3-space form a commutative [[monoid]] with unique [[prime factorization]], which allows us to define what is meant by a [[prime knot]].  Proof of commutativity can be seen by letting one summand shrink until it is very small and then pulling it along the other knot.  The unknot is the unit.  The two trefoil knots are the simplest [[prime knot]]s. Higher-dimensional knots can be added by splicing the <math>n</math>-spheres.\n\nIn three dimensions, the unknot cannot be written as the sum of two non-trivial knots. This fact follows from additivity of [[knot genus]]; another proof relies on an infinite construction sometimes called the [[Mazur swindle]].  In higher dimensions (with codimension at least three), it is possible to get an unknot by adding two nontrivial knots.\n\nIf one does '''not''' take into account the orientations of the knots, the connected sum operation is not well defined on isotopy classes of (nonoriented) knots.  To see this, consider two noninvertible knots ''K, L'' which are not equivalent (as unoriented knots); for example take the two pretzel knots ''K'' = ''P''(3,5,7) and ''L'' = ''P''(3,5,9).  Let ''K''<sub>+</sub> and ''K''<sub>−</sub> be ''K'' with its two inequivalent orientations, and let ''L''<sub>+</sub> and ''L''<sub>−</sub> be ''L'' with its two inequivalent orientations.  There are four oriented connected sums we may form:\n\n* ''A'' = ''K''<sub>+</sub> # ''L''<sub>+</sub>\n* ''B'' = ''K''<sub>−</sub> # ''L''<sub>−</sub>\n* ''C'' = ''K''<sub>+</sub> # ''L''<sub>−</sub>\n* ''D'' = ''K''<sub>−</sub> # ''L''<sub>+</sub>\n\nThe oriented ambient isotopy classes of these four oriented knots are all distinct.  And, when one considers ambient isotopy of the knots without regard to orientation, there are '''two distinct''' equivalence classes:  { ''A'' ~ ''B'' } and { ''C'' ~ ''D'' }.  To see that ''A'' and ''B'' are unoriented equivalent, simply note that they both may be constructed from the same pair of disjoint knot projections as above, the only difference being the orientations of the knots.  Similarly, one sees that ''C'' and ''D'' may be constructed from the same pair of disjoint knot projections.\n\n== See also ==\n\n*[[Band sum]]\n*[[Prime decomposition (3-manifold)]]\n*[[Manifold decomposition]]\n*[[Satellite knot]]\n\n== Further reading ==\n* [[Robert Gompf]]: A new construction of symplectic manifolds, ''Annals of Mathematics'' 142 (1995), 527–595\n* William S. Massey, ''A Basic Course in Algebraic Topology'', Springer-Verlag, 1991. {{ISBN|0-387-97430-X}}.\n\n==References==\n{{Reflist}}\n\n[[Category:Differential topology]]\n[[Category:Geometric topology]]\n[[Category:Knot theory]]\n[[Category:Binary operations]]"
    },
    {
      "title": "Convolution",
      "url": "https://en.wikipedia.org/wiki/Convolution",
      "text": "{{About||the usage in formal language theory|Convolution (computer science)|other uses|Convolute (disambiguation){{!}}Convolute}}\n\n[[File:comparison convolution correlation.svg|thumb|400px|Visual comparison of convolution, [[cross-correlation]], and [[autocorrelation]].  For the operations involving function {{mvar|f}}, and assuming the height of {{mvar|f}} is 1.0, the value of the result at 5 different points is indicated by the shaded area below each point.  Also, the symmetry of {{mvar|f}} is the reason <math>f*g</math> and <math>f \\star g</math> are identical in this example.\n<!--Note that f∗g and g∗f would be identical even without the symmetry of f, so please don't change the statement above.-->]]\n\nIn [[mathematics]] (in particular, [[functional analysis]]) '''convolution''' is a [[operation (mathematics)|mathematical operation]] on two [[function (mathematics)|function]]s ({{mvar|f}} and {{mvar|g}}) to produce a third function that expresses how the shape of one is modified by the other.  The term ''convolution'' refers to both the result function and to the process of computing it.  Some features of convolution are similar to [[cross-correlation]]: for real-valued functions, of a continuous or discrete variable, it differs from cross-correlation only in that either {{math|''f'' (''x'')}} or {{math|''g''(''x'')}} is reflected about the y-axis; thus it is a cross-correlation of {{math|''f'' (''x'')}} and {{math|''g''(−''x'')}}, or {{math|''f'' (−''x'')}} and {{math|''g''(''x'')}}.<ref group=\"note\">Reasons for the reflection include:\n*It is necessary to implement the equivalent of the pointwise product of the Fourier transforms of {{mvar|f}} and {{mvar|g}}.\n*When the convolution is viewed as a [[Moving-average_model|moving weighted average]], the weighting function, {{math|''g''(−''x'')}}, is often specified in terms of another function, {{math|''g''(''x'')}}, called the [[impulse response]] of a [[Linear_time-invariant_system#Impulse_response_and_convolution|linear time-invariant system]].</ref>&nbsp; For continuous functions, the cross-correlation operator is the [[Hermitian adjoint|adjoint]] of the convolution operator.\n\nConvolution has applications that include [[probability]], [[statistics]], [[computer vision]], [[natural language processing]], [[image processing|image]] and [[signal processing]], [[engineering]], and [[differential equations]].{{citation needed|date=October 2017}}\n\nThe convolution can be defined for functions on [[Euclidean space]], and other [[group (mathematics)|groups]]. {{citation needed|date=October 2017}} For example, [[periodic function]]s, such as the [[discrete-time Fourier transform]], can be defined on a [[circle]] and convolved by ''periodic convolution''. (See row 13 at [[DTFT#Properties|DTFT &sect; Properties]].) A ''discrete convolution'' can be defined for functions on the set of [[integers]].\n\nGeneralizations of convolution have applications in the field of [[numerical analysis]] and [[numerical linear algebra]], and in the design and implementation of [[finite impulse response]] filters in signal processing.{{citation needed|date=October 2017}}\n\nComputing the inverse of the convolution operation is known as [[deconvolution]].\n\n==Definition==\n\nThe convolution of {{mvar|f}} and {{mvar|g}} is written {{math|''f''∗''g''}}, using an [[asterisk]]. It is defined as the integral of the product of the two functions after one is reversed and shifted. As such, it is a particular kind of [[integral transform]]:\n\n{{Equation box 1\n|indent =\n|title=\n|equation = <math>\n(f * g)(t) \\triangleq\\ \\int_{-\\infty}^\\infty f(\\tau) g(t - \\tau) \\, d\\tau.\n</math>\n|cellpadding= 6\n|border\n|border colour = #0073CF\n|background colour=#F5FFFA}}\n\nAn equivalent definition is (see [[#Properties|commutativity]]):\n\n:<math>(f * g)(t) \\triangleq\\ \\int_{-\\infty}^\\infty f(t-\\tau) g(\\tau)\\, d\\tau.</math>\n\nWhile the symbol {{mvar|t}} is used above, it need not represent the time domain.&nbsp; But in that context, the convolution formula can be described as a weighted average of the function {{math|''f'' (''τ'')}} at the moment {{mvar|t}} where the weighting is given by {{math|''g''(–''τ'')}} simply shifted by amount {{math|''t''}}.&nbsp; As {{mvar|t}} changes, the weighting function emphasizes different parts of the input function.\n\nFor functions {{mvar|f}}, {{mvar|g}} [[Support (mathematics)|supported]] on only {{math|[0, ∞)}} (i.e., zero for negative arguments), the integration limits can be truncated, resulting in:\n\n:<math>(f * g )(t) = \\int_{0}^{t} f(\\tau) g(t - \\tau)\\, d\\tau \\quad \\ \\text{for } f, g : [0, \\infty) \\to \\mathbb{R}.</math>\n\nFor the multi-dimensional formulation of convolution, see ''[[#Domain of definition|domain of definition]]'' (below).\n\n=== Notation ===\n\nA common engineering convention is:<ref>\n{{cite book|last1=Smith|first1=Stephen W|title=The Scientist and Engineer's Guide to Digital Signal Processing|date=1997|publisher=California Technical Publishing|isbn=0966017633|edition=1|url=http://www.dspguide.com/ch13/2.htm|accessdate=22 April 2016|chapter=13.Convolution}}</ref>\n\n:<math> f(t) * g(t) \\, \\triangleq\\ \\underbrace{\\int_{-\\infty}^\\infty f(\\tau) g(t - \\tau)\\, d\\tau}_{(f * g )(t)},</math>\n\nwhich has to be interpreted carefully to avoid confusion.&nbsp; For instance, {{math|''f'' (''t'')∗''g''(''t'' − ''t''<sub>0</sub>)}} is equivalent to {{math|(''f'' ∗''g'')(''t'' − ''t''<sub>0</sub>)}},&nbsp; but {{math|''f'' (''t'' − ''t''<sub>0</sub>)∗''g''(''t'' − ''t''<sub>0</sub>)}} is in fact equivalent to {{math|(''f'' ∗''g'')(''t'' − 2''t''<sub>0</sub>)}}.<ref>{{cite book|last1=Irwin|first1=J. David|title=The Industrial Electronics Handbook|date=1997|publisher=CRC Press|location=Boca Raton, FL|isbn=0849383439|page=75|edition=1|chapter=4.3}}</ref>\n\n=== Derivations ===\nConvolution describes the output (in terms of the input) of an important class of operations known as ''linear time-invariant'' (LTI). See [[LTI system theory#Overview|LTI system theory]] for a derivation of convolution as the result of LTI constraints. In terms of the [[Fourier transforms]] of the input and output of an LTI operation, no new frequency components are created. The existing ones are only modified (amplitude and/or phase). In other words, the output transform is the pointwise product of the input transform with a third transform (known as a [[transfer function]]). See [[Convolution theorem]] for a derivation of that property of convolution. Conversely, convolution can be derived as the inverse Fourier transform of the pointwise product of two Fourier transforms.\n\n== Visual explanation ==\n\n{| class=\"wikitable\"\n|-\n!colspan=2 |Visual explanations of convolution\n|-\n|\n# Express each function in terms of a [[Free variables and bound variables|dummy variable]] <math>\\tau.</math>\n# Reflect one of the functions: <math>g(\\tau)</math>→<math>g(-\\tau).</math>\n# Add a time-offset, {{mvar|t}}, which allows <math>g(t-\\tau)</math> to slide along the <math>\\tau</math>-axis.\n# Start {{mvar|t}} at {{math|−∞}} and slide it all the way to {{math|+∞}}. Wherever the two functions intersect, find the integral of their product. In other words, compute a <u>sliding</u>, weighted-sum of function <math>f(\\tau),</math> where the weighting function is <math>g(-\\tau).</math>\n:The resulting [[waveform]] (not shown here) is the convolution of functions {{mvar|f}} and {{mvar|g}}.\n\n:If {{math|''f'' (''t'')}} is a [[unit impulse]], the result of this process is simply {{math|''g''(''t'')}}. Formally:\n\n::<math>\\int_{-\\infty}^\\infty \\delta(\\tau) g(t - \\tau)\\, d\\tau = g(t)</math>\n|[[File:Convolution3.svg|right|452x452px]]\n|-\n|\n:In this example, the red-colored \"pulse\", <math>\\ g(\\tau),</math> is an [[even function]] <math>(\\ g(-\\tau)=g(\\tau)\\ ),</math> so convolution is equivalent to correlation. A snapshot of this \"movie\" shows functions <math> g(t-\\tau)</math> and <math> f(\\tau)</math> (in blue) for some value of parameter <math> t,</math> which is arbitrarily defined as the distance from the <math> \\tau=0</math> axis to the center of the red pulse. The amount of yellow is the area of the product <math> f(\\tau)\\cdot g(t-\\tau),</math> computed by the convolution/correlation integral. The movie is created by continuously changing <math> t</math> and recomputing the integral. The result (shown in black) is a function of <math> t,</math> but is plotted on the same axis as <math> \\tau,</math> for convenience and comparison.\n|[[File:Convolution of box signal with itself2.gif|thumb|right|475px]]\n|-\n|\n:In this depiction, <math>f(\\tau)</math> could represent the response of an RC circuit to a narrow pulse that occurs at <math>\\tau=0.</math> In other words, if <math>g(\\tau)=\\delta(\\tau),</math> the result of convolution is just <math>f(t).</math> But when <math>g(\\tau)</math> is the wider pulse (in red), the response is a \"smeared\" version of <math>f(t).</math> It begins at <math>t=-0.5,</math> because we defined <math> t</math> as the distance from the <math> \\tau=0</math> axis to the <u>center</u> of the wide pulse (instead of the leading edge).\n|[[File:Convolution of spiky function with box2.gif|thumb|right|475px]]\n|}\n\n==Historical developments==\nOne of the earliest uses of the convolution integral appeared in [[Jean le Rond d'Alembert|D'Alembert]]'s derivation of [[Taylor's theorem]] in ''Recherches sur différents points importants du système du monde,'' published in 1754.<ref>Dominguez-Torres, p 2</ref>\n\nAlso, an expression of the type:\n\n:<math>\\int f(u)\\cdot g(x-u) \\, du</math>\n\nis used by [[Sylvestre François Lacroix]] on page 505 of his book entitled ''Treatise on differences and series'', which is the last of 3 volumes of the encyclopedic series: ''Traité du calcul différentiel et du calcul intégral'', Chez Courcier, Paris, 1797–1800.<ref>Dominguez-Torres, p 4</ref> Soon thereafter, convolution operations appear in the works of [[Pierre Simon Laplace]], [[Jean-Baptiste Joseph Fourier]], [[Siméon Denis Poisson]], and others. The term itself did not come into wide use until the 1950s or 60s. Prior to that it was sometimes known as ''Faltung'' (which means ''folding'' in [[German language|German]]), ''composition product'', ''superposition integral'', and ''Carson's integral''.<ref>\n{{Citation\n | chapter = Early work on imaging theory in radio astronomy\n | author = R. N. Bracewell\n | editor = W. T. Sullivan\n | title = The Early Years of Radio Astronomy: Reflections Fifty Years After Jansky's Discovery\n | edition = \n | publisher = Cambridge University Press\n | year = 2005\n | isbn = 978-0-521-61602-7\n | page = 172\n | url = https://books.google.com/books?id=v2SqL0zCrwcC&pg=PA172\n }}</ref>\nYet it appears as early as 1903, though the definition is rather unfamiliar in older uses.<ref>\n{{Citation\n | title = The algebra of invariants\n | author = John Hilton Grace and Alfred Young\n | publisher = Cambridge University Press\n | year = 1903\n | page = 40\n | url = https://books.google.com/books?id=NIe4AAAAIAAJ&pg=PA40\n }}</ref><ref>\n{{Citation\n | title = Algebraic invariants\n | author = Leonard Eugene Dickson\n | publisher = J. Wiley\n | year = 1914\n | page = 85\n | url = https://books.google.com/books?id=LRGoAAAAIAAJ&pg=PA85\n }}</ref>\n\nThe operation:\n\n:<math>\\int_0^t\\varphi(s)\\psi(t-s) \\, ds, \\qquad 0\\le t<\\infty,</math>\n\nis a particular case of composition products considered by the Italian mathematician [[Vito Volterra]] in 1913.<ref>\nAccording to\n[Lothar von Wolfersdorf (2000), \"Einige Klassen quadratischer Integralgleichungen\",\n''Sitzungsberichte der Sächsischen Akademie der Wissenschaften zu Leipzig'',\n''Mathematisch-naturwissenschaftliche Klasse'', volume '''128''', number 2, 6–7], the source is Volterra, Vito (1913),\n\"Leçons sur les fonctions de linges\". Gauthier-Villars, Paris 1913.</ref>\n\n== Circular convolution ==\n{{Main article|Circular convolution}}\n\nWhen a function {{math|''g''{{sub|''T''}}}} is periodic, with period {{mvar|T}}, then for functions, {{mvar|f}}, such that {{math|''f'' ∗ ''g''{{sub|''T''}}}} exists, the convolution is also periodic and identical to:\n\n:<math>(f * g_T)(t) \\equiv \\int_{t_0}^{t_0+T} \\left[\\sum_{k=-\\infty}^\\infty f(\\tau + kT)\\right] g_T(t - \\tau)\\, d\\tau,</math>\n\nwhere {{math|''t''<sub>0</sub>}} is an arbitrary choice. The summation is called a [[periodic summation]] of the function {{mvar|f}}.\n\nWhen {{math|''g''{{sub|{{sub|''T''}}}}}} is a periodic summation of another function, {{mvar|g}}, then {{math|''f'' ∗ ''g''{{sub|''T''}}}} is known as a ''circular'' or ''cyclic'' convolution of {{mvar|f}} and {{mvar|g}}.\n\nAnd if the periodic summation above is replaced by {{math|''f''{{sub|''T''}}}}, the operation is called a ''periodic'' convolution of {{math|''f''{{sub|''T''}}}} and {{math|''g''{{sub|''T''}}}}.\n\n==Discrete convolution==\n\nFor complex-valued functions {{math|''f'', ''g''}} defined on the set '''Z''' of integers, the ''discrete convolution'' of {{mvar|f}} and {{mvar|g}} is given by:<ref>{{harvnb|Damelin|Miller|2011|p=232}}</ref>\n\n{{Equation box 1\n|indent =\n|title=\n|equation = <math>(f * g)[n] = \\sum_{m=-\\infty}^\\infty f[m] g[n - m]</math>\n|cellpadding= 6\n|border\n|border colour = #0073CF\n|background colour=#F5FFFA}}\n\nor equivalently (see [[#Properties|commutativity]]) by:\n\n:<math>(f * g)[n] = \\sum_{m=-\\infty}^\\infty f[n-m] g[m].</math>\n\nThe convolution of two finite sequences is defined by extending the sequences to finitely supported functions on the set of integers. When the sequences are the coefficients of two [[polynomial]]s, then the coefficients of the ordinary product of the two polynomials are the convolution of the original two sequences. This is known as the [[Cauchy product]] of the coefficients of the sequences.\n\nThus when {{mvar|g}} has finite support in the set <math>\\{-M,-M+1,\\dots,M-1,M\\}</math> (representing, for instance, a [[finite impulse response]]), a finite summation may be used:<ref>{{cite book |last1=Press |first1=William H. |last2=Flannery |first2=Brian P.|last3=Teukolsky |first3=Saul A. |last4=Vetterling |first4=William T. |title= Numerical Recipes in Pascal|edition= |year=1989 |publisher=Cambridge University Press |isbn=0-521-37516-9|page=450}}</ref>\n:<math>(f* g)[n]=\\sum_{m=-M}^M f[n-m]g[m].</math>\n\n=== Circular discrete convolution ===\nWhen a function {{math|''g''{{sub|{{sub|''N''}}}}}} is periodic, with period {{mvar|''N''}}, then for functions, {{mvar|f}}, such that {{math|''f''∗''g''{{sub|{{sub|''N''}}}}}} exists, the convolution is also periodic and identical to:\n\n:<math>(f * g_N)[n] \\equiv \\sum_{m=0}^{N-1} \\left(\\sum_{k=-\\infty}^\\infty {f}[m+kN] \\right) g_N[n-m].</math>\n\nThe summation on {{mvar|k}} is called a [[periodic summation]] of the function {{mvar|f}}.\n\nIf {{math|''g''{{sub|{{sub|''N''}}}}}} is a periodic summation of another function, {{mvar|g}}, then {{math|''f''∗''g''{{sub|{{sub|''N''}}}}}} is known as a [[circular convolution]] of {{mvar|f}} and {{mvar|g}}.\n\nWhen the non-zero durations of both {{mvar|f}} and {{mvar|g}} are limited to the interval {{math|[0, ''N''−1]}},&nbsp; {{math|''f''∗''g''{{sub|{{sub|''N''}}}}}} reduces to these common forms:\n\n{{NumBlk|:|<math>\\begin{align}\n(f * g_N)[n] &= \\sum_{m=0}^{N-1} f[m]g_N[n-m]\\\\\n &= \\sum_{m=0}^n f[m]g[n-m] + \\sum_{m=n+1}^{N-1} f[m]g[N+n-m]\\\\\n &= \\sum_{m=0}^{N-1} f[m]g[(n-m)_{\\bmod{N}}] \\triangleq (f *_{_N} g)[n]\n\\end{align}</math>|{{EquationRef|Eq.1}}}}\n\nThe notation ({{math|''f'' ∗{{sub|{{sub|''N''}}}} ''g''}}) for ''cyclic convolution'' denotes convolution over the [[cyclic group]] of [[modular arithmetic|integers modulo {{math|''N''}}]].\n\nCircular convolution arises most often in the context of fast convolution with a [[fast Fourier transform]] (FFT) algorithm.\n\n===Fast convolution algorithms===\n\nIn many situations, discrete convolutions can be converted to circular convolutions so that fast transforms with a convolution property can be used to implement the computation. For example, convolution of digit sequences is the kernel operation in [[multiplication]] of multi-digit numbers, which can therefore be efficiently implemented with transform techniques ({{harvnb|Knuth|1997|loc=§4.3.3.C}}; {{harvnb|von zur Gathen|Gerhard|2003|loc=§8.2}}).\n\n{{EquationNote|Eq.1}} requires {{mvar|N}} arithmetic operations per output value and {{math|''N''<sup>2</sup>}} operations for {{mvar|N}} outputs. That can be significantly reduced with any of several fast algorithms. [[Digital signal processing]] and other applications typically use fast convolution algorithms to reduce the cost of the convolution to O({{mvar|N}} log {{mvar|N}}) complexity.\n\nThe most common fast convolution algorithms use [[fast Fourier transform]] (FFT) algorithms via the [[Discrete Fourier transform#Circular convolution theorem and cross-correlation theorem|circular convolution theorem]]. Specifically, the [[circular convolution]] of two finite-length sequences is found by taking an FFT of each sequence, multiplying pointwise, and then performing an inverse FFT. Convolutions of the type defined above are then efficiently implemented using that technique in conjunction with zero-extension and/or discarding portions of the output. Other fast convolution algorithms, such as the [[Schönhage–Strassen algorithm]] or the Mersenne transform,<ref name=Rader1972>{{cite journal|last=Rader|first=C.M.|title=Discrete Convolutions via Mersenne Transforms|journal=IEEE Transactions on Computers|date=December 1972|volume=21|issue=12|pages=1269–1273|doi=10.1109/T-C.1972.223497|url=http://doi.ieeecomputersociety.org/10.1109/T-C.1972.223497|accessdate=17 May 2013}}</ref> use fast Fourier transforms in other [[ring (mathematics)|ring]]s.\n\nIf one sequence is much longer than the other, zero-extension of the shorter sequence and fast circular convolution is not the most computationally efficient method available.<ref name=Madisetti1999>{{cite book|last=Madisetti|first=Vijay K.|title=\"Fast Convolution and Filtering\" in the \"Digital Signal Processing Handbook\"|year=1999|publisher=CRC Press LLC|isbn=9781420045635|page=Section 8|url=ftp://idc18.seu.edu.cn/Pub2/EBooks/Books_from_EngnetBase/pdf/2135/08.PDF}}</ref> Instead, decomposing the longer sequence into blocks and convolving each block allows for faster algorithms such as the [[Overlap–save method]] and [[Overlap–add method]].<ref name=Juang2004>{{cite web|last=Juang|first=B.H.|title=Lecture 21: Block Convolution|url=http://users.ece.gatech.edu/~juang/4270/BHJ4270-21.pdf|publisher=EECS at the Georgia Institute of Technology|accessdate=17 May 2013}}</ref> A hybrid convolution method that combines block and [[Finite impulse response|FIR]] algorithms allows for a zero input-output latency that is useful for real-time convolution computations.<ref name=Gardner1994>{{cite journal|last=Gardner|first=William G.|title=Efficient Convolution without Input/Output Delay|journal=Audio Engineering Society Convention 97|date=November 1994|series=Paper 3897|url=http://www.cs.ust.hk/mjg_lib/bibs/DPSu/DPSu.Files/Ga95.PDF|accessdate=17 May 2013}}</ref>\n\n==Domain of definition==\n\nThe convolution of two complex-valued functions on {{math|'''R'''<sup>''d''</sup>}} is itself a complex-valued function on {{math|'''R'''<sup>''d''</sup>}}, defined by:\n\n:<math>(f * g )(x) = \\int_{\\mathbf{R}^d} f(y)g(x-y)\\,dy = \\int_{\\mathbf{R}^d} f(x-y)g(y)\\,dy,</math>\n\nis well-defined only if {{mvar|f}} and {{mvar|g}} decay sufficiently rapidly at infinity in order for the integral to exist. Conditions for the existence of the convolution may be tricky, since a blow-up in {{mvar|g}} at infinity can be easily offset by sufficiently rapid decay in {{mvar|f}}. The question of existence thus may involve different conditions on {{mvar|f}} and {{mvar|g}}:\n\n===Compactly supported functions===\nIf {{mvar|f}} and {{mvar|g}} are [[compact support|compactly supported]] [[continuous function]]s, then their convolution exists, and is also compactly supported and continuous {{harv|Hörmander|1983|loc=Chapter 1}}. More generally, if either function (say {{mvar|f}}) is compactly supported and the other is [[locally integrable function|locally integrable]], then the convolution {{math|''f''∗''g''}} is well-defined and continuous.\n\nConvolution of {{mvar|f}} and {{mvar|g}} is also well defined when both functions are locally square integrable on {{math|'''R'''}} and supported on an interval of the form {{math|[''a'', +∞)}} (or both supported on {{math|[−∞, ''a'']}}).\n\n===Integrable functions===\nThe convolution of {{mvar|f}} and {{mvar|g}} exists if {{mvar|f}} and {{mvar|g}} are both [[Lebesgue integral|Lebesgue integrable functions]] in [[Lp space|{{math|''L''<sup>1</sup>}}({{math|'''R'''<sup>''d''</sup>}})]], and in this case {{math|''f''∗''g''}} is also integrable {{harv|Stein|Weiss|1971|loc=Theorem 1.3}}. This is a consequence of [[Fubini's theorem#Tonelli's theorem|Tonelli's theorem]]. This is also true for functions in {{math|''L''<sup>1</sup>}}, under the discrete convolution, or more generally for the [[#Convolutions on groups|convolution on any group]].\n\nLikewise, if {{math|''f'' ∈ ''L''<sup>1</sup>}}({{math|'''R'''<sup>''d''</sup>}})&nbsp; and &nbsp;{{math|''g'' ∈ ''L''<sup>''p''</sup>}}({{math|'''R'''<sup>''d''</sup>}})&nbsp; where {{math|1 ≤ ''p'' ≤ ∞}},&nbsp; then &nbsp;{{math|''f''∗''g'' ∈ ''L''<sup>''p''</sup>}}({{math|'''R'''<sup>''d''</sup>}}),&nbsp; and\n\n:<math>\\|{f}* g\\|_p\\le \\|f\\|_1\\|g\\|_p.</math>\n\nIn the particular case {{math|''p'' {{=}} 1}}, this shows that {{math|''L''<sup>1</sup>}} is a [[Banach algebra]] under the convolution (and equality of the two sides holds if {{mvar|f}} and {{mvar|g}} are non-negative almost everywhere).\n \nMore generally, [[Young's convolution inequality|Young's inequality]] implies that the convolution is a continuous bilinear map between suitable {{math|''L''<sup>''p''</sup>}} spaces. Specifically, if {{math| 1 ≤ ''p'', ''q'', ''r'' ≤ ∞ }} satisfy:\n\n:<math>\\frac{1}{p}+\\frac{1}{q}=\\frac{1}{r}+1,</math>\n\nthen\n\n:<math>\\left\\Vert f*g\\right\\Vert_r\\le\\left\\Vert f\\right\\Vert_p\\left\\Vert g\\right\\Vert_q,\\quad f\\in\\mathcal{L}^p,\\ g\\in\\mathcal{L}^q,</math>\n\nso that the convolution is a continuous bilinear mapping from {{math|''L''<sup>''p''</sup>×''L''<sup>''q''</sup>}} to {{math|''L''<sup>r</sup>}}.\nThe Young inequality for convolution is also true in other contexts (circle group, convolution on {{math|'''Z'''}}). The preceding inequality is not sharp on the real line: when {{math| 1 < ''p'', ''q'', ''r'' < ∞}}, there exists a constant {{math|''B''<sub>''p'',''q''</sub> < 1}} such that:\n\n:<math>\\left\\Vert f*g\\right\\Vert_r\\le B_{p,q}\\left\\Vert f\\right\\Vert_p\\left\\Vert g\\right\\Vert_q,\\quad f\\in\\mathcal{L}^p,\\ g\\in\\mathcal{L}^q.</math>\n\nThe optimal value of {{math|''B''<sub>''p'',''q''</sub>}} was discovered in 1975.<ref>\n[[William Beckner (mathematician)|Beckner, William]] (1975), \"Inequalities in Fourier analysis\", Ann. of Math. (2) '''102''': 159&ndash;182. Independently, Brascamp, Herm J. and [[Elliott H. Lieb|Lieb, Elliott H.]] (1976), \"Best constants in Young's inequality, its converse, and its generalization to more than three functions\", Advances in Math. '''20''': 151&ndash;173. See [[Brascamp–Lieb inequality]]</ref>\n\nA stronger estimate is true provided {{math| 1 < ''p'', ''q'', ''r'' < ∞ }}:\n:<math>\\|f* g\\|_r\\le C_{p,q}\\|f\\|_p\\|g\\|_{q,w}</math>\nwhere <math>\\|g\\|_{q,w}</math> is the [[Lp space#Weak Lp|weak {{math|''L''<sup>''q''</sup>}}]] norm. Convolution also defines a bilinear continuous map <math>L^{p,w}\\times L^{q,w}\\to L^{r,w}</math> for <math>1< p,q,r<\\infty</math>, owing to the weak Young inequality:<ref>{{harvnb|Reed|Simon|1975|loc=IX.4}}</ref>\n:<math>\\|f* g\\|_{r,w}\\le C_{p,q}\\|f\\|_{p,w}\\|g\\|_{r,w}.</math>\n\n===Functions of rapid decay===\nIn addition to compactly supported functions and integrable functions, functions that have sufficiently rapid decay at infinity can also be convolved. An important feature of the convolution is that if ''f'' and ''g'' both decay rapidly, then ''f''∗''g'' also decays rapidly. In particular, if ''f'' and ''g'' are [[rapidly decreasing function]]s, then so is the convolution ''f''∗''g''. Combined with the fact that convolution commutes with differentiation (see [[#Properties]]), it follows that the class of [[Schwartz function]]s is closed under convolution {{harv|Stein|Weiss|1971|loc=Theorem 3.3}}.\n\n===Distributions===\n{{Main article|Distribution (mathematics)}}\nUnder some circumstances, it is possible to define the convolution of a function with a distribution, or of two distributions. If ''f'' is a [[Support (mathematics)#Compact_support|compactly supported]] function and ''g'' is a distribution, then ''f''∗''g'' is a smooth function defined by a distributional formula analogous to\n\n:<math>\\int_{\\mathbf{R}^d} {f}(y)g(x-y)\\,dy.</math>\n\nMore generally, it is possible to extend the definition of the convolution in a unique way so that the associative law\n\n:<math>f* (g* \\varphi) = (f* g)* \\varphi</math>\n\nremains valid in the case where ''f'' is a distribution, and ''g'' a compactly supported distribution {{harv|Hörmander|1983|loc=§4.2}}.\n\n===Measures===\n\nThe convolution of any two [[Borel measure]]s μ and ν of [[bounded variation]] is the measure λ defined by {{harv|Rudin|1962}}\n:<math>\\int_{\\mathbf{R}^d} f(x) \\, d\\lambda(x) = \\int_{\\mathbf{R}^d}\\int_{\\mathbf{R}^d}f(x+y)\\,d\\mu(x)\\,d\\nu(y).</math>\nThis agrees with the convolution defined above when μ and ν are regarded as distributions, as well as the convolution of L<sup>1</sup> functions when μ and ν are absolutely continuous with respect to the Lebesgue measure.\n\nThe convolution of measures also satisfies the following version of Young's inequality\n:<math>\\|\\mu* \\nu\\|\\le \\|\\mu\\|\\|\\nu\\| </math>\nwhere the norm is the [[total variation]] of a measure. Because the space of measures of bounded variation is a [[Banach space]], convolution of measures can be treated with standard methods of [[functional analysis]] that may not apply for the convolution of distributions.\n\n==Properties==\n\n===Algebraic properties===\n{{See also|Convolution algebra}}\nThe convolution defines a product on the [[linear space]] of integrable functions. This product satisfies the following algebraic properties, which formally mean that the space of integrable functions with the product given by convolution is a commutative [[associative algebra]] without [[identity element|identity]] {{harv|Strichartz|1994|loc=§3.3}}. Other linear spaces of functions, such as the space of continuous functions of compact support, are [[closure (mathematics)|closed]] under the convolution, and so also form commutative associative algebras.\n\n;[[Commutativity]]\n: <math>f * g = g * f </math>\n\nProof: By definition\n: <math>(f * g)(t) = \\int^\\infty_{-\\infty} f(\\tau)g(t-\\tau) \\, d\\tau</math>\nChanging the variable of integration to <math>u=t-\\tau</math> and the result follows.\n\n;[[Associativity]]\n: <math>f * (g * h) = (f * g) * h </math>\n\nProof: This follows from using [[Fubini's theorem]] (i.e., double integrals can be evaluated as\niterated integrals in either order).\n\n;[[Distributivity]]\n: <math>f * (g + h) = (f * g) + (f * h)</math>\nProof: This follows from linearity of the integral.\n\n;Associativity with scalar multiplication\n: <math>a (f * g) = (a f) * g</math>\nfor any real (or complex) number <math>a</math>.\n\n;[[Multiplicative identity]]\nNo algebra of functions possesses an identity for the convolution. The lack of identity is typically not a major inconvenience, since most collections of functions on which the convolution is performed can be convolved with a [[Dirac delta|delta distribution]] or, at the very least (as is the case of ''L''<sup>1</sup>) admit [[Nascent delta function|approximations to the identity]]. The linear space of compactly supported distributions does, however, admit an identity under the convolution. Specifically,\n:<math>f* \\delta = f</math>\nwhere ''δ'' is the delta distribution.\n\n;Inverse element\n\nSome distributions have an [[inverse element]] for the convolution, ''S''<sup>(&minus;1)</sup>, which is defined by\n: <math>S^{(-1)} * S = \\delta.</math>\nThe set of invertible distributions forms an [[abelian group]] under the convolution.\n\n;Complex conjugation\n\n: <math>\\overline{f * g} = \\overline{f} * \\overline{g}</math>\n\n;Relationship with differentiation\n\n: <math>(f* g)'=f'* g=f* g'</math>\n\nProof:\n: <math>\n\\begin{align}\n(f* g)' & = \\frac{d}{dt} \\int^\\infty_{-\\infty} f(\\tau) g(t-\\tau) \\, d\\tau \\\\[4pt]\n& =\\int^\\infty_{-\\infty} f(\\tau) \\frac{\\partial}{\\partial t} g(t-\\tau) \\, d\\tau \\\\[4pt]\n& =\\int^\\infty_{-\\infty} f(\\tau) g'(t-\\tau) \\, d\\tau= f* g'.\n\\end{align}\n</math>\n\n;Relationship with integration\n:If <math>F(t)=\\int^t_{-\\infty} f(\\tau) d\\tau,</math> and <math>G(t)=\\int^t_{-\\infty} g(\\tau) \\, d\\tau,</math> then\n:<math>(F* g)(t)=(f* G)(t)=\\int^t_{-\\infty}(f* g)(\\tau)\\,d\\tau.</math>\n\n===Integration===\nIf ''f'' and ''g'' are integrable functions, then the integral of their convolution on the whole space is simply obtained as the product of their integrals:\n\n:<math>\\int_{\\mathbf{R}^d}(f* g)(x) \\, dx=\\left(\\int_{\\mathbf{R}^d}f(x) \\, dx\\right) \\left(\\int_{\\mathbf{R}^d}g(x) \\, dx\\right).</math>\n\nThis follows from [[Fubini's theorem]]. The same result holds if ''f'' and ''g'' are only assumed to be nonnegative measurable functions, by [[Fubini's theorem#Tonelli's theorem|Tonelli's theorem]].\n\n===Differentiation===\nIn the one-variable case,\n\n: <math>\\frac{d}{dx}(f * g) = \\frac{df}{dx} * g = f * \\frac{dg}{dx}</math>\n\nwhere ''d''/''dx'' is the [[derivative]]. More generally, in the case of functions of several variables, an analogous formula holds with the [[partial derivative]]:\n\n:<math>\\frac{\\partial}{\\partial x_i}(f * g) = \\frac{\\partial f}{\\partial x_i} * g = f * \\frac{\\partial g}{\\partial x_i}.</math>\n\nA particular consequence of this is that the convolution can be viewed as a \"smoothing\" operation: the convolution of ''f'' and ''g'' is differentiable as many times as ''f'' and ''g'' are in total.\n\nThese identities hold under the precise condition that ''f'' and ''g'' are absolutely integrable and at least one of them has an absolutely integrable (L<sup>1</sup>) weak derivative, as a consequence of [[Young's convolution inequality]]. For instance, when ''f'' is continuously differentiable with compact support, and ''g'' is an arbitrary locally integrable function,\n\n:<math>\\frac{d}{dx}(f * g) = \\frac{df}{dx} * g.</math>\n\nThese identities also hold much more broadly in the sense of tempered distributions if one of ''f'' or ''g'' is a compactly supported distribution or a Schwartz function and the other is a tempered distribution. On the other hand, two positive integrable and infinitely differentiable functions may have a nowhere continuous convolution.\n\nIn the discrete case, the [[difference operator]] ''D'' ''f''(''n'') = ''f''(''n'' + 1) &minus; ''f''(''n'') satisfies an analogous relationship:\n\n:<math>D(f* g) = (Df)* g = f* (Dg).</math>\n\n===Convolution theorem===\nThe [[convolution theorem]] states that\n\n:<math> \\mathcal{F}\\{f * g\\} = k\\cdot \\mathcal{F}\\{f\\}\\cdot \\mathcal{F}\\{g\\}</math>\n\nwhere <math> \\mathcal{F}\\{f\\}</math> denotes the [[Fourier transform]] of <math>f</math>, and <math>k</math> is a constant that depends on the specific [[Normalizing constant|normalization]] of the Fourier transform. Versions of this theorem also hold for the [[Laplace transform]], [[two-sided Laplace transform]], [[Z-transform]] and [[Mellin transform]].\n\nSee also the less trivial [[Titchmarsh convolution theorem]].\n\n===Translational equivariance===\nThe convolution commutes with translations, meaning that\n\n:<math>\\tau_x (f * g) = (\\tau_x f)* g = {f}* (\\tau_x g)</math>\n\nwhere τ<sub>''x''</sub>f is the translation of the function ''f'' by ''x'' defined by\n\n:<math>(\\tau_x f)(y) = f(y-x).</math>\n\nIf ''f'' is a [[Schwartz function]], then τ<sub>''x''</sub>''f'' is the convolution with a translated Dirac delta function ''τ''<sub>''x''</sub>''f'' = ''f'' ∗ ''τ''<sub>''x''</sub> ''δ''. So translation invariance of the convolution of Schwartz functions is a consequence of the associativity of convolution.\n\nFurthermore, under certain conditions, convolution is the most general translation invariant operation. Informally speaking, the following holds\n\n* Suppose that ''S'' is a bounded [[linear operator]] acting on functions which commutes with translations: ''S''(τ<sub>''x''</sub>''f'') = τ<sub>''x''</sub>(''Sf'') for all ''x''. Then ''S'' is given as convolution with a function (or distribution) ''g''<sub>''S''</sub>; that is ''Sf'' = ''g''<sub>''S''</sub> ∗ ''f''.\n\nThus some translation invariant operations can be represented as convolution. Convolutions play an important role in the study of [[time-invariant system]]s, and especially [[LTI system theory]]. The representing function ''g''<sub>''S''</sub> is the [[impulse response]] of the transformation ''S''.\n\nA more precise version of the theorem quoted above requires specifying the class of functions on which the convolution is defined, and also requires assuming in addition that ''S'' must be a [[continuous linear operator]] with respect to the appropriate [[topology]]. It is known, for instance, that every continuous translation invariant continuous linear operator on ''L''<sup>1</sup> is the convolution with a finite [[Borel measure]]. More generally, every continuous translation invariant continuous linear operator on ''L''<sup>''p''</sup> for 1 ≤ ''p'' < ∞ is the convolution with a [[Distribution (mathematics)#Tempered distributions and Fourier transform|tempered distribution]] whose [[Fourier transform]] is bounded. To wit, they are all given by bounded [[Fourier multiplier]]s.\n\n==Convolutions on groups==\nIf ''G'' is a suitable [[group (mathematics)|group]] endowed with a [[measure (mathematics)|measure]] λ, and if ''f'' and ''g'' are real or complex valued [[Lebesgue integral|integrable]] functions on ''G'', then we can define their convolution by\n\n:<math>(f * g)(x) = \\int_G f(y) g(y^{-1}x)\\,d\\lambda(y).</math>\n\nIt is not commutative in general. In typical cases of interest ''G'' is a [[locally compact]] [[Hausdorff space|Hausdorff]] [[topological group]] and λ is a (left-) [[Haar measure]]. In that case, unless ''G'' is [[unimodular group|unimodular]], the convolution defined in this way is not the same as <math>\\textstyle{\\int f(xy^{-1})g(y) \\, d\\lambda(y)}</math>. The preference of one over the other is made so that convolution with a fixed function ''g'' commutes with left translation in the group:\n\n:<math>L_h(f* g) = (L_hf)* g.</math>\n\nFurthermore, the convention is also required for consistency with the definition of the convolution of measures given below. However, with a right instead of a left Haar measure, the latter integral is preferred over the former.\n\nOn locally compact [[abelian group]]s, a version of the [[convolution theorem]] holds: the Fourier transform of a convolution is the pointwise product of the Fourier transforms. The [[circle group]] '''T''' with the Lebesgue measure is an immediate example. For a fixed ''g'' in ''L''<sup>1</sup>('''T'''), we have the following familiar operator acting on the [[Hilbert space]] ''L''<sup>2</sup>('''T'''):\n\n:<math>T {f}(x) = \\frac{1}{2 \\pi} \\int_{\\mathbf{T}} {f}(y) g( x - y) \\, dy.</math>\n\nThe operator ''T'' is [[compact operator on Hilbert space|compact]]. A direct calculation shows that its adjoint ''T* '' is convolution with\n\n:<math>\\bar{g}(-y).</math>\n\nBy the commutativity property cited above, ''T'' is [[normal operator|normal]]: ''T''* ''T'' = ''TT''* . Also, ''T'' commutes with the translation operators. Consider the family ''S'' of operators consisting of all such convolutions and the translation operators. Then ''S'' is a commuting family of normal operators. According to [[compact operator on Hilbert space|spectral theory]], there exists an orthonormal basis {''h<sub>k</sub>''} that simultaneously diagonalizes ''S''. This characterizes convolutions on the circle. Specifically, we have\n\n:<math>h_k (x) = e^{ikx}, \\quad k \\in \\mathbb{Z},\\;</math>\n\nwhich are precisely the [[Character (mathematics)|character]]s of '''T'''. Each convolution is a compact [[multiplication operator]] in this basis. This can be viewed as a version of the convolution theorem discussed above.\n\nA discrete example is a finite [[cyclic group]] of order ''n''. Convolution operators are here represented by [[circulant matrices]], and can be diagonalized by the [[discrete Fourier transform]].\n\nA similar result holds for compact groups (not necessarily abelian): the matrix coefficients of finite-dimensional [[unitary representation]]s form an orthonormal basis in ''L''<sup>2</sup> by the [[Peter–Weyl theorem]], and an analog of the convolution theorem continues to hold, along with many other aspects of [[harmonic analysis]] that depend on the Fourier transform.\n\n==Convolution of measures==\nLet ''G'' be a (multiplicatively written) topological group.\nIf μ and ν are finite [[Borel measure]]s on ''G'', then their convolution μ∗ν is defined as the [[pushforward measure]] of the [[Group action (mathematics)|group action]] and can be written as\n<!--\n\nPLEASE READ THIS BEFORE EDITING:\n\nGroups are written multiplicatively, so please don't change this from multiplication in the group to \"+\".\n\n:<math>(\\mu * \\nu)(E) = \\iint 1_E(xy) \\,d\\mu(x) \\,d\\nu(y)</math>\n\n-->:<math>(\\mu * \\nu)(E) = \\iint 1_E(xy) \\,d\\mu(x) \\,d\\nu(y)</math>\n\nfor each measurable subset ''E'' of ''G''. The convolution is also a finite measure, whose [[total variation]] satisfies\n:<math>\\|\\mu * \\nu\\| \\le \\|\\mu\\| \\|\\nu\\|.</math>\n\nIn the case when ''G'' is [[locally compact]] with (left-)[[Haar measure]] λ, and μ and ν are [[absolute continuity|absolutely continuous]] with respect to a λ, [[Radon–Nikodym theorem|so that each has a density function]], then the convolution μ∗ν is also absolutely continuous, and its density function is just the convolution of the two separate density functions.\n\nIf μ and ν are [[probability measure]]s on the topological group {{nowrap|('''R''',+),}} then the convolution μ∗ν is the [[probability distribution]] of the sum ''X'' + ''Y'' of two [[statistical independence|independent]] [[random variable]]s ''X'' and ''Y'' whose respective distributions are μ and ν.\n\n==Bialgebras==\nLet (''X'', Δ, ∇, ''ε'', ''η'') be a [[bialgebra]] with comultiplication Δ, multiplication ∇, unit η, and counit ε. The convolution is a product defined on the [[endomorphism algebra]] End(''X'') as follows. Let φ, ψ ∈ End(''X''), that is, φ,ψ : ''X'' → ''X'' are functions that respect all algebraic structure of ''X'', then the convolution φ∗ψ is defined as the composition\n\n:<math>X \\xrightarrow{\\Delta} X\\otimes X \\xrightarrow{\\phi\\otimes\\psi} X\\otimes X \\xrightarrow{\\nabla} X.</math>\n\nThe convolution appears notably in the definition of [[Hopf algebra]]s {{harv|Kassel|1995|loc=§III.3}}. A bialgebra is a Hopf algebra if and only if it has an antipode: an endomorphism ''S'' such that\n:<math>S * \\operatorname{id}_X = \\operatorname{id}_X * S = \\eta\\circ\\varepsilon.</math>\n\n==Applications==\n[[File:Halftone, Gaussian Blur.jpg|thumb|right|[[Gaussian blur]] can be used to obtain a smooth grayscale digital image of a [[halftone]] print.]]\nConvolution and related operations are found in many applications in science, engineering and mathematics.\n* In [[image processing]]\n::In [[digital image processing]] convolutional filtering plays an important role in many important [[algorithm]]s in [[edge detection]] and related processes.\n::In [[optics]], an out-of-focus photograph is a convolution of the sharp image with a lens function. The photographic term for this is [[bokeh]].\n::In image processing applications such as adding blurring.\n* In digital data processing\n::In [[analytical chemistry]], [[Savitzky–Golay smoothing filter]]s are used for the analysis of spectroscopic data. They can improve [[signal-to-noise ratio]] with minimal distortion of the spectra.<!-- <ref name=Scafer2011>{{cite journal|last=Schafer|first=Ronald W.|title=What Is a Savitzky–Golay Filter?|journal=Signal Processing Magazine, IEEE|date=July 2011|volume=28|issue=4|pages=111–117|doi=10.1109/MSP.2011.941097|url=http://www-inst.eecs.berkeley.edu/~ee123/fa12/docs/SGFilter.pdf|accessdate=18 May 2013}}</ref> -->\n:: In [[statistics]], a weighted [[moving average]] is a convolution.\n* In [[acoustics]], [[reverberation]] is the convolution of the original sound with [[echo (phenomenon)|echoes]] from objects surrounding the sound source.\n:: In digital signal processing, convolution is used to map the [[impulse response]] of a real room on a digital audio signal.\n:: In [[electronic music]] convolution is the imposition of a [[Spectrum|spectral]] or rhythmic structure on a sound. Often this envelope or structure is taken from another sound. The convolution of two signals is the filtering of one through the other.<ref>Zölzer, Udo, ed. (2002). ''DAFX:Digital Audio Effects'', p.48–49. {{isbn|0471490784}}.</ref>\n* In [[electrical engineering]], the convolution of one function (the [[Signal (electrical engineering)|input signal]]) with a second function (the impulse response) gives the output of a [[linear time-invariant system]] (LTI). At any given moment, the output is an accumulated effect of all the prior values of the input function, with the most recent values typically having the most influence (expressed as a multiplicative factor). The impulse response function provides that factor as a function of the elapsed time since each input value occurred.\n* In [[physics]], wherever there is a [[linear system]] with a \"[[superposition principle]]\", a convolution operation makes an appearance. For instance, in [[spectroscopy]] line broadening due to the Doppler effect on its own gives a [[Normal distribution|Gaussian]] [[spectral line shape]] and collision broadening alone gives a [[Cauchy distribution|Lorentzian]] line shape. When both effects are operative, the line shape is a convolution of Gaussian and Lorentzian, a [[Voigt function]].\n:: In [[Time-resolved spectroscopy#Time-resolved fluorescence spectroscopy|time-resolved fluorescence spectroscopy]], the excitation signal can be treated as a chain of delta pulses, and the measured fluorescence is a sum of exponential decays from each delta pulse.\n:: In [[computational fluid dynamics]], the [[large eddy simulation]] (LES) [[turbulence model]] uses the convolution operation to lower the range of length scales necessary in computation thereby reducing computational cost.\n* In [[probability theory]], the [[probability distribution]] of the sum of two [[independent (probability)|independent]] [[random variable]]s is the convolution of their individual distributions.\n:: In [[kernel density estimation]], a distribution is estimated from sample points by convolution with a kernel, such as an isotropic Gaussian. {{harv|Diggle|1995}}.\n* In radiotherapy treatment planning systems, most part of all modern codes of calculation applies a [[convolution-superposition algorithm]].{{Clarify|date=May 2013}}\n* [[Convolutional neural networks]] apply multiple cascaded convolution kernels with applications in [[machine vision]] and [[artificial intelligence]]\n* In structural reliability, the reliability index can be defined based on the convolution theorem.\n:: The definition of reliability index for limit state functions with nonnormal distributions can be established corresponding to the [[joint distribution function]]. In fact, the joint distribution function can be obtained using the convolution theory. {{harv|Ghasemi-Nowak|2017}}.\n\n==See also==\n* [[Analog signal processing]]\n* [[Circulant matrix]]\n* [[Convolution for optical broad-beam responses in scattering media]]\n* [[Convolution power]]\n* [[Dirichlet convolution]]\n* [[Generalized signal averaging]]\n* [[Jan Mikusinski]]\n* [[List of convolutions of probability distributions]]\n* [[LTI system theory#Impulse response and convolution]]\n* [[Multidimensional discrete convolution]]\n* [[Scaled correlation]]\n* [[Titchmarsh convolution theorem]]\n* [[Toeplitz matrix]] (convolutions can be considered a Toeplitz matrix operation where each row is a shifted copy of the convolution kernel)\n\n== Notes ==\n{{reflist|group=note}}\n\n==References==\n{{reflist}}\n\n==Further reading==\n* {{citation | author=Bracewell, R.| title=The Fourier Transform and Its Applications| edition=2nd |\npublisher=McGraw–Hill | year=1986 | isbn=0-07-116043-4}}.\n* {{citation|last1=Damelin|first1=S.|last2=Miller|first2=W.|title=The Mathematics of Signal Processing|publisher=Cambridge University Press| isbn=978-1107601048 | year=2011}}\n* {{citation|last=Diggle|first=P. J. |title=A kernel method for smoothing point process data|journal=Journal of the Royal Statistical Society, Series C | volume = 34 | pages = 138–147 |doi=10.2307/2347366 | year=1985 }}\n* Dominguez-Torres, Alejandro (Nov 2, 2010). \"Origin and history of convolution\". 41 pgs. http://www.slideshare.net/Alexdfar/origin-adn-history-of-convolution. Cranfield, Bedford MK43 OAL, UK. Retrieved Mar 13, 2013.\n* {{citation|last=Diggle|first=P. J. |title=A kernel method for smoothing point process data|journal=Journal of the Royal Statistical Society, Series C | volume = 34 | pages = 138–147 |doi=10.2307/2347366 | year=1985 }}\n* {{Citation | last1=Ghasemi | first1=S. Hooman| last2=Nowak| first2=Andrzej S. |title=Reliability Index for Non-normal Distributions of Limit State Functions| doi=10.12989/sem.2017.62.3.365 | year=2017 | journal=Structural Engineering and Mechanics | volume=62 | issue=3 | pages=365-372 }}\n*{{Citation | last1=Grinshpan | first1=A. Z. | title=An inequality for multiple convolutions with respect to Dirichlet probability measure | doi=10.1016/j.aam.2016.08.001 | year=2017 | \njournal=Advances in Applied Mathematics | volume=82 | issue=1 | pages=102-119 }}\n* {{Citation | last1=Hewitt | first1=Edwin | last2=Ross | first2=Kenneth A. | title=Abstract harmonic analysis. Vol. I | publisher=[[Springer-Verlag]] | location=Berlin, New York | edition=2nd | series=Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences] | isbn=978-3-540-09434-0 | mr=551496 | year=1979 | volume=115}}.\n* {{Citation | last1=Hewitt | first1=Edwin | last2=Ross | first2=Kenneth A. | title=Abstract harmonic analysis. Vol. II: Structure and analysis for compact groups. Analysis on locally compact Abelian groups | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Die Grundlehren der mathematischen Wissenschaften, Band 152 | mr=0262773 | year=1970}}.\n* {{citation|mr=0717035|first=L.|last= Hörmander|authorlink=Lars Hörmander|title=The analysis of linear partial differential operators I|series= Grundl. Math. Wissenschaft. |volume= 256 |publisher= Springer |year=1983|isbn=3-540-12104-8 |doi=10.1007/978-3-642-96750-4}}.\n* {{Citation | last1=Kassel | first1=Christian | title=Quantum groups | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Graduate Texts in Mathematics | isbn=978-0-387-94370-1 | mr=1321145 | year=1995 | volume=155 | doi=10.1007/978-1-4612-0783-2}}.\n* {{citation|first=Donald|last=Knuth|authorlink=Donald Knuth|title=Seminumerical Algorithms|edition=3rd.|publication-place=Reading, Massachusetts|publisher=Addison–Wesley|year=1997|isbn=0-201-89684-2}}.\n* {{citation |last=Reed|first= Michael|last2= Simon|first2= Barry|author2-link=Barry Simon |title=Methods of modern mathematical physics. II. Fourier analysis, self-adjointness |publisher=Academic Press Harcourt Brace Jovanovich, Publishers|publication-place=New York-London|year= 1975|pages= xv+361|isbn =0-12-585002-6|mr=0493420}}\n* {{Citation | last1=Rudin | first1=Walter | author1-link=Walter Rudin | title=Fourier analysis on groups | publisher=Interscience Publishers (a division of John Wiley and Sons), New York–London | series=Interscience Tracts in Pure and Applied Mathematics, No. 12 | mr=0152834 | year=1962 | isbn=0-471-52364-X}}.\n* {{springer|id=C/c026430|title=Convolution of functions|year=2001|first=V.I.|last=Sobolev}}.\n* {{citation|first1=Elias|last1=Stein|authorlink1=Elias Stein|first2=Guido|last2=Weiss|title=Introduction to Fourier Analysis on Euclidean Spaces|publisher=Princeton University Press|year=1971|isbn=0-691-08078-X}}.\n* {{citation|first=R.|last=Strichartz|year=1994|title=A Guide to Distribution Theory and Fourier Transforms|publisher=CRC Press|isbn=0-8493-8273-4}}.\n* {{citation|last=Titchmarsh|first=E|authorlink=Edward Charles Titchmarsh|title=Introduction to the theory of Fourier integrals|isbn=978-0-8284-0324-5|year=1948|edition=2nd|publication-date=1986|publisher=Chelsea Pub. Co.|location=New York, N.Y.}}.\n* {{citation|last=Uludag|first=A. M. |authorlink=A. Muhammed Uludag|title=On possible deterioration of smoothness under the operation of convolution|journal=J. Math. Anal. Appl. 227 no. 2, 335–358|year=1998}}\n* {{citation|first=François|last=Treves|title=Topological Vector Spaces, Distributions and Kernels|publisher=Academic Press|year=1967|isbn=0-486-45352-9}}.\n* {{citation|first1=J.|last1=von zur Gathen|first2=J.|last2=Gerhard|title=Modern Computer Algebra|isbn=0-521-82646-2|year=2003|publisher=Cambridge University Press}}.\n\n==External links==\n{{Wiktionary|convolution}}\n{{Commons category|Convolution}}\n* [http://jeff560.tripod.com/c.html Earliest Uses: The entry on Convolution has some historical information.]\n* [https://web.archive.org/web/20060221234856/http://rkb.home.cern.ch/rkb/AN16pp/node38.html#SECTION000380000000000000000 Convolution], on [https://web.archive.org/web/20060512020859/http://rkb.home.cern.ch/rkb/titleA.html The Data Analysis BriefBook]\n* http://www.jhu.edu/~signals/convolve/index.html Visual convolution Java Applet\n* http://www.jhu.edu/~signals/discreteconv2/index.html Visual convolution Java Applet for discrete-time functions\n* [https://archive.org/details/Lectures_on_Image_Processing Lectures on Image Processing: A collection of 18 lectures in pdf format from Vanderbilt University. Lecture 7 is on 2-D convolution.], by Alan Peters\n* * https://archive.org/details/Lectures_on_Image_Processing\n* [http://micro.magnet.fsu.edu/primer/java/digitalimaging/processing/kernelmaskoperation/ Convolution Kernel Mask Operation Interactive tutorial]\n* [http://mathworld.wolfram.com/Convolution.html Convolution] at [[MathWorld]]\n* [http://www.nongnu.org/freeverb3/ Freeverb3 Impulse Response Processor]: Opensource zero latency impulse response processor with VST plugins\n* Stanford University CS 178 [http://graphics.stanford.edu/courses/cs178/applets/convolution.html interactive Flash demo ] showing how spatial convolution works.\n* [https://www.youtube.com/watch?v=IW4Reburjpc A video lecture on the subject of convolution] given by [[Salman Khan (educator)|Salman Khan]]\n* [http://www.dspguide.com/ch24/6.htm Example of FFT convolution for pattern-recognition (image processing)]\n\n[[Category:Functional analysis]]\n[[Category:Image processing]]\n[[Category:Binary operations]]\n[[Category:Fourier analysis]]\n[[Category:Bilinear operators]]\n[[Category:Feature detection (computer vision)]]"
    },
    {
      "title": "Courant bracket",
      "url": "https://en.wikipedia.org/wiki/Courant_bracket",
      "text": "In a field of [[mathematics]] known as [[differential geometry]], the '''Courant bracket''' is a generalization of the [[Lie derivative|Lie bracket]] from an operation on the [[tangent bundle]] to an operation on the [[direct sum of vector bundles|direct sum]] of the tangent bundle and the [[vector bundle]] of [[differential form|''p''-forms]].\n\nThe case ''p'' = 1 was introduced by [[Theodore James Courant]] in his 1990 doctoral dissertation as a structure that bridges [[Poisson manifold|Poisson geometry]] and pre[[symplectic geometry]], based on work with his advisor [[Alan Weinstein]].  The twisted version of the Courant bracket was introduced in 2001 by [[Pavol Severa]], and studied in collaboration with Weinstein.\n\nToday a [[complex number|complex]] version of the ''p''=1 Courant bracket plays a central role in the field of [[generalized complex geometry]], introduced by [[Nigel Hitchin]] in 2002.  Closure under the Courant bracket is the [[integrability condition]] of a [[Almost complex manifold#Generalized almost complex structure|generalized almost complex structure]].\n\n==Definition==\nLet ''X'' and ''Y'' be [[vector field]]s on an N-dimensional real [[manifold (mathematics)|manifold]] ''M'' and let ''ξ'' and ''η'' be ''p''-forms.  Then ''X+ξ'' and ''Y+η'' are [[fiber bundle#Sections|sections]] of the direct sum of the tangent bundle and the bundle of ''p''-forms.  The Courant bracket of ''X+ξ'' and ''Y+η'' is defined to be\n\n:<math>[X+\\xi,Y+\\eta]=[X,Y]\n+\\mathcal{L}_X\\eta-\\mathcal{L}_Y\\xi\n-\\frac{1}{2}d(i(X)\\eta-i(Y)\\xi)</math>\n\nwhere <math>\\mathcal{L}_X</math> is the [[Lie derivative]] along the vector field ''X'', ''d'' is the [[exterior derivative]] and ''i'' is the [[Exterior algebra#The interior product or insertion operator|interior product]].\n\n==Properties==\n\nThe Courant bracket is [[Antisymmetric relation|antisymmetric]] but it does not satisfy the [[Jacobi identity]] for ''p'' greater than zero.\n\n===The Jacobi identity===\n\nHowever, at least in the case ''p=1'', the [[Jacobiator]], which measures a bracket's failure to satisfy the Jacobi identity, is an [[exact form]].  It is the exterior derivative of a form which plays the role of the [[Nijenhuis tensor]] in generalized complex geometry.\n\nThe Courant bracket is the antisymmetrization of the [[Courant bracket#Dorfman bracket|Dorfman bracket]], which does satisfy a kind of Jacobi identity.\n\n===Symmetries===\n\nLike the Lie bracket, the Courant bracket is invariant under diffeomorphisms of the manifold ''M''.  It also enjoys an additional symmetry under the vector bundle [[automorphism]]\n\n:::<math>X+\\xi\\mapsto X+\\xi+i(X)\\alpha</math>\n\nwhere ''α'' is a closed ''p+1''-form.  In the ''p=1'' case, which is the relevant case for the geometry of [[Compactification (physics)#Flux compactification|flux compactification]]s in [[string theory]], this transformation is known in the physics literature as a shift in the [[Kalb-Ramond field|B field]].\n\n==Dirac and generalized complex structures==\n\nThe [[cotangent bundle]], <math>{\\mathbf T}^*</math> of M is the bundle of differential one-forms.  In the case ''p''=1 the Courant bracket maps two sections of <math>{\\mathbf T}\\oplus{\\mathbf{T}}^*</math>, the direct sum of the tangent and cotangent bundles, to another section of <math>{\\mathbf T}\\oplus{\\mathbf{T}}^*</math>.  The fibers of <math>{\\mathbf T}\\oplus{\\mathbf{T}}^*</math> admit [[inner product]]s with [[signature of a quadratic form|signature]] (N,N) given by\n:::<math>\\langle X+\\xi,Y+\\eta\\rangle=\\frac{1}{2}(\\xi(Y)+\\eta(X)).</math>\n\nA [[linear subspace]] of <math>{\\mathbf T}\\oplus{\\mathbf{T}}^*</math> in which all pairs of vectors have zero inner product is said to be an [[isotropic subspace]]. The fibers of <math>{\\mathbf T}\\oplus{\\mathbf{T}}^*</math> are ''2N''-dimensional and the maximal dimension of an isotropic subspace is ''N''.  An ''N''-dimensional isotropic subspace is called a maximal isotropic subspace.\n\nA [[Dirac structure]] is a maximally isotropic subbundle of <math>{\\mathbf T}\\oplus{\\mathbf{T}}^*</math> whose sections are closed under the Courant bracket.  Dirac structures include as special cases [[symplectic geometry|symplectic structures]], [[Poisson manifold|Poisson structures]] and [[foliation|foliated geometries]].\n\nA [[generalized complex structure]] is defined identically, but one [[tensor product|tensors]] <math>{\\mathbf T}\\oplus{\\mathbf{T}}^*</math> by the complex numbers and uses the [[complex dimension]] in the above definitions and one imposes that the direct sum of the subbundle and its [[complex conjugate]] be the entire original bundle ('''T'''<math>\\oplus</math>\n'''T'''<sup>*</sup>)<math>\\otimes</math>'''C'''. Special cases of generalized complex structures include [[Complex manifold|complex structure]] and a version of [[Kähler manifold|Kähler structure]] which includes the B-field.\n\n==Dorfman bracket==\n\nIn 1987 [[Irene Dorfman]] introduced the Dorfman bracket [,]<sub>D</sub>, which like the Courant bracket provides an integrability condition for Dirac structures.  It is defined by\n\n::<math>[A,B]_D=[A,B]+d\\langle A,B\\rangle</math>.\n\nThe Dorfman bracket is not antisymmetric, but it is often easier to calculate with than the Courant bracket because it satisfies a [[Product rule|Leibniz rule]] which resembles the Jacobi identity\n\n::<math>[A,[B,C]_D]_D=[[A,B]_D,C]_D+[B,[A,C]_D]_D.</math>\n\n==Courant algebroid==\n\nThe Courant bracket does not satisfy the [[Jacobi identity]] and so it does not define a [[Lie algebroid]], in addition it fails to satisfy the Lie algebroid condition on the [[anchor map]].  Instead it defines a more general structure introduced by [[Zhang-Ju Liu]], [[Alan Weinstein]] and [[Ping Xu]] known as a [[Courant algebroid]].\n\n==Twisted Courant bracket==\n\n===Definition and properties===\nThe Courant bracket may be twisted by a ''(p+2)''-form ''H'', by adding the interior product of the vector fields ''X'' and ''Y'' of ''H''.  It remains antisymmetric and invariant under the addition of the interior product with a ''(p+1)''-form ''B''.  When ''B'' is not closed then this invariance is still preserved if one adds ''dB'' to the final ''H''.\n\nIf ''H'' is closed then the Jacobiator is exact and so the twisted Courant bracket still defines a Courant algebroid.  In [[string theory]], ''H'' is interpreted as the [[Kalb-Ramond field|Neveu-Schwarz 3-form]].\n\n===''p=0'': Circle-invariant vector fields===\n\nWhen ''p''=0 the Courant bracket reduces to the Lie bracket on a [[principal bundle|principal]] [[circle bundle]] over ''M'' with [[Riemann curvature tensor|curvature]] given by the 2-form twist ''H''.  The bundle of 0-forms is the trivial bundle, and a section of the direct sum of the tangent bundle and the trivial bundle defines a circle [[invariant vector field]] on this circle bundle.\n\nConcretely, a section of the sum of the tangent and trivial bundles is given by a vector field ''X'' and a function ''f'' and the Courant bracket is\n::<math>[X+f,Y+g]=[X,Y]+Xg-Yf</math>\nwhich is just the Lie bracket of the vector fields\n:::<math>[X+f,Y+g]=[X+f\\frac{\\partial}{\\partial\\theta},Y+g\\frac{\\partial}{\\partial\\theta}]_{Lie}</math>\nwhere ''θ'' is a coordinate on the circle fiber.  Note in particular that the Courant bracket satisfies the Jacobi identity in the case ''p=0''.\n\n===Integral twists and gerbes===\n\nThe curvature of a circle bundle always represents an integral [[cohomology]] class, the [[Chern class]] of the circle bundle.  Thus the above geometric interpretation of the twisted ''p=0'' Courant bracket only exists when ''H'' represents an integral class.   Similarly at higher values of ''p'' the twisted Courant brackets can be geometrically realized as untwisted Courant brackets twisted by [[gerbe]]s when ''H'' is an integral cohomology class.\n\n==References==\n\n*Courant, Theodore, ''Dirac manifolds'', Trans. Amer. Math. Soc., 319:631-661, (1990).\n*Gualtieri, Marco, [http://xxx.lanl.gov/abs/math.DG/0401221 Generalized complex geometry], PhD Thesis (2004).\n\n[[Category:Differential geometry]]\n[[Category:Binary operations]]"
    },
    {
      "title": "Covariance operator",
      "url": "https://en.wikipedia.org/wiki/Covariance_operator",
      "text": "In [[probability theory]], for a [[probability measure]] '''P''' on a [[Hilbert space]] ''H'' with [[inner product]] <math>\\langle \\cdot,\\cdot\\rangle </math>, the '''covariance''' of '''P''' is the [[bilinear form]] Cov:&nbsp;''H''&nbsp;&times;&nbsp;''H''&nbsp;→&nbsp;'''R''' given by\n\n:<math>\\mathrm{Cov}(x, y) = \\int_{H} \\langle x, z \\rangle \\langle y, z \\rangle \\, \\mathrm{d} \\mathbf{P} (z)</math>\n\nfor all ''x'' and ''y'' in ''H''. The '''covariance operator''' ''C'' is then defined by\n\n:<math>\\mathrm{Cov}(x, y) = \\langle Cx, y \\rangle</math>\n\n(from the [[Riesz representation theorem]], such operator exists if Cov is [[Bilinear form#On normed vector spaces|bounded]]). Since Cov is symmetric in its arguments, the covariance operator is\n[[self-adjoint operator|self-adjoint]] (the infinite-dimensional analogy of the transposition symmetry in the finite-dimensional case). When '''P''' is a centred [[Gaussian measure]], ''C'' is also a [[nuclear operator]]. In particular, it is a [[compact operator]] of [[trace class]], that is, it has finite [[trace (mathematics)|trace]].\n\nEven more generally, for a [[probability measure]] '''P''' on a [[Banach space]] ''B'', the covariance of '''P''' is the [[bilinear form]] on the [[algebraic dual]] ''B''<sup>#</sup>, defined by\n\n:<math>\\mathrm{Cov}(x, y) = \\int_{B} \\langle x, z \\rangle \\langle y, z \\rangle \\, \\mathrm{d} \\mathbf{P} (z)</math>\n\nwhere <math> \\langle x, z \\rangle </math> is now the value of the linear functional ''x'' on the element ''z''.\n\nQuite similarly, the [[covariance function]] of a function-valued [[random element]] (in special cases called [[random process]] or [[random field]]) ''z'' is\n\n:<math>\\mathrm{Cov}(x, y) = \\int z(x) z(y) \\, \\mathrm{d} \\mathbf{P} (z) = E(z(x) z(y))</math>\n\nwhere ''z''(''x'') is now the value of the function ''z'' at the point ''x'', i.e., the value of the [[linear functional]] <math> u \\mapsto u(x) </math> evaluated at ''z''.\n\n\n{{probability-stub}}\n\n[[Category:Probability theory]]\n[[Category:Covariance and correlation]]\n[[Category:Bilinear forms]]\n[[Category:Binary operations]]"
    },
    {
      "title": "Cross product",
      "url": "https://en.wikipedia.org/wiki/Cross_product",
      "text": "{{About|the cross product of two vectors in three-dimensional Euclidean space}}\nIn [[mathematics]] and [[vector algebra]], the '''cross product''' or '''vector product''' (occasionally '''directed area product''' to emphasize the geometric significance) is a [[binary operation]] on two [[Euclidean vector|vector]]s in [[three-dimensional space]] <math>\\left(\\mathbb{R}^3\\right)</math> and is denoted by the symbol <math>\\times</math>. Given two [[linearly independent vectors]] <math>\\mathbf{a}</math> and <math>\\mathbf{b}</math>, the cross product, <math>\\mathbf{a}\\times\\mathbf{b}</math> (read \"a cross b\"), is a vector that is [[perpendicular]] to both <math>\\mathbf{a}</math> and <math>\\mathbf{b}</math> and thus [[Normal (geometry)|normal]] to the plane containing them. It has many applications in mathematics, [[physics]], [[engineering]], and [[computer programming]]. It should not be confused with the [[dot product]] (projection product).\n\nIf two vectors have the same direction (or have the exact opposite direction from one another, i.e. are ''not'' linearly independent) or if either one has zero length, then their cross product is zero. More generally, the magnitude of the product equals the area of a [[parallelogram]] with the vectors for sides; in particular, the magnitude of the product of two perpendicular vectors is the product of their lengths. The cross product is [[anticommutativity|anticommutative]] (i.e., <math>\\mathbf{a}\\times \\mathbf{b} = -\\mathbf{b}\\times \\mathbf{a}</math>) and is [[distributivity|distributive]] over addition (i.e., <math>\\mathbf{a}\\times (\\mathbf{b} + \\mathbf{c}) = \\mathbf{a}\\times\\mathbf{b} + \\mathbf{a}\\times\\mathbf{c}</math>). The space <math>\\mathbb{R}^3</math> together with the cross product is an [[algebra over a field|algebra over the real numbers]], which is neither [[commutative]] nor [[associative]], but is a [[Lie algebra]] with the cross product being the [[Lie bracket]].\n\nLike the [[dot product]], it depends on the [[metric space|metric]] of Euclidean space, but unlike the dot product, it also depends on a choice of [[orientation (mathematics)|orientation]] or \"[[Right-hand rule|handedness]]\". The product can be generalized in various ways; it can be made independent of orientation by changing the result to [[pseudovector]], or in arbitrary dimensions the [[exterior algebra|exterior product]] of vectors can be used with a [[bivector]] or [[two-form]] result. Also, using the orientation and metric structure just as for the traditional 3-dimensional cross product, one can in <math>n</math> dimensions take the product of <math>n-1</math> vectors to produce a vector perpendicular to all of them. But if the product is limited to non-trivial binary products with vector results, it exists only in three and [[Seven-dimensional cross product|seven dimensions]].<ref name=Massey2>{{cite journal |title=Cross products of vectors in higher dimensional Euclidean spaces |author=WS Massey |year=1983 |jstor=2323537|quote=If one requires only three basic properties of the cross product ... it turns out that a cross product of vectors exists only in 3-dimensional and 7-dimensional Euclidean space. |pages=697–701 |journal=The American Mathematical Monthly |volume=90 |issue=10 |ref=harv |doi=10.2307/2323537}}</ref> If one adds the further requirement that the product be uniquely defined, then only the 3-dimensional cross product qualifies. (See [[Cross product#Generalizations|§ Generalizations]], below, for other dimensions.)  \n[[File:Cross product vector.svg|thumb|right|The cross product in respect to a right-handed coordinate system]]\n\n== Definition ==\n[[File:Right hand rule cross product.svg|thumb|Finding the direction of the cross product by the [[right-hand rule]].]]\n\nThe cross product of two vectors '''a''' and '''b''' is defined only in three-dimensional space and is denoted by {{nowrap|1='''a''' × '''b'''}}. In [[physics]], sometimes the notation {{nowrap|1='''a''' ∧ '''b'''}} is used,<ref>{{cite book|author1=Jeffreys, H  |author2=Jeffreys, BS|title=Methods of mathematical physics|year=1999|publisher=Cambridge University Press|oclc=41158050}}</ref> though this is avoided in mathematics to avoid confusion with the [[exterior product]].\n\nThe cross product {{nowrap|'''a''' × '''b'''}} is defined as a vector '''c''' that is [[perpendicular]] (orthogonal) to both '''a''' and '''b''', with a direction given by the [[right-hand rule]]<!-- this is how first time students, who also use right-hand coordinates, learn --> and a magnitude equal to the area of the [[parallelogram]] that the vectors span.\n\nThe cross product is defined by the formula<ref>{{harvnb|Wilson|1901|page=60–61}}</ref><ref name=Cullen>{{cite book |title=Advanced engineering mathematics |author1=Dennis G. Zill |author2=Michael R. Cullen |edition=3rd  |year=2006 |publisher=Jones & Bartlett Learning |url=https://books.google.com/?id=x7uWk8lxVNYC&pg=PA324 |page=324 |chapter=Definition 7.4: Cross product of two vectors |isbn=0-7637-4591-X}}</ref>\n\n:<math>\\mathbf{a} \\times \\mathbf{b} = \\left\\| \\mathbf{a} \\right\\| \\left\\| \\mathbf{b} \\right\\| \\sin (\\theta) \\ \\mathbf{n}</math>\n\nwhere ''θ'' is the [[angle]] between '''a''' and '''b''' in the plane containing them (hence, it is between 0° and 180°), ‖'''a'''‖ and ‖'''b'''‖ are the [[Magnitude (vector)|magnitudes]] of vectors '''a''' and '''b''', and '''n''' is a [[unit vector]] [[perpendicular]] to the plane containing '''a''' and '''b''' in the direction given by the right-hand rule (illustrated). If the vectors '''a''' and '''b''' are parallel (i.e., the angle ''θ'' between them is either 0° or 180°), by the above formula, the cross product of '''a''' and '''b''' is the [[zero vector]] '''0'''.\n\n[[File:Cross product.gif|left|thumb|The cross product {{nowrap|'''a''' × '''b'''}} (vertical, in purple) changes as the angle between the vectors '''a''' (blue) and '''b''' (red) changes. The cross product is always orthogonal to both vectors, and has magnitude zero when the vectors are parallel and maximum magnitude ‖'''a'''‖‖'''b'''‖ when they are orthogonal.]]\n\nBy convention, the direction of the vector '''n''' is given by the right-hand rule, where one simply points the forefinger of the right hand in the direction of '''a''' and the middle finger in the direction of '''b'''. Then, the vector '''n''' is coming out of the thumb (see the adjacent picture). Using this rule implies that the cross product is [[Anticommutativity|anti-commutative]], i.e., {{nowrap|1='''b''' × '''a''' = −('''a''' × '''b''')}}. By pointing the forefinger toward '''b''' first, and then pointing the middle finger toward '''a''', the thumb will be forced in the opposite direction, reversing the sign of the product vector.\n\nUsing the cross product requires the handedness of the coordinate system to be taken into account (as explicit in the definition above). If a [[Cartesian coordinate system#Orientation and handedness|left-handed coordinate system]] is used, the direction of the vector '''n''' is given by the left-hand rule and points in the opposite direction.\n\nThis, however, creates a problem because transforming from one arbitrary reference system to another (e.g., a mirror image transformation from a right-handed to a left-handed coordinate system), should not change the direction of '''n'''. The problem is clarified by realizing that the cross product of two vectors is not a (true) vector, but rather a ''[[pseudovector]]''.  See [[Cross product#Cross product and handedness|cross product and handedness]] for more detail.\n\n== Names ==\n[[File:Sarrus rule.svg|upright=1.25|thumb|right|According to [[Sarrus's rule]], the [[determinant]] of a 3×3 matrix involves multiplications between matrix elements identified by crossed diagonals]]\n\nIn 1881, [[Josiah Willard Gibbs]], and independently [[Oliver Heaviside]], introduced both the [[dot product]] and the cross product using a period ({{nowrap|1='''a''' . '''b'''}}) and an \"x\" ({{nowrap|1='''a''' x '''b'''}}), respectively, to denote them.<ref name=ucd>[https://www.math.ucdavis.edu/~temple/MAT21D/SUPPLEMENTARY-ARTICLES/Crowe_History-of-Vectors.pdf ''A History of Vector Analysis'' by Michael J. Crowe], Math. UC Davis</ref>\n\nIn 1877, to emphasize the fact that the result of a dot product is a [[scalar (mathematics)|scalar]] while the result of a cross product is a [[Euclidean vector|vector]], [[William Kingdon Clifford]] coined the alternative names '''scalar product''' and '''vector product''' for the two operations.<ref name=ucd/> These alternative names are still widely used in the literature.\n\nBoth the cross notation ({{nowrap|1='''a''' × '''b'''}}) and the name '''cross product''' were possibly inspired by the fact that each [[scalar component]] of {{nowrap|1='''a''' × '''b'''}} is computed by multiplying non-corresponding components of '''a''' and '''b'''. Conversely, a dot product {{nowrap|1='''a''' ⋅ '''b'''}} involves multiplications between corresponding components of '''a''' and '''b'''. As explained [[#Matrix notation|below]], the cross product can be expressed in the form of a [[determinant]] of a special {{nowrap|3 × 3}} matrix. According to [[Sarrus's rule]], this involves multiplications between matrix elements identified by crossed diagonals.\n\n==Computing the cross product==\n\n===Coordinate notation===\n[[File:3D Vector.svg|300px|thumb|right|[[Standard basis]] vectors ('''i''', '''j''', '''k''', also denoted '''e'''<sub>1</sub>, '''e'''<sub>2</sub>, '''e'''<sub>3</sub>) and [[vector component]]s of '''a''' ('''a'''<sub>x</sub>, '''a'''<sub>y</sub>, '''a'''<sub>z</sub>, also denoted '''a'''<sub>1</sub>, '''a'''<sub>2</sub>, '''a'''<sub>3</sub>)]]\nThe [[standard basis]] vectors '''i''', '''j''', and '''k''' satisfy the following equalities in a right hand coordinate system:\n:<math>\\begin{align}\n \\mathbf{\\color{ProcessBlue}{i}}\\times\\mathbf{\\color{red}{j}} &= \\mathbf{\\color{Dandelion}{k}}\\\\\n  \\mathbf{\\color{red}{j}}\\times\\mathbf{\\color{Dandelion}{k}} &= \\mathbf{\\color{ProcessBlue}{i}}\\\\\n  \\mathbf{\\color{Dandelion}{k}}\\times\\mathbf{\\color{ProcessBlue}{i}} &= \\mathbf{\\color{red}{j}}\n\\end{align}</math>\n\nwhich imply, by the [[anticommutativity]] of the cross product, that \n:<math>\\begin{align}\n\\mathbf{{\\color{red}{j}}\\times {\\color{ProcessBlue}{i}}} &= -\\mathbf{\\color{Dandelion}{k}}\\\\\n \\mathbf{{\\color{Dandelion}{k}}\\times {\\color{red}{j}}} &= -\\mathbf{\\color{ProcessBlue}{i}}\\\\\n \\mathbf{{\\color{ProcessBlue}{i}}\\times {\\color{Dandelion}{k}}} &= -\\mathbf{\\color{red}{j}}\n \n\\end{align}</math>\n\nThe anticommutativity of the cross product also implies that\n:<math>\\mathbf{\\color{ProcessBlue}{i}}\\times\\mathbf{\\color{ProcessBlue}{i}} = \\mathbf{\\color{red}{j}}\\times\\mathbf{\\color{red}{j}} = \\mathbf{\\color{Dandelion}{k}}\\times\\mathbf{\\color{Dandelion}{k}} = \\mathbf{0}</math> (the [[zero vector]]).\n\nThese equalities, together with the [[distributivity]] and [[linearity]] of the cross product (but both do not follow easily from the definition given above), are sufficient to determine the cross product of any two vectors '''a''' and '''b'''. Each vector can be defined as the sum of three orthogonal components parallel to the standard basis vectors:\n:<math>\\begin{align}\n  \\mathbf{a} &= a_1\\mathbf{\\color{ProcessBlue}{i}} + a_2\\mathbf{\\color{red}{j}} + a_3\\mathbf{\\color{Dandelion}{k}} \\\\\n  \\mathbf{b} &= b_1\\mathbf{\\color{ProcessBlue}{i}} + b_2\\mathbf{\\color{red}{j}} + b_3\\mathbf{\\color{Dandelion}{k}}\n\\end{align}</math>\n\nTheir cross product {{nowrap|1='''a''' × '''b'''}} can be expanded using distributivity:\n:<math> \\begin{align}\n \\mathbf{a}\\times\\mathbf{b} = {} &(a_1\\mathbf{\\color{ProcessBlue}{i}} + a_2\\mathbf{\\color{red}{j}} + a_3\\mathbf{\\color{Dandelion}{k}}) \\times (b_1\\mathbf{\\color{ProcessBlue}{i}} + b_2\\mathbf{\\color{red}{j}} + b_3\\mathbf{\\color{Dandelion}{k}})\\\\\n                            = {} &a_1b_1(\\mathbf{\\color{ProcessBlue}{i}} \\times \\mathbf{\\color{ProcessBlue}{i}}) + a_1b_2(\\mathbf{\\color{ProcessBlue}{i}} \\times \\mathbf{\\color{red}{j}}) + a_1b_3(\\mathbf{\\color{ProcessBlue}{i}} \\times \\mathbf{\\color{Dandelion}{k}}) + {}\\\\\n                                 &a_2b_1(\\mathbf{\\color{red}{j}} \\times \\mathbf{\\color{ProcessBlue}{i}}) + a_2b_2(\\mathbf{\\color{red}{j}} \\times \\mathbf{\\color{red}{j}}) + a_2b_3(\\mathbf{\\color{red}{j}} \\times \\mathbf{\\color{Dandelion}{k}}) + {}\\\\\n                                 &a_3b_1(\\mathbf{\\color{Dandelion}{k}} \\times \\mathbf{\\color{ProcessBlue}{i}}) + a_3b_2(\\mathbf{\\color{Dandelion}{k}} \\times \\mathbf{\\color{red}{j}}) + a_3b_3(\\mathbf{\\color{Dandelion}{k}} \\times \\mathbf{\\color{Dandelion}{k}})\\\\\n\\end{align}</math>\n\nThis can be interpreted as the decomposition of {{nowrap|1='''a''' × '''b'''}} into the sum of nine simpler cross products involving vectors aligned with '''i''', '''j''', or '''k'''. Each one of these nine cross products operates on two vectors that are easy to handle as they are either parallel or orthogonal to each other. From this decomposition, by using the above-mentioned [[#Coordinate notation|equalities]] and collecting similar terms, we obtain{{Clarify|reason=It isn't clear where negations at the zeroes are coming from. Please, clarify this.|date=March 2019}}:\n:<math>\\begin{align}\n \\mathbf{a}\\times\\mathbf{b} = {} &- a_1b_1\\mathbf{0} + a_1b_2\\mathbf{\\color{Dandelion}{k}} - a_1b_3\\mathbf{\\color{red}{j}} \\\\\n                                 &- a_2b_1\\mathbf{\\color{Dandelion}{k}} - a_2b_2\\mathbf{0} + a_2b_3\\mathbf{\\color{ProcessBlue}{i}} \\\\\n                                 &+ a_3b_1\\mathbf{\\color{red}{j}} - a_3b_2\\mathbf{\\color{ProcessBlue}{i}} - a_3b_3\\mathbf{0} \\\\\n                            = {} &(a_2b_3 - a_3b_2)\\mathbf{\\color{ProcessBlue}{i}} + (a_3b_1 - a_1b_3)\\mathbf{\\color{red}{j}} + (a_1b_2 - a_2b_1)\\mathbf{\\color{Dandelion}{k}}\\\\\n\\end{align}</math>\n\nmeaning that the three [[scalar component]]s of the resulting vector '''s''' = ''s''<sub>1</sub>'''i''' + ''s''<sub>2</sub>'''j''' + ''s''<sub>3</sub>'''k''' = {{nowrap|1='''a''' × '''b'''}} are\n:<math>\\begin{align}\n  s_1 &= a_2b_3-a_3b_2\\\\\n  s_2 &= a_3b_1-a_1b_3\\\\\n  s_3 &= a_1b_2-a_2b_1\n\\end{align}</math>\n\nUsing [[column vector]]s, we can represent the same result as follows:\n:<math>\\begin{pmatrix}s_1\\\\s_2\\\\s_3\\end{pmatrix}=\\begin{pmatrix}a_2b_3-a_3b_2\\\\a_3b_1-a_1b_3\\\\a_1b_2-a_2b_1\\end{pmatrix}</math>\n\n===Matrix notation===\n[[File:Sarrus rule cross product.svg|thumb|Use of Sarrus's rule to find the cross product of '''a''' and '''b''']]\nThe cross product can also be expressed as the [[formal calculation|formal]] [[determinant]]<ref group=\"note\">Here, \"formal\" means that this notation has the form of a determinant, but does not strictly adhere to the definition; it is a mnemonic used to remember the expansion of the cross product.</ref>:\n\n:<math>\\mathbf{a\\times b} = \\begin{vmatrix}\n  \\mathbf{i}&\\mathbf{j}&\\mathbf{k}\\\\\n  a_1&a_2&a_3\\\\\n  b_1&b_2&b_3\\\\\n\\end{vmatrix}</math>\n\nThis determinant can be computed using [[Rule of Sarrus|Sarrus's rule]] or [[cofactor expansion]]. Using Sarrus's rule, it expands to\n:<math>\\begin{align}\n  \\mathbf{a\\times b} &=(a_2b_3\\mathbf{i}+a_3b_1\\mathbf{j}+a_1b_2\\mathbf{k}) - (a_3b_2\\mathbf{i}+a_1b_3\\mathbf{j}+a_2b_1\\mathbf{k})\\\\\n                     &=(a_2b_3 - a_3b_2)\\mathbf{i} +(a_3b_1 - a_1b_3)\\mathbf{j} +(a_1b_2 - a_2b_1)\\mathbf{k}. \n\\end{align}</math>\n\nUsing [[Minor (linear algebra)|cofactor]] expansion along the first row instead, it expands to<ref name=Cullen2>{{cite book |title=''cited work'' |url=https://books.google.com/?id=x7uWk8lxVNYC&pg=PA321 |page=321 |chapter= Equation 7: '''a''' × '''b''' as sum of determinants |isbn=0-7637-4591-X |author1=Dennis G. Zill |author2=Michael R. Cullen |publisher=Jones & Bartlett Learning |year=2006}}</ref>\n:<math>\\begin{align}\n\\mathbf{a\\times b} &=\n  \\begin{vmatrix}\n    a_2&a_3\\\\\n    b_2&b_3\n  \\end{vmatrix}\\mathbf{i} -\n  \\begin{vmatrix}\n     a_1&a_3\\\\\n     b_1&b_3\n  \\end{vmatrix}\\mathbf{j} +\n  \\begin{vmatrix}\n    a_1&a_2\\\\\n    b_1&b_2\n  \\end{vmatrix}\\mathbf{k} \\\\\n\n &=(a_2b_3 - a_3b_2)\\mathbf{i} -(a_1b_3 - a_3b_1)\\mathbf{j} +(a_1b_2 - a_2b_1)\\mathbf{k},\n\\end{align}</math>\nwhich gives the components of the resulting vector directly.\n\n== Properties ==\n\n=== Geometric meaning ===\n{{See also|Triple product}}\n[[File:Cross product parallelogram.svg|right|thumb|Figure 1. The area of a parallelogram as the magnitude of a cross product]]\n[[File:Parallelepiped volume.svg|right|thumb|240px|Figure 2. Three vectors defining a parallelepiped]]\n\nThe [[Euclidean norm|magnitude]] of the cross product can be interpreted as the positive [[area]] of the [[parallelogram]] having '''a''' and '''b''' as sides (see Figure 1):\n\n:<math> \\left\\| \\mathbf{a} \\times \\mathbf{b} \\right\\| = \\left\\| \\mathbf{a} \\right\\| \\left\\| \\mathbf{b} \\right\\| \\sin \\theta .</math>\n\nIndeed, one can also compute the volume ''V'' of a [[parallelepiped]] having '''a''', '''b''' and '''c''' as edges by using a combination of a cross product and a dot product, called [[scalar triple product]] (see Figure 2):\n\n:<math>\n  \\mathbf{a}\\cdot(\\mathbf{b}\\times \\mathbf{c})=\n  \\mathbf{b}\\cdot(\\mathbf{c}\\times \\mathbf{a})=\n  \\mathbf{c}\\cdot(\\mathbf{a}\\times \\mathbf{b}).\n</math>\n\nSince the result of the scalar triple product may be negative, the volume of the parallelepiped is given by its absolute value. For instance,\n\n:<math>V = |\\mathbf{a} \\cdot (\\mathbf{b} \\times \\mathbf{c})|.</math>\n\nBecause the magnitude of the cross product goes by the sine of the angle between its arguments, the cross product can be thought of as a measure of ''perpendicularity'' in the same way that the [[dot product]] is a measure of ''parallelism''. Given two [[unit vectors]], their cross product has a magnitude of 1 if the two are perpendicular and a magnitude of zero if the two are parallel. The dot product of two unit vectors behaves just oppositely: it is zero when the unit vectors are perpendicular and 1 if the unit vectors are parallel.\n\nUnit vectors enable two convenient identities: the dot product of two unit vectors yields the cosine (which may be positive or negative) of the angle between the two unit vectors. The magnitude of the cross product of the two unit vectors yields the sine (which will always be positive).\n\n=== Algebraic properties ===\n\n[[File:Cross product scalar multiplication.svg|350px|thumb|Cross product scalar multiplication. '''Left:''' Decomposition of '''b''' into components parallel and perpendicular to '''a'''. Right: Scaling of the perpendicular components by a positive real number ''r'' (if negative, '''b''' and the cross product are reversed).]]\n\n[[File:Cross product distributivity.svg|350px|thumb|Cross product distributivity over vector addition. '''Left:''' The vectors '''b''' and '''c''' are resolved into parallel and perpendicular components to '''a'''. '''Right:''' The parallel components vanish in the cross product, only the perpendicular components shown in the plane perpendicular to '''a''' remain.<ref>{{cite book|title=Vector Analysis|author1=M. R. Spiegel |author2=S. Lipschutz |author3=D. Spellman |series=Schaum's outlines|year=2009|page=29|publisher=McGraw Hill|isbn=978-0-07-161545-7}}</ref>]]\n\n[[File:Cross product triple.svg|thumb|350px|The two nonequivalent triple cross products of three vectors '''a''', '''b''', '''c'''. In each case, two vectors define a plane, the other is out of the plane and can be split into parallel and perpendicular components to the cross product of the vectors defining the plane. These components can be found by [[vector projection]] and [[vector rejection|rejection]]. The triple product is in the plane and is rotated as shown.]]\n\nIf the cross product of two vectors is the zero vector (i.e. {{nowrap|1='''a''' × '''b''' = '''0'''}}), then either one or both of the inputs is the zero vector, ({{nowrap|1='''a''' = '''0'''}} or {{nowrap|1='''b''' = '''0'''}}) or else they are parallel or antiparallel ({{nowrap|'''a''' ∥ '''b'''}}) so that the sine of the angle between them is zero ({{nowrap|1=''θ'' = 0°}} or {{nowrap|1=''θ'' = 180°}} and {{nowrap|1=sin''θ'' = 0}}).\n\nThe self cross product of a vector is the zero vector:\n:<math>\\mathbf{a} \\times \\mathbf{a} = \\mathbf{0}</math>\nThe cross product is [[anticommutativity|anticommutative]],\n:<math>\\mathbf{a} \\times \\mathbf{b} = -(\\mathbf{b} \\times \\mathbf{a}),</math>\n[[distributive property|distributive]] over addition,\n: <math>\\mathbf{a} \\times (\\mathbf{b} + \\mathbf{c}) = (\\mathbf{a} \\times \\mathbf{b}) + (\\mathbf{a} \\times \\mathbf{c}),</math>\nand compatible with scalar multiplication so that\n:<math>(r\\mathbf{a}) \\times \\mathbf{b} = \\mathbf{a} \\times (r\\mathbf{b}) = r(\\mathbf{a} \\times \\mathbf{b}).</math>\n\nIt is not [[associative]], but satisfies the [[Jacobi identity]]:\n:<math>\\mathbf{a} \\times (\\mathbf{b} \\times \\mathbf{c}) + \\mathbf{b} \\times (\\mathbf{c} \\times \\mathbf{a}) + \\mathbf{c} \\times (\\mathbf{a} \\times \\mathbf{b}) = \\mathbf{0}.</math>\nDistributivity, linearity and Jacobi identity show that the '''R'''<sup>3</sup> [[Euclidean space#Real coordinate space|vector space]] together with vector addition and the cross product forms a [[Lie algebra]], the Lie algebra of the real [[orthogonal group]] in 3 dimensions, [[SO(3)]].\nThe cross product does not obey the [[cancellation law]]: that is, {{nowrap|1='''a''' × '''b''' = '''a''' × '''c'''}} with {{nowrap|'''a''' ≠ '''0'''}} does not imply {{nowrap|1='''b''' = '''c'''}}, but only that:\n:<math> \\begin{align}\n  \\mathbf{0} &= (\\mathbf{a} \\times \\mathbf{b}) - (\\mathbf{a} \\times \\mathbf{c})\\\\\n             &= \\mathbf{a} \\times (\\mathbf{b} - \\mathbf{c}).\\\\\n\\end{align}</math>\n\nThis can be the case where '''b''' and '''c''' cancel, but additionally where '''a''' and {{nowrap|'''b''' − '''c'''}} are parallel; that is, they are related by a scale factor ''t'', leading to:\n:<math>\\mathbf{c} = \\mathbf{b} + t\\mathbf{a},</math>\nfor some scalar ''t''.\n\nIf, in addition to {{nowrap|1='''a''' × '''b''' = '''a''' × '''c'''}} and {{nowrap|'''a''' ≠ '''0'''}} as above, it is the case that {{nowrap|1='''a''' ⋅ '''b''' = '''a''' ⋅ '''c'''}} then\n:<math>\\begin{align}\n  \\mathbf{a} \\times (\\mathbf{b} - \\mathbf{c}) &= \\mathbf{0} \\\\\n   \\mathbf{a} \\cdot (\\mathbf{b} - \\mathbf{c}) &= 0,\n\\end{align}</math>\nAs {{nowrap|1='''b''' − '''c'''}} cannot be simultaneously parallel (for the cross product to be '''0''') and perpendicular (for the dot product to be 0) to '''a''', it must be the case that '''b''' and '''c''' cancel: {{nowrap|1='''b''' = '''c'''}}.\n\nFrom the geometrical definition, the cross product is invariant under proper [[Rotation (mathematics)|rotations]] about the axis defined by {{nowrap|'''a''' × '''b'''}}. In formulae:\n:<math>(R\\mathbf{a}) \\times (R\\mathbf{b}) = R(\\mathbf{a} \\times \\mathbf{b})</math>, where <math>R</math> is a [[rotation matrix]] with <math>\\det(R)=1</math>.\n\nMore generally, the cross product obeys the following identity under [[matrix (math)|matrix]] transformations:\n:<math>(M\\mathbf{a}) \\times (M\\mathbf{b}) = (\\det M) \\left(M^{-1}\\right)^\\mathrm{T}(\\mathbf{a} \\times \\mathbf{b}) = \\operatorname{cof} M (\\mathbf{a} \\times \\mathbf{b}) </math>\nwhere <math>M</math> is a 3-by-3 [[matrix (math)|matrix]] and <math>\\left(M^{-1}\\right)^\\mathrm{T}</math> is the [[transpose]] of the [[inverse matrix|inverse]] and <math>\\operatorname{cof}</math> is the cofactor matrix. It can be readily seen how this formula reduces to the former one if <math>M</math> is a rotation matrix.\n\nThe cross product of two vectors lies in the [[null space]] of the {{nowrap|2 × 3}} matrix with the vectors as rows:\n:<math>\\mathbf{a} \\times \\mathbf{b} \\in NS\\left(\\begin{bmatrix}\\mathbf{a} \\\\ \\mathbf{b}\\end{bmatrix}\\right).</math>\nFor the sum of two cross products, the following identity holds:\n:<math>\\mathbf{a} \\times \\mathbf{b} + \\mathbf{c} \\times \\mathbf{d} = (\\mathbf{a} - \\mathbf{c}) \\times (\\mathbf{b} - \\mathbf{d}) + \\mathbf{a} \\times \\mathbf{d} + \\mathbf{c} \\times \\mathbf{b}.</math>\n\n=== Differentiation ===\n{{Main|Vector-valued_function#Derivative_and_vector_multiplication|l1=Vector-valued function § Derivative and vector multiplication}}\n\nThe [[product rule]] of differential calculus applies to any bilinear operation, and therefore also to the cross product:\n:<math>\\frac{d}{dt}(\\mathbf{a} \\times \\mathbf{b}) = \\frac{d\\mathbf{a}}{dt} \\times \\mathbf{b} + \\mathbf{a} \\times \\frac{d\\mathbf{b}}{dt} ,</math>\n\nwhere '''a''' and '''b''' are vectors that depend on the real variable ''t''.\n\n=== Triple product expansion ===\n{{Main|Triple product}}\n\nThe cross product is used in both forms of the triple product. The [[scalar triple product]] of three vectors is defined as\n\n:<math>\\mathbf{a} \\cdot (\\mathbf{b} \\times \\mathbf{c}), </math>\n\nIt is the signed volume of the [[parallelepiped]] with edges '''a''', '''b''' and '''c''' and as such the vectors can be used in any order that's an [[even permutation]] of the above ordering. The following therefore are equal:\n\n:<math>\\mathbf{a} \\cdot (\\mathbf{b} \\times \\mathbf{c}) = \\mathbf{b} \\cdot (\\mathbf{c} \\times \\mathbf{a}) = \\mathbf{c} \\cdot (\\mathbf{a} \\times \\mathbf{b}), </math>\n\nThe [[vector triple product]] is the cross product of a vector with the result of another cross product, and is related to the dot product by the following formula\n\n:<math>\\mathbf{a} \\times (\\mathbf{b} \\times \\mathbf{c}) = \\mathbf{b}(\\mathbf{a} \\cdot \\mathbf{c}) - \\mathbf{c}(\\mathbf{a} \\cdot \\mathbf{b}).</math>\n\nThe [[mnemonic]] \"BAC minus CAB\" is used to remember the order of the vectors in the right hand member. This formula is used in [[physics]] to simplify vector calculations. A special case, regarding [[gradient]]s and useful in [[vector calculus]], is\n:<math>\\begin{align}\n  \\nabla \\times (\\nabla \\times \\mathbf{f}) &= \\nabla (\\nabla \\cdot \\mathbf{f} ) - (\\nabla \\cdot \\nabla) \\mathbf{f} \\\\\n                                           &= \\nabla (\\nabla \\cdot \\mathbf{f} ) - \\nabla^2 \\mathbf{f},\\\\\n\\end{align}</math>\n\nwhere ∇<sup>2</sup> is the [[vector Laplacian]] operator.\n\nOther identities relate the cross product to the scalar triple product:\n:<math>\\begin{align}\n(\\mathbf{a}\\times \\mathbf{b})\\times (\\mathbf{a}\\times \\mathbf{c}) &= (\\mathbf{a}\\cdot(\\mathbf{b}\\times \\mathbf{c})) \\mathbf{a} \\\\\n(\\mathbf{a}\\times \\mathbf{b})\\cdot(\\mathbf{c}\\times \\mathbf{d}) &= \\mathbf{b}^\\mathrm{T} \\left( \\left( \\mathbf{c}^\\mathrm{T} \\mathbf{a}\\right)I - \\mathbf{c} \\mathbf{a}^\\mathrm{T} \\right) \\mathbf{d}\\\\ &= (\\mathbf{a}\\cdot \\mathbf{c})(\\mathbf{b}\\cdot \\mathbf{d})-(\\mathbf{a}\\cdot \\mathbf{d}) (\\mathbf{b}\\cdot \\mathbf{c})\n\\end{align}</math>\n\nwhere ''I'' is the identity matrix.\n\n===Alternative formulation===\nThe cross product and the dot product are related by:\n:<math> \\left\\| \\mathbf{a} \\times \\mathbf{b} \\right\\| ^2  = \\left\\| \\mathbf{a}\\right\\|^2  \\left\\|\\mathbf{b}\\right\\|^2 - (\\mathbf{a} \\cdot \\mathbf{b})^2 .</math>\n\nThe right-hand side is the [[Gramian matrix|Gram determinant]] of '''a''' and '''b''', the square of the area of the parallelogram defined by the vectors. This condition determines the magnitude of the cross product. Namely, since the dot product is defined, in terms of the angle ''θ'' between the two vectors, as:\n\n:<math> \\mathbf{a \\cdot b} = \\left\\| \\mathbf a \\right\\| \\left\\| \\mathbf b \\right\\| \\cos \\theta , </math>\n\nthe above given relationship can be rewritten as follows:\n\n:<math>  \\left\\| \\mathbf{a \\times b} \\right\\|^2 = \\left\\| \\mathbf{a} \\right\\| ^2 \\left\\| \\mathbf{b}\\right \\| ^2 \\left(1-\\cos^2 \\theta \\right) .</math>\n\nInvoking the [[Pythagorean trigonometric identity]] one obtains:\n:<math> \\left\\| \\mathbf{a} \\times \\mathbf{b} \\right\\| = \\left\\| \\mathbf{a} \\right\\| \\left\\| \\mathbf{b} \\right\\| \\left| \\sin \\theta \\right| ,</math>\n\nwhich is the magnitude of the cross product expressed in terms of ''θ'', equal to the area of the parallelogram defined by '''a''' and '''b''' (see [[#Definition|definition]] above).\n\nThe combination of this requirement and the property that the cross product be orthogonal to its constituents '''a''' and '''b''' provides an alternative definition of the cross product.<ref name=Massey>{{cite journal |title=Cross products of vectors in higher dimensional Euclidean spaces |author=WS Massey |journal=The American Mathematical Monthly |volume=90 |date=Dec 1983 |pages=697–701 |issue=10 |doi=10.2307/2323537 |publisher=The American Mathematical Monthly, Vol. 90, No. 10 |ref=harv |jstor=2323537}}</ref>\n\n===Lagrange's identity===\nThe relation:\n:<math>\n  \\left\\| \\mathbf{a} \\times \\mathbf{b} \\right\\|^2 \\equiv\n  \\det \\begin{bmatrix}\n    \\mathbf{a} \\cdot \\mathbf{a} & \\mathbf{a} \\cdot \\mathbf{b} \\\\\n    \\mathbf{a} \\cdot \\mathbf{b}  & \\mathbf{b} \\cdot \\mathbf{b}\\\\\n  \\end{bmatrix} \\equiv\n  \\left\\| \\mathbf{a} \\right\\| ^2  \\left\\| \\mathbf{b} \\right\\| ^2 - (\\mathbf{a} \\cdot \\mathbf{b})^2 .\n</math>\n\ncan be compared with another relation involving the right-hand side, namely [[Lagrange's identity]] expressed as:<ref name=Boichenko>{{cite book |title=Dimension theory for ordinary differential equations |author1=Vladimir A. Boichenko |author2=Gennadiĭ Alekseevich Leonov |author3=Volker Reitmann |url=https://books.google.com/?id=9bN1-b_dSYsC&pg=PA26 |page=26 |isbn=3-519-00437-2 |year=2005 |publisher=Vieweg+Teubner Verlag}}</ref>\n\n:<math>\n  \\sum_{1 \\le i < j \\le n} \\left( a_ib_j - a_jb_i \\right)^2 \\equiv\n  \\left\\| \\mathbf a \\right\\|^2 \\left\\| \\mathbf b \\right\\|^2 - ( \\mathbf{a \\cdot b } )^2\\ ,\n</math>\n\nwhere '''a''' and '''b''' may be ''n''-dimensional vectors. This also shows that the [[Riemannian volume form]] for surfaces is exactly the [[Volume form|surface element]] from vector calculus. In the case where {{nowrap|1=''n'' = 3}}, combining these two equations results in the expression for the magnitude of the cross product in terms of its components:<ref name=Lounesto1>{{cite book |url=https://books.google.com/?id=kOsybQWDK4oC&pg=PA94&dq=%22which+in+coordinate+form+means+Lagrange%27s+identity%22&cd=1#v=onepage&q=%22which%20in%20coordinate%20form%20means%20Lagrange%27s%20identity%22 |author=Pertti Lounesto |page=94 |title=Clifford algebras and spinors |isbn=0-521-00551-5 |edition=2nd |publisher=Cambridge University Press |year=2001}}</ref>\n \n:<math>\\begin{align}\n           &\\left\\|\\mathbf{a} \\times \\mathbf{b}\\right\\|^2 \\equiv \\sum_{1 \\le i < j \\le 3}\\left(a_ib_j - a_jb_i \\right)^2 \\\\\n  \\equiv{} &\\left(a_1b_2 - b_1a_2\\right)^2 + \\left(a_2b_3 - a_3b_2\\right)^2 + \\left(a_3b_1 - a_1b_3\\right)^2 \\ .\n\\end{align}</math>\n\nThe same result is found directly using the components of the cross product found from:\n:<math>\\mathbf{a} \\times \\mathbf{b} \\equiv \\det \\begin{bmatrix}\n  \\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\\n  a_1 & a_2 & a_3 \\\\\n  b_1 & b_2 & b_3 \\\\\n\\end{bmatrix}.</math>\n\nIn '''R'''<sup>3</sup>, Lagrange's equation is a special case of the multiplicativity {{nowrap|1={{abs|'''vw'''}} = {{abs|'''v'''}}{{abs|'''w'''}}}} of the norm in the [[Quaternion#Algebraic properties|quaternion algebra]].\n\nIt is a special case of another formula, also sometimes called Lagrange's identity, which is the three dimensional case of the [[Binet–Cauchy identity]]:<ref name=Liu/><ref name=Weisstein>by {{cite book |author=Eric W. Weisstein |chapter=Binet-Cauchy identity |title=CRC concise encyclopedia of mathematics |url=https://books.google.com/?id=8LmCzWQYh_UC&pg=PA228 |page=228 |isbn=1-58488-347-2 |edition=2nd |year=2003 |publisher=CRC Press}}</ref>\n\n:<math>\n  (\\mathbf{a} \\times \\mathbf{b}) \\cdot (\\mathbf{c} \\times \\mathbf{d}) \\equiv\n  (\\mathbf{a} \\cdot \\mathbf{c})(\\mathbf{b} \\cdot \\mathbf{d}) - (\\mathbf{a} \\cdot \\mathbf{d})(\\mathbf{b} \\cdot \\mathbf{c}).\n</math>\n\nIf {{nowrap|1='''a''' = '''c'''}} and {{nowrap|1='''b''' = '''d'''}} this simplifies to the formula above.\n\n===Infinitesimal generators of rotations===\nThe cross product conveniently describes the infinitesimal generators of [[rotation (mathematics)|rotation]]s in '''R'''<sup>3</sup>. Specifically, if '''n''' is a unit vector in '''R'''<sup>3</sup> and ''R''(''φ'', '''n''') denotes a rotation about the axis through the origin specified by '''n''', with angle φ (measured in radians, counterclockwise when viewed from the tip of '''n'''), then\n:<math>\\left.{d\\over d\\phi} \\right|_{\\phi=0} R(\\phi,\\boldsymbol{n}) \\boldsymbol{x} = \\boldsymbol{n} \\times \\boldsymbol{x}</math>\nfor every vector '''x''' in '''R'''<sup>3</sup>. The cross product with '''n''' therefore describes the infinitesimal generator of the rotations about '''n'''. These infinitesimal generators form the [[Lie algebra]] '''so'''(3) of the [[rotation group SO(3)]], and we obtain the result that the Lie algebra '''R'''<sup>3</sup> with cross product is isomorphic to the Lie algebra '''so'''(3).\n\n== Alternative ways to compute the cross product ==\n\n=== Conversion to matrix multiplication ===\nThe vector cross product also can be expressed as the product of a [[skew-symmetric matrix]] and a vector:<ref name=Liu>{{cite journal |title=Hadamard, Khatri-Rao, Kronecker and other matrix products |journal=Int J Information and systems sciences |volume=4 |pages=160–177 |year=2008 |publisher=Institute for scientific computing and education |url=http://www.math.ualberta.ca/ijiss/SS-Volume-4-2008/No-1-08/SS-08-01-17.pdf |author1=Shuangzhe Liu |author2=Gõtz Trenkler |issue=1 |ref=harv}}</ref>\n:<math>\\mathbf{a} \\times \\mathbf{b} = [\\mathbf{a}]_{\\times} \\mathbf{b} = \\begin{bmatrix}\\,0&\\!-a_3&\\,\\,a_2\\\\ \\,\\,a_3&0&\\!-a_1\\\\-a_2&\\,\\,a_1&\\,0\\end{bmatrix}\\begin{bmatrix}b_1\\\\b_2\\\\b_3\\end{bmatrix}</math>\n:<math>\\mathbf{a} \\times \\mathbf{b} = [\\mathbf{b}]_{\\times}^\\mathrm T \\mathbf{a} = \\begin{bmatrix}\\,0&\\,\\,b_3&\\!-b_2\\\\ -b_3&0&\\,\\,b_1\\\\\\,\\,b_2&\\!-b_1&\\,0\\end{bmatrix}\\begin{bmatrix}a_1\\\\a_2\\\\a_3\\end{bmatrix} ,</math>\n\nwhere superscript <sup>T</sup> refers to the [[transpose]] operation, and ['''a''']<sub>×</sub> is defined by:\n\n:<math>[\\mathbf{a}]_{\\times} \\stackrel{\\rm def}{=} \\begin{bmatrix}\\,\\,0&\\!-a_3&\\,\\,\\,a_2\\\\\\,\\,\\,a_3&0&\\!-a_1\\\\\\!-a_2&\\,\\,a_1&\\,\\,0\\end{bmatrix}.</math>\n\nThe columns ['''a''']<sub>×,i</sub> of the skew-symmetric matrix for a vector '''a''' can be also obtained by calculating the cross product with [[unit vectors]], i.e.:\n\n:<math>[\\mathbf{a}]_{\\times, i} = \\mathbf{a} \\times \\mathbf{\\hat{e}_i}, \\; i\\in \\{1,2,3\\} </math>\n\nor\n\n:<math>[\\mathbf{a}]_{\\times} = \\sum_{i=1}^3(\\mathbf{a} \\times \\mathbf{\\hat{e}_i})\\otimes\\mathbf{\\hat{e}_i},</math>\n\nwhere <math>\\otimes</math> is the [[outer product]] operator.\n\nAlso, if '''a''' is itself expressed as a cross product:\n\n:<math>\\mathbf{a} = \\mathbf{c} \\times \\mathbf{d}</math>\n\nthen\n\n:<math>[\\mathbf{a}]_{\\times} = \\mathbf{d}\\mathbf{c}^\\mathrm{T}- \\mathbf{c}\\mathbf{d}^\\mathrm{T} .</math>\n\n:{| class=\"toccolours collapsible collapsed\" width=\"60%\" style=\"text-align:left\"\n!Proof by substitution\n|-\n|Evaluation of the cross product gives\n:<math> \\mathbf{a} = \\mathbf{c} \\times \\mathbf{d} = \\begin{pmatrix} \nc_2 d_3 - c_3 d_2 \\\\\nc_3 d_1 - c_1 d_3 \\\\\nc_1 d_2 - c_2 d_1 \\end{pmatrix}\n</math>\nHence, the left hand side equals\n:<math> [\\mathbf{a}]_{\\times} = \\begin{bmatrix} \n        0         & c_2 d_1 - c_1 d_2 & c_3 d_1 - c_1 d_3 \\\\\nc_1 d_2 - c_2 d_1 &         0         & c_3 d_2 - c_2 d_3 \\\\\nc_1 d_3 - c_3 d_1 & c_2 d_3 - c_3 d_2 &         0 \\end{bmatrix}\n</math>\nNow, for the right hand side,\n:<math> \\mathbf{c} \\mathbf{d}^{\\mathrm T} = \\begin{bmatrix}\nc_1 d_1 & c_1 d_2 & c_1 d_3 \\\\\nc_2 d_1 & c_2 d_2 & c_2 d_3 \\\\\nc_3 d_1 & c_3 d_2 & c_3 d_3 \\end{bmatrix}\n</math>\nAnd its transpose is\n:<math> \\mathbf{d} \\mathbf{c}^{\\mathrm T} = \\begin{bmatrix}\nc_1 d_1 & c_2 d_1 & c_3 d_1 \\\\\nc_1 d_2 & c_2 d_2 & c_3 d_2 \\\\\nc_1 d_3 & c_2 d_3 & c_3 d_3 \\end{bmatrix}\n</math>\nEvaluation of the right hand side gives\n:<math> \\mathbf{d} \\mathbf{c}^{\\mathrm T} - \n\\mathbf{c} \\mathbf{d}^{\\mathrm T} = \\begin{bmatrix} \n0 & c_2 d_1 - c_1 d_2 & c_3 d_1 - c_1 d_3 \\\\\n c_1 d_2 - c_2 d_1 & 0 & c_3 d_2 - c_2 d_3 \\\\\nc_1 d_3 - c_3 d_1 & c_2 d_3 - c_3 d_2 & 0 \\end{bmatrix}\n</math>\nComparison shows that the left hand side equals the right hand side.\n|}\n\nThis result can be generalized to higher dimensions using [[geometric algebra]]. In particular in any dimension [[bivector]]s can be identified with skew-symmetric matrices, so the product between a skew-symmetric matrix and vector is equivalent to the grade-1 part of the product of a bivector and vector.<ref name=\"lounesto2001\">{{cite book\n  | author = Lounesto, Pertti\n  | title = Clifford algebras and spinors\n  | publisher = Cambridge: Cambridge University Press\n  | year = 2001\n  | isbn = 978-0-521-00551-7\n  | pages = 193\n}}</ref>  In three dimensions bivectors are [[Hodge dual|dual]] to vectors so the product is equivalent to the cross product, with the bivector instead of its vector dual. In higher dimensions the product can still be calculated but bivectors have more degrees of freedom and are not equivalent to vectors.<ref name=\"lounesto2001\"/>\n\nThis notation is also often much easier to work with, for example, in [[epipolar geometry]].\n\nFrom the general properties of the cross product follows immediately that\n\n:<math>[\\mathbf{a}]_{\\times} \\, \\mathbf{a} = \\mathbf{0}</math> &nbsp; and &nbsp; <math>\\mathbf{a}^\\mathrm T \\, [\\mathbf{a}]_{\\times} = \\mathbf{0}</math>\n\nand from fact that ['''a''']<sub>×</sub> is skew-symmetric it follows that\n\n:<math>\\mathbf{b}^\\mathrm T \\, [\\mathbf{a}]_{\\times} \\, \\mathbf{b} = 0. </math>\n\nThe above-mentioned triple product expansion (bac–cab rule) can be easily proven using this notation.\n\nAs mentioned above, the Lie algebra '''R'''<sup>3</sup> with cross product is isomorphic to the Lie algebra '''so(3)''', whose elements can be identified with the 3×3 [[skew-symmetric matrix|skew-symmetric matrices]]. The map '''a''' → ['''a''']<sub>×</sub> provides an isomorphism between '''R'''<sup>3</sup> and '''so(3)'''. Under this map, the cross product of 3-vectors corresponds to the [[commutator]] of 3x3 skew-symmetric matrices.\n\n:{| class=\"toccolours collapsible collapsed\" width=\"70%\" style=\"text-align:left\"\n!Matrix conversion for cross product with canonical base vectors\n|-\n|Denoting with <math>\\mathbf{e}_i \\in \\mathbf{R}^{3 \\times 1}</math> the <math>i</math>-th canonical base vector, the cross product of a generic vector <math>\\mathbf{v} \\in \\mathbf{R}^{3 \\times 1}</math> with <math>\\mathbf{e}_i</math> is given by: <math>\\mathbf{v} \\times \\mathbf{e}_i = \\mathbf{C}_i \\mathbf{v}</math>, where\n\n<math> \\mathbf{C}_1 = \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & -1 & 0 \\end{bmatrix}, \\quad\n\\mathbf{C}_2 = \\begin{bmatrix} 0 & 0 & -1 \\\\ 0 & 0 & 0 \\\\ 1 & 0 & 0 \\end{bmatrix}, \\quad\n\\mathbf{C}_3 = \\begin{bmatrix} 0 & 1 & 0 \\\\ -1 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}\n</math>\n\nThese matrices share the following properties:\n* <math>\\mathbf{C}^T_i = - \\mathbf{C}_i</math> ([[skew-symmetric matrix|skew-symmetric]]);\n* Both trace and determinant are zero;\n* <math>\\text{rank}(\\mathbf{C}_i)=2</math>;\n* <math>\\mathbf{C}_i \\mathbf{C}^T_i = \\mathbf{P}^{ ^\\perp}_{\\mathbf{e}_i}</math> (see below);\n\nThe [[projection (linear algebra)#Orthogonal projection|orthogonal projection matrix]] of a vector <math>\\mathbf{v} \\neq \\mathbf{0}</math> is given by <math>\\mathbf{P}_{\\mathbf{v}}=\\mathbf{v}(\\mathbf{v}^T \\mathbf{v})^{-1} \\mathbf{v}^T</math>. The projection matrix onto the [[orthogonal complement]] is given by <math>\\mathbf{P}^{ ^\\perp}_{\\mathbf{v}} = \\mathbf{I} - \\mathbf{P}_{\\mathbf{v}}</math>, where <math>\\mathbf{I}</math> is the identity matrix. For the special case of <math>\\mathbf{v} = \\mathbf{e}_i</math>, it can be verified that\n\n<math> \\mathbf{P}^{ ^\\perp}_{\\mathbf{e}_1} = \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}, \\quad\n\\mathbf{P}^{ ^\\perp}_{\\mathbf{e}_2} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}, \\quad\n\\mathbf{P}^{ ^\\perp}_{\\mathbf{e}_3} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}\n</math>\n\nFor other properties of orthogonal projection matrices, see [[projection (linear algebra)]].\n|}\n\n===Index notation for tensors===\nThe cross product can alternatively be defined in terms of the [[Levi-Civita symbol]] ''ε<sub>ijk</sub>'' and a dot product ''η<sup>mi</sup>'' (= δ<sup>''mi''</sup> for an orthonormal basis), which are useful in converting vector notation for tensor applications:\n\n:<math>\\mathbf{c} = \\mathbf{a \\times b} \\Leftrightarrow\\ c^m = \\sum_{i=1}^3 \\sum_{j=1}^3 \\sum_{k=1}^3 \\eta^{mi} \\varepsilon_{ijk} a^j b^k</math>\n\nwhere the [[Indexed family|indices]] <math>i,j,k</math> correspond to vector components.  This characterization of the cross product is often expressed more compactly using the [[Einstein summation convention]] as\n:<math>\\mathbf{c} = \\mathbf{a \\times b} \\Leftrightarrow\\ c^m = \\eta^{mi} \\varepsilon_{ijk} a^j b^k</math>\n\nin which repeated indices are summed over the values 1 to 3. Note that this representation is another form of the skew-symmetric representation of the cross product:\n\n:<math>\\eta^{mi} \\varepsilon_{ijk} a^j = [\\mathbf{a}]_\\times.</math>\n\nIn [[classical mechanics]]: representing the cross product by using the Levi-Civita symbol can cause mechanical symmetries to be obvious when physical systems are [[isotropic]]. (An example: consider a particle in a Hooke's Law potential in three-space, free to oscillate in three dimensions; none of these dimensions are \"special\" in any sense, so symmetries lie in the cross-product-represented angular momentum, which are made clear by the abovementioned Levi-Civita representation).{{Citation needed|date=November 2009}}\n\n=== Mnemonic ===\n{{redirect|Xyzzy (mnemonic)||Xyzzy (disambiguation){{!}}Xyzzy}}\n\nThe word \"xyzzy\" can be used to remember the definition of the cross product.\n\nIf\n\n:<math>\\mathbf{a} = \\mathbf{b} \\times \\mathbf{c}</math>\n\nwhere:\n\n:<math>\n  \\mathbf{a} = \\begin{bmatrix}a_x\\\\a_y\\\\a_z\\end{bmatrix},\n  \\mathbf{b} = \\begin{bmatrix}b_x\\\\b_y\\\\b_z\\end{bmatrix},\n  \\mathbf{c} = \\begin{bmatrix}c_x\\\\c_y\\\\c_z\\end{bmatrix}\n</math>\n\nthen:\n\n:<math>a_x = b_y c_z - b_z c_y </math>\n:<math>a_y = b_z c_x - b_x c_z </math>\n:<math>a_z = b_x c_y - b_y c_x. </math>\n\nThe second and third equations can be obtained from the first by simply vertically rotating the subscripts, {{nowrap|''x'' → ''y'' → ''z'' → ''x''}}. The problem, of course, is how to remember the first equation, and two options are available for this purpose: either to remember the relevant two diagonals of Sarrus's scheme (those containing '''''i'''''), or to remember the xyzzy sequence.\n\nSince the first diagonal in Sarrus's scheme is just the [[main diagonal]] of the [[cross product#Matrix notation|above]]-mentioned 3×3 matrix, the first three letters of the word xyzzy can be very easily remembered.\n\n=== Cross visualization ===\nSimilarly to the mnemonic device above, a \"cross\" or X can be visualized between the two vectors in the equation. This may be helpful for remembering the correct cross product formula.\n\nIf\n\n:<math>\\mathbf{a} = \\mathbf{b} \\times \\mathbf{c}</math>\n\nthen:\n\n:<math>\n  \\mathbf{a} =\n    \\begin{bmatrix}b_x\\\\b_y\\\\b_z\\end{bmatrix} \\times\n    \\begin{bmatrix}c_x\\\\c_y\\\\c_z\\end{bmatrix}.\n</math>\n\nIf we want to obtain the formula for <math>a_x</math> we simply drop the <math>b_x</math> and <math>c_x</math> from the formula, and take the next two components down:\n\n:<math>\n  a_x =\n    \\begin{bmatrix}b_y\\\\b_z\\end{bmatrix} \\times\n    \\begin{bmatrix}c_y\\\\c_z\\end{bmatrix}.\n</math>\n\nWhen doing this for <math>a_y</math> the next two elements down should \"wrap around\" the matrix so that after the z component comes the x component. For clarity, when performing this operation for <math>a_y</math>, the next two components should be z and x (in that order). While for <math>a_z</math> the next two components should be taken as x and y.\n\n:<math>\n  a_y =\n    \\begin{bmatrix}b_z\\\\b_x\\end{bmatrix} \\times\n    \\begin{bmatrix}c_z\\\\c_x\\end{bmatrix},\n  a_z =\n    \\begin{bmatrix}b_x\\\\b_y\\end{bmatrix} \\times\n    \\begin{bmatrix}c_x\\\\c_y\\end{bmatrix}\n</math>\n\nFor <math>a_x</math> then, if we visualize the cross operator as pointing from an element on the left to an element on the right, we can take the first element on the left and simply multiply by the element that the cross points to in the right hand matrix. We then subtract the next element down on the left, multiplied by the element that the cross points to here as well. This results in our <math>a_x</math> formula –\n\n:<math>a_x = b_y c_z - b_z c_y.</math>\n\nWe can do this in the same way for <math>a_y</math> and <math>a_z</math> to construct their associated formulas.\n\n== Applications ==\nThe cross product has applications in various contexts:  e.g. it is used in computational geometry, physics and engineering.\nA non-exhaustive list of examples follows.\n\n=== Computational geometry ===\nThe cross product appears in the calculation of the distance of two [[Skew lines#Distance|skew lines]] (lines not in the same plane) from each other in three-dimensional space.\n\nThe cross product can be used to calculate the normal for a triangle or polygon, an operation frequently performed in [[computer graphics]]. For example, the winding of a polygon (clockwise or anticlockwise) about a point within the polygon can be calculated by triangulating the polygon (like spoking a wheel) and summing the angles (between the spokes) using the cross product to keep track of the sign of each angle.\n\nIn [[computational geometry]] of [[the plane]], the cross product is used to determine the sign of the [[acute angle]] defined by three points <math> p_1=(x_1,y_1), p_2=(x_2,y_2)</math> and <math> p_3=(x_3,y_3)</math>. It corresponds to the direction (upward or downward) of the cross product of the two coplanar [[vector (geometry)|vector]]s defined by the two pairs of points <math>(p_1, p_2)</math> and <math>(p_1, p_3)</math>. The sign of the acute angle is the sign of the expression\n:<math> P = (x_2-x_1)(y_3-y_1)-(y_2-y_1)(x_3-x_1),</math>\nwhich is the signed length of the cross product of the two vectors.\n\nIn the \"right-handed\" coordinate system,  if the result is 0, the points are [[collinear]]; if it is positive, the three points constitute a positive angle of rotation around <math> p_1</math> from <math> p_2</math> to <math> p_3</math>, otherwise a negative angle. From another point of view, the sign of <math>P</math> tells whether <math> p_3</math> lies to the left or to the right of line <math> p_1, p_2.</math>\n\nThe cross product is used in calculating the volume of a [[polyhedron]] such as a [[tetrahedron#Volume|tetrahedron]] or [[parallelepiped#Volume|parallelepiped]].\n\n===Angular momentum and torque===\n\nThe [[angular momentum]] <math>\\mathbf{L}</math> of a particle about a given origin is defined as:\n\n:  <math>\\mathbf{L} = \\mathbf{r} \\times \\mathbf{p},</math>\n\nwhere <math>\\mathbf{r}</math> is the position vector of the particle relative to the origin, <math>\\mathbf{p}</math> is the linear momentum of the particle.\n\nIn the same way, the [[Moment (physics)|moment]] <math>\\mathbf{M}</math> of a force <math>\\mathbf{F}_\\mathrm{B}</math> applied at point B around point A is given as:\n\n:  <math> \\mathbf{M}_\\mathrm{A} = \\mathbf{r}_\\mathrm{AB} \\times \\mathbf{F}_\\mathrm{B}\\,</math>\n\nIn mechanics the ''moment of a force'' is also called ''[[torque]]'' and written as <math>\\mathbf{\\tau}</math>\n\nSince position <math>\\mathbf{r}</math>, linear momentum <math>\\mathbf{p}</math> and force <math>\\mathbf{F}</math> are all ''true'' vectors, both the angular momentum <math>\\mathbf{L}</math> and the moment of a force <math>\\mathbf{M}</math> are [[pseudovector|''pseudovectors'']] or ''axial vectors''.\n\n===Rigid body===\nThe cross product frequently appears in the description of rigid motions. Two points ''P''  and ''Q'' on a [[rigid body]] can be related by:\n\n: <math>\\mathbf{v}_P - \\mathbf{v}_Q = \\mathbf{\\omega} \\times \\left( \\mathbf{r}_P - \\mathbf{r}_Q \\right)\\,</math>\n\nwhere <math>\\mathbf{r}</math> is the point's position, <math>\\mathbf{v}</math> is its velocity and <math>\\mathbf{\\omega}</math> is the body's [[angular velocity]].\n\nSince position <math>\\mathbf{r}</math> and velocity <math>\\mathbf{v}</math> are ''true'' vectors, the angular velocity <math>\\mathbf{\\omega}</math> is a ''[[pseudovector]]'' or ''axial vector''.\n\n===Lorentz force===\n{{see also|Lorentz force}}\nThe cross product is used to describe the [[Lorentz force]] experienced by a moving electric charge <math> q_e</math>:\n: <math>\\mathbf{F} = q_e \\left( \\mathbf{E}+ \\mathbf{v} \\times \\mathbf{B} \\right)</math>\n\nSince velocity <math>\\mathbf{v}</math>, force <math>\\mathbf{F}</math> and electric field <math>\\mathbf{E}</math> are all ''true'' vectors, the magnetic field <math>\\mathbf{B}</math> is a ''[[pseudovector]]''.\n\n=== Other ===\nIn [[vector calculus]], the cross product is used to define the formula for the [[vector operator]] [[Curl (mathematics)|curl]].\n\nThe trick of rewriting a cross product in terms of a matrix multiplication appears frequently in [[epipolar geometry|epipolar]] and multi-view geometry, in particular when deriving matching constraints.\n\n== Cross product as an external product ==\n[[File:Exterior calc cross product.svg|right|thumb|The cross product in relation to the exterior product. In red are the orthogonal [[unit vector]], and the \"parallel\" unit bivector.]]\nThe cross product can be defined in terms of the [[exterior product]]. In this context,{{which?|date=June 2019}} it is an [[Cross product#External product|external product]].<ref>{{cite book|author=Greub, W|title=Multilinear Algebra|year=1978}}</ref> This view{{which?|date=June 2019}} allows for a natural geometric interpretation of the cross product. In [[exterior algebra]] the exterior product of two vectors is a [[bivector]]. A bivector is an oriented plane element, in much the same way that a vector is an oriented line element. Given two vectors ''a'' and ''b'', one can view the bivector {{nowrap|1=''a'' ∧ ''b''}} as the oriented parallelogram spanned by ''a'' and ''b''. The cross product is then obtained by taking the [[Hodge star]] of the bivector {{nowrap|1=''a'' ∧ ''b''}}, mapping [[p-vector|2-vectors]] to vectors:\n\n:<math>a \\times b = \\star (a \\wedge b) \\,.</math>\n\nThis can be thought of as the oriented multi-dimensional element \"perpendicular\" to the bivector. Only in three dimensions is the result an oriented line element – a vector – whereas, for example, in 4 dimensions the Hodge dual of a bivector is two-dimensional – another oriented plane element. So, only in three dimensions is the cross product of ''a'' and ''b'' the vector dual to the bivector {{nowrap|1=''a'' ∧ ''b''}}: it is perpendicular to the bivector, with orientation dependent on the coordinate system's handedness, and has the same magnitude relative to the unit normal vector as {{nowrap|1=''a'' ∧ ''b''}} has relative to the unit bivector; precisely the properties described above.\n\n== Cross product and handedness ==<!-- This section is linked from [[Vector calculus]] -->\n\nWhen measurable quantities involve cross products, the ''handedness'' of the coordinate systems used cannot be arbitrary. However, when physics laws are written as equations, it should be possible to make an arbitrary choice of the coordinate system (including handedness). To avoid problems, one should be careful to never write down an equation where the two sides do not behave equally under all transformations that need to be considered. For example, if one side of the equation is a cross product of two vectors, one must take into account that when the handedness of the coordinate system is ''not'' fixed a priori, the result is not a (true) vector but a [[pseudovector]]. Therefore, for consistency, the other side must also be a pseudovector.{{Citation needed|date=April 2008}}\n\nMore generally, the result of a cross product may be either a vector or a pseudovector, depending on the type of its operands (vectors or pseudovectors). Namely, vectors and pseudovectors are interrelated in the following ways under application of the cross product:\n\n* vector &times; vector = pseudovector\n* pseudovector &times; pseudovector = pseudovector\n* vector &times; pseudovector = vector\n* pseudovector &times; vector = vector.\n\nSo by the above relationships, the unit basis vectors '''i''', '''j''' and '''k''' of an orthonormal, right-handed (Cartesian) coordinate frame '''must''' all be pseudovectors (if a basis of mixed vector types is disallowed, as it normally is) since {{nowrap|1='''i''' × '''j''' = '''k'''}}, {{nowrap|1='''j''' × '''k''' = '''i'''}} and {{nowrap|1='''k''' × '''i''' = '''j'''}}.\n\nBecause the cross product may also be a (true) vector, it may not change direction with a mirror image transformation. This happens, according to the above relationships, if one of the operands is a (true) vector and the other one is a pseudovector (e.g., the cross product of two vectors). For instance, a [[vector triple product]] involving three (true) vectors is a (true) vector.\n\nA handedness-free approach is possible using [[exterior algebra]].\n\n== Generalizations ==\nThere are several ways to generalize the cross product to the higher dimensions.\n\n=== Lie algebra ===\n{{Main|Lie algebra}}\nThe cross product can be seen as one of the simplest Lie products, and is thus generalized by [[Lie algebra]]s, which are axiomatized as binary products satisfying the axioms of multilinearity, skew-symmetry, and the Jacobi identity. Many Lie algebras exist, and their study is a major field of mathematics, called [[Lie theory]].\n\nFor example, the [[Heisenberg algebra]] gives another Lie algebra structure on <math>\\mathbf{R}^3.</math> In the basis <math>\\{x,y,z\\},</math> the product is <math>[x,y]=z, [x,z]=[y,z]=0.</math>\n\n=== Quaternions ===\n{{Further|quaternions and spatial rotation}}\nThe cross product can also be described in terms of [[quaternion]]s, and this is why the letters '''i''', '''j''', '''k''' are a convention for the standard basis on '''R'''<sup>3</sup>. The unit vectors '''i''', '''j''', '''k''' correspond to \"binary\" (180 deg) rotations about their respective axes (Altmann, S. L., 1986, Ch. 12), said rotations being represented by \"pure\" quaternions (zero real part) with unit norms.\n\nFor instance, the above given cross product relations among '''i''', '''j''', and '''k''' agree with the multiplicative relations among the quaternions ''i'', ''j'', and ''k''. In general, if a vector {{nowrap|[''a''<sub>1</sub>, ''a''<sub>2</sub>, ''a''<sub>3</sub>]}} is represented as the quaternion {{nowrap|''a''<sub>1</sub>''i'' + ''a''<sub>2</sub>''j'' + ''a''<sub>3</sub>''k''}}, the cross product of two vectors can be obtained by taking their product as quaternions and deleting the real part of the result. The real part will be the negative of the [[dot product]] of the two vectors.\n\nAlternatively, using the above identification of the 'purely imaginary' quaternions with '''R'''<sup>3</sup>, the cross product may be thought of as half of the [[commutator]] of two quaternions.\n\n=== Octonions ===\n{{See also|Seven-dimensional cross product|Octonion}}\nA cross product for 7-dimensional vectors can be obtained in the same way by using the [[octonions]] instead of the quaternions. The nonexistence of nontrivial vector-valued cross products of two vectors in other dimensions is related to the result from [[Hurwitz's theorem (normed division algebras)|Hurwitz's theorem]] that the only [[normed division algebra]]s are the ones with dimension 1, 2, 4, and 8.\n\n=== Exterior product ===\n{{Main|Exterior algebra|Comparison of vector algebra and geometric algebra#Cross and exterior products}}\nIn general dimension, there is no direct analogue of the binary cross product that yields specifically a vector. There is however the [[exterior product]], which has similar properties, except that the exterior product of two vectors is now a [[p-vector|2-vector]] instead of an ordinary vector. As mentioned above, the cross product can be interpreted as the exterior product in three dimensions by using the Hodge star operator to map 2-vectors to vectors.  The Hodge dual of the exterior product yields an {{nowrap|(''n'' − 2)}}-vector, which is a natural generalization of the cross product in any number of dimensions.\n\nThe exterior product and dot product can be combined (through summation) to form the [[Geometric algebra|geometric product]] in [[geometric algebra]].\n\n=== External product ===\nAs mentioned above, the cross product can be interpreted in three dimensions as the Hodge dual of the exterior product. In any finite ''n'' dimensions, the Hodge dual of the exterior product of ''n''-1 vectors is a vector. So, instead of a binary operation, in arbitrary finite dimensions, the cross product is generalized as the Hodge dual of the exterior product of some given ''n-1'' vectors. This generalization is called '''external product'''<ref>{{cite book|editor=Hogben, L|title=Handbook of Linear Algebra|year=2007}}</ref>\n\n=== Commutator product ===\n{{Main|Geometric algebra#Extensions of the inner and exterior products|Cross product#Cross product and handedness|Cross product#Lie algebra}}\n\nInterpreting the three-dimensional [[vector space]] of the algebra as the [[bivector|2-vector]] (not the 1-vector) [[Graded vector space|subalgebra]] of the three-dimensional [[geometric algebra]], where <math>\\mathbf{i} = \\mathbf{e_2} \\mathbf{e_3}</math>, <math>\\mathbf{j} = \\mathbf{e_1} \\mathbf{e_3}</math>, and <math>\\mathbf{k} = \\mathbf{e_1} \\mathbf{e_2}</math>, the cross product corresponds exactly to the [[geometric algebra#Extensions of the inner and exterior products|commutator product]] in geometric algebra and both use the same symbol <math>\\times</math>. The commutator product is defined for 2-vectors <math>A</math> and <math>B</math> in geometric algebra as:\n\n: <math>A \\times B = \\tfrac{1}{2}(AB - BA) </math>\n\nwhere <math>AB</math> is the [[geometric algebra#The geometric product|geometric product]].<ref>{{cite book|title=Understanding Geometric Algebra for Electromagnetic Theory|year=2011|last1=Arthur|first1=John W.|page=49|isbn=978-0470941638|publisher=[[IEEE Press]]|url=https://books.google.com/books/about/Understanding_Geometric_Algebra_for_Elec.html?id=rxGCaDvBCoAC}}</ref>\n\nThe commutator product could be generalised to arbitrary [[multivector#Geometric algebra|multivectors]] in three dimensions, which results in a multivector consisting of only elements of [[Graded vector space|grades]] 1 (1-vectors/[[Cross product#Cross product and handedness|true vectors]]) and 2 (2-vectors/[[pseudovector]]s). While the commutator product of two 1-vectors is indeed the same as the [[exterior product]] and yields a 2-vector, the commutator of a 1-vector and a 2-vector yields a true vector, corresponding instead to the [[Geometric algebra#Extensions of the inner and exterior products|left and right contractions]] in geometric algebra. The commutator product of two 2-vectors has no corresponding equivalent product, which is why the commutator product is defined in the first place for 2-vectors. Furthermore, the commutator triple product of three 2-vectors is the same as the [[vector triple product]] of the same three pseudovectors in vector algebra. However, the commutator triple product of three 1-vectors in geometric algebra is instead the [[Sign (mathematics)#Sign of a direction|negative]] of the [[vector triple product]] of the same three true vectors in vector algebra.\n\nGeneralizations to higher dimensions is provided by the same commutator product of 2-vectors in higher-dimensional geometric algebras, but the 2-vectors are no longer pseudovectors. Just as the commutator product/cross product of 2-vectors in three dimensions [[Cross product#Lie algebra|correspond to the simplest Lie algebra]], the 2-vector subalgebras of higher dimensional geometric algebra equipped with the commutator product also correspond to the Lie algebras.<ref>{{cite book|title=Geometric Algebra for Physicists|year=2003|last1=Doran|first1=Chris|last2=Lasenby|first2=Anthony|pages=401-408|isbn=978-0521715959|publisher=[[Cambridge University Press]]|url=https://books.google.com/books/about/Geometric_Algebra_for_Physicists.html?id=VW4yt0WHdjoC}}</ref> Also as in three dimensions, the commutator product could be further generalised to arbitrary multivectors.\n\n=== Multilinear algebra ===\nIn the context of [[multilinear algebra]], the cross product can be seen as the (1,2)-tensor (a [[mixed tensor]], specifically a [[bilinear map]]) obtained from the 3-dimensional [[volume form]],<ref group=\"note\">By a volume form one means a function that takes in ''n'' vectors and gives out a scalar, the volume of the [[Parallelepiped#Parallelotope|parallelotope]] defined by the vectors: <math> V\\times \\cdots \\times V \\to \\mathbf{R}.</math> This is an ''n''-ary multilinear skew-symmetric form. In the presence of a basis, such as on <math>\\mathbf{R}^n,</math> this is given by the [[determinant]], but in an abstract vector space, this is added structure. In terms of [[G-structure|''G''-structures]], a volume form is an [[Special linear group|<math> SL</math>]]-structure.</ref> a (0,3)-tensor, by [[Raising and lowering indices|raising an index]].\n\nIn detail, the 3-dimensional volume form defines a product <math> V \\times V \\times V \\to \\mathbf{R},</math> by taking the determinant of the matrix given by these 3 vectors.\nBy [[Dual space|duality]], this is equivalent to a function <math> V \\times V \\to V^*,</math> (fixing any two inputs gives a function <math> V \\to \\mathbf{R}</math> by evaluating on the third input) and in the presence of an [[inner product]] (such as the [[dot product]]; more generally, a non-degenerate bilinear form), we have an isomorphism <math> V \\to V^*,</math> and thus this yields a map <math> V \\times V \\to V,</math> which is the cross product: a (0,3)-tensor (3 vector inputs, scalar output) has been transformed into a (1,2)-tensor (2 vector inputs, 1 vector output) by \"raising an index\".\n\nTranslating the above algebra into geometry, the function \"volume of the parallelepiped defined by <math> (a,b,-)</math>\" (where the first two vectors are fixed and the last is an input), which defines a function <math> V \\to \\mathbf{R}</math>, can be ''represented'' uniquely as the dot product with a vector: this vector is the cross product <math> a \\times b.</math> From this perspective, the cross product is ''defined'' by the [[scalar triple product]], <math>\\mathrm{Vol}(a,b,c) = (a\\times b)\\cdot c.</math>\n\nIn the same way, in higher dimensions one may define generalized cross products by raising indices of the ''n''-dimensional volume form, which is a <math> (0,n)</math>-tensor.\nThe most direct generalizations of the cross product are to define either:\n* a <math> (1,n-1)</math>-tensor, which takes as input <math> n-1</math> vectors, and gives as output 1 vector – an <math> (n-1)</math>-ary vector-valued product, or\n* a <math> (n-2,2)</math>-tensor, which takes as input 2 vectors and gives as output [[skew-symmetric tensor]] of rank {{nowrap|''n'' − 2}} – a binary product with rank {{nowrap|''n'' − 2}} tensor values.  One can also define <math>(k,n-k)</math>-tensors for other ''k''.\n\nThese products are all multilinear and skew-symmetric, and can be defined in terms of the determinant and [[parity (physics)|parity]].\n\nThe <math> (n-1)</math>-ary product can be described as follows: given <math> n-1</math> vectors <math> v_1,\\dots,v_{n-1}</math> in <math>\\mathbf{R}^n,</math> define their generalized cross product <math> v_n = v_1 \\times \\cdots \\times v_{n-1}</math> as:\n* perpendicular to the hyperplane defined by the <math> v_i,</math>\n* magnitude is the volume of the parallelotope defined by the <math> v_i,</math> which can be computed as the [[Gram determinant]] of the <math> v_i,</math>\n* oriented so that <math> v_1,\\dots,v_n</math> is positively oriented.\nThis is the unique multilinear, alternating product which evaluates to <math> e_1 \\times \\cdots \\times e_{n-1} = e_n</math>, <math> e_2 \\times \\cdots \\times e_n = e_1,</math> and so forth for cyclic permutations of indices.\n\nIn coordinates, one can give a formula for this <math> (n-1)</math>-ary analogue of the cross product in '''R'''<sup>''n''</sup> by:\n\n:<math>\\bigwedge(\\mathbf{v}_1,\\dots,\\mathbf{v}_{n-1}) =\n  \\begin{vmatrix}\n    v_1{}^1 &\\cdots &v_1{}^{n}\\\\\n    \\vdots  &\\ddots &\\vdots\\\\\n    v_{n-1}{}^1 & \\cdots &v_{n-1}{}^{n}\\\\\n    \\mathbf{e}_1 &\\cdots &\\mathbf{e}_{n}\n  \\end{vmatrix}.\n</math>\n\nThis formula is identical in structure to the determinant formula for the normal cross product in '''R'''<sup>3</sup> except that the row of basis vectors is the last row in the determinant rather than the first. The reason for this is to ensure that the ordered vectors ('''v'''<sub>1</sub>, ...,'''v'''<sub>''n''−1</sub>, Λ('''v'''<sub>1</sub>, ...,'''v'''<sub>''n''−1</sub>)) have a positive [[orientation (mathematics)|orientation]] with respect to ('''e'''<sub>1</sub>, ..., '''e'''<sub>''n''</sub>). If ''n'' is odd, this modification leaves the value unchanged, so this convention agrees with the normal definition of the binary product. In the case that ''n'' is even, however, the distinction must be kept. This <math> (n-1)</math>-ary form enjoys many of the same properties as the vector cross product: it is [[alternating form|alternating]] and linear in its arguments, it is perpendicular to each argument, and its magnitude gives the hypervolume of the region bounded by the arguments. And just like the vector cross product, it can be defined in a coordinate independent way as the Hodge dual of the wedge product of the arguments.\n\n=== Skew-symmetric matrix ===\n\nIf the cross product is defined as a binary operation, it takes as ''input'' exactly two vectors. If its ''output'' is not required to be a vector or a pseudovector but instead a ''[[Matrix (mathematics)|matrix]]'', then it can be generalized in an arbitrary number of dimensions.<ref name=cross_prod_Maxwell_ND>{{Cite journal |author = A. W. McDavid |author2 = C. D. McMullen |title = Generalizing Cross Products and Maxwell's Equations to Universal Extra Dimensions |year = 2006 |url = https://arxiv.org/ftp/hep-ph/papers/0609/0609260.pdf}}</ref><ref name=prod_vett_ND>{{cite book |author =C. A. Gonano |title = Estensione in N-D di prodotto vettore e rotore e loro applicazioni |publisher = Politecnico di Milano, Italy. |year = 2011 |url= https://www.politesi.polimi.it/bitstream/10589/34061/1/2011_12_Gonano.pdf |isbn=<!--none--> }}</ref><ref name=cross_prod_ND>{{Cite journal |author = C. A. Gonano |author2 = R. E. Zich |title = Cross product in N Dimensions – the doublewedge product |year = 2014 |url = http://arxiv-web3.library.cornell.edu/pdf/1408.5799v1.pdf }}</ref>\n\nIn mechanics, for example, the [[angular velocity]] can be interpreted either as a pseudovector <math>\\omega</math> or as a [[anti-symmetric matrix]] or [[skew-symmetric tensor]] <math>\\Omega</math>. In the latter case, the velocity law for a [[rigid body]] looks:\n: <math>\\mathbf{v}_P - \\mathbf{v}_Q = {\\Omega} \\cdot\\left( \\mathbf{r}_P - \\mathbf{r}_Q \\right)</math>\n\nwhere Ω is formally defined from the rotation matrix <math>R^{N\\times N}</math> associated to body's frame: <math>\\Omega \\triangleq \\frac{d R}{dt}R^\\mathrm{T}.</math> In three-dimensions holds:\n: <math>\\Omega =  [\\omega]_{\\times} =\n\\begin{bmatrix}\n\\,\\,0&\\!-\\omega_3&\\,\\,\\,\\omega_2\\\\\n\\,\\,\\,\\omega_3&0&\\!-\\omega_1\\\\\n\\!-\\omega_2&\\,\\,\\omega_1&\\,\\,0\n\\end{bmatrix}\n</math>\n\nIn [[quantum mechanics]] the [[angular momentum]] <math>L</math> is often represented as an anti-symmetric matrix or tensor. More precisely, it is the result of cross product involving position <math>\\mathbf{x}</math> and linear momentum <math>\\mathbf{p}</math>:\n\n: <math>L_{ij} = x_i p_j -  p_i x_j </math>\n\nSince both <math>\\mathbf{x}</math> and <math>\\mathbf{p}</math> can have an arbitrary number <math>N</math> of components, that kind of cross product can be extended to any dimension, holding the \"physical\" interpretation of the operation.\n\nSee {{section link||Alternative ways to compute the cross product}} for numerical details.\n\n== History ==\n\nIn 1773, [[Joseph-Louis Lagrange]] introduced the component form of both the dot and cross products in order to study the [[tetrahedron]] in three dimensions.<ref>{{cite book|author=Lagrange, JL|title=Oeuvres|volume=vol 3|chapter=Solutions analytiques de quelques problèmes sur les pyramides triangulaires|year=1773}}</ref>  In 1843, [[William Rowan Hamilton]] introduced the [[quaternion]] product, and with it the terms \"vector\" and \"scalar\". Given two quaternions {{nowrap|[0, '''u''']}} and {{nowrap|[0, '''v''']}}, where '''u''' and '''v''' are vectors in '''R'''<sup>3</sup>, their quaternion product can be summarized as {{nowrap|[−'''u''' ⋅ '''v''', '''u''' × '''v''']}}. [[James Clerk Maxwell]] used Hamilton's quaternion tools to develop his famous [[Maxwell's equations|electromagnetism equations]], and for this and other reasons quaternions for a time were an essential part of physics education.\n\nIn 1878 [[William Kingdon Clifford]] published his [[Elements of Dynamic]] which was an advanced text for its time. He defined the product of two vectors<ref>[[William Kingdon Clifford]] (1878) [http://dlxs2.library.cornell.edu/cgi/t/text/text-idx?c=math;cc=math;view=toc;subview=short;idno=04370002 Elements of Dynamic]{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }}, Part I, page 95, London: MacMillan & Co; online presentation by [[Cornell University]] ''Historical Mathematical Monographs''</ref> to have magnitude equal to the [[area]] of the [[parallelogram]] of which they are two sides, and direction perpendicular to their plane.\n\n[[Oliver Heaviside]] and [[Josiah Willard Gibbs]] also felt that quaternion methods were too cumbersome, often requiring the scalar or vector part of a result to be extracted. Thus, about forty years after the quaternion product, the [[dot product]] and cross product were introduced &mdash; to heated opposition. Pivotal to (eventual) acceptance was the efficiency of the new approach, allowing Heaviside to reduce the equations of electromagnetism from Maxwell's original 20 to the four commonly seen today.<ref>{{Cite book |first = Paul J.|last = Nahin |title = Oliver Heaviside: the life, work, and times of an electrical genius of the Victorian age|publisher = JHU Press |isbn = 0-8018-6909-9 |year = 2000 |pages = 108–109}}</ref>\n\nLargely independent of this development, and largely unappreciated at the time, [[Hermann Grassmann]] created a geometric algebra not tied to dimension two or three, with the [[exterior product]] playing a central role. In 1853 [[Augustin-Louis Cauchy]], a contemporary of Grassmann, published a paper on algebraic keys which were used to solve equations and had the same multiplication properties as the cross product.<ref>{{cite book|last=Crowe|first=Michael J.|title=A History of Vector Analysis|year=1994|publisher=Dover|isbn=0-486-67910-1|page=83}}</ref><ref>{{cite book|last=Cauchy|first=Augustin-Louis|title=Ouvres |volume=12 |page=[https://books.google.com/books?id=0k9eAAAAcAAJ&pg=PA16#v=onepage&q&f=false 16]|year=1900}}</ref> Clifford combined the algebras of Hamilton and Grassmann to produce [[Clifford algebra]], where in the case of three-dimensional vectors the bivector produced from two vectors dualizes to a vector, thus reproducing the cross product.\n\nThe cross notation and the name \"cross product\" began with Gibbs. Originally they appeared in privately published notes for his students in 1881 as ''Elements of Vector Analysis''. The utility for mechanics was noted by [[Aleksandr Kotelnikov]]. Gibbs's notation and the name \"cross product\" later reached a wide audience through [[Vector Analysis]], a textbook by [[Edwin Bidwell Wilson]], a former student. Wilson rearranged material from Gibbs's lectures, together with material from publications by Heaviside, Föpps, and Hamilton. He divided [[vector analysis]] into three parts:\n{{quote|First, that which concerns addition and the scalar and vector products of vectors. Second, that which concerns the differential and integral calculus in its relations to scalar and vector functions. Third, that which contains the theory of the linear vector function.}}\n\nTwo main kinds of vector multiplications were defined, and they were called as follows:\n*The '''direct''', '''scalar''', or '''dot''' product of two vectors\n*The '''skew''', '''vector''', or '''cross''' product of two vectors\nSeveral kinds of [[triple product]]s and products of more than three vectors were also examined. The above-mentioned triple product expansion was also included.\n\n== See also ==\n{{Portal|Geometry}}\n* [[Bivector]]\n* [[Cartesian product]] – A product of two sets\n* [[Dot product]]\n* [[Exterior algebra]]\n* [[Geometric_algebra#Rotating_systems|Geometric algebra: Rotating systems]]\n* [[Multiple cross products]] – Products involving more than three vectors\n* [[Pseudovector]]\n* [[×]] (the symbol)\n\n== Notes ==\n{{Reflist|group=note}}\n\n== References ==\n{{reflist|30em}}\n* {{Cite book | last=Cajori | first=Florian | author-link=Florian Cajori | title=A History Of Mathematical Notations Volume II | year=1929 | publisher=[[Open Court Publishing Company|Open Court Publishing]] | url=https://archive.org/details/historyofmathema027671mbp |  isbn=978-0-486-67766-8 | page=&nbsp;134 | ref=harv | postscript=<!--None-->}}\n* [[E. A. Milne]] (1948) [[Vectorial Mechanics]], Chapter 2: Vector Product, pp 11 –31, London: [[Methuen Publishing]].\n* {{Cite book | last=Wilson | first=Edwin Bidwell | title=Vector Analysis: A text-book for the use of students of mathematics and physics, founded upon the lectures of J. Willard Gibbs | year=1901 | publisher=[[Yale University Press]] | isbn=<!--none--> | url=https://archive.org/details/117714283 | ref=harv | postscript=<!--None-->}}\n* {{Cite book |author = T. Levi-Civita |author2 = U. Amaldi |title = Lezioni di meccanica razionale |publisher = Zanichelli editore |location = Bologna |year = 1949 |language = Italian}}\n\n== External links ==\n* {{Springer |title = Cross product |id = p/c027120}}\n* {{Mathworld |title = Cross Product |urlname = CrossProduct}}\n* [http://behindtheguesses.blogspot.com/2009/04/dot-and-cross-products.html A quick geometrical derivation and interpretation of cross products]\n* {{cite arXiv |title = Cross product in N Dimensions – the doublewedge product |eprint = 1408.5799 |first1 = Carlo Andrea |last1 = Gonano |first2 = Riccardo Enrico |last2 = Zich |class = math.GM |date = 21 July 2014 }} Polytechnic University of Milan, Italy.\n* {{cite journal |title = Multi-dimensional vector product |arxiv = math/0204357 |first = Zurab K. |last = Silagadze |date = 30 April 2002 |doi=10.1088/0305-4470/35/23/310 |volume=35 |journal=Journal of Physics A: Mathematical and General |pages=4949–4953|bibcode=2002JPhA...35.4949S }} (it is only possible in 7-D space)\n* [https://web.archive.org/web/20060424151900/http://physics.syr.edu/courses/java-suite/crosspro.html An interactive tutorial] created at [[Syracuse University]] – (requires [[Java (programming language)|java]])\n* [http://www.cs.berkeley.edu/~wkahan/MathH110/Cross.pdf W. Kahan (2007). Cross-Products and Rotations in Euclidean 2- and 3-Space. University of California, Berkeley (PDF).]\n\n{{linear algebra}}\n\n{{DEFAULTSORT:Cross Product}}\n[[Category:Bilinear operators]]\n[[Category:Binary operations]]\n[[Category:Linear algebra]]\n[[Category:Analytic geometry]]\n[[Category:Vectors (mathematics and physics)]]"
    },
    {
      "title": "Cup product",
      "url": "https://en.wikipedia.org/wiki/Cup_product",
      "text": "In [[mathematics]], specifically in [[algebraic topology]], the '''cup product''' is a method of adjoining two [[cocycle]]s of degree ''p'' and ''q'' to form a composite cocycle of degree ''p'' + ''q''.  This defines an associative (and distributive) graded commutative product operation in  cohomology, turning the cohomology of a space ''X'' into a graded ring, ''H''<sup>∗</sup>(''X''), called the [[cohomology ring]].  The cup product was introduced in work of [[James Waddell Alexander II|J. W. Alexander]], [[Eduard Čech]] and [[Hassler Whitney]] from 1935–1938, and, in full generality, by [[Samuel Eilenberg]] in 1944.\n\n==Definition==\nIn [[singular cohomology]], the '''cup product''' is a construction giving a product on the [[graded ring|graded]] [[cohomology ring]] ''H''<sup>∗</sup>(''X'') of a [[topological space]] ''X''.\n\nThe construction starts with a product of [[Cochain (algebraic topology)|cochain]]s: if ''c''<sup>''p''</sup> is a ''p''-cochain and \n''d''<sup>''q''</sup> is a ''q''-cochain, then \n:<math>(c^p \\smile d^q)(\\sigma) = c^p(\\sigma \\circ \\iota_{0,1, ... p}) \\cdot d^q(\\sigma \\circ \\iota_{p, p+1 ,..., p + q})</math>\nwhere σ is a [[Singular homology|singular]] (''p'' + ''q'') -[[simplex]] and <math>\\iota_S , S \\subset \\{0,1,...,p+q \\} </math> \nis the canonical [[embedding]] of the simplex spanned by S into the <math>(p+q)</math>-simplex whose vertices are indexed by <math>\\{0,...,p+q \\}</math>.\n\nInformally, <math> \\sigma \\circ \\iota_{0,1, ..., p}</math> is the ''p''-th '''front face''' and <math>\\sigma \\circ \\iota_{p, p+1, ..., p + q}</math>  is the ''q''-th '''back face''' of σ, respectively.\n\nThe [[coboundary]] of the cup product of cochains c<sup>''p''</sup> and d<sup>''q''</sup> is given by\n:<math>\\delta(c^p \\smile d^q) = \\delta{c^p} \\smile d^q + (-1)^p(c^p \\smile \\delta{d^q}).</math>\nThe cup product of two cocycles is again a cocycle, and the product of a coboundary with a cocycle (in either order) is a coboundary.  The cup product operation induces a bilinear operation on cohomology,\n: <math> H^p(X) \\times H^q(X) \\to H^{p+q}(X). </math>\n\n==Properties==\nThe cup product operation in cohomology satisfies the identity\n:<math>\\alpha^p \\smile \\beta^q = (-1)^{pq}(\\beta^q \\smile \\alpha^p)</math>\nso that the corresponding multiplication is [[supercommutative|graded-commutative]].\n\nThe cup product is [[functor]]ial, in the following sense:  if \n:<math>f\\colon X\\to Y</math>\nis a continuous function, and \n:<math>f^*\\colon H^*(Y)\\to H^*(X)</math>\nis the induced [[homomorphism]] in cohomology, then \n:<math>f^*(\\alpha \\smile \\beta) =f^*(\\alpha) \\smile f^*(\\beta),</math>\nfor all classes α, β in ''H'' <sup>*</sup>(''Y''). In other words, ''f'' <sup>*</sup> is a (graded) [[ring homomorphism]].\n\n==Interpretation==\nIt is possible to view the cup product <math> \\smile \\colon H^p(X) \\times H^q(X) \\to H^{p+q}(X)</math> as induced from the following composition:\n\n<math> \\displaystyle C^\\bullet(X) \\times C^\\bullet(X) \\to C^\\bullet(X \\times X) \\overset{\\Delta^*}{\\to} C^\\bullet(X) </math>\n\nin terms of the [[chain complex]]es of <math>X</math> and <math>X \\times X</math>, where the first map is the [[Künneth formula|Künneth map]] and the second is the map induced by the [[diagonal functor|diagonal]] <math> \\Delta \\colon X \\to X \\times X</math>.\n\nThis composition passes to the quotient to give a well-defined map in terms of cohomology, this is the cup product. This approach explains the existence of a cup product for cohomology but not for homology: <math> \\Delta \\colon X \\to X \\times X</math> induces a map <math>\\Delta^* \\colon H^\\bullet(X \\times X) \\to H^\\bullet(X)</math> but would also induce a map <math>\\Delta_* \\colon H_\\bullet(X) \\to H_\\bullet(X \\times X)</math>, which goes the wrong way round to allow us to define a product. This is however of use in defining the [[cap product]].\n\nBilinearity follows from this presentation of cup product, i.e. <math> (u_1 + u_2) \\smile v = u_1 \\smile v + u_2 \\smile v </math> and <math> u \\smile (v_1 + v_2) = u \\smile v_1 + u \\smile v_2. </math>\n\n==Examples==\nCup products may be used to distinguish manifolds from wedges of spaces with identical cohomology groups.  The space <math>X:= S^2\\vee S^1\\vee S^1</math> has the same cohomology groups as the torus ''T'', but with a different cup product.  In the case of ''X'' the multiplication of the [[cochain]]s associated to the copies of <math>S^1</math> is degenerate, whereas in ''T'' multiplication in the first cohomology group can be used to decompose the torus as a 2-cell diagram, thus having product equal to '''Z''' (more generally ''M'' where this is the base module).\n\n==Other definitions==\n\n===Cup product and differential forms===\nIn [[de Rham cohomology]], the cup product of [[differential forms]] is induced by the [[wedge product]]. In other words, the wedge product of\ntwo closed differential forms belongs to the de Rham class of the cup product of the two original de Rham classes.\n\n===Cup product and geometric intersections===\n[[File:Linking Number 1.svg|thumb|The [[linking number]] can be defined in terms of a non-vanishing cup product on the complement of a link. The complement of these two linked circles deformation retracts to a torus, which has a non-vanishing cup product.]]\nFor manifolds, there is a geometric heuristic  that \"the cup product is dual to intersections.\"<ref name=\":0\">{{Cite web|url=https://math.berkeley.edu/~hutching/teach/215b-2011/cup.pdf|title=Cup Product and Intersections|last=Hutchings|first=Michael|date=|website=|archive-url=|archive-date=|dead-url=|access-date=}}</ref><ref>{{Citation|last=Ciencias TV|title=Informal talk in Derived Geometry (Jacob Lurie)|date=2016-12-10|url=https://www.youtube.com/watch?v=YWpD6c69k_M|accessdate=2018-04-26}}</ref>\n\nIndeed,  let <math>M</math> be a  [[smooth manifold]] of dimension <math>n</math>. If two submanifolds <math>A,B</math>  of codimension <math>i</math> and <math>j</math> intersect [[Transversality (mathematics)|transversely]], then their intersection <math>A \\cap B</math> is again a submanifold of codimension <math>i+j</math>.  By taking the images of the fundamental homology classes of these manifolds under inclusion, one can obtain a bilinear product on homology.  This product is  [[Poincaré duality|Poincaré dual]] to the cup product, in the sense that taking the Poincaré pairings <math>[A]^*, [B]^* \\in H^{i},H^{j}</math> then there is  the following equality :\n\n<math>[A]^* \\smile [B]^*=[A \\cap B]^* \\in H^{i+j}(X, \\mathbb Z)</math>.<ref name=\":0\" />\n\nSimilarly, the [[linking number]] can be defined in terms of intersections, shifting dimensions by 1, or alternatively in terms of a non-vanishing cup product on the complement of a link.\n\n==Massey products==\n[[File:BorromeanRings.svg|thumb|[[Massey product]]s generalize cup product, allowing one to define \"higher order linking numbers\", the [[Milnor invariants]].]]\n{{main|Massey product}}\nThe cup product is a binary (2-ary) operation; one can define a ternary (3-ary) and higher order operation called the [[Massey product]], which generalizes the cup product. This is a higher order [[cohomology operation]], which is only partly defined (only defined for some triples).\n\n==See also==\n*[[Singular homology]]\n*[[Homology theory]]\n*[[Cap product]]\n*[[Massey product]]\n*[[Torelli group]]\n\n==References==\n{{Reflist}}\n* James R. Munkres, \"Elements of Algebraic Topology\", Perseus Publishing, Cambridge Massachusetts (1984) {{ISBN|0-201-04586-9}} (hardcover) {{ISBN|0-201-62728-0}} (paperback)\n* [[Glen E. Bredon]], \"Topology and Geometry\", Springer-Verlag, New York (1993) {{ISBN|0-387-97926-3}}\n* Allen Hatcher, \"[http://www.math.cornell.edu/~hatcher/AT/ATpage.html Algebraic Topology]\", Cambridge Publishing Company (2002) {{ISBN|0-521-79540-0}}\n\n[[Category:Homology theory]]\n[[Category:Algebraic topology]]\n[[Category:Binary operations]]"
    },
    {
      "title": "DE-9IM",
      "url": "https://en.wikipedia.org/wiki/DE-9IM",
      "text": "[[File:DE-9IM-logoSmall.png|right]]\n\n{{Technical|date=May 2019}}\n\nThe '''Dimensionally Extended nine-Intersection Model (DE-9IM)''' is a [[topological]] [[Interpretation (logic)|model]]\nand a [[Specification (technical standard)|standard]] used to describe the [[spatial relation]]s of\ntwo regions (two [[2D geometric model|geometries in two-dimensions]], '''R'''<sup>2</sup>),\nin [[Geometry]], [[Point-set topology]], [[Geospatial topology]], and fields related to [[Spatial analysis|computer spatial analysis]].\nSince the spatial relations expressed by the model are topological they are invariant to [[Rotation (mathematics)|rotation]], [[Translation (geometry)|translation]] and [[Scaling (geometry)|scaling]] transformations.\n\nThe matrix provides an approach for classifying geometry relations.  Roughly speaking, with a true/false matrix domain,\nthere are 512 possible 2D topologic relations, that can be grouped into ''binary classification schemes''.\nFor English speakers, there are about 10 schemes (relations) that have a name that reflects their semantics (e.g. \"Intersects\", \"Touches\", \"Equals\", and others.) When testing two geometries against a scheme, the result of this test is a '''''spatial predicate''''' named by the scheme.\n\nThe model was developed by Clementini and others<ref>\n{{cite book |last=Clementini |first=Eliseo |first2 = Paolino | last2 = Di Felice | first3 = Peter | last3 = van Oosterom |editor1-first=David |editor1-last=Abel |editor2-last=Ooi |editor2-first=Beng Chin |chapter=A small set of formal topological relationships suitable for end-user interaction |title=Advances in Spatial Databases: Third International Symposium, SSD '93 Singapore, June 23–25, 1993 Proceedings |series=Lecture Notes in Computer Science |volume=692/1993 |year=1993 |publisher=Springer |doi=10.1007/3-540-56869-7_16 |pages=277–295}}</ref><ref>{{cite journal |last1=Clementini |first1=Eliseo |last2=Sharma |first2=Jayant |last3=Egenhofer |first3=Max J. |year=1994 |title=Modelling topological spatial relations: Strategies for query processing |journal=Computers & Graphics |volume=18 |issue=6 |pages=815–822 |doi=10.1016/0097-8493(94)90007-8 }}</ref>\nbased on the seminal works of Egenhofer and others.<ref>{{cite journal | first1 = M.J. | last1 = Egenhofer | first2 = R.D.| last2 = Franzosa | year = 1991 | title = Point-set topological spatial relations | doi = 10.1080/02693799108927841 | journal = Int. J. GIS | volume = 5 | issue = 2 | pages = 161–174 }}</ref><ref name=\"sdh1990\">{{cite journal|first1=M.J. |last1=Egenhofer |first2=J.R. |last2=Herring |year=1990 |title=A Mathematical Framework for the Definition of Topological Relationships |url=http://www.spatial.maine.edu/~max/MJEJRH-SDH1990.pdf |deadurl=yes |archiveurl=https://web.archive.org/web/20100614161335/http://www.spatial.maine.edu/~max/MJEJRH-SDH1990.pdf |archivedate=2010-06-14 |df= }}</ref>\nIt has been used as a basis for standards of ''[[Information retrieval|queries]]'' and ''[[First-order logic|assertions]]'' in [[geographic information systems]] (GIS) and [[spatial database]]s.\n\n== Matrix model ==\nThe '''DE-9IM''' model is based on a 3×3 intersection [[Matrix (mathematics)|matrix]] with the form:\n\n{{NumBlk|:|<math>\n\\operatorname{DE9IM}(a,b) = \\begin{bmatrix}\n\\dim(I(a) \\cap I(b)) & \\dim(I(a) \\cap B(b)) & \\dim(I(a) \\cap E(b)) \\\\\n\\dim(B(a) \\cap I(b)) & \\dim(B(a) \\cap B(b)) & \\dim(B(a) \\cap E(b))\\\\\n\\dim(E(a) \\cap I(b)) & \\dim(E(a) \\cap B(b)) & \\dim(E(a) \\cap E(b))\n\\end{bmatrix}\n</math>\n|{{EquationRef|1}}}}\n\nwhere {{tmath|\\dim}} is the [[dimension]] of the [[Intersection (set theory)|intersection]] (∩) of the [[Interior of a manifold|interior]] (I), [[Boundary of a manifold|boundary]] (B), and [[Exterior (topology)|exterior]] (E) of geometries ''a'' and ''b''.\n\nNote that in this article the words ''interior'' and ''boundary'' are used in the sense used in algebraic topology and manifold theory, not in the sense used in general topology: e.g. by the interior of a line segment we mean the line segment without its endpoints and by its boundary, the two endpoints (in the general topology sense, the interior of a line segment in the plane is empty and the line segment is its own boundary).\n\nIn the notation of topological space operators, the matrix elements can be expressed also as\n{{NumBlk|:|{{math|1=''I''(''a'')=''a''<sup>o</sup> &nbsp;&nbsp; ''B''(''a'')=∂''a'' &nbsp;&nbsp; ''E''(''a'')=''a''<sup>''e''</sup>}}|{{EquationRef|2}}}}\n\nThe dimension of [[empty set]]s (∅) are denoted as −1 or {{mono|F}} (false). The dimension of non-empty sets (¬∅) are denoted with the maximum number of dimensions of the intersection, specifically {{mono|0}} for [[Point (geometry)|points]], {{mono|1}} for [[Line (geometry)|lines]], {{mono|2}} for [[area]]s.  Then, the [[Data domain|domain]] of the model is {{mset|1={{mono|0}},{{mono|1}},{{mono|2}},{{mono|F}} }}.\n\nA simplified version of {{tmath|\\dim(x)}} values are obtained mapping the values {{mset|1={{mono|0,1,2}} }} to {{mono|T}} (true), so using the [[boolean domain]] {{mset|1={{mono|T}},{{mono|F}} }}.  The matrix, denoted with operators, can be expressed as\n{{NumBlk|:|<math>\n\\operatorname{bin}(\\operatorname{DE9IM}(a,b)) = \\operatorname{9IM}(a,b) = \\begin{bmatrix}\na^o  \\cap b^o \\ne \\emptyset   &   a^o \\cap \\partial{b} \\ne \\emptyset & a^o \\cap b^e \\ne \\emptyset \\\\\n\\partial{a} \\cap b^o\\ne\\emptyset   &    \\partial{a} \\cap \\partial{b}\\ne\\emptyset  &  \\partial{a} \\cap b^e\\ne\\emptyset \\\\\na^e \\cap b^o\\ne\\emptyset    &    a^e \\cap \\partial{b}\\ne\\emptyset   &   a^e  \\cap b^e\\ne\\emptyset\n\\end{bmatrix}\n</math>\n|{{EquationRef|3}}}}\n\nThe elements of the matrix can be named as shown below:\n\n{{NumBlk|:|<math>\n\\begin{bmatrix}\nII & IB & IE \\\\\nBI & BB & BE \\\\\nEI & EB & EE\n\\end{bmatrix}\n</math>\n|{{EquationRef|4}}}}\n\nBoth matrix forms, with dimensional and boolean domains, can be [[Serialization|serialized]] as \"'''''DE-9IM''' string codes''\", which represent them in a single-line string pattern. Since 1999 the ''string codes'' have a [[#Standards|standard]]<ref name=\"firstStd\">The \"[[OpenGIS]] Simple Features Specification For SQL\", [http://portal.opengeospatial.org/files/?artifact_id=829 Revision 1.1], was released at May 5, 1999. It was the first international standard to establish the format conventions for ''DE-9IM string codes'', and the names of the \"Named Spatial Relationship predicates based on the DE-9IM\" (see section with this title).</ref> format.\n\nFor output checking or pattern analysis, a matrix value (or a string code) can be checked by a \"[[Mask (computing)|mask]]\": a desired output value with optional [[asterisk]] symbols as [[Wildcard character|wildcards]] — that is, \"{{mono|*}}\" indicating output positions that the designer does not care about (free values or \"don't-care positions\").\nThe domain of the mask elements is {{mset|1={{mono|0}},{{mono|1}},{{mono|2}},{{mono|F}},{{mono|*}} }}, or {{mset|1={{mono|T}},{{mono|F}},{{mono|*}} }} for the boolean form.\n\nThe simpler models ''4-Intersection'' and ''9-Intersection'' were proposed before '''DE-9IM''' for expressing ''spatial relations''<ref name=\"4vs9\">M. J. Egenhofer, J. Sharma, and D. Mark (1993) \"[http://www.spatial.maine.edu/~max/4Vs9.pdf A Critical Comparison of the 4-Intersection and 9-Intersection Models for Spatial Relations: Formal Analysis]\", In: [http://mapcontext.com/autocarto/proceedings/auto-carto-11/index.html Auto-Carto XI].</ref> (and originated the terms ''4IM'' and ''9IM'').  They can be used instead of the '''DE-9IM''' to optimize computation when input conditions satisfy specific constraints.\n\n=== Illustration ===\nVisually, for two overlapping polygonal geometries, this looks like:<ref name=\"PostGISch4\" />\n{| border=\"0\" align=\"center\" \n|-\n|\n| align=\"center\" |\n{| border=\"0\" summary=\"manufactured viewport for HTML img\" cellspacing=\"0\" cellpadding=\"0\"\n|-\n| align=\"center\" valign=\"middle\" | ''b'' &nbsp; [[File:DE9IM b.svg|100px]]\n|}\n|-\n| align=\"center\" valign=\"middle\" |\n{| border=\"0\" summary=\"manufactured viewport for HTML img\" cellspacing=\"0\" cellpadding=\"0\"\n|-\n| align=\"center\" valign=\"middle\" | ''a''<br>[[File:DE9IM a.svg|100px]]\n|}\n|\n{| border=\"0\" class=\"wikitable\"\n|-\n! align=\"center\" | \n! align=\"center\" | '''Interior'''\n! align=\"center\" | '''Boundary'''\n! align=\"center\" | '''Exterior'''\n|-\n| align=\"center\" | '''Interior'''\n| align=\"center\" style=\"border-right: 2px solid #BCB; border-bottom: 2px solid #BCB;\"| [[File:DE9IM II.svg|100px]]\n\n&nbsp; <math>\\dim[I(a){\\color{red}\\cap}I(b)] = 2</math> &nbsp;\n| align=\"center\" style=\"border-right: 2px solid #BCB; border-bottom: 2px solid #BCB;\" | [[File:DE9IM IB.svg|100px]]\n\n&nbsp; <math>\\dim[I(a){\\color{red}\\cap}B(b)] = 1</math> &nbsp;\n| align=\"center\" style=\"border-bottom: 2px solid #BCB;\" | [[File:DE9IM IE.svg|100px]]\n\n&nbsp; <math>\\dim[I(a){\\color{red}\\cap}E(b)] = 2</math> &nbsp;\n|-\n| align=\"center\" | <span class=\"bold\">'''Boundary'''</span>\n| align=\"center\" style=\"border-right: 2px solid #BCB; border-bottom: 2px solid #BCB;\" | <div class=\"informalfigure\"><div>[[File:DE9IM BI.svg|100px]]</div></div>\n\n&nbsp; <math>\\dim[B(a){\\color{red}\\cap}I(b)] = 1</math> &nbsp;\n| align=\"center\" style=\"border-right: 2px solid #BCB; border-bottom: 2px solid #BCB;\" | <div class=\"informalfigure\"><div>[[File:DE9IM BB.svg|100px]]</div></div>\n\n&nbsp; <math>\\dim[B(a){\\color{red}\\cap}B(b)] = 0</math> &nbsp;\n| align=\"center\" style=\"border-bottom: 2px solid #BCB;\"| <div class=\"informalfigure\"><div>[[File:DE9IM BE.svg|100px]]</div></div>\n\n&nbsp; <math>\\dim[B(a){\\color{red}\\cap}E(b)] = 1</math> &nbsp;\n|-\n| align=\"center\" | <span class=\"bold\">'''Exterior'''</span>\n| align=\"center\" style=\"border-right: 2px solid #BCB;\"| <div class=\"informalfigure\"><div>[[File:DE9IM EI.svg|100px]]</div></div>\n\n&nbsp; <math>\\dim[E(a){\\color{red}\\cap}I(b)] = 2</math> &nbsp;\n| align=\"center\" style=\"border-right: 2px solid #BCB;\"| <div class=\"informalfigure\"><div>[[File:DE9IM EB.svg|100px]]</div></div>\n\n&nbsp; <math>\\dim[E(a){\\color{red}\\cap}B(b)] = 1</math> &nbsp;\n| align=\"center\" | <div class=\"informalfigure\"><div>[[File:DE9IM EE.svg|100px]]</div></div>\n\n&nbsp; <math>\\dim[E(a){\\color{red}\\cap}E(b)] = 2</math> &nbsp;\n|}\n|}\nReading from left-to-right and top-to-bottom, the '''DE-9IM'''(''a'',''b'') string code is '{{mono|'''212101212'''}}', the compact representation of {{tmath|1=II=2,\\,IB=1,\\,IE=2,\\,BI=1,\\,BB=0,\\,BE=1,\\,EI=2,\\,EB=1,\\,EE=2}}.\n\n== Spatial predicates ==\n'''Spatial predicates''' are [[Topological property|topologically-invariant]] [[Binary relation|binary]] [[spatial relation]]s based on the '''DE-9IM'''.  For ease of use \"named spatial predicates\" have been defined for some common relations.\n\nThe ''spatial predicate'' [[Subroutine|functions]] that can be derived (expressed by masks) from '''DE-9IM''' include:<ref name=\"sdh1990\"/>\n<ref>{{citation |url=http://www.vividsolutions.com/jts/javadoc/com/vividsolutions/jts/geom/IntersectionMatrix.html |title=JTS: Class IntersectionMatrix |publisher=Vivid Solutions, Inc. |deadurl=yes |archiveurl=https://web.archive.org/web/20110321145301/http://www.vividsolutions.com/jts/javadoc/com/vividsolutions/jts/geom/IntersectionMatrix.html |archivedate=2011-03-21 |df= }}</ref>\n\nPredicates defined with masks of domain {{mset|1={{mono|T}},{{mono|F}},{{mono|*}} }}\n{| class=\"wikitable\"\n|-\n! style=\"width:8em\" | Name (synonym)\n!colspan=\"4\"| Intersection matrix and mask code string<br>([[Boolean algebra#Basic operations|boolean OR]] between matrices)\n!Meaning and definition<ref name=\"sdh1990\"/>\n!Equivalent\n|-\n!rowspan=\"2\"| Equals\n|colspan=\"4\"| <math>\\Bigl[\\begin{smallmatrix}\n\\mathrm{T}&\\mathrm{*}&\\mathrm{F}\\\\\n\\mathrm{*}&\\mathrm{*}&\\mathrm{F}\\\\\n\\mathrm{F}&\\mathrm{F}&\\mathrm{*}\n\\end{smallmatrix}\\Bigr]</math>\n|rowspan=\"2\"|{{NumBlk|:|{{math|''II'' ∧ ~''IE'' ∧ ~''BE'' ∧ ~''EI'' ∧ ~''EB''}}|{{EquationRef|5}}}}''a'' and ''b'' are topologically [[Equality (relational operator)|equal]]. \"Two geometries are topologically equal if their interiors intersect and no part of the interior or boundary of one geometry intersects the exterior of the other\".<ref>JTS Technical Specifications of 2003.</ref>\n|rowspan=\"2\"|''Within'' & ''Contains''\n|-\n| <code>T*F**FFF*</code>\n|\n|\n|\n|-\n!rowspan=\"2\"| Disjoint\n|colspan=\"4\"| <math>\\Bigl[\\begin{smallmatrix}\n\\mathrm{F}&\\mathrm{F}&\\mathrm{*}\\\\\n\\mathrm{F}&\\mathrm{F}&\\mathrm{*}\\\\\n\\mathrm{*}&\\mathrm{*}&\\mathrm{*}\n\\end{smallmatrix}\\Bigr]</math>\n|rowspan=\"2\"|{{NumBlk|:|{{math|''~II'' ∧ ~''IB'' ∧ ~''BI'' ∧ ~''BB''}}|{{EquationRef|6}}}}''a'' and ''b'' are [[Disjoint sets|disjoint]]: they have no point in common. They form a set of  [[Disconnected (topology)#Disconnected spaces|disconnected]] geometries.\n|rowspan=\"2\"|''not Intersects''\n|-\n| <code>FF*FF****</code>\n|\n|\n|\n|-\n!rowspan=\"2\"| Touches<br>(meets)\n| <math>\\Bigl[\\begin{smallmatrix}\n\\mathrm{F}&\\mathrm{T}&\\mathrm{*}\\\\\n\\mathrm{*}&\\mathrm{*}&\\mathrm{*}\\\\\n\\mathrm{*}&\\mathrm{*}&\\mathrm{*}\n\\end{smallmatrix}\\Bigr]</math>\n| <math>\\Bigl[\\begin{smallmatrix}\n\\mathrm{F}&\\mathrm{*}&\\mathrm{*}\\\\\n\\mathrm{T}&\\mathrm{*}&\\mathrm{*}\\\\\n\\mathrm{*}&\\mathrm{*}&\\mathrm{*}\n\\end{smallmatrix}\\Bigr]</math>\n|colspan=\"2\"| <math>\\Bigl[\\begin{smallmatrix}\n\\mathrm{F}&\\mathrm{*}&\\mathrm{*}\\\\\n\\mathrm{*}&\\mathrm{T}&\\mathrm{*}\\\\\n\\mathrm{*}&\\mathrm{*}&\\mathrm{*}\n\\end{smallmatrix}\\Bigr]</math>\n|rowspan=\"2\"|{{NumBlk|:|{{math|''~II'' ∧ (''IB'' ∨ ''BI'' ∨ ''BB'')}}|{{EquationRef|7}}}}''a'' touches ''b'': they have at least one point in common, but their interiors do not intersect.\n|rowspan=\"2\"|\n|-\n| <code>FT*******</code>\n| <code>F**T*****</code>\n|colspan=\"2\"| <code>F***T****</code>\n|-\n!rowspan=\"2\"| Contains\n|colspan=\"4\"| <math>\\Bigl[\\begin{smallmatrix}\n\\mathrm{T}&\\mathrm{*}&\\mathrm{*}\\\\\n\\mathrm{*}&\\mathrm{*}&\\mathrm{*}\\\\\n\\mathrm{F}&\\mathrm{F}&\\mathrm{*}\n\\end{smallmatrix}\\Bigr]</math>\n|rowspan=\"2\"|{{NumBlk|:|{{math|''II'' ∧ ~''EI'' ∧ ~''EB''}}|{{EquationRef|8}}}}''a'' contains ''b'': geometry ''b'' lies in ''a'', and the interiors intersect. Another definition: \"''a'' contains ''b'' iff no points of ''b''  lie in the exterior of ''a'', and at least one point of the interior of ''b'' lies in the interior of ''a''\".<ref name=\"davis2007\">M. Davis (2007), \"[http://lin-ear-th-inking.blogspot.com.br/2007/06/subtleties-of-ogc-covers-spatial.html Quirks of the 'Contains' Spatial Predicate]\".</ref>\n|rowspan=\"2\"|''Within''(''b'',''a'')\n|-\n|colspan=\"4\"| <code>T*****FF*</code>\n|-\n!rowspan=\"2\"| Covers\n| <math>\\Bigl[\\begin{smallmatrix}\n\\mathrm{T}&\\mathrm{*}&\\mathrm{*}\\\\\n\\mathrm{*}&\\mathrm{*}&\\mathrm{*}\\\\\n\\mathrm{F}&\\mathrm{F}&\\mathrm{*}\n\\end{smallmatrix}\\Bigr]</math>\n| <math>\\Bigl[\\begin{smallmatrix}\n\\mathrm{*}&\\mathrm{T}&\\mathrm{*}\\\\\n\\mathrm{*}&\\mathrm{*}&\\mathrm{*}\\\\\n\\mathrm{F}&\\mathrm{F}&\\mathrm{*}\n\\end{smallmatrix}\\Bigr]</math>\n| <math>\\Bigl[\\begin{smallmatrix}\n\\mathrm{*}&\\mathrm{*}&\\mathrm{*}\\\\\n\\mathrm{T}&\\mathrm{*}&\\mathrm{*}\\\\\n\\mathrm{F}&\\mathrm{F}&\\mathrm{*}\n\\end{smallmatrix}\\Bigr]</math>\n| <math>\\Bigl[\\begin{smallmatrix}\n\\mathrm{*}&\\mathrm{*}&\\mathrm{*}\\\\\n\\mathrm{*}&\\mathrm{T}&\\mathrm{*}\\\\\n\\mathrm{F}&\\mathrm{F}&\\mathrm{*}\n\\end{smallmatrix}\\Bigr]</math>\n|rowspan=\"2\"|{{NumBlk|:|{{math|(''II'' ∨ ''IB'' ∨ ''BI'' ∨ ''BB'') ∧ ~''EI'' ∧ ~''EB''}}|{{EquationRef|9}}}}''a'' covers ''b'': geometry ''b'' lies in ''a''. Other definitions: \"At least one point of ''b'' lies in ''a'', and no point of ''b'' lies in the exterior of ''a''\", or \"Every point of ''b'' is a point of (the interior or boundary of) ''a''\".\n|rowspan=\"2\"|''CoveredBy''(''b'',''a'')\n|-\n| <code>T*****FF*</code>\n| <code>*T****FF*</code>\n| <code>***T**FF*</code>\n| <code>****T*FF*</code>\n|}\n\nPredicates that can be obtained from the above by [[Negation#Definition|logical negation]] or parameter inversion ([[transposed matrix|matrix transposition]]), as indicated by the last column:\n{| class=\"wikitable\"\n|-\n!rowspan=\"2\" style=\"width:8em\" | Intersects\n| <math>\\Bigl[\\begin{smallmatrix}\n\\mathrm{T}&\\mathrm{*}&\\mathrm{*}\\\\\n\\mathrm{*}&\\mathrm{*}&\\mathrm{*}\\\\\n\\mathrm{*}&\\mathrm{*}&\\mathrm{*}\n\\end{smallmatrix}\\Bigr]</math>\n| <math>\\Bigl[\\begin{smallmatrix}\n\\mathrm{*}&\\mathrm{T}&\\mathrm{*}\\\\\n\\mathrm{*}&\\mathrm{*}&\\mathrm{*}\\\\\n\\mathrm{*}&\\mathrm{*}&\\mathrm{*}\n\\end{smallmatrix}\\Bigr]</math>\n| <math>\\Bigl[\\begin{smallmatrix}\n\\mathrm{*}&\\mathrm{*}&\\mathrm{*}\\\\\n\\mathrm{T}&\\mathrm{*}&\\mathrm{*}\\\\\n\\mathrm{*}&\\mathrm{*}&\\mathrm{*}\n\\end{smallmatrix}\\Bigr]</math>\n| <math>\\Bigl[\\begin{smallmatrix}\n\\mathrm{*}&\\mathrm{*}&\\mathrm{*}\\\\\n\\mathrm{*}&\\mathrm{T}&\\mathrm{*}\\\\\n\\mathrm{*}&\\mathrm{*}&\\mathrm{*}\n\\end{smallmatrix}\\Bigr]</math>\n|rowspan=\"2\"|''a'' [[Intersection (set theory)|intersects]] ''b'': geometries ''a'' and ''b'' have at least one point in common.\n|rowspan=\"2\"|''not Disjoint''\n|-\n| <code>T********</code>\n| <code>*T*******</code>\n| <code>***T*****</code>\n| <code>****T****</code>\n|-\n!rowspan=\"2\"| Within <br>(inside)\n|colspan=\"4\"| <math>\\Bigl[\\begin{smallmatrix}\n\\mathrm{T}&\\mathrm{*}&\\mathrm{F}\\\\\n\\mathrm{*}&\\mathrm{*}&\\mathrm{F}\\\\\n\\mathrm{*}&\\mathrm{*}&\\mathrm{*}\n\\end{smallmatrix}\\Bigr]</math>\n|rowspan=\"2\"|''a'' is within ''b'': ''a'' lies in the interior of ''b''.\n|rowspan=\"2\"|''Contains''(''b'',''a'')\n|-\n|colspan=\"4\"| <code>T*F**F***</code>\n|-\n!rowspan=\"2\"| CoveredBy\n| <math>\\Bigl[\\begin{smallmatrix}\n\\mathrm{T}&\\mathrm{*}&\\mathrm{F}\\\\\n\\mathrm{*}&\\mathrm{*}&\\mathrm{F}\\\\\n\\mathrm{*}&\\mathrm{*}&\\mathrm{*}\n\\end{smallmatrix}\\Bigr]</math>\n| <math>\\Bigl[\\begin{smallmatrix}\n\\mathrm{*}&\\mathrm{T}&\\mathrm{F}\\\\\n\\mathrm{*}&\\mathrm{*}&\\mathrm{F}\\\\\n\\mathrm{*}&\\mathrm{*}&\\mathrm{*}\n\\end{smallmatrix}\\Bigr]</math>\n| <math>\\Bigl[\\begin{smallmatrix}\n\\mathrm{*}&\\mathrm{*}&\\mathrm{F}\\\\\n\\mathrm{T}&\\mathrm{*}&\\mathrm{F}\\\\\n\\mathrm{*}&\\mathrm{*}&\\mathrm{*}\n\\end{smallmatrix}\\Bigr]</math>\n| <math>\\Bigl[\\begin{smallmatrix}\n\\mathrm{*}&\\mathrm{*}&\\mathrm{F}\\\\\n\\mathrm{*}&\\mathrm{T}&\\mathrm{F}\\\\\n\\mathrm{*}&\\mathrm{*}&\\mathrm{*}\n\\end{smallmatrix}\\Bigr]</math>\n|rowspan=\"2\"|''a'' is covered by ''b'' (extends ''Within''): geometry ''a'' lies in ''b''. Other definitions: \"At least one point of ''a'' lies in ''b'', and no point of ''a'' lies in the exterior of ''b''\", or \"Every point of ''a'' is a point of (the interior or boundary of) ''b''\".\n|rowspan=\"2\"|''Covers''(''b'',''a'')\n|-\n| <code>T*F**F***</code>\n| <code>*TF**F***</code>\n| <code>**FT*F***</code>\n| <code>**F*TF***</code>\n|}\n\nPredicates that utilize the input dimensions, and are defined with masks of domain {{mset|1={{mono|0}},{{mono|1}},{{mono|T}},{{mono|F}},{{mono|*}} }}\n{| class=\"wikitable\"\n|-\n!rowspan=\"2\" style=\"width:8em\" | '''Crosses'''<span style=\"font-weight:normal\"><br>{{tmath|1=\\dim(a)\\ne\\dim(b)}} or <br>{{math|1=dim(any) = 1}}</span>\n| <math>\\Bigl[\\begin{smallmatrix}\n\\mathrm{T}&\\mathrm{*}&\\mathrm{T}\\\\\n\\mathrm{*}&\\mathrm{*}&\\mathrm{*}\\\\\n\\mathrm{*}&\\mathrm{*}&\\mathrm{*}\n\\end{smallmatrix}\\Bigr]</math>\n| <math>\\Bigl[\\begin{smallmatrix}\n\\mathrm{T}&\\mathrm{*}&\\mathrm{*}\\\\\n\\mathrm{*}&\\mathrm{*}&\\mathrm{*}\\\\\n\\mathrm{T}&\\mathrm{*}&\\mathrm{*}\n\\end{smallmatrix}\\Bigr]</math>\n|colspan=\"2\"| <math>\\Bigl[\\begin{smallmatrix}\n\\mathrm{0}&\\mathrm{*}&\\mathrm{*}\\\\\n\\mathrm{*}&\\mathrm{*}&\\mathrm{*}\\\\\n\\mathrm{*}&\\mathrm{*}&\\mathrm{*}\n\\end{smallmatrix}\\Bigr]</math>\n|rowspan=\"2\" colspan=\"2\"|''a'' crosses ''b'': they have some but not all interior points in common, and the dimension of the intersection is less than that of at least one of them. Mask selection rules are checked only when {{tmath|\\dim(a)\\ne\\dim(b)}}, except by line/line inputs, otherwise is false:<ref name=\"ST_Crosses\">[http://postgis.org/docs/ST_Crosses.html ST_Crosses]</ref>\n{{NumBlk|:|{{math|(''II''{{=}}0)}} for lines, &nbsp; {{math|(''II'' ∧ ''IE'')}} when {{tmath|\\dim(a)<\\dim(b)}}, &nbsp;  {{math|(''II'' ∧ ''EI'')}} when {{tmath|\\dim(a)>\\dim(b)}}|{{EquationRef|10}}}}\n|-\n| <code>T*T******</code> <small><br>{{tmath|\\dim(a)<\\dim(b)}}</small>\n| <code>T*****T**</code> <small><br>{{tmath|\\dim(a)>\\dim(b)}}</small>\n|colspan=\"2\"| <code>0********</code> <small><br>{{math|1=dim(any) = 1}}</small>\n|-\n!rowspan=\"2\"| Overlaps<span style=\"font-weight:normal\"><br>{{tmath|1=\\dim(a)=\\dim(b)}}</span>\n| <math>\\Bigl[\\begin{smallmatrix}\n\\mathrm{T}&\\mathrm{*}&\\mathrm{T}\\\\\n\\mathrm{*}&\\mathrm{*}&\\mathrm{*}\\\\\n\\mathrm{T}&\\mathrm{*}&\\mathrm{*}\n\\end{smallmatrix}\\Bigr]</math>\n|colspan=\"3\"| <math>\\Bigl[\\begin{smallmatrix}\n\\mathrm{1}&\\mathrm{*}&\\mathrm{T}\\\\\n\\mathrm{*}&\\mathrm{*}&\\mathrm{*}\\\\\n\\mathrm{T}&\\mathrm{*}&\\mathrm{*}\n\\end{smallmatrix}\\Bigr]</math>\n|rowspan=\"2\" colspan=\"2\"|''a'' overlaps ''b'': they have some but not all points in common, they have the same dimension, and the intersection of the interiors of the two geometries has the same dimension as the geometries themselves. Mask selection rules are checked only when {{tmath|1=\\dim(a)=\\dim(b)}}, otherwise is false:\n{{NumBlk|:|{{math|(''II'' ∧ ''IE'' ∧ ''EI'')}} for points or surfaces, &nbsp; {{math|(''II''{{=}}1 ∧ ''IE'' ∧ ''EI'')}} for lines|{{EquationRef|11}}}}\n|-\n| <code>T*T***T**</code> <small><br>{{math|1=dim = 0 or 2}}</small>\n| colspan=\"2\" | <code>1*T***T**</code> <small><br>{{math|1=dim = 1}}</small>\n|}\n\nNotice that:\n\n* The ''topologically equal'' definition does not imply that they have the same points or even that they are of the same class.\n* The output of {{tmath|\\operatorname{DE-9IM}(a,b)}} have the information contained in a list of all interpretable predicates about geometries ''a'' and ''b''.\n* All predicates are computed by masks. Only ''Crosses'' and ''Overlaps'' have additional conditions about {{tmath|\\dim(a)}} and {{tmath|\\dim(b)}}.<!--\n** ''Overlaps'': ''da=db''. If the geometries have different dimensions (ex. surface and line), the predicate is false.\n** ''Crosses'':  ''(da,db)'' is in {(0,1); (0,2); (1,2); (1,0); (1,2); (2,1); (1,1)}, for any other combination of dimensions the predicate is false.<ref name=\"ST_Crosses\" />  There are also mask selection rules: <code>T*T******</code> for {(0,1); (0,2); (1,2)} situations, <code>T*****T**</code> for {(1,0); (1,2); (2,1)} situations, and <code>0********</code> for (1,1) situations.\n-->\n\n* All mask string codes end with <code>*</code>.  This is because ''EE'' is trivially true, and thus provides no useful information<!--, it is the opposite of ''II''? -->.\n* The ''Equals'' mask, <code>T*F**FFF*</code>, is the \"merge\" of ''Contains'' (<code>T*****FF*</code>) and ''Within'' (<code>T*F**F***</code>): {{math|(''II'' ∧ ~''EI'' ∧ ~''EB'') ∧ (''II'' ∧ ~''IE'' ∧ ~''BE'')}}.\n*  There are no mask for situations involving complex types, like a Point/Multipoint situation. Example: with the above definition the code <code>0FFFFF0F2</code> have the ''Crosses'' predicate (satisfies the mask <code>T*****T**</code>), but by a more rigorous definition, like the ''JTS'' definition, not.<ref name=\"test1case4\">JTS test case of \"point A within one of B points\", http://www.vividsolutions.com/jts/tests/Run1Case4.html</ref>\n* The mask <code>T*****FF*</code> occurs in the definition of both ''Contains'' and ''Covers''.  ''Covers'' is a more inclusive relation. In particular, unlike ''Contains'' it does not distinguish between points in the boundary and in the interior of geometries. For most situations, ''Covers'' should be used in preference to ''Contains''.\n* Similarly, the mask <code>T*F**F***</code> occurs in the definition of both ''Within'' and ''CoveredBy''.  For most situations, ''CoveredBy'' should be used in preference to ''Within''.\n\n=== Properties ===\nThe spatial predicates have the following properties of [[Binary relation|binary relations]]:\n\n* [[Reflexive_relation|Reflexive]]: Equals, Contains, Covers, CoveredBy, Intersects, Within \n* [[Reflexive_relation|Anti-reflexive]]: Disjoint\n* [[Symmetric_relation|Symmetric]]: Equals, Intersects, Crosses, Touches, Overlaps\n* [[Transitive_relation|Transitive]]: Equals, Contains, Covers, CoveredBy, Within\n \n=== Interpretation ===\n[[File:TopologicSpatialRelarions2.png|thumb|400px|Examples of spatial relations.]]\nThe choice of terminology and semantics for the spatial predicates is based on reasonable conventions and the tradition of topological studies.<ref name=\"sdh1990\"/>\nRelationships such as ''Intersects'', ''Disjoint'', ''Touches'', ''Within'', ''Equals''  (between two geometries ''a'' and ''b'')  have an obvious semantic:<ref name=\"davis2007\"/><ref name=\"cafrca\">{{cite journal | first1 = G. | last1 = Câmara | first2 = U. M. | last2 = Freitas | first3 = M. A. | last3 = Casanova | year = 1995 | title = Fields and Objects Algebras for GIS Operations | citeseerx = 10.1.1.17.991 }}</ref>\n; ''Equals'':   ''a'' = ''b''  that is  (''a'' ∩ ''b'' = ''a'') ∧ (''a'' ∩ ''b'' = ''b'')\n; ''Within'':   ''a'' ∩ ''b'' = ''a''\n; ''Intersects'':   ''a'' ∩ ''b'' ≠ ∅\n; ''Touches'': (''a'' ∩ ''b'' ≠ ∅) ∧ (''a''<sup>ο</sup> ∩ ''b''<sup>ο</sup> = ∅)\n\nThe predicates ''Contains'' and ''Within''  have subtle aspects to their definition which are contrary to intuition.\nFor example,<ref name=\"davis2007\"/>  a line ''L'' which is completely contained in the boundary of a polygon ''P'' is ''not'' considered to be contained in ''P''.   This quirk can be expressed as \"Polygons do not contain their boundary\". This issue is caused by the final clause of the ''Contains'' definition above: \"at least one point of the interior of B lies in the interior of A\".  For this case, the predicate ''Covers''  has more intuitive semantics (see definition), avoiding boundary considerations.\n\nFor better understanding, the dimensionality of inputs can be used as justification for a gradual introduction of semantic complexity:\n:{|class=\"wikitable\"\n|-\n!Relations between\n!Appropriate predicates\n!Semantic added\n|-\n|point/point\n|''Equals'', ''Disjoint''\n|Other valid predicates collapses into ''Equals''.\n|-\n|point/line\n|adds ''Intersects''\n|''Intersects'' is a refinement of ''Equals'': \"some equal point at the line\".\n|-\n|line/line\n|width=\"180\"|adds ''Touches'', ''Crosses'', ...\n|''Touches'' is a refinement of ''Intersects'', about \"boundaries\" only. ''Crosses'' is about \"only one point\".\n|}\n\n=== Coverage on possible matrix results ===\nThe number of possible results in a boolean ''9IM'' matrix is 2<sup>9</sup>=512, and in a '''DE-9IM''' matrix is 3<sup>9</sup>=6561. The probability of one of these results come to satisfy a specific predicate is determined as following,\n{{aligned table|class=wikitable|cols=2|row1header=y|col1style=text-align:right;\n|Probability|Name\n| 93.7% |''Intersects''\n| 43.8% |''Touches''\n| 25% |''Crosses'' (for valid inputs, 0% otherwise)\n| 23.4% |''Covers'' and ''CoveredBy''\n| 12.5% |''Contains'', ''Overlaps'' (for valid inputs, 0% otherwise) and ''Within''\n| 6.3%   |''Disjoint''\n| 3.1%   |''Equals''\n}}\nOn usual applications the geometries intersects ''a priori'', and the other relations are checked.\n\nThe composite predicates \"''Intersects'' OR ''Disjoint''\" and \"''Equals'' OR ''Different''\"  have the sum 100% (always true predicates),\nbut \"''Covers'' OR ''CoveredBy''\" have 41%, that is not the sum, because they are not logical complements neither independent relations; idem \"''Contains'' OR ''Within''\", that have 21%. The sum 25%+12.5%=37.5% is obtained when ignoring overlapping of lines in \"''Crosses''  OR ''Overlaps''\", because the valid input sets are disjoints.\n\n== Queries and assertions ==\nThe ''DE-9IM'' offers a full descriptive assertion about the two input geometries. It is a mathematical function that represents a [[Functional completeness|complete set]] of all possible relations about two entities, like a [[Truth table]], the [[Three-way comparison]], a [[Karnaugh map#2-variable map examples|Karnaugh map]] or a [[Venn diagram]]. Each output value is like a truth table line, that represent relations of specific inputs.\n\nAs illustrated above, the output '212101212' resulted from ''DE-9IM''(''a'',''b'') is a complete description of all topologic relations between specific geometries ''a'' and ''b''. It say to us that {{tmath|1=II=2,\\,IB=1,\\,IE=2,\\,BI=1,\\,BB=0,\\,BE=1,\\,EI=2,\\,EB=1,\\,EE=2}}.\n\nBy other hand, if we check predicates like ''Intersects''(''a'',''b'') or  ''Touches''(''a'',''b'') — for the same example we have \"''Intersects''={{mono|true}} and ''Touches''={{mono|true}}\" — it is an incomplete description of \"all topologic relations\".\nPredicates also do not say any thing about the dimensionality of the geometries (it doesn't matter if ''a'' and ''b'' are lines, areas or points).\n\nThis independence of geometry-type and the lack of [[Completeness (logic)|completeness]], on ''predicates'', are useful for [[Query language|general queries]] about two geometries:\n:{|border=\"0\" class=\"wikitable\"\n|\n!interior/boundary/exterior semantic\n!usual semantic\n|-\n!Assertions\n|style=\"background-color:#DDC\" align=\"center\"|'''more descriptive'''<br>  \" ''a'' and ''b'' have {{math|1=DE-9IM(''a'',''b'')='212101212'}} \"\n|style=\"background-color:#DDC\" align=\"center\"|'''less descriptive'''<br> \" ''a Touches b'' \"\n|-\n!Queries\n|style=\"background-color:#DDC\" align=\"center\"|'''more restrictive'''<br>\" Show all pair of geometries where {{math|1=DE-9IM(''a'',''b'')='212101212'}} \"\n|style=\"background-color:#DDC\" align=\"center\"|'''more general'''<br>\" Show all pair of geometries where ''Touches''(''a'',''b'') \"\n|}\n\nFor usual applications, the use of ''spatial predicates'' also is justified by being more [[Human-readable medium|human-readable]] than ''DE-9IM'' descriptions: a typical user  have better intuition about predicates (than a set of interiors/border/exterior intersections).\n\nPredicates have useful [[Semantics (computer science)|semantic]] into usual applications, so it is useful the translation of a ''DE-9IM'' description into a list of all associated predicates,<ref>A [https://github.com/ppKrauss/postgis-st-relate-summary ''DE-9IM'' translator], of all associated predicates of a spatial relation.</ref><ref name=\"SDO_RELATE\">Note. The [http://docs.oracle.com/cd/B19306_01/appdev.102/b14255/sdo_operat.htm#i78531 Oracle's spatial funcion {{mono|SDO_RELATE()}}] do only a partial translation, internally, offering to user a mask for a or-list of predicates to be checked, instead the DE-9IM string.</ref> that is like a [[Type conversion|casting process]] between the two different semantic types. Examples:\n\n* The string codes \"{{mono|0F1F00102}}\" and \"{{mono|0F1FF0102}}\" have the semantic of \"''Intersects & Crosses & Overlaps''\".\n* The string code \"{{mono|1FFF0FFF2}}\" have the semantic of \"''Equals''\".\n* The string codes \"{{mono|F01FF0102}}\",  \"{{mono|FF10F0102}}\", \"{{mono|FF1F00102}}\", \"{{mono|F01FFF102}}\",  and \"{{mono|FF1F0F1F2}}\" have the semantic of \"''Intersects & Touches''\".\n\n=== Standards ===\nThe [[Open Geospatial Consortium]]  (OGC) has standardized the typical spatial predicates (Contains, Crosses, Intersects, Touches, etc.) as boolean functions, and the DE-9IM model,<ref name=\"ogs1\">\"OpenGIS Implementation Specification for Geographic information - Simple feature access - Part 2: SQL option\", [[Open Geospatial Consortium|OGC]], http://www.opengeospatial.org/standards/sfs</ref> as a function that returns a string (the DE-9IM code), with domain of {{mset|1={{mono|0}},{{mono|1}},{{mono|2}},{{mono|F}} }},  meaning {{mono|0}}=point, {{mono|1}}=line, {{mono|2}}=area, and {{mono|F}}=\"empty set\". This DE-9IM string code is a standardized format for data interchange.\n\nThe [[Simple Feature Access]] (ISO 19125) standard,<ref name=\"SFS2007\">\n[[Open Geospatial Consortium|Open Geospatial Consortium Inc.]] (2007), \"OpenGIS® Implementation Standard for Geographic\ninformation - Simple feature access - Part 2: SQL option\", [http://www.opengeospatial.org/standards/sfs OGC document] ''06-104r4'' version 1.2.1 (review of 2010-08-04).</ref> in the chapter 7.2.8, \"SQL routines on type Geometry\", recommends as supported routines the ''SQL/MM Spatial''<ref>ISO 13249-3 Part 3: Spatial, summarized in  [http://www.sigmod.org/record/issues/0112/standards.pdf SQL Multimedia and Application Packages (SQL/MM)].</ref> (ISO 13249-3 Part 3: Spatial)  ''ST_Dimension'', ''ST_GeometryType'', ''ST_IsEmpty'', ''ST_IsSimple'', ''ST_Boundary'' for all Geometry Types.\nThe same standard, consistent with the definitions of relations in \"Part 1, Clause 6.1.2.3\"\nof the SQL/MM, recommends (shall be supported) the function labels: ''ST_Equals'', ''ST_Disjoint'', ''ST_Intersects'', ''ST_Touches'', ''ST_Crosses'', ''ST_Within'', ''ST_Contains'', ''ST_Overlaps'' and ''ST_Relate''.\n\nThe DE-9IM in the OGC standards use the following definitions of Interior and Boundary, for the main OGC standard geometry types:<ref>\"Encyclopedia of GIS\", edited by Shashi Shekhar and Hui Xiong. SpringerScience 2008. pg. 242</ref>\n{|class=\"wikitable\" align=\"center\"\n!Subtypes\n!Dim\n!Interior ({{mvar|I}})\n!boundary ({{mvar|B}})\n|-\n|Point, MultiPoint\n|0\n|Point, Points\n|Empty\n|-\n|LineString, Line\n|1\n|Points that are left when the boundary points are removed.\n|Two end points.\n|-\n|LinearRing\n|1\n|All points along the geometry.\n|Empty.\n|-\n|MultilineString\n|1\n|Points that are left when the boundary points are removed.\n|Those points that are in the boundaries of an odd number of its elements (curves).\n|-\n|Polygon\n|2\n|Points within the rings.\n|Set of rings.\n|-\n|MultiPolygon\n|2\n|Points within the rings.\n|Set of rings of its elements (polygons).\n|-\n|colspan=\"4\"|NOTICE: '''exterior points (E)''' are points ''p'' not in the ''interior'' or ''boundary'', so not need extra interpretation,  {{mono|E(p){{equals}}not(I(p) or B(p))}}.\n|}\n\n=== Implementation and practical use ===\nMost spatial databases, such as [[PostGIS]], implements the ''DE-9IM()'' model by the standard functions:<ref>\nST_Relate() [[PostGIS]] function  [http://www.postgis.org/docs/ST_Relate.html online documentation].</ref> <code>ST_Relate</code>, <code>ST_Equals</code>, <code>ST_Intersects</code>, etc. The function <code>ST_Relate(a,b)</code> outputs the standard OGC's ''DE-9IM string code''.\n\nExamples: two geometries, ''a'' and ''b'', that intersects and touches with a point (for instance with {{tmath|1=\\dim(B(a)\\cap I(b))=0}} and {{tmath|1=\\dim(I(a)\\cap I(b))=F}}), can be <code>ST_Relate(a,b)='FF1F0F1F2'</code> or <code>ST_Relate(a,b)='FF10F0102'</code> or <code>ST_Relate(a,b)='FF1F0F1F2'</code>. It also satisfies <code>ST_Intersects(a,b)=true</code> and <code>ST_Touches(a,b)=true</code>.\nWhen <code>ST_Relate(a,b)='0FFFFF212'</code>, the returned DE-9IM code have the semantic of \"Intersects(a,b) & Crosses(a,b) & Within(a,b) & CoveredBy(a,b)\", that is, returns <code>true</code> on the boolean expression <code>ST_Intersects(a,b) AND ST_Crosses(a,b) AND ST_Within(a,b) AND  ST_Coveredby(a,b)</code>.\n\nThe use of {{mono|ST_Relate()}} is faster than direct computing of a set of correspondent predicates.<ref name=\"PostGISch4\">[http://www.postgis.org/docs/using_postgis_dbmanagement.html#DE-9IM Chapter 4. Using PostGIS: Data Management and Queries]</ref> There are cases where using {{mono|ST_Relate()}} is the only way to compute a complex predicate — see the example of the code <code>0FFFFF0F2</code>,<ref name=\"test1case4\"/> of a point that not \"crosses\" a multipoint (a object that is a set of points), but predicate ''Crosses'' (when defined by a mask) returns ''true''.\n\nIt is usual to [[Function overloading|overload]] the {{mono|ST_Relate()}} by adding a mask parameter,\nor use a returned {{mono|ST_Relate(a,b)}} string into the {{mono|ST_RelateMatch()}} function.<ref>\nST_RelateMatch() [[PostGIS]] function  [http://www.postgis.org/docs/ST_RelateMatch.html online documentation].</ref>\nWhen using {{mono|ST_Relate(a,b,mask)}}, it returns a boolean. Examples:\n* <code>ST_Relate(a,b,'*FF*FF212')</code> returns ''true'' when <code>ST_Relate(a,b)</code> is <code>0FFFFF212</code> or <code>01FFFF212</code>, and returns ''false''  when <code>01FFFF122</code> or <code>0FF1FFFFF</code>.\n* <code>ST_RelateMatch('0FFFFF212','*FF*FF212')</code> and <code>ST_RelateMatch('01FFFF212','TTF*FF212')</code> are ''true'',  <code>ST_RelateMatch('01FFFF122','*FF*FF212')</code> is ''false''.\n\n== Synonyms  ==\n* \"Egenhofer-Matrix\" is a synonym for the ''9IM'' 3x3 matrix of boolean domain.<ref name=\"encygis\">\"Encyclopedia of GIS\", S. Shekhar, H. Xiong. {{ISBN|978-0-387-35975-5}}.</ref>\n* \"Clementini-Matrix\" is a synonym for the [[#Matrix_model|DE-9IM]] 3x3 matrix of {{mset|1={{mono|0}},{{mono|1}},{{mono|2}},{{mono|F}} }} domain.<ref name=\"encygis\"/>\n* \"Egenhofer operators\" and \"Clementini operators\" are sometimes a reference to matrix elements as ''II'', ''IE'', etc. that can be used in boolean operations. Example: the predicate \"''G<sub>1</sub>'' contains ''G<sub>2</sub>''\"  can be expressed by \"{{math|{{angbr|''G<sub>1</sub>''{{!}} II ∧ ~EI ∧ ~EB {{!}}''G<sub>1</sub>''}}}}\", that can be translated to mask syntax, <code>T*****FF*</code>.\n* [[#Spatial_predicates|Predicate]]s \"meets\" is a synonym for ''touches''; \"inside\" is a synonym for ''within''\n* Oracle's<ref name=\"SDO_RELATE\"/> \"ANYINTERACT\" is  a synonym for ''intersects'' and  \"OVERLAPBDYINTERSECT\" is  a synonym for ''overlaps''.  Its  \"OVERLAPBDYDISJOINT\" does not have a corresponding named predicate.\n* In [[Region connection calculus]]  operators offer some synonyms for [[#Spatial_predicates|predicate]]s: ''disjoint'' is DC (disconnected), ''touches'' is EC (externally connected), ''equals'' is EQ. Other, like ''Overlaps'' as PO (partially overlapping),  need context analysis or composition.<ref>\"Multidimensional Region Connection Calculus\" (2017), http://qrg.northwestern.edu/qr2017/papers/QR2017_paper_8.pdf</ref><ref>\"Identification of Relations in Region Connection Calculus: 9-Intersection Reduced to 3+ -Intersection Predicates\" (2013), https://pdfs.semanticscholar.org/8184/abc9b25ed340f9195cc904249bda415bb0c3.pdf</ref>\n\n== See also ==\n{|\n|-\n|valign=\"top\"|Standards:\n* [[Simple feature access]] (ISO 19125)\n* [[Open Geospatial Consortium]]\n* [[GeoSPARQL]]  (related to)\n|&nbsp; &nbsp; &nbsp;\n|valign=\"top\"|Software:\n* [[JTS Topology Suite]]\n* [[PostGIS]]\n* [[Spatial database]]\n|&nbsp; &nbsp; &nbsp;\n|valign=\"top\"|Related topics\n* [[Geospatial topology]]\n* '''[[Spatial relation]]'''\n* [[Spatial analysis]]\n* [[Karnaugh map#2-variable map examples|Karnaugh (2-variable) map]]\n* [[Relational operator]]\n|}\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* [http://docs.geotools.org/latest/userguide/library/jts/dim9.html Point Set Theory and the DE-9IM Matrix]\n* [http://edndoc.esri.com/arcsde/9.1/general_topics/understand_spatial_relations.htm Illustrated Tutorial for DE-9IM]\n\n[[Category:Matrices]]\n[[Category:Geometric topology]]\n[[Category:Geographic data and information]]\n[[Category:Binary operations]]"
    },
    {
      "title": "Dirichlet convolution",
      "url": "https://en.wikipedia.org/wiki/Dirichlet_convolution",
      "text": "{{More footnotes|date=June 2018}}\n\nIn [[mathematics]], the '''Dirichlet convolution''' is a [[binary operation]] defined for [[arithmetic function]]s; it is important in [[number theory]]. It was developed by [[Peter Gustav Lejeune Dirichlet]].\n\n==Definition==\nIf <math>f , g : \\mathbb{N}\\to\\mathbb{C}</math> are two arithmetic functions from the positive [[integer]]s to the [[complex number]]s, the ''Dirichlet [[convolution]]'' {{nowrap|''f'' ∗ ''g''}} is a new arithmetic function defined by:\n\n:<math>\n(f*g)(n) \\ =\\ \\sum_{d\\,\\mid \\,n} f(d)\\,g\\!\\left(\\frac{n}{d}\\right) \\ =\\  \\sum_{ab\\,=\\,n}\\!f(a)\\,g(b)\n</math>\n\nwhere the sum extends over all positive [[divisor]]s ''d'' of&nbsp;''n'', or equivalently over all distinct pairs {{nowrap|(''a'', ''b'')}} of positive integers whose product is ''n''.\n\nThis product occurs naturally in the study of [[Dirichlet series]] such as the [[Riemann zeta function]]. It describes the multiplication of two Dirichlet series in terms of their coefficients:\n:<math>\\left(\\sum_{n\\geq 1}\\frac{f(n)}{n^s}\\right)\n\\left(\\sum_{n\\geq 1}\\frac{g(n)}{n^s}\\right)\n\\ = \\ \n\\left(\\sum_{n\\geq 1}\\frac{(f*g)(n)}{n^s}\\right).\n</math>\n\n==Properties==\n\nThe set of arithmetic functions forms a [[commutative ring]], the '''{{visible anchor|Dirichlet ring}}''', under [[pointwise addition]], where {{nowrap|''f'' + ''g''}} is defined by {{nowrap|1=(''f'' + ''g'')(''n'') = ''f''(''n'') + ''g''(''n'')}}, and Dirichlet convolution. The multiplicative identity is the [[unit function]] ''ε'' defined by {{nowrap|1=''ε''(''n'') = 1}} if {{nowrap|1=''n'' = 1}} and {{nowrap|1=''ε''(''n'') = 0}} if {{nowrap|''n'' > 1}}. The [[unit (ring theory)|unit]]s (invertible elements) of this ring are the arithmetic functions ''f'' with {{nowrap|''f''(1) ≠ 0}}.\n\nSpecifically, Dirichlet convolution is<ref>Proofs of all these facts are in Chan, ch. 2</ref> [[associativity|associative]],\n:<math>(f * g) * h = f * (g * h),</math>\n[[distributivity|distributes]] over addition\n:<math>f * (g + h) = f * g + f * h</math>,\nis [[commutativity|commutative]],\n:<math>f * g = g * f</math>,\nand has an identity element,\n: <math>f * \\varepsilon</math> = <math>\\varepsilon * f = f</math>.\nFurthermore, for each <math>f</math> having <math>f(1) \\neq 0</math>, there exists an arithmetic function <math>f^{-1}</math> with <math>f * f^{-1} = \\varepsilon</math>, called the '''{{visible anchor|Dirichlet inverse}}''' of <math>f</math>.\n\nThe Dirichlet convolution of two [[multiplicative function]]s is again multiplicative, and every multiplicative function has a Dirichlet inverse which is also multiplicative. The article on multiplicative functions lists several convolution relations among important multiplicative functions.\n\nAnother operation on arithmetic functions is pointwise multiplication:  {{nowrap|''fg''}} is defined by {{nowrap|1=(''fg'')(''n'') = ''f''(''n'') ''g''(''n'')}}. Given a [[completely multiplicative function]] <math>h</math>, pointwise multiplication by <math>h</math>distributes over Dirichlet convolution: <math>(f * g)h = (fh) * (gh)</math>.<ref>A proof is in the article [[Completely multiplicative function#Proof of distributive property]].</ref> The convolution of two completely multiplicative functions is multiplicative, but not  necessarily completely multiplicative.\n\n==Examples==\n\nIn these formulas, we use the following [[arithmetical function]]s:\n* <math>\\varepsilon</math> is the multiplicative identity: <math>\\varepsilon(1) = 1</math>, otherwise 0.\n* <math>1</math> is the constant function with value 1:  <math>1(n) = 1</math> for all <math>n</math>. Keep in mind that <math>1</math> is not the identity. (Some authors [[Incidence algebra#Special_elements|denote this]] as <math>\\zeta</math> because the associated Dirichlet series is the [[Riemann zeta function]].)\n* <math>1_C</math> for <math>C \\subset \\mathbb{Z}</math> is a set [[indicator function]]:  <math>1_C(n) = 1</math> iff <math>n \\in C</math>, otherwise 0.\n*<math>\\text{Id}</math> is the identity function with value ''n'':  <math>\\text{Id}(n) = n</math>.\n*<math>\\text{Id}_k</math>is the ''k''th power function:  <math>\\text{Id}_k(n)=n^k</math>.\n\nThe following relations hold:\n\n* <math>1 * \\mu = \\varepsilon</math>,&nbsp;the Dirichlet inverse of the constant function <math>1</math> is the [[Möbius function]]. Hence:\n*<math>g = f * 1</math>  if and only if  <math>f = g * \\mu</math>, the [[Möbius inversion formula]]\n*<math>\\sigma_k = \\text{Id}_k * 1</math>, the [[divisor function|kth-power-of-divisors sum function]] ''σ''<sub>''k''</sub>\n*<math>\\sigma = \\text{Id} * 1</math>, the sum-of-divisors function {{nowrap|1=''σ'' = ''σ''<sub>1</sub>}}\n*<math>d = 1 * 1</math> , the number-of-divisors function {{nowrap|1=''d''(''n'') = ''σ''<sub>0</sub>}}\n*<math>\\text{Id}_k = \\sigma_k * \\mu</math>,&nbsp; by Möbius inversion of the formulas for ''σ''<sub>''k''</sub>, ''σ'', and ''d''\n*<math>\\text{Id} = \\sigma * \\mu</math>\n*<math>1 = d * \\mu</math>\n*<math>\\phi * 1 = \\text{Id}</math> , proved under [[Euler's totient function#Divisor sum|Euler's totient function]]\n*<math>\\phi = \\text{Id} * \\mu</math> , by Möbius inversion\n*<math>\\sigma = \\phi * d</math> &nbsp;, from convolving 1 on both sides of <math>\\phi * 1 = \\text{Id}</math>\n*<math>\\lambda * |\\mu| = \\epsilon</math> &nbsp;where ''λ'' is [[Liouville's function]]\n*<math>\\lambda * 1 = 1_{\\text{Sq}}</math>&nbsp;where Sq = {1, 4, 9, ...} is the set of squares\n*<math>\\text{Id}_k * (\\text{Id}_k \\mu) = \\epsilon </math>\n*<math>d^3 * 1 = (d * 1)^2</math>\n*<math>J_k * 1 = \\text{Id}_k</math>,  [[Jordan's totient function]]\n*<math>(\\text{Id}_s J_r) * J_s = J_{s + r} </math>\n*<math>\\Lambda * 1 = \\log</math>, where <math>\\Lambda</math> is [[von Mangoldt function|von Mangoldt's function]]\n* <math>|\\mu| \\ast 1 = 2^{\\omega},</math> where <math>\\omega(n)</math> is the [[prime omega function]] counting ''distinct'' prime factors of ''n''\n<!-- * ''μ'' ∗ 1 = ''ε'' ∗ (''μ'' ∗ Id<sub>''k''</sub>) ∗ Id<sub>''k''</sub> = ''ε'' (generalized Möbius inversion) -->\n\n==Dirichlet inverse==\n\nGiven an arithmetic function <math>f</math> its Dirichlet inverse <math>g = f^{-1} </math> may be calculated recursively: the value of <math>g(n) </math> is in terms of <math>g(m)</math> for <math>m<n</math>.\n\nFor <math>n=1</math>:\n:<math>(f * g) (1) = f(1) g(1) = \\epsilon(1) = 1 </math>,  so\n:<math>g(1) = 1/f(1)</math>. This implies that <math>f</math> does not have a Dirichlet inverse if <math>f(1) = 0</math>.\n\nFor <math>n=2</math>:\n:<math>(f * g) (2) = f(1) g(2) + f(2) g(1) = \\epsilon(2) = 0</math>,\n:<math>g(2) = -1/f(1) (f(2) g(1))</math>,\n\nFor <math>n=3</math>:\n: <math>(f * g) (3) = f(1) g(3) + f(3) g(1) = \\epsilon(3) = 0</math>,\n: <math>g(3) = -1/f(1) (f(3) g(1))</math>,\n\nFor <math>n=4</math>:\n: <math>(f * g) (4) = f(1) g(4) + f(2) g(2) + f(4) g(1) = \\epsilon(4) = 0</math>,\n: <math>g(4) = -1/f(1) (f(4) g(1) + f(2) g(2))</math>,\n\nand in general for <math>n>1</math>,\n\n:<math>\ng(n) \\ =\\ \n\\frac {-1}{f(1)} \\mathop{\\sum_{d\\,\\mid \\,n}}_{d < n}\nf\\left(\\frac{n}{d}\\right) g(d).\n</math>\n\nSince the only division is by ''f''(1), this shows ''f'' has a Dirichlet inverse if and only if {{nowrap|''f''(1) ≠ 0}}. An exact, non-recursive formula for the Dirichlet inverse of any [[arithmetic function]] ''f'' is given in [[Divisor sum identities#The Dirichlet inverse of an arithmetic function|Divisor sum identities]].\n\n{| class=\"wikitable\" border=\"1\"\n! Arithmetic function !! Dirichlet inverse:<ref>See Apostol Chapter 2.</ref>\n|-\n| Constant function with value 1 ||[[Möbius function]] ''μ''\n|-\n| <math>n^{\\alpha}</math> || <math>\\mu(n) \\,n^\\alpha</math>\n|-\n| [[Liouville's function]] ''λ'' || Absolute value of Möbius function {{abs|''μ''}}\n|-\n| [[Euler's totient function]] <math>\\varphi</math> ||<math>\\sum_{d|n} d\\, \\mu(d)</math>\n|-\n| The [[sum of divisors|generalized sum-of-divisors function]] <math>\\sigma_{\\alpha}</math> || <math>\\sum_{d|n} d^{\\alpha} \\mu(d) \\mu\\left(\\frac{n}{d}\\right)</math> \n|}\n\nThe following properties of the Dirichlet inverse hold:<ref>Again see Apostol Chapter 2 and the exercises at the end of the chapter.</ref>\n\n* The Dirichlet inverse of a [[multiplicative function]] is again multiplicative. \n* The Dirichlet inverse of a Dirichlet convolution is the convolution of the inverses of each function: <math>(f \\ast g)^{-1} = f^{-1} \\ast g^{-1}</math>. \n* A multiplicative function ''f'' is [[completely multiplicative]] if and only if <math>f^{-1}(n) = \\mu(n) f(n)</math>.\n* If ''f'' is [[completely multiplicative]] then <math>(f \\cdot g)^{-1} = f \\cdot g^{-1}</math> whenever <math>g(1) \\neq 0</math> and where <math>\\cdot</math> denotes pointwise multiplication of functions.\n\n==Dirichlet series==\nIf ''f'' is an arithmetic function, one defines its [[Dirichlet series]] [[generating function]] by\n\n:<math>\nDG(f;s) = \\sum_{n=1}^\\infty \\frac{f(n)}{n^s}\n</math>\n\nfor those [[complex number|complex]] arguments ''s'' for which the series converges (if there are any). The multiplication of Dirichlet series is compatible with Dirichlet convolution in the following sense:\n\n:<math>\nDG(f;s) DG(g;s) = DG(f*g;s)\\,\n</math>\n\nfor all ''s'' for which both series of the left hand side converge, one of them at least converging \nabsolutely (note that simple convergence of both series of the left hand side DOES NOT imply convergence of the right hand side!). This is akin to the [[convolution theorem]] if one thinks of Dirichlet series as a [[Fourier transform]].\n\n==Related concepts==\n{{expand section|date=December 2013}}\nThe restriction of the divisors in the convolution to [[Unitary divisor|unitary]], [[Bi-unitary divisor|bi-unitary]] or infinitary divisors defines similar commutative operations which share many features with the Dirichlet convolution (existence of a Möbius inversion, persistence of multiplicativity, definitions of totients, Euler-type product formulas over associated primes, etc.).\n\nDirichlet convolution is the convolution of the [[incidence algebra]] for the positive integers ordered by divisibility.\n\n==See also==\n\n* [[Arithmetic function]]\n* [[Divisor sum identities]]\n* [[Möbius inversion formula]]\n\n==References==\n\n{{Reflist}}\n* {{Apostol IANT}}\n* {{cite book |\nauthor=Chan, Heng Huat |\ntitle=Analytic Number Theory for Undergraduates |\nseries=Monographs in Number Theory|\nyear=2009 |\nisbn=981-4271-36-5 |\npublisher=World Scientific Publishing Company \n}}\n* {{cite book | author=Hugh L. Montgomery | authorlink=Hugh Montgomery (mathematician) |author2=Robert C. Vaughan |authorlink2=Robert Charles Vaughan (mathematician)  | title=Multiplicative number theory I. Classical theory | series=Cambridge tracts in advanced mathematics | volume=97 | year=2007 | isbn=0-521-84903-9 | page=38 | publisher=Cambridge Univ. Press | location=Cambridge }}\n* {{Cite news\n|first1=Eckford\n|last1=Cohen\n|title=A class of residue systems (mod r) and related arithmetical functions. I. A generalization of Möbius inversion\n|journal=Pacific J. Math.\n|volume=9\n|number=1\n|pages=13–23\n|year=1959\n|mr=0109806\n}}\n* {{Cite news\n|first1=Eckford\n|last1=Cohen\n|title=Arithmetical functions associated with the unitary divisors of an integer\n|journal=[[Mathematische Zeitschrift]]\n|volume=74\n|year=1960\n|pages=66–80\n|mr=0112861\n|doi=10.1007/BF01180473\n}}\n* {{Cite news\n|first1=Eckford\n|last1=Cohen\n|title=The number of unitary divisors of an integer\n|volume=67\n|number=9\n|pages=879–880\n|mr=0122790\n|year=1960\n|journal=[[American Mathematical Monthly]]\n}}\n* {{Cite news\n|first1=Graeme L.\n|last1=Cohen\n|title=On an integers' infinitary divisors\n|volume=54\n|number=189\n|pages=395–411\n|mr=0993927\n|doi=10.1090/S0025-5718-1990-0993927-5\n|journal=Math. Comp.\n|year=1990\n}}\n* {{Cite news\n|first1=Graeme L.\n|last1=Cohen\n|title=Arithmetic functions associated with infinitary divisors of an integer\n|volume=16\n|number=2\n|pages=373–383\n|doi=10.1155/S0161171293000456\n|journal=Int. J. Math. Math. Sci.\n|year=1993\n}}\n* {{cite journal\n|first1=Jozsef\n|last1=Sandor\n|first2=Antal\n|last2=Berge\n|title=The Möbius function: generalizations and extensions\n|journal=Adv. Stud. Contemp. Math. (Kyungshang)\n|volume=6\n|number=2\n|pages=77–128\n|year=2003\n|mr=1962765\n}}\n* {{cite web\n |first1      = Steven\n |last1       = Finch\n |title       = Unitarism and Infinitarism\n |url         = http://www.people.fas.harvard.edu/~sfinch/csolve/try.pdf\n |year        = 2004\n |deadurl     = yes\n |archiveurl  = https://web.archive.org/web/20150222094526/http://www.people.fas.harvard.edu/~sfinch/csolve/try.pdf\n |archivedate = 2015-02-22\n |df          = \n}}\n\n==External links==\n* {{springer|title=Dirichlet convolution|id=p/d130150}}\n\n[[Category:Arithmetic functions]]\n[[Category:Bilinear operators]]\n[[Category:Binary operations]]\n\n[[de:Zahlentheoretische Funktion#Faltung]]"
    },
    {
      "title": "Division (mathematics)",
      "url": "https://en.wikipedia.org/wiki/Division_%28mathematics%29",
      "text": "{{short description|Arithmetic operation}}\n{{about|the arithmetical operation||Division (disambiguation)}}\n{{redirect|Divided}}\n{{Refimprove|date=October 2014}}\n[[File:Divide20by4.svg|thumb|20 ÷ 4 = 5 with apples. This is said verbally, \"twenty divided by four equals five. ]]\n\n'''Division''' is one of the four basic operations of [[arithmetic]], the others being [[addition]], [[subtraction]], and [[multiplication]]. The [[mathematical symbol]]s used for the division operator are the [[obelus]] (÷), the [[colon]] (:) and the [[slash (punctuation)|slash]] (/).\n\nAt an elementary level the division of two [[natural number]]s is – among other [[Quotition and partition|possible interpretations]] – the process of calculating the number of times one number is contained within another one.<ref>{{cite book|last1=Blake|first1=A. G.|title=Arithmetic|date=1887|publisher=[[Alexander Thom (almanac editor)|Alexander Thom & Company]]|location=[[Dublin|Dublin, Ireland]]}}</ref>{{rp|7}} This number of times is not always an [[integer]], and this led to two different concepts.\n\nThe [[division with remainder]] or [[Euclidean division]] of two [[natural numbers]] provides a ''quotient'', which is the number of times the second one is contained in the first one, and a ''remainder'', which is the part of the first number that remains, when in the course of computing the quotient, no further full chunk of the size of the second number can be allocated.\n\nFor a modification of this division to yield only one single result, the natural numbers must be extended to [[rational number]]s or [[real number]]s. In these enlarged [[number system]]s, division is the inverse operation to multiplication, that is {{math|''a'' {{=}} ''c'' ÷ ''b''}} means {{math|''a'' × ''b'' {{=}} ''c''}}, as long as {{math|''b''}} is not zero. If {{math|1=''b'' = 0}}, then this is a [[division by zero]], which is not defined.{{efn|Division by zero may be defined in some circumstances, either by extending the real numbers to the [[extended real number line]] or to the [[projectively extended real line]] or when occurring as limit of divisions by numbers tending to 0. For example: {{math|lim<sub>''x''→0</sub> {{sfrac|sin ''x''|''x''}} {{=}} 1.}}<ref name=\"mwdiv\" /><ref name=\"db0\">{{MathWorld|id=DivisionbyZero|title=Division by Zero}}</ref>}}<ref>{{cite book|last1=Derbyshire|first1=John|title=Prime Obsession: Bernhard Riemann and the Greatest Unsolved Problem in Mathematics|date=2004|publisher=[[Penguin Books]]|location=[[New York City]]|isbn=978-0-452-28525-5}}</ref>{{rp|246}}\n\nBoth forms of divisions appear in various [[algebraic structure]]s. Those in which a Euclidean division (with remainder) is defined are called [[Euclidean domain]]s and include [[polynomial ring]]s in one [[indeterminate (variable)|indeterminate]]. Those in which a division (with a single result) by all nonzero elements is defined are called [[field (mathematics)|fields]] and [[division ring]]s. In a [[ring (mathematics)|ring]] the elements by which division is always possible are called the [[unit (ring theory)|units]] (for example, 1 and –1 in the ring of integers).\n\n==Introduction==\nIn its simplest form, division can be viewed either as a [[quotition and partition|quotition or a partition]]. In terms of quotition, {{math|20&nbsp;÷&nbsp;5}} means the number of 5s that must be added to get&nbsp;20. In terms of partition, {{math|20&nbsp;÷&nbsp;5}} means the size of each of 5 parts into which a set of size 20 is divided. For example, 20 apples divide into four groups of five apples, meaning that ''twenty divided by five is equal to four''. This is denoted as {{math|1=20 / 5 = 4}}, {{math|20 ÷ 5 {{=}} 4}}, or {{math|1={{sfrac|20|5}} = 4}}.<ref name=\"mwdiv\">{{MathWorld|id=Division|title=Division}}</ref> Notationally, the ''dividend'' is divided by the ''divisor'' to get a ''quotient''. In the example, 20 is the dividend, 5 is the divisor, and 4 is the quotient.\n\nUnlike the other basic operations, when dividing natural numbers there is sometimes a [[remainder]] that will not go evenly into the dividend; for example, {{math|10 ÷ 3}} leaves a remainder of 1, as 10 is not a multiple of 3. Sometimes this remainder is added to the quotient as a [[fractional part]], so {{math|10 ÷ 3}} is equal to {{math|{{sfrac|3|1|3}}}} or {{math|3.33...}}, but in the context of [[integer]] division, where numbers have no fractional part, the remainder is kept separately or discarded.<ref name=\"mwintdiv\">{{MathWorld|id=IntegerDivision|title=Integer Division}}</ref> When the remainder is kept as a fraction, it leads to a [[rational number]]. The set of all rational numbers is created by every possible division using integers. In modern mathematical terms, this is known as ''extending the system''.\n\nUnlike multiplication and addition, Division is not [[commutative]], meaning that {{math|''a'' ÷ ''b''}} is not always equal to {{math|''b'' ÷ ''a''}}.<ref>http://www.mathwords.com/c/commutative.htm Retrieved October 23 2018</ref> Division is also not, in general, [[associative]], meaning that when dividing multiple times, the order of division can change the result.<ref>http://www.mathwords.com/a/associative_operation.htm Retrieved October 23 2018</ref> For example, {{math|(20 ÷ 5) ÷ 2 {{=}} 2}}, but {{math|20 ÷ (5 ÷ 2) {{=}} 8}} (where the use of parentheses indicates that the operations inside parentheses are performed before the operations outside parentheses).\n\nDivision is, however, distributive, in the sense that {{math|(''a''+''b'') ÷ ''c'' {{=}} (''a'' ÷ ''c'') + (''b'' ÷ ''c'')}} for every number. Specifically, division has the right-[[distributive property]] over addition and subtraction. That means:\n: <math>\\frac{a + b}{c} = (a + b) \\div c = \\frac{a}{c} + \\frac{b}{c}</math>\n\nThis is the same as [[multiplication]]: <math>(a + b) \\times c = a \\times c + b \\times c</math>. However, division is ''not'' left-distributive:\n: <math>\\frac{a}{b + c} = a \\div (b + c) = \\left(\\frac{b}{a} + \\frac{c}{a}\\right)^{-1} \\ne \\frac{a}{b} + \\frac{a}{c}</math>\n\nwhich is unlike the case in multiplication.\n\nIf there are multiple divisions in a row,the order of calculation traditionally goes from left to right<ref>George Mark Bergman: [https://math.berkeley.edu/~gbergman/misc/numbers/ord_ops.html Order of arithmetic operations]</ref><ref>Education Place: [http://eduplace.com/math/mathsteps/4/a/index.html The Order of Operations]</ref>, which is called [[Left associative operator|left-associative]]:\n: <math>a \\div b \\div c = (a \\div b) \\div c = a \\div (b \\times c) = a \\times b^{-1} \\times c^{-1}</math>.\n\n== Notation ==\nDivision is often shown in algebra and science by placing the ''dividend'' over the ''divisor'' with a horizontal line, also called a [[fraction bar]], between them. For example, \"''a'' divided by ''b\"'' can written as:\n:<math>\\frac ab</math>\n\nwhich can also be read out loud as \"''a'' by ''b''\" or \"''a'' over ''b''\". A way to express division all on one line is to write the ''dividend'' (or numerator), then a [[Slash (punctuation)|slash]], then the ''divisor'' (or denominator), as follows:\n:<math>a/b</math>\n\nThis is the usual way of specifying division in most computer [[programming language]]s, since it can easily be typed as a simple sequence of [[ASCII]] characters. Some [[mathematical software]], such as [[MATLAB]] and [[GNU Octave]], allows the operands to be written in the reverse order by using the [[backslash]] as the division operator:\n:<math>b\\backslash a</math>\n\nA typographical variation halfway between these two forms uses a [[solidus (punctuation)|solidus]] (fraction slash) but elevates the dividend, and lowers the divisor:\n:<math>{}^{a}/{}_{b}</math>\n\nAny of these forms can be used to display a [[fraction (mathematics)|fraction]]. A fraction is a division expression where both dividend and divisor are [[integer]]s (typically called the ''numerator'' and ''denominator''), and there is no implication that the division must be evaluated further. A second way to show division is to use the [[obelus]] (or division sign), common in arithmetic, in this manner:\n:<math>a \\div b</math>\n\nThis form is infrequent except in elementary arithmetic. [[ISO 80000-2]]-9.6 states it should not be used. The obelus is also used alone to represent the division operation itself, as for instance as a label on a key of a [[calculator]]. The obelus was introduced by Swiss mathematician [[Johann Rahn]] in 1659 in ''Teutsche Algebra''.<ref name=\"Cajori\">{{cite book|author=Cajori, Florian|title=A History of Mathematical Notations|publisher=Open Court Pub. Co.|year=1929}}</ref>{{rp|211}}\n:<math>a : b</math>\n\nIn some non-[[English language|English]]-speaking countries colon is used to denote division.<ref>{{cite book|title=Mathematics for Teachers: An Interactive Approach for Grades K–8|page=126|author=Thomas Sonnabend|publisher=Brooks/Cole, Cengage Learning (Charles Van Wagner)|year=2010|isbn=978-0-495-56166-8}}</ref> This notation was introduced by [[Gottfried Wilhelm Leibniz]] in his 1684 ''Acta eruditorum''.<ref name=\"Cajori\" />{{rp|295}} Leibniz disliked having separate symbols for ratio and division. However, in English usage the [[colon (punctuation)|colon]] is restricted to expressing the related concept of [[ratio]]s.\n\nSince the 19th century US textbooks have used <math>b)~a</math> or <math>b \\overline{)a}</math> to denote ''a'' divided by ''b'', especially when discussing [[long division]]. The history of this notation is not entirely clear because it evolved over time.<ref name=\"Smith\">{{cite book|title=History Of Mathematics Vol II|author=Smith, David Eugene|publisher=Ginn And Company|year=1925}}</ref>\n\n== Computing ==\n{{main|Long division|Division algorithm}}\n\n===Manual methods===\nDivision is often introduced through the notion of \"sharing out\" a set of objects, for example a pile of lollies, into a number of equal portions. Distributing the objects several at a time in each round of sharing to each portion leads to the idea of \"[[Chunking (division)|chunking]]\" — a form of division where one repeatedly subtracts multiples of the divisor from the dividend itself. \n\nBy allowing one to subtract more multiples than what the partial remainder allows at a given stage, more flexible methods, such as the bidirectional variant of chunking, can be developed as well.<ref>{{Cite web|url=https://mathvault.ca/long-division/|title=The Definitive Higher Math Guide to Long Division and Its Variants — for Integers|date=2019-02-24|website=Math Vault|language=en-US|access-date=2019-06-24}}</ref>\n\nMore systematic and more efficient (but also more formalised, more rule-based, and more removed from an overall holistic picture of what division is achieving), a person who knows the [[multiplication tables]] can divide two integers with pencil and paper using the method of [[short division]], if the divisor is small, or [[long division]], if the divisor is larger. If the dividend has a [[fraction (mathematics)|fractional]] part (expressed as a [[decimal fraction]]), one can continue the algorithm past the ones place as far as desired. If the divisor has a fractional part, one can restate the problem by moving the decimal to the right in both numbers until the divisor has no fraction.\n\nA person can calculate division with an [[abacus]] by repeatedly placing the dividend on the abacus, and then subtracting the divisor the offset of each digit in the result, counting the number of divisions possible at each offset.<ref>{{Cite web|url=https://owlcation.com/stem/How-to-do-Division-on-the-Abacus-in-Easy-Steps|title=How to Do Division on the Abacus in Easy Steps|website=Owlcation|language=en|access-date=2019-06-24}}</ref>\n\nA person can use [[logarithm tables]] to divide two numbers, by subtracting the two numbers' logarithms, then looking up the antilogarithm of the result.\n\nA person can calculate division with a [[slide rule]] by aligning the divisor on the C scale with the dividend on the D scale. The quotient can be found on the D scale where it is aligned with the left index on the C scale. The user is responsible, however, for mentally keeping track of the decimal point.\n\n===By computer or with computer assistance===\nModern computers compute division by methods that are faster than long division, with the more efficient ones relying on approximation techniques from numerical analysis. For [[division with remainder]], see [[Division algorithm]].\n\nIn [[modular arithmetic]] (modulo a prime number) and for [[real numbers]], nonzero numbers have a [[modular multiplicative inverse|multiplicative inverse]]. In these cases, a division by {{mvar|x}} may be computed as the product by the multiplicative inverse of {{mvar|x}}. This approach is often associated with the faster methods in computer arithmetic.\n\n==Division in different contexts==\n=== Euclidean division ===\n{{main|Euclidean division}}\nThe Euclidean division is the mathematical formulation of the outcome of the usual process of division of integers. It asserts that, given two integers, ''a'', the ''dividend'', and ''b'', the ''divisor'', such that ''b'' ≠ 0, there are [[Uniqueness quantification|unique]] integers ''q'', the ''quotient'', and ''r'', the remainder,  such that ''a'' = ''bq'' + ''r'' and 0 ≤ ''r'' < {{abs|''b''}}, where {{abs|''b''}} denotes the [[absolute value]] of ''b''.\n\n=== Of integers ===\nIntegers are not [[Closure (mathematics)|closed]] under division. Apart from division by zero being undefined, the quotient is not an integer unless the dividend is an integer multiple of the divisor. For example, 26 cannot be divided by 11 to give an integer. Such a case uses one of five approaches:\n# Say that 26 cannot be divided by 11; division becomes a [[partial function]].\n# Give an approximate answer as a [[decimal fraction]] or a [[mixed number]], so <math>\\tfrac{26}{11} \\simeq 2.36</math> or <math>\\tfrac{26}{11} \\simeq 2 \\tfrac {36}{100}.</math> This is the approach usually taken in [[numerical computation]].\n# Give the answer as a [[fraction (mathematics)|fraction]] representing a [[rational number]], so the result of the division of 26 by 11 is <math>\\tfrac{26}{11}.</math> But, usually, the resulting fraction should be simplified: the result of the division of 52 by 22 is also <math>\\tfrac{26}{11}</math>. This simplification may be done by factoring out the [[greatest common divisor]].\n# Give the answer as an integer ''[[quotient]]'' and a ''[[remainder]]'', so <math>\\tfrac{26}{11} = 2 \\mbox{ remainder } 4.</math> To make the distinction with the previous case, this division, with two integers as result, is sometimes called ''[[Euclidean division]]'', because it is the basis of the [[Euclidean algorithm]].\n# Give the integer quotient as the answer, so <math>\\tfrac{26}{11} = 2.</math> This is sometimes called ''integer division''.\nDividing integers in a [[computer program]] requires special care. Some [[programming language]]s, such as [[C (programming language)|C]], treat integer division as in case 5 above, so the answer is an integer. Other languages, such as [[MATLAB]] and every [[computer algebra system]] return a rational number as the answer, as in case 3 above. These languages also provide functions to get the results of the other cases, either directly or from the result of case 3.\n\nNames and symbols used for integer division include div, /, \\, and %. Definitions vary regarding integer division when the dividend or the divisor is negative: rounding may be toward zero (so called T-division) or toward [[Extended real number line|−∞]] (F-division); rarer styles can occur – see [[Modulo operation]] for the details.\n\n[[Divisibility rule]]s can sometimes be used to quickly determine whether one integer divides exactly into another.\n\n=== Of rational numbers ===\nThe result of dividing two [[rational number]]s is another rational number when the divisor is not 0. The division of two rational numbers ''p''/''q'' and ''r''/''s'' can be computed as\n:<math>{p/q \\over r/s} = {p \\over q} \\times {s \\over r} = {ps \\over qr}.</math>\n\nAll four quantities are integers, and only ''p'' may be 0. This definition ensures that division is the inverse operation of [[multiplication]].\n\n=== Of real numbers ===\nDivision of two [[real number]]s results in another real number (when the divisor is nonzero). It is defined such that ''a''/''b'' = ''c'' if and only if ''a'' = ''cb'' and ''b'' ≠ 0.\n\n=== Of complex numbers ===\nDividing two [[complex number]]s (when the divisor is nonzero) results in another complex number, which is found using the conjugate of the denominator:\n:<math>{p+iq \\over r+is} = {(p+iq)(r-is) \\over (r+is)(r-is)} = {pr+qs + i(qr-ps) \\over r^2+s^2} = {pr+qs \\over r^2+s^2} + i{qr-ps \\over r^2+s^2}.</math>\n\nThis process of multiplying and dividing by <math>r-is</math> is called 'realisation' or (by analogy) [[Rationalisation (mathematics)|rationalisation]]. All four quantities ''p'', ''q'', ''r'', ''s'' are real numbers, and ''r'' and ''s'' may not both be 0.\n\nDivision for complex numbers expressed in polar form is simpler than the definition above:\n:<math>{p e^{iq} \\over r e^{is}} = {p e^{iq} e^{-is} \\over r e^{is} e^{-is}} = {p \\over r}e^{i(q - s)}. </math>\n\nAgain all four quantities ''p'', ''q'', ''r'', ''s'' are real numbers, and ''r'' may not be 0.\n\n=== Of polynomials ===\nOne can define the division operation for [[polynomial]]s in one variable over a [[field (mathematics)|field]]. Then, as in the case of integers, one has a remainder. See [[Euclidean division of polynomials]], and, for hand-written computation, [[polynomial long division]] or [[synthetic division]].\n\n=== Of matrices ===\nOne can define a division operation for matrices. The usual way to do this is to define {{nowrap|1=''A'' / ''B'' = ''AB''<sup>−1</sup>}}, where {{nowrap|''B''<sup>−1</sup>}} denotes the [[inverse matrix|inverse]] of ''B'', but it is far more common to write out {{nowrap|''AB''<sup>−1</sup>}} explicitly to avoid confusion. An [[elementwise division]] can also be defined in terms of the [[Hadamard product (matrices)|Hadamard product]].\n\n==== Left and right division ====\nBecause [[matrix multiplication]] is not [[commutative]], one can also define a [[left division]] or so-called ''backslash-division'' as {{nowrap|1=''A'' \\ ''B'' = ''A''<sup>−1</sup>''B''}}. For this to be well defined, {{nowrap|''B''<sup>−1</sup>}} need not exist, however {{nowrap|''A''<sup>−1</sup>}} does need to exist. To avoid confusion, division as defined by {{nowrap|1=''A'' / ''B'' = ''AB''<sup>−1</sup>}} is sometimes called ''right division'' or ''slash-division'' in this context.\n\nNote that with left and right division defined this way, {{nowrap|''A'' / (''BC'')}} is in general not the same as {{nowrap|(''A'' / ''B'') / ''C''}}, nor is {{nowrap|(''AB'') \\ ''C''}} the same as {{nowrap|''A'' \\ (''B'' \\ ''C'')}}. However, {{nowrap|1=''A'' / (''BC'') = (''A'' / ''C'') / ''B''}} and {{nowrap|1=(''AB'') \\ ''C'' = ''B'' \\ (''A'' \\ ''C'')}}.\n\n==== Pseudoinverse ====\nTo avoid problems when {{nowrap|''A''<sup>−1</sup>}} and/or {{nowrap|''B''<sup>−1</sup>}} do not exist, division can also be defined as multiplication by the [[Generalized inverse|pseudoinverse]]. That is, {{nowrap|1=''A'' / ''B'' = ''AB''<sup>+</sup>}} and {{nowrap|1=''A'' \\ ''B'' = ''A''<sup>+</sup>''B''}}, where {{nowrap|''A''<sup>+</sup>}} and {{nowrap|''B''<sup>+</sup>}} denote the pseudoinverses of ''A'' and ''B''.\n\n=== Abstract algebra ===\nIn [[abstract algebra]], given a [[Magma (algebra)|magma]] with binary operation ∗ (which could nominally be termed multiplication), left division of ''b'' by ''a'' (written {{nowrap|''a'' \\ ''b''}}) is typically defined as the solution ''x'' to the equation {{nowrap|1=''a'' ∗ ''x'' = ''b''}}, if this exists and is unique.  Similarly, right division of ''b'' by ''a'' (written {{nowrap|''b'' / ''a''}}) is the solution ''y'' to the equation {{nowrap|1=''y'' ∗ ''a'' = ''b''}}.  Division in this sense does not require ∗ to have any particular properties (such as commutativity, associativity, or an identity element).\n\n\"Division\" in the sense of \"cancellation\" can be done in any magma by an element with the [[cancellation property]].  Examples include [[Matrix (mathematics)|matrix]] algebras and [[quaternion]] algebras.  A [[quasigroup]] is a structure in which division is always possible, even without an identity element and hence inverses.  In an [[integral domain]], where not every element need have an inverse, ''division'' by a cancellative element ''a'' can still be performed on elements of the form ''ab'' or ''ca'' by left or right cancellation, respectively.  If a [[Ring (mathematics)|ring]] is finite and every nonzero element is cancellative, then by an application of the [[pigeonhole principle]], every nonzero element of the ring is invertible, and ''division'' by any nonzero element is possible. To learn about when ''algebras'' (in the technical sense) have a division operation, refer to the page on [[division algebra]]s. In particular [[Bott periodicity]] can be used to show that any [[real number|real]] [[normed division algebra]] must be [[isomorphic]] to either the real numbers '''R''', the [[complex number]]s '''C''', the [[quaternion]]s '''H''', or the [[octonion]]s '''O'''.\n<!-- Left vs right, definition of quasigroup, relationship to inverse elements in presence of associativity, examples: groups, octonions -->\n\n=== Calculus ===\nThe [[derivative]] of the quotient of two functions is given by the [[quotient rule]]:\n:<math>{\\left(\\frac fg\\right)}' = \\frac{f'g - fg'}{g^2}.</math>\n\n== Division by zero ==\n{{main|Division by zero}}\nDivision of any number by [[zero]] in most mathematical systems is undefined, because zero multiplied by any finite number always results in a [[multiplication|product]] of zero.<ref>http://mathworld.wolfram.com/DivisionbyZero.html Retrieved October 23 2018</ref> Entry of such an expression into most [[calculator]]s produces an error message. However, in certain higher level mathematics division by zero is possible by the [[zero ring]] and algebras such as [[Wheel theory|wheels]].<ref>Jesper Carlström. [https://www2.math.su.se/reports/2001/11/2001-11.pdf \"On Division by Zero\"] Retrieved October 23 2018</ref> In these algebras, the meaning of division is different from traditional definitions.\n\n== See also ==\n* [[Rod calculus#Division|400AD Sunzi division algorithm]]\n* [[Division by two]]\n* [[Galley division]]\n* [[Group (mathematics)|Group]]\n* [[Inverse element]]\n* [[Order of operations]]\n* [[Repeating decimal]]\n\n==Notes==\n{{Notelist}}\n\n== References ==\n{{Reflist}}\n\n== External links ==\n{{Commons category|Division (mathematics)}}\n* [https://planetmath.org/division Planetmath division]\n* [http://webhome.idirect.com/~totton/abacus/pages.htm#Division1 Division on a Japanese abacus] selected from [http://webhome.idirect.com/~totton/abacus/ Abacus: Mystery of the Bead]\n* [http://webhome.idirect.com/~totton/suanpan/sh_div/ Chinese Short Division Techniques on a Suan Pan]\n* [http://www.math.wichita.edu/history/topics/arithmetic.html#div Rules of divisibility]\n\n{{Elementary arithmetic}}\n{{Fractions and ratios}}\n{{Hyperoperations}}\n{{Authority control}}\n\n[[Category:Elementary arithmetic]]\n[[Category:Binary operations]]\n[[Category:Division (mathematics)| ]]"
    },
    {
      "title": "Dot product",
      "url": "https://en.wikipedia.org/wiki/Dot_product",
      "text": "{{short description|Algebraic operation on coordinates vectors}}\n{{redirect|Scalar product|the abstract scalar product|Inner product space|the product of a vector and a scalar|Scalar multiplication}}\n\nIn [[mathematics]], the '''dot product''' or '''scalar product'''<ref group=note>The term ''scalar product'' is often also used more generally to mean a [[symmetric bilinear form]], for example for a [[pseudo-Euclidean space]].{{cn|date=March 2017}}</ref> is an [[algebraic operation]] that takes two equal-length sequences of numbers (usually [[coordinate vector]]s) and returns a single number.  In [[Euclidean geometry]], the dot product of the [[Cartesian coordinates]] of two [[vector (mathematics and physics)|vectors]] is widely used and often called \"the\" '''inner product''' (or rarely '''projection product''') of Euclidean space even though it is not the only inner product that can be defined on Euclidean space; see also [[inner product space]].\n\nAlgebraically, the dot product is the sum of the [[Product (mathematics)|products]] of the corresponding entries of the two sequences of numbers. Geometrically, it is the product of the [[Euclidean vector#Length|Euclidean magnitude]]s of the two vectors and the [[cosine]] of the angle between them. These definitions are equivalent when using Cartesian coordinates. In modern [[geometry]], [[Euclidean space]]s are often defined by using [[vector space]]s. In this case, the dot product is used for defining lengths (the length of a vector is the [[square root]] of the dot product of the vector by itself) and angles (the cosine of the angle of two vectors is the quotient of their dot product by the product of their lengths).\n\nThe name \"dot product\" is derived from the [[Interpunct|centered dot]] \"&nbsp;'''·'''&nbsp;\" that is often used to designate this operation; the alternative name \"scalar product\" emphasizes that the result is a [[scalar (mathematics)|scalar]], rather than a [[Euclidean vector|vector]], as is the case for the [[vector product]] in three-dimensional space.\n\n==Definition==\nThe dot product may be defined algebraically or geometrically.  The geometric definition is based on the notions of angle and distance (magnitude of vectors). The equivalence of these two definitions relies on having a [[Cartesian coordinate system]] for Euclidean space.\n\nIn modern presentations of [[Euclidean geometry]], the points of space are defined in terms of their [[Cartesian coordinates]], and [[Euclidean space]] itself is commonly identified with the [[real coordinate space]] '''R'''<sup>''n''</sup>. In such a presentation, the notions of length and angles are defined by means of the dot product. The length of a vector is defined as the [[square root]] of the dot product of the vector by itself, and the [[cosine]] of the (non oriented) angle of two vectors of length one is defined as their dot product.  So the equivalence of the two definitions of the dot product is a part of the equivalence of the classical and the modern formulations of Euclidean geometry.\n\n===Algebraic definition===\nThe dot product of two vectors {{nowrap|1={{ font color | red |  '''a'''}}  = {{ font color | red |[''a''<sub>1</sub>, ''a''<sub>2</sub>, …, ''a''<sub>''n''</sub>] }}}} and {{nowrap|1={{ font color | blue | '''b''' }} = {{ font color | blue |[''b''<sub>1</sub>, ''b''<sub>2</sub>, …, ''b''<sub>''n''</sub>] }}}} is defined as:<ref name=\"Lipschutz2009\">{{cite book |author1=S. Lipschutz |author2=M. Lipson |title= Linear Algebra (Schaum’s Outlines)|edition= 4th |year= 2009|publisher= McGraw Hill|isbn=978-0-07-154352-1}}</ref>\n\n:<math>\\mathbf{\\color{red}a}\\cdot\\mathbf{\\color{blue}b}=\\sum_{i=1}^n {\\color{red}a}_i{\\color{blue}b}_i={\\color{red}a}_1{\\color{blue}b}_1+{\\color{red}a}_2{\\color{blue}b}_2+\\cdots+{\\color{red}a}_n{\\color{blue}b}_n</math>\n\nwhere Σ denotes [[Summation|summation]] and ''n'' is the dimension of the [[vector space]]. For instance, in [[Three-dimensional space (mathematics)|three-dimensional space]], the dot product of vectors {{nowrap|{{ font color | red |[1, 3, −5]}}}} and {{nowrap|{{ font color | blue |[4, −2, −1]}}}} is:\n\n:<math>\n\\begin{align}\n\\ [{\\color{red}1, 3, -5}] \\cdot  [{\\color{blue}4, -2, -1}] &= ({\\color{red}1} \\times {\\color{blue}4}) + ({\\color{red}3}\\times{\\color{blue}-2}) + ({\\color{red}-5}\\times{\\color{blue}-1}) \\\\\n&= 4 - 6 + 5 \\\\\n&= 3\n\\end{align}\n</math>\n\nIf vectors are identified with [[row matrix|row matrices]], the dot product can also be written as a [[matrix multiplication|matrix product]]\n\n:<math>\\mathbf{\\color{red}a} \\cdot \\mathbf{\\color{blue}b} = \\mathbf{\\color{red}a}\\mathbf{\\color{blue}b}^\\top,</math>\nwhere <math>\\mathbf{\\color{blue}b}^\\top</math> denotes the [[transpose]] of <math>\\mathbf{\\color{blue}b}</math>.\n\nExpressing  the above example in this way, a 1 × 3 matrix ([[row vector]]) is multiplied by a 3 × 1 matrix ([[column vector]]) to get a 1 × 1 matrix that is identified with its unique entry: \n:<math>\n  \\begin{bmatrix}\n   \\color{red}1 & \\color{red}3 & \\color{red}-5\n  \\end{bmatrix}\n  \\begin{bmatrix}\n   \\color{blue}4 \\\\ \\color{blue}-2 \\\\ \\color{blue}-1\n  \\end{bmatrix} = \\color{purple}3 \n</math>.\n\n===Geometric definition===\n[[File:Inner-product-angle.svg|thumb|Illustration showing how to find the angle between vectors using the dot product]]\nIn [[Euclidean space]], a [[Euclidean vector]] is a geometric object that possesses both a magnitude and a direction.  A vector can be pictured as an arrow.  Its magnitude is its length, and its direction is the direction that the arrow points to. The magnitude of a vector '''a''' is denoted by <math> \\left\\| \\mathbf{a} \\right\\| </math>. The dot product of two Euclidean vectors '''a''' and '''b''' is defined by<ref name=\"Spiegel2009\">{{cite book |author1=M.R. Spiegel |author2=S. Lipschutz |author3=D. Spellman |title= Vector Analysis (Schaum’s Outlines)|edition= 2nd |year= 2009|publisher= McGraw Hill|isbn=978-0-07-161545-7}}</ref><ref>{{cite book|author1=A I Borisenko|author2=I E Taparov|title=Vector and tensor analysis with applications|publisher=Dover|translator=Richard Silverman|year=1968|page=14}}</ref>\n:<math>\\mathbf{a}\\cdot\\mathbf{b}=\\|\\mathbf{a}\\|\\ \\|\\mathbf{b}\\|\\cos\\theta ,</math>\nwhere {{mvar|θ}} is the [[angle]] between {{math|'''a'''}} and {{math|'''b'''}}.\n\nIn particular, if the vectors  {{math|'''a'''}} and {{math|'''b'''}} are [[orthogonal]] (their angle is {{math|{{pi}} / 2}} or 90°), then <math>\\cos \\frac \\pi 2 = 0</math> implies\n\n:<math>\\mathbf a \\cdot \\mathbf b = 0 .</math>\nAt the other extreme, if they are codirectional, then the angle between them is zero and\n:<math>\\mathbf a \\cdot \\mathbf b = \\left\\| \\mathbf a \\right\\| \\, \\left\\| \\mathbf b \\right\\| </math>\nThis implies that the dot product of a vector '''a''' with itself is\n:<math>\\mathbf a \\cdot \\mathbf a = \\left\\| \\mathbf a \\right\\| ^2 ,</math>\nwhich gives\n: <math> \\left\\| \\mathbf a \\right\\| = \\sqrt{\\mathbf a \\cdot \\mathbf a} ,</math>\nthe formula for the [[Euclidean length]] of the vector.\n\n===Scalar projection and first properties===\n[[File:Dot Product.svg|thumb|right|Scalar projection]]\nThe [[scalar projection]] (or scalar component) of a Euclidean vector '''a''' in the direction of a Euclidean vector '''b''' is given by\n:<math> a_b = \\left\\| \\mathbf a \\right\\| \\cos \\theta ,</math>\nwhere {{mvar|θ}} is the angle between '''a''' and '''b'''.\n\nIn terms of the geometric definition of the dot product, this can be rewritten\n:<math>a_b = \\mathbf a \\cdot \\widehat{\\mathbf b} ,</math>\nwhere <math> \\widehat{\\mathbf b} = \\mathbf b / \\left\\| \\mathbf b \\right\\| </math> is the [[unit vector]] in the direction of '''b'''.\n\n[[File:Dot product distributive law.svg|thumb|right|Distributive law for the dot product]]\nThe dot product is thus characterized geometrically by<ref>{{cite book | last1=Arfken | first1=G. B. | last2=Weber | first2=H. J. | title=Mathematical Methods for Physicists | publisher=[[Academic Press]] | location=Boston, MA | edition=5th | isbn=978-0-12-059825-0 | year=2000 | pages=14–15 }}.</ref>\n:<math> \\mathbf a \\cdot \\mathbf b = a_b \\left\\| \\mathbf{b} \\right\\| = b_a \\left\\| \\mathbf{a} \\right\\| .</math>\nThe dot product, defined in this manner, is homogeneous under scaling in each variable, meaning that for any scalar ''α'',\n:<math> ( \\alpha \\mathbf{a} ) \\cdot \\mathbf b = \\alpha ( \\mathbf a \\cdot \\mathbf b ) = \\mathbf a \\cdot ( \\alpha \\mathbf b ) .</math>\nIt also satisfies a [[distributive law]], meaning that\n:<math> \\mathbf a \\cdot ( \\mathbf b + \\mathbf c ) = \\mathbf a \\cdot \\mathbf b + \\mathbf a \\cdot \\mathbf c .</math>\n\nThese properties may be summarized by saying that the dot product is a [[bilinear form]]. Moreover, this bilinear form is [[positive definite bilinear form|positive definite]], which means that \n<math> \\mathbf a \\cdot \\mathbf a </math>\nis never negative and is zero if and only if <math> \\mathbf a = \\mathbf 0 </math>, the zero vector.\n\n===Equivalence of the definitions===\nIf '''e'''<sub>1</sub>, ..., '''e'''<sub>''n''</sub> are the [[standard basis|standard basis vectors]] in '''R'''<sup>''n''</sup>, then we may write\n:<math>\\begin{align}\n\\mathbf a &= [a_1 , \\dots , a_n] = \\sum_i a_i \\mathbf e_i \\\\\n\\mathbf b &= [b_1 , \\dots , b_n] = \\sum_i b_i \\mathbf e_i.\n\\end{align}\n</math>\nThe vectors '''e'''<sub>''i''</sub> are an [[orthonormal basis]], which means that they have unit length and are at right angles to each other.  Hence since these vectors have unit length\n:<math> \\mathbf e_i \\cdot \\mathbf e_i = 1 </math>\nand since they form right angles with each other, if {{nowrap|''i'' ≠ ''j''}},\n:<math> \\mathbf e_i \\cdot \\mathbf e_j = 0 .</math>\nThus in general we can say that:\n:<math> \\mathbf e_i \\cdot \\mathbf e_j = \\delta_ {ij} .</math>\nWhere δ <sub> ij </sub> is the [[Kronecker delta]].\n\nAlso, by the geometric definition, for any vector '''e'''<sub>''i''</sub> and a vector '''a''', we note\n:<math> \\mathbf a \\cdot \\mathbf e_i = \\left\\| \\mathbf a \\right\\| \\, \\left\\| \\mathbf e_i \\right\\| \\cos \\theta = \\left\\| \\mathbf a \\right\\| \\cos \\theta = a_i ,</math>\nwhere ''a''<sub>''i''</sub> is the component of vector '''a''' in the direction of '''e'''<sub>''i''</sub>.\n\nNow applying the distributivity of the geometric version of the dot product gives\n:<math> \\mathbf a \\cdot \\mathbf b = \\mathbf a \\cdot \\sum_i b_i \\mathbf e_i = \\sum_i b_i ( \\mathbf a \\cdot \\mathbf e_i ) = \\sum_i b_i a_i= \\sum_i a_i b_i  ,</math>\nwhich is precisely the algebraic definition of the dot product.  So the geometric dot product equals the algebraic dot product.\n\n==Properties==\nThe dot product fulfills the following properties if '''a''', '''b''', and '''c''' are real [[vector (geometry)|vectors]] and ''r'' is a [[scalar (mathematics)|scalar]].<ref name=\"Lipschutz2009\" /><ref name=\"Spiegel2009\" />\n\n# '''[[Commutative]]:'''\n#: <math> \\mathbf{a} \\cdot \\mathbf{b} = \\mathbf{b} \\cdot \\mathbf{a} ,</math>\n#: which follows from the definition (''θ'' is the angle between '''a''' and '''b'''):\n#: <math> \\mathbf{a} \\cdot \\mathbf{b} = \\left\\| \\mathbf{a} \\right\\| \\left\\| \\mathbf{b} \\right\\| \\cos \\theta = \\left\\| \\mathbf{b} \\right\\| \\left\\| \\mathbf{a} \\right\\| \\cos \\theta = \\mathbf{b} \\cdot \\mathbf{a} .</math>\n# '''[[Distributive property|Distributive]] over vector addition:'''\n#: <math> \\mathbf{a} \\cdot (\\mathbf{b} + \\mathbf{c}) = \\mathbf{a} \\cdot \\mathbf{b} + \\mathbf{a} \\cdot \\mathbf{c} .</math>\n# '''[[bilinear form|Bilinear]]''':\n#: <math> \\mathbf{a} \\cdot ( r \\mathbf{b} + \\mathbf{c} ) = r ( \\mathbf{a} \\cdot \\mathbf{b} ) + ( \\mathbf{a} \\cdot \\mathbf{c} ) .</math>\n# '''[[Scalar multiplication]]:'''\n#: <math> ( c_1 \\mathbf{a} ) \\cdot ( c_2 \\mathbf{b} ) = c_1 c_2 ( \\mathbf{a} \\cdot \\mathbf{b} ) .</math>\n# '''Not [[associative]]''' because the dot product between a scalar ('''a ⋅ b''') and a vector ('''c''') is not defined, which means that the expressions involved in the associative property, ('''a ⋅ b''') ⋅ '''c''' or '''a''' ⋅ ('''b ⋅ c''') are both ill-defined.<ref>Weisstein, Eric W. \"Dot Product.\" From MathWorld--A Wolfram Web Resource. http://mathworld.wolfram.com/DotProduct.html</ref> Note however that the previously mentioned scalar multiplication property is sometimes called the \"associative law for scalar and dot product\"<ref name=\"BanchoffWermer1983\">{{cite book|author1=T. Banchoff|author2=J. Wermer|title=Linear Algebra Through Geometry|year=1983|publisher=Springer Science & Business Media|isbn=978-1-4684-0161-5|page=12}}</ref> or one can say that \"the dot product is associative with respect to scalar multiplication\" because ''c'' ('''a''' ⋅ '''b''') = (''c'' '''a''') ⋅ '''b''' = '''a''' ⋅ (''c'' '''b''').<ref name=\"BedfordFowler2008\">{{cite book|author1=A. Bedford|author2=Wallace L. Fowler|title=Engineering Mechanics: Statics|year=2008|publisher=Prentice Hall|isbn=978-0-13-612915-8|edition=5th|page=60}}</ref>\n# '''[[Orthogonal]]:'''\n#: Two non-zero vectors '''a''' and '''b''' are ''orthogonal'' [[if and only if]] {{nowrap|1='''a''' ⋅ '''b''' = 0}}.\n# '''No [[cancellation law|cancellation]]:'''\n#: Unlike multiplication of ordinary numbers, where if {{nowrap|1=''ab'' = ''ac''}}, then ''b'' always equals ''c'' unless ''a'' is zero, the dot product does not obey the [[cancellation law]]:\n#: If {{nowrap|1='''a''' ⋅ '''b''' = '''a''' ⋅ '''c'''}} and {{nowrap|'''a''' ≠ '''0'''}}, then we can write: {{nowrap|1='''a''' ⋅ ('''b''' − '''c''') = 0}} by the [[distributive law]]; the result above says this just means that '''a''' is perpendicular to {{nowrap|('''b''' − '''c''')}}, which still allows {{nowrap|('''b''' − '''c''') ≠ '''0'''}}, and therefore {{nowrap|'''b''' ≠ '''c'''}}.\n# '''[[Product Rule]]:''' If '''a''' and '''b''' are [[function (mathematics)|functions]], then the derivative ([[Notation for differentiation#Lagrange's notation|denoted by a prime]] ′) of {{nowrap|'''a''' ⋅ '''b'''}} is {{nowrap|'''a'''′ ⋅ '''b''' + '''a''' ⋅ '''b'''′}}.\n\n===Application to the law of cosines===\n[[File:Dot product cosine rule.svg|100px|thumb|Triangle with vector edges '''a''' and '''b''', separated by angle ''θ''.]]\n\n{{main|Law of cosines}}\n\nGiven two vectors '''a''' and '''b''' separated by angle ''θ'' (see image right), they form a triangle with a third side {{nowrap|1={{ font color | gold |'''c'''}} = {{ font color | red |'''a'''}} − {{ font color | blue |'''b'''}}}}. The dot product of this with itself is:\n\n:<math>\n\\begin{align}\n\\mathbf{\\color{gold}c} \\cdot \\mathbf{\\color{gold}c}  & = ( \\mathbf{\\color{red}a} - \\mathbf{\\color{blue}b}) \\cdot ( \\mathbf{\\color{red}a} - \\mathbf{\\color{blue}b} ) \\\\\n & = \\mathbf{\\color{red}a} \\cdot \\mathbf{\\color{red}a} - \\mathbf{\\color{red}a} \\cdot \\mathbf{\\color{blue}b} - \\mathbf{\\color{blue}b} \\cdot \\mathbf{\\color{red}a} + \\mathbf{\\color{blue}b} \\cdot \\mathbf{\\color{blue}b} \\\\\n & = {\\color{red}a}^2 - \\mathbf{\\color{red}a} \\cdot \\mathbf{\\color{blue}b} - \\mathbf{\\color{red}a} \\cdot \\mathbf{\\color{blue}b} + {\\color{blue}b}^2 \\\\\n & = {\\color{red}a}^2 - 2 \\mathbf{\\color{red}a} \\cdot \\mathbf{\\color{blue}b} + {\\color{blue}b}^2 \\\\\n {\\color{gold}c}^2 & = {\\color{red}a}^2 + {\\color{blue}b}^2 - 2 {\\color{red}a} {\\color{blue}b} \\cos {\\color{purple}\\theta} \\\\\n\\end{align}\n</math>\n\nwhich is the [[law of cosines]].\n{{clear}}\n\n==Triple product==\n{{Main|Triple product}}\n\nThere are two [[ternary operation]]s involving dot product and [[cross product]].\n\nThe '''scalar triple product''' of three vectors is defined as\n:<math> \\mathbf{a} \\cdot ( \\mathbf{b} \\times \\mathbf{c} ) = \\mathbf{b} \\cdot ( \\mathbf{c} \\times \\mathbf{a} )=\\mathbf{c} \\cdot ( \\mathbf{a} \\times \\mathbf{b} ).</math>\nIts value is the [[determinant]] of the matrix whose columns are the [[Cartesian coordinates]] of the three vectors. It is the signed [[volume]] of the [[Parallelepiped]] defined by the three vectors.\n\nThe '''vector triple product''' is defined by<ref name=\"Lipschutz2009\" /><ref name=\"Spiegel2009\" />\n:<math> \\mathbf{a} \\times ( \\mathbf{b} \\times \\mathbf{c} ) = \\mathbf{b} ( \\mathbf{a} \\cdot \\mathbf{c} ) - \\mathbf{c} ( \\mathbf{a} \\cdot \\mathbf{b} ).</math>\nThis identity, also known as ''Lagrange's formula'' [[mnemonic|may be remembered]] as \"BAC minus CAB\", keeping in mind which vectors are dotted together. This formula finds application in simplifying vector calculations in [[physics]].\n\n==Physics==\nIn [[physics]], vector magnitude is a [[scalar (physics)|scalar]] in the physical sense, i.e. a [[physical quantity]] independent of the coordinate system, expressed as the [[product (mathematics)|product]]  of a [[number|numerical value]] and a [[physical unit]], not just a number. The dot product is also a scalar in this sense, given by the formula, independent of the coordinate system.  Examples include:<ref name=\"Riley2010\">{{cite book |author1=K.F. Riley |author2=M.P. Hobson |author3=S.J. Bence |title= Mathematical methods for physics and engineering|edition= 3rd|year= 2010|publisher= Cambridge University Press|isbn=978-0-521-86153-3}}</ref><ref>{{cite book |author1=M. Mansfield |author2=C. O’Sullivan |title= Understanding Physics|edition= 4th |year= 2011|publisher= John Wiley & Sons|isbn=978-0-47-0746370}}</ref>\n* [[Mechanical work]] is the dot product of [[force]] and [[Displacement (vector)|displacement]] vectors,\n*[[Power (physics)|Power]] is the dot product of [[force]] and [[velocity]].\n\n==Generalizations==\n\n===Complex vectors===\nFor vectors with [[complex number|complex]] entries, using the given definition of the dot product would lead to quite different properties. For instance the dot product of a vector with itself would be an arbitrary complex number, and could be zero without the vector being the zero vector (such vectors are called [[Isotropic quadratic form|isotropic]]); this in turn would have consequences for notions like length and angle. Properties such as the positive-definite norm can be salvaged at the cost of giving up the symmetric and bilinear properties of the scalar product, through the alternative definition<ref>{{citation|page=287|first=Sterling K.|last=Berberian|title=Linear Algebra|year=2014|origyear=1992|publisher=Dover|isbn=978-0-486-78055-9}}</ref><ref name=\"Lipschutz2009\" />\n\n:<math> \\mathbf{a} \\cdot \\mathbf{b} = \\sum{a_i \\overline{b_i}} ,</math>\nwhere <span style=\"text-decoration: overline\">''b''</span>''<sub>i</sub>'' is the [[complex conjugate]] of ''b<sub>i</sub>''. Then the scalar product of any vector with itself is a non-negative real number, and it is nonzero except for the zero vector. However this scalar product is thus [[sesquilinear]] rather than bilinear: it is [[conjugate linear]] and not linear in '''a''', and the scalar product is not symmetric, since\n:<math> \\mathbf{a} \\cdot \\mathbf{b} = \\overline{\\mathbf{b} \\cdot \\mathbf{a}} .</math>\nThe angle between two complex vectors is then given by\n:<math> \\cos \\theta = \\frac{\\operatorname{Re} ( \\mathbf{a} \\cdot \\mathbf{b} )}{ \\left\\| \\mathbf{a} \\right\\| \\, \\left\\| \\mathbf{b} \\right\\| } .</math>\n\nThis type of scalar product is nevertheless useful, and leads to the notions of [[Hermitian form]] and of general [[inner product space]]s.\n\n===Inner product===\n{{main|Inner product space}}\nThe inner product generalizes the dot product to [[vector space|abstract vector spaces]] over a [[field (mathematics)|field]] of [[scalar (mathematics)|scalars]], being either the field of [[real number]]s <math> \\R </math> or the field of [[complex number]]s <math> \\Complex </math>. It is usually denoted using [[angular brackets]] by <math> \\left\\langle \\mathbf{a} \\, , \\mathbf{b} \\right\\rangle </math>.\n\nThe inner product of two vectors over the field of complex numbers is, in general, a complex number, and  is [[Sesquilinear form|sesquilinear]] instead of bilinear. An inner product space is a [[normed vector space]], and the inner product of a vector with itself is real and positive-definite.\n\n===Functions===\nThe dot product is defined for vectors that have a finite number of [[coordinate vector|entries]]. Thus these vectors can be regarded as [[discrete function]]s: a length-{{math|''n''}} vector {{math|''u''}} is, then, a function with [[domain of a function|domain]] {{math|{''k'' ∈ ℕ ∣ 1 ≤ ''k'' ≤ ''n''}}}, and {{math|''u''<sub>''i''</sub>}} is a notation for the image of {{math|''i''}} by the function/vector {{math|''u''}}.\n\nThis notion can be generalized to [[continuous function]]s: just as the inner product on vectors uses a sum over corresponding components, the inner product on functions is defined as an integral over some [[Interval (mathematics)|interval]] {{math|''a'' ≤ ''x'' ≤ ''b''}} (also denoted {{math|[''a'', ''b'']}}):<ref name=\"Lipschutz2009\" />\n\n:<math> \\left\\langle u , v \\right\\rangle = \\int_a^b u(x) v(x) d x </math>\n\nGeneralized further to [[complex function]]s {{math|''ψ''(''x'')}} and {{math|''χ''(''x'')}}, by analogy with the complex inner product above, gives<ref name=\"Lipschutz2009\" />\n\n:<math> \\left\\langle \\psi , \\chi \\right\\rangle = \\int_a^b \\psi(x) \\overline{\\chi(x)} d x .</math>\n\n===Weight function===\nInner products can have a [[weight function]], i.e. a function which weights each term of the inner product with a value. Explicitly, the inner product of functions <math>u(x)</math> and <math>v(x)</math> with respect to the weight function <math>r(x)>0</math> is\n\n:<math> \\left\\langle u , v \\right\\rangle = \\int_a^b r(x) u(x) v(x) d x.</math>\n\n===Dyadics and matrices===\n[[Matrix (mathematics)|Matrices]] have the [[Frobenius inner product]], which is analogous to the vector inner product. It is defined as the sum of the products of the corresponding components of two matrices '''A''' and '''B''' having the same size:\n\n:<math> \\mathbf{A} : \\mathbf{B} = \\sum_i \\sum_j A_{ij} \\overline{B_{ij}} = \\mathrm{tr} ( \\mathbf{B}^\\mathrm{H} \\mathbf{A} ) = \\mathrm{tr} ( \\mathbf{A} \\mathbf{B}^\\mathrm{H} ) .</math>\n:<math> \\mathbf{A} : \\mathbf{B} = \\sum_i \\sum_j A_{ij} B_{ij} = \\mathrm{tr} ( \\mathbf{B}^\\mathrm{T} \\mathbf{A} ) = \\mathrm{tr} ( \\mathbf{A} \\mathbf{B}^\\mathrm{T} ) = \\mathrm{tr} ( \\mathbf{A}^\\mathrm{T} \\mathbf{B} ) = \\mathrm{tr} ( \\mathbf{B} \\mathbf{A}^\\mathrm{T} ) .</math> (For real matrices)\n\n[[Dyadics]] have a dot product and \"double\" dot product defined on them, see [[Dyadics#Product of dyadic and dyadic|Dyadics (Product of dyadic and dyadic)]] for their definitions.\n\n===Tensors===\nThe inner product between a [[tensor]] of order ''n'' and a tensor of order ''m'' is a tensor of order {{nowrap|''n'' + ''m'' − 2}}, see [[tensor contraction]] for details.\n\n==Computation==\n\n===Algorithms===\n\nThe straightforward algorithm for calculating a floating-point dot product of vectors can suffer from [[catastrophic cancellation]]. To avoid this, approaches such as the [[Kahan summation algorithm]] are used.\n\n===Libraries===\nA dot product function is included in [[BLAS]] level 1.\n\n==See also==\n* [[Cauchy–Schwarz inequality]]\n* [[Cross product]]\n* [[Matrix multiplication]]\n* [[Metric tensor]]\n\n==Notes==\n{{Reflist|group=note}}\n==References==\n{{reflist}}\n\n==External links==\n{{Commonscat|Scalar product}}\n* {{springer|title=Inner product|id=p/i051240}}\n* {{mathworld|urlname=DotProduct|title=Dot product}}\n* [http://www.mathreference.com/la,dot.html Explanation of dot product including with complex vectors]\n* [http://demonstrations.wolfram.com/DotProduct/ \"Dot Product\"] by Bruce Torrence, [[Wolfram Demonstrations Project]], 2007.\n\n{{linear algebra}}\n{{tensors}}\n\n[[Category:Articles containing proofs]]\n[[Category:Bilinear forms]]\n[[Category:Linear algebra]]\n[[Category:Vectors (mathematics and physics)]]\n[[Category:Analytic geometry]]\n[[Category:Tensors]]\n[[Category:Binary operations]]"
    },
    {
      "title": "Elvis operator",
      "url": "https://en.wikipedia.org/wiki/Elvis_operator",
      "text": "{{About|the use of the '''[[?:]]''' operator as a '''binary operator'''|use as a '''ternary operator'''|?:}}\nIn certain [[computer programming]] languages, the '''Elvis operator''', often written '''<code>?:</code>''', <code>or</code> or <code>||</code>, is a [[binary operator]] that returns its first operand if that operand evaluates to a true value, and otherwise evaluates and returns its second operand. The Elvis operator is a variant of the ternary [[conditional operator]], <code>[[%3F:|? :]]</code> in the sense that the expression with the Elvis operator <code>A ?: B</code> is approximately equivalent to the expression with the ternary operator <code>A ? A : B</code>.\n\nSome computer programming languages (e.g. [[C Sharp (programming language)|C#]]) have different semantics for this operator: instead of the first operand having to result in a boolean, it must result in an object reference.<ref>{{cite web |title=?? Operator |url=https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/operators/null-coalescing-operator |website=C# Reference |publisher=Microsoft |accessdate=5 December 2018}}</ref> If the resulting object reference is not [[Null pointer|null]], it is returned. Otherwise the value of the second operand (which may also be null) is returned. This distinction is necessary because in C#, references are not [[Type conversion#Implicit_type_conversion|implicitly convertible]] to a boolean.<ref>{{cite web |title=The bool type |url=https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/language-specification/types#the-bool-type |website=C# 6.0 draft specification |publisher=Microsoft |accessdate=5 December 2018}}</ref>\n\nThe name \"Elvis operator\" refers to the resemblance of one of its notations, <code>?:</code>, to an [[emoticon]] of [[Elvis Presley]].<ref>{{cite book|title=Java Programming|author=Joyce Farrell|isbn=978-1285081953|page=276|quote=The new operator is called Elvis operator because it uses a question mark and a colon together (?:); if you view it sideways, it reminds you of Elvis Presley.}}</ref>\n\n==Example==\n\n===Boolean variant===\n\nIn a language that supports the Elvis operator, something like this:\n:<code> x = f() ?: g() </code>\n\nwill set <code>x</code> equal to the result of <code>f()</code> if that result is a true value, and to the result of <code>g()</code> otherwise.\n\nIt is equivalent to this example, using the  [[?:|conditional ternary operator]]:\n:<code> x = f() ? f() : g() </code>\n\nexcept that it does not evaluate the <code>f()</code> twice if it is true.\n\n===Object reference variant===\n\nThis code will result in a reference to an object that is guaranteed to not be null. Function <code>f()</code> returns an object reference instead of a boolean, and may return null:\n:<code> x = f() ?: \"default value\" </code>\n\n==Languages supporting the Elvis operator==\n\n* In GNU [[C (programming language)|C]] and [[C++]] (that is: in C and C++ with [[GNU Compiler Collection|GCC]] extensions), the second operand of the ternary operator is optional.<ref>{{cite web|url=https://gcc.gnu.org/onlinedocs/gcc/Conditionals.html#Conditionals|title=Using the GNU Compiler Collection (GCC): Conditionals|website=gcc.gnu.org}}</ref> This has been the case since at least GCC 2.95.3<ref name=\"gcc-conditionals\">{{cite web|url=https://gcc.gnu.org/onlinedocs/gcc-2.95.3/gcc_4.html#SEC70|title=Using and Porting the GNU Compiler Collection (GCC): C Extensions|website=gcc.gnu.org}}</ref> (March 2001).\n* In [[Groovy (programming language)|Apache Groovy]], the \"Elvis operator\" <code>?:</code> is documented as a distinct operator;<ref name=\"groovyOperators\">{{cite web|url=http://docs.groovy-lang.org/latest/html/documentation/index.html#_elvis_operator|title=Elvis Operator (?: )}}</ref> this feature was added in Groovy 1.5<ref>{{cite web|url=http://groovy-lang.org/releasenotes/groovy-1.5.html|title=The Apache Groovy programming language - Groovy 1.5 release notes|website=groovy-lang.org}}</ref> (December 2007). Groovy, unlike GNU C and PHP, does ''not'' simply allow the second operand of ternary <code>?:</code> to be omitted; rather, binary <code>?:</code> must be written as a single operator, with no whitespace in between.\n* In [[PHP]], it is possible to leave out the middle part of the ternary operator since PHP 5.3.<ref>{{cite web |url=http://php.net/manual/en/language.operators.comparison.php#language.operators.comparison.ternary |publisher=PHP website |title=PHP: Comparison Operators - Manual |accessdate=2014-02-17}}</ref> (June 2009).\n* The [[Fantom (programming language)|Fantom]] programming language has the <code>?:</code> binary operator that compares its first operand with <code>null</code>.\n* In [[Kotlin (programming language)|Kotlin]], the Elvis operator returns its left-hand side if it is not null, and its right-hand side otherwise.<ref>{{cite web|url=https://kotlinlang.org/docs/reference/null-safety.html#elvis-operator|title=Null Safety - Kotlin Programming Language|website=Kotlin}}</ref> A common pattern is to use it with <code>return</code>, like this: <code>val foo = bar() ?: return</code>\n* In [[Gosu (programming language)|Gosu]], the <code>?:</code> operator returns the right operand if the left is null as well.\n* In [[C Sharp (programming language)|C#]], the [[Safe navigation operator|null-conditional]] operator, <code>?.</code> is referred to as the \"Elvis operator\",<ref>{{cite book |last1=Albahari |first1=Joseph |last2=Albahari |first2=Ben |date=2015 |title=C# 6.0 in a Nutshell |edition=6 |publisher=O'Reilly Media |isbn=978-1491927069 |page=59}}</ref> but it does not perform the same function. Instead, the null-coalescing operator <code>??</code> does.\n* In [[ColdFusion]] and [[CFML]], the Elvis operator was introduced using the <code>?:</code> syntax.\n* The [[Xtend]] programming language has an Elvis operator.<ref>{{cite web|url=https://eclipse.org/xtend/documentation/203_xtend_expressions.html#elvis-operator|title=Xtend - Expressions|first=Sven|last=Efftinge|website=eclipse.org}}</ref>\n* In Google's [[Google Closure Tools#Closure Templates|Closure Templates]], the Elvis operator is a [[null coalescing operator]], equivalent to <code>isNonnull($a) ? $a : $b</code>.<ref>{{cite web|url=https://developers.google.com/closure/templates/docs/concepts#operators|title=Closure Template Concepts  -  Closure Templates|website=Google Developers}}</ref>\n* [[Swift (programming language)|Swift]] supports this concept with its Nil-coalescing operator <code>??</code>,<ref>{{cite web|url=https://developer.apple.com/library/content/documentation/Swift/Conceptual/Swift_Programming_Language/BasicOperators.html|title=The Swift Programming Language (Swift 4.1): Basic Operators|website=developer.apple.com}}</ref> e.g. <code>(a ?? b)</code>.\n* [[SQL]] supports this concept with its COALESCE function, e.g. <code>COALESCE(a, b)</code>.\n* In [[Ballerina (programming language)|Ballerina]], the Elvis operator <code>L ?: R</code> returns the value of <code>L</code> if it's not nil. Otherwise, return the value of <code>R</code>. <ref>{{cite web|url=https://ballerina.io/learn/by-example/elvis-operator.html|title=Elvis Operator - Ballerina Programming Language|website=Ballerina}}</ref>\n* [[Clojure|Clojure]] supports this concept with the <code>or</code><ref>{{cite web|url=https://clojure.github.io/clojure/clojure.core-api.html#clojure.core/or|title=clojure.core or macro API reference}}</ref> macro, e.g. <code>(or a b)</code>. In the case of Clojure, it is var-arg, and not binary, e.g. <code>(or a b c d e)</code> will return the first non false value.\n\n==Analogous use of the short-circuiting OR operator==\nIn several languages, such as [[Common Lisp]], [[Clojure]], [[Lua (programming language)|Lua]], [[Perl]], [[Python (programming language)|Python]], [[Ruby (programming language)|Ruby]], and [[JavaScript]], the OR operator (typically <code>||</code> or <code>or</code>) has the same behavior as the above: returning its first operand if it would evaluate to true in a boolean environment, and otherwise evaluating and returning its second operand. When the left hand side is true, the right hand side is not even evaluated; it is \"[[Short-circuit evaluation|short-circuited]].\"\n\n==See also==\n*'''[[?:|<code>?:</code>]]''' or [[conditional operator]], when used as a [[ternary operator]]\n*[[Null coalescing operator]], <code>??</code> or <code>//</code> operator\n*[[Safe navigation operator]], often <code>?.</code>\n*[[Spaceship operator]] <code>&lt;=&gt;</code>\n*[[Option type]]\n\n==References==\n{{reflist}}\n\n[[Category:Operators (programming)]]\n[[Category:Binary operations]]\n[[Category:Conditional constructs]]\n[[Category:Articles with example code]]"
    },
    {
      "title": "Ext functor",
      "url": "https://en.wikipedia.org/wiki/Ext_functor",
      "text": "In [[mathematics]], the '''Ext functors''' are the [[derived functor]]s of the [[Hom functor]]. Along with the [[Tor functor]], Ext is one of the core concepts of [[homological algebra]], in which ideas from [[algebraic topology]] are used to define invariants of algebraic structures. The [[group cohomology|cohomology of groups]], [[Lie algebra cohomology|Lie algebra]]s, and [[Hochschild cohomology|associative algebra]]s can all be defined in terms of Ext. The name comes from the fact that the first Ext group Ext<sup>1</sup> classifies [[group extension|extensions]] of one [[module (mathematics)|module]] by another.\n\nIn the special case of [[abelian group]]s, Ext was introduced by [[Reinhold Baer]] (1934). It was named by [[Samuel Eilenberg]] and [[Saunders MacLane]] (1942), and applied to topology (the [[universal coefficient theorem for cohomology]]). For modules over any [[ring (mathematics)|ring]], Ext was defined by [[Henri Cartan]] and Eilenberg in their 1956 book ''Homological Algebra''.<ref>Weibel (1999); Cartan & Eilenberg (1956), section VI.1.</ref>\n\n==Definition==\nLet ''R'' be a ring and let ''R''-Mod be the [[category (mathematics)|category]] of modules over ''R''. (One can take this to mean either left ''R''-modules or right ''R''-modules.) For a fixed ''R''-module ''A'', let ''T''(''B'') = Hom<sub>''R''</sub>(''A'', ''B'') for ''B'' in ''R''-Mod. (Here Hom<sub>''R''</sub>(''A'', ''B'') is the abelian group of ''R''-linear maps from ''A'' to ''B''; this is an ''R''-module if ''R'' is [[commutative ring|commutative]].) This is a [[left exact functor]] from ''R''-Mod to the [[category of abelian groups]] Ab, and so it has right [[derived functor]]s ''R<sup>i</sup>T''. The Ext groups are the abelian groups defined by\n\n:<math>\\operatorname{Ext}_R^i(A,B)=(R^iT)(B),</math>\n\nfor an [[integer]] ''i''. By definition, this means: take any [[injective resolution]]\n\n:<math>0 \\to B \\to I^0 \\to I^1 \\to \\cdots,</math>\n\nremove the term ''B'', and form the [[cochain complex]]:\n\n:<math>0 \\to \\operatorname{Hom}_R(A,I^0) \\to \\operatorname{Hom}_R(A,I^1) \\to \\cdots.</math>\n\nFor each integer ''i'', Ext{{supsub|''i''|''R''}}(''A'', ''B'') is the [[chain complex|cohomology]] of this complex at position ''i''. It is zero for ''i'' negative. For example, Ext{{supsub|0|''R''}}(''A'', ''B'') is the [[kernel (linear algebra)|kernel]] of the map Hom<sub>''R''</sub>(''A'', ''I''<sup>0</sup>) → Hom<sub>''R''</sub>(''A'', ''I''<sup>1</sup>), which is [[isomorphic]] to Hom<sub>''R''</sub>(''A'', ''B'').\n\nAn alternative definition uses the functor ''G''(''A'')=Hom<sub>''R''</sub>(''A'', ''B''), for a fixed ''R''-module ''B''. This is a [[Covariance and contravariance of functors|contravariant]] functor, which can be viewed as a left exact functor from the [[opposite category]] (''R''-Mod)<sup>op</sup> to Ab. The Ext groups are defined as the right derived functors ''R<sup>i</sup>G'':\n\n:<math>\\operatorname{Ext}_R^i(A,B)=(R^iG)(A).</math>\n\nThat is, choose any [[projective resolution]]\n\n:<math>\\cdots \\to P_1 \\to P_0 \\to A \\to 0, </math>\n\nremove the term ''A'', and form the cochain complex:\n\n:<math>0\\to \\operatorname{Hom}_R(P_0,B)\\to \\operatorname{Hom}_R(P_1,B) \\to \\cdots.</math>\n\nThen Ext{{supsub|''i''|''R''}}(''A'', ''B'') is the cohomology of this complex at position ''i''.\n\nCartan and Eilenberg showed that these constructions are independent of the choice of projective or injective resolution, and that both constructions yield the same Ext groups.<ref>Weibel (1994), sections 2.4 and 2.5 and Theorem 2.7.6.</ref> Moreover, for a fixed ring ''R'', Ext is a functor in each variable (contravariant in ''A'', covariant in ''B'').\n\nFor a commutative ring ''R'' and ''R''-modules ''A'' and ''B'', Ext{{supsub|''i''|''R''}}(''A'', ''B'') is an ''R''-module (using that Hom<sub>''R''</sub>(''A'', ''B'') is an ''R''-module in this case). For a non-commutative ring ''R'', Ext{{supsub|''i''|''R''}}(''A'', ''B'') is only an abelian group, in general. If ''R'' is an [[algebra over a ring|algebra]] over a ring ''S'' (which means in particular that ''S'' is commutative), then Ext{{supsub|''i''|''R''}}(''A'', ''B'') is at least an ''S''-module.\n\n==Properties of Ext==\nHere are some of the basic properties and computations of Ext groups.<ref>Weibel (1994), Chapters 2 and 3.</ref>\n\n*Ext{{supsub|0|''R''}}(''A'', ''B'') ≅ Hom<sub>''R''</sub>(''A'', ''B'') for any ''R''-modules ''A'' and ''B''.\n\n*Ext{{su|b=''R''|p=''i''}}(''A'', ''B'') = 0 for all ''i'' > 0 if the ''R''-module ''A'' is [[projective module|projective]] (for example, [[free module|free]]) or if ''B'' is [[injective module|injective]].\n\n*The converses also hold:\n**If Ext{{su|b=''R''|p=1}}(''A'', ''B'') = 0 for all ''B'', then ''A'' is projective (and hence Ext{{su|b=''R''|p=''i''}}(''A'', ''B'') = 0 for all ''i'' > 0).\n**If Ext{{su|b=''R''|p=1}}(''A'', ''B'') = 0 for all ''A'', then ''B'' is injective (and hence Ext{{su|b=''R''|p=''i''}}(''A'', ''B'') = 0 for all ''i'' > 0).\n\n*<math>\\operatorname{Ext}^i_{\\Z}(A,B) = 0</math> for all ''i'' ≥ 2 and all abelian groups ''A'' and ''B''.<ref>Weibeil (1994), Lemma 3.3.1.</ref>\n\n*If ''R'' is a commutative ring and ''u'' in ''R'' is not a [[zero divisor]], then\n::<math>\\operatorname{Ext}_R^i(R/(u),B)\\cong\\begin{cases} B[u] & i=0\\\\ B/uB & i=1\\\\ 0 &\\text{otherwise,}\\end{cases}</math>\n:for any ''R''-module ''B''. Here ''B''[''u''] denotes the ''u''-torsion subgroup of ''B'', {''x'' ∈ ''B'': ''ux'' = 0}. Taking ''R'' to be the ring <math>\\Z</math> of integers, this calculation can be used to compute <math>\\operatorname{Ext}^1_{\\Z}(A,B)</math> for any [[finitely generated abelian group]] ''A''.\n\n*Generalizing the previous example, one can compute Ext groups when the first module is the quotient of a commutative ring by any [[regular sequence]], using the [[Koszul complex]].<ref>Weibel (1994), section 4.5.</ref> For example, if ''R'' is the [[polynomial ring]] ''k''[''x''<sub>1</sub>,...,''x''<sub>''n''</sub>] over a field ''k'', then Ext{{supsub|*|''R''}}(''k'',''k'') is the [[exterior algebra]] ''S'' over ''k'' on ''n'' generators in Ext<sup>1</sup>. Moreover, Ext{{supsub|*|''S''}}(''k'',''k'') is the polynomial ring ''R''; this is an example of [[Koszul duality]].\n\n*By the general properties of derived functors, there are two basic [[exact sequence]]s for Ext.<ref>Weibel (1994), Definition 2.1.1.</ref> First, a [[short exact sequence]] 0 → ''K'' → ''L'' → ''M'' → 0 of ''R''-modules induces a long exact sequence of the form\n::<math>0 \\to \\mathrm{Hom}_R(A,K) \\to \\mathrm{Hom}_R(A,L) \\to \\mathrm{Hom}_R(A,M) \\to \\mathrm{Ext}^1_R(A,K) \\to \\mathrm{Ext}^1_R(A,L) \\to \\cdots,</math>\n:for any ''R''-module ''A''. Also, a short exact sequence 0 → ''K'' → ''L'' → ''M'' → 0 induces a long exact sequence of the form\n::<math>0 \\to \\mathrm{Hom}_R(M,B) \\to \\mathrm{Hom}_R(L,B) \\to \\mathrm{Hom}_R(K,B) \\to \\mathrm{Ext}^1_R(M,B) \\to \\mathrm{Ext}^1_R(L,B) \\to \\cdots,</math>\n:for any ''R''-module ''B''.\n\n*Ext takes [[direct sum of modules|direct sums]] (possibly infinite) in the first variable and [[direct product#Direct product of modules|product]]s in the second variable to products.<ref>Weibel (1994), Proposition 3.3.4.</ref> That is:\n::<math>\\begin{align}\n\\operatorname{Ext}^i_R \\left(\\bigoplus_\\alpha M_\\alpha,N \\right) &\\cong\\prod_\\alpha \\operatorname{Ext}^i_R (M_\\alpha,N) \\\\\n\\operatorname{Ext}^i_R \\left(M,\\prod_\\alpha N_\\alpha \\right ) &\\cong\\prod_\\alpha \\operatorname{Ext}^i_R (M,N_\\alpha)\n\\end{align}</math>\n\n* Let ''A'' be a finitely generated module over a commutative [[Noetherian ring]] ''R''. Then Ext commutes with [[localization of a ring|localization]], in the sense that for every [[multiplicatively closed set]] ''S'' in ''R'', every ''R''-module ''B'', and every integer ''i'',<ref>Weibel (1994), Lemma 3.3.8.</ref>\n::<math>S^{-1} \\operatorname{Ext}_R^i(A, B) \\cong \\operatorname{Ext}_{S^{-1} R}^i \\left (S^{-1} A, S^{-1} B \\right ).</math>\n\n==Ext and extensions== <!-- \"Extension of modules\" redirects here -->\n===Equivalence of extensions===\n\nThe Ext groups derive their name from their relation to extensions of modules. Given ''R''-modules ''A'' and ''B'', an '''extension of ''A'' by ''B''''' is a short exact sequence of ''R''-modules\n\n:<math>0\\to B\\to E\\to A\\to 0.</math>\n\nTwo extensions\n\n:<math>0\\to B\\to E\\to A\\to 0</math>\n:<math>0\\to B\\to E' \\to A\\to 0</math>\n\nare said to be '''equivalent''' (as extensions of ''A'' by ''B'') if there is a [[commutative diagram]]:\n\n:[[Image:EquivalenceOfExtensions.png]]\n\nNote that the [[Five lemma]] implies that the middle arrow is an isomorphism. An extension of ''A'' by ''B'' is called '''split''' if it is equivalent to the '''trivial extension'''\n\n:<math>0\\to B\\to A\\oplus B\\to A\\to 0.</math>\n\nThere is a one-to-one correspondence between [[equivalence class]]es of extensions of ''A'' by ''B'' and elements of Ext{{supsub|1|''R''}}(''A'', ''B'').<ref>Weibel (1994), Theorem 3.4.3.</ref> The trivial extension corresponds to the zero element of Ext{{supsub|1|''R''}}(''A'', ''B'').\n\n===The Baer sum of extensions===\nThe '''Baer sum''' is an explicit description of the abelian group structure on Ext{{supsub|1|''R''}}(''A'', ''B''), viewed as the set of equivalence classes of extensions of ''A'' by ''B''.<ref>Weibel (1994), Corollary 3.4.5.</ref> Namely, given two extensions\n\n:<math>0\\to B\\xrightarrow[f]{} E \\xrightarrow[g]{} A\\to 0</math>\n\nand\n\n:<math>0\\to B\\xrightarrow[f']{} E'\\xrightarrow[g']{} A\\to 0,</math>\n\nfirst form the [[Pullback (category theory)|pullback]] over <math>A</math>,\n\n:<math>\\Gamma = \\left\\{ (e, e') \\in E \\oplus E' \\; | \\; g(e) = g'(e')\\right\\}.</math>\n\nThen form the [[quotient module]]\n\n:<math>Y = \\Gamma / \\{(f(b), -f'(b)) \\;|\\;b \\in B\\}.</math>\n\nThe Baer sum of ''E'' and ''E′'' is the extension\n\n:<math>0\\to B\\to Y\\to A\\to 0,</math>\n\nwhere the first map is <math>b \\mapsto [(f(b), 0)] = [(0, f'(b))]</math> and the second is <math>(e, e') \\mapsto g(e) = g'(e')</math>.\n\n[[Up to]] equivalence of extensions, the Baer sum is commutative and has the trivial extension as identity element. The negative of an extension 0 → ''B'' → ''E'' → ''A'' → 0 is the extension involving the same module ''E'', but with the homomorphism ''E'' → ''A'' replaced by its negative.\n\n==Construction of Ext in abelian categories==\n[[Nobuo Yoneda]] defined the abelian groups Ext{{su|b='''C'''|p=''n''}}(''A'', ''B'') for objects ''A'' and ''B'' in any [[abelian category]] '''C'''; this agrees with the definition in terms of resolutions if '''C''' has [[projective object#Enough projectives|enough projectives]] or [[injective object#Enough injectives and injective hulls|enough injectives]]. First, Ext{{supsub|0|'''C'''}}(''A'',''B'') = Hom<sub>'''C'''</sub>(''A'', ''B''). Next, Ext{{su|b='''C'''|p=1}}(''A'', ''B'') is the set of equivalence classes of extensions of ''A'' by ''B'', forming an abelian group under the Baer sum. Finally, the higher Ext groups Ext{{su|b='''C'''|p=''n''}}(''A'', ''B'') are defined as equivalence classes of ''n-extensions'', which are exact sequences\n\n:<math>0\\to B\\to X_n\\to\\cdots\\to X_1\\to A\\to 0,</math>\n\nunder the [[equivalence relation]] generated by the relation that identifies two extensions\n\n:<math>\\begin{align}\n\\xi : 0 &\\to B\\to X_n\\to\\cdots\\to X_1\\to A\\to 0 \\\\\n\\xi': 0 &\\to B\\to X'_n\\to\\cdots\\to X'_1\\to A\\to 0\n\\end{align}</math>\n\nif there are maps <math>X_m \\to X'_m</math> for all ''m'' in {1, 2, ..., ''n''} so that every resulting [[Commutative diagram|square commutes]], that is, if there is a [[chain map]] ξ → ξ' which is the identity on ''A'' and ''B''.\n\nThe Baer sum of two ''n''-extensions as above is formed by letting <math>X''_1</math> be the [[Pullback (category theory)|pullback]] of <math>X_1</math> and <math>X'_1</math> over ''A'', and <math>X''_n</math> be the [[Pushout (category theory)|pushout]] of <math>X_n</math> and <math>X'_n</math> under ''B''.<ref>Weibel (1994), Vists 3.4.6. Some minor corrections are in the [http://www.math.rutgers.edu/~weibel/Hbook.errors.edition2.pdf errata].</ref> Then the Baer sum of the extensions is\n\n:<math>0\\to B\\to X''_n\\to X_{n-1}\\oplus X'_{n-1}\\to\\cdots\\to X_2\\oplus X'_2\\to X''_1\\to A\\to 0.</math>\n\n==The derived category and the Yoneda product==\nAn important point is that Ext groups in an abelian category '''C''' can be viewed as sets of morphisms in a category associated to '''C''', the [[derived category]] ''D''('''C''').<ref>Weibel (1994), sections 10.4 and 10.7; Gelfand & Manin (2003), Chapter III.</ref> The objects of the derived category are complexes of objects in '''C'''. Specifically, one has\n\n:<math>\\operatorname{Ext}^i_{\\mathbf C}(A,B) = \\operatorname{Hom}_{D({\\mathbf C})}(A,B[i]),</math>\n\nwhere an object of '''C''' is viewed as a complex concentrated in degree zero, and [''i''] means shifting a complex ''i'' steps to the left. From this interpretation, there is a [[bilinear map]], sometimes called the [[Yoneda product]]:\n\n:<math>\\operatorname{Ext}^i_{\\mathbf C}(A,B) \\times \\operatorname{Ext}^j_{\\mathbf C}(B,C) \\to \\operatorname{Ext}^{i+j}_{\\mathbf C}(A,C),</math>\n\nwhich is simply the composition of morphisms in the derived category.\n\nThe Yoneda product can also be described in more elementary terms. For ''i'' = ''j'' = 0, the product is the composition of maps in the category '''C'''. In general, the product can be defined by splicing together two Yoneda extensions.\n\nAlternatively, the Yoneda product can be defined in terms of resolutions. (This is close to the definition of the derived category.) For example, let ''R'' be a ring, with ''R''-modules ''A'', ''B'', ''C'', and let ''P'', ''Q'', and ''T'' be projective resolutions of ''A'', ''B'', ''C''. Then Ext{{supsub|''i''|''R''}}(''A'',''B'') can be identified with the group of [[chain homotopy]] classes of chain maps ''P'' → ''Q''[''i'']. The Yoneda product is given by composing chain maps:\n\n:<math>P\\to Q[i]\\to T[i+j].</math>\n\nBy any of these interpretations, the Yoneda product is associative. As a result, <math>\\operatorname{Ext}^*_R(A,A)</math> is a [[graded ring]], for any ''R''-module ''A''. For example, this gives the ring structure on [[group cohomology]] <math>H^*(G, \\Z),</math> since this can be viewed as <math>\\operatorname{Ext}^*_{\\Z[G]}(\\Z,\\Z)</math>. Also by associativity of the Yoneda product: for any ''R''-modules ''A'' and ''B'', <math>\\operatorname{Ext}^*_R(A,B)</math> is a module over <math>\\operatorname{Ext}^*_R(A,A)</math>.\n\n==Important special cases==\n\n*[[Group cohomology]] is defined by <math>H^*(G,M)=\\operatorname{Ext}_{\\Z[G]}^*(\\Z, M)</math>, where ''G'' is a group, ''M'' is a [[group representation|representation]] of ''G'' over the integers, and <math>\\Z[G]</math> is the [[group ring]] of ''G''.\n\n*For an [[algebra over a field|algebra]] ''A'' over a field ''k'' and an ''A''-[[bimodule]] ''M'', [[Hochschild cohomology]] is defined by\n\n::<math>HH^*(A,M)=\\operatorname{Ext}^*_{A\\otimes_k A^{\\text{op}}} (A, M).</math>\n\n*[[Lie algebra cohomology]] is defined by <math>H^*(\\mathfrak g,M)=\\operatorname{Ext}^*_{U\\mathfrak g}(k,M)</math>, where <math>\\mathfrak g</math> is a [[Lie algebra]] over a commutative ring ''k'', ''M'' is a <math>\\mathfrak g</math>-module, and <math>U\\mathfrak g</math> is the [[universal enveloping algebra]].\n\n*For a [[topological space]] ''X'', [[sheaf cohomology]] can be defined as <math>H^*(X, A) = \\operatorname{Ext}^*(\\Z_X, A).</math> Here Ext is taken in the abelian category of [[sheaf (mathematics)|sheaves]] of abelian groups on ''X'', and <math>\\Z_X</math> is the sheaf of [[locally constant]] <math>\\Z</math>-valued functions.\n\n*For a commutative Noetherian [[local ring]] ''R'' with residue field ''k'', <math>\\operatorname{Ext}^*_R(k,k)</math> is the universal enveloping algebra of a [[graded Lie algebra]] π*(''R'') over ''k'', known as the '''homotopy Lie algebra''' of ''R''. (To be precise, when ''k'' has [[characteristic of a field|characteristic]] 2, π*(''R'') has to be viewed as an \"adjusted Lie algebra\".<ref>Sjödin (1980), Notation 14.</ref>) There is a natural homomorphism of graded Lie algebras from the [[André–Quillen cohomology]] ''D''*(''k''/''R'',''k'') to π*(''R''), which is an isomorphism if ''k'' has characteristic zero.<ref>Avramov (2010), section 10.2.</ref>\n\n==See also==\n*[[global dimension]]\n*[[bar resolution]]\n*[[Grothendieck group#Grothendieck group and extensions|Grothendieck group]]\n*[[Grothendieck local duality]]\n\n==Notes==\n{{reflist|30em}}\n\n==References==\n*{{Citation|author1-last=Avramov | author1-first=Luchezar | author1-link=Luchezar L. Avramov | chapter=Infinite free resolutions | title=Six lectures on commutative algebra | pages=1–108 | publisher=[[Birkhäuser]] | year=2010 | isbn=978-3-7643-5951-5 | doi=10.1007/978-3-0346-0329-4_1 | mr=2641236}}\n*{{Citation|author1-last=Baer | author1-first=Reinhold | author1-link=Reinhold Baer | title=Erweiterung von Gruppen und ihren Isomorphismen | journal=[[Mathematische Zeitschrift]] | volume=38 | issue=1 | year=1934 | pages=375–416 | doi=10.1007/BF01170643 | zbl= 0009.01101}}\n*{{Citation|author1-last=Cartan | author1-first=Henri | author1-link=Henri Cartan | author2-last=Eilenberg | author2-first=Samuel | author2-link=Samuel Eilenberg | title=Homological algebra | origyear=1956 | year=1999 | publisher=[[Princeton University Press]] | location=Princeton | mr=0077480 | isbn=0-691-04991-2}}\n*{{Citation|author1-last=Eilenberg | author1-first=Samuel | author1-link=Samuel Eilenberg | author2-last=MacLane | author2-first= Saunders | author2-link=Saunders MacLane | title=Group extensions and homology | journal=[[Annals of Mathematics]] | volume=43 | issue=4 | year=1942 | pages=757–931 | doi=10.2307/1968966 | mr=0007108| jstor=1968966 }}\n*{{Citation|last1=Gelfand | first1=Sergei I. | last2=Manin | first2=Yuri Ivanovich | author2-link= Yuri Ivanovich Manin | title= Methods of homological algebra | publisher=[[Springer-Verlag]] | location=Berlin, New York | isbn=978-3-540-43583-9 | doi=10.1007/978-3-662-12492-5 | mr=1950475 | year= 2003}}\n*{{Citation | author1-last=Sjödin | author1-first=Gunnar | title=Hopf algebras and derivations | journal=[[Journal of Algebra]] | volume=64 |year=1980 | pages=218–229 | doi=10.1016/0021-8693(80)90143-X |mr=0575792}} \n*{{Weibel IHA}}\n*{{Citation|author1-last=Weibel | author1-first=Charles A. | author1-link=Charles Weibel | chapter=History of homological algebra | title=History of topology | pages=797–836 | publisher=North-Holland | location=Amsterdam | year=1999 | mr=1721123 | isbn=9780444823755 | chapter-url= http://sites.math.rutgers.edu/~weibel/HA-history.pdf}}\n\n[[Category:Homological algebra]]\n[[Category:Binary operations]]"
    },
    {
      "title": "Frölicher–Nijenhuis bracket",
      "url": "https://en.wikipedia.org/wiki/Fr%C3%B6licher%E2%80%93Nijenhuis_bracket",
      "text": "In [[mathematics]], the '''Frölicher–Nijenhuis bracket''' is an extension of the [[Lie bracket of vector fields|Lie bracket]] of [[vector fields]] to [[vector-valued differential form]]s on a [[differentiable manifold]].\n\nIt is useful in the study of [[connection (mathematics)|connections]], notably the [[Ehresmann connection]], as well as in the more general study of [[projection (linear algebra)|projections]] in the [[tangent bundle]].\nIt was introduced by  [[Alfred Frölicher]] and [[Albert Nijenhuis]] (1956) and is related to the work of [[Jan Arnoldus Schouten|Schouten]] (1940).\n\nIt is related to but not the same as the [[Nijenhuis–Richardson bracket]] and the [[Schouten–Nijenhuis bracket]].\n\n==Definition==\nLet Ω*(''M'') be the [[sheaf (mathematics)|sheaf]] of [[exterior algebra]]s of [[differential form]]s on a [[smooth manifold]] ''M''.  This is a [[graded algebra]] in which forms are graded by degree:\n:<math>\\Omega^*(M) = \\bigoplus_{k=0}^\\infty \\Omega^k(M).</math>\nA [[graded derivation]] of degree ℓ is a mapping\n:<math>D:\\Omega^*(M)\\to\\Omega^{*+l}(M)</math>\nwhich is linear with respect to constants and satisfies\n:<math>D(\\alpha\\wedge\\beta) = D(\\alpha)\\wedge\\beta + (-1)^{\\ell\\deg(\\alpha)}\\alpha\\wedge D(\\beta).</math>\nThus, in particular, the [[interior product]] with a vector defines a graded derivation of degree ℓ&nbsp;=&nbsp;&minus;1, whereas the [[exterior derivative]] is a graded derivation of degree ℓ&nbsp;=&nbsp;1.\n\nThe vector space of all derivations of degree ℓ is denoted by Der<sub>ℓ</sub>Ω*(''M'').  The direct sum of these spaces is a [[graded vector space]] whose homogeneous components consist of all graded derivations of a given degree; it is denoted\n:<math>\\mathrm{Der}\\, \\Omega^*(M) = \\bigoplus_{k=-\\infty}^\\infty \\mathrm{Der}_k\\, \\Omega^*(M).</math>\nThis forms a [[graded Lie algebra#graded Lie superalgebras|graded Lie superalgebra]] under the anticommutator of derivations defined on homogeneous derivations ''D''<sub>1</sub> and ''D''<sub>2</sub> of degrees ''d''<sub>1</sub> and ''d''<sub>2</sub>, respectively, by\n:<math>[D_1,D_2] = D_1\\circ D_2 - (-1)^{d_1d_2}D_2\\circ D_1.</math>\n\nAny [[vector-valued differential form]] ''K'' in Ω<sup>''k''</sup>(''M'',&nbsp;T''M'') with values in the [[tangent bundle]] of ''M'' defines a graded derivation of degree ''k''&nbsp;&minus;&nbsp;1, denoted by ''i''<sub>''K''</sub>, and called the insertion operator.  For ω&nbsp;∈&nbsp;Ω<sup>ℓ</sup>(''M''),\n:<math>i_K\\,\\omega(X_1,\\dots,X_{k+\\ell-1})=\\frac{1}{k!(\\ell-1)!}\\sum_{\\sigma\\in{S}_{k+\\ell-1}}\\textrm{sign}\\,\\sigma \\cdot\n\\omega(K(X_{\\sigma(1)},\\dots,X_{\\sigma(k)}),X_{\\sigma(k+1)},\\dots,X_{\\sigma(k+\\ell-1)})\n</math>\nThe [[Lie derivative#Nijenhuis–Lie derivative|Nijenhuis–Lie derivative]] along ''K''&nbsp;∈&nbsp;Ω<sup>k</sup>(''M'',&nbsp;T''M'') is defined by\n:<math>\\mathcal{L}_K = [d,i_K] =d\\,{\\circ}\\,  i_K-(-1)^{k-1}i_K{\\circ}\\, d</math>\nwhere ''d'' is the exterior derivative and ''i''<sub>K</sub> is the insertion operator.\n\nThe Frölicher–Nijenhuis bracket is defined to be the unique vector-valued differential form\n\n:<math>[\\cdot, \\cdot]_{FN} : \\Omega^k(M,\\mathrm{T}M) \\times \\Omega^\\ell(M,\\mathrm{T}M) \\to \\Omega^{k+\\ell}(M,\\mathrm{T}M) : (K, L) \\mapsto [K, L]_{FN}</math> \nsuch that\n\n:<math>\\mathcal{L}_{[K, L]_{FN}} = [\\mathcal{L}_K, \\mathcal{L}_L].</math>\n\nHence,\n\n:<math>\n[K, L]_{FN}=-(-1)^{kl}[L,K]_{FN}.\n</math>\n\nIf ''k''&nbsp;=&nbsp;0, so that ''K''&nbsp;∈&nbsp;Ω<sup>0</sup>(''M'',&nbsp;T''M'')\nis a vector field, the usual homotopy formula for the Lie derivative is recovered\n:<math>\\mathcal{L}_K = [d,i_K] =d \\,{\\circ}\\, i_K+i_K \\,{\\circ}\\, d.</math>\n\nIf ''k''=''ℓ''=1, so that ''K,L''&nbsp;∈&nbsp;Ω<sup>1</sup>(''M'',&nbsp;T''M''),\none has for any vector fields ''X'' and ''Y''\n:<math>\n[K, L]_{FN}(X,Y) = [KX, LY]+[LX, KY]+(KL+LK)[X,Y]-K([LX,Y]+[X, LY])-L([KX,Y]+[X, KY]).\n</math>\n\nIf ''k''=0 and ''ℓ''=1, so that ''K=Z''∈&nbsp;Ω<sup>0</sup>(''M'',&nbsp;T''M'') is a vector field and ''L''&nbsp;∈&nbsp;Ω<sup>1</sup>(''M'',&nbsp;T''M''), one has for any vector field ''X''\n:<math>\n[Z, L]_{FN}(X) = [Z, LX]-L[Z,X].\n</math>\n\nAn explicit formula for the Frölicher–Nijenhuis bracket of <math>\\phi\\otimes X</math> and <math>\\psi\\otimes Y</math> (for forms φ and ψ and vector fields ''X'' and ''Y'') is given by\n:<math>\\left.\\right.[\\phi \\otimes X,\\psi \\otimes Y]_{FN} = \\phi\\wedge\\psi\\otimes [X,Y] + \\phi\\wedge\\mathcal{L}_X \\psi\\otimes Y - \\mathcal{L}_Y \\phi\\wedge\\psi \\otimes  X +(-1)^{\\deg(\\phi)}(d\\phi \\wedge i_X(\\psi)\\otimes Y +i_Y(\\phi) \\wedge d\\psi \\otimes X).</math>\n\n==Derivations of the ring of forms==\n\nEvery derivation of Ω<sup>*</sup>(''M'') can be written as \n:<math>i_L + \\mathcal{L}_K</math>\nfor unique elements ''K'' and ''L'' of  Ω<sup>*</sup>(''M'', T''M''). The Lie bracket of these derivations is given as follows.\n*The derivations of the form <math>\\mathcal{L}_K</math> form the Lie superalgebra of all derivations commuting with ''d''. The bracket is given by \n::<math>[\\mathcal{L}_{K_1},\\mathcal{L}_{K_2}]= \\mathcal{L}_{[K_1,K_2]}</math> \n:where the bracket on the right is the Frölicher–Nijenhuis bracket. In particular the Frölicher–Nijenhuis bracket defines a [[graded Lie algebra]] structure on <math>\\Omega(M,\\mathrm{T}M)</math>, which extends the [[Lie bracket of vector fields|Lie bracket]] of [[vector field]]s.\n*The derivations of the form <math>i_L</math> form the Lie superalgebra of all derivations vanishing on functions Ω<sup>0</sup>(''M''). The bracket is given by \n::<math>[i_{L_1},i_{L_2}]= i_{[L_1,L_2]^\\land}</math> \n:where the bracket on the right is the  [[Nijenhuis–Richardson bracket]].\n*The bracket of derivations of different types is given by\n::<math>[\\mathcal{L}_{K}, i_L]= i_{[K,L]} - (-1)^{kl}\\mathcal{L}_{i_LK}</math>\n: for ''K'' in &Omega;<sup>k</sup>(''M'', T''M''), ''L'' in &Omega;<sup>l+1</sup>(''M'', T''M'').\n\n==Applications==\nThe [[Nijenhuis tensor]] of an [[almost complex structure]] ''J'', is the Frölicher–Nijenhuis bracket of ''J'' with itself. An almost complex structure is a complex structure if and only if the Nijenhuis tensor is zero.\n\nWith the Frölicher–Nijenhuis bracket it is possible to define the [[curvature]] and [[cocurvature]] of a vector-valued 1-form which is a [[projection (mathematics)|projection]]. This generalizes the concept of the curvature of a [[Connection (mathematics)|connection]].\n\nThere is a common generalization of the Schouten–Nijenhuis bracket and the Frölicher–Nijenhuis bracket; for details see the article on the [[Schouten–Nijenhuis bracket]].\n\n==References==\n*{{citation\n | last1 = Frölicher | first1 = A.\n | last2 = Nijenhuis | first2 = A. | author2-link = Albert Nijenhuis\n | journal = [[Indagationes Mathematicae]]\n | pages = 338–360\n | title = Theory of vector valued differential forms. Part I.\n | volume = 18\n | year = 1956}}.\n*{{citation\n | last1 = Frölicher | first1 = A.\n | last2 = Nijenhuis | first2 = A. | author2-link = Albert Nijenhuis\n | journal = Communicationes Mathematicae Helveticae\n | pages = 227–248\n | title = Invariance of vector form operations under mappings\n | volume = 34\n | year = 1960 | doi=10.1007/bf02565938}}.\n*{{springer|id=F/f120230|author=P. W. Michor|title=Frölicher–Nijenhuis bracket}}\n*{{citation\n | last = Schouten | first = J. A. | author-link = Jan Arnoldus Schouten\n | journal = [[Indagationes Mathematicae]]\n | pages = 449–452\n | title = Über Differentialkonkomitanten zweier kontravarianten Grössen\n | volume = 2\n | year = 1940}}.\n\n{{DEFAULTSORT:Frolicher-Nijenhuis Bracket}}\n[[Category:Bilinear operators]]\n[[Category:Binary operations]]\n[[Category:Differential geometry]]"
    },
    {
      "title": "Function composition",
      "url": "https://en.wikipedia.org/wiki/Function_composition",
      "text": "{{about|function composition in mathematics|function composition in computer science|Function composition (computer science)}}\n{{Use dmy dates|date=May 2019|cs1-dates=y}}\n{{Functions}}\nIn mathematics, '''function composition''' is an operation that takes two [[function (mathematics)|functions]] {{math|''f''}} and {{math|''g''}} and produces a function {{math|''h''}} such that {{math|1=''h''(''x'') = ''g''(''f''(''x''))}}. In this operation, the function {{math|''g''}} is [[function application|applied]] to the result of applying the function {{math|''f''}} to {{math|''x''}}. That is, the functions {{math|''f'' : ''X'' → ''Y''}} and {{math|''g'' : ''Y'' → ''Z''}} are '''composed''' to yield a function that maps {{math|''x''}} in {{math|''X''}} to {{math|''g''(''f''(''x''))}} in {{math|''Z''}}.\n\nIntuitively, if {{math|''z''}} is a function of {{math|''y''}}, and {{math|''y''}} is a function of {{math|''x''}}, then {{math|''z''}} is a function of {{math|''x''}}. The resulting ''composite'' function is denoted {{math|''g'' ∘ ''f'' : ''X'' → ''Z''}}, defined by {{math|1=(''g'' ∘ ''f'' )(''x'') = ''g''(''f''(''x''))}} for all {{math|''x''}} in&nbsp;{{math|''X''}}.<ref group=note>Some authors use {{math|''f'' ∘ ''g'' : ''X'' → ''Z''}}, defined by {{math|1=(''f'' ∘ ''g'' )(''x'') = ''g''(''f''(''x''))}} instead. This is common when a [[postfix notation]] is used, especially if functions are represented by exponents, as, for instance, in the study of [[Group action (mathematics)|group action]]s. See {{citation|first1=John D.|last1=Dixon|first2=Brian|last2=Mortimer|title=Permutation groups|year=1996|publisher=Springer|isbn=0-387-94599-7|page=5}}</ref>\nThe notation {{math|''g'' ∘ ''f''}} is read as \"{{math|''g''}} circle {{math|''f''}} \", \"{{math|''g''}} round {{math|''f''}} \", \"{{math|''g''}} about {{math|''f''}} \", \"{{math|''g''}} composed with {{math|''f''}} \", \"{{math|''g''}} after {{math|''f''}} \", \"{{math|''g''}} following {{math|''f''}} \", \"{{math|''g''}} of {{math|''f''}}\", or \"{{math|''g''}} on {{math|''f''}} \". Intuitively, composing two functions is a chaining process in which the output of the inner function becomes the input of the outer function.\n\nThe composition of functions is a special case of the [[composition of relations]], so all properties of the latter are true of composition of functions.<ref name=\"Velleman2006\">{{cite book|author=Daniel J. Velleman|title=How to Prove It: A Structured Approach|url=https://books.google.com/books?id=sXt-ROLLNHcC&pg=PA232|year=2006|publisher=Cambridge University Press|isbn=978-1-139-45097-3|page=232}}</ref> The composition of functions has some additional properties.\n\n==Examples==\n[[Image:Compfun.svg|thumb|{{math|''g'' ∘ ''f'' }}, the '''composition''' of {{math|''f''}} and {{math|''g''}}. For example, {{math|1=(''g'' ∘ ''f'' )(c) = #}}.]]\n[[File:Example for a composition of two functions.svg|thumb|Concrete example for the composition of two functions.]]\n* Composition of functions on a finite set: If {{math|1=''f'' = {(1, 3), (2, 1), (3, 4), (4, 6)} }}, and {{math|1=''g'' = {(1, 5), (2, 3), (3, 4), (4, 1), (5, 3), (6, 2)} }}, then {{math|1=''g'' ∘ ''f'' = {(1, 4), (2, 5), (3, 1), (4, 2)} }}.\n* Composition of functions on an [[infinite set]]: If {{math|''f'': ℝ → ℝ}} (where {{math|ℝ}} is the set of all [[real number]]s) is given by {{math|1=''f''(''x'') = 2''x'' + 4}} and {{math|''g'': ℝ → ℝ}} is given by {{math|1=''g''(''x'') = ''x''<sup>3</sup>}}, then:\n:{{math|1=(''f'' ∘ ''g'')(''x'') = ''f''(''g''(''x'')) = ''f''(''x''<sup>3</sup>) = 2''x''<sup>3</sup> + 4}}, and\n:{{math|1=(''g'' ∘ ''f'')(''x'') = ''g''(''f''(''x'')) = ''g''(2''x'' + 4) = (2''x'' + 4)<sup>3</sup>}}.\n* If an airplane's elevation at time&nbsp;{{math|''t''}} is given by the function {{math|''h''(''t'')}}, and the oxygen concentration at elevation {{math|''x''}} is given by the function {{math|''c''(''x'')}}, then {{math|(''c'' ∘ ''h'')(''t'')}} describes the oxygen [[concentration]] around the plane at time&nbsp;{{math|''t''}}.\n\n==Properties==\nThe composition of functions is always [[associative]]—a property inherited from the [[composition of relations]].<ref name=\"Velleman2006\"/> That is, if {{math|''f''}}, {{math|''g''}}, and {{math|''h''}} are three functions with suitably chosen [[Domain of a function|domain]]s and [[codomain]]s, then {{math|1=''f'' ∘ (''g'' ∘ ''h'') = (''f'' ∘ ''g'') ∘ ''h''}}, where the parentheses serve to indicate that composition is to be performed first for the parenthesized functions. Since there is no distinction between the choices of placement of parentheses, they may be left off without causing any ambiguity.\n\nIn a strict sense, the composition {{math|1=''g'' ∘ ''f''}} can be built only if {{math|''f''}}'s codomain equals {{math|''g''}}'s domain; in a wider sense it is sufficient that the former is a [[subset]] of the latter.<ref group=note>The strict sense is used, ''e.g.'', in [[category theory]], where a subset relation is modelled explicitly by an [[inclusion function]].</ref>\nMoreover, it is often convenient to tacitly restrict {{math|''f''}}'s domain such that {{math|''f''}} produces only values in {{math|''g''}}'s domain; for example, the composition {{math|1=''g'' ∘ ''f''}} of the functions {{math|''f'' : [[real number|ℝ]] → [[interval (mathematics)#Infinite endpoints|(−∞,+9] ]]}} defined by {{math|1=''f''(''x'') = 9 − ''x''<sup>2</sup>}} and {{math|''g'' : [[interval (mathematics)#Infinite endpoints|[0,+∞)]] → ℝ}} defined by {{math|1=''g''(''x'') = {{radic|''x''}}}} can be defined on the [[interval (mathematics)|interval]] {{math|[−3,+3]}}.\n\n[[Image:Absolute value composition.svg|thumb|upright=1|Compositions of two [[real number|real]] functions, [[absolute value]] and a [[cubic function]], in different orders show a non-commutativity of the composition.]]\nThe functions {{math|''g''}} and {{math|''f''}} are said to [[commutative|commute]] with each other if {{math|1=''g'' ∘ ''f'' = ''f'' ∘ ''g''}}. Commutativity is a special property, attained only by particular functions, and often in special circumstances. For example, {{math|1={{abs|''x''}} + 3 = {{abs|''x'' + 3}}}} only when {{math|''x'' ≥ 0}}. The picture shows another example.\n\nThe composition of one-to-one functions is always [[One-to-one function|one-to-one]]. Similarly, the composition of two onto functions is always [[Onto function|onto]]. It follows that composition of two [[bijection]]s is also a bijection. The [[inverse function]] of a composition (assumed invertible) has the property that {{math|1=(''f'' ∘ ''g'')<sup>−1</sup> = ( ''g''<sup>−1</sup> ∘ ''f'' <sup>−1</sup>)}}.<ref name=\"Rodgers2000\">{{cite book|author=Nancy Rodgers|title=Learning to Reason: An Introduction to Logic, Sets, and Relations|year=2000|publisher=John Wiley & Sons|isbn=978-0-471-37122-9|pages=359–362|url=https://books.google.com/books?id=NuN2Iyqzqp4C&printsec=frontcover#v=onepage&q=composition&f=false}}</ref>\n\n[[Derivative]]s of compositions involving differentiable functions can be found using the [[chain rule]]. [[Higher derivative]]s of such functions are given by [[Faà di Bruno's formula]].\n\n==Composition monoids==\n{{main|Transformation monoid}}\nSuppose one has two (or more) functions {{math|''f'': ''X'' → ''X'',}} {{math|''g'': ''X'' → ''X''}} having the same domain and codomain; these are often called ''[[Transformation (function)|transformation]]''s. Then one can form chains of transformations composed together, such as {{math|''f'' ∘ ''f'' ∘ ''g'' ∘ ''f''}}. Such chains have the [[algebraic structure]] of a [[monoid]], called a [[transformation monoid]] or (much more seldom) a ''composition monoid''.  In general, transformation monoids can have remarkably complicated structure. One particular notable example is the [[de Rham curve]]. The set of ''all'' functions {{math|''f'': ''X'' → ''X''}} is called the [[full transformation semigroup]]<ref>{{cite book|author=Christopher Hollings|title=Mathematics across the Iron Curtain: A History of the Algebraic Theory of Semigroups|url=https://books.google.com/books?id=O9wJBAAAQBAJ&pg=PA334|year=2014|publisher=American Mathematical Society|isbn=978-1-4704-1493-1|page=334}}</ref> or ''symmetric semigroup''<ref name=\"Grillet1995\">{{cite book|author=Pierre A. Grillet|title=Semigroups: An Introduction to the Structure Theory|url=https://books.google.com/books?id=yM544W1N2UUC&pg=PA2|year=1995|publisher=CRC Press|isbn=978-0-8247-9662-4|page=2}}</ref> on&nbsp;{{math|''X''}}. (One can actually define two semigroups depending how one defines the semigroup operation as the left or right composition of functions.<ref name=\"DömösiNehaniv2005\">{{cite book|author1=Pál Dömösi|author2=Chrystopher L. Nehaniv|title=Algebraic Theory of Automata Networks: A Introduction|url=https://books.google.com/books?id=W0i5nfQLOGIC&pg=PA8|year=2005|publisher=SIAM|isbn=978-0-89871-569-9|pages=8}}</ref>)\n\n[[File:Academ Example of similarity with ratio square root of 2.svg|thumb|upright=1.2|The [[Similarity (geometry)|similarity]] that transforms triangle ''EFA'' into triangle ''ATB''  is the composition of a [[Homothetic transformation|homothety]] {{math|''H''}}&nbsp; and a [[Rotation (mathematics)|rotation]]&nbsp;{{math|''R''}}, of&nbsp;which the common&nbsp;centre is&nbsp;''S.&nbsp;'' For&nbsp;example, [[Image (mathematics)|the&nbsp;image]] of&nbsp;''A&nbsp;'' under the rotation&nbsp;{{math|''R''}} is&nbsp;''U'',&nbsp; which may be written&nbsp; {{nowrap|1={{math|''R ''}}(''A'') = ''U.&nbsp;''}} {{nowrap|1=And&nbsp; {{math|''H''}}(''U'') = ''B''&nbsp;}} means that the [[map (mathematics)|mapping]]&nbsp;{{math|''H''}}  transforms {{Nowrap|''U''&nbsp; into ''B.&nbsp;''}} {{nowrap|1=Thus&nbsp; {{math|''H''(''R'' }}(''A'')) = {{math|(''H ∘ R '')}}(''A'') = ''B''}}.]]\nIf the transformations are [[bijective]] (and thus invertible), then the set of all  possible combinations of these functions forms a [[transformation group]]; and one says that the group is [[group generator|generated]] by these functions.  A fundamental result in group theory, [[Cayley's theorem]], essentially says that any group is in fact just a subgroup of a permutation group (up to [[isomorphism]]).<ref name=\"Carter2009\">{{cite book|author=Nathan Carter|title=Visual Group Theory|url=https://books.google.com/books?id=T_o0CnMZecMC&pg=PA95|date=9 April 2009|publisher=MAA|isbn=978-0-88385-757-1|page=95}}</ref>\n\nThe set of all bijective functions {{math|''f'': ''X'' → ''X''}} (called [[permutation]]s) forms a group with respect to the composition operator. This is the [[symmetric group]], also sometimes called the ''composition group''.\n\nIn the symmetric semigroup (of all transformations) one also finds a weaker, non-unique notion of inverse (called a pseudoinverse) because the symmetric semigroup is a [[regular semigroup]].<ref name=\"GanyushkinMazorchuk2008\">{{cite book|author1=Olexandr Ganyushkin|author2=Volodymyr Mazorchuk|title=Classical Finite Transformation Semigroups: An Introduction|url=https://books.google.com/books?id=LC3jxfGEcpYC&pg=PA24|year=2008|publisher=Springer Science & Business Media|isbn=978-1-84800-281-4|page=24}}</ref>\n\n==Functional powers==\n{{main|Iterated function}}\nIf {{math|''Y'' [[subset|⊆]] ''X''}}, then {{math|''f'': ''X''→''Y''}} may compose with itself; this is sometimes denoted as {{math|''f''<sup> 2</sup>}}. That is:\n\n:{{math|1=(''f'' ∘ ''f'')(x) = ''f''(''f''(''x'')) = ''f''&thinsp;<sup>2</sup>(''x'')}}\n\n:{{math|1=(''f'' ∘ ''f'' ∘ ''f'')(x) = ''f''(''f''(''f''(''x''))) = ''f''&thinsp;<sup>3</sup>(''x'')}}\n\n:{{math|1=(''f'' ∘ ''f'' ∘ ''f'' ∘ ''f'')(x) = ''f''(''f''(''f''(''f''(''x'')))) = ''f''&thinsp;<sup>4</sup>(''x'')}}\n\nMore generally, for any [[natural number]] {{math|''n'' ≥ 2}}, the {{math|''n''}}th '''functional [[Exponentiation|power]]''' can be defined inductively by {{math|1=''f''&thinsp;<sup>''n''</sup> = ''f'' ∘ ''f''&thinsp;<sup>''n''−1</sup> = ''f''&thinsp;<sup>''n''−1</sup> ∘ ''f''}}. Repeated composition of such a function with itself is called '''[[iterated function]]'''.\n* By convention, {{math|''f''&thinsp;<sup>0</sup>}} is defined as the identity map on {{math|''f''&thinsp;}}'s domain, {{math|''id''<sub>''X''</sub>}}.\n* If even {{math|1=''Y'' = ''X''}} and {{math|''f'': ''X'' → ''X''}} admits an [[inverse function]] {{math|''f''&thinsp;<sup>−1</sup>}}, negative functional powers {{math|''f''&thinsp;<sup>−''n''</sup>}} are defined for {{math|''n'' > 0}} as the [[additive inverse|negated]] power of the inverse function: {{math|1=''f''&thinsp;<sup>−''n''</sup> = (''f''&thinsp;<sup>−1</sup>)<sup>''n''</sup>}}.\n\n'''Note:''' If {{math|''f''}} takes its values in a [[ring (mathematics)|ring]] (in particular for real or complex-valued {{math|''f''&thinsp;}}), there is a risk of confusion, as {{math|''f''&thinsp;<sup>''n''</sup>}} could also stand for the {{math|''n''}}-fold product of&nbsp;{{math|''f''}}, e.g. {{math|1=''f''&thinsp;<sup>2</sup>(''x'') = ''f''(''x'') · ''f''(''x'')}}. For trigonometric functions, usually the latter is meant, at least for positive exponents. For example, in [[trigonometry]], this superscript notation represents standard [[exponentiation]] when used with [[trigonometric functions]]:\n{{math|1=sin<sup>2</sup>(''x'') = sin(''x'') · sin(''x'')}}.\nHowever, for negative exponents (especially &minus;1), it nevertheless usually refers to the inverse function, e.g., {{math|1=tan<sup>−1</sup> = arctan ≠ 1/tan}}.\n\nIn some cases, when, for a given function {{math|''f''}}, the equation {{math|1=''g'' ∘ ''g'' = ''f''}} has a unique solution {{math|''g''}}, that function can be defined as the [[functional square root]] of {{math|''f''}}, then written as {{math|1=''g'' = ''f''&thinsp;<sup>1/2</sup>}}.\n\nMore generally, when {{math|1=''g''<sup>''n''</sup> = ''f''}} has a unique solution for some natural number {{math|''n'' > 0}}, then {{math|''f''&thinsp;<sup>''m''/''n''</sup>}} can be defined as {{math|''g''<sup>''m''</sup>}}.\n\nUnder additional restrictions,<!---I guess, solvability of g^n = f for all n, and something like [[uniform convergence]] of f^(m/n) for m/n→r, is needed to define f^r for arbitrary r∈ℝ. A citation is needed about that, anyway.---> this idea can be generalized so that the [[iterated function|iteration count]]  becomes a continuous parameter; in this case, such a system is called a [[flow (mathematics)|flow]], specified through solutions of [[Schröder's equation]]. Iterated functions and flows occur naturally in the study of [[fractals]] and [[dynamical systems]].\n\nTo avoid ambiguity, some mathematicians choose to write  {{math|''f'' °<sup>''n''</sup>}} for the ''n''-th iterate  of the function {{mvar|''f''}}.\n\n==Alternative notations==\n\nMany mathematicians, particularly in [[group theory]], omit the composition symbol, writing {{math|''gf''}} for {{math|''g'' ∘ ''f''}}.<ref name=\"Ivanov2009\">{{cite book|author=Oleg A. Ivanov|title=Making Mathematics Come to Life: A Guide for Teachers and Students|url=https://books.google.com/books?id=z7EHBAAAQBAJ&pg=PA217|date=1 January 2009|publisher=American Mathematical Soc.|isbn=978-0-8218-4808-1|pages=217–}}</ref>\n\nIn the mid-20th century, some mathematicians decided that writing \"{{math|''g'' ∘ ''f'' }}\" to mean \"first apply {{math|''f''}}, then apply {{math|''g''}}\" was too confusing and decided to change notations. They write \"{{math|''xf''&thinsp;}}\" for \"{{math|''f''(''x'')}}\" and \"{{math|(''xf'')''g''}}\" for \"{{math|''g''(''f''(''x''))}}\".<ref name=\"Gallier2011\">{{cite book|author=Jean Gallier|authorlink= Jean Gallier |title=Discrete Mathematics|url=https://books.google.com/books?id=HXSjIP0OgCUC&pg=PA118|year=2011|publisher=Springer |isbn=978-1-4419-8047-2|page=118}}</ref> This can be more natural and seem simpler than writing [[prefix notation|functions on the left]] in some areas – in [[linear algebra]], for instance, when {{math|''x''}} is a [[row vector]] and {{math|''f''}} and {{math|''g''}} denote [[matrix (mathematics)|matrices]] and the composition is by [[matrix multiplication]]. This alternative notation is called [[postfix notation]]. The order is important because function composition is not necessarily commutative (e.g. matrix multiplication). Successive transformations applying and composing to the right agrees with the left-to-right reading sequence.\n\nMathematicians who use postfix notation may write \"{{math|''fg''}}\", meaning first apply {{math|''f''}} and then apply {{math|''g''}}, in keeping with the order the symbols occur in postfix notation, thus making the notation \"{{math|''fg''}}\" ambiguous.  Computer scientists may write \"{{math|''f'' ; ''g''}}\" for this,<ref name=\"BarrWells1990\">{{cite book|author1=Michael Barr|author2=Charles Wells|title=Category Theory for Computing Science|url=http://www.math.mcgill.ca/triples/Barr-Wells-ctcs.pdf|year=1998|page=6}} This is the updated and free version of book originally published by Prentice Hall in 1990 as {{isbn|978-0-13-120486-7}}.</ref> thereby disambiguating the order of composition. To distinguish the left composition operator from a text semicolon, in the [[Z notation]] the ⨾  character is used for left [[relation composition]].<ref>ISO/IEC 13568:2002(E), p. 23</ref> Since all functions are  [[Binary relation#Special types of binary relations|binary relations]], it is correct to use the [fat] semicolon for function composition as well (see the article on [[composition of relations]] for further details on this notation).\n\n==Composition operator==\n{{main|Composition operator}}\nGiven a function&nbsp;{{math|''g''}}, the '''composition operator''' {{math|''C''<sub>''g''</sub>}} is defined as that [[Operator (mathematics)|operator]] which maps functions to functions as\n\n::<math>C_g f = f \\circ g.</math>\n\nComposition operators are studied in the field of [[operator theory]].\n\n==In programming languages==\n{{main|Function composition (computer science)}}\nFunction composition appears in one form or another in numerous [[programming language]]s.\n\n==Multivariate functions==\nPartial composition is possible for [[multivariate function]]s. The function resulting when some argument {{math|''x''<sub>''i''</sub>}} of the function {{math|''f''}} is replaced by the function {{math|''g''}} is called a composition of {{math|''f''}} and {{math|''g''}} in some computer engineering contexts, and is denoted {{math|1=''f'' {{!}}<sub>''x''<sub>''i''</sub> = ''g''</sub>}}\n:<math>f|_{x_i = g} = f (x_1, \\ldots, x_{i-1}, g(x_1, x_2, \\ldots, x_n), x_{i+1}, \\ldots, x_n).</math>\n\nWhen {{math|''g''}} is a simple constant {{math|''b''}}, composition degenerates into a (partial) valuation, whose result is also known as [[Restriction (mathematics)|restriction]] or ''co-factor''.<ref>{{cite journal |author=Bryant, R.E. |title=Logic Minimization Algorithms for VLSI Synthesis |journal=IEEE Transactions on Computers|volume=C-35 |issue=8|date=August 1986 |pages=677–691 |doi=10.1109/tc.1986.1676819 |url=http://www.cs.cmu.edu/~bryant/pubdir/ieeetc86.pdf}}\n</ref>\n\n:<math>f|_{x_i = b} = f (x_1, \\ldots, x_{i-1}, b, x_{i+1}, \\ldots, x_n).</math>\n\nIn general, the composition of multivariate functions may involve several other functions as arguments, as in the definition of [[primitive recursive function]]. Given {{math|''f''}}, a {{math|''n''}}-ary function, and {{math|''n''}} {{math|''m''}}-ary functions {{math|''g''<sub>1</sub>, ..., ''g''<sub>''n''</sub>}}, the composition of {{math|''f''}} with {{math|''g''<sub>1</sub>, ..., ''g''<sub>''n''</sub>}}, is the {{math|''m''}}-ary function\n:<math>h(x_1,\\ldots,x_m) = f(g_1(x_1,\\ldots,x_m),\\ldots,g_n(x_1,\\ldots,x_m))</math>.\n\nThis is sometimes called the '''generalized composite''' of ''f'' with {{math|''g''<sub>1</sub>, ..., ''g''<sub>''n''</sub>}}.<ref name=\"Bergman2011\">{{cite book|author=Clifford Bergman|title=Universal Algebra: Fundamentals and Selected Topics|url=https://books.google.com/books?id=QXi3BZWoMRwC&pg=PA79|year=2011|publisher=CRC Press|isbn=978-1-4398-5129-6|pages=79–80}}</ref> The partial composition in only one argument mentioned previously can be instantiated from this more general scheme by setting all argument functions except one to be suitably chosen [[projection function]]s. Note also that {{math|''g''<sub>1</sub>, ..., ''g''<sub>''n''</sub>}} can be seen as a single vector/[[tuple]]-valued function in this generalized scheme, in which case this is precisely the standard definition of function composition.<ref name=\"Tourlakis2012\">{{cite book|author=George Tourlakis|title=Theory of Computation|url=https://books.google.com/books?id=zy3M24m5cykC&pg=PA100|year=2012|publisher=John Wiley & Sons|isbn=978-1-118-31533-0|page=100}}</ref>\n\nA set of finitary [[operation (mathematics)|operation]]s on some base set ''X'' is called a [[clone (algebra)|clone]] if it contains all projections and is closed under generalized composition. Note that a clone generally contains operations of various [[arity|arities]].<ref name=\"Bergman2011\"/> The notion of commutation also finds an interesting generalization in the multivariate case; a function ''f'' of arity ''n'' is said to commute with a function ''g'' of arity ''m'' if ''f'' is a [[homomorphism]] preserving ''g'', and vice versa i.e.:<ref name=\"Bergman2011b\">{{cite book|author=Clifford Bergman|title=Universal Algebra: Fundamentals and Selected Topics|url=https://books.google.com/books?id=QXi3BZWoMRwC&pg=PA90|year=2011|publisher=CRC Press|isbn=978-1-4398-5129-6|pages=90–91}}</ref>\n:<math>f(g(a_{11},\\ldots,a_{1m}),\\ldots,g(a_{n1},\\ldots,a_{nm})) = g(f(a_{11},\\ldots,a_{n1}),\\ldots,f(a_{1m},\\ldots,a_{nm}))</math>.\n\nA unary operation always commutes with itself, but this is not necessarily the case for a binary (or higher arity) operation. A binary (or higher arity) operation that commutes with itself is called [[Medial magma|medial or entropic]].<ref name=\"Bergman2011b\"/>\n\n==Generalizations==\n[[Composition of relations|Composition]] can be generalized to arbitrary [[binary relation]]s.\nIf {{math|''R'' ⊆ ''X'' [[cartesian product|×]] ''Y''}} and {{math|''S'' ⊆ ''Y'' × ''Z''}} are two binary relations, then their composition {{math|''S''∘''R''}} is the relation defined as {{math|{(''x'', ''z'') ∈ ''X'' × ''Z'' : [[existential quantification|∃]]''y'' ∈ ''Y''. (''x'', ''y'') ∈ ''R'' [[logical conjunction|∧]] (''y'', ''z'')  ∈ ''S''}{{void}}}}.\nConsidering a function as a special case of a binary relation (namely [[functional relation]]s), function composition satisfies the definition for relation composition.\n\nThe composition is defined in the same way for [[partial function]]s and Cayley's theorem has its analogue called the [[Wagner-Preston theorem]].<ref>S. Lipscomb, \"Symmetric Inverse Semigroups\", AMS Mathematical Surveys and Monographs (1997), {{isbn|0-8218-0627-0}}, p. xv</ref>\n\nThe [[category of sets]] with functions as [[morphism]]s is the prototypical [[Category (mathematics)|category]]. The axioms of a category are in fact inspired from the properties (and also the definition) of function composition.<ref name=\"HiltonWu1989\">{{cite book|author1=Peter Hilton|author2=Yel-Chiang Wu|title=A Course in Modern Algebra|url=https://books.google.com/books?id=ua5gKZt3R6AC&pg=PA65|year=1989|publisher=John Wiley & Sons|isbn=978-0-471-50405-4|page=65}}</ref> The structures given by composition are axiomatized and generalized in [[category theory]] with the concept of [[morphism]] as the category-theoretical replacement of functions. The reversed order of composition in the formula {{math|1=(''f'' ∘ ''g'')<sup>−1</sup> = (''g''<sup>−1</sup> ∘ ''f'' <sup>−1</sup>)}} applies for [[composition of relations]] using [[converse relation]]s, and thus in [[group theory]]. These structures form [[dagger category|dagger categories]].\n\n==Typography==\n\nThe composition symbol {{math|∘ }} is encoded as {{unichar|2218|ring operator|html=}}; see the [[Degree symbol#Lookalikes|Degree symbol]] article for similar-appearing Unicode characters. In [[TeX]], it is written <code>\\circ</code>.\n\n==See also==\n* [[Combinatory logic]]\n* [[Function composition (computer science)]]\n* [[Functional decomposition]]\n* [[Iterated function]]\n* [[Infinite compositions of analytic functions]]\n* [[Flow (mathematics)]]\n* [[Higher-order function]]\n* [[Cobweb plot]] – a graphical technique for functional composition\n* [[Lambda calculus]]\n* [[Functional square root]]\n* [[Composition ring]], a formal axiomatization of the composition operation\n* [[Random variable#Functions of random variables|Function of random variable]], distribution of a function of a random variable\n\n==Notes==\n{{reflist|group=note}}\n\n==References==\n{{Reflist|30em}}\n\n==External links==\n* {{springer|title=Composite function|id=p/c024260}}\n* \"[http://demonstrations.wolfram.com/CompositionOfFunctions/ Composition of Functions]\" by Bruce Atwood, the [[Wolfram Demonstrations Project]], 2007.\n\n[[Category:Functions and mappings]]\n[[Category:Basic concepts in set theory]]\n[[Category:Binary operations]]"
    },
    {
      "title": "Graph product",
      "url": "https://en.wikipedia.org/wiki/Graph_product",
      "text": "In [[mathematics]], a '''graph product''' is a [[binary operation]] on [[Graph (discrete mathematics)|graph]]s.  Specifically, it is an operation that takes two graphs ''G''<sub>1</sub> and ''G''<sub>2</sub> and produces a graph ''H'' with the following properties:\n* The [[vertex (graph theory)|vertex set]] of ''H'' is the [[Cartesian product]] ''V''(''G''<sub>1</sub>)&nbsp;&times;&nbsp;''V''(''G''<sub>2</sub>), where ''V''(''G''<sub>1</sub>) and ''V''(''G''<sub>2</sub>) are the vertex sets of ''G''<sub>1</sub> and ''G''<sub>2</sub>, respectively.\n* Two vertices (''u''<sub>1</sub>,&nbsp;''u''<sub>2</sub>) and (''v''<sub>1</sub>,&nbsp;''v''<sub>2</sub>) of ''H'' are connected by an [[edge (graph theory)|edge]] if and only if the vertices ''u''<sub>1</sub>, ''u''<sub>2</sub>, ''v''<sub>1</sub>, ''v''<sub>2</sub> satisfy a condition that takes into account the edges of ''G''<sub>1</sub> and ''G''<sub>2</sub>. The graph products differ in exactly which this condition is.\nThe terminology and notation for specific graph products in the literature varies quite a lot; even if the following may be considered standard, readers are advised to check what definition a particular author uses for a graph product, especially in older texts.\n\n== Overview table ==\nThe following table shows the most common graph products, with <math>\\sim</math> denoting &ldquo;is connected by an edge to&rdquo;, and <math>\\not\\sim</math> denoting non-connection.  The operator symbols listed here are by no means standard, especially in older papers.\n{| class=\"wikitable\" style=\"text-align:center\"\n|-\n! Name\n! Condition for <math>(u_1,u_2)\\sim(v_1,v_2)</math>\n! Size (number of edges)<br/><math>\\begin{array}{cc} n_1 = \\vert\\mathrm{V}(G_1)\\vert & n_2 = \\vert\\mathrm{V}(G_2)\\vert \\\\ m_1 = \\vert\\mathrm{E}(G_1)\\vert & m_2 = \\vert\\mathrm{E}(G_2)\\vert \\end{array}</math>\n! Example\n|-\n| [[Cartesian product of graphs|Cartesian product<br/><math>G_1 \\square G_2</math>]]\n| (&nbsp;<math>u_1</math>&nbsp;=&nbsp;<math>v_1</math>&nbsp;and&nbsp;<math>u_2</math>&nbsp;<math>\\sim</math>&nbsp;<math>v_2</math>&nbsp;)<br/>or<br/>\n(&nbsp;<math>u_1</math>&nbsp;<math>\\sim</math>&nbsp;<math>v_1</math>&nbsp;and&nbsp;<math>u_2</math>&nbsp;=&nbsp;<math>v_2</math>&nbsp;)\n| <math>m_2 n_1 + m_1 n_2</math>\n| [[Image:Graph-Cartesian-product.svg|200px]]\n|-\n| [[Tensor product of graphs|Tensor product]]<br/>(Categorical product)<br/><math>G_1 \\times G_2</math>\n| <math>u_1</math>&nbsp;<math>\\sim</math>&nbsp;<math>v_1</math>&nbsp;and &nbsp;<math>u_2</math>&nbsp;<math>\\sim</math>&nbsp;<math>v_2</math>\n| <math>2 m_1 m_2</math>\n| [[Image:Graph-tensor-product.svg|200px]]\n|-\n| [[Lexicographical product of graphs|Lexicographical product]]<br/><math>G_1 \\cdot G_2</math> or <math>G_1[G_2]</math>\n| ''u''<sub>1</sub>&nbsp;&sim;&nbsp;''v''<sub>1</sub><br/> or <br/>(&nbsp;''u''<sub>1</sub>&nbsp;=&nbsp;''v''<sub>1</sub> and ''u''<sub>2</sub>&nbsp;&sim;&nbsp;''v''<sub>2</sub>&nbsp;)\n| <math>m_2 n_1 + m_1 n_2^2</math>\n| [[Image:Graph-lexicographic-product.svg|200px]]\n|-\n| [[Strong product of graphs|Strong product]]<br/>(Normal product, AND product)<br/><math>G_1 \\boxtimes G_2</math>\n| (&nbsp;''u''<sub>1</sub>&nbsp;=&nbsp;''v''<sub>1</sub>&nbsp;and&nbsp;''u''<sub>2</sub>&nbsp;&sim;&nbsp;''v''<sub>2</sub>&nbsp;)<br/>or<br/>(&nbsp;''u''<sub>1</sub>&nbsp;&sim;&nbsp;''v''<sub>1</sub>&nbsp;and&nbsp;''u''<sub>2</sub>&nbsp;=&nbsp;''v''<sub>2</sub>&nbsp;)<br/>or<br/>(&nbsp;''u''<sub>1</sub>&nbsp;&sim;&nbsp;''v''<sub>1</sub>&nbsp;and&nbsp;''u''<sub>2</sub>&nbsp;&sim;&nbsp;''v''<sub>2</sub>&nbsp;)\n| <math>n_1 m_2 + n_2 m_1 + 2 m_1 m_2</math>\n|\n|-\n| [[Co-normal product of graphs|Co-normal product]]<br/>(disjunctive product, OR product)<br/><math>G_1 * G_2</math>\n| ''u''<sub>1</sub>&nbsp;&sim;&nbsp;''v''<sub>1</sub><br/>or<br/>''u''<sub>2</sub>&nbsp;&sim;&nbsp;''v''<sub>2</sub>\n| \n|\n|-\n| [[Modular product of graphs|Modular product]]\n| <math>(u_1 \\sim v_1 \\text{ and } u_2 \\sim v_2)</math><br/>or<br />\n<math>(u_1 \\not\\sim v_1 \\text{ and } u_2 \\not\\sim v_2)</math>\n| \n|\n|-\n| [[Rooted product of graphs|Rooted product]]\n| see article\n| <math>m_2 n_1 + m_1</math>\n| [[Image:Graph-rooted-product.svg|200px]]\n|-\n| [[Zig-zag product]]\n| see article\n| see article\n| see article\n|-\n| [[Replacement product]]\n|\n|\n|\n|-\n| Homomorphic product<ref name=rm12>{{cite journal |arxiv=1212.1724|last1= Roberson|first1= David E.|title= Graph Homomorphisms for Quantum Players|last2= Mancinska|first2= Laura|year= 2012|doi=10.1016/j.jctb.2015.12.009|volume=118|journal=Journal of Combinatorial Theory, Series B|pages=228–267}}</ref>{{refn|The hom-product of <ref>{{Cite book | doi = 10.1007/BFb0030878| chapter = Semidefinite programming and its applications to NP problems| title = Computing and Combinatorics| volume = 959| pages = 566| series = Lecture Notes in Computer Science| year = 1995| last1 = Bačík | first1 = R. | last2 = Mahajan | first2 = S. | isbn = 978-3-540-60216-3}}</ref> is the graph complement of the homomorphic product of.<ref name=rm12/>}}<br><math>G_1 \\ltimes G_2</math>\n| <math>(u_1 = v_1)</math><br/>or<br/><math>(u_1 \\sim v_1 \\text{ and } u_2 \\not\\sim v_2)</math>\n|\n|\n|}\n\nIn general, a graph product is determined by any condition for (''u''<sub>1</sub>,&nbsp;''u''<sub>2</sub>)&nbsp;&sim;&nbsp;(''v''<sub>1</sub>,&nbsp;''v''<sub>2</sub>) that can be expressed in terms of the statements ''u''<sub>1</sub>&nbsp;&sim;&nbsp;''v''<sub>1</sub>, ''u''<sub>2</sub>&nbsp;&sim;&nbsp;''v''<sub>2</sub>, ''u''<sub>1</sub>&nbsp;=&nbsp;''v''<sub>1</sub>, and ''u''<sub>2</sub>&nbsp;=&nbsp;''v''<sub>2</sub>.\n\n==Mnemonic==\n\nLet <math>K_2</math> be the complete graph on two vertices (i.e. a single edge).  The product graphs <math>K_2 \\square K_2</math>, <math>K_2 \\times K_2</math>, and <math>K_2 \\boxtimes K_2</math> look exactly like the graph representing the operator.  For example, <math>K_2 \\square K_2</math> is a four cycle (a square) and <math>K_2 \\boxtimes K_2</math> is the complete graph on four vertices.  The <math>G_1[G_2]</math> notation for lexicographic product serves as a reminder that this product is not commutative.\n\n==See also==\n* [[Graph operations]]\n\n==Notes==\n\n{{Reflist}}\n\n==References==\n\n{{refbegin}}\n* {{Cite book| last1 = Imrich | first1 = Wilfried\n  | last2 = Klavžar | first2 = Sandi\n  | title = Product Graphs: Structure and Recognition\n  | publisher = Wiley\n  | year = 2000\n  | isbn = 978-0-471-37039-0| ref = harv\n  | postscript = <!-- Bot inserted parameter. Either remove it; or change its value to \".\" for the cite to end in a \".\", as necessary. -->&#123;&#123;inconsistent citations&#125;&#125;\n  }}.\n{{refend}}\n\n[[Category:Graph products]]\n[[Category:Graph operations]]\n[[Category:Binary operations]]"
    },
    {
      "title": "Intersection (set theory)",
      "url": "https://en.wikipedia.org/wiki/Intersection_%28set_theory%29",
      "text": "{{broader|Intersection (mathematics)}}\n[[File:Venn0001.svg|thumb|Intersection of two sets:<br><math>~A \\cap B</math>]]\nIn [[mathematics]], the '''intersection''' ''A'' ∩ ''B'' of two [[Set (mathematics)|sets]] ''A'' and ''B'' is the set that contains all elements of ''A'' that also belong to ''B'' (or equivalently, all elements of ''B'' that also belong to ''A''), but no other elements.<ref>{{cite web|url=http://people.richland.edu/james/lecture/m170/ch05-rul.html |title=Stats: Probability Rules |publisher=People.richland.edu |date= |accessdate=2012-05-08}}</ref>\n\nFor explanation of the symbols used in this article, refer to the [[table of mathematical symbols]].\n\n==Definition==\n[[File:Venn 0000 0001.svg|thumb|Intersection of three sets:<br><math>~A \\cap B \\cap C</math>]]\n[[File:Venn diagram gr la ru.svg|thumb|Intersections of the [[Greek alphabet|Greek]], [[Latin alphabet|Latin]] and [[Russian alphabet|Russian]] alphabet, considering only the shapes of the letters and ignoring their pronunciation]]\n[[File:PolygonsSetIntersection.svg|thumb|Example of an intersection with sets]]\nThe intersection of two sets ''A'' and ''B'', denoted by {{math|''A'' ∩ ''B''}}, is the set of all objects that are members of both the sets {{math|''A''}} and {{math|''B''}}.\nIn symbols,\n\n:<math>A \\cap B = \\{ x: x \\in A \\text{ and } x \\in B\\}.</math>\n\nThat is, ''x'' is an element of the intersection ''A'' ∩ ''B'' [[if and only if]] ''x'' is both an element of ''A'' and an element of ''B''.\n\nFor example:\n* The intersection of the sets {1, 2, 3} and {2, 3, 4} is {2, 3}.\n* The number 9 is {{em|not}} in the intersection of the set of [[prime number]]s {2, 3, 5, 7, 11, ...} and the set of [[odd numbers]] {1, 3, 5, 7, 9, 11, ...}, because 9 is not prime.\n\nIntersection is an [[associative]] operation; that is, for any sets ''A'', ''B'', and ''C'', one has ''A'' ∩ (''B'' ∩ ''C'')&nbsp;= (''A'' ∩ ''B'') ∩ ''C''.  Intersection is also [[Commutative property|commutative]]; for any ''A'' and ''B'', one has ''A'' ∩ ''B''&nbsp;= ''B'' ∩ ''A.''  It thus makes sense to talk about intersections of multiple sets.  The intersection of ''A'', ''B'', ''C'', and ''D'', for example, is unambiguously written ''A'' ∩ ''B'' ∩ ''C'' ∩ ''D''.\n\nInside a universe ''U'' one may define the [[Complement (set theory)|complement]] ''A''<sup>c</sup> of ''A'' to be the set of all elements of ''U'' not in ''A''. Now the intersection of ''A'' and ''B'' may be written as the complement of the [[Union (set theory)|union]] of their complements, derived easily from [[De Morgan's laws]]:<br>\n''A'' ∩ ''B'' = (''A''<sup>c</sup> ∪ ''B''<sup>c</sup>)<sup>c</sup>\n\n===Intersecting and disjoint sets===\nWe say that ''A intersects (meets) B at an element x'' if ''x'' belongs to ''A'' and ''B''. We say that ''A intersects (meets) B'' if ''A'' intersects B at some element. ''A'' intersects ''B'' if their intersection is [[inhabited set|inhabited]].\n\nWe say that ''A and B are disjoint'' if ''A'' does not intersect ''B''. In plain language, they have no elements in common. ''A'' and ''B'' are disjoint if their intersection is [[empty set|empty]], denoted <math>A\\cap B=\\varnothing</math>.\n\nFor example, the sets {1, 2} and {3, 4} are disjoint, while the set of even numbers intersects the set of [[Multiple (mathematics)|multiples]] of 3 at the multiples of 6.\n\n==Arbitrary intersections==\n\nThe most general notion is the intersection of an arbitrary ''nonempty'' collection of sets.\nIf ''M'' is a [[empty set|nonempty]] set whose elements are themselves sets, then ''x'' is an element of the ''intersection'' of ''M'' if and only if [[universal quantification|for every]] element ''A'' of ''M'', ''x'' is an element of ''A''.\nIn symbols:\n\n:<math>\\left( x \\in \\bigcap_{A \\in M} A \\right) \\Leftrightarrow \\left( \\forall A \\in M, \\ x \\in A \\right).</math>\n\nThe notation for this last concept can vary considerably. [[Set theory|Set theorists]] will sometimes write \"⋂<!--U+22C2-->''M''\", while others will instead write \"⋂<!--U+22C2--><sub>''A''∈''M''&nbsp;</sub>''A''\".\nThe latter notation can be generalized to \"⋂<!--U+22C2--><sub>''i''∈''I''</sub>&nbsp;''A''<sub>''i''</sub>\", which refers to the intersection of the collection {''A''<sub>''i''</sub>&nbsp;: ''i''&nbsp;∈&nbsp;''I''}.\nHere ''I'' is a nonempty set, and ''A''<sub>''i''</sub> is a set for every ''i'' in ''I''.\n\nIn the case that the [[index set]] ''I'' is the set of [[natural number]]s, notation analogous to that of an [[infinite product]] may be seen:\n\n:<math>\\bigcap_{i=1}^{\\infty} A_i.</math>\n\nWhen formatting is difficult, this can also be written \"''A''<sub>1</sub>&nbsp;∩ ''A''<sub>2</sub>&nbsp;∩ ''A''<sub>3</sub>&nbsp;∩ ...\".  This last example, an intersection of countably many sets, is actually very common; for an example see the article on [[sigma algebra|σ-algebras]].\n\n==Nullary intersection==\n\n[[File:Multigrade operator AND.svg|thumb|[[Logical conjunction|Conjunctions]] of the arguments in parentheses<br><br>The conjunction of no argument is the [[tautology (logic)|tautology]] (compare: [[empty product]]); accordingly the intersection of no set is the [[universe (set theory)|universe]].]]\nNote that in the previous section we excluded the case where ''M'' was the [[empty set]] (∅). The reason is as follows: The intersection of the collection ''M'' is defined as the set (see [[set-builder notation]])\n:<math>\\bigcap_{A \\in M} A = \\{x : \\forall A \\in M, x \\in A\\}.</math>\nIf ''M'' is empty there are no sets ''A'' in ''M'', so the question becomes \"which ''x''<nowiki>'</nowiki>s satisfy the stated condition?\" The answer seems to be ''every possible x''. When ''M'' is empty the condition given above is an example of a [[vacuous truth]]. So the intersection of the empty family should be the [[universal set]] (the [[identity element]] for the operation of intersection) <ref>{{citation\n | last = Megginson | first = Robert E. | authorlink = Robert Megginson\n | title = An introduction to Banach space theory\n | series = [[Graduate Texts in Mathematics]]\n | volume = 183\n | publisher = Springer-Verlag\n | location = New York\n | year = 1998\n | pages = xx+596\n | isbn = 0-387-98431-3\n | chapter = Chapter 1\n}}</ref>\n\nUnfortunately, according to standard ([[Zermelo–Fraenkel set theory|ZFC]]) set theory, the universal set does not exist. A fix for this problem can be found if we note that the intersection over a set of sets is always a subset of the union over that set of sets. This can symbolically be written as\n:<math>\\bigcap_{A \\in M} A \\subseteq \\bigcup_{A \\in M} A.</math>\nTherefore, we can modify the definition slightly to\n:<math>\\bigcap_{A \\in M} A = \\left\\{x \\in \\bigcup_{A \\in M} A : \\forall A \\in M, x \\in A\\right\\}.</math>\nNow if ''M'' is empty there is no problem. The intersection is the empty set, because the union over the empty set is the empty set. In fact, this is the operation that we would have defined in the first place if we were defining the set in ZFC, as except for the operations defined by the axioms (the [[power set]] of a set, for instance), every set must be defined as the subset of some other set or by [[Axiom schema of replacement | replacement]].\n\n==See also==\n{{commons category}}\n* [[complement (set theory)|Complement]]\n* [[Intersection graph]]\n* [[Logical conjunction]]\n* [[Naive set theory]]\n* [[Symmetric difference]]\n* [[Union (set theory)|Union]]\n* [[Cardinality]]\n* [[Iterated binary operation]]\n* [[MinHash]]\n\n==References==\n<references />\n\n==Further reading==\n* {{cite book |authorlink=Keith J. Devlin |last=Devlin |first=K. J. |title=The Joy of Sets: Fundamentals of Contemporary Set Theory |edition=Second |publisher=Springer-Verlag |location=New York, NY |year=1993 |isbn=3-540-94094-4 }}\n* {{cite book |last=Munkres |first=James R. |authorlink=James Munkres |title=Topology |edition=Second |location=Upper Saddle River |publisher=Prentice Hall |chapter=Set Theory and Logic |year=2000 |isbn=0-13-181629-2 }}\n* {{cite book |title=Discrete Mathematics and Its Applications |first=Kenneth |last=Rosen |location=Boston |publisher=McGraw-Hill |year=2007 |edition=Sixth |isbn=978-0-07-322972-0 |chapter=Basic Structures: Sets, Functions, Sequences, and Sums }}\n\n==External links==\n*{{MathWorld |title=Intersection |id=Intersection }}\n\n{{Set theory}}\n\n[[Category:Basic concepts in set theory]]\n[[Category:Binary operations]]"
    },
    {
      "title": "Iterated binary operation",
      "url": "https://en.wikipedia.org/wiki/Iterated_binary_operation",
      "text": "In [[mathematics]], an '''iterated binary operation''' is an extension of a [[binary operation]] on a set  ''S'' to a [[function (mathematics)|function]] on finite [[sequence]]s of elements of ''S'' through repeated application.<ref name=IBO1>{{cite book|title=Categories for the Working Mathematician |author= Saunders MacLane |page=142 |location=New York |publisher= Springer-Verlag |date= 1971 |isbn=0387900357}}</ref> Common examples include the extension of the [[addition]] operation to the [[summation]] operation, and the extension of the [[multiplication]] operation to the [[Product (mathematics)|product]] operation.  Other operations, e.g., the set theoretic operations [[union (set theory)|union]] and [[intersection (set theory)|intersection]], are also often iterated, but the iterations are not given separate names.  In print, summation and product are represented by special symbols; but other iterated operators often are denoted by larger variants of the symbol for the ordinary binary operator.  Thus, the iterations of the four operations mentioned above are denoted\n\n::<math>\\sum,\\ \\prod,\\ \\bigcup,</math> and <math>\\bigcap</math>, respectively.\n\nMore generally, iteration of a binary function is generally denoted by a slash: iteration of <math>f</math> over the sequence <math>(a_{1}, a_{2} \\ldots, a_{n})</math> is denoted by <math>f / (a_{1}, a_{2} \\ldots, a_{n})</math>, following the notation for [[Fold (higher-order function)|reduce]] in [[Bird–Meertens formalism]].\n\nIn general, there is more than one way to extend a binary operation to operate on finite sequences, depending on whether the operator is [[associative]], and whether the operator has [[identity element]]s.\n\n== Definition ==\n\nDenote by '''a'''<sub>''j'',''k''</sub>, with {{nowrap|''j'' ≥ 0}} and {{nowrap|''k'' ≥ ''j''}}, the finite sequence of length {{nowrap|''k'' − ''j''}} of elements of ''S'', with members (''a''<sub>i</sub>), for {{nowrap|''j'' ≤ ''i'' &lt; ''k''}}. Note that if {{nowrap|''k'' {{=}} ''j''}}, the sequence is empty.\n\nFor {{nowrap|''f'' : ''S'' × ''S''}}, define a new function ''F''<sub>''l''</sub> on finite nonempty sequences of elements of ''S'', where\n\n:<math>F_l(\\mathbf{a}_{0,k})=\n\\begin{cases}\na_0, &k=1\\\\\nf(F_l(\\mathbf{a}_{0,k-1}), a_{k-1}), &k>1\n\\end{cases}.</math>\n\nSimilarly, define\n\n:<math>F_r(\\mathbf{a}_{0,k})=\n\\begin{cases}\na_0, &k=1\\\\\nf(a_0, F_r(\\mathbf{a}_{1,k})), &k>1\n\\end{cases}.</math>\n\nIf ''f'' has a unique left identity ''e'', the definition of ''F''<sub>''l''</sub> can be modified to operate on empty sequences by defining the value of ''F''<sub>''l''</sub> on an empty sequence to be ''e'' (the previous base case on sequences of length 1 becomes redundant). Similarly, ''F''<sub>''r''</sub> can be modified to operate on empty sequences if ''f'' has a unique right identity.\n\nIf ''f'' is associative, then ''F''<sub>''l''</sub> equals ''F''<sub>''r''</sub>, and we can simply write ''F''.  Moreover, if an identity element ''e'' exists, then it is unique (see [[Monoid]]).\n\nIf ''f'' is commutative and associative, then ''F'' can operate on any non-empty finite [[multiset]] by applying it to an arbitrary enumeration of the multiset.  If ''f'' moreover has an identity element ''e'', then this is defined to be the value of ''F'' on an empty multiset.  If ''f'' is idempotent, then the above definitions can be extended to [[finite set]]s.\n\nIf ''S'' also is equipped with a [[Metric (mathematics)|metric]] or more generally with [[topology]] that is [[Hausdorff space|Hausdorff]], so that the concept of a [[limit of a sequence]] is defined in ''S'', then an ''[[Infinity|infinite]] iteration'' on a countable sequence in ''S'' is defined exactly when the corresponding sequence of finite iterations converges. Thus, e.g., if ''a<sub>0</sub>'', ''a<sub>1</sub>'', ''a<sub>2</sub>'', ''a<sub>3</sub>'',&nbsp;... is an infinite sequence of real numbers, then the [[infinite product]]&nbsp;<math>\\prod_{i=0}^\\infty a_i\\,</math> is defined, and equal to <math>\\lim\\limits_{n\\rightarrow\\infty}\\prod_{i=0}^na_i,</math>&nbsp; if and only if that limit exists.\n\n==Non-associative binary operation==\nThe general, non-associative binary operation is given by a [[magma (algebra)|magma]]. The act of iterating on a non-associative binary operation may be represented as a [[binary tree]].\n\n==Notation==\n\nIterated binary operations are used to represent an operation that will be repeated over a set subject to some constraints. Typically the lower bound of a restriction is written under the symbol, and the upper bound over the symbol, though they may also be written as superscripts and subscripts in compact notation.  Interpolation is performed over positive integers from the lower to upper bound, to produce the set which will be substituted into the index (below denoted as i) for the repeated operations.  It is possible to specify set membership or other logical constraints in place of explicit indices, in order to implicitly specify which elements of a set shall be used.\n\nCommon notations include the big '''S'''igma ( [[summation|repeated '''s'''um]] ) and big '''P'''i notations( [[Product (mathematics)|repeated '''p'''roduct]] ) notations.\n\n<math>\\sum_{i=0}^{n-1} i = 0+1+2+...+ (n-1)</math>\n\n\n\n<math>\\prod_{i=0}^{n-1} i = 0*1*2*...*(n-1)</math>\n\nThough [[binary operators]] including but not limited to [[exclusive or]] and [[set union]] may be used.<ref>{{cite web|last1=W.|first1=Weisstein, Eric|title=Union|url=http://mathworld.wolfram.com/Union.html|website=mathworld.wolfram.com|publisher=Wolfram Mathworld|accessdate=30 January 2018|language=en}}</ref>\n\nLet S be a set of sets\n\n<math>\\bigcup_{s \\in S} s_i= s_1 \\cup s_2 \\cup ... \\cup s_N</math>\n\nLet S be a set of logical [[proposition|propositions]]\n\n<math>\\bigoplus_{s \\in S} s_i= s_1 \\oplus s_2 \\oplus ... \\oplus s_N</math>\n\nNote how in the above, no upper bound is used, because it suffices to express that the elements <math>s_i</math> are elements of the set S.\n\nIt is also to produce a repeated operation given a number of constraint joined by a [[logical and| conjunction (and)]], for example:\n\n<math>\\sum_{i=0\\ \\wedge\\ \\text{i is even}}^{n} i = 0+2+4...+ n</math>\n\n==See also==\n*[[Continued fraction]]\n*[[Fold (higher-order function)]]\n*[[Infinite product]]\n*[[Infinite series]]\n\n== References ==\n<references />\n==External links==\n* [https://web.archive.org/web/20071009181156/http://www.short-fuze.co.uk/~eddy/math/associate.html Bulk action] <!-- archived from the original -->\n* [http://wotug.ukc.ac.uk/parallel/acronyms/hpccgloss/P.html#parallel%20prefix Parallel prefix operation]\n* [http://www.cs.cornell.edu/Info/People/sfa/Nuprl/iterated_binops/Xiter_via_intseg_remark_INTRO.html Nuprl iterated binary operations]\n[[Category:Binary operations]]"
    },
    {
      "title": "Join (topology)",
      "url": "https://en.wikipedia.org/wiki/Join_%28topology%29",
      "text": "[[Image:Join.svg|right|thumb|Geometric join of two [[line segment]]s. The original spaces are shown in green and blue. The join is a three-dimensional solid in gray.]]\nIn [[topology]], a field of [[mathematics]], the '''join''' of two [[topological space]]s ''A'' and ''B'', often denoted by <math>A\\ast B</math> or <math>A\\star B</math>, is defined to be the [[Quotient space (topology)|quotient space]]\n:<math> (A \\times B \\times I) / R, \\, </math>\nwhere ''I'' is the [[interval (mathematics)|interval]] [0, 1] and ''R'' is the [[equivalence relation]] generated by\n:<math> (a, b_1, 0) \\sim (a, b_2, 0) \\quad\\mbox{for all } a \\in A \\mbox{ and } b_1,b_2 \\in B,</math>\n\n:<math> (a_1, b, 1) \\sim (a_2, b, 1) \\quad\\mbox{for all } a_1,a_2 \\in A \\mbox{ and } b \\in B.</math>\nAt the endpoints, this collapses <math>A\\times B\\times \\{0\\}</math> to <math>A</math> and <math>A\\times B\\times \\{1\\}</math> to <math>B</math>.\n\nIntuitively, <math>A\\star B</math> is formed by taking the [[disjoint union (topology)|disjoint union]] of the two spaces and attaching line segments joining every point in ''A'' to every point in ''B''.\n\n== Examples ==\n\n* The join of a space ''X'' with a one-point space is called the [[cone (topology)|cone]] ''CX'' of ''X''. \n* The join of a space ''X'' with <math>S^0</math> (the 0-dimensional [[sphere]], or, the [[discrete space]] with two points) is called the [[Suspension (topology)|suspension]] <math>SX</math> of ''X''.\n* The join of the spheres <math>S^n</math> and <math>S^m</math> is the sphere <math>S^{n+m+1}</math>.\n\n==Properties==\n\n* The join of two spaces is [[Homeomorphism|homeomorphic]] to a sum of [[Product topology|cartesian products]] of [[Cone (topology)|cones]] over the spaces and the spaces themselves, where the sum is taken over the cartesian product of the spaces: \n\n:<math>A\\star B\\cong C(A)\\times B\\cup_{A\\times B} C(B)\\times A.</math>\n* Given basepointed [[CW complex|CW complexes]] (''A'',''a''<sub>0</sub>) and (''B'',''b''<sub>0</sub>), the \"reduced join\"\n:<math>\\frac{A\\star B}{A \\star \\{b_0\\} \\cup \\{a_0\\} \\star B}</math>\n\nis homeomorphic to the reduced [[Suspension (topology)|suspension]]\n\n:<math>\\Sigma(A\\wedge B)</math>\nof the [[smash product]]. Consequently, since <math>{A \\star \\{b_0\\} \\cup \\{a_0\\} \\star B}</math> is [[Contractible space|contractible]], there is a [[homotopy equivalence]]\n:<math>A\\star B\\simeq \\Sigma(A\\wedge B).</math>\n\n==See also==\n\n*[[Desuspension]]\n\n==References==\n*[[Allen Hatcher|Hatcher, Allen]], [http://www.math.cornell.edu/~hatcher/AT/ATpage.html ''Algebraic topology.''] Cambridge University Press, Cambridge, 2002. xii+544 pp. {{ISBN|0-521-79160-X}} and {{ISBN|0-521-79540-0}}\n*{{PlanetMath attribution|id=3985|title=Join}}\n* [[Ronald Brown (mathematician)|Brown, Ronald]], [http://pages.bangor.ac.uk/~mas010/topgpds.html ''Topology and Groupoids''] Section 5.7 Joins.\n\n[[Category:Algebraic topology]]\n[[Category:Binary operations]]"
    },
    {
      "title": "Join and meet",
      "url": "https://en.wikipedia.org/wiki/Join_and_meet",
      "text": "[[File:Join and meet.svg|thumb|This [[Hasse diagram]] depicts a partially ordered set with four elements - '''a''', '''b''', the [[maximal element]] equal to the join of '''a''' and '''b''' ('''a''' &or; '''b''') and the [[minimal element]] equal to the meet of '''a''' and '''b''' ('''a''' &and; '''b'''). The join/meet of a maximal/minimal element and another element is the maximal/minimal element and conversely the meet/join of a maximal/minimal element with another element is the other element. Thus every pair in this poset has both a meet and a join and the poset can be classified as a [[lattice (order theory)]].]]\n\nIn a [[partially ordered set]] ''P'', the '''join''' and '''meet''' of a [[subset]] ''S'' are respectively the [[supremum]] (least upper bound) of ''S'', denoted &#8897;''S'', and [[infimum]] (greatest lower bound) of ''S'', denoted &#8896;''S''. In general, the join and meet of a subset of a partially ordered set need not exist; when they do exist, they are elements of ''P''. \n\nJoin and meet can also be defined as a [[commutative]], [[associative]] and [[idempotent]] [[Partial function|partial]] [[binary operation]] on pairs of elements from ''P''. If '''a''' and '''b''' are elements from ''P'', the join is denoted as '''a''' &or; '''b''' and the meet is denoted '''a''' &and; '''b'''. \n\nJoin and meet are symmetric [[duality (order theory)|dual]]s with respect to order inversion.  The join/meet of a subset of a [[total order|totally ordered set]] is simply its maximal/minimal element, if such an element exists.\n\nA partially ordered set in which all pairs have a join is a [[join-semilattice]].  Dually, a partially ordered set in which all pairs have a meet is a [[meet-semilattice]].  A partially ordered set that is both a join-semilattice and a meet-semilattice is a [[lattice (order)|lattice]].  A lattice in which every subset, not just every pair, possesses a meet and a join is  a [[complete lattice]].  It is also possible to define a [[partial lattice]], in which not all pairs have a meet or join but the operations (when defined) satisfy certain axioms.{{sfn|Grätzer|1996|p=[https://books.google.com/books?id=SoGLVCPuOz0C&pg=PA52 52]}}\n\n==Partial order approach==\nLet ''A'' be a set with a [[partial order]] ≤, and let ''x'' and ''y'' be two elements in ''A''.  An element ''z'' of ''A'' is the meet (or greatest lower bound or infimum) of ''x'' and ''y'', if the following two conditions are satisfied:\n# ''z'' ≤ ''x'' and ''z'' ≤ ''y'' (i.e., ''z'' is a lower bound of ''x'' and ''y'').\n# For any ''w'' in ''A'', such that {{Nowrap|''w'' ≤ ''x''}} and {{Nowrap|''w'' ≤ ''y''}}, we have {{Nowrap|''w'' ≤ ''z''}} (i.e., ''z'' is greater than or equal to any other lower bound of ''x'' and ''y'').\nIf there is a meet of ''x'' and ''y'', then it is unique, since if both ''z'' and ''z''&prime; are greatest lower bounds of ''x'' and ''y'', then {{Nowrap|''z'' ≤ ''z''&prime;}} and {{Nowrap|''z''&prime; ≤ ''z''}}, and thus {{Nowrap begin}}''z'' = ''z''&prime;{{Nowrap end}}.  If the meet does exist, it is denoted {{Nowrap|''x'' &and; ''y''}}.\nSome pairs of elements in ''A'' may lack a meet, either since they have no lower bound at all, or since none of their lower bounds is greater than all the others.  If all pairs of elements have meets, then the meet is a binary operation on ''A'', and it is easy to see that this operation fulfills the following three conditions: For any elements ''x'', ''y'', and ''z'' in ''A'',\n:'''a.'''  ''x'' ∧ ''y'' = ''y'' ∧ ''x'' ([[commutativity]]),\n:'''b.'''  ''x'' ∧ (''y'' ∧ ''z'') = (''x'' ∧ ''y'') ∧ ''z'' ([[associativity]]), and\n:'''c.'''  ''x'' ∧ ''x'' = ''x'' ([[idempotency]]).\n\n==Universal algebra approach==\nBy definition, a [[binary operation]] ∧ on a set ''A'' is a ''meet'', if it satisfies the three conditions '''a''', '''b''', and '''c'''.  The pair (''A'',∧) then is a [[meet-semilattice]].  Moreover, we then may define a [[binary relation]] ≤ on ''A'', by stating that {{Nowrap|''x'' ≤ ''y''}} if and only if {{Nowrap begin}}''x'' ∧ ''y'' = ''x''{{Nowrap end}}.  In fact, this relation is a [[partial order]] on ''A''.  Indeed, for any elements ''x'', ''y'', and ''z'' in ''A'',\n* ''x'' ≤ ''x'', since ''x'' ∧ ''x'' = ''x'' by '''c''';\n* if ''x'' ≤ ''y'' and ''y'' ≤ ''x'', then {{Nowrap begin}}''x'' = ''x'' ∧ ''y'' = ''y'' ∧ ''x'' = ''y''{{Nowrap end}} by '''a'''; and\n* if ''x'' ≤ ''y'' and ''y'' ≤ ''z'', then ''x'' ≤ ''z'', since then ''x'' ∧ ''z'' = (''x'' ∧ ''y'') ∧ ''z'' = ''x'' ∧ (''y'' ∧ ''z'') = ''x'' ∧ ''y'' = ''x'' by '''b'''.\n\nNote that both meets and joins equally satisfy this definition: a couple of associated meet and join operations yield partial orders which are the reverse of each other. When choosing one of these orders as the main ones, one also fixes which operation is considered a meet (the one giving the same order) and which is considered a join (the other one).\n\n==Equivalence of approaches==\nIf (''A'',≤) is a [[partially ordered set]], such that each pair of elements in ''A'' has a meet, then indeed {{Nowrap begin}}''x'' ∧ ''y'' = ''x''{{Nowrap end}} if and only if {{Nowrap|''x'' ≤ ''y''}}, since in the latter case indeed ''x'' is a lower bound of ''x'' and ''y'', and since clearly ''x'' is the ''greatest'' lower bound if and only if it is a lower bound.  Thus, the partial order defined by the meet in the universal algebra approach coincides with the original partial order.\n\nConversely, if (''A'',∧) is a [[meet-semilattice]], and the partial order ≤ is defined as in the universal algebra approach, and {{Nowrap begin}}''z'' = ''x'' ∧ ''y''{{Nowrap end}} for some elements ''x'' and ''y'' in ''A'', then ''z'' is the greatest lower bound of ''x'' and ''y'' with respect to ≤, since\n:''z'' ∧ ''x'' = ''x'' ∧ ''z'' = ''x'' ∧ (''x'' ∧ ''y'') = (''x'' ∧ ''x'') ∧ ''y'' = ''x'' ∧ ''y'' = ''z''\nand therefore {{Nowrap|''z'' ≤ ''x''}}. Similarly, {{Nowrap|''z'' ≤ ''y''}}, and if ''w'' is another lower bound of ''x'' and ''y'', then {{Nowrap begin}}''w'' ∧ ''x'' = ''w'' ∧ ''y'' = w{{Nowrap end}}, whence\n:''w'' ∧ ''z'' = ''w'' ∧ (''x'' ∧ ''y'') = (''w'' ∧ ''x'') ∧ ''y'' = ''w'' ∧ ''y'' = ''w''.\nThus, there is a meet defined by the partial order defined by the original meet, and the two meets coincide.\n\nIn other words, the two approaches yield essentially equivalent concepts, a set equipped with both a binary relation and a binary operation, such that each one of these structures determines the other, and fulfil the conditions for partial orders or meets, respectively.\n\n==Meets of general subsets==\nIf (''A'',∧) is a meet-semilattice, then the meet may be extended to a well-defined meet of any non-empty finite set, by the technique described in [[iterated binary operation]]s.  Alternatively, if the meet defines or is defined by a partial order, some subsets of ''A'' indeed have [[Infimum#Infima in partially ordered sets|infima]] with respect to this, and it is reasonable to consider such an infimum as the meet of the subset.  For non-empty finite subsets, the two approaches yield the same result, whence either may be taken as a definition of meet.  In the case where ''each'' subset of ''A'' has a meet, in fact (''A'',≤) is a [[complete lattice]]; for details, see [[completeness (order theory)]].\n\n==Notes==\n{{reflist}}\n\n==References==\n{{refbegin}}\n*{{cite book | zbl=1002.06001 | last1=Davey | first1=B.A. | last2=Priestley | first2=H.A. | title=Introduction to Lattices and Order | edition=2nd | location=Cambridge | publisher=[[Cambridge University Press]] | year=2002 | isbn=0-521-78451-4 }}\n*{{cite book | first=Steven | last=Vickers | authorlink=Steve Vickers (computer scientist) | title=Topology via Logic | series=Cambridge Tracts in Theoretic Computer Science | volume=5 | isbn=0-521-36062-5 | year=1989 | zbl=0668.54001 }}\n{{refend}}\n\n{{DEFAULTSORT:Join And Meet}}\n[[Category:Binary operations]]\n[[Category:Lattice theory]]\n[[Category:Order theory]]"
    },
    {
      "title": "Lagrange bracket",
      "url": "https://en.wikipedia.org/wiki/Lagrange_bracket",
      "text": "'''Lagrange brackets''' are certain expressions closely related to [[Poisson bracket]]s that were introduced by [[Joseph Louis Lagrange]] in 1808–1810 for the purposes of mathematical formulation of [[classical mechanics]], but unlike the Poisson brackets, have fallen out of use.\n\n== Definition ==\nSuppose that (''q''<sub>1</sub>, &hellip;, ''q''<sub>''n''</sub>, ''p''<sub>1</sub>, &hellip;, ''p''<sub>''n''</sub>) is a system of [[canonical coordinates]] on a [[phase space]]. If each of them is expressed as a function of two variables, ''u'' and ''v'', then the Lagrange bracket of ''u'' and ''v'' is defined by the formula\n\n:<math>\n[ u, v ]_{p,q} = \\sum_{i=1}^n \\left(\\frac{\\partial q_i}{\\partial u} \\frac{\\partial p_i}{\\partial v} - \\frac{\\partial p_i}{\\partial u} \\frac{\\partial q_i}{\\partial v} \\right).\n</math>\n\n== Properties ==\n\n* Lagrange brackets do not depend on the system of [[canonical coordinates]] (''q'', ''p''). If (''Q'',''P'')&nbsp;=&nbsp;(''Q''<sub>1</sub>, &hellip;, ''Q''<sub>''n''</sub>, ''P''<sub>1</sub>, &hellip;, ''P''<sub>''n''</sub>) is another system of canonical coordinates, so that\n\n::<math> Q=Q(q,p), P=P(q,p) </math>\n\n:is a [[canonical transformation]], then the Lagrange bracket is an invariant of the transformation, in the sense that\n\n:: <math> [ u, v]_{q,p} = [u , v]_{Q,P}</math>\n\n:Therefore, the subscripts indicating the canonical coordinates are often omitted.\n\n* If ''&Omega;'' is the [[symplectic form]] on the ''2n''-dimensional phase space ''W'' and ''u''<sub>''1''</sub>,&hellip;,''u''<sub>''2n''</sub> form a system of coordinates on ''W'', then canonical coordinates (''q'',''p'') may be expressed as functions of the coordinates ''u'' and the [[matrix (mathematics)|matrix]] of the Lagrange brackets\n\n:: <math> [ u_i, u_j ]_{p,q}, \\quad 1\\leq i,j\\leq 2n </math>\n\n:represents the components of ''&Omega;'', viewed as a [[tensor]], in the coordinates ''u''. This matrix is the [[inverse matrix|inverse]] of the matrix formed by the Poisson brackets\n\n:: <math> \\{u_i, u_j\\}, \\quad 1\\leq i,j\\leq 2n </math>\n\n:of the coordinates ''u''.\n\n* As a corollary of the preceding properties, coordinates (''Q''<sub>1</sub>, &hellip;, ''Q''<sub>''n''</sub>, ''P''<sub>1</sub>, &hellip;, ''P''<sub>''n''</sub>) on a phase space are canonical if and only if the Lagrange brackets between them have the form\n\n:: <math> [Q_i, Q_j]_{p,q}=0, \\quad [P_i,P_j]_{p,q}=0,\\quad [Q_i, P_j]_{p,q}=-[P_j, Q_i]_{p,q}=\\delta_{ij}. </math>\n\n== See also ==\n* [[Lagrangian mechanics]]\n* [[Hamiltonian mechanics]]\n\n== References ==\n\n* [[Cornelius Lanczos]], ''The Variational Principles of Mechanics'', Dover (1986), {{ISBN|0-486-65067-7}}.\n* Iglesias, Patrick, ''Les origines du calcul symplectique chez Lagrange'' [The origins of symplectic calculus in Lagrange's work], L'Enseign. Math. (2) 44 (1998), no. 3-4, 257–277. {{MathSciNet|id=1659212}}\n\n== External links ==\n\n* {{mathworld |urlname=LagrangeBracket |title=Lagrange bracket|author=[[Eric W. Weisstein]]}}\n* {{SpringerEOM|author=A.P. Soldatov|title=Lagrange bracket|urlname=L/l057130}}\n\n[[Category:Bilinear operators]]\n[[Category:Binary operations]]\n[[Category:Hamiltonian mechanics]]"
    },
    {
      "title": "Lie bracket of vector fields",
      "url": "https://en.wikipedia.org/wiki/Lie_bracket_of_vector_fields",
      "text": "{{Use dmy dates|date=October 2011}}\nIn the mathematical field of [[differential topology]], the '''Lie bracket of vector fields''', also known as the '''Jacobi&ndash;Lie bracket''' or the '''commutator of vector fields''', is an operator that assigns to any two [[vector field]]s ''X'' and ''Y'' on a [[smooth manifold]] ''M'' a third vector field denoted {{nowrap|[''X'', ''Y'']}}.\n\nConceptually, the Lie bracket {{nowrap|[''X'', ''Y'']}} is the derivative of ''Y'' along the [[vector flow|flow]] generated by ''X'', and is sometimes denoted ''<math>\\mathcal{L}_X Y</math>'' (\"Lie derivative of Y along X\"). This generalizes to the [[Lie derivative]] of any [[tensor field]] along the flow generated by ''X''.\n\nThe Lie bracket is an '''R'''-[[bilinear operator|bilinear]] operation and turns the set of all [[Smoothness|smooth]] vector fields on the manifold ''M'' into an (infinite-dimensional) [[Lie algebra]].\n\nThe Lie bracket plays an important role in [[differential geometry]] and [[differential topology]], for instance in the [[Frobenius theorem (differential topology)|Frobenius integrability theorem]], and is also fundamental in the geometric theory of [[nonlinear control theory|nonlinear control systems]].<ref>{{harvnb|Isaiah|2009|pp=20&ndash;21}}, [[nonholonomic system]]s; {{harvnb|Khalil|2002|pp=523&ndash;530}}, [[feedback linearization]].</ref>\n\n==Definitions==\nThere are three conceptually different but equivalent approaches to defining the Lie bracket:\n\n===Vector fields as derivations===\nEach smooth vector field ''X'' on a manifold ''M''\nmay be regarded as a [[differential operator]] acting on smooth\nfunctions ''C''<sup>∞</sup>(''M''). Indeed, each\nsmooth vector field ''X'' becomes a [[Derivation (abstract algebra)|derivation]] on ''C''<sup>∞</sup>(''M'') when we define ''X''(''f'') to be function whose value at a point ''p'' is the [[directional derivative]] of ''f'' at ''p'' in the direction ''X''(''p''). Furthermore, any derivation on ''C''<sup>∞</sup>(''M'') arises from a unique smooth vector field ''X''.\n\nIn general, the [[commutator]] <math>\\delta_1\\circ \\delta_2 - \\delta_2\\circ\\delta_1</math> of any two derivations <math>\\delta_1</math> and <math>\\delta_2</math> is again a derivation, where <math>\\circ</math> denotes composition of operators. This can be used to define the Lie bracket as the vector field corresponding to the commutator derivation:\n\n:<math>[X,Y](f) = X(Y(f))-Y(X(f)) \\;\\;\\text{  for all } f\\in C^\\infty(M).</math>\n\n===Flows and limits===\nLet <math>\\Phi^X_t</math> be the [[Flow (mathematics)|flow]] associated with the vector field ''X'', and let D denote the [[Pushforward (differential)|tangent map derivative operator]]. Then the Lie bracket of ''X'' and ''Y'' at the point {{nowrap|''x'' &isin; ''M''}} can be defined as the [[Lie derivative]]:\n\n:<math>[X, Y]_x \\ =\\ (\\mathcal{L}_X Y)_x \\ :=\\ \\lim_{t \\to 0}\\frac{(\\mathrm{D}\\Phi^X_{-t}) Y_{\\Phi^X_t(x)} \\,-\\, Y_x}t \n\\ =\\ \n\\left.\\tfrac{\\mathrm{d}}{\\mathrm{d} t}\\right|_{t=0} (\\mathrm{D}\\Phi^X_{-t}) Y_{\\Phi^X_t(x)} .</math>\n\nThis also measures the failure of the flow in the successive directions <math>X,Y,-X,-Y</math> to return to the point ''x'':\n\n:<math>[X, Y]_x \\ =\\ \\left.\\tfrac12\\tfrac{\\mathrm{d}^2}{\\mathrm{d}t^2}\\right|_{t=0} (\\Phi^Y_{-t} \\circ \\Phi^X_{-t} \\circ \\Phi^Y_{t} \\circ \\Phi^X_{t})(x) \n\\ =\\  \\left.\\tfrac{\\mathrm{d}}{\\mathrm{d} t}\\right|_{t=0} (\\Phi^Y_{\\!-\\sqrt{t}} \\circ \\Phi^X_{\\!-\\sqrt{t}} \\circ \\Phi^Y_{\\!\\sqrt{t}} \\circ \\Phi^X_{\\!\\sqrt{t}})(x) .</math>\n\n===In coordinates===\nThough the above definitions of Lie bracket are [[Differential geometry#Intrinsic versus extrinsic|intrinsic]] (independent of the choice of coordinates on the manifold ''M''), in practice one often wants to compute the bracket in terms of a specific coordinate system <math>\\{ x^i \\}</math>. We write <math>\\partial_i = \\tfrac{\\partial}{\\partial x^i}</math> for the associated local basis of the tangent bundle, so that general vector fields can be written <math>\\textstyle X=\\sum_{i=1}^n X^i \\partial_i</math>and <math>\\textstyle Y=\\sum_{i=1}^n Y^i \\partial_i</math>for smooth functions <math>X^i, Y^i:M\\to\\mathbb{R}</math>.  Then the Lie bracket can be computed as:\n\n:<math>[X,Y] := \\sum_{i=1}^n\\left(X(Y^i) - Y(X^i)\\right) \\partial_i = \\sum_{i=1}^n \\sum_{j=1}^n \\left(X^j \\partial_j Y^i - Y^j \\partial_j X^i \\right) \\partial_i .</math>\n\nIf ''M'' is (an open subset of) '''R'''<sup>''n''</sup>, then the vector fields ''X'' and ''Y'' can be written as smooth maps of the form <math>X:M\\to\\mathbb{R}^n</math> and <math>Y:M\\to\\mathbb{R}^n</math>, and the Lie bracket <math>[X,Y]:M\\to\\mathbb{R}^n</math> is given by:\n:<math>[X,Y] := J_Y  X - J_X Y</math>\n\nwhere <math>J_Y</math> and <math>J_X</math> are ''n×n'' [[Jacobian matrix|Jacobian matrices]] multiplying the ''n×''1 column vectors ''X'' and ''Y''.\n\n==Properties==\n\nThe Lie bracket of vector fields equips the real vector space <math>V=\\Gamma(TM)</math> of all vector fields on ''M'' (i.e., smooth sections of the tangent bundle <math>TM\\to M</math>) with the structure of a [[Lie algebra]], which means [ • , • ]  is a map <math>V\\times V\\to V</math> with:\n*'''R'''-[[Bilinear map|bilinearity]]\n*Anti-symmetry, <math>[X, Y] = -[Y, X]</math>\n*[[Jacobi identity]], <math>[X, [Y, Z]] + [Z, [X, Y]] + [Y, [Z, X]] = 0 .</math>\n\nAn immediate consequence of the second property is that <math>[X, X] = 0</math> for any <math>X</math>.\n\nFurthermore, there is a \"[[product rule]]\" for Lie brackets. Given a smooth (scalar-valued) function ''f'' on ''M'' and a vector field ''Y'' on ''M'', we get a new vector field ''fY'' by multiplying the vector ''Y<sub>x</sub>'' by the scalar ''f''(''x'') at each point {{nowrap|''x'' &isin; ''M''}}. Then:\n\n*<math> [X, fY] \\ =\\  X\\!(f)\\, Y \\,+\\, f\\, [X,Y] ,</math>\n\nwhere we multiply the scalar function ''X''(''f'') with the vector field ''Y'', and the scalar function ''f'' with the vector field {{nowrap|[''X'', ''Y'']}}.\nThis turns the vector fields with the Lie bracket into a [[Lie algebroid]].\n\nVanishing of the Lie bracket of ''X'' and ''Y'' means that following the flows in these directions defines a surface embedded in ''M'', with ''X'' and ''Y'' as coordinate vector fields:\n\n'''Theorem:'''  <math>[X,Y]=0\\,</math> iff the flows of ''X'' and ''Y'' commute locally, meaning <math>(\\Phi^Y_t \\Phi^X_s) (x) =(\\Phi^X_{s}\\, \\Phi^Y_t)(x)</math> for all {{nowrap|''x'' &isin; ''M''}} and sufficiently small ''s'', ''t''.\n\nThis is a special case of the [[Frobenius integrability theorem]].\n\n== Examples ==\nFor a [[Lie group]] ''G'', the corresponding [[Lie algebra]] <math>\\mathfrak{g}</math> is the tangent space at the identity <math>T_eG</math>, which can be identified with the vector space of left invariant vector fields on ''G''. The Lie bracket of two left invariant vector fields is also left invariant, which defines the Jacobi–Lie bracket operation <math>[\\,\\cdot\\,,\\,\\cdot\\,]: \\mathfrak g \\times \\mathfrak g\\to \\mathfrak g</math>.\n\nFor a matrix Lie group, whose elements are matrices <math>g \\in G \\subset M_{n\\times n}(\\mathbb{R})</math>, each tangent space can be represented as matrices: <math>T_{g}G = g\\cdot T_I G \\subset M_{n\\times n}(\\mathbb{R})</math>, where <math>\\cdot</math> means matrix multiplication and ''I'' is the identity matrix. The invariant vector field corresponding to <math>X\\in \\mathfrak{g}=T_IG</math>is given by <math>X_g = g\\cdot X\\in T_gG</math>, and a computation shows the Lie bracket on <math>\\mathfrak g</math> corresponds to the usual [[Commutator#Ring theory|commutator]] of matrices:\n\n:<math>[X,Y] \\ =\\ X\\cdot Y - Y\\cdot X .</math>\n\n== Applications ==\n\nThe Jacobi&ndash;Lie bracket is essential to proving [[small-time local controllability]] (STLC) for driftless [[affine control systems]].\n\n==Generalizations==\nAs mentioned above, the [[Lie derivative]] can be seen as a generalization of the Lie bracket. Another generalization of the Lie bracket (to [[vector-valued differential form]]s) is the [[Frölicher–Nijenhuis bracket]].\n\n==References==\n{{Reflist}}\n* {{springer|title=Lie bracket|id=p/l058550}}\n* {{citation|last=Isaiah|first=Pantelis|title=Controlled parking [Ask the experts]|journal=IEEE Control Systems Magazine|year=2009|volume=29|issue=3|pages=17&ndash;21, 132|doi=10.1109/MCS.2009.932394}}\n* {{citation\n | last = Khalil\n | first = H.K.\n | authorlink = Hassan K. Khalil\n | year = 2002\n | edition = 3rd\n | url = http://www.egr.msu.edu/~khalil/NonlinearSystems/\n | isbn = 0-13-067389-7\n | title = Nonlinear Systems\n | publisher=[[Prentice Hall]]\n | location = Upper Saddle River, NJ}}\n* {{Citation|author=Kolář, I., Michor, P., and Slovák, J.|title=Natural operations in differential geometry|url=http://www.emis.de/monographs/KSM/index.html|publisher=Springer-Verlag|year=1993}} Extensive discussion of Lie brackets, and the general theory of Lie derivatives.\n* {{Citation|author=Lang, S.|title=Differential and Riemannian manifolds|publisher=Springer-Verlag|year=1995|isbn=978-0-387-94338-1}} For generalizations to infinite dimensions.\n* {{Citation|author=Lewis, Andrew D.|url=http://penelope.mast.queensu.ca/math890-03/ps/math890.pdf|title=Notes on (Nonlinear) Control Theory}}{{dead link|date=December 2017 |bot=InternetArchiveBot |fix-attempted=yes }}\n*{{Citation | last =Warner | first = Frank | title = Foundations of differentiable manifolds and Lie groups | origyear = 1971 | edition = | year = 1983 | publisher=Springer-Verlag | location = New York-Berlin | isbn = 0-387-90894-3 }}\n\n[[Category:Bilinear operators]]\n[[Category:Binary operations]]\n[[Category:Differential topology]]\n[[Category:Riemannian geometry]]"
    },
    {
      "title": "Logical consequence",
      "url": "https://en.wikipedia.org/wiki/Logical_consequence",
      "text": "{{short description|Fundamental concept in logic}}\n{{redirect|Entailment||Entail (disambiguation)}}\n{{redirect|Therefore|the therefore symbol  ∴|Therefore sign}}\n{{redirect|Logical implication|the binary connective|Material conditional}}\n{{redirect|⊧|the symbol|Double turnstile}}\n\n'''Logical consequence''' (also '''entailment''') is a fundamental [[concept]] in [[logic]], which describes the relationship between [[statement (logic)|statement]]s that hold true when one statement logically ''follows from'' one or more statements. A [[Validity (logic)|valid]] logical [[argument]] is one in which the [[Consequent|conclusion]] is entailed by the [[premise]]s, because the conclusion is the consequence of the premises. The [[philosophical analysis]] of logical consequence involves the questions: In what sense does a conclusion follow from its premises?  and What does it mean for a conclusion to be a consequence of premises?<ref name=\"sep\" >Beall, JC and Restall, Greg, ''[http://plato.stanford.edu/archives/fall2009/entries/logical-consequence/ Logical Consequence]'' The Stanford Encyclopedia of Philosophy (Fall 2009 Edition), Edward N. Zalta (ed.).</ref>  All of [[philosophical logic]] is meant to provide accounts of the nature of logical consequence and the nature of [[logical truth]].<ref>[[Willard Van Orman Quine|Quine, Willard Van Orman]], ''Philosophy of Logic''.</ref>\n\nLogical consequence is [[logical truth|necessary]] and [[Formalism (philosophy of mathematics)|formal]], by way of examples that explain with [[formal proof]] and [[interpretation (logic)|models of interpretation]].<ref name=\"sep\" /> A sentence is said to be a logical consequence of a set of sentences, for a given [[Formal language|language]], [[if and only if]], using only logic (i.e. without regard to any ''personal'' interpretations of the sentences) the sentence must be true if every sentence in the set is true.<ref name=\"iep\">[[Matthew W. McKeon|McKeon, Matthew]], ''[http://www.iep.utm.edu/logcon/ Logical Consequence]'' Internet Encyclopedia of Philosophy.</ref>\n\nLogicians make precise accounts of logical consequence regarding a given [[formal language|language]] <math>\\mathcal{L}</math>, either by constructing a [[deductive system]] for <math>\\mathcal{L}</math> or by formal [[Intended interpretation|intended semantics]] for language <math>\\mathcal{L}</math>.  The Polish logician [[Alfred Tarski]] identified three features of an adequate characterization of entailment: (1) The logical consequence relation relies on the [[logical form]] of the sentences, (2) The relation is [[a priori and a posteriori|a priori]], i.e. it can be determined with or without regard to [[empirical evidence]] (sense experience), and (3) The logical consequence relation has a [[modal logic|modal]] component.<ref name=\"iep\" />\n\n== Formal accounts ==\nThe most widely prevailing view on how to best account for logical consequence is to appeal to formality. This is to say that whether statements follow from one another logically depends on the structure or [[logical form]] of the statements without regard to the contents of that form.\n\nSyntactic accounts of logical consequence rely on [[schema (logic)|schemes]] using [[inference rule]]s. For instance, we can express the logical form of a valid argument as:\n\n: All ''X'' are ''Y''\n: All ''Y'' are ''Z''\n: Therefore, all ''X'' are ''Z''.\n\nThis argument is formally valid, because every [[Substitution (logic)|instance]] of arguments constructed using this scheme is valid.\n\nThis is in contrast to an argument like \"Fred is Mike's brother's son. Therefore Fred is Mike's nephew.\" Since this argument depends on the meanings of the words \"brother\", \"son\", and \"nephew\", the statement \"Fred is Mike's nephew\" is a so-called [[material conditional|material consequence]] of \"Fred is Mike's brother's son,\" not a formal consequence. A formal consequence must be true ''in all cases'', however this is an incomplete definition of formal consequence, since even the argument \"''P'' is ''Q''&apos;s brother's son, therefore ''P'' is ''Q''&apos;s nephew\" is valid in all cases, but is not a ''formal'' argument.<ref name=\"sep\" />\n\n== A priori property of logical consequence ==\n\nIf you know that <math>Q</math> follows logically from <math>P</math> no information about the possible interpretations of <math>P</math> or <math>Q</math> will affect that knowledge. Our knowledge that <math>Q</math> is a logical consequence of <math>P</math> cannot be influenced by [[A priori and a posteriori|empirical knowledge]].<ref name=\"sep\" /> Deductively valid arguments can be known to be so without recourse to experience, so they must be knowable a priori.<ref name=\"sep\" /> However, formality alone does not guarantee that logical consequence is not influenced by empirical knowledge. So the a priori property of logical consequence is considered to be independent of formality.<ref name=\"sep\" />\n\n== Proofs and models ==\nThe two prevailing techniques for providing accounts of logical consequence involve expressing the concept in terms of ''proofs'' and via ''models''. The study of the syntactic consequence (of a logic) is called (its) [[proof theory]] whereas the study of (its) semantic consequence is called (its) [[model theory]].<ref name=\"ChiaraDoets1996\">{{cite book|editor1=Maria Luisa Dalla Chiara |editor2=Kees Doets |editor3=Daniele Mundici |editor4=Johan van Benthem |title=Logic and Scientific Methods: Volume One of the Tenth International Congress of Logic, Methodology and Philosophy of Science, Florence, August 1995|chapter-url=https://books.google.com/books?id=TCthvF8xLIAC&pg=PA292|year=1996|publisher=Springer|isbn=978-0-7923-4383-7|page=292|chapter=Logical consequence: a turn in style|author=Kosta Dosen}}</ref>\n\n=== Syntactic consequence ===\n{{See also|Therefore_sign|label 1= ∴|Turnstile_(symbol)|label 2= ⊢}}\n\nA formula <math>A</math> is a '''syntactic consequence'''<ref>[[Michael Dummett|Dummett, Michael]] (1993) [https://books.google.com/books?id=EYP7uCZIRQYC&pg=PA82&lpg=PA82&dq=syntactic+consequence&source=bl&ots=Ms58438B6w&sig=FE38FCaZpRpAr18gtG7INX4wieM&hl=en&ei=qOy7SoLlEI7KsQPgnYG7BA&sa=X&oi=book_result&ct=result&resnum=6#v=onepage&q=syntactic%20consequence&f=false''Frege: philosophy of language''] Harvard University Press, p.82ff</ref><ref>[[Jonathan Lear|Lear, Jonathan]] (1986) [https://books.google.com/books?id=lXI7AAAAIAAJ&pg=PA1&lpg=PA1&dq=syntactic+consequence&source=bl&ots=8IYWyFYTN-&sig=wrOg75cFxQwn1Uq-8LShBNXf9w0&hl=en&ei=I-y7SpHtLZLotgOsnLHcBQ&sa=X&oi=book_result&ct=result&resnum=10#v=onepage&q=syntactic%20consequence&f=false''Aristotle and Logical Theory''] Cambridge University Press, 136p.</ref><ref>Creath, Richard, and [[Michael Friedman (philosopher)|Friedman, Michael]] (2007) [https://books.google.com/books?id=87BcFLgJmxMC&pg=PA189&lpg=PA189&dq=syntactic+consequence&source=bl&ots=Fn2zomcMZP&sig=8hnJWsJFysNhmWLskICo4IQDYAc&hl=en&ei=I-y7SpHtLZLotgOsnLHcBQ&sa=X&oi=book_result&ct=result&resnum=6#v=onepage&q=syntactic%20consequence&f=false''The Cambridge companion to Carnap''] Cambridge University Press, 371p.</ref><ref>[http://www.swif.uniba.it/lei/foldop/foldoc.cgi?syntactic+consequence FOLDOC: \"syntactic consequence\"] {{webarchive|url=https://web.archive.org/web/20130403201417/http://www.swif.uniba.it/lei/foldop/foldoc.cgi?syntactic+consequence |date=2013-04-03 }}</ref> within some [[formal system]] <math>\\mathcal{FS}</math> of a set <math>\\Gamma</math> of formulas if there is a [[formal proof]] in <math>\\mathcal{FS}</math> of <math>A</math> from the set <math>\\Gamma</math>.\n\n:<math>\\Gamma \\vdash_{\\mathcal {FS} } A</math>\n\nSyntactic consequence does not depend on any [[interpretation (logic)|interpretation]] of the formal system.<ref>[[Geoffrey Hunter (logician)|Hunter, Geoffrey]], Metalogic: An Introduction to the Metatheory of Standard First-Order Logic, University of California Pres, 1971, p. 75.</ref>\n\n=== Semantic consequence ===\n{{See also|Double turnstile|label 1= ⊨}}\n\nA formula <math>A</math> is a '''semantic consequence''' within some formal system <math>\\mathcal{FS}</math> of a set of statements <math>\\Gamma</math>\n\n:<math>\\Gamma \\models_{\\mathcal {FS} } A,</math>\n\nif and only if there is no model <math>\\mathcal{I}</math> in which all members of <math>\\Gamma</math> are true and <math>A</math> is false.<ref>[[John Etchemendy|Etchemendy, John]], ''Logical consequence'',  The Cambridge Dictionary of Philosophy</ref>  Or, in other words, the set of the interpretations that make all members of <math>\\Gamma</math> true is a subset of the set of the interpretations that make <math>A</math> true.\n\n== Modal accounts ==\n\n[[Modal logic|Modal]] accounts of logical consequence are variations on the following basic idea:\n\n:<math>\\Gamma</math> <math>\\vdash</math> <math>A</math> is true if and only if it is ''necessary'' that if all of the elements of <math>\\Gamma</math> are true, then <math>A</math> is true.\n\nAlternatively (and, most would say, equivalently):\n\n:<math>\\Gamma</math> <math>\\vdash</math> <math>A</math> is true if and only if it is ''impossible'' for all of the elements of <math>\\Gamma</math> to be true and <math>A</math> false.\n\nSuch accounts are called \"modal\" because they appeal to the modal notions of [[Logical truth|logical necessity]]  and  [[logical possibility]]. 'It is necessary that' is often expressed as a [[universal quantification|universal quantifier]] over [[possible world]]s, so that the accounts above translate as:\n\n:<math>\\Gamma</math> <math>\\vdash</math> <math>A</math> is true if and only if there is no possible world at which all of the elements of <math>\\Gamma</math> are true and <math>A</math> is false (untrue).\n\nConsider the modal account in terms of the argument given as an example above:\n\n:All frogs are green.\n:Kermit is a frog.\n:Therefore, Kermit is green.\n\nThe conclusion is a logical consequence of the premises because we can't imagine a possible world where (a) all frogs are green; (b) Kermit is a frog; and (c) Kermit is not green.\n\n=== Modal-formal accounts ===\n\nModal-formal accounts of logical consequence combine the modal and formal accounts above, yielding variations on the following basic idea:\n\n:<math>\\Gamma</math> <math>\\vdash</math> <math>A</math> if and only if it is impossible for an argument with the same logical form as <math>\\Gamma</math>/<math>A</math> to have true premises and a false conclusion.\n\n=== Warrant-based accounts ===\n\nThe accounts considered above are all \"truth-preservational,\" in that they all assume that the characteristic feature of a good inference is that it never allows one to move from true premises to an untrue conclusion. As an alternative, some have proposed \"[[Theory of justification|warrant]]-preservational\" accounts, according to which the characteristic feature of a good inference is that it never allows one to move from justifiably assertible premises to a conclusion that is not justifiably assertible. This is (roughly) the account favored by [[intuitionist]]s such as [[Michael Dummett]].\n\n=== Non-monotonic logical consequence ===\n{{main|Non-monotonic logic}}\n\nThe accounts discussed above all yield [[Monotonicity of entailment|monotonic]] consequence relations, i.e. ones such that if <math>A</math> is a consequence of <math>\\Gamma</math>, then <math>A</math> is a consequence of any superset of <math>\\Gamma</math>. It is also possible to specify non-monotonic consequence relations to capture the idea that, e.g., 'Tweety can fly' is a logical consequence of\n\n:{Birds can typically fly, Tweety is a bird}\n\nbut not of\n\n:{Birds can typically fly, Tweety is a bird, Tweety is a penguin}.\n\nFor more on this, see [[Belief revision#Non-monotonic inference relation|Non-monotonic inference relation]].\n\n==See also==\n{{div col begin}}\n* [[Abstract algebraic logic]]\n* [[Ampheck]]\n* [[Boolean algebra (logic)]]\n* [[Boolean domain]]\n* [[Boolean function]]\n* [[Boolean logic]]\n* [[Causality]]\n* [[Deductive reasoning]]\n* [[Logic gate]]\n* [[Logical graph]]\n* [[Peirce's law]]\n* [[Probabilistic logic]]\n* [[Propositional calculus]]\n* [[Sole sufficient operator]]\n* [[Strict conditional]]\n* [[Tautology (logic)]]\n* [[Tautological consequence]]\n* [[Therefore sign]]\n* [[Turnstile (symbol)]]\n* [[Double turnstile]]\n* [[Validity (logic)|Validity]]\n{{div col end}}\n\n== Notes ==\n{{reflist}}\n\n== Resources ==\n* {{citation|last1=Anderson|first1=A.R.|last2=Belnap|first2=N.D., Jr.|title=Entailment|year=1975|publisher=Princeton|location=Princeton, NJ|volume=1}}.\n* {{citation|last=Augusto|first=Luis M.|year=2017|title=Logical consequences. Theory and applications: An introduction.}} London: College Publications. Series: [http://www.collegepublications.co.uk/logic/mlf/?00029 Mathematical logic and foundations].\n* {{citation|last1=Barwise|first1=Jon|author1link=Jon Barwise|last2=Etchemendy|first2=John|author2link=John Etchemendy|year=2008|title=Language, Proof and Logic|publisher=CSLI Publications|location=Stanford}}.\n* {{citation|last=Brown | first=Frank Markham | year=2003 |title=Boolean Reasoning: The Logic of Boolean Equations}} 1st edition, Kluwer Academic Publishers, Norwell, MA. 2nd edition, Dover Publications, Mineola, NY, 2003.\n* {{citation|authorlink=Martin Davis (mathematician)|last=Davis|first= Martin, (editor)|title=The Undecidable, Basic Papers on Undecidable Propositions, Unsolvable Problems And Computable Functions|publisher=Raven Press|location=New York|year=1965|url=https://books.google.com/books?id=qW8x7sQ4JXgC&printsec=frontcover#v=onepage&q=consequence&f=false|isbn=9780486432281}}. Papers include those by [[Gödel]], [[Alonzo Church|Church]], [[J. Barkley Rosser|Rosser]], [[Kleene]], and [[Emil Leon Post|Post]].\n* {{citation |first=Michael |last=Dummett |year=1991 |title=The Logical Basis of Metaphysics |publisher=Harvard University Press|url=https://books.google.com/books?id=lvsVFxK3BPcC&printsec=frontcover#v=onepage&q=consequence&f=false|isbn=9780674537866 }}.\n* {{citation|last=Edgington| first=Dorothy|year=2001|title=Conditionals|publisher=Blackwell}} in Lou Goble (ed.), ''The Blackwell Guide to Philosophical Logic''.\n* {{citation|last=Edgington| first=Dorothy|year=2006|title=Conditionals|chapter-url=http://plato.stanford.edu/entries/conditionals| chapter=Indicative Conditionals| publisher=Metaphysics Research Lab, Stanford University}} in Edward N. Zalta (ed.), ''The Stanford Encyclopedia of Philosophy''.\n* {{citation |first=John |last= Etchemendy |year= 1990 |title=The Concept of Logical Consequence |publisher= Harvard University Press}}.\n* {{citation |last=Goble |first=Lou, ed.|year=2001 |title=The Blackwell Guide to Philosophical Logic |publisher= Blackwell}}.\n* {{citation |last=Hanson |first= William H|year= 1997 |title=The concept of logical consequence| journal=The Philosophical Review| volume=106|issue= 3|pages= 365–409|jstor= 2998398|doi= 10.2307/2998398}} 365–409.\n* {{citation |authorlink=Vincent F. Hendricks|last=Hendricks |first=Vincent F. |year=2005 |title=Thought 2 Talk: A Crash Course in Reflection and Expression |location=New York |publisher=Automatic Press / VIP |isbn= 978-87-991013-7-5}}\n* {{citation |last=Planchette |first=P. A. |year=2001 |title=Logical Consequence}} in Goble, Lou, ed., ''The Blackwell Guide to Philosophical Logic''. Blackwell.\n* {{citation|authorlink=W.V. Quine|last=Quine|first=W.V.| year=1982| title=Methods of Logic|location=Cambridge, MA|publisher=Harvard University Press}} (1st ed. 1950), (2nd ed. 1959), (3rd ed. 1972), (4th edition, 1982).\n*{{citation |authorlink=Stewart Shapiro |last=Shapiro |first=Stewart |year=2002 |title=Necessity, meaning, and rationality: the notion of logical consequence}} in D. Jacquette, ed., ''A Companion to Philosophical Logic''. Blackwell.\n*{{citation |authorlink=Alfred Tarski |last=Tarski  |first=Alfred |year= 1936 |title=On the concept of logical consequence}} Reprinted in Tarski, A., 1983. ''Logic, Semantics, Metamathematics'', 2nd ed. [[Oxford University Press]]. Originally published in [[Polish language|Polish]] and [[German language|German]].\n* {{cite book|author=Ryszard Wójcicki|title=Theory of Logical Calculi: Basic Theory of Consequence Operations|year=1988|publisher=Springer|isbn=978-90-277-2785-5}}\n* A paper on 'implication' from math.niu.edu, [http://www.math.niu.edu/~richard/Math101/implies.pdf Implication]\n* A definition of 'implicant' [http://www.allwords.com/word-implicant.html AllWords]\n\n==External links==\n{{Commonscat}}\n*{{cite SEP |url-id=logical-consequence |title=Logical Consequence|date=2013-11-19|edition=Winter 2016|last=Beall|first=Jc|last2=Restall|first2=Greg|author-link=Jc Beall|author2-link=Greg Restall}}\n*{{cite IEP |url-id=logcon/ |title=Logical consequence}}\n*{{InPho|taxonomy|2409}}\n*{{PhilPapers|category|logical-consequence-and-entailment}}\n*{{springer|title=Implication|id=p/i050280}}\n\n{{Logic}}\n{{Mathematical logic}}\n{{Logical connectives}}\n\n[[Category:Philosophical logic]]\n[[Category:Metalogic]]\n[[Category:Propositional calculus]]\n[[Category:Deductive reasoning]]\n[[Category:Concepts in logic]]\n[[Category:Concepts in logic#]]\n[[Category:Syntax (logic)]]\n[[Category:Logical consequence| ]]\n[[Category:Binary operations]]"
    },
    {
      "title": "Matrix addition",
      "url": "https://en.wikipedia.org/wiki/Matrix_addition",
      "text": "{{Use American English|date = January 2019}}\n{{Short description|Notions of sums for matrices in linear algebra}}\nIn [[mathematics]], '''matrix addition''' is the operation of adding two [[matrix (mathematics)|matrices]] by adding the corresponding entries together. However, there are other operations which could also be considered as a kind of [[addition]] for matrices, the [[direct sum]] and the [[Kronecker sum]].\n\n==Entrywise sum==\nTwo matrices must have an equal number of rows and columns to be added.<ref>Elementary Linear Algebra by Rorres Anton 10e p53</ref> The sum of two matrices '''A''' and '''B''' will be a matrix which has the same number of rows and columns as do '''A''' and '''B'''. The sum of '''A''' and '''B''', denoted '''A''' + '''B''', is computed by adding corresponding elements of '''A''' and '''B''':{{sfn |Lipschutz |Lipson}}<ref>{{cite book |title=Mathematical methods for physics and engineering |first1=K.F. |last1=Riley |first2=M.P.|last2=Hobson |first3=S.J. |last3=Bence | publisher=Cambridge University Press |year=2010 |isbn=978-0-521-86153-3}}</ref>\n\n:<math>\\begin{align}\n\\mathbf{A}+\\mathbf{B} & = \\begin{bmatrix}\n a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n \\vdots & \\vdots & \\ddots & \\vdots \\\\\n a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\\n\\end{bmatrix} +\n\n\\begin{bmatrix}\n b_{11} & b_{12} & \\cdots & b_{1n} \\\\\n b_{21} & b_{22} & \\cdots & b_{2n} \\\\\n \\vdots & \\vdots & \\ddots & \\vdots \\\\\n b_{m1} & b_{m2} & \\cdots & b_{mn} \\\\\n\\end{bmatrix} \\\\\n& = \\begin{bmatrix}\n a_{11} + b_{11} & a_{12} + b_{12} & \\cdots & a_{1n} + b_{1n} \\\\\n a_{21} + b_{21} & a_{22} + b_{22} & \\cdots & a_{2n} + b_{2n} \\\\\n \\vdots & \\vdots & \\ddots & \\vdots \\\\\n a_{m1} + b_{m1} & a_{m2} + b_{m2} & \\cdots & a_{mn} + b_{mn} \\\\\n\\end{bmatrix} \\\\\n\n\\end{align}\\,\\!</math>\n\nFor example:\n\n:<math>\n  \\begin{bmatrix}\n    1 & 3 \\\\\n    1 & 0 \\\\\n    1 & 2\n  \\end{bmatrix}\n+\n  \\begin{bmatrix}\n    0 & 0 \\\\\n    7 & 5 \\\\\n    2 & 1\n  \\end{bmatrix}\n=\n  \\begin{bmatrix}\n    1+0 & 3+0 \\\\\n    1+7 & 0+5 \\\\\n    1+2 & 2+1\n  \\end{bmatrix}\n=\n  \\begin{bmatrix}\n    1 & 3 \\\\\n    8 & 5 \\\\\n    3 & 3\n  \\end{bmatrix}\n</math>\n\nWe can also subtract one matrix from another, as long as they have the same dimensions. '''A''' &minus; '''B''' is computed by subtracting elements of '''B''' from corresponding elements of '''A''', and has the same dimensions as '''A''' and '''B'''. For example:\n\n:<math>\n\\begin{bmatrix}\n 1 & 3 \\\\\n 1 & 0 \\\\    \n 1 & 2\n\\end{bmatrix}\n-\n\\begin{bmatrix}\n 0 & 0 \\\\\n 7 & 5 \\\\\n 2 & 1\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n 1-0 & 3-0 \\\\\n 1-7 & 0-5 \\\\\n 1-2 & 2-1\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n 1 & 3 \\\\\n -6 & -5 \\\\\n -1 & 1\n\\end{bmatrix}\n</math>\n\n==<span id=\"directsum\"></span>Direct sum==\nAnother operation, which is used less often, is the direct sum (denoted by ⊕). Note the Kronecker sum is also denoted ⊕; the context should make the usage clear.  The direct sum of any pair of matrices '''A''' of size ''m'' &times; ''n'' and '''B''' of size ''p'' &times; ''q'' is a matrix of size (''m'' + ''p'') &times; (''n'' + ''q'') defined as <ref>{{MathWorld |id=MatrixDirectSum |title=Matrix Direct Sum}}</ref>{{sfn |Lipschutz |Lipson}}\n\n:'''<math>\n  \\mathbf{A} \\oplus \\mathbf{B} =\n  \\begin{bmatrix} \\mathbf{A} & \\boldsymbol{0} \\\\ \\boldsymbol{0} & \\mathbf{B} \\end{bmatrix} =\n  \\begin{bmatrix}\n     a_{11} & \\cdots & a_{1n} &      0 & \\cdots &      0 \\\\\n     \\vdots & \\ddots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    a_{m 1} & \\cdots & a_{mn} &      0 & \\cdots &      0 \\\\\n          0 & \\cdots &      0 & b_{11} & \\cdots &  b_{1q} \\\\\n     \\vdots & \\ddots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n          0 & \\cdots &      0 & b_{p1} & \\cdots &  b_{pq}\n  \\end{bmatrix}\n</math>'''\n\nFor instance,\n\n:<math>\n  \\begin{bmatrix}\n    1 & 3 & 2 \\\\\n    2 & 3 & 1\n  \\end{bmatrix}\n\\oplus\n  \\begin{bmatrix}\n    1 & 6 \\\\\n    0 & 1\n  \\end{bmatrix}\n=\n  \\begin{bmatrix}\n    1 & 3 & 2 & 0 & 0 \\\\\n    2 & 3 & 1 & 0 & 0 \\\\\n    0 & 0 & 0 & 1 & 6 \\\\\n    0 & 0 & 0 & 0 & 1\n  \\end{bmatrix}\n</math>\n\nThe direct sum of matrices is a special type of [[block matrix]], in particular the direct sum of square matrices is a [[Block matrix#Block diagonal matrices|block diagonal matrix]].\n\nThe [[adjacency matrix]] of the union of disjoint [[Graph (discrete mathematics)|graphs]] or [[multigraph]]s is the direct sum of their adjacency matrices. Any element in the [[Direct sum of modules|direct sum]] of two [[vector space]]s of matrices can be represented as a direct sum of two matrices.\n\nIn general, the direct sum of ''n'' matrices is:{{sfn |Lipschutz |Lipson}}\n:<math>\n\\bigoplus_{i=1}^{n} \\mathbf{A}_{i} = {\\rm diag}( \\mathbf{A}_1, \\mathbf{A}_2, \\mathbf{A}_3 \\cdots \\mathbf{A}_n)=\n\\begin{bmatrix}\n \\mathbf{A}_1 & \\boldsymbol{0} & \\cdots & \\boldsymbol{0} \\\\\n \\boldsymbol{0} & \\mathbf{A}_2 & \\cdots & \\boldsymbol{0} \\\\\n \\vdots & \\vdots & \\ddots & \\vdots \\\\\n \\boldsymbol{0} & \\boldsymbol{0} & \\cdots & \\mathbf{A}_n \\\\\n\\end{bmatrix}\\,\\!</math>\n\nwhere the zeros are actually blocks of zeros, i.e. zero matrices.\n\n==Kronecker sum==\n{{main article|Kronecker sum}}\nThe Kronecker sum is different from the direct sum but is also denoted by ⊕. It is defined using the [[Kronecker product]] ⊗ and normal matrix addition. If '''A''' is ''n''-by-''n'', '''B''' is ''m''-by-''m'' and   <math>\\mathbf{I}_k</math> denotes the ''k''-by-''k'' identity matrix then the Kronecker sum is defined by:\n:<math> \\mathbf{A} \\oplus \\mathbf{B} = \\mathbf{A} \\otimes \\mathbf{I}_m + \\mathbf{I}_n \\otimes \\mathbf{B}. </math>\n\n==See also==\n* [[Matrix multiplication]]\n* [[Vector addition]]\n\n==Notes==\n{{reflist}}\n\n==References==\n* {{cite book |ref=harv |title=Linear Algebra |first1=S. |last1=Lipschutz |first2=M. |last2=Lipson |series=Schaum's Outline Series |year=2009 |isbn=978-0-07-154352-1}}\n\n==External links==\n*{{PlanetMath |urlname=DirectSumOfMatrices |title= Direct sum of matrices}}\n* [http://drexel28.wordpress.com/2010/12/22/direct-sum-of-linear-transformations-and-direct-sum-of-matrices-pt-iii/ Abstract nonsense: Direct Sum of Linear Transformations and Direct Sum of Matrices]\n* [http://www.mymathlib.com/matrices/arithmetic/direct_sum.html Mathematics Source Library: Arithmetic Matrix Operations]\n* [http://www.aps.uoguelph.ca/~lrs/ABMethods/NOTES/CDmatrix.pdf Matrix Algebra and R]\n\n[[Category:Linear algebra]]\n[[Category:Binary operations]]"
    },
    {
      "title": "Frobenius inner product",
      "url": "https://en.wikipedia.org/wiki/Frobenius_inner_product",
      "text": "{{refimprove|date=March 2017}}\nIn [[mathematics]], the '''Frobenius inner product''' is a binary operation that takes two [[matrix (mathematics)|matrices]] and returns a number. It is often denoted <math>\\langle \\mathbf{A},\\mathbf{B} \\rangle_\\mathrm{F}</math>. The operation is a component-wise [[inner product]] of two matrices as though they are vectors. The two matrices must have the same dimension&mdash;same number of rows and columns&mdash;but are not restricted to be [[square matrix|square matrices]].\n\n==Definition==\n\nGiven two [[complex number]]-valued ''n''×''m'' matrices '''A''' and '''B''', written explicitly as\n\n:<math> \\mathbf {A} ={\\begin{pmatrix}A_{11}&A_{12}&\\cdots &A_{1m}\\\\A_{21}&A_{22}&\\cdots &A_{2m}\\\\\\vdots &\\vdots &\\ddots &\\vdots \\\\A_{n1}&A_{n2}&\\cdots &A_{nm}\\\\\\end{pmatrix}} \\,, \\quad \\mathbf {B} ={\\begin{pmatrix}B_{11}&B_{12}&\\cdots &B_{1m}\\\\B_{21}&B_{22}&\\cdots &B_{2m}\\\\\\vdots &\\vdots &\\ddots &\\vdots \\\\B_{n1}&B_{n2}&\\cdots &B_{nm}\\\\\\end{pmatrix}}</math>\n\nthe Frobenius inner product is defined by the following [[summation]] Σ of matrix elements,\n\n:<math> \\langle \\mathbf{A}, \\mathbf{B} \\rangle_\\mathrm{F} =\\sum_{i,j}\\overline{A_{ij}} B_{ij} \\,, = \\mathrm{tr}\\left(\\overline{\\mathbf{A}^T} \\mathbf{B}\\right)</math>\n\nwhere the overline denotes the [[complex conjugate]]. Explicitly this sum is\n\n:<math>\\begin{align} \\langle \\mathbf{A}, \\mathbf{B} \\rangle_\\mathrm{F} = & \\overline{A}_{11} B_{11} + \\overline{A}_{12} B_{12} + \\cdots + \\overline{A}_{1m} B_{1m} \\\\\n & + \\overline{A}_{21} B_{21} + \\overline{A}_{22} B_{22} + \\cdots + \\overline{A}_{2m} B_{2m} \\\\\n & \\vdots \\\\\n & + \\overline{A}_{n1} B_{n1} + \\overline{A}_{n2} B_{n2} + \\cdots + \\overline{A}_{nm} B_{nm} \\\\\n\\end{align}</math>\n\nThe calculation is very similar to the [[dot product]], which in turn is an example of an inner product.\n\n===Properties===\n\nIt is a [[sesquilinear form]], for four complex-valued matrices '''A''', '''B''', '''C''', '''D''', and two complex numbers ''a'' and ''b'':\n\n:<math>\\langle a\\mathbf{A}, b\\mathbf{B} \\rangle_\\mathrm{F} = \\overline{a}b\\langle \\mathbf{A}, \\mathbf{B} \\rangle_\\mathrm{F} </math>\n:<math>\\langle \\mathbf{A}+\\mathbf{C}, \\mathbf{B} + \\mathbf{D} \\rangle_\\mathrm{F} = \\langle \\mathbf{A}, \\mathbf{B} \\rangle_\\mathrm{F} + \\langle \\mathbf{A}, \\mathbf{D} \\rangle_\\mathrm{F} + \\langle \\mathbf{C}, \\mathbf{B} \\rangle_\\mathrm{F} + \\langle \\mathbf{C}, \\mathbf{D} \\rangle_\\mathrm{F} </math>\n\nAlso, exchanging the matrices amounts to complex conjugation:\n\n:<math>\\langle \\mathbf{B}, \\mathbf{A} \\rangle_\\mathrm{F} = \\overline{\\langle \\mathbf{A}, \\mathbf{B} \\rangle_\\mathrm{F}} </math>\n\nFor the same matrix,\n\n:<math>\\langle \\mathbf{A}, \\mathbf{A} \\rangle_\\mathrm{F} \\geq 0 \\,. </math>\n\n==Examples==\n\n===Real-valued matrices===\n\nFor two real-valued matrices, if\n\n:<math>\\mathbf{A} = \\begin{pmatrix} 2 & 0 & 6 \\\\ 1 & -1 & 2 \\end{pmatrix} \\,,\\quad \\mathbf{B} = \\begin{pmatrix} 8 & -3 & 2 \\\\ 4 & 1 & -5 \\end{pmatrix} </math>\n\nthen\n\n:<math>\\begin{align}\\langle \\mathbf{A} ,\\mathbf{B}\\rangle_\\mathrm{F} & = 2\\cdot 8 + 0\\cdot (-3) + 6\\cdot 2 + 1\\cdot 4 + (-1)\\cdot 1 + 2\\cdot(-5) \\\\\n& = 16 + 12 + 4 - 1 - 10 \\\\\n& = 21 \\end{align} </math>\n\n===Complex-valued matrices===\n\nFor two complex-valued matrices, if\n\n:<math>\\mathbf{A} = \\begin{pmatrix} 1+i & -2i \\\\ 3 & -5 \\end{pmatrix} \\,,\\quad \\mathbf{B} = \\begin{pmatrix} -2 & 3i \\\\ 4-3i & 6 \\end{pmatrix} </math>\n\nthen the complex conjugates (without transpose) are\n\n:<math>\\overline{\\mathbf{A}} = \\begin{pmatrix} 1-i & +2i \\\\ 3 & -5 \\end{pmatrix} \\,,\\quad \\overline{\\mathbf{B}} = \\begin{pmatrix} -2 & -3i \\\\ 4+3i & 6 \\end{pmatrix} </math>\n\nand\n\n:<math>\\begin{align} \\langle \\mathbf{A} ,\\mathbf{B}\\rangle_\\mathrm{F} & = (1-i)\\cdot (-2) + (+2i)\\cdot 3i + 3\\cdot (4-3i) + (-5)\\cdot 6 \\\\\n& = (-2+2i) + -6 + 12-9i + -30 \\\\\n& = -26 -7i \\end{align} </math>\n\nwhile\n\n:<math>\\begin{align} \\langle \\mathbf{B} ,\\mathbf{A}\\rangle_\\mathrm{F} & = (-2)\\cdot (1+i) + (-3i)\\cdot (-2i) + (4+3i)\\cdot 3 + 6 \\cdot (-5) \\\\\n& = -26 + 7i \\end{align} </math>\n\nThe Frobenius inner products of '''A''' with itself, and '''B''' with itself, are respectively\n\n:<math>\\langle \\mathbf{A}, \\mathbf{A} \\rangle_\\mathrm{F} = 2 + 4 + 9 + 25 = 40 </math>\n:<math>\\langle \\mathbf{B}, \\mathbf{B} \\rangle_\\mathrm{F} = 4 + 9 + 25 + 36 = 74 </math>\n\n==Frobenius norm==\n\nThe inner product induces the [[Frobenius norm]]\n\n:<math>\\|\\mathbf{A}\\|_\\mathrm{F} = \\sqrt{\\langle \\mathbf{A}, \\mathbf{A} \\rangle_\\mathrm{F}} \\,.</math>\n\n==Relation to other products==\n\nIf '''A''' and '''B''' are each [[real number|real]]-valued matrices, the Frobenius inner product is the sum of the entries of the [[Hadamard product (matrices)|Hadamard product]].\n\nIf the matrices are [[Vectorization (mathematics)|vectorised]] (denoted by \"vec\", converted into column vectors) as follows,\n\n:<math> \\mathrm{vec}(\\mathbf {A}) = {\\begin{pmatrix} A_{11} \\\\ A_{12} \\\\ \\vdots \\\\ A_{21} \\\\ A_{22} \\\\ \\vdots \\\\ A_{nm} \\end{pmatrix}},\\quad \\mathrm{vec}(\\mathbf {B}) = {\\begin{pmatrix} B_{11} \\\\ B_{12} \\\\ \\vdots \\\\ B_{21} \\\\ B_{22} \\\\ \\vdots \\\\ B_{nm} \\end{pmatrix}} \\,, </math>\n\nthe matrix product\n\n:<math> \\overline{\\mathrm{vec}(\\mathbf{A})}^T\\mathrm{vec}(\\mathbf {B}) = {\\begin{pmatrix} \\overline{A}_{11} & \\overline{A}_{12} & \\cdots & \\overline{A}_{21} & \\overline{A}_{22} & \\cdots & \\overline{A}_{nm} \\end{pmatrix}} {\\begin{pmatrix} B_{11} \\\\ B_{12} \\\\ \\vdots \\\\ B_{21} \\\\ B_{22} \\\\ \\vdots \\\\ B_{nm} \\end{pmatrix}} </math>\n\nreproduces the definition, therefore\n\n:<math> \\langle \\mathbf{A}, \\mathbf{B} \\rangle_\\mathrm{F} = \\overline{\\mathrm{vec}(\\mathbf{A})}^T \\mathrm{vec}(\\mathbf{B}) \\, . </math>\n\n==See also==\n\n*[[Hadamard product (matrices)]]\n*[[Hilbert–Schmidt inner product]]\n*[[Kronecker product]]\n*[[Matrix analysis]]\n*[[Matrix multiplication]]\n*[[Matrix norm]]\n\n{{algebra-footer}}\n\n{{DEFAULTSORT:Matrix Multiplication}}\n[[Category:Matrix theory]]\n[[Category:Bilinear operators]]\n[[Category:Binary operations]]\n[[Category:Multiplication]]\n[[Category:Numerical linear algebra]]"
    },
    {
      "title": "Matrix multiplication",
      "url": "https://en.wikipedia.org/wiki/Matrix_multiplication",
      "text": "{{short description|Mathematical operation in  linear algebra}}\nIn [[mathematics]], '''matrix multiplication''' or '''matrix product''' is a [[binary operation]] that produces a [[matrix (mathematics)|matrix]] from two matrices with entries in a [[field (mathematics)|field]], or, more generally, in a [[ring (mathematics)|ring]] or even a [[semiring]]. The matrix product is designed for representing the [[composition of functions|composition]] of [[linear map]]s that are represented by matrices. Matrix multiplication is thus a basic tool of [[linear algebra]], and as such has numerous applications in many areas of mathematics, as well as in [[applied mathematics]], [[statistics]], [[physics]], [[economics]], and [[engineering]].<ref name=\"Physics 1991\">{{cite book|title=Encyclopaedia of Physics|edition=2nd|first1=R. G. |last1=Lerner | first2= G. L. |last2 = Trigg|publisher=VHC publishers|date=1991|isbn=978-3-527-26954-9}}</ref><ref>{{cite book|title=McGraw Hill Encyclopaedia of Physics|edition=2nd|first = C. B. |last = Parker|date=1994|isbn=978-0-07-051400-3}}</ref> In more detail, if {{math|'''A'''}} is an {{math|''n'' × ''m''}} matrix and {{math|'''B'''}} is an {{math|''m'' × ''p''}} matrix, their matrix product {{math|'''AB'''}} is an {{math|''n'' × ''p''}} matrix, in which the {{math|''m''}} entries across a row of {{math|'''A'''}} are multiplied with the {{math|''m''}} entries down a column of {{math|'''B'''}} and summed to produce an entry of {{math|'''AB'''}}.  When two linear maps are represented by matrices, then the matrix product represents the composition of the two maps.\n\nThe definition of matrix product requires that the entries belong to a semiring, and does not require multiplication of elements of the semiring to be [[commutative property|commutative]]. In many applications, the matrix elements belong to a field, although the [[tropical semiring]] is also a common choice for graph [[shortest path]] problems.<ref>{{cite book|title=Randomized Algorithms|first1=Rajeev|last1=Motwani|author1-link=Rajeev Motwani|first2=Prabhakar|last2=Raghavan|author2-link=Prabhakar Raghavan|publisher=Cambridge University Press|year=1995|isbn=9780521474658|page=280|url=https://books.google.com/books?id=QKVY4mDivBEC&pg=PA280}}</ref> Even in the case of matrices over fields, the product is not commutative in general, although it is [[Associative property|associative]] and is [[Distributive property|distributive]] over [[matrix addition]]. The [[identity matrices]] (which are the [[square matrices]] whose entries are zero outside of the main diagonal and 1 on the main diagonal) are [[identity element]]s of the matrix product. It follows that the {{math|''n'' × ''n''}} matrices over a [[Ring (mathematics)|ring]] form a ring, which is noncommutative except if {{math|1=''n'' = 1}} and the ground ring is commutative.\n\nA square matrix may have a [[multiplicative inverse]], called an [[inverse matrix]]. In the common case where the entries belong to a [[commutative ring]] {{mvar|r}}, a matrix has an inverse if and only if its [[determinant]] has a multiplicative inverse in {{mvar|r}}. The determinant of a product of square matrices is the product of the determinants of the factors. The {{math|''n'' × ''n''}} matrices that have an inverse form a [[group (mathematics)|group]] under matrix multiplication, the [[subgroup]]s of which are called [[matrix group]]s. Many classical groups (including all [[finite group]]s) are [[group isomorphism|isomorphic]] to matrix groups; this is the starting point of the theory of [[group representation]]s.\n\nComputing matrix products is a central operation in all computational applications of linear algebra. Its [[computational complexity]] is {{tmath|O(n^3)}} (for {{math|''n'' × ''n''}} matrices) for the basic algorithm (this complexity is {{tmath|O(n^{2.373})}} for the asymptotically fastest known algorithm<ref name=\"LeGall2014\">{{Citation | last1=Le Gall | first1=François | contribution=Powers of tensors and fast matrix multiplication | year = 2014 | arxiv=1401.7714 | title = Proceedings of the 39th International Symposium on Symbolic and Algebraic Computation (ISSAC 2014)| bibcode=2014arXiv1401.7714L }}</ref>). This [[linear time|nonlinear complexity]] means that matrix product is often the critical part of many algorithms. This is enforced by the fact that many operations on matrices, such as matrix inversion, determinant, solving [[systems of linear equations]], have the same complexity. Therefore various algorithms have been devised for computing products of large matrices, taking into account the architecture of [[computer]]s (see [[BLAS]], for example).\n\n==Notation==\n\nThis article will use the following notational conventions: matrices are represented by capital letters in bold, e.g. {{math|'''A'''}}, [[Euclidean vector|vectors]] in lowercase bold, e.g. {{math|'''a'''}}, and entries of vectors and matrices are italic (since they are numbers from a field), e.g. {{math|''A''}} and {{math|''a''}}. [[Index notation]] is often the clearest way to express definitions, and is used as standard in the literature. The {{math|''i, j''}} entry of matrix {{math|'''A'''}} is indicated by {{math|('''A''')<sub>''ij''</sub>}}, {{math|''A''<sub>''ij''</sub>}} or {{math|''a''<sub>''ij''</sub>}}, whereas a numerical label (not matrix entries) on a collection of matrices is subscripted only, e.g. {{math|'''A'''<sub>1</sub>, '''A'''<sub>2</sub>}}, etc.\n\n==Definition==\nIf {{math|'''A'''}} is an {{math|''n'' × ''m''}} matrix and {{math|'''B'''}} is an {{math|''m'' × ''p''}} matrix,\n\n:<math>\\mathbf{A}=\\begin{pmatrix}\n a_{11} & a_{12} & \\cdots & a_{1m} \\\\\n a_{21} & a_{22} & \\cdots & a_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n a_{n1} & a_{n2} & \\cdots & a_{nm} \\\\\n\\end{pmatrix},\\quad\\mathbf{B}=\\begin{pmatrix}\n b_{11} & b_{12} & \\cdots & b_{1p} \\\\\n b_{21} & b_{22} & \\cdots & b_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n b_{m1} & b_{m2} & \\cdots & b_{mp} \\\\\n\\end{pmatrix}</math>\n\nthe ''matrix product'' {{math|1='''C''' = '''AB'''}}  (denoted without multiplication signs or dots) is defined to be the {{math|''n'' × ''p''}} matrix<ref>{{cite book|title=Linear Algebra|edition=4th| first1 = S. | last1 = Lipschutz | first2 = M. | last2 = Lipson|series=Schaum's Outlines|publisher=McGraw Hill (USA)|date=2009|pages=30–31|isbn=978-0-07-154352-1}}</ref><ref>{{cite book|title=Mathematical methods for physics and engineering| first1 = K. F. | last1 = Riley | first2 = M. P. | last2 = Hobson | first3 = S. J. | last3 = Bence|publisher=Cambridge University Press|date=2010|isbn=978-0-521-86153-3}}</ref><ref>{{cite book|title=Calculus, A Complete Course|edition=3rd| first = R. A. | last = Adams|publisher=Addison Wesley|date=1995|page=627|isbn=0 201 82823 5}}</ref><ref>{{cite book|title=Matrix Analysis| last = Horn | first = Johnson|edition=2nd|publisher=Cambridge University Press|date=2013|page=6|isbn=978 0 521 54823 6}}</ref>\n:<math>\\mathbf{C}=\\begin{pmatrix}\n c_{11} & c_{12} & \\cdots & c_{1p} \\\\\n c_{21} & c_{22} & \\cdots & c_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n c_{n1} & c_{n2} & \\cdots & c_{np} \\\\\n\\end{pmatrix}</math>\nsuch that \n:<math> c_{ij} = a_{i1}b_{1j} +\\cdots + a_{im}b_{mj}= \\sum_{k=1}^m a_{ik}b_{kj}, </math>\nfor {{math|1=''i'' = 1, ..., ''n''}} and {{math|1=''j'' = 1, ..., ''p''}}.\n\nThat is, the entry {{tmath|c_{ij} }} of the product is obtained by multiplying term-by-term the entries of the {{mvar|i}}th row of {{math|'''A'''}} and the {{mvar|j}}th column of {{math|'''B'''}}, and summing these {{mvar|m}} products. In other words, {{tmath|c_{ij} }} is the [[dot product]] of the {{mvar|i}}th row of {{math|'''A'''}} and the {{mvar|j}}th column of {{math|'''B'''}}.\n\nThus the product {{math|'''AB'''}} is defined if and only if the number of columns in {{math|'''A'''}} equals the number of rows in {{math|'''B'''}}, in this case {{math|''m''}}.\n\nUsually the entries are numbers, but they may be any kind [[mathematical object]]s for which an addition and a multiplication are defined, that are [[associative property|associative]], and such that the addition is [[commutative property|commutative]], and the multiplication is [[distributive property|distributive]] with respect to the addition. In particular, the entries may be matrices themselves (see [[block matrix]]).\n\n=== Illustration ===\n[[File:Matrix multiplication diagram 2.svg|right]]\n\nThe figure to the right illustrates diagrammatically the product of two matrices {{math|'''A'''}} and {{math|'''B'''}}, showing how each intersection in the product matrix corresponds to a row of {{math|'''A'''}} and a column of {{math|'''B'''}}.\n\n:<math>\n\\overset{4\\times 2 \\text{ matrix}}{\\begin{bmatrix}\n{a_{11}} & {a_{12}} \\\\\n\\cdot & \\cdot \\\\\n{a_{31}} & {a_{32}} \\\\\n\\cdot & \\cdot \\\\\n\\end{bmatrix}}\n\n\\overset{2\\times 3\\text{ matrix}}{\\begin{bmatrix}\n\\cdot & {b_{12}} & {b_{13}} \\\\\n\\cdot & {b_{22}} & {b_{23}} \\\\\n\\end{bmatrix}}\n\n= \\overset{4\\times 3\\text{ matrix}}{\\begin{bmatrix}\n\\cdot & x_{12} & x_{13} \\\\\n\\cdot & \\cdot & \\cdot \\\\\n\\cdot & x_{32} & x_{33} \\\\\n\\cdot & \\cdot & \\cdot \\\\\n\\end{bmatrix}}\n</math>\n\nThe values at the intersections marked with circles are:\n\n:<math>\\begin{align}\nx_{12} & = {{a_{11}}}{{b_{12}}} + {{a_{12}}}{{b_{22}}} \\\\\nx_{33} & = {{a_{31}}}{{b_{13}}} + {{a_{32}}}{{b_{23}}}\n\\end{align}</math>\n\n==Fundamental applications==\n\nHistorically, matrix multiplication has been introduced for making easier and clarifying computations in [[linear algebra]]. This strong relationship between matrix multiplication and linear algebra remains fundamental in all mathematics, as well as in [[physics]], [[engineering]] and [[computer science]].\n\n===Linear maps===\nIf a [[vector space]] has a finite [[basis (linear algebra)|basis]], its elements (vectors) are uniquely represented by a finite [[sequence (mathematics)|sequence]], called [[coordinate vector]], or scalars, which are the [[coordinates]] of the vector on the basis. These coordinates are commonly organized as a [[column matrix]] (also called ''column vector''), that is a matrix with only one column.\n\nA [[linear map]] {{mvar|A}} from a vector space of dimension {{mvar|n}} into a vector space of dimension {{mvar|m}} maps a column vector\n:<math>\\mathbf x=\\begin{pmatrix}x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n\\end{pmatrix}</math>\nonto the column vector\n:<math>\\mathbf y= A(\\mathbf x)= \\begin{pmatrix}a_{11}x_1+\\cdots + a_{1n}x_n\\\\ a_{21}x_1+\\cdots + a_{2n}x_n \\\\ \\vdots \\\\ a_{m1}x_1+\\cdots + a_{mn}x_n\\end{pmatrix}.</math>\nThe linear map {{mvar|A}} is thus defined by the matrix \n::<math>\\mathbf{A}=\\begin{pmatrix}\n a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\\n\\end{pmatrix}, </math>\nand maps the column vector <math>\\mathbf x</math> to the matrix product \n:<math>\\mathbf y = \\mathbf {Ax}.</math>\n\nIf {{mvar|B}} is another linear map from the preceding vector space of dimension {{mvar|m}}, into a vector space of dimension {{mvar|p}}, it is represented by a {{tmath|p\\times m}} matrix <math>\\mathbf B.</math> A straightforward computation shows that the matrix of the [[function composition|composite map]] {{tmath|B\\circ A}} is the matrix product <math>\\mathbf {BA}.</math> The general formula of the function composition (that is, {{tmath|1=(B\\circ A)(\\mathbf x) = B(A(\\mathbf x))}}) is instanced here as a specific case of associativity of matrix product (see [[#Associativity|below]]):\n:<math>(\\mathbf{BA})\\mathbf x = \\mathbf{B}(\\mathbf {Ax}) = \\mathbf{BAx}.</math>\n\n===System of linear equations===\nThe general form of a [[system of linear equations]] is\n:<math>\\begin{matrix}a_{11}x_1+\\cdots + a_{1n}x_n=b_1\n\\\\ a_{21}x_1+\\cdots + a_{2n}x_n =b_2\n\\\\ \\vdots \n\\\\ a_{m1}x_1+\\cdots + a_{mn}x_n =b_m\\end{matrix}.</math>\n\nUsing same notation as above, such a system is equivalent with the single matrix [[equation]]\n:<math>\\mathbf{Ax}=\\mathbf b.</math>\n\n===Dot product, bilinear form and inner product ===\nThe [[dot product]] of two column vectors is the matrix product \n:<math>\\mathbf x^\\mathsf T \\mathbf y,</math>\nwhere <math>\\mathbf x^\\mathsf T</math> is the [[row vector]] obtained by [[transpose|transposing]] <math>\\mathbf x</math> and the resulting 1×1 matrix is identified with its unique entry.\n\nMore generally, any [[bilinear form]] over a vector space of finite dimension may be expressed as a matrix product\n:<math>\\mathbf x^\\mathsf T \\mathbf {Ay},</math>\nand any [[inner product]] may be expressed as \n:<math>\\mathbf x^\\dagger \\mathbf {Ay},</math>\nwhere <math>\\mathbf x^\\dagger</math> denotes the [[conjugate transpose]] of <math>\\mathbf x</math> (conjugate of the transpose, or equivalently transpose of the conjugate).\n\n== General properties==\nMatrix multiplication shares some properties with usual [[multiplication]]. However, matrix multiplication is not defined if the number of columns of the first factor differs from the number of rows of the second factor, and it is [[non-commutative]], even when the product remains definite after changing the order of the factors.<ref>{{cite book|title=Linear Algebra|edition=4th| first1 = S. | last1 = Lipcshutz | first2 = M. | last2 = Lipson|series=Schaum's Outlines|publisher=McGraw Hill (USA)|chapter=2|date=2009|isbn=978-0-07-154352-1}}</ref><ref>{{cite book|title=Matrix Analysis| last = Horn | first =  Johnson|edition=2nd|chapter=0|publisher=Cambridge University Press|date=2013|isbn=978 0 521 54823 6}}</ref>\n\n{{for|[[differential of a function|differentials]] and [[derivative]]s of products of matrices|matrix calculus}}\n\n=== Non-commutativity ===\nAn operation is [[commutative property|commutative]] if, given two elements {{math|'''A'''}} and {{math|'''B'''}} such that the product <math>\\mathbf{A}\\mathbf{B}</math> is defined, then <math>\\mathbf{B}\\mathbf{A}</math> is also defined, and <math>\\mathbf{A}\\mathbf{B}=\\mathbf{B}\\mathbf{A}.</math>\n\nIf {{math|'''A'''}} and {{math|'''B'''}} are matrices of respective sizes {{tmath|m\\times n}} and {{tmath|p\\times q}}, then <math>\\mathbf{A}\\mathbf{B}</math> is defined if {{tmath|1=n=p}}, and <math>\\mathbf{B}\\mathbf{A}</math> is defined if {{tmath|1=m=q}}. Therefore, if one of the products is defined, the other is not defined in general. If {{tmath|1=m=q\\neq n=p}}, the two products are defined, but have different sizes; thus they cannot be equal.\n\nIt follows that the equality of the two products makes sense only if {{tmath|1=m=q= n=p}}, that is if {{math|'''A'''}} and {{math|'''B'''}} are [[square matrices]] of the same size. Even in this case, one has in general\n:<math>\\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}.</math> \nFor example\n:<math>\\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}\\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix}=\\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix},</math>\nand\n:<math>\\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix}\\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix}.</math>\n\nThis example may be expanded for showing that, if {{math|'''A'''}} is a {{tmath|n\\times n}} matrix with entries in a [[field (mathematics)|field]] {{mvar|F}}, then <math>\\mathbf{A}\\mathbf{B} = \\mathbf{B}\\mathbf{A}</math> for every {{tmath|n\\times n}} matrix {{math|'''B'''}} with entries in {{mvar|F}}, [[if and only if]] <math>\\mathbf{A}=c\\,\\mathbf{I}</math> where {{tmath|c\\in F}}, and {{math|'''I'''}} is the {{tmath|n\\times n}} [[identity matrix]]. If, instead of a field, the entries are supposed to belong to a [[ring (mathematics)|ring]], then one must add the condition that {{mvar|c}} belongs to the [[center (ring theory)|center]] of the ring.\n\n===Distributivity===\nThe matrix product is [[distributive property|distributive]] with respect of [[matrix addition]]. That is, if {{math|'''A''', '''B''', '''C''', '''D'''}} are matrices of respective sizes {{math|''m'' × ''n''}}, {{math|''n'' × ''p''}},  {{math|''n'' × ''p''}}, and {{math|''p'' × ''q''}}, one has (left distributivity)\n:<math>\\mathbf{A}(\\mathbf{B} + \\mathbf{C}) = \\mathbf{AB} + \\mathbf{AC},</math>\nand (right distributivity)\n:<math>(\\mathbf{B} + \\mathbf{C} )\\mathbf{D} = \\mathbf{BD} + \\mathbf{CD}.</math>\n\nThis results from the distributivity for coefficients by \n:<math>\\sum_k a_{ik}(b_{kj} + c_{kj}) = \\sum_k a_{ik}b_{kj} + \\sum_k a_{ik}c_{kj} </math>\n:<math>\\sum_k (b_{ik} + c_{ik}) d_{kj} = \\sum_k b_{ik}d_{kj} + \\sum_k c_{ik}d_{kj}. </math>\n\n===Product with a scalar===\nIf {{math|'''A'''}} is a matrix and {{mvar|c}} a scalar, then the matrices <math>c\\mathbf{A}</math> and <math>\\mathbf{A}c</math> are obtained by left or right multiplying all entries of {{math|'''A'''}} by {{mvar|c}}. If the scalars have the [[commutative property]], then <math>c\\mathbf{A} = \\mathbf{A}c.</math>\n\nIf the product <math>\\mathbf{AB}</math> is defined (that is the number of columns of {{math|'''A'''}} equals the number of rows of {{math|'''B'''}}, then \n:<math> c(\\mathbf{AB}) = (c \\mathbf{A})\\mathbf{B}</math> and <math> (\\mathbf{A} \\mathbf{B})c=\\mathbf{A}(\\mathbf{B}c).</math>\nIf the scalars have the commutative property, then all four matrices are equal. More generally, all four are equal if {{math|''c''}} belongs to the [[center (algebra)|center]] of a [[ring (mathematics)|ring]] containing the entries of the matrices, because in this case {{math|''c'''''X''' {{=}} '''X'''''c''}} for all matrices {{math|'''X'''}}.\n\nThese properties result from the [[bilinearity]] of the product of scalars:\n:<math>c \\left(\\sum_k a_{ik}b_{kj}\\right) = \\sum_k (c a_{ik} ) b_{kj} </math>\n:<math>\\left(\\sum_k a_{ik}b_{kj}\\right) c = \\sum_k a_{ik} ( b_{kj}c). </math>\n\n===Transpose===\nIf the scalars have the [[commutative property]], the [[transpose]] of a product of matrices is the product, in the reverse order, of the transposes of the factors. That is \n:<math> (\\mathbf{AB})^\\mathsf{T} = \\mathbf{B}^\\mathsf{T}\\mathbf{A}^\\mathsf{T} </math>\nwhere <sup>T</sup> denotes the transpose, that is the interchange of rows and columns.\n\nThis identity does not hold for noncommutative entries, since the order between the entries of {{math|'''A'''}} and {{math|'''B'''}} is reversed, when one expands the definition of the matrix product.\n\n===Complex conjugate===\nIf {{math|'''A'''}} and {{math|'''B'''}} have [[complex number|complex]] entries, then\n:<math> (\\mathbf{AB})^* = \\mathbf{A}^*\\mathbf{B}^* </math>\nwhere {{math|<sup>*</sup>}} denotes the entry-wise [[complex conjugate]] of a matrix.\n\nThis results of applying to the definition of matrix product the fact that the conjugate of a sum is the sum of the conjugates of the summands and the conjugate of a product is the product of the conjugates of the factors.\n\nTransposition acts on the indices of the entries, while conjugation acts independently on the entries themselves. It results that, if {{math|'''A'''}} and {{math|'''B'''}} have complex entries, one has\n\n:<math> (\\mathbf{AB})^\\dagger = \\mathbf{B}^\\dagger\\mathbf{A}^\\dagger ,</math>\n\nwhere {{math|<sup>†</sup>}} denotes the [[conjugate transpose]] (conjugate of the transpose, or equivalently transpose of the conjugate).\n\n=== Associativity ===\nGiven three matrices {{math|'''A''', '''B'''}} and {{math|'''C'''}}, the products {{math|('''AB''')'''C'''}} and {{math|'''A'''('''BC''')}} are defined if and only if the number of columns of {{math|'''A'''}} equals the number of rows of {{math|'''B'''}}  and the number of columns of {{math|'''B'''}} equals the number of rows of {{math|'''C'''}} (in particular, if one of the products is defined, the other is also defined). In this case, one has the [[associative property]]\n:<math>(\\mathbf{AB})\\mathbf{C}=\\mathbf{A}(\\mathbf{BC}).</math>\n\nAs for any associative operation, this allows omitting parentheses, and writing the above products as {{tmath|\\mathbf{ABC}.}}\n\nThis extends naturally to the product of any number of matrices provided that the dimensions match. That is, if {{math|'''A'''<sub>1</sub>, '''A'''<sub>2</sub>, ..., '''A'''<sub>''n''</sub>}} are matrices such that the number of columns of {{math|'''A'''<sub>''i''</sub>}} equals the number of rows of {{math|'''A'''<sub>''i'' + 1</sub>}} for {{math|1=''i'' = 1, ..., ''n'' – 1}}, then the product \n:<math> \\prod_{i=1}^n \\mathbf{A}_i = \\mathbf{A}_1\\mathbf{A}_2\\cdots\\mathbf{A}_n </math>\nis defined and does not depend on the [[order of operations|order of the multiplications]], if the order of the matrices is kept fixed.\n\nThese properties may be proved by straightforward but complicated [[summation]] manipulations. This result also follows from the fact that matrices represent [[linear map]]s. Therefore, the associative property of matrices is simply a specific case of the associative property of [[function composition]].\n\n==== Complexity is not associative ====\n\nAlthough the result of a sequence of matrix products does not depend on the [[order of operation]] (provided that the order of the matrices is not changed), the [[computational complexity]] may depend dramatically on this order.\n\nFor example, if {{math|'''A''', '''B'''}} and {{math|'''C'''}} are matrices of respective sizes {{math|10×30, 30×5, 5×60}}, computing {{math|('''AB''')'''C'''}} needs {{math|1=10×30×5 + 10×5×60 = 4,500}} multiplications, while computing {{math|'''A'''('''BC''')}} needs {{math|1=30×5×60 + 10×30×60 = 27,000}} multiplications.\n\nAlgorithms have been designed for choosing the best order of products, see [[Matrix chain multiplication]]. When the number {{mvar|n}} of matrices increases, it has been shown that the choice of the best order has a complexity of <math>O(n \\log n).</math>\n\n====Application to similarity====\nAny [[invertible matrix]] <math>\\mathbf{P}</math> defines a [[similar matrix|similarity transformation]] (on square matrices of the same size as <math>\\mathbf{P}</math>)\n:<math>S_\\mathbf{P}(\\mathbf{A}) = \\mathbf{P}^{-1} \\mathbf{A} \\mathbf{P}.</math>\n\nSimilarity transformations map product to products, that is \n:<math>S_\\mathbf{P}(\\mathbf{AB}) = S_\\mathbf{P}(\\mathbf{A})S_\\mathbf{P}(\\mathbf{B}).</math>\n\nIn fact, one has \n:<math>\\mathbf{P}^{-1} (\\mathbf{AB}) \\mathbf{P} \n= \\mathbf{P}^{-1} \\mathbf{A}(\\mathbf{P}\\mathbf{P}^{-1})\\mathbf{B} \\mathbf{P}\n=(\\mathbf{P}^{-1} \\mathbf{A}\\mathbf{P})(\\mathbf{P}^{-1}\\mathbf{B} \\mathbf{P}).</math>\n\n==Square matrices==\nLet us denote <math>{\\mathcal M}_n(R)</math> the set of {{math|''n''×''n''}} [[square matrices]] with entries in a [[ring (mathematics)|ring]] {{mvar|R}}, which, in practice, is often a [[field (mathematics)|field]].\n\nIn <math>{\\mathcal M}_n(R)</math>, the product is defined for every pair of matrices. This makes <math>{\\mathcal M}_n(R)</math> a [[ring (mathematics)|ring]], which has the [[identity matrix]] {{math|'''I'''}} as [[identity element]] (the matrix whose diagonal entries are equal to 1 and all other entries are 0). This ring is also an [[associative algebra|associative {{mvar|R}}-algebra]].\n\nIf {{math|''n'' > 1}}, many matrices do not have a [[multiplicative inverse]]. For example, a matrix such that all entries of a row (or a column) are 0 does not have an inverse. If it exists, the inverse of a matrix {{math|'''A'''}} is denoted {{math|'''A'''<sup>−1</sup>}}, and, thus verifies\n:<math> \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}. </math>\nA matrix that has an inverse is an [[invertible matrix]]. Otherwise, it is a [[singular matrix]].\n\nA product of matrices is invertible if and only if each factor is invertible. In this case, one has\n\n:<math>(\\mathbf{A}\\mathbf{B})^{-1} = \\mathbf{B}^{-1}\\mathbf{A}^{-1}.</math>\n\nWhen {{mvar|R}} is [[commutative ring|commutative]], and, in particular, when it is a field, the [[determinant]] of a product is the product of the determinants. As determinants are scalars, and scalars commute, one has thus \n:<math> \\det(\\mathbf{AB}) = \\det(\\mathbf{BA}) =\\det(\\mathbf{A})\\det(\\mathbf{B}). </math>\n\nThe other matrix [[invariant (mathematics)|invariants]] do not behave as well with products. Nevertheless, if {{mvar|R}} is commutative, <math>\\mathbf{AB}</math> and <math>\\mathbf{BA}</math> have the same [[Trace (linear algebra)|trace]], the same [[characteristic polynomial]], and the same [[eigenvalues]] with the same multiplicities.\nHowever, the [[eigenvector]]s are generally different if <math>\\mathbf{AB} \\neq \\mathbf{BA}.</math>\n\n=== Powers of a matrix ===\nOne may raise a square matrix to any [[exponentiation|nonnegative integer power]] multiplying it by itself repeatedly in the same way as for ordinary numbers. That is,\n:<math>\\mathbf{A}^0 = \\mathbf{I},</math>\n:<math>\\mathbf{A}^1 = \\mathbf{A},</math>\n:<math>\\mathbf{A}^k = \\underbrace{\\mathbf{A}\\mathbf{A}\\cdots\\mathbf{A}}_{k\\text{ times}}.</math>\n\nComputing the {{mvar|k}}th power of a matrix needs {{math|''k'' – 1}} times the time of a single matrix multiplication, if it is done with the trivial algorithm (repeated multiplication). As this may be very time consuming, one generally prefers using [[exponentiation by squaring]], which requires less than {{math|2 log<sub>2</sub> ''k''}} matrix multiplications, and is therefore much more efficient.\n\nAn easy case for exponentiation is that of a [[diagonal matrix]]. Since the product of diagonal matrices amounts to simply multiplying corresponding diagonal elements together, the {{math|''k''}}th power of a diagonal matrix is obtained by raising the entries to the power {{math|''k''}}:\n:<math>\n  \\begin{pmatrix}\n    a_{11} & 0 & \\cdots & 0 \\\\\n    0 & a_{22} & \\cdots & 0 \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    0 & 0 & \\cdots & a_{nn}\n  \\end{pmatrix}^{k} =\n\\begin{pmatrix}\n    a_{11}^k & 0 & \\cdots & 0 \\\\\n    0 & a_{22}^k &  \\cdots & 0 \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    0 & 0 & \\cdots & a_{nn}^k\n  \\end{pmatrix}.\n</math>\n\n== Complexity==\n{{For|implementation techniques (in particular parallel and distributed algorithms)|Matrix multiplication algorithm}}\n[[File:Bound on matrix multiplication omega over time.svg|thumb|400px|right|The bound on {{math|ω}} over time (2014 improvement not yet shown).]]\n\nThe matrix multiplication [[algorithm]] that results of the definition requires, in the [[worst-case complexity|worst case]], {{tmath|n^3}} multiplications of scalars and {{tmath|(n-1)n^2}} additions for computing the product of two square {{math|''n''×''n''}} matrices. Its [[computational complexity]] is therefore {{tmath|O(n^3)}}, in a [[model of computation]] for which the scalar operations require a constant time (in practice, this is the case for [[floating point]] numbers, but not for integers).\n\nRather surprisingly, this complexity is not optimal, as shown in 1969 by [[Volker Strassen]], who provided an algorithm, now called [[Strassen's algorithm]], with a complexity of <math>O( n^{\\log_{2}7}) \\approx O(n^{2.807}).</math> The exponent appearing in the complexity of matrix multiplication has been improved several times, leading to \n[[Coppersmith–Winograd algorithm]] with a complexity of {{math|''O''(''n''<sup>2.376</sup>)}} (1990).<ref>{{cite web |last=Williams |first=Virginia Vassilevska |authorlink=Virginia Vassilevska Williams |title=Multiplying matrices faster than Coppersmith-Winograd |url=http://www.cs.stanford.edu/~virgi/matrixmult-f.pdf}}</ref> This algorithm has been slightly improved in 2013 by [[Virginia Vassilevska Williams]] (exponent 2.3729) and in 2014 by François Le Gall, for a final (up to date) complexity of {{math|''O''(''n''<sup>2.3728639</sup>)}}.<ref name=\"LeGall2014\" />\n\nThe [[greatest lower bound]] for the exponent of matrix multiplication algorithm is generally called {{tmath|\\omega}}. One has {{tmath|2\\le \\omega}}, because one has to read the {{tmath|n^2}} elements of a matrix for multiplying it by another matrix. Thus {{tmath|2\\le \\omega < 2.373}}. It is unknown whether {{tmath|2< \\omega}}. The largest known lower bound for matrix-multiplication complexity is {{math|Ω(''n''<sup>2</sup> log(''n''))}}, for a restricted kind of [[Arithmetic circuit complexity|arithmetic circuits]], and is due [[Ran Raz]].<ref>{{Cite journal|last=Raz|first=Ran|date=January 2003|title=On the Complexity of Matrix Product|journal=SIAM Journal on Computing|volume=32|issue=5|pages=1356–1369|doi=10.1137/s0097539702402147|issn=0097-5397}}</ref>\n\n===Related complexities===\nThe importance of the computational complexity of matrix multiplication relies on the facts that many algorithmic problems may be solved by means of matrix computation, and most problems on matrices have a complexity which is either the same as that of matrix multiplication (up to a multiplicative constant), or may be expressed in term of the complexity of matrix multiplication or its exponent <math>\\omega.</math>\n\nThere are several advantages of expressing complexities in terms of the exponent <math>\\omega</math> of matrix multiplication. Firstly, if <math>\\omega</math> is improved, this will automatically improve the known upper bound of complexity of many algorithms. Secondly, in practical implementations, one never uses the matrix multiplication algorithm that has the best asymptotical complexity, because the constant hidden behind the [[big O notation]] is too large for making the algorithm competitive for sizes of matrices that can be manipulated in a computer. Thus expressing complexities in terms of <math>\\omega</math> provide a more realistic complexity, since it remains valid whichever algorithm is chosen for matrix computation.\n\nProblems that have the same asymptotic complexity as matrix multiplication include [[determinant]], [[matrix inversion]], [[Gaussian elimination]] (see next section). Problems with complexity that is expressible in terms of <math>\\omega</math> include [[characteristic polynomial]], [[eigenvalues]] (but not [[eigenvectors]]), [[Hermite normal form]], and [[Smith normal form]].{{citation needed|date=March 2018}}\n\n===Matrix inversion, determinant and Gaussian elimination===\nIn his 1969 paper, where he proved the complexity <math>O(n^{2.807})</math> for matrix computation, Strassen proved also the [[Matrix inversion]], [[determinant]] and [[Gaussian elimination]] have, up to a multiplicative constant, the same [[computational complexity]] as matrix multiplication. The proof does not make any assumptions on matrix multiplication that is used, except that its complexity is <math>O(n^\\omega)</math> for some <math>\\omega \\ge 2</math>\n\nThe starting point of Strassen's proof is using [[block matrix]] multiplication. Specifically, a matrix of even dimension {{math|2''n''×2''n''}} may be partitioned in four {{math|''n''×''n''}} blocks\n:<math>\\begin{bmatrix} {A} & {B} \\\\{C} & {D} \\end{bmatrix}.</math>\nUnder this form, its inverse is \n:<math>\n\\begin{bmatrix} {A} & {B} \\\\ {C} & {D} \\end{bmatrix}^{-1} = \n\\begin{bmatrix} \n{A}^{-1}+{A}^{-1}{B}({D}-{CA}^{-1}{B})^{-1}{CA}^{-1} & -{A}^{-1}{B}({D}-{CA}^{-1}{B})^{-1} \n\\\\ -({D}-{CA}^{-1}{B})^{-1}{CA}^{-1} & ({D}-{CA}^{-1}{B})^{-1} \n\\end{bmatrix},\n</math>\nprovided that {{mvar|A}} and <math>{D}-{CA}^{-1}{B}</math> are invertible.\n\nThus, the inverse of a {{math|2''n''×2''n''}} matrix may be computed with two inversions, six multiplications and four additions or additive inverses of {{math|''n''×''n''}} matrices. It follows that, denoting respectively by {{math|''I''(''n'')}}, {{math|''M''(''n'')}} and {{math|1= ''A''(''n'') = ''n''<sup>2</sup>}} the number of operations needed for multiplying, inverting and adding {{math|''n''×''n''}} matrices, one has \n:<math>I(2n) \\le 2I(n) + 6M(n)+ 4 A(n).</math>\nIf <math>n=2^k,</math> one may apply this formula recursively:\n:<math>\\begin{align}\nI(2^k) &\\le 2I(2^{k-1}) + 6M(2^{k-1})+ 4 A(2^{k-1})\\\\\n&\\le 2^2I(2^{k-2}) + 6(M(2^{k-1})+2M(2^{k-2})) + 4(A(2^{k-1}) + 2A(2^{k-2}))\\\\\n&\\ldots\n\\end{align}</math>\nIf <math>M(n)\\le cn^\\omega,</math> and <math>\\alpha=2^\\omega\\ge 4,</math> one gets eventually \n:<math>\\begin{align}\nI(2^k) &\\le 2^k I(1) + 6c(\\alpha^{k-1}+2\\alpha^{k-2} + \\cdots +2^{k-1}\\alpha^0) + k 2^{k+1}\\\\\n&\\le 2^k + 6c\\frac{\\alpha^k-2^k}{\\alpha-2} + k 2^{k+1}\\\\\n&\\le d(2^k)^\\omega.\n\\end{align}</math>\nfor some constant {{mvar|d}}.\n\nFor matrices whose dimension is not a power of two, the same complexity is reached by increasing the dimension of the matrix to a power of two, by padding the matrix with rows and columns whose entries are 1 on the diagonal and 0 elsewhere.\n\nThis proves the asserted complexity for matrices such that all submatrices that have to be inverted are indeed invertible. This complexity is thus proved for almost all matrices, as a matrix with randomly chosen entries is invertible with probability one.\n\nThe same argument applies to [[LU decomposition]], as, if the matrix {{mvar|A}} is invertible, the equality\n:<math>\\begin{bmatrix} {A} & {B} \\\\{C} & {D} \\end{bmatrix} \n= \\begin{bmatrix}I & 0\\\\CA^{-1}&I\\end{bmatrix}\\,\\begin{bmatrix}A&B\\\\0&D-CA^{-1}B\\end{bmatrix}</math> \ndefines a block LU decomposition that may be applied recursively to <math>A</math> and <math>D-CA^{-1}B,</math> for getting eventually a true LU decomposition of the original matrix.\n\nThe argument applies also for the determinant, since it results from the block LU decomposition that \n:<math>\\det \\begin{bmatrix} {A} & {B} \\\\{C} & {D} \\end{bmatrix} = \n\\det(A)\\det(D-CA^{-1}B).</math>\n\n==Other matrix multiplications==\n\nThe term \"matrix multiplication\" is most commonly reserved for the definition given in this article. It could be more loosely applied to other operations on matrices.\n\n*[[Block matrix#Block matrix multiplication|Block matrix multiplication]]\n*[[Hadamard product (matrices)|Hadamard product]] of two matrices of the same size, resulting in a matrix of the same size, which is the product entry-by-entry\n*[[Frobenius inner product]], the [[dot product]] of matrices considered as vectors, or, equivalently the sum of the entries of the Hadamard product\n*[[Outer product]], also called [[dyadic product]] or [[tensor product]] of two column matrices, which is <math>\\mathbf{a}\\mathbf{b}^\\mathsf{T}</math>\n*[[Kronecker product]] or [[tensor product]], the generalization to any size of the preceding \n*[[Cracovian product]], defined as {{math|1='''A''' ∧ '''B''' = '''B'''<sup>T</sup>'''A'''}}\n\n== Notes ==\n{{Reflist|30em}}\n\n== References ==\n{{Commons category|Matrix multiplication|matrix multiplication}}\n{{wikibooks\n|1= Linear Algebra\n|2= Matrix Multiplication\n|3= Matrix multiplication\n}}\n{{wikibooks|Applicable Mathematics|Matrices#Multiplying Matrices|Multiplying Matrices}}\n{{refbegin}}\n* Henry Cohn, [[Robert Kleinberg]], [[Balázs Szegedy]], and Chris Umans. Group-theoretic Algorithms for Matrix Multiplication. {{arxiv|math.GR/0511460}}. ''Proceedings of the 46th Annual Symposium on Foundations of Computer Science'', 23–25 October 2005, Pittsburgh, PA, IEEE Computer Society, pp.&nbsp;379–388.\n* Henry Cohn, Chris Umans. A Group-theoretic Approach to Fast Matrix Multiplication. {{arxiv|math.GR/0307321}}. ''Proceedings of the 44th Annual IEEE Symposium on Foundations of Computer Science'', 11–14 October 2003, Cambridge, MA, IEEE Computer Society, pp.&nbsp;438–449.\n* {{cite journal | last1 = Coppersmith | first1 = D. | last2 = Winograd | first2 = S. | year = 1990 | title = Matrix multiplication via arithmetic progressions | url = | journal = J. Symbolic Comput. | volume = 9 | issue = 3| pages = 251–280 | doi=10.1016/s0747-7171(08)80013-2}}\n* {{Citation | last1=Horn | first1=Roger A. | last2=Johnson | first2=Charles R. | title=Topics in Matrix Analysis | publisher=[[Cambridge University Press]] | isbn=978-0-521-46713-1 | year=1991}}\n* [[Donald Knuth|Knuth, D.E.]], ''[[The Art of Computer Programming]] Volume 2: Seminumerical Algorithms''. Addison-Wesley Professional; 3 edition (November 14, 1997). {{isbn|978-0-201-89684-8}}. pp.&nbsp;501.\n* {{Citation | last1=Press | first1=William H. | last2=Flannery | first2=Brian P. | last3=Teukolsky | first3=Saul A. | author3-link=Saul Teukolsky | last4=Vetterling | first4=William T. | title=Numerical Recipes: The Art of Scientific Computing | publisher=[[Cambridge University Press]] | edition=3rd | isbn=978-0-521-88068-8 | year=2007| title-link=Numerical Recipes }}.\n* [[Ran Raz]]. On the complexity of matrix product. In Proceedings of the thirty-fourth annual ACM symposium on Theory of computing. ACM Press, 2002. {{doi|10.1145/509907.509932}}.\n* Robinson, Sara, ''Toward an Optimal Algorithm for Matrix Multiplication,'' SIAM News 38(9), November 2005. [http://www.siam.org/pdf/news/174.pdf PDF]\n* Strassen, Volker, ''Gaussian Elimination is not Optimal'', Numer. Math. 13, p.&nbsp;354-356, 1969.\n* {{Citation | doi=10.1016/0024-3795(73)90023-2 | last=Styan | first=George P. H. | title=Hadamard Products and Multivariate Statistical Analysis | journal=Linear Algebra and its Applications | year=1973 | volume=6 | pages=217–240}}\n* {{Cite book|last=Williams|first=Virginia Vassilevska|date=2012-05-19|chapter=Multiplying matrices faster than coppersmith-winograd|chapter-url=http://dl.acm.org/citation.cfm?id=2213977.2214056|publisher=ACM|pages=887–898|doi=10.1145/2213977.2214056|isbn=9781450312455|title=Proceedings of the 44th symposium on Theory of Computing - STOC '12|citeseerx=10.1.1.297.2680}}\n{{refend}}\n{{algebra-footer}}\n\n{{DEFAULTSORT:Matrix Multiplication}}\n[[Category:Matrix theory]]\n[[Category:Bilinear operators]]\n[[Category:Binary operations]]\n[[Category:Multiplication]]\n[[Category:Numerical linear algebra]]"
    },
    {
      "title": "Mean operation",
      "url": "https://en.wikipedia.org/wiki/Mean_operation",
      "text": "In [[algebraic topology]], a '''mean''' or '''mean operation''' on a [[topological space]] ''X'' is a [[continuous function|continuous]], [[commutative property|commutative]], [[idempotence|idempotent]] [[binary operation]] on ''X''. If the operation is also [[associative property|associative]], it defines a [[semilattice]]. A classic problem is to determine which spaces admit a mean. For example, [[Euclidean space]]s admit a mean -- the usual [[arithmetic mean |average]] of two vectors -- but [[n-sphere|spheres of positive dimension]] do not, including the [[circle]].\n\n==Further reading==\n*{{citation\n | last = Aumann | first = G.\n | journal = Mathematische Annalen\n | pages = 210–215\n | title = Über Räume mit Mittelbildungen.\n | url = http://eudml.org/doc/160126\n | volume = 119\n | issue = 2\n | year = 1943\n | doi=10.1007/bf01563741}}.\n*{{citation\n | last = Sobolewski | first = Mirosław\n | doi = 10.1090/s0002-9939-08-09414-8\n | issue = 10\n | journal = Proceedings of the American Mathematical Society\n | pages = 3701–3707\n | title = Means on chainable continua\n | volume = 136\n | year = 2008}}.\n*{{citation\n | last = T. Banakh | first = W. Kubis, R. Bonnet\n | issue = 1\n | journal = Topological Algebra and its Applications\n | title = Means on scattered compacta\n | url = http://eudml.org/doc/266591\n | volume = 2\n | year = 2014| doi = 10.2478/taa-2014-0002\n }}.\n*{{citation\n | last = Charatonik | first = Janusz J.\n | title = Selected problems in continuum theory\n | url = http://topology.auburn.edu/tp/reprints/v27/tp27107.pdf\n | issue = 1\n | journal = Topology Proceedings\n | mr = 2048922\n | pages = 51–78\n | department = Proceedings of the Spring Topology and Dynamical Systems Conference\n | volume = 27\n | year = 2003}}.\n\n[[Category:Binary operations]]\n[[Category:Means]]\n\n{{topology-stub}}"
    },
    {
      "title": "Mediant (mathematics)",
      "url": "https://en.wikipedia.org/wiki/Mediant_%28mathematics%29",
      "text": ":''For mediant in music, see [[mediant]].  \"Mediant\" should not be confused with [[median]].''\n\nIn [[mathematics]], the '''mediant''' of two [[vulgar fraction|fraction]]s, generally made up of four positive integers\n\n:<math> \\frac{a}{c} \\quad</math> and <math>\\quad \\frac{b}{d} \\quad</math> is defined as <math>\\quad \\frac{a+b}{c+d}. </math>\n\nThat is to say, the [[numerator]] and [[denominator]] of the mediant are the sums of the numerators and denominators of the given fractions, respectively. It is sometimes called the '''freshman sum''', as it is a common mistake in the early stages of learning about [[Fraction (mathematics)#Addition|addition of fractions]].\n\nTechnically, this is a [[binary operation]] on valid [[fractions]] (nonzero denominator), considered as [[ordered pair]]s of appropriate integers, a priori disregarding the perspective on [[rational numbers]] as equivalence classes of fractions. For example, the mediant of the fractions 1/1 and 1/2 is 2/3. However, if the fraction 1/1 is replaced by the fraction 2/2, which is an [[equivalent fraction]] denoting the same rational number 1, the mediant of the fractions 2/2 and 1/2 is 3/4. For a stronger connection to rational numbers the fractions may be required to be reduced to [[lowest terms]], thereby selecting unique representatives from the respective equivalence classes.\n\nThe [[Stern-Brocot tree]] provides an enumeration of all positive rational numbers via mediants in lowest terms, obtained purely by iterative computation of the mediant according to a simple algorithm.\n\n==Properties==\n\n* '''The mediant inequality:''' An important property (also explaining its name) of the mediant is that it lies strictly between the two fractions of which it is the mediant: If <math>a/c < b/d </math> and <math>a,b,c,d\\geq 0</math>, then\n\n::<math>\\frac a c < \\frac{a+b}{c+d} < \\frac b d. </math>\n\n:This property follows from the two relations\n\n::<math>\\frac{a+b}{c+d}-\\frac a c={{bc-ad}\\over{c(c+d)}} ={d\\over{c+d}}\\left( \\frac{b}{d}-\\frac a c \\right)</math> \n:and\n\n::<math>\\frac b d-\\frac{a+b}{c+d}={{bc-ad}\\over{d(c+d)}} ={c\\over{c+d}}\\left( \\frac{b}{d}-\\frac a c \\right). </math>\n\n* Assume that the pair of fractions ''a''/''c'' and ''b''/''d'' satisfies the determinant relation <math>bc-ad=1</math>. Then the mediant has the property that it is the ''simplest'' fraction in the interval (''a''/''c'', ''b''/''d''), in the sense of being the fraction with the smallest denominator. More precisely, if the fraction <math> a'/c' </math> with positive denominator c' lies (strictly) between ''a''/''c'' and ''b''/''d'', then its numerator and denominator can be written as <math> \\,a'=\\lambda_1 a+\\lambda_2  b </math> and <math> \\,c'=\\lambda_1 c+\\lambda_2  d </math> with two ''positive'' real (in fact rational) numbers <math> \\lambda_1,\\,\\lambda_2 </math>. To see why the <math> \\lambda_i </math> must be positive note that\n\n::<math>\\frac{\\lambda_1 a+\\lambda_2  b}{\\lambda_1 c+\\lambda_2  d }-\\frac a c=\\lambda_2 {{bc-ad}\\over{c(\\lambda_1 c+\\lambda_2  d)}}\n</math>\n\n:and\n\n::<math>\\frac b d-\\frac{\\lambda_1 a+\\lambda_2  b}{\\lambda_1 c+\\lambda_2  d }=\\lambda_1 {{bc-ad}\\over{d(\\lambda_1 c+\\lambda_2  d )}} </math>\n\n:must be positive. The determinant relation \n::<math>bc-ad=1 \\, </math> \n:then implies that both <math> \\lambda_1,\\,\\lambda_2 </math> must be integers,  solving the system of linear equations \n::<math>\\, a'=\\lambda_1 a+\\lambda_2 b </math>\n::<math>\\, c'=\\lambda_1 c+\\lambda_2 d </math>\n:for <math> \\lambda_1,\\lambda_2 </math>. Therefore <math> c'\\ge c+d. </math>\n\n* The converse is also true: assume that the pair of [[reduced fraction]]s ''a''/''c''&nbsp;<&nbsp;''b''/''d''  has the property that the ''reduced'' fraction with smallest denominator lying in the interval (''a''/''c'',&nbsp;''b''/''d'') is equal to the mediant of the two fractions. Then the determinant relation ''bc''&nbsp;&minus;&nbsp;''ad'' = 1 holds. This fact may be deduced e.g. with the help of [[Pick's theorem]] which expresses the area of a plane triangle whose vertices have integer coordinates in terms of the number v<sub>interior</sub> of lattice points (strictly) inside the triangle and the number v<sub>boundary</sub> of lattice points on the boundary of the triangle. Consider the triangle <math> \\Delta(v_1,v_2,v_3)</math> with the three vertices ''v''<sub>1</sub> = (0,&nbsp;0), ''v''<sub>2</sub> = (''a'',&nbsp;''c''), ''v''<sub>3</sub> = (''b'',&nbsp;''d''). Its area is equal to \n::<math> \\text{area}(\\Delta)={{bc-ad}\\over 2} \\ .</math>\n:A point <math> p=(p_1,p_2) </math> inside the triangle can be parametrized as \n::<math> p_1=\\lambda_1 a+\\lambda_2 b,\\; p_2=\\lambda_1 c+\\lambda_2 d,  \n</math>\n:where\n::<math> \\lambda_1\\ge 0,\\,\\lambda_2 \\ge 0, \\,\\lambda_1+\\lambda_2 \\le 1. \\, </math> \n:The Pick formula \n::<math> \\text{area}(\\Delta)=v_\\mathrm{interior}+{v_\\mathrm{boundary}\\over 2}-1 \n</math>\n:now implies that there must be a lattice point ''q''&nbsp;=&nbsp;(''q''<sub>1</sub>,&nbsp;''q''<sub>2</sub>) lying inside the triangle different from the three vertices if ''bc''&nbsp;&minus;&nbsp;''ad''&nbsp;>1 (then the area of the triangle is <math> \\ge 1 </math>). The corresponding fraction ''q''<sub>1</sub>/''q''<sub>2</sub> lies (strictly) between the given (by assumption reduced) fractions and has denominator\n\n::<math> q_2=\\lambda_1c+\\lambda_2d \\le \\max(c,d)<c+d \n</math>\n:as \n::<math> \\lambda_1+\\lambda_2 \\le 1. \\, </math>\n\n*Relatedly, if ''p''/''q'' and ''r''/''s'' are reduced fractions on the unit interval such that |''ps''&nbsp;&minus;&nbsp;''rq''| = 1 (so that they are adjacent elements of a row of the [[Farey sequence]]) then \n:<math>?\\left(\\frac{p+r}{q+s}\\right) = \\frac12 \\left(?\\bigg(\\frac pq\\bigg) + {}?\\bigg(\\frac rs\\bigg)\\right)</math>\n:where ? is [[Minkowski's question mark function]].\n\n:In fact, mediants commonly occur in the study of [[continued fraction]]s and in particular, [[Farey fraction]]s. The ''n''th [[Farey sequence]] ''F''<sub>''n''</sub> is defined as the (ordered with respect to magnitude) sequence of reduced fractions ''a''/''b'' (with [[coprime]] ''a'', ''b'') such that ''b''&nbsp;&le;&nbsp;''n''. If two fractions ''a''/''c''&nbsp;<&nbsp;''b''/''d'' are adjacent (neighbouring) fractions in a segment of F<sub>n</sub> then the determinant relation <math> bc-ad=1</math> mentioned above is generally valid and therefore the mediant is the ''simplest'' fraction in the interval (''a''/''c'',&nbsp;''b''/''d''), in the sense of being the fraction with the smallest denominator. Thus the mediant will then (first) appear in the (''c''&nbsp;+&nbsp;''d'')th Farey sequence and is the \"next\" fraction which is inserted in any Farey sequence between ''a''/''c'' and ''b''/''d''. This gives the rule how the Farey sequences ''F''<sub>''n''</sub> are successively built up with increasing ''n''.\n\n==Graphical determination of mediants==\n\n[[File:Mediant.png|thumb|Determining the mediant of two rational numbers graphically. The slopes of the blue and red segments are two rational numbers; the slope of the green segment is their mediant.]]\nA positive [[rational number]] is one in the form <math>a/b</math> where <math>a,b</math> are positive [[natural numbers]]; ''i.e.'' <math>a,b\\in\\mathbb{N}^{+}</math>. The set of positive rational numbers <math>\\mathbb{Q}^{+}</math> is, therefore, the [[Cartesian product]] of <math>\\mathbb{N}^{+}</math> by itself; ''i.e.'' <math>\\mathbb{Q}^{+}=(\\mathbb{N}^{+})^2</math>. A point with coordinates <math>(b,a)</math> represents the rational number <math>a/b</math>, and the slope of a segment connecting the origin of coordinates to this point is <math>a/b</math>. Since <math>a,b</math> are not required to be [[coprime]], point <math>(b,a)</math> represents one and only one rational number, but a rational number is represented by more than one point; ''e.g.'' <math>(4,2),(60,30),(48,24)</math> are all representations of the rational number <math>1/2</math>. This is a slight modification of the [[Rational_number#Formal_construction|formal definition]] of rational numbers, restricting them to positive values, and flipping the order of the terms in the ordered pair <math>(b,a)</math> so that the slope of the segment becomes equal to the rational number.\n\nTwo points <math>(b,a)\\neq(d,c)</math> where <math>a,b,c,d\\in\\mathbb{N}^{+}</math> are two representations of (possibly equivalent) rational numbers <math>a/b</math> and <math>c/d</math>. The line segments connecting the origin of coordinates to <math>(b,a)</math> and <math>(d,c)</math> form two adjacent sides in a parallelogram. The vertex of the parallelogram opposite to the origin of coordinates is the point <math>(b+d,a+c)</math>, which is the mediant of <math>a/b</math> and <math>c/d</math>.\n\nThe area of the parallelogram is <math>bc-ad</math>, which is also the magnitude of the [[cross product]] of vectors <math>\\langle b,a\\rangle</math> and <math>\\langle d,c\\rangle</math>. It follows from the [[Rational_number#Formal_construction|formal definition of rational number equivalence]] that the area is zero if <math>a/b</math> and <math>c/d</math> are equivalent. In this case, one segment coincides with the other, since their slopes are equal. The area of the parallelogram formed by two consecutive rational numbers in the [[Stern-Brocot tree]] is always 1.<ref>Austin, David. [http://www.ams.org/featurecolumn/archive/stern-brocot.html Trees, Teeth, and Time: The mathematics of clock making], Feature Column from the AMS</ref>\n\n==Generalization==\n\nThe notion of mediant can be generalized to ''n'' fractions, and a generalized mediant inequality holds,<ref>{{cite journal| first=Michael|last=Bensimhoun| url=https://commons.wikimedia.org/wiki/File:Extension_of_the_mediant_inequality.pdf|format=PDF\n| title = A note on the mediant inequality|year=2013}}</ref> a fact that seems to have been first noticed by Cauchy. More precisely, the weighted mediant <math>m_w</math> of ''n'' fractions <math>a_1/b_1,\\ldots,a_n/b_n</math> is defined by <math>\\frac{\\sum_i w_i a_i}{\\sum_i w_i b_i}</math> (with <math>w_i>0</math>). It can be shown that <math>m_w</math> lies somewhere between the smallest and the largest fraction among the <math>a_i/b_i</math>.\n\n==References==\n{{Reflist}}\n\n==External links==\n* [http://www.cut-the-knot.org/blue/Mediant.shtml Mediant Fractions] at [[cut-the-knot]]\n* [http://mathpages.com/home/kmath055/kmath055.htm  MATHPAGES, Kevin Brown: Generalized Mediant]\n\n[[Category:Fractions (mathematics)]]\n[[Category:Elementary arithmetic]]\n[[Category:Binary operations]]"
    },
    {
      "title": "Modular multiplicative inverse",
      "url": "https://en.wikipedia.org/wiki/Modular_multiplicative_inverse",
      "text": "In [[mathematics]], in particular the area of [[number theory]], a '''modular multiplicative inverse''' of an [[integer]] {{mvar|a}} is an integer {{mvar|x}} such that the product {{mvar|ax}} is [[Congruence relation#Basic example|congruent]] to 1 with respect to the modulus {{mvar|m}}.<ref name=\"Rosen132\">{{harvnb|Rosen|1993|page=132}}</ref> In the standard notation of [[modular arithmetic]] this congruence is written as\n:<math>ax \\equiv 1 \\pmod{m},</math>\nwhich is the shorthand way of writing the statement that {{mvar|m}} divides (evenly) the quantity {{math|''ax'' − 1}}, or, put another way, the remainder after dividing {{mvar|ax}} by the integer {{mvar|m}} is 1. If {{mvar|a}} does have an inverse modulo {{mvar|m}} there are an infinite number of solutions of this congruence which form a [[modular arithmetic#Congruence classes|congruence class]] with respect to this modulus. Furthermore, any integer that is congruent to {{mvar|a}} (i.e., in {{mvar|a}}'s congruence class) will have any element of {{mvar|x}}'s congruence class as a modular multiplicative inverse. Using the notation of <math>\\overline{w}</math> to indicate the congruence class containing {{mvar|w}}, this can be expressed by saying that the ''modulo multiplicative inverse'' of the congruence class <math>\\overline{a}</math> is the congruence class <math>\\overline{x}</math> such that:\n:<math>\\overline{a}\\cdot_{m} \\overline{x} = \\overline{1},</math>\nwhere the symbol <math>\\cdot_m </math> denotes the multiplication of equivalence classes modulo {{mvar|m}}.<ref name=\":1\">{{harvnb|Schumacher|1996|loc=p. 88}}</ref>\nWritten in this way the analogy with the usual concept of a [[multiplicative inverse]] in the set of [[Rational number|rational]] or [[real number]]s is clearly represented, replacing the numbers by congruence classes and altering the  [[binary operation]] appropriately.  \n\nAs with the analogous operation on the real numbers, a fundamental use of this operation is in solving, when possible, linear congruences of the form,\n:<math>ax \\equiv b \\pmod{m}.</math>\nFinding modular multiplicative inverses also has practical applications in the field of [[cryptography]], i.e. [[public-key cryptography]] and the [[RSA (cryptosystem)|RSA Algorithm.]]<ref>{{citation|first=Douglas R.|last=Stinson|title=Cryptography / Theory and Practice|year=1995|publisher=CRC Press|isbn=0-8493-8521-0|pages=124–128}}</ref><ref>{{harvnb|Trappe|Washington|2006|loc=pp. 164−169}}</ref><ref>{{Cite web|url=https://tools.ietf.org/html/rfc8017#section-3|title=PKCS #1: RSA Cryptography Specifications Version 2.2|last=Moriarty|first=K.|last2=Kaliski|first2=B.|date=|year=2016|website=Internet Engineering Task Force RFC 8017|publisher=Internet Engineering Task Force|access-date=January 21, 2017|last3=Jonsson|first3=J.|last4=Rusch|first4=A.}}</ref> A benefit for the computer implementation of these applications is that there exists a very fast algorithm (the [[extended Euclidean algorithm]]) that can be used for the calculation of modular multiplicative inverses.\n\n==Modular arithmetic==\n{{main|Modular arithmetic}}\nFor a given positive integer {{mvar|m}}, two integers, {{mvar|a}} and {{mvar|b}}, are said to be '''congruent modulo {{mvar|m}}''' if {{mvar|m}} divides their difference. This [[binary relation]] is denoted by, \n:<math>a \\equiv b \\pmod{m}.</math>\nThis is an [[equivalence relation]] on the set of integers, {{math|ℤ}}, and the equivalence classes are called '''congruence classes modulo {{mvar|m}}''' or '''residue classes modulo {{mvar|m}}'''. Let <math>\\overline{a}</math> denote the congruence class containing the integer {{mvar|a}},<ref>Other notations are often used, including {{math|[''a'']}} and {{math|[''a'']<sub>''m''</sub>}}.</ref> then\n:<math>\\overline{a} = \\{b \\in \\mathbb{Z} \\mid a \\equiv b \\pmod{m} \\}.</math>\nA '''linear congruence''' is a modular congruence of the form\n:<math>ax \\equiv b \\pmod{m}.</math>\nUnlike linear equations over the reals, linear congruences may have zero, one or several solutions. If {{mvar|x}} is a solution of a linear congruence then every element in <math>\\overline{x}</math> is also a solution, so, when speaking of the number of solutions of a linear congruence we are referring to the number of different congruence classes that contain solutions. \n\nIf {{mvar|d}} is the [[greatest common divisor]] of {{mvar|a}} and {{mvar|m}} then the linear congruence {{math|''ax'' ≡ ''b'' (mod ''m'')}} has solutions if and only if {{mvar|d}} divides {{mvar|b}}. If {{mvar|d}} divides {{mvar|b}}, then there are exactly {{mvar|d}} solutions.<ref>{{harvnb|Ireland|Rosen|1990|page=32}}</ref>\n\nA modular multiplicative inverse of an integer {{mvar|a}} with respect to the modulus {{mvar|m}} is a solution of the linear congruence\n:<math>ax \\equiv 1 \\pmod{m}.</math>\nThe previous result says that a solution exists if and only if {{math|1=gcd(''a'', ''m'') = 1}}, that is, {{mvar|a}} and {{mvar|m}} must be [[Coprime integers|relatively prime]] (i.e. coprime). Furthermore, when this condition holds, there is exactly one solution, i.e., when it exists, a modular multiplicative inverse is unique.<ref>{{citation|last=Shoup|first=Victor|title=A Computational Introduction to Number Theory and Algebra|url=https://books.google.com/books?id=-RzJs-mPfX0C&pg=PA15|year=2005|authorlink=Victor Shoup|at=Theorem&nbsp;2.4, p.&nbsp;15|publisher=Cambridge University Press|isbn=9780521851541}}</ref>\n\nWhen {{math|''ax'' ≡ 1 (mod ''m'')}} has a solution it is often denoted in this way −\n:<math>x \\equiv a^{-1} \\pmod{m},</math>\nbut this is an [[abuse of notation]] since a modular multiplicative inverse is an integer and {{math|''a''<sup>−1</sup>}} is not an integer when {{mvar|a}} is an integer other than 1 or - 1. The notation would be proper if {{mvar|a}} is interpreted as a token standing for the congruence class <math>\\overline{a}</math>, as the multiplicative inverse of a congruence class is a congruence class with the multiplication defined in the next section.\n\n===Integers modulo {{mvar|m}}===\nThe congruence relation, modulo {{mvar|m}}, partitions the set of integers into {{mvar|m}} congruence classes. Operations of addition and multiplication can be defined on these {{mvar|m}} objects in the following way: To either add or multiply two congruence classes, first pick a representative (in any way) from each class, then perform the usual operation for integers on the two representatives and finally take the congruence class that the result of the integer operation lies in as the result of the operation on the congruence classes. In symbols, with <math>+_m</math> and <math>\\cdot_m</math> representing the operations on congruence classes, these definitions are \n:<math>\\overline{a} +_m \\overline{b} = \\overline{a + b}</math>\nand\n:<math>\\overline{a} \\cdot_m \\overline{b} = \\overline{ab}.</math>\nThese operations are [[well-defined]], meaning that the end result does not depend on the choices of representatives that were made to obtain the result.\n\nThe {{mvar|m}} congruence classes with these two defined operations form a [[Ring (mathematics)|ring]], called the '''ring of integers modulo {{mvar|m}}'''. There are several notations used for these algebraic objects, most often <math>\\mathbb{Z}/m\\mathbb{Z}</math> or <math>\\mathbb{Z}/m</math>, but several elementary texts and application areas use a simplified notation <math>\\mathbb{Z}_m</math> when confusion with other algebraic objects is unlikely.\n\nThe congruence classes of the integers modulo {{mvar|m}} were traditionally known as ''residue classes modulo m'', reflecting the fact that all the elements of a congruence class have the same remainder (i.e., \"residue\") upon being divided by {{mvar|m}}. Any set of {{mvar|m}} integers selected so that each comes from a different congruence class modulo m is called a [[Complete residue system modulo m|'''complete system of residues modulo {{mvar|m}}''']].<ref>{{harvnb|Rosen|1993|page=121}}</ref> The [[Division algorithm for integers|division algorithm]] shows that the set of integers, {{math|{0, 1, 2, ..., ''m'' − 1} }} form a complete system of residues modulo {{mvar|m}}, known as the [[Least residue system modulo m|'''least residue system modulo {{mvar|m}}''']]. In working with arithmetic problems it is sometimes more convenient to work with a complete system of residues and use the language of congruences while at other times the point of view of the congruence classes of the ring <math>\\mathbb{Z}/m\\mathbb{Z}</math> is more useful.<ref>{{harvnb|Ireland|Rosen|1990|page=31}}</ref>\n\n===Multiplicative group of integers modulo {{mvar|m}}===\n{{main|Multiplicative group of integers modulo n}}\nNot every element of a complete residue system modulo {{mvar|m}} has a modular multiplicative inverse, for instance, zero never does. After removing the elements of a complete residue system that are not relatively prime to {{mvar|m}}, what is left is called a '''[[reduced residue system]]''', all of whose elements have modular multiplicative inverses. The number of elements in a reduced residue system is <math>\\phi(m)</math>, where <math>\\phi</math> is the [[Euler's totient function|Euler totient function]], i.e., the number of positive integers less than {{mvar|m}} that are relatively prime to {{mvar|m}}.\n\nIn a general [[Ring with a unit|ring with unity]] not every element has a [[multiplicative inverse]] and those that do are called [[Unit (ring theory)|'''units''']]. As the product of two units is a unit, the units of a ring form a [[Group (mathematics)|group]], the '''group of units of the ring''' and often denoted by {{math|''R''<sup>×</sup>}} if {{mvar|R}} is the name of the ring. The group of units of the ring of integers modulo {{mvar|m}} is called the '''multiplicative group of integers modulo {{mvar|m}}''', and it is [[Isomorphism|isomorphic]] to a reduced residue system. In particular, it has [[Order (group theory)|order]] (size), <math>\\phi(m)</math>.\n\nIn the case that {{mvar|m}} is a [[Prime number|prime]], say {{mvar|p}}, then <math>\\phi(p) = p-1</math> and all the non-zero elements of <math>\\mathbb{Z}/p\\mathbb{Z}</math> have multiplicative inverses, thus <math>\\mathbb{Z}/p\\mathbb{Z}</math> is a [[finite field]]. In this case, the multiplicative group of integers modulo {{mvar|p}} form a [[cyclic group]] of order {{math|''p'' − 1}}.\n\n==Example==\n\nTo illustrate the above definitions consider the following example using the modulus 10.\n\nTwo integers are congruent mod 10 if and only if their difference is divisible by 10, for instance\n:<math>32 \\equiv 12 \\pmod{10}</math> since 10 divides 32 − 12 = 20, and\n:<math>111 \\equiv 1 \\pmod{10}</math> since 10 divides 111 − 1 = 110.\n\nSome of the ten congruence classes with respect to this modulus are:\n:<math>\\overline{0} = \\{ \\cdots, -20, -10, 0, 10, 20, \\cdots \\}</math> \n:<math>\\overline{1} = \\{ \\cdots, -19, -9, 1, 11, 21, \\cdots \\}</math>\n:<math>\\overline{5} = \\{ \\cdots, -15, -5, 5, 15, 25, \\cdots \\}</math> and\n:<math>\\overline{9} = \\{ \\cdots, -11, -1, 9, 19, 29, \\cdots \\}.</math>\n\nThe linear congruence {{math|4''x'' ≡ 5 (mod 10)}} has no solutions since the integers that are congruent to 5 (i.e., those in <math>\\overline{5}</math>) are all odd while {{math|4''x''}} is always even. However, the linear congruence {{math|4''x'' ≡ 6 (mod 10)}} has two solutions, namely, {{math|1=''x'' = 4}} and {{math|1=''x'' = 9}}. The {{math|1=gcd(4, 10) = 2}} and 2 does not divide 5, but does divide 6.\n\nSince {{math|1=gcd(3, 10) = 1}}, the linear congruence {{math|3''x'' ≡ 1 (mod 10)}} will have solutions, that is, modular multiplicative inverses of 3 modulo 10 will exist. In fact, 7 satisfies this congruence (i.e., 21 − 1 = 20). However, other integers also satisfy the congruence, for instance 17 and −3 (i.e., 3(17) − 1 = 50 and 3(−3) − 1 = −10). In particular, every integer in <math>\\overline{7}</math> will satisfy the congruence since these integers have the form {{math|7 + 10''r''}} for some integer {{mvar|r}} and \n:<math>3(7 + 10 r) - 1 = 21 + 30 r -1 = 20 + 30 r = 10(2 + 3r), </math>\nis clearly divisible by 10. This congruence has only this one congruence class of solutions. The solution in this case could have been obtained by checking all possible cases, but systematic algorithms would be needed for larger moduli and these will be given in the next section. \n\nThe product of congruence classes <math>\\overline{5}</math> and <math>\\overline{8}</math> can be obtained by selecting an element of <math>\\overline{5}</math>, say 25, and an element of <math>\\overline{8}</math>, say −2, and observing that their product (25)(−2) = −50 is in the congruence class <math>\\overline{0}</math>. Thus, <math>\\overline{5} \\cdot_{10} \\overline{8} = \\overline{0}</math>. Addition is defined in a similar way. The ten congruence classes together with these operations of addition and multiplication of congruence classes form the ring of integers modulo 10, i.e., <math>\\mathbb{Z}/10\\mathbb{Z}</math>.\n\nA complete residue system modulo 10 can be the set {10, −9, 2, 13, 24, −15, 26, 37, 8, 9} where each integer is in a different congruence class modulo 10. The unique least residue system modulo 10 is {0, 1, 2, ..., 9}. A reduced residue system modulo 10 could be {1, 3, 7, 9}. The product of any two congruence classes represented by these numbers is again one of these four congruence classes. This implies that these four congruence classes form a group, in this case the cyclic group of order four, having either 3 or 7 as a (multiplicative) generator. The represented congruence classes form the group of units of the ring <math>\\mathbb{Z}/10\\mathbb{Z}</math>. These congruence classes are precisely the ones which have modular multiplicative inverses.\n\n==Computation==\n===Extended Euclidean algorithm===\n{{main|Extended Euclidean algorithm}}\n{{wikibooks|Algorithm Implementation|Mathematics/Extended Euclidean algorithm|Extended Euclidean algorithm}}\nA modular multiplicative inverse of {{math|''a''}} modulo {{math|''m''}} can be found by using the extended Euclidean algorithm.\n\nThe [[Euclidean algorithm]] determines the greatest common divisor (gcd) of two integers, say {{mvar|a}} and {{mvar|m}}. If {{mvar|a}} has a multiplicative inverse modulo {{mvar|m}}, this gcd must be 1. The last of several equations produced by the algorithm may be solved for this gcd. Then, using a method called \"back substitution\", an expression connecting the original parameters and this gcd can be obtained. In other words, integers {{mvar|x}} and {{mvar|y}} can be found to satisfy [[Bézout's identity]],\n\n:<math>ax + my = \\gcd(a, m)= 1.</math>\n\nRewritten, this is\n\n:<math>ax - 1 = (-y)m,</math>\n\nthat is,\n\n:<math>ax \\equiv 1 \\pmod{m},</math>\n\nso, a modular multiplicative inverse of {{mvar|a}} has been calculated. A more efficient version of the algorithm is the extended Euclidean algorithm, which, by using auxiliary equations, reduces two passes through the algorithm (back substitution can be thought of as passing through the algorithm in reverse) to just one.\n\nIn [[big O notation]], this algorithm runs in time {{math|O(log(''m'')<sup>2</sup>)}}, assuming {{math|{{abs|''a''}} < ''m''}}, and is considered to be very fast and generally more efficient than its alternative, exponentiation.\n\n===Using Euler's theorem===\nAs an alternative to the extended Euclidean algorithm, Euler's theorem may be used to compute modular inverses.<ref>Thomas Koshy. [https://books.google.com/books?id=d5Z5I3gnFh0C&pg=PA346 Elementary number theory with applications], 2nd edition. {{isbn|978-0-12-372487-8}}. P. 346.</ref>\n\nAccording to [[Euler's theorem]], if {{math|''a''}} is [[coprime]] to {{math|''m''}}, that is, {{math|1=gcd(''a'', ''m'') = 1}}, then\n\n:<math>a^{\\phi(m)} \\equiv 1 \\pmod{m},</math>\n\nwhere <math>\\phi</math> is Euler's totient function. This follows from the fact that {{math|''a''}} belongs to the multiplicative group <math>(\\mathbb{Z}/m\\mathbb{Z})</math><sup>×</sup> [[if and only if]] {{math|''a''}} is [[coprime]] to {{math|''m''}}. Therefore, a modular multiplicative inverse can be found directly:\n\n:<math>a^{\\phi(m)-1} \\equiv a^{-1} \\pmod{m}.</math>\n\nIn the special case where {{math|''m''}} is a prime, <math>\\phi (m) = m - 1</math> and a modular inverse is given by \n: <math>a^{-1} \\equiv a^{m-2} \\pmod{m}.</math>\n\nThis method is generally slower than the extended Euclidean algorithm, but is sometimes used when an implementation for modular exponentiation is already available. Some disadvantages of this method include:\n*The value <math>\\phi (m)</math> must be known and the most efficient known computation requires {{math|''m''}}'s [[factorization]]. Factorization is widely believed to be a computationally hard problem.  However, calculating <math>\\phi (m)</math> is straightforward when the prime factorization of {{math|''m''}} is known.\n*The relative cost of exponentiation. Though it can be implemented more efficiently using [[modular exponentiation]], when large values of {{math|''m''}} are involved this is most efficiently computed with the [[Montgomery reduction]] method. This algorithm itself requires a modular inverse mod {{math|''m''}}, which is what was to be calculated in the first place. Without the Montgomery method, the standard [[binary exponentiation]], which requires division mod {{math|''m''}} at every step, is a slow operation when {{math|''m''}} is large.\n\n==Applications==\n\nFinding a modular multiplicative inverse has many applications in algorithms that rely on the theory of modular arithmetic. For instance, in cryptography the use of modular arithmetic permits some operations to be carried out more quickly and with fewer storage requirements, while other operations become more difficult.<ref>{{harvnb|Trappe|Washington|2006|loc=p. 167}}</ref> Both of these features can be used to advantage. In particular, in the RSA algorithm, encrypting and decrypting a message is done using a pair of numbers that are multiplicative inverses with respect to a carefully selected modulus. One of these numbers is made public and can be used in a rapid encryption procedure, while the other, used in the decryption procedure, is kept hidden. Determining the hidden number from the public number is considered to be computationally infeasible and this is what makes the system work to ensure privacy.<ref>{{harvnb|Trappe|Washington|2006|loc=p. 165}}</ref>\n\nAs another example in a different context, consider the exact division problem in computer science where you have a list of odd word-sized numbers each divisible by {{math|''k''}} and you wish to divide them all by {{math|''k''}}. One solution is as follows:\n# Use the extended Euclidean algorithm to compute {{math|''k''<sup>−1</sup>}}, the modular multiplicative inverse of {{math|''k'' mod 2<sup>''w''</sup>}}, where {{math|''w''}} is the number of bits in a word. This inverse will exist since the numbers are odd and the modulus has no odd factors.\n# For each number in the list, multiply it by {{math|''k''<sup>−1</sup>}} and take the least significant word of the result.\n\nOn many machines, particularly those without hardware support for division, division is a slower operation than multiplication, so this approach can yield a considerable speedup. The first step is relatively slow but only needs to be done once.\n\nModular multiplicative inverses are used to obtain a solution of a system of linear congruences that is guaranteed by the [[Chinese Remainder Theorem]]. \n\nFor example, the system\n::{{mvar|X}} ≡ 4  (mod 5)\n::{{mvar|X}} ≡ 4  (mod 7)\n::{{mvar|X}} ≡ 6  (mod 11)\nhas common solutions since 5,7 and 11 are pairwise [[coprime]]. A solution is given by\n: {{mvar|X}} = {{math|''t''<sub>1</sub>}} (7 × 11) × 4 + {{math|''t''<sub>2</sub>}} (5 × 11) × 4 + {{math|''t''<sub>3</sub>}} (5 × 7) × 6\nwhere\n:{{math|''t''<sub>1</sub>}} = 3 is the modular multiplicative inverse of 7 × 11 (mod 5), \n:{{math|''t''<sub>2</sub>}} = 6 is the modular multiplicative inverse of 5 × 11 (mod 7) and \n:{{math|''t''<sub>3</sub>}} = 6 is the modular multiplicative inverse of 5 × 7 (mod 11). \nThus,\n:     {{mvar|X}} = 3 × (7 × 11) × 4 + 6 × (5 × 11) × 4 + 6 × (5 × 7) × 6 = 3504\nand in its unique reduced form\n:     {{mvar|X}} ≡ 3504 ≡ 39 (mod 385)\nsince 385 is the [[Least common multiple|LCM]] of 5,7 and 11.\n\n==See also==\n* [[Inversive congruential generator]] - a pseudo-random number generator that uses modular multiplicative inverses\n* [[Rational reconstruction (mathematics)]]\n\n==Notes==\n{{reflist}}\n\n==References==\n* {{citation|first1=Kenneth|last1=Ireland|first2=Michael|last2=Rosen|title=A Classical Introduction to Modern Number Theory|year=1990|publisher=Springer-Verlag|edition=2nd|isbn=0-387-97329-X}}\n* {{citation|first=Kenneth H.|last=Rosen|title=Elementary Number Theory and its Applications|edition=3rd|year=1993|publisher=Addison-Wesley|isbn=978-0-201-57889-8}}\n* {{Cite book|title=Chapter Zero: Fundamental Notions of Abstract Mathematics|last=Schumacher|first=Carol|publisher=Addison-Wesley|year=1996|isbn=0-201-82653-4}}\n* {{citation|first1=Wade|last1=Trappe|first2=Lawrence C.|last2=Washington|title=Introduction to Cryptography with Coding Theory|edition=2nd|year=2006|publisher=Prentice-Hall|isbn=978-0-13-186239-5}}\n\n==External links==\n*{{MathWorld |title=Modular Inverse |id=ModularInverse}}\n* [http://www.math.utah.edu/~fguevara/ Guevara Vasquez, Fernando] provides a [https://www.math.utah.edu/~fguevara/ACCESS2013/Euclid.pdf solved example] of solving the modulo multiplicative inverse using Euclid's Algorithm\n{{DEFAULTSORT:Modular Multiplicative Inverse}}\n[[Category:Modular arithmetic]]\n[[Category:Binary operations]]"
    },
    {
      "title": "Modulo operation",
      "url": "https://en.wikipedia.org/wiki/Modulo_operation",
      "text": "[[Image:Divmod.svg|thumb|right|250px|{{colorbox|red}} Quotient ({{math|''q''}}) and {{colorbox|lightgreen}} remainder ({{math|''r''}}) as functions of dividend ({{math|''a''}}), using different algorithms]]\nIn [[computing]], the '''modulo''' operation finds the [[remainder]]  after [[division (mathematics)|division]] of one number by another (called the ''[[modular arithmetic|modulus]]'' of the operation).\n\nGiven two positive numbers, {{math|''a''}} and {{math|''n''}}, {{math|''a''}} modulo {{math|''n''}} (abbreviated as {{math|''a'' mod ''n''}}) is the remainder of the [[Euclidean division]] of {{math|''a''}} by {{math|''n''}}, where {{math|''a''}} is the [[Division (mathematics)|dividend]] and {{math|''n''}} is the [[divisor]].\n\nFor example, the expression \"5 mod 2\" would evaluate to 1 because 5 divided by 2 has a [[quotient]] of 2 and a remainder of 1, while \"9 mod 3\" would evaluate to 0 because the division of 9 by 3 has a quotient of 3 and leaves a remainder of 0; there is nothing to subtract from 9 after multiplying 3 times 3. (Doing the division with a calculator will not show the result referred to here by this operation; the quotient will be expressed as a decimal fraction.)\n\nAlthough typically performed with {{math|''a''}} and {{math|''n''}} both being integers, many computing systems allow other types of numeric operands. The range of numbers for an integer modulo of {{math|''n''}} is 0 to {{math|''n'' − 1}}. ({{math|''a''}} mod 1 is always 0; {{math|''a'' mod 0}} is undefined, possibly resulting in a [[division by zero]] error in programming languages.) See [[modular arithmetic]] for an older and related convention applied in [[number theory]].\n\nWhen either {{math|''a''}} or {{math|''n''}} is negative, the naive definition breaks down and [[programming language]]s differ in how these values are defined.\n\n==Remainder calculation for the modulo operation==\n\n{| class=\"wikitable sortable\"  style=\"float:right; margin-left:1em; margin-right:0; width:30%;\"\n|+ Integer modulo operators in various programming languages\n|-\n! [[Programming language|Language]]\n! Operator\n! abbr=\"Sign\" | Result has same sign as\n|-\n| [[ABAP]]\n| <code>MOD</code>\n| Nonnegative always\n|-\n| [[ActionScript]]\n| <code>%</code>\n| Dividend\n|-\n| rowspan=\"2\" | [[Ada (programming language)|Ada]]\n| <code>mod</code>\n| Divisor\n|-\n| <code>rem</code>\n| Dividend\n|-\n| [[ALGOL 68]]\n| <code>÷×</code>, <code>mod</code>\n| Nonnegative always\n|-\n| [[AMPL]]\n| <code>mod</code>\n| Dividend\n|-\n| [[APL (programming language)|APL]]\n| <code><nowiki>|</nowiki></code>{{ref|5a}}\n| Divisor\n|-\n| [[AppleScript]]\n| <code>mod</code>\n| Dividend\n|-\n| [[AutoLISP]]\n| <code>(rem d n)</code>\n| Dividend\n|-\n| [[AWK]]\n| <code>%</code>\n| Dividend\n|-\n| [[BASIC]]\n| <code>Mod</code>\n| Undefined\n|-\n| [[Bash (Unix shell)|bash]]\n| <code>%</code>\n| Dividend\n|-\n| [[bc (programming language)|bc]]\n| <code>%</code>\n| Dividend\n|-\n| rowspan=\"2\" | [[C (programming language)|C]] (ISO 1990)\n| <code>%</code>\n| Implementation-defined\n|-\n| <code>div</code>\n| Dividend\n|-\n| rowspan=\"2\" | [[C++]] (ISO 1998)\n| <code>%</code>\n| Implementation-defined<ref>{{Cite journal |title=ISO/IEC 14882:2003: Programming languages – C++ |publisher=[[International Organization for Standardization]] (ISO), [[International Electrotechnical Commission]] (IEC) |year=2003 |location=5.6.4 |postscript=<!--None-->}}. \"the binary % operator yields the remainder from the division of the first expression by the second. .... If both operands are nonnegative then the remainder is nonnegative; if not, the sign of the remainder is implementation-defined\".</ref>\n|-\n| <code>div</code>\n| Dividend\n|-\n| [[C99|C (ISO 1999)]]\n| <code>%</code>, <code>div</code>\n| Dividend<ref name=\"C99\">{{cite web |title=C99 specification (ISO/IEC 9899:TC2) |url=http://www.open-std.org/jtc1/sc22/wg14/www/docs/n1124.pdf |accessdate=16 August 2018 |location=6.5.5 Multiplicative operators |date=2005-05-06}}</ref>\n|-\n| [[C++11|C++ (ISO 2011)]]\n| <code>%</code>, <code>div</code>\n| Dividend\n|-\n| [[C Sharp (programming language)|C#]]\n| <code>%</code>\n| Dividend\n|-\n| [[Clarion (programming language)|Clarion]]\n| <code>%</code>\n| Dividend\n|-\n| [[Clean (programming language)|Clean]]\n| <code>rem</code>\n| Dividend\n|-\n| rowspan=\"2\" | [[Clojure]]\n| <code>mod</code>\n| Divisor\n|-\n| <code>rem</code>\n| Dividend\n|-\n| [[COBOL]]{{ref|4}}\n| <code>FUNCTION&nbsp;MOD</code>\n| Divisor\n|-\n| rowspan=\"2\" | [[CoffeeScript]]\n| <code>%</code>\n| Dividend\n|-\n| <code>%%</code>\n| Divisor<ref name=\"CoffeeScript\">[http://coffeescript.org/#operators CoffeeScript operators]</ref>\n|-\n| [[ColdFusion]]\n| <code>%</code>, <code>MOD</code>\n| Dividend\n|-\n| rowspan=\"2\" | [[Common Lisp]]\n| <code>mod</code>\n| Divisor\n|-\n| <code>rem</code>\n| Dividend\n|-\n| [https://www.scirra.com/construct2 Construct 2]\n| <code>%</code>\n|\n|-\n| [[D (programming language)|D]]\n| <code>%</code>\n| Dividend<ref>{{cite web |title=Expressions |url=http://www.digitalmars.com/d/2.0/expression.html#MulExpression |work=D Programming Language 2.0 |publisher=Digital Mars |accessdate=29 July 2010}}</ref>\n|-\n| rowspan=\"2\" | [[Dart (programming language)|Dart]]\n| <code>%</code>\n| Nonnegative always\n|-\n| <code>remainder()</code>\n| Dividend\n|-\n| [[Eiffel (programming language)|Eiffel]]\n| <code>\\\\</code>\n| Dividend\n|-\n| rowspan=\"2\" | [[Elm (programming_language)|Elm]]\n| <code>modBy</code>\n| Divisor\n|-\n| <code>remainderBy</code>\n| Dividend\n|-\n| [[Erlang (programming language)|Erlang]]\n| <code>rem</code>\n| Dividend\n|-\n| rowspan=\"2\" | [[Euphoria (programming language)|Euphoria]]\n| <code>mod</code>\n| Divisor\n|-\n| <code>remainder</code>\n| Dividend\n|-\n| [[F Sharp (programming language)|F#]]\n| <code>%</code>\n| Dividend\n|-\n| [[FileMaker]]\n| <code>Mod</code>\n| Divisor\n|-\n| [[Forth (programming language)|Forth]]\n| <code>mod</code>\n| implementation defined\n|-\n| rowspan=\"2\" | [[Fortran]]\n| <code>mod</code>\n| Dividend\n|-\n| <code>modulo</code>\n| Divisor\n|-\n| Frink\n| <code>mod</code>\n| Divisor\n|-\n| [[GameMaker Studio]] (GML)\n| <code>mod</code>, <code>%</code>\n| Dividend\n|-\n| [[Godot (game engine)|GDScript]]\n| <code>%</code>\n| Dividend\n|-\n| [[Go (programming language)|Go]]\n| <code>%</code>\n| Dividend\n|-\n| rowspan=\"2\" | [[Haskell (programming language)|Haskell]]\n| <code>mod</code>\n| Divisor\n|-\n| <code>rem</code>\n| Dividend\n|-\n| [[Haxe]]\n| <code>%</code>\n| Dividend\n|-\n| [[Kotlin (programming language)|Kotlin]]\n| <code>%</code>\n| Dividend\n|-\n| [[J (programming language)|J]]\n| <code><nowiki>|</nowiki></code>{{ref|5b}}\n| Divisor\n|-\n| rowspan=\"2\" | [[Java (programming language)|Java]]\n| <code>%</code>\n| Dividend\n|-\n| <code>Math.floorMod</code>\n| Divisor\n|-\n| [[JavaScript]]\n| <code>%</code>\n| Dividend\n|-\n| rowspan=\"2\" | [[Julia (programming language)|Julia]]\n| <code>mod</code>\n| Divisor\n|-\n| <code>%</code>, <code>rem</code>\n| Dividend\n|-\n|[[LabVIEW]]\n|<code>mod</code>\n|Dividend\n|-\n| [[LibreOffice]]\n| <code>=MOD()</code>\n| Divisor\n|-\n| rowspan=\"2\" | [[Logo (programming language)|Logo]]\n| <code>MODULO</code>\n| Divisor\n|-\n| <code>REMAINDER</code>\n| Dividend\n|-\n| [[Lua (programming language)|Lua]] 5\n| <code>%</code>\n| Divisor\n|-\n| [[Lua (programming language)|Lua]] 4\n| <code>mod(x,y)</code>\n| Divisor\n|-\n| [[Liberty BASIC]]\n| <code>MOD</code>\n| Dividend\n|-\n| [[Mathcad]]\n| <code>mod(x,y)</code>\n| Divisor\n|-\n| [[Maple (software)|Maple]]\n| <code>e mod m</code>\n| Nonnegative always\n|-\n| [[Mathematica]]\n| <code>Mod[a, b]</code>\n| Divisor\n|-\n| rowspan=\"2\" | [[MATLAB]]\n| <code>mod</code>\n| Divisor\n|-\n| <code>rem</code>\n| Dividend\n|-\n| rowspan=\"2\" | [[Maxima (software)|Maxima]]\n| <code>mod</code>\n| Divisor\n|-\n| <code>remainder</code>\n| Dividend\n|-\n| [[Maya Embedded Language]]\n| <code>%</code>\n| Dividend\n|-\n| [[Microsoft Excel]]\n| <code>=MOD()</code>\n| Divisor\n|-\n| [[Minitab]]\n| <code>MOD</code>\n| Divisor\n|-\n| [[Korn shell|mksh]]\n| <code>%</code>\n| Dividend\n|-\n| rowspan=\"2\" | [[Modula-2]]\n| <code>MOD</code>\n| Divisor\n|-\n| <code>REM</code>\n| Dividend\n|-\n| [[MUMPS]]\n| <code>#</code>\n| Divisor\n|-\n| rowspan=\"2\" | [[Netwide Assembler]] (NASM, NASMX)\n| <code>%</code>\n| Modulo operator unsigned\n|-\n|  <code>%%</code>\n| Modulo operator signed\n|-\n| [[Nim (programming_language)|Nim]]\n| <code>mod</code>\n| Dividend\n|-\n| [[Oberon (programming language)|Oberon]]\n| <code>MOD</code>\n| Divisor{{ref|3}}\n|-\n| [[Object Pascal]], [[Delphi (programming language)|Delphi]]\n| <code>mod</code>\n| Dividend\n|-\n| [[OCaml]]\n| <code>mod</code>\n| Dividend\n|-\n| [[Occam (programming language)|Occam]]\n| <code>\\</code>\n| Dividend\n|-\n| [[Pascal (programming language)|Pascal]] (ISO-7185 and -10206)\n| <code>mod</code>\n| Nonnegative always\n|-\n| Programming Code Advanced (PCA)\n| <code>\\</code>\n| Undefined\n|-\n| [[Perl]]\n| <code>%</code>\n| Divisor{{ref|1}}\n|-\n| rowspan=\"2\" | [[Phix]]\n| <code>mod</code>\n| Divisor\n|-\n| <code>remainder</code>\n| Dividend\n|-\n| [[PHP]]\n| <code>%</code>\n| Dividend\n|-\n| PIC [[BASIC]] Pro\n| <code>\\\\</code>\n| Dividend\n|-\n| [[PL/I]]\n| <code>mod</code>\n| Divisor (ANSI PL/I)\n|-\n| [[PowerShell]]\n| <code>%</code>\n| Dividend\n|-\n| Programming Code (PRC)\n| <code> MATH.OP - 'MOD; (\\)'</code>\n| Undefined\n|-\n| [[OpenEdge Advanced Business Language|Progress]]\n| <code>modulo</code>\n| Dividend\n|-\n| rowspan=\"2\"| [[Prolog]] (ISO 1995)\n| <code>mod</code>\n| Divisor\n|-\n| <code>rem</code>\n| Dividend\n|-\n| [[PureBasic]]\n| <code>%</code>, <code>Mod(x,y)</code>\n| Dividend\n|-\n| rowspan=\"2\" | [[Python (programming language)|Python]]\n| <code>%</code>\n| Divisor\n|-\n| <code>math.fmod</code>\n| Dividend\n|-\n| [[Q Sharp|Q#]]\n| <code>%</code>\n| Dividend<ref>{{Cite web|url=https://docs.microsoft.com/en-us/quantum/quantum-qr-expressions?view=qsharp-preview#numeric-expressions|title=Expressions|last=QuantumWriter|website=docs.microsoft.com|language=en-us|access-date=2018-07-11}}</ref>\n|-\n| [[Racket (programming language)|Racket]]\n| <code>remainder</code>\n| Dividend\n|-\n| [[RealBasic]]\n| <code>MOD</code>\n| Dividend\n|-\n| [[R (programming language)|R]]\n| <code>%%</code>\n| Divisor\n|-\n| [[Rexx]]\n| <code>//</code>\n| Dividend\n|-\n| [[RPG (programming language)|RPG]]\n| <code>%REM</code>\n| Dividend\n|-\n| rowspan=\"2\" | [[Ruby (programming language)|Ruby]]\n| <code>%</code>, <code>modulo()</code>\n| Divisor\n|-\n| <code>remainder()</code>\n| Dividend\n|-\n| [[Rust (programming language)|Rust]]\n| <code>%</code>\n| Dividend\n|-\n|[[SAS language|SAS]]\n| <code>MOD</code>\n| Dividend\n|-\n| [[Scala (programming language)|Scala]]\n| <code>%</code>\n| Dividend\n|-\n| rowspan=\"2\" | [[Scheme (programming language)|Scheme]]\n| <code>modulo</code>\n| Divisor\n|-\n| <code>remainder</code>\n| Dividend\n|-\n| rowspan=\"2\" | [[Scheme (programming language)|Scheme]] R<sup>6</sup>RS\n| <code>mod</code>\n| Nonnegative always<ref name=\"r6rs\">[http://www.r6rs.org/final/html/r6rs/r6rs-Z-H-14.html#node_sec_11.7.3.1 r6rs.org]</ref>\n|-\n| <code>mod0</code>\n| Nearest to zero<ref name=\"r6rs\"/>\n|-\n| rowspan=\"2\" | [[Seed7]]\n| <code>mod</code>\n| Divisor\n|-\n| <code>rem</code>\n| Dividend\n|-\n| rowspan=\"2\" | [[SenseTalk]]\n| <code>modulo</code>\n| Divisor\n|-\n| <code>rem</code>\n| Dividend\n|-\n| rowspan=\"2\" | [[Smalltalk]]\n| <code>\\\\</code>\n| Divisor\n|-\n| <code>rem:</code>\n| Dividend\n|-\n| [[Snap!_(programming_language)|Snap!]]\n| <code>mod</code>\n| Divisor\n|-\n| \n| <code>//</code>\n| Divisor\n|-\n|[[Solidity]]\n| <code>%</code>\n| Divisor\n|-\n| [[SQL]] ([[SQL:1999]])\n| <code>mod(x,y)</code>\n| Dividend\n|-\n| SQL (SQL:2012)\n| <code>%</code>\n| Dividend\n|-\n| rowspan=\"2\" | [[Standard ML]]\n| <code>mod</code>\n| Divisor\n|-\n| <code>Int.rem</code>\n| Dividend\n|-\n| [[Stata]]\n| <code>mod(x,y)</code>\n| Nonnegative always\n|-\n| [[Swift (programming language)|Swift]]\n| <code>%</code>\n| Dividend\n|-\n| [[Tcl]]\n| <code>%</code>\n| Divisor\n|-\n| [[Torque (game engine)|Torque]]\n| <code>%</code>\n| Dividend\n|-\n| [[Turing (programming language)|Turing]]\n| <code>mod</code>\n| Divisor\n|-\n| [[Verilog]] (2001)\n| <code>%</code>\n| Dividend\n|-\n| rowspan=\"2\" | [[VHDL]]\n| <code>mod</code>\n| Divisor\n|-\n| <code>rem</code>\n| Dividend\n|-\n| [[Vimscript|VimL]]\n| <code>%</code>\n| Dividend\n|-\n| [[Visual Basic]]\n| <code>Mod</code>\n| Dividend\n|-\n| [[WebAssembly]]\n| <code>i32.rem_s</code>, <code>i64.rem_s</code>\n| Dividend\n|-\n| [[x86 assembly language|x86 assembly]]\n| <code>IDIV</code>\n| Dividend\n|-\n|-\n| rowspan=\"2\" | [[XBase++]]\n| <code>%</code>\n| Dividend\n|-\n| <code>Mod()</code>\n| Divisor\n|-\n| Z3 theorem prover\n| <code>div</code>, <code>mod</code>\n| Nonnegative always\n|}\n{| class=\"wikitable sortable\"  style=\"float:right; clear:right; margin-left:1em; margin-right:0; width:30%;\"\n|+ Floating-point modulo operators in various programming languages\n|-\n! [[Programming language|Language]]\n! Operator\n! abbr=\"Sign\" | Result has same sign as\n|-\n| [[ABAP]]\n| <code>MOD</code>\n| Nonnegative always\n|-\n| [[C (programming language)|C]] (ISO 1990)\n| <code>fmod</code>\n| Dividend<ref>{{Cite journal|title=ISO/IEC 9899:1990: Programming languages – C |publisher=[[International Organization for Standardization|ISO]], [[International Electrotechnical Commission|IEC]] |year=1990 |location=7.5.6.4 |postscript=<!--None-->}} \"The <code>fmod</code> function returns the value <code>x - i * y</code>, for some integer <code>i</code> such that, if <code>y</code> is nonzero, the result as the same sign as <code>x</code> and magnitude less than the magnitude of <code>y</code>.\".</ref>\n|-\n| rowspan=\"2\" | [[C99|C (ISO 1999)]]\n| <code>fmod</code>\n| Dividend\n|-\n| <code>remainder</code>\n| Nearest to zero\n|-\n| [[C++]] (ISO 1998)\n| <code>std::fmod</code>\n| Dividend\n|-\n| rowspan=\"2\" | [[C++11|C++ (ISO 2011)]]\n| <code>std::fmod</code>\n| Dividend\n|-\n| <code>std::remainder</code>\n| Nearest to zero\n|-\n| [[C Sharp (programming language)|C#]]\n| <code>%</code>\n| Dividend\n|-\n| rowspan=\"2\" | [[Common Lisp]]\n| <code>mod</code>\n| Divisor\n|-\n| <code>rem</code>\n| Dividend\n|-\n| [[D (programming language)|D]]\n| <code>%</code>\n| Dividend\n|-\n| rowspan=\"2\" | [[Dart (programming language)|Dart]]\n| <code>%</code>\n| Nonnegative always\n|-\n| <code>remainder()</code>\n| Dividend\n|-\n| [[F Sharp (programming language)|F#]]\n| <code>%</code>\n| Dividend\n|-\n| rowspan=\"2\" | [[Fortran]]\n| <code>mod</code>\n| Dividend\n|-\n| <code>modulo</code>\n| Divisor\n|-\n| [[Go (programming language)|Go]]\n| <code>math.Mod</code>\n| Dividend\n|-\n| [[Haskell (programming language)|Haskell]] (GHC)\n| <code>Data.Fixed.mod'</code>\n| Divisor\n|-\n| [[Java (programming language)|Java]]\n| <code>%</code>\n| Dividend\n|-\n| [[JavaScript]]\n| <code>%</code>\n| Dividend\n|-\n| [[LabVIEW]]\n| <code>mod</code>\n| Dividend\n|-\n| [[Microsoft Excel]]\n| <code>=MOD()</code>\n| Divisor\n|-\n| [[OCaml]]\n| <code>mod_float</code>\n| Dividend\n|-\n| [[Perl]]\n| <code>POSIX::fmod</code>\n| Dividend\n|-\n| [[Perl6]]\n| <code>%</code>\n| Divisor\n|-\n| [[PHP]]\n| <code>fmod</code>\n| Dividend\n|-\n| rowspan=\"2\"| [[Python (programming language)|Python]]\n| <code>%</code>\n| Divisor\n|-\n| <code>math.fmod</code>\n| Dividend\n|-\n| [[Rexx]]\n| <code>//</code>\n| Dividend\n|-\n| rowspan=\"2\" | [[Ruby (programming language)|Ruby]]\n| <code>%</code>, <code>modulo()</code>\n| Divisor\n|-\n| <code>remainder()</code>\n| Dividend\n|-\n| rowspan=\"2\" | [[Scheme (programming language)|Scheme]] R<sup>6</sup>RS\n| <code>flmod</code>\n| Nonnegative always\n|-\n| <code>flmod0</code>\n| Nearest to zero\n|-\n| [[Standard ML]]\n| <code>Real.rem</code>\n| Dividend\n|-\n| [[Swift (programming language)|Swift]]\n| <code>truncatingRemainder(dividingBy:)</code>\n| Dividend\n|-\n| rowspan=\"2\" | [[XBase++]] \n| <code>%</code>\n| Dividend\n|-\n| <code>Mod()</code>\n| Divisor\n|}\nIn [[mathematics]], the result of the modulo operation is the remainder of the [[Euclidean division]]. However, other conventions are possible. Computers and calculators have various ways of storing and representing numbers; thus their definition of the modulo operation depends on the [[programming language]] or the underlying [[computer hardware|hardware]].\n\nIn nearly all computing systems, the [[quotient]] {{math|''q''}} and the remainder {{math|''r''}} of {{math|''a''}} divided by {{math|''n''}} satisfy\n: {{NumBlk|:|<math>\\begin{align}\nq \\,&\\in \\mathbb{Z} \\\\\na \\,&= n q + r \\\\\n|r| \\,&< |n|\n\\end{align}</math>|{{EquationRef|1}}}}\n\nHowever, this still leaves a sign ambiguity if the remainder is nonzero: two possible choices for the remainder occur, one negative and the other positive, and two possible choices for the quotient occur. Usually, in number theory, the positive remainder is always chosen, but programming languages choose depending on the language and the signs of {{math|''a''}} or {{math|''n''}}.{{ref|2}} Standard [[Pascal (programming language)|Pascal]] and [[ALGOL 68]] give a positive remainder (or 0) even for negative divisors, and some programming languages, such as C90, leave it to the implementation when either of {{math|''n''}} or {{math|''a''}} is negative. See the table for details. {{math|''a''}} modulo 0 is undefined in most systems, although some do define it as {{math|''a''}}.\n\n{{bulleted list\n| Many implementations use ''truncated division'', where the quotient is defined by [[truncation]] {{math|''q'' {{=}} trunc({{sfrac|''a''|''n''}})}} and thus according to equation ({{EquationNote|1}}) the remainder would have ''same sign as the dividend''.  The quotient is rounded towards zero: equal to the first integer in the direction of zero from the exact rational quotient.\n:<math>r = a - n \\operatorname{trunc}\\left(\\frac{a}{n}\\right)</math>\n\n| [[Donald Knuth]]<ref>{{cite book|\nfirst=Donald. E.|last=Knuth|title=The Art of Computer Programming\n|publisher=Addison-Wesley|year=1972\n}}</ref> described ''floored division'' where the quotient is defined by the [[floor function]] {{math|''q'' {{=}} ⌊{{sfrac|''a''|''n''}}⌋}} and thus according to equation ({{EquationNote|1}}) the remainder would have the ''same sign as the divisor''.  Due to the floor function, the quotient is always rounded downwards, even if it is already negative.\n:<math>r = a - n \\left\\lfloor\\frac{a}{n}\\right\\rfloor</math>\n\n| Raymond T. Boute<ref>{{cite journal\n | last = Boute\n | first = Raymond T.\n | title = The Euclidean definition of the functions div and mod\n | journal = ACM Transactions on Programming Languages and Systems\n | volume = 14\n | issue = 2\n | pages = 127–144\n | publisher = ACM Press (New York, NY, USA)\n | date = April 1992\n | url = http://portal.acm.org/citation.cfm?id=128862&coll=portal&dl=ACM\n | doi = 10.1145/128861.128862}}</ref> describes the Euclidean definition in which the remainder is nonnegative always, {{math|0 ≤ ''r''}}, and is thus consistent with the ''[[Euclidean division]]'' algorithm.  In this case,\n: <math>n > 0 \\Rightarrow q = \\left\\lfloor\\frac{a}{n}\\right\\rfloor</math>\n: <math>n < 0 \\Rightarrow q = \\left\\lceil\\frac{a}{n}\\right\\rceil</math>\nor equivalently\n: <math>q = \\sgn(n) \\left\\lfloor \\frac{a}{\\left|n\\right|} \\right\\rfloor</math>\nwhere {{math|sgn}} is the [[sign function]], and thus\n: <math>r = a - |n| \\left\\lfloor \\frac{a}{\\left|n\\right|} \\right\\rfloor</math>\n\n| Common Lisp also defines round-division and ceiling-division where the quotient is given by {{math|''q'' {{=}} round({{sfrac|''a''|''n''}})}} and {{math|q {{=}} ⌈{{sfrac|''a''|''n''}}⌉}} respectively.\n\n| [[IEEE 754-1985|IEEE 754]] defines a remainder function where the quotient is {{math|{{sfrac|''a''|''n''}}}} rounded according to the [[IEEE 754-1985#Rounding floating-point numbers|round to nearest convention]].  Thus, the sign of the remainder is chosen to be ''nearest to zero''.\n\n}}\n\nAs described by Leijen,\n{{Quote|text=Boute argues that Euclidean division is superior to the other ones in terms of regularity and useful mathematical properties, although floored division, promoted by Knuth, is also a good definition.  Despite its widespread use, truncated division is shown to be inferior to the other definitions.|sign=Daan Leijen|source=''Division and Modulus for Computer Scientists''<ref>{{cite web\n | last = Leijen\n | first = Daan\n | title = Division and Modulus for Computer Scientists\n | date = December 3, 2001\n | url = http://research.microsoft.com/pubs/151917/divmodnote.pdf\n | format = PDF\n | accessdate =2014-12-25}}</ref>}}\n\nHowever, Boute concentrates on the properties of the modulo operation itself and does not rate the fact that the truncated division shows the symmetry {{math|(-''a'') div ''n'' {{=}} -(''a'' div ''n'')}} and {{math|''a'' div (-''n'') {{=}} -(''a'' div ''n'')}}, which is similar to the ordinary division. As neither floor division nor Euclidean division offer this symmetry, Boute's judgement is at least incomplete.{{citation needed|date=January 2018}}{{OR|date=January 2018}}\n\n==Common pitfalls==\nWhen the result of a modulo operation has the sign of the dividend, it can lead to surprising mistakes.\n\nFor example, to test if an integer is odd, one might be inclined to test if the remainder by 2 is equal to 1:\n\n<source lang=\"cpp\">\nbool is_odd(int n) {\n    return n % 2 == 1;\n}\n</source>\n\nBut in a language where modulo has the sign of the dividend, that is incorrect, because when {{math|''n''}} (the dividend) is negative and odd, {{math|''n''}} mod 2 returns −1, and the function returns false.\n\nOne correct alternative is to test that it is not 0 (because remainder 0 is the same regardless of the signs):\n\n<source lang=\"cpp\">\nbool is_odd(int n) {\n    return n % 2 != 0;\n}\n</source>\n\nOr, by understanding in the first place that for any odd number, the modulo remainder may be either 1 or −1:\n\n<source lang=\"cpp\">\nbool is_odd(int n) {\n    return n % 2 == 1 || n % 2 == -1;\n}\n</source>\n\n==Notation==\n{{About|the binary ''mod'' operation|the ''(mod m)'' notation|congruence relation|section=yes}}\n\nSome calculators have a {{math|mod()}} function button, and many programming languages have a similar function, expressed as {{math|mod(''a'', ''n'')}}, for example. Some also support expressions that use \"%\", \"mod\", or \"Mod\" as a modulo or remainder [[Operator (programming)|operator]], such as\n:<code>a % n</code>\nor\n:<code>a mod n</code>\nor equivalent, for environments lacking a {{math|mod()}} function ('int' inherently produces the truncated value of {{math|{{sfrac|''a''|''n''}}}})\n:<code>a - (n * int(a/n))</code>\n\n==Performance issues==\nModulo operations might be implemented such that a division with a remainder is calculated each time. For special cases, on some hardware, faster alternatives exist. For example, the modulo of powers of 2 can alternatively be expressed as a [[Bitwise operation|bitwise]] AND operation:\n:<code>x % 2<sup>n</sup> == x & (2<sup>n</sup> - 1)</code>\n\nExamples (assuming {{math|''x''}} is a positive integer):\n:<code>x % 2 == x & 1</code>\n:<code>x % 4 == x & 3</code>\n:<code>x % 8 == x & 7</code>\n\nIn devices and software that implement bitwise operations more efficiently than modulo, these alternative forms can result in faster calculations.<ref>{{cite web |first= Adam |last= Horvath |url= http://blog.teamleadnet.com/2012/07/faster-division-and-modulo-operation.html |title= Faster division and modulo operation - the power of two |date= July 5, 2012}}</ref>\n\n[[Compiler optimization|Optimizing]] [[compiler]]s may recognize expressions of the form <code>expression % constant</code> where <code>constant</code> is a power of two and automatically implement them as <code>expression & (constant-1)</code>, allowing to write clearer code without compromising performance. This simple optimization is not possible for languages in which the result of the modulo operation has the sign of the dividend (including C), unless the dividend is of an [[Signedness|unsigned]] integer type. This is because, if the dividend is negative, the modulo will be negative, whereas <code>expression &amp; (constant-1)</code> will always be positive. For these languages, the equivalence <code>x % 2<sup>n</sup> == x < 0 ? x | ~(2<sup>n</sup> - 1) : x & (2<sup>n</sup> - 1)</code> has to be used instead, expressed using bitwise OR, NOT and AND operations.\n\n==Equivalences==\nSome modulo operations can be factored or expanded similarly to other mathematical operations.  This may be useful in [[cryptography]] proofs, such as the [[Diffie–Hellman key exchange]].\n*Identity:\n**{{math|(''a'' mod ''n'') mod ''n'' {{=}} ''a'' mod ''n''}}.\n**{{math|''n''{{sup|''x''}} mod ''n'' {{=}} 0}} for all positive integer values of {{math|''x''}}.\n** If {{math|''p''}} is a [[prime number]] which is not a [[divisor]] of {{math|''b''}}, then {{math|''ab''{{sup|''p''−1}} mod ''p'' {{=}} ''a'' mod ''p''}}, due to [[Fermat's little theorem]].\n*Inverse:\n**{{math|[(−''a'' mod ''n'') + (''a'' mod ''n'')] mod ''n'' {{=}} 0}}.\n**{{math|''b''{{sup|−1}} mod ''n''}} denotes the [[modular multiplicative inverse]], which is defined if and only if {{math|''b''}} and {{math|''n''}} are [[relatively prime]], which is the case when the left hand side is defined: {{math|[(''b''{{sup|−1}} mod ''n'')(''b'' mod ''n'')] mod ''n'' {{=}} 1}}.\n*Distributive:\n**{{math|(''a'' + ''b'') mod ''n'' {{=}} [(''a'' mod ''n'') + (''b'' mod ''n'')] mod ''n''}}.\n**{{math|''ab'' mod ''n'' {{=}} [(''a'' mod ''n'')(''b'' mod ''n'')] mod ''n''}}.\n**{{math|''d'' mod (''abc'') {{=}} (''d'' mod ''a'') + ''a''[(''d'' \\ ''a'') mod ''b''] + ''ab''[(''d'' \\ ''a'' \\ ''b'') mod ''c'']}}, where {{math|\\}} is the operator for the quotient from [[Euclidean division]].\n**{{math|''c'' mod (''a+b'') {{=}} (''c'' mod ''a'') + [''bc'' \\ (''a''+''b'')] mod ''b'' - [''bc'' \\ (''a'' + ''b'')] mod ''a''}}.\n*Division (definition):  {{math|{{sfrac|''a''|''b''}} mod ''n'' {{=}} [(''a'' mod ''n'')(''b''{{sup|−1}} mod ''n'')] mod ''n''}}, when the right hand side is defined (that is when {{math|''b''}} and {{math|''n''}} are  [[coprime]]). Undefined otherwise. \n*Inverse multiplication:  {{math|[(''ab'' mod ''n'')(''b''{{sup|−1}} mod ''n'')] mod ''n'' {{=}} ''a'' mod ''n''}}.\n\n==See also==\n*[[Modulo (disambiguation)]] and [[modulo (jargon)]] – many uses of the word ''modulo'', all of which grew out of [[Carl F. Gauss]]'s introduction of ''[[modular arithmetic]]'' in 1801.\n*[[Modular exponentiation]]\n\n==Notes==\n* {{note|1}} Perl usually uses arithmetic modulo operator that is machine-independent. For examples and exceptions, see the Perl documentation on multiplicative operators.<ref>[http://perldoc.perl.org/perlop.html#Multiplicative-Operators Perl documentation]</ref>\n* {{note|2}} Mathematically, these two choices are but two of the infinite number of choices available for [[remainder#The inequality satisfied by the remainder|the inequality satisfied by a remainder]].\n* {{note|3}} Divisor must be positive, otherwise undefined.\n* {{note|4}} As implemented in ACUCOBOL, Micro Focus COBOL, and possible others.\n* {{note|5a}}{{note|5b}} Argument order reverses, i.e., <code>α|ω</code> computes <math>\\omega\\bmod\\alpha</math>, the remainder when dividing <code>ω</code> by <code>α</code>.\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Modulo operation}}\n[[Category:Computer arithmetic]]\n[[Category:Articles with example C++ code]]\n[[Category:Operators (programming)]]\n[[Category:Modular arithmetic]]\n[[Category:Binary operations]]\n\n[[de:Division mit Rest#Modulo]]"
    },
    {
      "title": "Multiplication",
      "url": "https://en.wikipedia.org/wiki/Multiplication",
      "text": "{{short description|Arithmetical operation}}\n{{about|the mathematical operation}}\n{{refimprove|date=April 2012}}\n[[File:Multiply 4 bags 3 marbles.svg|thumb|right|Four bags with three [[Marble (toy)|marbles]] per bag gives twelve marbles (4 × 3 = 12).]]\n[[File:Multiply scaling.svg|thumb|right|Multiplication can also be thought of as [[Scale factor|scaling]]. Here we see 2 being multiplied by 3 using scaling, giving 6 as a result.]]\n[[File:Multiplication as scaling integers.gif|thumb|Animation for the multiplication 2 × 3 = 6.]]\n[[File:Multiplication scheme 4 by 5.jpg|thumb|right|4 × 5 = 20. The large rectangle is composed of 20 squares, each having dimensions of 1 by 1.]]\n[[File:Multiply field fract.svg|thumb|right|Area of a cloth {{nowrap|1=4.5m × 2.5m = 11.25m<sup>2</sup>}}; {{nowrap|1=4½ × 2½ = 11¼}}]]\n'''Multiplication''' (often denoted by the cross symbol \"'''[[Multiplication sign|×]]'''\", by a point \"'''[[Interpunct|⋅]]'''\", by [[juxtaposition]], or, on computers, by an [[asterisk]] \"'''∗'''\") is one of the four [[Elementary arithmetic|elementary]] [[Operation (mathematics)|mathematical operations]] of [[arithmetic]], with the others being [[addition]], [[subtraction]] and [[division (mathematics)|division]].\n\nThe multiplication of [[Natural number|whole numbers]] may be thought as a [[Multiplication and repeated addition|repeated addition]]; that is, the multiplication of two numbers is equivalent to adding as many copies of one of them, the ''multiplicand'', as the value of the other one, the ''multiplier''. The multiplier can be written first and multiplicand second (though the custom can vary by culture<ref>{{cite web |url=http://d.hatena.ne.jp/enomoto-2009/touch/20090930/1254292133 |title=小学校の掛け算の授業では、順序に意味があるらしい。 |language=Japanese |trans-title=In elementary school multiplication lessons, the order would appear to be meaningful |date=September 30, 2009 |accessdate=May 14, 2017}}</ref>); both can be called ''factors''.\n:<math>a\\times b = \\underbrace{b + \\cdots + b}_a</math>\n\nFor example, 4 multiplied by 3 (often written as <math> 3 \\times 4 </math> and spoken as \"3 times 4\") can be calculated by adding 3 copies of 4 together:\n:<math>3 \\times 4 = 4 + 4 + 4 = 12</math>\nHere 3 and 4 are the ''factors'' and 12 is the ''product''.\n\nOne of the main [[#Properties|properties]] of multiplication is the [[commutative property]]: adding 3 copies of 4 gives the same result as adding 4 copies of 3:\n:<math>4 \\times 3 = 3 + 3 + 3 + 3 = 12</math>\n\nThus the designation of multiplier and multiplicand does not affect the result of the multiplication<ref name=\"Devlin\">{{cite web |last=Devlin |first=Keith |url=http://www.maa.org/external_archive/devlin/devlin_01_11.html |title=What Exactly is Multiplication? |author-link=Keith Devlin |publisher=[[Mathematical Association of America]] |date=January 2011 |quote=With multiplication you have a multiplicand (written second) multiplied by a multiplier (written first) |accessdate=May 14, 2017}}</ref>.\n\nThe multiplication of [[integer]]s (including negative numbers), [[rational number]]s (fractions) and [[real number]]s is defined by a systematic [[Multiplication#Multiplication of different kinds of numbers|generalization]] of this basic definition.\n\nMultiplication can also be visualized as counting objects arranged in a [[rectangle]] (for whole numbers) or as finding the [[area]] of a rectangle whose sides have given [[length]]s. The area of a rectangle does not depend on which side is measured first, which illustrates the commutative property. The product of two measurements is a new type of measurement, for instance multiplying the lengths of the two sides of a rectangle gives its area, this is the subject of [[dimensional analysis]].\n\nThe inverse operation of multiplication is [[division (mathematics)|division]]. For example, since 4 multiplied by 3 equals 12, then 12 divided by 3 equals 4. Multiplication by 3, followed by division by 3, yields the original number (since the division of a number other than 0 by itself equals 1).\n\nMultiplication is also defined for other types of numbers, such as [[complex number]]s, and more abstract constructs, like [[matrix (mathematics)|matrices]]. For some of these more abstract constructs, the order in which the operands are multiplied together matters. A listing of the many different kinds of products that are used in mathematics is given in the [[product (mathematics)]] page.\n\n==Notation and terminology==\n{{See also|Multiplier (linguistics)}}\n[[Image:Multiplication Sign.svg|thumb|right|The multiplication sign ×]]\nIn [[arithmetic]], multiplication is often written using the sign \"×\" between the terms; that is, in [[infix notation]].<ref>{{Citation |last=Khan Academy |title=Intro to multiplication {{!}} Multiplication and division {{!}} Arithmetic {{!}} Khan Academy |date=2015-08-14 |url=https://www.youtube.com/watch?v=RNxwasijbAo |accessdate=2017-03-07}}</ref> For example,\n:<math>2\\times 3 = 6</math> (verbally, \"two times three [[equals sign|equals]] six\")\n:<math>3\\times 4 = 12</math>\n:<math>2\\times 3\\times 5 = 6\\times  5 = 30</math>\n:<math>2\\times 2\\times 2\\times 2\\times 2 = 32</math>\n\nThe sign is encoded in Unicode at {{unichar|D7|MULTIPLICATION SIGN|nlink=Multiplication sign|html=}}.\n\nThere are other [[mathematical notation]]s for multiplication:\n* Multiplication is also denoted by [[Interpunct|dot signs]],<ref>{{Citation |last=Khan Academy |title=Why aren't we using the multiplication sign? {{!}} Introduction to algebra {{!}} Algebra I {{!}} Khan Academy |date=2012-09-06 |url=https://www.youtube.com/watch?v=vDaIKB19TvY |accessdate=2017-03-07}}</ref> usually a middle-position dot (rarely [[full stop|period]]):\n:{{math|5 ⋅ 2}} or {{math|5 . 3}}\n:The middle dot notation, encoded in Unicode as {{unichar|22C5|dot operator}}, is standard in the United States, the United Kingdom, and other countries where the period is used as a [[decimal separator|decimal point]]. When the dot operator character is not accessible, the [[interpunct]]&nbsp;(·) is used. In other countries that use a [[Comma (punctuation)|comma]] as a decimal mark, either the period or a middle dot is used for multiplication.{{citation needed|date=August 2011}}\n* {{anchor|Implicit|Explicit}}In [[algebra]], multiplication involving [[Variable (mathematics)|variables]] is often written as a [[wikt:juxtaposition|juxtaposition]] (e.g., ''xy'' for ''x'' times ''y'' or 5''x'' for five times ''x''), also called ''implied multiplication''.<ref>{{cite book |title=Announcing the TI Programmable 88! |publisher=[[Texas Instruments]] |date=1982<!--or 1983--> |url=http://www.datamath.net/Leaflets/TI-88_Announcement.pdf |access-date=2017-08-03 |dead-url=no |archive-url=https://web.archive.org/web/20170803091337/http://www.datamath.net/Leaflets/TI-88_Announcement.pdf |archive-date=2017-08-03}}</ref> The notation can also be used for quantities that are surrounded by [[parentheses]] (e.g., 5(2) or (5)(2) for five times two). This implicit usage of multiplication can cause ambiguity when the concatenated variables happen to match the name of another variable, when a variable name in front of a parenthesis can be confused with a function name, or in the correct determination of the [[order of operations]].\n* In [[vector multiplication]], there is a distinction between the cross and the dot symbols. The cross symbol generally denotes the taking a [[cross product]] of two [[vector (mathematics)|vectors]], yielding a vector as the result, while the dot denotes taking the [[dot product]] of two vectors, resulting in a [[scalar (mathematics)|scalar]].\n\nIn [[computer programming]], the [[asterisk]] (as in <code>5*2</code>) is still the most common notation. This is due to the fact that most computers historically were limited to small [[character set]]s (such as [[ASCII]] and [[EBCDIC]]) that lacked a multiplication sign (such as <code>⋅</code> or <code>×</code>), while the asterisk appeared on every keyboard. This usage originated in the [[Fortran|FORTRAN]] programming language.\n\n{{anchor|Terminology|multiplier}}\nThe numbers to be multiplied are generally called the \"[[factorization|factors]]\". The number to be multiplied is the \"multiplicand\", and the number by which it is multiplied is the \"multiplier\". Usually the multiplier is placed first and the multiplicand is placed second;<ref name=\"Devlin\"/> however sometimes the first factor is the multiplicand and the second the multiplier.<ref>{{cite web |author=Crewton Ramone |url=http://www.crewtonramoneshouseofmath.com/multiplicand-and-multiplier.html |title=Multiplicand and Multiplier |date= |accessdate=10 November 2015 |publisher=Crewton Ramone's House of Math}}.</ref> Also as the result of a multiplication does not depend on the order of the factors, the distinction between \"multiplicand\" and \"multiplier\" is useful only at a very elementary level and in some [[multiplication algorithm]]s, such as the [[long multiplication]]. Therefore, in some sources, the term \"multiplicand\" is regarded as a synonym for \"factor\".<ref>{{cite book |author=Chester Litvin |url=https://books.google.com/?id=-ULmPYAA8voC&pg=PA6&lpg=PA6&dq=Can+the+multiplicand+be+the+first+number#v=onepage&q=multiplicand&f=false |title=Advance Brain Stimulation by Psychoconduction |via=[[Google Book Search]] |date= 2012|accessdate=|isbn=978-1-4669-0152-0 |pages=2–3, 5–6}}</ref> In algebra, a number that is the multiplier of a variable or expression (e.g., the 3 in 3''xy''<sup>2</sup>) is called a [[coefficient]].\n\nThe result of a multiplication is called a [[product (mathematics)|product]]. A product of integers is a [[multiple (mathematics)|multiple]] of each factor. For example, 15 is the product of 3 and 5, and is both a multiple of 3 and a multiple of 5.\n\n==Computation==\n[[file:צעצוע מכני משנת 1918 לחישובי לוח הכפל The Educated Monkey.jpg|200px|right|thumb|The Educated Monkey – a tin toy dated 1918, used as a multiplication “calculator”. <small><small>For example: set the monkey’s feet to 4 and 9, and get the product – 36 – in its hands.</small></small>]]\n\nThe common methods for multiplying numbers using pencil and paper require a [[multiplication table]] of memorized or consulted products of small numbers (typically any two numbers from 0 to 9), however one method, the [[Ancient Egyptian multiplication|peasant multiplication]] algorithm, does not.<!--Many mathematics curricula developed according to the 1989 standards of the [[NCTM]] do not teach standard arithmetic methods, instead guiding students to invent their own methods of computation. Though widely adopted by many school districts in nations such as the United States, they have encountered resistance from some parents and mathematicians, and some districts have since abandoned such curricula in favor of [[traditional mathematics]].-->\n\nMultiplying numbers to more than a couple of decimal places by hand is tedious and error prone. [[Common logarithm]]s were invented to simplify such calculations, since adding logarithms is equivalent to multiplying. The [[slide rule]] allowed numbers to be quickly multiplied to about three places of accuracy. Beginning in the early 20th century, mechanical [[calculator]]s, such as the [[Marchant Calculator|Marchant]], automated multiplication of up to 10&nbsp;digit numbers. Modern electronic [[computer]]s and calculators have greatly reduced the need for multiplication by hand.\n\n===Historical algorithms===\nMethods of multiplication were documented in the [[Ancient Egypt|Egyptian]], [[Ancient Greece|Greek]], [[Ancient India|Indian]] and [[History of China#Ancient China|Chinese]] civilizations.\n\nThe [[Ishango bone]], dated to about 18,000 to 20,000&nbsp;BC, hints at a knowledge of multiplication in the [[Upper Paleolithic]] era in [[Central Africa]].\n\n====Egyptians====\n{{Main|Ancient Egyptian multiplication}}\nThe Egyptian method of multiplication of integers and fractions, documented in the [[Ahmes Papyrus]], was by successive additions and doubling. For instance, to find the product of 13 and 21 one had to double 21 three times, obtaining {{nowrap|1=2 × 21 = 42}}, {{nowrap|1=4 × 21 = 2 × 42 = 84}}, {{nowrap|1=8 × 21 =  2 × 84 = 168}}. The full product could then be found by adding the appropriate terms found in the doubling sequence:\n:13 × 21 = (1 + 4 + 8) × 21 = (1 × 21) + (4 × 21) + (8 × 21) = 21 + 84 + 168 = 273.\n\n====Babylonians====\nThe [[Babylonians]] used a [[sexagesimal]] [[positional number system]], analogous to the modern day [[decimal expansion|decimal system]]. Thus, Babylonian multiplication was very similar to modern decimal multiplication.  Because of the relative difficulty of remembering {{nowrap|60 × 60}} different products, Babylonian mathematicians employed [[multiplication table]]s. These tables consisted of a list of the first twenty multiples of a certain ''principal number'' ''n'': ''n'', 2''n'', ..., 20''n''; followed by the multiples of 10''n'': 30''n'' 40''n'', and 50''n''. Then to compute any sexagesimal product, say 53''n'', one only needed to add 50''n'' and 3''n'' computed from the table.\n\n====Chinese====\n{{seealso|Chinese multiplication table}}\n[[File:Multiplication algorithm.GIF|thumb|right|250px|{{nowrap|1=38 × 76 = 2888}}]]\nIn the mathematical text ''[[Zhoubi Suanjing]]'', dated prior to 300&nbsp;BC, and the ''[[Nine Chapters on the Mathematical Art]]'', multiplication calculations were written out in words, although the early Chinese mathematicians employed [[Rod calculus]] involving place value addition, subtraction, multiplication and division.  Chinese were already using a [[Chinese multiplication table|decimal multiplication table]] since the [[Warring States]] period<ref name=\"Nature\">{{cite journal | url =http://www.nature.com/news/ancient-times-table-hidden-in-chinese-bamboo-strips-1.14482| title =Ancient times table hidden in Chinese bamboo strips | journal =Nature |author=Jane Qiu|date=7 January 2014| accessdate =22 January 2014 | doi =10.1038/nature.2014.14482 }}</ref>.\n\n===Modern methods===\n[[Image:Gelosia multiplication 45 256.png|right|250px|thumb|Product of 45 and 256. Note the order of the numerals in 45 is reversed down the left column. The carry step of the multiplication can be performed at the final stage of the calculation (in bold), returning the final product of {{nowrap|1=45 × 256 = 11520}}. This is a variant of [[Lattice multiplication]].]]\nThe modern method of multiplication based on the [[Hindu–Arabic numeral system]] was first described by [[Brahmagupta]]. Brahmagupta gave rules for addition, subtraction, multiplication and division. [[Henry Burchard Fine]], then professor of Mathematics at [[Princeton University]], wrote the following:\n:''The Indians are the inventors not only of the positional decimal system itself, but of most of the processes involved in elementary reckoning with the system. Addition and subtraction they performed quite as they are performed nowadays; multiplication they effected in many ways, ours among them, but division they did cumbrously.''<ref>{{cite book |last=Fine |first=Henry B. |author-link=Henry Burchard Fine |title=The Number System of Algebra – Treated Theoretically and Historically |edition=2nd |date=1907 |page=90 |url=https://archive.org/download/numbersystemofal00fineuoft/numbersystemofal00fineuoft.pdf}}</ref>\nThese place value decimal arithmetic algorithms were introduced to Arab countries by [[Al Khwarizmi]] in the early 9th&nbsp;century, and popularized in the Western world by [[Fibonacci]] in the 13th century.\n\n====Grid Method====\n[[Grid method multiplication]] or the box method, is used in primary schools in England and Wales & in some areas of the United States to help teach an understanding of how multiple digit multiplication works. An example of multiplying 34 by 13 would be to lay the numbers out in a grid like:\n\n:{|class=\"wikitable\" border=1 cellspacing=0 cellpadding=15 style=\"text-align: center;\"\n! scope=\"col\" width=\"40pt\" | &nbsp;\n! scope=\"col\" width=\"120pt\" | 30\n! scope=\"col\" width=\"40pt\" | 4\n|-\n! scope=\"row\" | 10\n|300\n|40\n|-\n! scope=\"row\" | 3\n|90\n|12\n|}\n\nand then add the entries.\n\n===Computer algorithms===\n{{Main|Multiplication algorithm}}\nThe classical method of multiplying two {{math|''n''}}-digit numbers requires {{math|''n''<sup>2</sup>}} digit multiplications. [[Multiplication algorithm]]s have been designed that reduce the computation time considerably when multiplying large numbers. Methods based on the [[Discrete Fourier transform#Multiplication of large integers|discrete Fourier transform]] reduce the [[computational complexity]] to {{math|''O''(''n'' log ''n'' log log ''n'')}}. Recently, the factor {{math|log log ''n''}} has been replaced by a function that increases much slower although it is still not constant (as it can be hoped).<ref>{{Cite journal|last=Harvey|first=David|last2=van der Hoeven|first2=Joris|last3=Lecerf|first3=Grégoire|title=Even faster integer multiplication|url=http://dx.doi.org/10.1016/j.jco.2016.03.001|year=2016|journal=Journal of Complexity|volume=36|pages=1–30|doi=10.1016/j.jco.2016.03.001|issn=0885-064X|arxiv=1407.3360}}</ref>\n\nIn March, 2019, David Harvey and Joris van der Hoeven submitted an article presenting an integer multiplication algorithm with a claimed complexity of <math>O(n\\log n).</math><ref>David Harvey, Joris Van Der Hoeven (2019). [https://hal.archives-ouvertes.fr/hal-02070778 Integer multiplication in time O(n log n)]</ref>\n\n==Products of measurements==\n{{Main|Dimensional analysis}}\nOne can only meaningfully add or subtract quantities of the same type but can multiply or divide quantities of different types. Four bags with three marbles each can be thought of as:<ref name=\"Devlin\"/>\n:[4 bags] × [3 marbles per bag] = 12 marbles.\n\nWhen two measurements are multiplied together the product is of a type depending on the types of the measurements. The general theory is given by [[dimensional analysis]]. This analysis is routinely applied in physics but has also found applications in finance.\n\nA common example is multiplying speed by time gives distance, so\n:50 kilometers per hour × 3 hours = 150 kilometers.\nIn this case, the hour units cancel out and we are left with only kilometer units.\n\nOther examples:\n:2.5 meters × 4.5 meters = 11.25 square meters\n:11 meters/seconds × 9 seconds = 99 meters\n:4.5 residents per house × 20 houses = 90 residents\n\n==Products of sequences==<!--linked from below-->\n===Capital Pi notation===<!--This section is linked from [[Pi (letter)]] and [[Capital Pi notation]]-->\nThe product of a sequence of terms can be written with the product symbol, which derives from the capital letter Π (Pi) in the [[Greek alphabet]]. Unicode position U+220F (∏) contains a glyph for denoting such a product, distinct from U+03A0 (Π), the letter. The meaning of this notation is given by:\n:<math>\\prod_{i=1}^4 i = 1\\cdot 2\\cdot 3\\cdot 4,</math>\nthat is\n:<math>\\prod_{i=1}^4 i = 24.</math>\n\nThe subscript gives the symbol for a [[free variables and bound variables|dummy variable]] (''i'' in this case), called the \"index of multiplication\" together with its lower bound (''1''), whereas the superscript (here ''4'') gives its upper bound. The lower and upper bound are expressions denoting integers. The factors of the product are obtained by taking the expression following the product operator, with successive integer values substituted for the index of multiplication, starting from the lower bound and incremented by 1 up to and including the upper bound. So, for example:\n:<math>\\prod_{i=1}^6 i = 1\\cdot 2\\cdot 3\\cdot 4\\cdot 5 \\cdot 6 = 720</math>\n\nMore generally, the notation is defined as\n:<math>\\prod_{i=m}^n x_i = x_m \\cdot x_{m+1} \\cdot x_{m+2} \\cdot \\,\\,\\cdots\\,\\, \\cdot x_{n-1} \\cdot x_n,</math>\nwhere ''m'' and ''n'' are integers or expressions that evaluate to integers. In case {{nowrap|1=''m'' = ''n''}}, the value of the product is the same as that of the single factor ''x''<sub>''m''</sub>. If {{nowrap|''m'' > ''n''}}, the product is the [[empty product]], with the value&nbsp;1.\n\n===Infinite products===\n{{Main|Infinite product}}\nOne may also consider products of infinitely many terms; these are called [[infinite product]]s. Notationally, we would replace ''n'' above by the [[lemniscate]] ∞. The product of such a series is defined as the [[limit of a sequence|limit]] of the product of the first ''n'' terms, as ''n'' grows without bound. That is, by definition,\n:<math>\\prod_{i=m}^\\infty x_i = \\lim_{n\\to\\infty} \\prod_{i=m}^n x_i.</math>\n\nOne can similarly replace ''m'' with negative infinity, and define:\n:<math>\\prod_{i=-\\infty}^\\infty x_i = \\left(\\lim_{m\\to-\\infty}\\prod_{i=m}^0 x_i\\right) \\cdot \\left(\\lim_{n\\to\\infty} \\prod_{i=1}^n x_i\\right),</math>\nprovided both limits exist.\n\n==Properties==\n[[Image:Multiplication chart.svg|thumb|right|Multiplication of numbers 0–10. Line labels = multiplicand. X axis = multiplier. Y axis = product.<br>Extension of this pattern into other quadrants gives the reason why a negative number times a negative number yields a positive number.<br>Note also how multiplication by zero causes a reduction in dimensionality, as does multiplication by a [[singular matrix]] where the [[determinant]] is 0. In this process, information is lost and cannot be regained.]]\nFor the [[real number|real]] and [[complex number|complex]] numbers, which includes for example [[natural number]]s, [[integer]]s, and [[rational number|fractions]], multiplication has certain properties:\n\n;[[Commutative property]]\n:The order in which two numbers are multiplied does not matter:\n::<math>x\\cdot y = y\\cdot x.</math>\n\n;[[Associative property]]\n:Expressions solely involving multiplication or addition are invariant with respect to [[order of operations]]:\n::<math>(x\\cdot y)\\cdot z = x\\cdot(y\\cdot z)</math>\n\n;[[Distributive property]]\n:Holds with respect to multiplication over addition. This identity is of prime importance in simplifying algebraic expressions:\n::<math>x\\cdot(y + z) = x\\cdot y + x\\cdot z </math>\n\n;[[Identity element]]\n:The multiplicative identity is 1; anything multiplied by 1 is itself. This feature of 1 is known as the '''identity property''':\n::<math>x\\cdot 1 = x</math>\n\n;[[Absorbing element|Property of 0]]\n:Any number multiplied by 0 is 0. This is known as the '''zero property''' of multiplication:\n::<math>x\\cdot 0 = 0</math>\n\n;[[Additive inverse|Negation]]\n:−1 times any number is equal to the '''[[additive inverse]]''' of that number.\n::<math>(-1)\\cdot x = (-x)</math> where <math>(-x)+x=0</math>\n\n:–1 times –1 is 1.\n::<math>(-1)\\cdot (-1) = 1</math>\n\n;[[Inverse element]]\n:Every number ''x'', [[division by zero|except 0]], has a '''[[multiplicative inverse]]''', <math>\\frac{1}{x}</math>, such that <math>x\\cdot\\left(\\frac{1}{x}\\right) = 1</math>.\n\n;[[Order theory|Order]] preservation\n:Multiplication by a positive number preserves [[Order theory|order]]:\n::For {{nowrap|''a'' > 0}}, if {{nowrap|''b'' > ''c''}} then {{nowrap|''ab'' > ''ac''}}.\n:Multiplication by a negative number reverses order:\n::For {{nowrap|''a'' < 0}}, if {{nowrap|''b'' > ''c''}} then {{nowrap|''ab'' < ''ac''}}.\n:The [[complex number]]s do not have an ordering.\n\nOther mathematical systems that include a multiplication operation may not have all these properties. For example, multiplication is not, in general, commutative for [[Matrix (mathematics)|matrices]] and [[quaternion]]s.\n\n==Axioms==\n{{Main|Peano axioms}}\nIn the book ''[[Arithmetices principia, nova methodo exposita]]'', [[Giuseppe Peano]] proposed axioms for arithmetic based on his axioms for natural numbers.<ref>{{cite web |url=http://planetmath.org/encyclopedia/PeanoArithmetic.html |title=Peano arithmetic |publisher=[[PlanetMath]]}}</ref> Peano arithmetic has two axioms for multiplication:\n:<math>x \\times 0 = 0</math>\n:<math>x \\times S(y) = (x \\times y) + x</math>\n\nHere ''S''(''y'') represents the [[Successor ordinal|successor]] of ''y'', or the natural number that ''follows'' ''y''. The various properties like associativity can be proved from these and the other axioms of Peano arithmetic including [[Mathematical induction|induction]]. For instance ''S''(0), denoted by 1, is a multiplicative identity because\n:<math>x \\times 1 = x \\times S(0) = (x \\times 0) + x = 0 + x = x</math>\n\nThe axioms for [[integer]]s typically define them as equivalence classes of ordered pairs of natural numbers. The model is based on treating (''x'',''y'') as equivalent to {{nowrap|''x'' − ''y''}} when ''x'' and ''y'' are treated as integers. Thus both (0,1) and (1,2) are equivalent to −1. The multiplication axiom for integers defined this way is\n:<math>(x_p,\\, x_m) \\times (y_p,\\, y_m) = (x_p \\times y_p + x_m \\times y_m,\\; x_p \\times y_m + x_m \\times y_p)</math>\n\nThe rule that −1 × −1 = 1 can then be deduced from\n:<math>(0, 1) \\times (0, 1) = (0 \\times 0 + 1 \\times 1,\\, 0 \\times 1 + 1 \\times 0) = (1,0)</math>\n\nMultiplication is extended in a similar way to [[rational number]]s and then to [[real number]]s.\n\n==Multiplication with set theory==\nThe product of non-negative integers can be defined with set theory using [[Cardinal number#Cardinal multiplication|cardinal numbers]] or the [[Peano axioms#Arithmetic|Peano axioms]]. See [[#Multiplication of different kinds of numbers|below]] how to extend this to multiplying arbitrary integers, and then arbitrary rational numbers. The product of real numbers is defined in terms of products of rational numbers, see [[construction of the real numbers]].\n\n==Multiplication in group theory==<!--linked from below-->\nThere are many sets that, under the operation of multiplication, satisfy the axioms that define [[group (mathematics)|group]] structure. These axioms are closure, associativity, and the inclusion of an identity element and inverses.\n\nA simple example is the set of non-zero [[rational numbers]]. Here we have identity 1, as opposed to groups under addition where the identity is typically 0. Note that with the rationals, we must exclude zero because, under multiplication, it does not have an inverse: there is no rational number that can be multiplied by zero to result in 1. In this example we have an [[abelian group]], but that is not always the case.\n\nTo see this, look at the set of invertible square matrices of a given dimension, over a given [[field (mathematics)|field]]. Now it is straightforward to verify closure, associativity, and inclusion of identity (the [[identity matrix]]) and inverses. However, matrix multiplication is not commutative, therefore this group is nonabelian.\n\nAnother fact of note is that the integers under multiplication is not a group, even if we exclude zero. This is easily seen by the nonexistence of an inverse for all elements other than 1 and&nbsp;−1.\n\nMultiplication in group theory is typically notated either by a dot, or by juxtaposition (the omission of an operation symbol between elements). So multiplying element '''a''' by element '''b''' could be notated '''a''' <math>\\cdot</math> '''b''' or '''ab'''.  When referring to a group via the indication of the set and operation, the dot is used, e.g., our first example could be indicated by <math>\\left( \\mathbb{Q}\\smallsetminus \\{ 0 \\} ,\\cdot \\right)</math>\n\n==Multiplication of different kinds of numbers==<!--linked from above-->\nNumbers can ''count'' (3&nbsp;apples), ''order'' (the 3rd&nbsp;apple), or ''measure'' (3.5&nbsp;feet high); as the history of mathematics has progressed from counting on our fingers to modelling quantum mechanics, multiplication has been generalized to more complicated and abstract types of numbers, and to things that are not numbers (such as [[Matrix (mathematics)|matrices]]) or do not look much like numbers (such as [[quaternion]]s).\n\n;Integers\n:<math>N\\times M</math> is the sum of ''N'' copies of ''M'' when ''N'' and ''M'' are positive whole numbers. This gives the number of things in an array ''N'' wide and ''M'' high. Generalization to negative numbers can be done by\n:<math>N\\times (-M) = (-N)\\times M = - (N\\times M)</math> and\n:<math>(-N)\\times (-M) = N\\times M</math>\n:The same sign rules apply to rational and real numbers.\n\n;[[Rational number]]s\n:Generalization to fractions <math>\\frac{A}{B}\\times \\frac{C}{D}</math> is by multiplying the numerators and denominators respectively: <math>\\frac{A}{B}\\times \\frac{C}{D} = \\frac{(A\\times C)}{(B\\times D)}</math>. This gives the area of a rectangle <math>\\frac{A}{B}</math> high and <math>\\frac{C}{D}</math> wide, and is the same as the number of things in an array when the rational numbers happen to be whole numbers.\n\n;[[Real number]]s\n:Real numbers and their products [[Construction of the real numbers#Construction from Cauchy sequences|can be defined in terms of sequences of rational numbers]].\n\n;[[Complex number]]s\n:Considering complex numbers <math>z_1</math> and <math>z_2</math> as ordered pairs of real numbers <math>(a_1, b_1)</math> and <math>(a_2, b_2)</math>, the product <math>z_1\\times z_2</math> is <math>(a_1\\times a_2 - b_1\\times b_2, a_1\\times b_2 + a_2\\times b_1)</math>. This is the same as for reals, <math>a_1\\times a_2</math>, when the ''imaginary parts'' <math>b_1</math> and <math>b_2</math> are zero.\n\n:Equivalently, denoting <math>\\sqrt{-1}</math> as <math>i</math>, we have <math>z_1 \\times z_2 = (a_1+b_1i)(a_2+b_2i)=(a_1 \\times a_2)+(a_1\\times b_2i)+(b_1\\times a_2i)+(b_1\\times b_2i^2)=(a_1a_2-b_1b_2)+(a_1b_2+b_1a_2)i.</math>\n\n;Further generalizations\n:See [[Multiplication#Multiplication in group theory|Multiplication in group theory]], above, and [[Multiplicative group]], which for example includes matrix multiplication. A very general, and abstract, concept of multiplication is as the \"multiplicatively denoted\" (second) binary operation in a [[Ring (mathematics)|ring]]. An example of a ring that is not any of the above number systems is a [[polynomial ring]] (you can add and multiply polynomials, but polynomials are not numbers in any usual sense.)\n\n;Division\n:Often division, <math>\\frac{x}{y}</math>, is the same as multiplication by an inverse, <math>x\\left(\\frac{1}{y}\\right)</math>. Multiplication for some types of \"numbers\" may have corresponding division, without inverses; in an [[integral domain]] ''x'' may have no inverse \"<math>\\frac{1}{x}</math>\" but <math>\\frac{x}{y}</math> may be defined. In a [[division ring]] there are inverses, but <math>\\frac{x}{y}</math> may be ambiguous in non-commutative rings since <math>x\\left(\\frac{1}{y}\\right)</math> need not be the same as <math>\\left(\\frac{1}{y}\\right)x</math>.\n\n==Exponentiation==\n{{Main|Exponentiation}}\nWhen multiplication is repeated, the resulting operation is known as '''exponentiation'''. For instance, the product of three factors of two (2×2×2) is \"two raised to the third power\", and is denoted by 2<sup>3</sup>, a two with a [[superscript]] three. In this example, the number two is the '''base''', and three is the '''exponent'''. In general, the exponent (or superscript) indicates how many times the base appears in the expression, so that the expression\n:<math>a^n = \\underbrace{a\\times a \\times \\cdots  \\times a}_n</math>\n\nindicates that ''n'' copies of the base ''a'' are to be multiplied together. This notation can be used whenever multiplication is known to be [[Power associativity|power associative]].\n\n==See also==\n{{col-begin}}\n{{col-break|width=33%}}\n* [[Dimensional analysis]]\n* [[Multiplication algorithm]]\n** [[Karatsuba algorithm]], for large numbers\n** [[Toom–Cook multiplication]], for very large numbers\n** [[Schönhage–Strassen algorithm]], for huge numbers\n{{col-break|width=33%}}\n* [[Multiplication table]]\n* [[Binary multiplier]], how computers multiply\n** [[Booth's multiplication algorithm]]\n** [[Floating point]]\n** [[Fused multiply–add]]\n** [[Multiply–accumulate]]\n** [[Wallace tree]]\n{{col-break}}\n* [[Multiplicative inverse]], reciprocal\n* [[Factorial]]\n* [[Genaille–Lucas rulers]]\n* [[Napier's bones]]\n* [[Peasant multiplication]]\n* [[Product (mathematics)]], for generalizations\n* [[Slide rule]]\n* [[Multiplicative calculus]]\n{{col-end}}\n\n==Notes==\n{{Reflist}}\n\n==References==\n* {{cite book |author=[[Carl Boyer|Boyer, Carl B.]] (revised by [[Uta Merzbach|Merzbach, Uta C.]]) |title=History of Mathematics |publisher=John Wiley and Sons, Inc. |year=1991 |isbn=978-0-471-54397-8}}\n\n==External links==\n* [http://www.cut-the-knot.org/do_you_know/multiplication.shtml Multiplication] and [http://www.cut-the-knot.org/blue/SysTable.shtml Arithmetic Operations In Various Number Systems] at [[cut-the-knot]]\n* [http://webhome.idirect.com/~totton/suanpan/mod_mult/ Modern Chinese Multiplication Techniques on an Abacus]\n\n{{Elementary arithmetic}}\n{{Hyperoperations}}\n{{Authority control}}\n\n[[Category:Elementary arithmetic]]\n[[Category:Binary operations]]\n[[Category:Mathematical notation]]\n[[Category:Articles containing proofs]]\n[[Category:Multiplication| ]]"
    },
    {
      "title": "Negacyclic convolution",
      "url": "https://en.wikipedia.org/wiki/Negacyclic_convolution",
      "text": "In mathematics, '''negacyclic convolution''' is a [[convolution]] between two vectors ''a'' and ''b''.\n\nIt is also called skew [[circular convolution]] or wrapped convolution. It results from multiplication of a [[skew circulant matrix]], generated by vector ''a'', with vector ''b''.\n\n== See also ==\n*[[Discrete_Fourier_transform#Circular_convolution_theorem_and_cross-correlation_theorem|Circular convolution theorem]]\n\n[[Category:Bilinear operators]]\n[[Category:Binary operations]]\n[[Category:Functional analysis]]\n[[Category:Image processing]]\n\n\n{{Mathanalysis-stub}}"
    },
    {
      "title": "Null coalescing operator",
      "url": "https://en.wikipedia.org/wiki/Null_coalescing_operator",
      "text": "The '''null coalescing operator''' (called the '''Logical Defined-Or operator''' in [[Perl]]) is a [[binary operator]] that is part of the syntax for a basic [[Conditional (programming)|conditional expression]] in several [[programming language]]s, including [[C Sharp (programming language)|C#]],<ref>{{cite web|url=http://msdn.microsoft.com/en-us/library/ms173224.aspx|title=?? Operator (C# Reference)|first=|last=BillWagner|website=msdn.microsoft.com}}</ref> [[Perl]] as of version 5.10,<ref>{{cite web|url=http://perldoc.perl.org/perlop.html#C-style-Logical-Defined-Or|title=perlop - perldoc.perl.org|website=perldoc.perl.org}}</ref> [[Swift (programming language)|Swift]],<ref>{{cite web|url=https://developer.apple.com/library/prerelease/ios/documentation/Swift/Conceptual/Swift_Programming_Language/BasicOperators.html#//apple_ref/doc/uid/TP40014097-CH6-XID_124|title=The Swift Programming Language (Swift 4): Basic Operators|website=developer.apple.com}}</ref> and [[PHP]] 7.0.0.<ref>{{cite web|url=http://php.net/archive/2015.php#id2015-11-12-1|title=PHP: News Archive - 2015|website=php.net}}</ref> While its behavior differs between implementations, the null coalescing operator generally returns the result of its left-most operand if it exists and is not [[Nullable type|null]], and otherwise returns the right-most operand.  This behavior allows a default value to be defined for cases where a more specific value is not available.\n\nIn contrast to the ternary [[?:|conditional if]] operator used as <code> x ? x : y</code>, but like the binary [[Elvis operator]] used as <code>x ?: y</code>, the null coalescing operator is a binary operator and thus evaluates its operands at most once, which is significant if the evaluation of <code> x </code> has [[Side effect (computer science)|side-effects]].\n\n== Examples by languages ==\n\n=== Bash ===\n\nIn [[Bash (Unix shell)|Bash]] \"If ''parameter'' is unset or null, the expansion of ''word'' is substituted. Otherwise, the value of ''parameter'' is substituted\":<ref>{{cite web|url=https://linux.die.net/man/1/bash|title=Bash man page|last=|first=|date=|website=|publisher=}}</ref><syntaxhighlight lang=\"bash\">\n#supplied_title='supplied title' # uncomment this line to use the supplied title\ntitle=${supplied_title:-'Default title'}\necho \"$title\" # prints: Default title\n\n</syntaxhighlight>\n\n=== C# ===\n\nIn [[C Sharp (programming language)|C#]], the null coalescing operator is <code>??</code>.\n\nIt is most often used to simplify expressions as follows:\n\n<source lang=\"csharp\">\npossiblyNullValue ?? valueIfNull\n</source>\n\nFor example, if one wishes to implement some C# code to give a page a default title if none is present, one may use the following statement:\n\n<source lang=\"csharp\">string pageTitle = suppliedTitle ?? \"Default Title\";</source>\n\ninstead of the more verbose\n<source lang=\"csharp\">\nstring pageTitle = (suppliedTitle != null) ? suppliedTitle : \"Default Title\";\n</source>\nor\n<source lang=\"csharp\">\nstring pageTitle;\n\nif (suppliedTitle != null)\n{\n    pageTitle = suppliedTitle;\n}\nelse\n{\n    pageTitle = \"Default Title\";\n}\n</source>\n\nThe three forms result in the same value being stored into the variable named <code>pageTitle</code>.\n\nNote that <code>suppliedTitle</code> is referenced only once when using the <code>??</code> operator, and twice in the other two code examples.\n\nThe operator can also be used multiple times in the same expression: \n<source lang=\"csharp\">\nreturn some_Value ?? some_Value2 ?? some_Value3; \n</source> \nOnce a non-null value is assigned to number, or it reaches the final value (which may or may not be null), the expression is completed.\n\n=== CFML ===\n\nAs of [[ColdFusion]] 11,<ref>{{cite web|url=https://wikidocs.adobe.com/wiki/display/coldfusionen/Elvis+operator|title=Elvis operator|website=wikidocs.adobe.com}}</ref> [[Railo]] 4.1,<ref>{{cite web|url=https://issues.jboss.org/browse/RAILO-2195|title=[RAILO-2195] add support for the Elvis Operator - JBoss Issue Tracker|website=issues.jboss.org}}</ref> [[CFML]] supports the null coalescing operator as a variation of the ternary operator, <code>?:</code>. It is functionally and syntactically equivalent to its C# counterpart, above. Example:\n\n<source lang=\"cfm\">\npossiblyNullValue ?: valueIfNull\n</source>\n\n=== Clojure ===\n\nClojure's <code>or</code> macro can be used similarly, because it returns the first non false value, and nil is considered false in Clojure. The inclusion of false makes it slightly different to traditional Elvis operators.\n<source lang=\"clojure\">\n(or page-title \"Default title\")\n</source>\n\nYou can also chain values.\n\n<source lang=\"clojure\">\n(or x y z) ;; returns first not-false value or nil\n</source>\n\nBe careful if you care about distinguishing between false and nil in this case, since <code>or</code> does not.\n\n=== F# ===\n\nThe null value is not normally used in [[F Sharp (programming language)|F#]] for values or variables.<ref>{{cite web|url=http://msdn.microsoft.com/en-us/library/dd233197.aspx|title=Null Values (F#)|first=|last=cartermp|website=msdn.microsoft.com}}</ref> However null values can appear for example when F# code is called from C#.\n\nF# does not have a built-in null coalescing operator but one can be defined as required as a custom operator:<ref>{{cite web|url=http://msdn.microsoft.com/en-us/library/dd233204.aspx|title=Operator Overloading (F#)|first=|last=cartermp|website=msdn.microsoft.com}}</ref>\n\n<source lang=\"fsharp\">\nlet (|?) lhs rhs = (if lhs = null then rhs else lhs)\n</source>\n\nThis custom operator can then be applied as per C#'s built-in null coalescing operator:\n\n<source lang=\"fsharp\">\nlet pageTitle = suppliedTitle |? \"Default Title\"\n</source>\n\n=== Freemarker ===\n\nMissing values will normally cause exceptions. However, both missing and null values can be handled, with an optional default value:<ref>{{cite web|url=http://freemarker.org/docs/dgui_template_exp.html#dgui_template_exp_missing|title=Expressions|website=Apache FreeMarker Manual}}</ref>\n\n<source lang=\"sh\">\n${missingVariable!\"defaultValue\"}\n</source>\nor, to leave the output blank:\n<source lang=\"sh\">\n${missingVariable!}\n</source>\n\n=== Haskell ===\n\nTypes in [[Haskell (programming language)|Haskell]] can in general not be null. Representation of computations that may or may not return a meaningful result is represented by the generic Maybe type, defined in the standard library<ref>{{cite web|url=http://hackage.haskell.org/package/base-4.8.0.0/docs/Prelude.html#t:Maybe|title=Hackage|accessdate=10 July 2015}}</ref> as\n<source lang=\"haskell\">\ndata Maybe a = Nothing | Just a\n</source>\n\nThe null coalescing operator replaces null pointers with a default value. The Haskell equivalent is a way of extracting a value from a Maybe by supplying a default value. This is the function fromMaybe.\n\n<source lang=\"haskell\">\nfromMaybe     :: a -> Maybe a -> a\nfromMaybe d x = case x of {Nothing -> d;Just v  -> v}\n</source>\n\nSome example usage follows.\n\n<source lang=\"haskell\">\nfromMaybe 0 (Just 3) -- returns 3\nfromMaybe \"\" (Nothing) -- returns \"\"\n</source>\n\n=== Kotlin ===\n\n[[Kotlin (programming language)|Kotlin]] uses the <code>?:</code> operator.<ref name=\"Null Safety\">{{cite web|url=http://confluence.jetbrains.com/display/Kotlin/Null-safety|title=Null safety}}.</ref> This is an unusual choice of symbol, given that ?: is typically used for the [[Elvis operator]], not null coalescing, but it was inspired by [[Groovy (programming language)]] where null is considered false.\n<syntaxhighlight lang=\"scala\">\nval title = suppliedTitle ?: \"Default title\"\n</syntaxhighlight>\n\n=== Objective-C ===\n\nIn [[Objective-C (programming language)|Obj-C]], the nil coalescing operator is <code>?:</code>. It can be used to provide a default for nil references:\n\n<source lang=\"objc\">\nid value = valueThatMightBeNil ?: valueIfNil;\n</source>\n\nThis is the same as writing\n<source lang=\"objc\">\nid value = valueThatMightBeNil ? valueThatMightBeNil : valueIfNil;\n</source>\n\n=== Perl ===\n\nIn [[Perl]] (starting with version 5.10), the operator is <code>//</code> and the equivalent Perl code is:\n\n<source lang=\"perl\">\n$possibly_null_value // $value_if_null\n</source>\n\nThe ''possibly_null_value'' is evaluated as ''null'' or ''not-null'' (in Perl terminology, ''undefined'' or ''defined'').  On the basis of the evaluation, the expression returns either ''value_if_null'' when ''possibly_null_value'' is null, or ''possibly_null_value'' otherwise. In the absence of [[Side effect (computer science)|side-effects]] this is similar to the way [[ternary operators]] ('''<code>[[?:]]</code>''' statements) work in languages that support them.  The above Perl code is equivalent to the use of the ternary operator below:\n<source lang=\"perl\">\ndefined($possibly_null_value) ? $possibly_null_value : $value_if_null\n</source>\nThis operator's most common usage is to minimize the amount of code used for a simple null check.\n\nPerl additionally has a <code>//=</code> assignment operator, where <source lang=\"perl\">$a //= $b</source> is largely equivalent to: <source lang=\"perl\">$a = $a // $b</source>\n\nThis operator differs from Perl's older <code>||</code> and <code>||=</code> operators in that it considers ''definedness,'' not ''truth.'' Thus they behave differently on values that are false but defined, such as 0 or &apos;&apos; (a zero-length string):\n<source lang=\"perl\">\n$a = 0;\n$b = 1;\n$c = $a // $b;  # $c = 0\n$c = $a || $b;  # $c = 1\n</source>\n\n=== PHP ===\n\nPHP 7 has introduced<ref>{{cite web|url=https://wiki.php.net/rfc/isset_ternary|title=PHP: rfc:isset_ternary|publisher=|accessdate=16 December 2014}}</ref> a null-coalescing operator with the <code>??</code> syntax. This checks strictly for NULL or a non-existent variable/array index/property. In this respect, it acts similarly to PHP's <code>isset()</code> pseudo-function:\n\n<source lang=\"php\">\n$name = $request->input['name'] ?? $request->query['name'] ?? 'default name';\n\n/* Equivalent to */\n\nif (isset($request->input['name'])) {\n    $name = $request->input['name'];\n} elseif (isset($request->query['name'])) {\n    $name = $request->query['name'];\n} else {\n    $name = 'default name';\n}\n</source><syntaxhighlight lang=\"php\">\n$user = $this->getUser() ?? $this->createGuestUser();\n\n/* Equivalent to */\n\n$user = $this->getUser();\n\nif (null === $user) {\n    $user = $this->createGuestUser();\n} \n</syntaxhighlight><source lang=\"php\">\n$pageTitle = $title ?? 'Default Title';\n\n/* Equivalent to */\n\n$pageTitle = isset($title) ? $title : 'Default Title';\n</source>\n\nVersion 7.4 of PHP will add the Null Coalescing Assignment Operator with the <code>??=</code> syntax:<ref>{{cite web|last1=Kocak|first1=Midori|title=PHP RFC: Null Coalescing Assignment Operator|url=https://wiki.php.net/rfc/null_coalesce_equal_operator|website=PHP.net|accessdate=20 July 2017}}</ref>\n\n<source lang=\"PHP\">\n// The following lines are doing the same\n$this->request->data['comments']['user_id'] = $this->request->data['comments']['user_id'] ?? 'value';\n// Instead of repeating variables with long names, the equal coalesce operator is used\n$this->request->data['comments']['user_id'] ??= 'value';\n</source>\n\n=== Python ===\n\nThe '''or''' operator provides this functionality. <ref>{{cite web|url=https://stackoverflow.com/questions/4978738/is-there-a-python-equivalent-of-the-c-sharp-null-coalescing-operator|title=Is there a Python equivalent of the C sharp null-coalescing operator|first=|last=|website=stackoverflow.com}}</ref>\n\n<source lang=\"python\">\nother = s or \"some default value\"\n</source>\n\nNote that the '''or''' operator does not return only '''True''' or '''False'''. Instead, it returns the first operand if the first operand evaluates to true, and it returns the second operand if the first operand evaluates to false.\n\nIn this case, the expression '''x''' or '''y''' returns '''x''' if it is '''True''' or evaluates to true when converted to boolean. Otherwise, it returns '''y'''. For most cases, this will serve for the very same purpose of C♯'s null-coalescing operator, but keep in mind:\n\n<source lang=\"python\">\n\n42    or \"something\"    # returns 42\n0     or \"something\"    # returns \"something\"\nNone  or \"something\"    # returns \"something\"\nFalse or \"something\"    # returns \"something\"\n\"\"    or \"something\"    # returns \"something\"\n\n</source>\n\nThere is a proposal to add a new operator or operators to handle this differently in Python 3.8.<ref>https://www.python.org/dev/peps/pep-0505/</ref>\n\n=== Scheme ===\n\nIn [[Scheme (programming language)|Scheme]], \"boolean false\" and \"null\" are represented by the same value, written as <tt>#f</tt>. Furthermore, <tt>#f</tt> is the only value in Scheme that is treated as false by boolean operators, unlike some other languages, in which values like 0, the empty string, and the empty list may function as false. The boolean 'or' operation, written as <tt>(or x y)</tt>, returns <tt>x</tt> if it is not false,\notherwise it returns <tt>y</tt>.  Thus, in Scheme, there is no need for a separate \"null coalescing operator\", because the <tt>or</tt> operator serves that purpose.\n\n=== SQL ===\n\nIn Oracle's [[PL/SQL]], the [[NVL]]() function provides the same outcome:\n<syntaxhighlight lang=\"sql\">\nNVL(possibly_null_value, 'value if null');\n</syntaxhighlight>\n\nIn [[Microsoft SQL Server|SQL Server]]/[[Transact-SQL]] there is the ISNULL function that follows the same prototype pattern:\n<syntaxhighlight lang=\"tsql\">\nISNULL(possibly_null_value, 'value if null');\n</syntaxhighlight>\nAttention should be taken to not confuse ''ISNULL'' with ''IS NULL'' – the latter serves to evaluate whether some contents are defined to be ''NULL'' or not.\n\nThe ANSI SQL-92 standard includes the COALESCE function implemented in [[Oracle Database|Oracle]],<ref>{{cite web|url=http://docs.oracle.com/cd/B28359_01/server.111/b28286/functions023.htm#SQLRF00617|title=Database SQL Language Reference|website=docs.oracle.com}}</ref> [[Microsoft SQL Server|SQL Server]],<ref>{{cite web|url=https://technet.microsoft.com/en-us/library/ms174075.aspx|title=COALESCE (SQL Server Compact)|website=technet.microsoft.com}}</ref> [[PostgreSQL]],<ref>{{cite web|url=http://www.postgresql.org/docs/9.1/static/functions-conditional.html#FUNCTIONS-COALESCE-NVL-IFNULL|title=PostgreSQL: Documentation: 9.1: Conditional Expressions|website=www.postgresql.org}}</ref> [[SQLite]]<ref>{{cite web|url=http://www.sqlite.org/lang_corefunc.html|title=SQLite Query Language: Core Functions|website=www.sqlite.org}}</ref>  and [[MySQL]].<ref>{{cite web|url=http://dev.mysql.com/doc/refman/5.5/en/comparison-operators.html#function_coalesce|title=MySQL :: MySQL 5.5 Reference Manual :: 12.3.2 Comparison Functions and Operators|website=dev.mysql.com}}</ref> The COALESCE function returns the first argument that is not null. If all terms are null, returns null.\n<syntaxhighlight lang=\"postgresql\">\nCOALESCE(possibly_null_value[, possibly_null_value, ...]);\n</syntaxhighlight>\n\n=== Swift ===\n\nIn [[Swift (programming language)|Swift]], the nil coalescing operator is <code>??</code>. It is used to provide a default when unwrapping an [[Option type|optional type]]:\n\n<source lang=\"swift\">\noptionalValue ?? valueIfNil\n</source>\n\nFor example, if one wishes to implement some Swift code to give a page a default title if none is present, one may use the following statement:\n\n<source lang=\"swift\">\nvar suppliedTitle: String? = ...\nvar pageTitle: String = suppliedTitle ?? \"Default Title\"\n</source>\n\ninstead of the more verbose\n<source lang=\"swift\">\nvar pageTitle: String = (suppliedTitle != nil) ? suppliedTitle! : \"Default Title\";\n</source>\n\n=== VB.NET ===\n\nIn [[Visual Basic .NET|VB.NET]] the <code>If</code><ref>{{cite web|url=https://docs.microsoft.com/en-us/dotnet/visual-basic/language-reference/operators/if-operator|title=If Operator (Visual Basic)|first=|last=dotnet-bot|website=docs.microsoft.com}}</ref> operator/keyword achieves the null coalescing operator effect.\n\n<syntaxhighlight lang=\"vbnet\">\nDim pageTitle = If(suppliedTitle, \"Default Title\")\n</syntaxhighlight>\n\nwhich is a more concise way of using its variation\n\n<syntaxhighlight lang=\"vbnet\">\nDim pageTitle = If(suppliedTitle <> Nothing, suppliedTitle, \"Default Title\")\n</syntaxhighlight>\n\n== See also ==\n\n*[[?:]] ([[conditional operator|conditional]])\n*[[Elvis operator]] (binary ?:)\n*[[Operator (programming)]]\n\n== References ==\n\n<references/>\n\n[[Category:Conditional constructs]]\n[[Category:Operators (programming)]]\n[[Category:Binary operations]]"
    },
    {
      "title": "Outer product",
      "url": "https://en.wikipedia.org/wiki/Outer_product",
      "text": "{{distinguish|text=[[Exterior product]]}}\nIn [[linear algebra]], the '''outer product''' of two [[coordinate vector]]s is a [[matrix (mathematics)|matrix]]. If the two vectors have dimensions ''n'' and ''m'', then their outer product is an ''n'' × ''m'' matrix. \nMore generally, given two [[tensors]] (multidimensional arrays of numbers), their outer product is a tensor. The outer product of tensors is also referred to as their [[tensor product]]\nand can be used to define the [[tensor algebra]].\n\nThe outer product contrasts with\n* the [[dot product]], which takes as input a pair of coordinate vectors and produces a [[scalar (mathematics)|scalar]].\n* the [[Kronecker product]], which takes as input a pair of matrices and produces a matrix\n* and [[matrix multiplication]].\n\n==Definition==\n\nGiven two vectors\n\n:<math>\\mathbf{u} =(u_1, u_2, \\dots, u_m)</math>\n:<math>\\mathbf{v} = (v_1, v_2, \\dots, v_n)</math>\n\ntheir outer product {{nowrap|'''u''' ⊗ '''v'''}} is defined as the {{nowrap|''m'' × ''n''}} matrix '''A''' obtained by multiplying each element of '''u''' by each element of '''v''':<ref>{{cite web |title=Kronecker Product |work=Wolfram MathWorld |url=http://mathworld.wolfram.com/KroneckerProduct.html }}</ref><ref>{{cite book |title=Encyclopaedia of Physics |edition=2nd |first=R. G. |last=Lerner |first2=G. L. |last2=Trigg |publisher=VHC |year=1991 |isbn=0-89573-752-3 }}</ref>\n\n:<math>\\mathbf{u} \\otimes \\mathbf{v} = \\mathbf{A} = \n\\begin{bmatrix}u_1v_1 & u_1v_2 & \\dots & u_1v_n \\\\ u_2v_1 & u_2v_2 & \\dots & u_2v_n \\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ u_mv_1 & u_mv_2 & \\dots & u_mv_n \\end{bmatrix}</math>\n\nOr in index notation:\n\n:<math>(\\mathbf{u} \\otimes \\mathbf{v})_{ij}=u_iv_j</math>\n\nThe outer product {{nowrap|'''u''' ⊗ '''v'''}} is equivalent to a [[matrix multiplication]] '''uv'''<sup>T</sup>, provided that '''u''' is represented as a {{nowrap|''m'' × 1}} [[column vector]] and '''v''' as a {{nowrap|''n'' × 1}} column vector (which makes '''v'''<sup>T</sup> a row vector).<ref>{{cite book |title=Linear Algebra |edition=4th |first=S. |last=Lipschutz |first2=M. |last2=Lipson |series=Schaum’s Outlines |publisher=McGraw-Hill |year=2009 |isbn=978-0-07-154352-1 }}</ref> For instance, if {{nowrap|1=''m'' = 4}} and {{nowrap|1=''n'' = 3}}, then\n:<math>\\begin{align}\\mathbf{u} \\otimes \\mathbf{v} = \\mathbf{u} \\mathbf{v}^\\top\n= \\begin{bmatrix}u_1 \\\\ u_2 \\\\ u_3 \\\\ u_4\\end{bmatrix}\n\\begin{bmatrix}v_1 & v_2 & v_3\\end{bmatrix}\n= \\begin{bmatrix}u_1v_1 & u_1v_2 & u_1v_3 \\\\ u_2v_1 & u_2v_2 & u_2v_3 \\\\ u_3v_1 & u_3v_2 & u_3v_3 \\\\ u_4v_1 & u_4v_2 & u_4v_3\\end{bmatrix}.\\end{align}</math><ref>James M. Ortega (1987) ''Matrix Theory: A Second Course'', page 7, [[Plenum Press]] {{ISBN|0-306-42433-9}}</ref>\n\nFor [[complex numbers|complex]] vectors, it is often useful to take the [[conjugate transpose]] of '''v''', denoted <math>\\mathbf{v}^{\\dagger}</math>or <math>(\\mathbf{v}^\\top)^{*}</math>{{thinsp}}:\n\n:<math>\\mathbf{u} \\otimes \\mathbf{v} = \\mathbf{u} \\mathbf{v}^\\dagger = \\mathbf{u} (\\mathbf{v}^\\top)^*\\ </math>.\n\n===Contrast with Euclidean inner product===\nIf {{nowrap|1=''m'' = ''n''}}, then one can take the matrix product the other way, yielding a scalar (or {{nowrap|1 × 1}} matrix):\n:<math>\\left\\langle \\mathbf{u}, \\mathbf{v}\\right\\rangle = \\mathbf{u}^\\top \\mathbf{v}</math>\nwhich is the standard [[inner product]] for [[Euclidean vector space]]s, better known as the [[dot product]]. The inner product is the [[trace (linear algebra)|trace]] of the outer product.<ref>{{cite book |first=Robert F. |last=Stengel |title=Optimal Control and Estimation |location=New York |publisher=Dover Publications |year=1994 |page=26 |isbn=0-486-68200-5 |url=https://books.google.com/books?id=jDjPxqm7Lw0C&pg=PA26 }}</ref> Unlike the [[inner product]], the outer product is not commutative.\n\n===The outer product of tensors===\n\nGiven two tensors '''u''', '''v''' with dimensions <math>(k_1, k_2 \\dots k_m)</math> and <math>(l_1, l_2, \\dots l_m)</math> their outer product <math> \\mathbf{u} \\otimes \\mathbf{v}</math> is a tensor with dimensions <math>(k_1, k_2 \\dots k_m, l_1, l_2, \\dots l_m)</math> and entries\n:<math>(\\mathbf{u} \\otimes \\mathbf{v})_{i_1, i_2, \\dots i_m, j_1, j_2 \\dots j_m} =\nu_{i_1, i_2, \\dots i_m} v_{j_1, j_2, \\dots j_m}</math>\n\nFor example, if '''A''' is of order 3 with dimensions {{nowrap|(3, 5, 7)}} and '''B''' is of order 2 with dimensions {{nowrap|(10, 100)}}, their outer product '''c''' is of order 5 with dimensions {{nowrap|(3, 5, 7, 10, 100)}}.  If '''A''' has a component {{nowrap|1=''A''<sub>[2, 2, 4]</sub> = 11}} and '''B''' has a component {{nowrap|1=''B''<sub>[8, 88]</sub> = 13}}, then the component of '''C''' formed by the outer product is {{nowrap|1=''C''<sub>[2, 2, 4, 8, 88]</sub> = 143}}.\n\n===Contrast with the Kronecker product===\nThe outer product and Kronecker product are closely related; in fact the same symbol is commonly used to denote both operations.\n\nIf <math>\\mathbf{u} = \\begin{bmatrix}1 & 2 & 3\\end{bmatrix}^\\top</math> and <math>\\mathbf{v} = \\begin{bmatrix}4 & 5\\end{bmatrix}^\\top</math>, we have\n\n:<math>\n\\begin{align}\n\\mathbf{u} \\otimes_{\\text{kron}} \\mathbf{v}\n&= \\begin{bmatrix} 4 \\\\ 5 \\\\ 8 \\\\ 10 \\\\ 12 \\\\ 15\\end{bmatrix}\n& \\mathbf{u} \\otimes_{\\text{outer}} \\mathbf{v}\n&= \\begin{bmatrix} 4 & 5 \\\\ 8 & 10 \\\\ 12 & 15\\end{bmatrix}\n\\end{align}\n</math>\n\nThe Kronecker product is a form of [[vectorization]] (or flattening) of the outer product.\nIn particular, we can write\n:<math>\\mathbf{u} \\otimes_{\\text{kron}} \\mathbf{v} = \\text{vec}(\\mathbf{v} \\otimes_{\\mathrm{outer}} \\mathbf{u} )</math>\n'''Note''': the order of the vectors is reversed!\n\n==Properties==\nThe outer product of vectors satisfies the following properties:\n\n:<math>(\\mathbf{u} \\otimes \\mathbf{v})^\\top = (\\mathbf{v} \\otimes \\mathbf{u})</math>\n:<math>(\\mathbf{v} + \\mathbf{w}) \\otimes \\mathbf{u} = \\mathbf{v} \\otimes \\mathbf{u} + \\mathbf{w} \\otimes \\mathbf{u}</math>\n:<math>\\mathbf{u} \\otimes (\\mathbf{v} + \\mathbf{w}) = \\mathbf{u} \\otimes \\mathbf{v} + \\mathbf{u} \\otimes \\mathbf{w}</math>\n:<math>c (\\mathbf{v} \\otimes \\mathbf{u}) = (c\\mathbf{v}) \\otimes \\mathbf{u} = \\mathbf{v} \\otimes (c\\mathbf{u})</math>\n\nThe outer product of tensors satisfies the additional [[associativity]] property:\n:<math>(\\mathbf{u} \\otimes \\mathbf{v}) \\otimes \\mathbf{w}\n=\n\\mathbf{u} \\otimes (\\mathbf{v} \\otimes \\mathbf{w})\n</math>\n\n===Rank of an outer product===\nIf '''u''' and '''v''' are both nonzero then the outer product matrix '''uv'''<sup>T</sup> always has [[Rank (linear algebra)|matrix rank]] 1. Indeed, the columns of the outer product are all proportional to the first column. Thus they are all [[linearly dependent]] on that one column, hence the matrix is of rank one.\n\n(\"Matrix rank\" should not be confused with \"[[tensor order]]\", or \"tensor degree\", which is sometimes referred to as \"rank\".)\n\n==Definition (abstract)==\nLet ''V'' and ''W'' be two [[vector space]]s. The outer product of \n<math>v \\in V</math> and <math>w \\in W</math> is the element <math>u \\otimes v \\in V \\otimes W</math>.\n\nIf ''V'' is an [[inner product space]] then it is possible to define the outer product as a linear map\n''V'' → ''W''. In this case the linear map <math>x \\mapsto \\langle v, x\\rangle</math> is an element of the [[dual space]] of ''V''. The outer product ''V'' → ''W'' is then given by\n:<math>(u \\otimes \\langle v |) (x) = \\langle v, x \\rangle w </math>\n\nThis shows why a conjugate transpose of 'v' is commonly taken in the complex case.\n\n==In programming languages==\n\nIn some programming languages, given a two-argument function ''f'' (or a binary operator), the outer product of ''f'' and two one-dimensional arrays A and B is a two-dimensional array C such that C[i,j] = f(A[i],B[j]). This is syntactically represented in various ways: in [[APL programming language|APL]], as the infix binary operator °.''f''; in [[R programming language|R]], as the function outer(A, B, ''f'');<ref>{{cite web |title=outer function |work=RDocumentation |date= |url=https://www.rdocumentation.org/packages/sets/versions/1.0-16/topics/outer }}</ref> in [[Wolfram Mathematica|Mathematica]], as Outer[''f'',A,B]. In MATLAB, the function kron(A,B) is used for this product. These often generalize to multi-dimensional arguments, and more than two arguments.\n===Python with NumPy===\nIn the [[Python (programming language)|Python]] library [[NumPy]], the outer product can be computed with function <code>np.outer()</code>\n<ref>https://docs.scipy.org/doc/numpy/reference/generated/numpy.outer.html</ref>\n<source lang=\"numpy\"> \n>>> import numpy as np\n>>> a = np.array([1, 2, 3])\n>>> b = np.array([2, 4, 8])\n>>> np.outer(a, b)\n\nOut[*]:  array([[ 2,  4,  8], \n                [ 4,  8, 16],\n                [ 6, 12, 24]])\n\n# in contrast np.kron, results in a flat array\n>>> np.kron(a, b)\n\nOut[*]:  array([ 2, 4, 8, 4, 8, 16, 6, 12, 24])\n</source>\nThe outer product of multidimensional arrays can be computed using <code>np.multiply.outer</code>.\n<source lang=\"numpy\">\n>>> np.multiply.outer(np.empty((1, 2, 3)), np.empty((4, 5))).shape\n\nOut[*]:  (1, 2, 3, 4, 5)\n</source>\n\n==Applications==\nAs the outer product is closely related to the [[Kronecker product]], some of the applications of the Kronecker product use outer products. Some of these applications to quantum theory, signal processing, and image compression are found in chapter 3, \"Applications\", in a book by Willi-Hans Steeb and Yorick Hardy.<ref> Willi-Hans Steeb and Yorick Hardy (2011) Matrix Calculus and Kronecker Product: A Practical Approach to Linear and Multilinear Algebra, second edition, [[World Scientific]] {{ISBN|981-4335-31-2}}</ref>\n\n===Spinors===\nSuppose ''s,t,w,z'' ∈ ℂ so that (''s,t'') and (''w,z'') are in ℂ<sup>2</sup>. Then the outer product of these complex 2-vectors is an element of M(2,ℂ), the 2 × 2 complex matrices:\n:<math>\\begin{pmatrix}sw & tw \\\\ sz & tz \\end{pmatrix}.</math> The [[determinant]] of this matrix is ''swtz'' − ''sztw'' = 0 because of the [[commutative property]] of ℂ.\n\nIn the theory of [[spinors in three dimensions]], these matrices are associated with [[isotropic vector]]s due to this null property. [[Élie Cartan]] described this construction in 1937<ref>[[Élie Cartan]] (1937) ''Lecons sur la theorie des spineurs'', translated 1966: ''The Theory of Spinors'', Hermann, Paris</ref> but it was introduced by [[Wolfgang Pauli]] in 1927<ref>Pertti Lounesto (1997) ''Clifford Algebras and Spinors'', page 51, [[Cambridge University Press]] {{ISBN|0-521-59916-4}}</ref> so that M(2,ℂ) has come to be called [[Pauli algebra]].\n\n===Concepts===\nThe block form of outer products is useful in classification. [[Concept analysis]] is a study that depends on certain outer products:\n\nWhen a vector has only zeros and ones as entries it is called a ''logical vector'', a special case of a [[logical matrix]]. The logical operation [[and (logic)|and]] takes the place of multiplication. The outer product of two logical vectors (''u''<sub>i</sub>) and (''v''<sub>j</sub>) is given by the logical matrix <math>(a_{i j}) \\ = \\ (u_i \\land v_j )</math>. This type of matrix is used in the study of [[binary relation]]s and is called a [[rectangular relation]] or a '''cross-vector'''.<ref>Ki Hang Kim (1982) ''Boolean Matrix Theory and Applications'', page 37, [[Marcel Dekker]] {{ISBN|0-8247-1788-0}}</ref>\n\n==See also==\n* [[Dyadics]]\n* [[Householder transformation]]\n* [[Linear algebra]]\n* [[Norm (mathematics)]]\n* [[Scatter matrix]]\n* [[Ricci calculus]]\n\n===Products===\n* [[Cross product]]\n* [[Exterior product]]\n* [[Cartesian product]]\n\n===Duality===\n* [[Complex conjugate]]\n* [[Conjugate transpose]]\n* [[Transpose]]\n* [[Bra–ket notation#Outer products|Bra–ket notation for outer product]]\n\n==References==\n{{reflist}}\n\n==Further reading==\n*{{cite book |first=Eric |last=Carlen |first2=Maria |last2=Canceicao Carvalho |title=Linear Algebra: From the Beginning |chapter=Outer Products and Orthogonal Projections |location= |publisher=Macmillan |year=2006 |pages=217–218 |chapterurl=https://books.google.com/books?id=EqGjBOXbPAoC&pg=PA217 }}\n\n{{Linear algebra}}\n\n{{DEFAULTSORT:Outer Product}}\n[[Category:Bilinear operators]]\n[[Category:Binary operations]]\n[[Category:Higher-order functions]]\n[[Category:Articles with example Python code]]"
    },
    {
      "title": "Pointwise product",
      "url": "https://en.wikipedia.org/wiki/Pointwise_product",
      "text": "{{For|entrywise product|Hadamard product (matrices)}}\n{{Unreferenced|date=December 2009}}\nThe '''pointwise product''' of two [[function (mathematics)|function]]s is another function, obtained by multiplying the [[Image (mathematics)|image]] of the two functions at each value in the domain. If ''f'' and ''g'' are both functions with [[domain (mathematics)|domain]] ''X'' and [[codomain]] ''Y'', and elements of ''Y'' can be multiplied (for instance, ''Y'' could be some set of numbers), then the pointwise product of ''f'' and ''g'' is another function from ''X'' to ''Y'' which maps ''x'' ∈ ''X'' to ''f''(''x'')''g''(''x'').\n\n==Formal definition==\nLet ''X'' and ''Y'' be [[set (mathematics)|set]]s, and let [[multiplication]] be defined in ''Y''&mdash;that is, for each ''y'' and ''z'' in ''Y'' let the [[Product (mathematics)|product]]\n: <math>\\cdot : Y \\times Y \\longrightarrow Y</math> given by <math>y \\cdot z = yz</math>\nbe well-defined. Let ''f'' and ''g'' be [[function (mathematics)|function]]s {{nowrap|''f'', ''g'' : ''X'' → ''Y''}}. Then the '''pointwise product''' {{nowrap|(''f'' ⋅ ''g'') : ''X'' → ''Y''}} is defined by\n\n: <math>(f \\cdot g)(x) = f(x) \\cdot g(x)</math>\n\nfor each ''x'' in ''X''. In the same manner in which the [[binary operator]] ⋅ is omitted from products, we say that {{nowrap|1=''f'' ⋅ ''g'' = ''fg''}}.\n\n==Examples==\nThe most common case of the pointwise product of two functions is when the [[codomain]] is a [[ring (mathematics)|ring]] (or [[field (mathematics)|field]]), in which multiplication is well-defined. \n\n{{unordered list\n| If ''Y'' is the set of [[real number]]s '''R''', then the pointwise product of ''f'', ''g'' : ''X'' → '''R''' is just normal multiplication of the images. For example, if we have ''f''(''x'') {{=}} 2''x'' and ''g''(''x'') {{=}} ''x'' + 1 then\n: <math>(fg)(x) = f(x)g(x) = 2x(x + 1) = 2x^2 + 2x\\,</math>\n\nfor every real number ''x'' in '''R'''.\n| The [[convolution theorem]] states that the Fourier transform of a convolution is the pointwise product of Fourier transforms:\n: <math>\\mathcal{F}\\{f*g\\} = \\mathcal{F}\\{f\\} \\cdot \\mathcal{F}\\{g\\}</math>\n}}\n\n==Algebraic application of pointwise products==\nLet ''X'' be a [[Set (mathematics)|set]] and let ''R'' be a [[ring (mathematics)|ring]]. Since [[addition]] and [[multiplication]] are defined in ''R'', we can construct an algebraic structure known as an [[K-algebra|algebra]]  out of the functions from ''X'' to ''R'' by defining addition, multiplication, and scalar multiplication of functions to be done pointwise. \n\nIf ''R''<sup>&thinsp;''X''</sup> denotes the set of functions from ''X'' to ''R'', then we say that if ''f'', ''g'' are elements of ''R''<sup>&thinsp;''X''</sup>, then ''f'' + ''g'', ''fg'', and ''rf'', the last of which is defined by\n: <math>(rf)(x) = rf(x)\\,</math>\n\nfor all ''r'' in ''R'', are all elements of ''R''<sup>&thinsp;''X''</sup>.\n\n==Generalization==\nIf both ''f'' and ''g'' have as their domain all possible assignments of a set of discrete variables, then their pointwise product is a function whose domain is constructed by all possible assignments of the [[union (set theory)|union]] of both sets. The value of each assignment is calculated as the product of the values of both functions given to each one the subset of the assignment that is in its domain.\n\nFor example, given the function ''f''<sub>1</sub>() for the boolean variables ''p'' and ''q'', and ''f''<sub>2</sub>() for the boolean variables ''q'' and ''r'', both with the [[range (mathematics)|range]] in '''R''', the pointwise product of ''f''<sub>1</sub>() and ''f''<sub>2</sub>() is shown in the next table:\n\n{| class=\"wikitable\" style=\"margin-left:auto; margin-right:auto; border:none; text-align:center;\"\n|- style=\"background:silver\" \n! ''p''\n! ''q''\n! ''r''\n! {{tmath|f_1(p, q)}}\n! {{tmath|f_2(q, r)}}\n! Pointwise product \n|-\n| T || T || T || 0.1 || 0.2 || 0.1 × 0.2   \n|-\n| T || T || F || 0.1 || 0.4 || 0.1 × 0.4   \n|-\n| T || F || T || 0.3 || 0.6 || 0.3 × 0.6   \n|-\n| T || F || F || 0.3 || 0.8 || 0.3 × 0.8   \n|-\n| F || T || T || 0.5 || 0.2 || 0.5 × 0.2   \n|-\n| F || T || F || 0.5 || 0.4 || 0.5 × 0.4   \n|-\n| F || F || T || 0.7 || 0.6 || 0.7 × 0.6   \n|-\n| F || F || F || 0.7 || 0.8 || 0.7 × 0.8   \n|}\n\n==See also==\n*[[Pointwise]]\n\n{{DEFAULTSORT:Pointwise Product}}\n[[Category:Elementary algebra]]\n[[Category:Binary operations]]"
    },
    {
      "title": "Poisson bracket",
      "url": "https://en.wikipedia.org/wiki/Poisson_bracket",
      "text": "In [[mathematics]] and [[classical mechanics]], the '''Poisson bracket''' is an important [[binary operation]] in [[Hamiltonian mechanics]], playing a central role in Hamilton's equations of motion, which govern the time evolution of a Hamiltonian [[dynamical system]]. The Poisson bracket also distinguishes a certain class of coordinate transformations, called ''[[canonical transformations]]'', which map [[Canonical coordinates|canonical coordinate systems]] into canonical coordinate systems. A \"canonical coordinate system\" consists of canonical position and momentum variables (below symbolized by <math>q_i</math> and <math>p_i</math>, respectively) that satisfy canonical Poisson bracket relations. The set of possible canonical transformations is always very rich. For instance, it is often possible to choose the Hamiltonian itself <math>H =H(q, p; t)</math> as one of the new canonical momentum coordinates.\n\nIn a more general sense, the Poisson bracket is used to define a [[Poisson algebra]], of which the algebra of functions on a [[Poisson manifold]] is a special case. There are other general examples, as well: it occurs in the theory of [[Lie algebra]]s, where the [[tensor algebra]] of a Lie algebra forms a Poisson algebra; a detailed construction of how this comes about is given in the [[universal enveloping algebra]] article. Quantum deformations of the universal enveloping algebra lead to the notion of [[quantum group]]s.\n\nAll of these objects are named in honour of [[Siméon Denis Poisson]].\n\n==Properties==\nGiven two functions <math>f,\\ g</math> that depend on [[phase space]] and time, their Poisson bracket <math>\\{f, g\\}</math> is another function that depends on phase space and time. The following rules hold for any three functions <math>f,\\, g,\\, h</math> of phase space and time:\n;[[Anticommutativity]]: <math>\\{f, g\\} = -\\{g, f\\}</math>\n;[[Bilinearity]]: <math>\\{af + bg, h\\} = a\\{f, h\\} + b\\{g, h\\}, \\quad \\{h, af + bg\\} = a\\{h, f\\} + b\\{h, g\\}, \\quad a, b \\in \\mathbb R</math>\n;[[Product rule|Leibniz's rule]]: <math>\\{fg, h\\} = \\{f, h\\}g + f\\{g, h\\}</math>\n;[[Jacobi identity]]: <math>\\{f, \\{g, h\\}\\} + \\{g, \\{h, f\\}\\} +  \\{h, \\{f, g\\}\\} = 0</math>\n\nAlso, if a function <math>k</math> is constant over phase space (but may depend on time), then <math>\\{f,\\, k\\} = 0</math> for any <math>f</math>.\n\n==Definition in canonical coordinates==\nIn [[canonical coordinates]] (also known as [[Darboux coordinates]]) <math> (q_i,\\, p_i)</math> on the [[phase space]], given two functions <math> f(p_i,\\, q_i, t)</math> and <math> g(p_i,\\, q_i, t)</math>,<ref group=Note><math> f(p_i,\\, q_i,\\, t)</math> means <math>f</math> is a function of the <math>2N + 1</math> independent variables: momentum, <math>p</math>{{sub|1…N}}; position, <math>q</math>{{sub|1…N}}; and time, <math>t</math></ref> the Poisson bracket takes the form\n\n:<math>\\{f, g\\} = \\sum_{i=1}^{N} \\left( \\frac{\\partial f}{\\partial q_{i}} \\frac{\\partial g}{\\partial p_{i}} - \\frac{\\partial f}{\\partial p_i} \\frac{\\partial g}{\\partial q_i}\\right).</math>\n\nThe Poisson brackets of the canonical coordinates are\n\n:<math>\\begin{align}\n  \\{q_i,q_j\\} &= 0 \\\\\n  \\{p_i,p_j\\} &= 0 \\\\\n  \\{q_i,p_j\\} &= \\delta_{ij}\n\\end{align}</math>\n\nwhere ''<math>\\delta_{ij}</math>'' is the [[Kronecker delta]].\n\n==Hamilton's equations of motion==\n[[Hamilton's equations of motion]] have an equivalent expression in terms of the Poisson bracket. This may be most directly demonstrated in an explicit coordinate frame. Suppose that <math>f(p, q, t)</math> is a function on the manifold. Then from the multivariable [[chain rule]],\n\n:<math>\\frac{\\mathrm{d}}{\\mathrm{d}t} f(p, q, t) = \\frac{\\partial f}{\\partial q} \\frac{\\mathrm{d}q}{\\mathrm{d}t} + \\frac {\\partial f}{\\partial p} \\frac{\\mathrm{d}p}{\\mathrm{d}t} + \\frac{\\partial f}{\\partial t}.</math>\n\nFurther, one may take <math>p=p(t)</math> and <math>q=q(t)</math> to be solutions to [[Hamilton's equations]]; that is,\n\n:<math>\\begin{cases}\n  \\dot{q} =  \\frac{\\partial H}{\\partial p} = \\{q, H\\}; \\\\ \n  \\dot{p} = -\\frac{\\partial H}{\\partial q} = \\{p, H\\}.\n\\end{cases}</math>\n\nThen \n:<math>\\begin{align}\n  \\frac {\\mathrm{d}}{\\mathrm{d}t} f(p, q, t) &= \\frac{\\partial f}{\\partial q} \\frac{\\partial H}{\\partial p} - \\frac{\\partial f}{\\partial p} \\frac{\\partial H}{\\partial q} + \\frac{\\partial f}{\\partial t} \\\\\n                                             &= \\{f, H\\} + \\frac{\\partial f}{\\partial t} ~.\n\\end{align}</math>\n\nThus, the time evolution of a function <math>f</math> on a [[symplectic manifold]] can be given as a [[flow (mathematics)|one-parameter family]] of [[symplectomorphism]]s (i.e., [[canonical transformations]], area-preserving diffeomorphisms), with the time <math>t</math> being the parameter:  Hamiltonian motion is a canonical transformation generated by the Hamiltonian. That is, Poisson brackets are preserved in it, so that ''any time <math>t</math>'' in the solution to Hamilton's equations, \n\n: <math> q(t) = \\exp (-t \\{ H, \\cdot \\} ) q(0), \\quad  p(t) = \\exp (-t \\{ H, \\cdot \\}) p(0), </math> \n\ncan serve as the bracket coordinates. ''Poisson brackets are [[Canonical transformation|canonical invariants]]''.\n\nDropping the coordinates, \n:<math>\\frac{\\text{d}}{\\text{d}t} f = \\left(\\frac{\\partial}{\\partial t} - \\{H, \\cdot\\}\\right)f.</math>\n\nThe operator in the convective part of the derivative, <math>i\\hat{L} = -\\{H, \\cdot\\}</math>, is sometimes referred to  as the Liouvillian (see [[Liouville's theorem (Hamiltonian)]]).\n\n==Constants of motion==\nAn [[integrable dynamical system]] will have [[constants of motion]] in addition to the energy. Such constants of motion will commute with the Hamiltonian under the Poisson bracket. Suppose some function <math>f(p, q)</math> is a constant of motion. This implies that if <math>p(t), q(t)</math> is a [[trajectory]] or solution to  the [[Hamilton's equations of motion]], then\n\n:<math>0 = \\frac{\\text{d}f}{\\text{d}t}</math>\n\nalong that trajectory. Then\n\n:<math>0 = \\frac{\\text{d}}{\\text{d}t} f(p,q) = \\{f, H\\} + \\frac{\\partial f}{\\partial t}=\\{f, H\\}</math>\n\nwhere, as above, the intermediate step follows by applying the equations of motion and we supposed that <math>f</math> does not explicitly depend on time. This equation is known as the [[Liouville's theorem (Hamiltonian)#Liouville equations|Liouville equation]]. The content of [[Liouville's theorem (Hamiltonian)|Liouville's theorem]] is that the time evolution of a [[measure (mathematics)|measure]] (or \"[[Distribution function (physics)|distribution function]]\" on the phase space) is given by the above.\n\nIf the Poisson bracket of <math>f</math> and <math>g</math> vanishes (<math>\\{f,g\\} = 0</math>), then <math>f</math> and <math>g</math> are said to be '''in involution'''.  In order for a Hamiltonian system to be [[completely integrable]], <math>n</math> independent constants of motion must be in [[Distribution (differential geometry)#Involutive distributions|mutual involution]], where <math>n</math> is the number of degrees of freedom.\n\nFurthermore, according to the '''Poisson's Theorem''', if two quantities <math>A</math> and <math>B</math> are explicitly time independent (<math>A(p, q), B(p, q)</math>) constants of motion, so is their Poisson bracket <math>\\{A,\\, B\\}</math>. This does not always supply a useful result, however, since the number of possible constants of motion is limited (<math>2n - 1</math> for a system with <math>n</math> degrees of freedom), and so the result may be trivial (a constant, or a function of <math>A</math> and <math>B</math>.)\n\n==The Poisson bracket in coordinate-free language==\nLet <math>M</math> be   [[symplectic manifold]], that is, a [[manifold]] equipped with a [[symplectic form]]: a [[Differential form|2-form]] <math>\\omega</math> which is both '''closed''' (i.e., its  [[exterior derivative]] <math>\\text{d} \\omega</math> vanishes) and '''non-degenerate'''.  For example, in the treatment above, take <math>M</math> to be <math>\\mathbb{R}^{2n}</math> and take\n\n:<math>\\omega = \\sum_{i=1}^{n} \\text{d} q_i \\wedge \\text{d} p_i.</math>\n\nIf <math> \\iota_v \\omega</math> is the [[interior product]] or [[Tensor contraction|contraction]] operation defined by <math> (\\iota_v \\omega)(w) =  \\omega(v,\\, w)</math>, then non-degeneracy is equivalent to saying that for every one-form <math>\\alpha</math> there is a unique vector field <math>\\Omega_\\alpha</math> such that <math> \\iota_{\\Omega_\\alpha} \\omega =  \\alpha</math>. Alternatively, <math> \\Omega_{\\text{d} H} = \\omega^{-1}(\\text{d} H)</math>. Then if <math>H</math> is a smooth function on <math>M</math>, the [[Hamiltonian vector field]] <math>X_H</math> can be defined to be <math> \\Omega_{\\text{d} H}</math>.  It is easy to see that\n\n:<math>\\begin{align}\n  X_{p_i} &=  \\frac{\\partial}{\\partial q_i} \\\\\n  X_{q_i} &= -\\frac{\\partial}{\\partial p_i}.\n\\end{align}</math>\n\nThe '''Poisson bracket''' <math>\\ \\{\\cdot,\\, \\cdot\\} </math> on (''M'', ω) is a [[bilinear map|bilinear operation]] on [[Differentiable function|differentiable]] [[function (mathematics)|function]]s, defined by <math> \\{f,\\, g\\} \\;=\\; \\omega(X_f,\\, X_g) </math>; the Poisson bracket of two functions on ''M'' is itself a function on ''M''.  The Poisson bracket is antisymmetric because:\n\n:<math>\\{f, g\\} = \\omega(X_f, X_g) = -\\omega(X_g, X_f) = -\\{g, f\\} </math>.\n\nFurthermore,\n\n{{NumBlk|:|<math>\\begin{align}\n  \\{f, g\\} &= \\omega(X_f, X_g) = \\omega(\\Omega_{df}, X_g) \\\\\n           &= (\\iota_{\\Omega_{df}}\\omega)(X_g) = df(X_g) \\\\\n           &= X_gf = \\mathcal{L}_{X_g} f\n\\end{align}</math>.|{{EquationRef|1}}}}\n\nHere ''X<sub>g</sub>f'' denotes the vector field ''X<sub>g</sub>'' applied to the function ''f'' as a directional derivative, and <math>\\mathcal{L}_{X_g} f</math> denotes the (entirely equivalent) [[Lie derivative]] of the function ''f''.\n\nIf α is an arbitrary one-form on ''M'', the vector field Ω<sub>α</sub> generates (at least locally) a [[flow (mathematics)|flow]] <math> \\phi_x(t)</math> satisfying the boundary condition <math> \\phi_x(0) = x</math> and the first-order differential equation\n\n:<math>\\frac{d\\phi_x}{dt} = \\left. \\Omega_\\alpha \\right|_{\\phi_x(t)}.</math>\n\nThe <math> \\phi_x(t)</math> will be [[symplectomorphism]]s ([[canonical transformation]]s) for every ''t'' as a function of ''x'' if and only if <math> \\mathcal{L}_{\\Omega_\\alpha}\\omega \\;=\\; 0</math>; when this is true, Ω<sub>α</sub> is called a [[symplectic vector field]].  Recalling [[Cartan's identity]] <math>  \\mathcal{L}_X\\omega \\;=\\; d (\\iota_X \\omega) \\,+\\, \\iota_X d\\omega</math> and ''d''ω = 0, it follows that <math> \\mathcal{L}_{\\Omega_\\alpha}\\omega \\;=\\; d\\left(\\iota_{\\Omega_\\alpha} \\omega\\right) \\;=\\; d\\alpha</math>. Therefore, Ω<sub>α</sub> is a symplectic vector field if and only if α is a [[Closed and exact differential forms|closed form]].  Since <math> d(df) \\;=\\; d^2f \\;=\\; 0</math>, it follows that every Hamiltonian vector field ''X<sub>f</sub>'' is a symplectic vector field, and that the Hamiltonian flow consists of canonical transformations.  From {{EquationNote|1|(1)}} above, under the Hamiltonian flow ''X<sub>H</sub>'',\n\n:<math>\\frac{d}{dt}f(\\phi_x(t)) = X_Hf = \\{f,H\\}.</math>\n\nThis is a fundamental result in Hamiltonian mechanics, governing the time evolution of functions defined on phase space.  As noted above, when ''{f,H} = 0'', ''f'' is a constant of motion of the system.  In addition, in canonical coordinates (with <math> \\{p_i,\\, p_j\\} \\;=\\; \\{q_i,q_j\\} \\;=\\; 0</math> and <math>\\{q_i,\\, p_j\\} \\;=\\; \\delta_{ij}</math>), Hamilton's equations for the time evolution of the system follow immediately from this formula.\n\nIt also follows from {{EquationNote|1|(1)}} that the Poisson bracket is a [[derivation (abstract algebra)|derivation]]; that is, it satisfies a non-commutative version of Leibniz's [[product rule]]:\n\n{{NumBlk|:|<math>\\{fg,h\\} = f\\{g,h\\} + g\\{f,h\\}</math>, and <math>\\{f,gh\\} = g\\{f,h\\} + h\\{f,g\\}</math>|{{EquationRef|2}}}}\n\nThe Poisson bracket is intimately connected to the [[Lie bracket of vector fields|Lie bracket]] of the Hamiltonian vector fields.  Because the Lie derivative is a derivation,\n\n:<math>\\mathcal L_v\\iota_w\\omega = \\iota_{\\mathcal L_vw}\\omega + \\iota_w\\mathcal L_v\\omega = \\iota_{[v,w]}\\omega + \\iota_w\\mathcal L_v\\omega</math>.\n\nThus if ''v'' and ''w'' are symplectic, using <math> \\mathcal{L}_v\\omega \\;=\\; 0</math>, Cartan's identity, and the fact that <math>\\iota_w\\omega</math> is a closed form,\n\n:<math>\\iota_{[v,w]}\\omega = \\mathcal L_v\\iota_w\\omega = d(\\iota_v\\iota_w\\omega) + \\iota_vd(\\iota_w\\omega) = d(\\iota_v\\iota_w\\omega) = d(\\omega(w,v)).</math>\n\nIt follows that <math>[v,w] = X_{\\omega(w,v)}</math>, so that\n{{NumBlk|:|<math>[X_f,X_g] = X_{\\omega(X_g,X_f)} = -X_{\\omega(X_f,X_g)} = -X_{\\{f,g\\}}</math>.|{{EquationRef|3}}}}\n\nThus, the Poisson bracket on functions corresponds to the Lie bracket of the associated Hamiltonian vector fields.  We have also shown that the Lie bracket of two symplectic vector fields is a Hamiltonian vector field and hence is also symplectic.  In the language of [[abstract algebra]], the symplectic vector fields form a [[subalgebra]] of the [[Lie algebra]] of smooth vector fields on ''M'', and the Hamiltonian vector fields form an [[algebraic ideal|ideal]] of this subalgebra.  The symplectic vector fields are the Lie algebra of the (infinite-dimensional) [[Lie group]] of [[symplectomorphism]]s of ''M''.\n\nIt is widely asserted that the [[Jacobi identity]] for the Poisson bracket,\n\n:<math>\\ \\{f,\\{g,h\\}\\} + \\{g,\\{h,f\\}\\} + \\{h,\\{f,g\\}\\} = 0</math>\n\nfollows from the corresponding identity for the Lie bracket of vector fields, but this is true only up to a locally constant function.  However, to prove the Jacobi identity for the Poisson bracket, it is [[Jacobi identity#Examples|sufficient]] to show that:\n\n:<math>\\operatorname{ad}_{\\{f,g\\}}=[\\operatorname{ad}_f,\\operatorname{ad}_g]</math>\n\nwhere the operator <math>\\operatorname{ad}_g</math> on smooth functions on ''M'' is defined by <math>\\operatorname{ad}_g(\\cdot) \\;=\\; \\{\\cdot,\\, g\\}</math> and the bracket on the right-hand side is the commutator of operators, <math> [\\operatorname A,\\, \\operatorname B] \\;=\\; \\operatorname A\\operatorname B - \\operatorname B\\operatorname A</math>.  By {{EquationNote|1|(1)}}, the operator <math>\\operatorname{ad}_g</math> is equal to the operator ''X<sub>g</sub>''.  The proof of the Jacobi identity follows from {{EquationNote|3|(3)}} because the Lie bracket of vector fields is just their commutator as differential operators.\n\nThe [[Algebra over a field|algebra]] of smooth functions on M, together with the Poisson bracket forms a [[Poisson algebra]], because it is a [[Lie algebra]] under the Poisson bracket, which additionally satisfies Leibniz's rule {{EquationNote|2|(2)}}.  We have shown that every [[symplectic manifold]] is a [[Poisson manifold]], that is a manifold with a \"curly-bracket\" operator on smooth functions such that the smooth functions form a Poisson algebra.  However, not every Poisson manifold arises in this way, because Poisson manifolds allow for degeneracy which cannot arise in the symplectic case.\n\n==A result on conjugate momenta==\nGiven a smooth [[vector field]] <math>X</math> on the configuration space, let <math>P_X</math> be its [[conjugate momentum]]. The conjugate momentum mapping is a [[Lie algebra]] anti-homomorphism from the Poisson bracket to the [[Lie bracket of vector fields|Lie bracket]]:\n\n:<math>\\{P_X, P_Y\\} = -P_{[X, Y]}.\\,</math>\n\nThis important result is worth a short proof. Write a vector field <math>X</math> at point <math>q</math> in the [[Configuration space (physics)|configuration space]] as\n\n:<math>X_q = \\sum_i X^i(q) \\frac{\\partial}{\\partial q^i}</math>\n\nwhere the <math>\\scriptstyle \\frac{\\partial}{\\partial q^i}</math> is the local coordinate frame. The conjugate momentum to <math>X</math> has the expression\n\n:<math>P_X(q, p) = \\sum_i X^i(q) \\;p_i</math>\n\nwhere the <math>p_i</math> are the momentum functions conjugate to the coordinates. One then has, for a point <math>(q,p)</math> in the [[phase space]],\n\n:<math>\\begin{align}\n  \\{P_X,P_Y\\}(q,p) &=  \\sum_i \\sum_j \\left\\{ X^i(q) \\;p_i, Y^j(q)\\; p_j \\right\\} \\\\\n                   &=  \\sum_{ij} p_i Y^j(q) \\frac{\\partial X^i}{\\partial q^j} -  p_j X^i(q) \\frac{\\partial Y^j}{\\partial q^i} \\\\\n                   &= -\\sum_i p_i \\; [X, Y]^i(q) \\\\\n                   &= - P_{[X, Y]}(q, p). \n\\end{align}</math>\n\nThe above holds for all <math>(q, p)</math>, giving the desired result.\n\n==Quantization==\nPoisson brackets [[Deformation theory|deform]] to [[Moyal bracket]]s upon [[Weyl quantization|quantization]], that is, they generalize to a different Lie algebra, the [[Moyal bracket|Moyal algebra]], or, equivalently in [[Hilbert space]], quantum [[commutator]]s. The Wigner-İnönü [[group contraction]] of these (the classical limit, {{math|ħ → 0}})  yields the above Lie algebra.\n\nTo state this more explicitly and precisely, the [[universal enveloping algebra]] of the [[Heisenberg algebra]] is the [[Weyl algebra]] (modulo the relation that the center be the unit). The Moyal product is then a special case of the star product on the algebra of symbols. An explicit definition of the algebra of symbols, and the star product is given in the article on the [[universal enveloping algebra]].\n\n==See also==\n{{colbegin}}\n*[[Commutator]]\n*[[Dirac bracket]]\n*[[Lagrange bracket]]\n*[[Moyal bracket]]\n*[[Peierls bracket]]\n*[[Phase space]]\n*[[Poisson algebra]]\n*[[Poisson ring]]\n*[[Poisson superalgebra]]\n*[[Poisson superbracket]]\n{{colend}}\n\n==Remarks==\n{{reflist|group=Note}}\n\n==Notes==\n{{reflist|4}}\n\n\n==References==\n* {{cite book |ref=harv|title=Mathematical Methods of Classical Mechanics |last=Arnold |first=Vladimir I. |authorlink=Vladimir Arnold  |edition=2nd |year=1989 |publisher=Springer |location=New York |isbn=978-0-387-96890-2 }}\n* {{cite book |ref=harv|title=Mechanics |volume=Vol. 1 |series=[[Course of Theoretical Physics]] |last1=Landau |first1=Lev D. |authorlink1=Lev Landau |last2=Lifshitz| first2= Evegeny M.| authorlink2=Evgeny Lifshitz|year=1982 |edition=3rd |publisher=Butterworth-Heinemann |isbn=978-0-7506-2896-9 }}\n*{{cite book|last1=Karasëv|first1=Mikhail V.|authorlink2=Victor Pavlovich Maslov|last2=Maslov|first2=Victor P.|title=Nonlinear Poisson brackets, Geometry and Quantization|translator-first1=Alexey|translator-last1=Sossinsky| translator-first2=M.A.|translator-last2=Shishkova|series=Translations of Mathematical Monographs|volume=119|publisher=American Mathematical Society|location=Providence, RI|year=1993|mr=1214142|isbn=978-0821887967 }}\n\n==External links==\n* {{springer|title=Poisson brackets|id=p/p073270}}\n* {{mathworld |urlname=PoissonBracket |title=Poisson bracket|author=[[Eric W. Weisstein]]}}\n\n[[Category:Symplectic geometry]]\n[[Category:Hamiltonian mechanics]]\n[[Category:Bilinear operators]]\n[[Category:Binary operations]]\n[[Category:Concepts in physics]]"
    },
    {
      "title": "Product of group subsets",
      "url": "https://en.wikipedia.org/wiki/Product_of_group_subsets",
      "text": "In [[mathematics]], one can define a '''product of group subsets''' in a natural way. If ''S'' and ''T'' are [[subset]]s of a [[group (mathematics)|group]] ''G'', then their product is the subset of ''G'' defined by\n:<math>ST = \\{st : s \\in S \\text{ and } t\\in T\\}.</math>\nThe subsets ''S'' and ''T'' need not be [[subgroup]]s for this product to be well defined. The [[associativity]] of this product [[follows from]] that of the group product. The product of group subsets therefore defines a natural [[monoid]] structure on the [[power set]] of ''G''.\n\nA lot more can be said in the case where ''S'' and ''T'' are subgroups. The product of two subgroups ''S'' and ''T'' of a group ''G'' is itself a subgroup of ''G'' if and only if ''ST'' = ''TS''.\n\n== Product of subgroups ==\nIf ''S'' and ''T'' are subgroups of ''G'', their product need not be a subgroup (for example, two distinct subgroups of order 2 in the [[symmetric group]] on 3 symbols). This product is sometimes called the ''Frobenius product''.<ref name=\"Ballester-BolinchesEsteban-Romero2010p1\">{{cite book|author1=Adolfo Ballester-Bolinches|author2=Ramon Esteban-Romero|author3=Mohamed Asaad|title=Products of Finite Groups|year=2010|publisher=Walter de Gruyter|isbn=978-3-11-022061-2|page=1}}</ref> In general, the product of two subgroups ''S'' and ''T'' is a subgroup if and only if ''ST'' = ''TS'',<ref name=\"Nicholson2012\">{{cite book|author=W. Keith Nicholson|title=Introduction to Abstract Algebra|year=2012|publisher=John Wiley & Sons|edition=4th|isbn=978-1-118-13535-8|at=Lemma 2, p. 125}}</ref> and the two subgroups are said to [[permutable subgroup|permute]]. ([[Walter Ledermann]] has called this fact the ''Product Theorem'',<ref>Walter Ledermann, ''Introduction to Group Theory'', 1976, Longman, {{ISBN|0-582-44180-3}}, p. 52</ref> but this name, just like \"Frobenius product\" is by no means standard.) In this case, ''ST'' is the group [[generating set of a group|generated]] by ''S'' and ''T''; i.e., ''ST'' = ''TS'' = ⟨''S'' ∪ ''T''⟩.\n\nIf either ''S'' or ''T'' is [[normal subgroup|normal]] then the condition ''ST'' = ''TS'' is satisfied and the product is a subgroup.<ref name=\"NT5\">Nicholson, 2012, Theorem 5, p. 125</ref><ref name=\"Wallace1998\">{{cite book|author=David A.R. Wallace|title=Groups, Rings and Fields|year=1998|publisher=Springer Science & Business Media|isbn=978-3-540-76177-8|at=Theorem 14, p. 123}}</ref> If both ''S'' and ''T'' are normal, then the product is normal as well.<ref name=\"NT5\"/>\n\nIf ''S'' and ''T'' are finite subgroups of a group ''G'', then ''ST'' is a subset of ''G'' of size ''|ST|'' given by the ''product formula'':\n:<math>|ST| = \\frac{|S||T|}{|S\\cap T|}</math>\nNote that this applies even if neither ''S'' nor ''T'' is normal.\n\n=== Modular law ===\n\nThe following '''modular law (for groups)''' holds for any ''Q'' a subgroup of ''S'', where ''T'' is any other arbitrary subgroup (and both ''S'' and ''T'' are subgroups of some group ''G''):\n:''Q''(''S'' ∩ ''T'') = ''S'' ∩ (''QT'').\nThe two products that appear in this equality are not necessarily subgroups.\n\nIf ''QT'' is a subgroup (equivalently, as noted above, if ''Q'' and ''T'' permute) then ''QT'' = ⟨''Q'' ∪ ''T''⟩ = ''Q'' ∨ ''T''; i.e., ''QT'' is the [[Join (mathematics)|join]] of ''Q'' and ''T'' in the [[lattice of subgroups]] of ''G'', and the modular law for such a pair may also be written as ''Q'' ∨ (''S'' ∩ ''T'') = ''S'' ∩ (''Q ∨ T''), which is the equation that defines a [[modular lattice]] if it holds for any three elements of the lattice with ''Q'' ≤ ''S''. In particular, since normal subgroups permute with each other, they form a modular [[sublattice]].\n\nA group in which every subgroup permutes is called an [[Iwasawa group]]. The subgroup lattice of an Iwasawa group is thus a modular lattice, so these groups are sometimes called ''modular groups''<ref>Ballester-Bolinches, Esteban-Romero, Asaad, p. 24</ref> (although this latter term may have other meanings.)\n\nThe assumption in the modular law for groups (as formulated above) that ''Q'' is a subgroup of ''S'' is essential. If ''Q'' is ''not'' a subgroup of ''S'', then the tentative, more general distributive property that one may consider ''S'' ∩ (''QT'') = (''S'' ∩ ''Q'')(''S'' ∩ ''T'') is ''false''.<ref name=\"Robinson1996\">{{cite book|author=Derek Robinson|title=A Course in the Theory of Groups|year=1996|publisher=Springer Science & Business Media|isbn=978-0-387-94461-6|page=15}}</ref><ref name=\"Cohn2000p248\">{{cite book|author=Paul Moritz Cohn|title=Classic algebra|year=2000|publisher=Wiley|isbn=978-0-471-87731-8|pages=248}}</ref>\n\n=== Product of subgroups with trivial intersection ===\nIn particular, if ''S'' and ''T'' intersect only in the identity, then every element of ''ST'' has a unique expression as a product  ''st'' with ''s'' in ''S'' and ''t'' in ''T''.  If ''S'' and ''T'' also commute, then ''ST'' is a group, and is called a [[Zappa–Szép product]].  Even further, if ''S'' or ''T'' is normal in ''ST'', then ''ST'' coincides with the [[semidirect product]] of ''S'' and ''T''.  Finally, if both ''S'' and ''T'' are normal in ''ST'', then ''ST'' coincides with the [[direct product of groups|direct product]] of ''S'' and ''T''.\n\nIf ''S'' and ''T'' are subgroups whose intersection is the trivial subgroup (identity element) and additionally ''ST'' = ''G'', then ''S'' is called a [[complement (group theory)|complement]] of ''T'' and vice versa.\n\nBy a (locally unambiguous) [[abuse of terminology]], two subgroups that intersect only on the (otherwise obligatory) identity are sometimes called [[Disjoint sets|disjoint]].<ref>{{cite book|author=L. Fuchs|title=Infinite Abelian Groups. Volume I|year=1970|publisher=Academic Press|isbn=978-0-08-087348-0|page=37}}</ref>\n\n=== Product of subgroups with non-trivial intersection ===\nA question that arises in the case of a non-trivial intersection between a normal subgroup ''N'' and a subgroup ''K'' is what is the structure of the quotient ''NK''/''N''. Although one might be tempted to just \"cancel out\" ''N'' and say the answer is ''K'', that is not correct because a homomorphism with kernel ''N'' will also \"collapse\" (map to 1) all elements of ''K'' that happen to be in ''N''. Thus the correct answer is that ''NK''/''N'' is isomorphic with ''K''/(''N''∩''K''). This fact is sometimes called the [[second isomorphism theorem]],<ref name=\"Saracino1980\">{{cite book|author=Dan Saracino|title=Abstract Algebra: A First Course|year=1980|publisher=Addison-Wesley|isbn=0-201-07391-9|page=123}}</ref> (although the numbering of these theorems sees some variation between authors); it has also been called the ''diamond theorem'' by [[Martin Isaacs|I. Martin Isaacs]] because of the shape of subgroup lattice involved,<ref name=\"Isaacs1994\">{{cite book|author=I. Martin Isaacs|title=Algebra: A Graduate Course|year=1994|publisher=American Mathematical Soc.|isbn=978-0-8218-4799-2|page=33}}</ref> and has also been called the ''parallelogram rule'' by [[Paul Moritz Cohn]], who thus emphasized analogy with the [[parallelogram rule]] for vectors because in the resulting subgroup lattice the two sides assumed to represent the quotient groups (''SN'')&nbsp;/&nbsp;''N'' and ''S''&nbsp;/&nbsp;(''S''&nbsp;∩&nbsp;''N'') are \"equal\" in the sense of isomorphism.<ref name=\"Cohn2000p245\">{{cite book|author=[[Paul Moritz Cohn]]|title=Classic Algebra|year=2000|publisher=Wiley|isbn=978-0-471-87731-8|page=245}}</ref>\n\n[[Frattini's argument]] guarantees the existence of a product of subgroups (giving rise to the whole group) in a case where the intersection is not necessarily trivial (and for this latter reason the two subgroups are not complements). More specifically, if ''G'' is a finite group with normal subgroup ''N'', and if ''P'' is a [[Sylow p-subgroup|Sylow ''p''-subgroup]] of ''N'', then ''G'' = ''N''<sub>''G''</sub>(''P'')''N'', where ''N''<sub>''G''</sub>(''P'') denotes the [[centralizer and normalizer|normalizer]] of ''P'' in ''G''. (Note that the normalizer of ''P'' includes ''P'', so the intersection between ''N'' and ''N''<sub>''G''</sub>(''P'') is at least ''P''.)\n\n==Generalization to semigroups==\nIn a [[semigroup]], the product of two subsets is always a subsemigroup; furthermore the power set of subsemigroups is a [[semiring]] with addition as union (of subsets) and multiplication as product of subsets.<ref name=\"Pin1989\">{{cite book|author=Jean E. Pin|title=Formal Properties of Finite Automata and Applications: LITP Spring School on Theoretical Computer Science, Ramatuelle, France, May 23–27, 1988. Proceedings|year=1989|publisher=Springer Science & Business Media|isbn=978-3-540-51631-6|pages=35}}</ref>\n\n==See also==\n* [[Central product]]\n* [[Double coset]]\n\n==References==\n{{reflist}}\n*{{cite book\n | first      = Joseph\n | last       = Rotman\n | year       = 1995\n | title      = An Introduction to the Theory of Groups\n | edition    = 4th\n | publisher  = Springer-Verlag\n | isbn       = 0-387-94285-8\n}}\n\n[[Category:Group theory]]\n[[Category:Binary operations]]"
    },
    {
      "title": "Product ring",
      "url": "https://en.wikipedia.org/wiki/Product_ring",
      "text": "In [[mathematics]], it is possible to combine several [[ring (mathematics)|rings]] into one large '''product ring'''. This is done by giving the Cartesian product of a possibly infinite collection of rings coordinatewise addition and multiplication. The resulting ring is called a '''direct product''' of the original rings.\n\n==Examples==\nAn important example is the ring '''Z'''/''n'''''Z''' of [[integer]]s [[modular arithmetic|modulo]] ''n''. If ''n'' is written as a product of [[prime number|prime]] powers (see [[fundamental theorem of arithmetic]]),\n\n:<math>n=p_1^{n_1} p_2^{n_2}\\cdots\\ p_k^{n_k},</math>\n\nwhere the ''p<sub>i</sub>'' are distinct primes, then '''Z'''/''n'''''Z''' is naturally [[isomorphic]] to the product ring\n\n:<math>\\mathbf{Z}/p_1^{n_1}\\mathbf{Z} \\ \\times \\ \\mathbf{Z}/p_2^{n_2}\\mathbf{Z} \\ \\times \\ \\cdots \\ \\times \\ \\mathbf{Z}/p_k^{n_k}\\mathbf{Z}.</math>\nThis follows from the [[Chinese remainder theorem]].\n\n==Properties==\nIf {{nowrap|1=''R'' = Π<sub>''i''∈''I''</sub> ''R''<sub>''i''</sub>}} is a product of rings, then for every ''i'' in ''I'' we have a [[surjective]] [[ring homomorphism]] {{nowrap|''p<sub>i</sub>'': ''R'' → ''R<sub>i</sub>''}} which projects the product on the ''i''th coordinate. The product ''R'', together with the projections ''p<sub>i</sub>'', has the following [[universal property]]: \n:if ''S'' is any ring and {{nowrap|''f<sub>i</sub>'': ''S'' → ''R<sub>i</sub>''}} is a ring homomorphism for every ''i'' in ''I'', then there exists ''precisely one'' ring homomorphism {{nowrap|''f'': ''S'' → ''R''}} such that {{nowrap|1=''p<sub>i</sub>'' ∘ ''f'' = ''f<sub>i</sub>''}} for every ''i'' in ''I''.\nThis shows that the product of rings is an instance of [[product (category theory)|products in the sense of category theory]]. \n\nWhen ''I'' is finite, the underlying additive group of {{nowrap|Π<sub>''i''∈''I''</sub> ''R''<sub>''i''</sub>}} coincides with the [[direct sum]] of the additive groups of the ''R''<sub>''i''</sub>. In this case, some authors call ''R'' the \"direct sum of the rings ''R''<sub>''i''</sub>\" and write {{nowrap|⊕<sub>''i''∈''I''</sub> ''R''<sub>''i''</sub>}}, but this is incorrect from the point of view of category theory, since it is usually not a [[coproduct]] in the category of rings: for example, when two or more of the ''R''<sub>''i''</sub> are nonzero, the inclusion map {{nowrap|''R<sub>i''</sub> → ''R''}} fails to map 1 to 1 and hence is not a ring homomorphism.\n\n(A finite coproduct in the category of commutative (associative) algebras over a commutative ring is a [[tensor product of algebras]]. A coproduct in the category of algebras is a [[free product of algebras]].)\n\nDirect products are commutative and associative (up to isomorphism), meaning that it doesn't matter in which order one forms the direct product.\n\nIf ''A<sub>i</sub>'' is an [[ideal (ring theory)|ideal]] of ''R<sub>i</sub>'' for each ''i'' in ''I'', then {{nowrap|1=''A'' = Π<sub>''i''∈''I''</sub> ''A<sub>i</sub>''}} is an ideal of ''R''.  If ''I'' is finite, then the converse is true, i.e., every ideal of ''R'' is of this form.  However, if ''I'' is infinite and the rings ''R<sub>i</sub>'' are non-zero, then the converse is false: the set of elements with all but finitely many nonzero coordinates forms an ideal which is not a direct product of ideals of the ''R<sub>i</sub>''.  The ideal ''A'' is a [[prime ideal]] in ''R'' if all but one of the ''A<sub>i</sub>'' are equal to ''R<sub>i</sub>'' and the remaining ''A<sub>i</sub>'' is a prime ideal in ''R<sub>i</sub>''. However, the converse is not true when ''I'' is infinite. For example, the [[Direct sum of modules|direct sum]] of the ''R<sub>i</sub>'' form an ideal not contained in any such ''A'', but the [[axiom of choice]] gives that it is contained in some [[maximal ideal]] which is [[a fortiori]] prime.\n\nAn element ''x'' in ''R'' is a unit if and only if all of its components are units, i.e., if and only if {{nowrap|''p<sub>i</sub>''(''x'')}} is a unit in ''R<sub>i</sub>'' for every ''i'' in ''I''. The group of units of ''R'' is the [[direct product of groups|product]] of the groups of units of ''R<sub>i</sub>''.\n\nA product of two or more non-zero rings always has nonzero [[zero divisors]]: if ''x'' is an element of the product whose coordinates are  all zero except {{nowrap|''p<sub>i</sub>''(''x'')}}, and ''y'' is an element of the product with all coordinates zero except {{nowrap|''p<sub>j</sub>''(''y'')}} where {{nowrap|''i'' ≠ ''j''}}, then {{nowrap|1=''xy'' = 0}} in the product ring.\n\n==See also==\n*[[Direct product]]\n\n==Notes==\n{{reflist}}\n\n==References==\n*{{Citation\n| last=Herstein\n| first=I.N.\n| author-link=Israel Nathan Herstein\n| title=Noncommutative rings\n| year=2005\n| publisher=[[Cambridge University Press]]\n| edition=5th\n| isbn=978-0-88385-039-8\n| origyear=1968\n}}\n*{{Lang Algebra|edition=3r|page=91}}\n\n{{DEFAULTSORT:Product Of Rings}}\n[[Category:Ring theory]]\n[[Category:Binary operations]]"
    },
    {
      "title": "Product topology",
      "url": "https://en.wikipedia.org/wiki/Product_topology",
      "text": "{{Redirect|Product space||The Product Space}}\nIn [[topology]] and related areas of [[mathematics]], a '''product space''' is the [[Cartesian product]] of a family of [[topological space]]s equipped with a [[natural topology]] called the '''product topology'''.  This topology differs from another, perhaps more obvious, topology called the [[box topology]], which can also be given to a product space and which agrees with the product topology when the product is over only finitely many spaces.  However, the product topology is \"correct\" in that it makes the product space a [[product (category theory)|categorical product]] of its factors, whereas the box topology is [[Comparison of topologies|too fine]]; this is the sense in which the product topology is \"natural\".\n\n==Definition==\nGiven ''X'' such that\n\n:<math>X := \\prod_{i \\in I} X_i</math>\n\nis the Cartesian product of the topological spaces ''X<sub>i</sub>'', [[index set|indexed]] by <math>i \\in I</math>, and the '''[[projection (set theory)|canonical projections]]''' ''p<sub>i</sub>'' : ''X'' &rarr; ''X<sub>i</sub>'', the '''product topology''' on ''X'' is defined to be the [[coarsest topology]] (i.e. the topology with the fewest open sets) for which all the projections ''p<sub>i</sub>'' are [[continuous (topology)|continuous]].  The product topology is sometimes called the '''Tychonoff topology'''.\n\nThe open sets in the product topology are unions (finite or infinite) of sets of the form <math>\\prod_{i\\in I} U_i</math>, where each ''U<sub>i</sub>'' is open in ''X<sub>i</sub>'' and ''U''<sub>''i''</sub>&nbsp;≠&nbsp;''X''<sub>''i''</sub> for only finitely many ''i''. In particular, for a finite product (in particular, for the product of two topological spaces), the set of all Cartesian products between one basis element from each ''X<sub>i</sub>'' gives a basis for the product topology of <math>\\prod_{i\\in I} X_i</math>. That is, for a finite product, the set of all <math>\\prod_{i\\in I} U_i</math>, where <math>U_i</math> is an element of the (chosen) basis of <math>X_i</math>, is a basis for the product topology of <math>\\prod_{i\\in I} X_i</math>.\n\nThe product topology on ''X'' is the topology generated by sets of the form ''p<sub>i</sub>''<sup>&minus;1</sup>(''U<sub>i</sub>''), where  ''i'' is in ''I ''  and ''U<sub>i</sub>'' is an open subset of ''X<sub>i</sub>''. In other words, the sets {''p<sub>i</sub>''<sup>&minus;1</sup>(''U<sub>i</sub>'')} form a [[subbase]] for the topology on ''X''. A [[subset]] of ''X'' is open if and only if it is a (possibly infinite) [[union (set theory)|union]] of [[intersection (set theory)|intersections]] of finitely many sets of the form ''p<sub>i</sub>''<sup>&minus;1</sup>(''U<sub>i</sub>''). The ''p<sub>i</sub>''<sup>&minus;1</sup>(''U<sub>i</sub>'') are sometimes called [[open cylinder]]s, and their intersections are [[cylinder set]]s.\n\nIn general, the product of the topologies of each ''X<sub>i</sub>'' forms a basis for what is called the [[box topology]] on ''X''. In general, the box topology is [[finer topology|finer]] than the product topology, but for finite products they coincide.\n\n== Examples ==\n\nIf one starts with the [[standard topology]] on the [[real line]] '''R''' and defines a topology on the product of ''n'' copies of '''R''' in this fashion, one obtains the ordinary [[Euclidean topology]] on '''R'''<sup>''n''</sup>.\n\nThe [[Cantor set]] is [[homeomorphic]] to the product of [[countable|countably many]] copies of the [[discrete space]] {0,1} and the space of [[irrational number]]s is homeomorphic to the product of countably many copies of the [[natural number]]s, where again each copy carries the discrete topology.\n\nSeveral additional examples are given in the article on the [[initial topology]].\n\n== Properties ==\n\nThe product space ''X'', together with the canonical projections, can be characterized by the following [[universal property]]: If ''Y'' is a topological space, and for every ''i'' in ''I'', ''f<sub>i</sub>'' : ''Y'' &rarr; ''X<sub>i</sub>'' is a continuous map, then there exists ''precisely one'' continuous map ''f'' : ''Y'' &rarr; ''X'' such that for each ''i'' in ''I'' the following diagram [[commutative diagram|commutes]]:\n[[Image:CategoricalProduct-02.png|center|Characteristic property of product spaces]]\nThis shows that the product space is a [[product (category theory)|product]] in the [[category of topological spaces]]. It follows from the above universal property that a map ''f'' : ''Y'' &rarr; ''X'' is continuous [[if and only if]] ''f<sub>i</sub>'' = ''p<sub>i</sub>'' o ''f'' is continuous for all ''i'' in ''I''. In many cases it is easier to check that the component functions ''f<sub>i</sub>'' are continuous. Checking whether a map ''f'' : ''Y'' &rarr; ''X'' is continuous is usually more difficult; one tries to use the fact that the ''p<sub>i</sub>'' are continuous in some way.\n\nIn addition to being continuous, the canonical projections ''p<sub>i</sub>'' : ''X'' &rarr; ''X<sub>i</sub>'' are [[open map]]s. This means that any open subset of the product space remains open when projected down to the ''X<sub>i</sub>''. The converse is not true: if ''W'' is a [[subspace (topology)|subspace]] of the product space whose projections down to all the ''X<sub>i</sub>'' are open, then ''W'' need not be open in ''X''. (Consider for instance ''W'' = '''R'''<sup>2</sup> \\ (0,1)<sup>2</sup>.) The canonical projections are not generally [[closed map]]s (consider for example the closed set <math>\\{(x,y) \\in \\mathbb{R}^2 \\mid xy = 1\\},</math> whose projections onto both axes are '''R''' \\ {0}).\n\nThe product topology is also called the ''topology of pointwise convergence'' because of the following fact: a [[sequence]] (or [[Net (mathematics)|net]]) in ''X'' converges if and only if all its projections to the spaces ''X''<sub>''i''</sub> converge. In particular, if one considers the space ''X'' = '''R'''<sup>''I''</sup> of all [[real number|real]] valued [[function (mathematics)|function]]s on ''I'', convergence in the product topology is the same as pointwise convergence of functions.\n\nAny product of closed subsets of ''X<sub>i</sub>'' is a closed set in ''X''.\n\nAn important theorem about the product topology is [[Tychonoff's theorem]]: any product of [[compact space]]s is compact. This is easy to show for finite products, while the general statement is equivalent to the [[axiom of choice]].\n\n== Relation to other topological notions ==\n* Separation\n** Every product of [[T0 space|T<sub>0</sub> space]]s is T<sub>0</sub>\n** Every product of [[T1 space|T<sub>1</sub> space]]s is T<sub>1</sub>\n** Every product of [[Hausdorff space]]s is Hausdorff<ref>{{planetmath reference|id=4317|title=Product topology preserves the Hausdorff property}}</ref>\n** Every product of [[regular space]]s is regular\n** Every product of [[Tychonoff space]]s is Tychonoff\n** A product of [[normal space]]s ''need not'' be normal\n* Compactness\n** Every product of compact spaces is compact ([[Tychonoff's theorem]])\n** A product of [[locally compact space]]s ''need not'' be locally compact. However, an arbitrary product of locally compact spaces where all but finitely many are compact ''is'' locally compact (This condition is sufficient and necessary). \n* Connectedness\n** Every product of [[connectedness|connected]] (resp. path-connected) spaces is connected (resp. path-connected)\n** Every product of hereditarily disconnected spaces is hereditarily disconnected.\n* Metric spaces\n** Countable products of [[metric space]]s are [[metric space|metrizable]]\n\n==Axiom of choice==\nThe [[axiom of choice]] is equivalent to the statement that the product of a collection of non-empty sets is non-empty. The proof is easy enough: one needs only to pick an element from each set to find a representative in the product. Conversely, a representative of the product is a set which contains exactly one element from each component.\n\nThe axiom of choice occurs again in the study of (topological) product spaces; for example, [[Tychonoff's theorem]] on compact sets is a more complex and subtle example of a statement that is equivalent to the axiom of choice.\n\n== See also ==\n*[[Disjoint union (topology)]]\n*[[Initial topology|Projective limit topology]]\n*[[Quotient space (topology)|Quotient space]]\n*[[Subspace (topology)]]\n\n==Notes==\n{{reflist}}\n\n==References==\n*{{cite book |last=Willard |first=Stephen |title=General Topology |year=1970 |publisher=Addison-Wesley Pub. Co. |location=Reading, Mass. |isbn=0486434796 |url=http://store.doverpublications.com/0486434796.html |accessdate=13 February 2013}}\n\n==External links==\n* {{planetmath reference|id=3100|title=product topology}}\n\n[[Category:Topology]]\n[[Category:General topology]]\n[[Category:Binary operations]]"
    },
    {
      "title": "Products in algebraic topology",
      "url": "https://en.wikipedia.org/wiki/Products_in_algebraic_topology",
      "text": "{{multiple issues|\n{{Orphan|date=September 2013}}\n{{no references|date=August 2018}}\n}}\n\nIn [[algebraic topology]], several types of products are defined on homological and cohomological theories.\n\n==The cross product==\n\n<math>H_p(X) \\otimes H_q(Y) \\to H_{p+q}(X\\times Y)</math>\n\n==The cap product==\n{{Main| Cap product}}\n\n:<math>\\frown\\ : H_p(X;R)\\times H^q(X;R) \\rightarrow H_{p-q}(X;R)</math>\n\n==The slant product==\n{{Main| slant product}}\n\n:<math>\\backslash\\ : H_p(X;R)\\times H^q(X\\times Y;R) \\rightarrow H^{q-p}(Y;R)</math>\n\n==The cup product==\n{{Main| cup product}}\n\n:<math>H^p(X) \\otimes H^q(X) \\to H^{p+q}(X)</math>\n\nThis product can be understood as induced by the exterior product of [[differential form]]s in [[de Rham cohomology]]. It makes the singular cohomology of a connected manifold into a unitary supercommutative ring.\n\n==See also==\n*[[Singular homology]]\n*[[Differential graded algebra]]: the algebraic structure arising on the cochain level for the cup product\n*[[Poincaré duality]]: swaps some of these\n*[[Intersection theory]]: for a similar theory in [[algebraic geometry]]\n\n[[Category:Algebraic topology]]\n[[Category:Homology theory]]\n[[Category:Binary operations]]"
    },
    {
      "title": "Pythagorean addition",
      "url": "https://en.wikipedia.org/wiki/Pythagorean_addition",
      "text": "In [[mathematics]], '''Pythagorean addition''' is the following [[binary operation]] on the [[real number]]s:\n:<math>a \\oplus b = \\sqrt{a^2+b^2}.</math>\nThe name recalls the [[Pythagorean theorem]], which states that the length of the [[hypotenuse]] of a [[right triangle]] is {{nowrap|''a'' ⊕ ''b'',}} where ''a'' and ''b'' are the lengths of the other sides.\n\nThis operation provides a simple notation and terminology when the summands are complicated; for example, the [[energy-momentum relation]] in [[physics]] becomes\n:<math>E = mc^2 \\oplus pc.</math>\n\n==Properties==\nThe operation ⊕ is associative and commutative, and \n:<math>\\sqrt{x_1^2 + x_2^2 + \\cdots + x_n^2} = x_1 \\oplus x_2 \\oplus \\cdots \\oplus x_n</math>.\nThis is enough to form the real numbers into a [[commutative]] [[semigroup]]. However, ⊕ is not a [[Group (mathematics)|group]] operation for the following reasons.\n\nThe only element which could potentially act as an [[identity element]] is 0, since an identity ''e'' must satisfy ''e''⊕''e''&nbsp;=&nbsp;''e''.  This yields the equation <math>\\sqrt{2}e=e</math>, but if ''e'' is nonzero that implies <math>\\sqrt{2}=1</math>, so ''e'' could only be zero.  Unfortunately 0 does not work as an identity element after all, since 0⊕(−1)&nbsp;=&nbsp;1.  This does indicate, however, that if the operation ⊕ is restricted to nonnegative real numbers, then 0 ''does'' act as an identity. Consequently, the operation ⊕ acting on the nonnegative real numbers forms a commutative [[monoid]].\n\n==See also==\n*[[Euclidean distance]]\n*[[Hypot]] function\n*[[Alpha max plus beta min algorithm]]\n\n==Further reading==\n*{{cite journal |author=Moler, Cleve and Donald Morrison |title=Replacing Square Roots by Pythagorean Sums |journal=IBM Journal of Research and Development |volume=27 |issue=6 |pages=577–581 |year=1983 |url=http://www.research.ibm.com/journal/rd/276/ibmrd2706P.pdf |doi=10.1147/rd.276.0577 | citeseerx = 10.1.1.90.5651 }}.\n*{{cite journal |first=Augustin A. |last=Dubrulle |title=A Class of Numerical Methods for the Computation of Pythagorean Sums |journal=IBM Journal of Research and Development |volume=27 |issue=6 |pages=582–589 |year=1983 |url=http://www.research.ibm.com/journal/rd/276/ibmrd2706Q.pdf |doi=10.1147/rd.276.0582 | citeseerx = 10.1.1.94.3443 }}.\n\n[[Category:Binary operations]]"
    },
    {
      "title": "Quasi-commutative property",
      "url": "https://en.wikipedia.org/wiki/Quasi-commutative_property",
      "text": "In [[mathematics]], the '''quasi-commutative property''' is an extension or generalization of the general [[commutative property]]. This property is used in specific applications with various definitions.\n\n==Applied to matrices==\n\nTwo [[Matrix (mathematics)|matrices]] ''p'' and ''q'' are said to have the [[commutative property]] whenever\n:<math>pq = qp</math>\n\nThe quasi-commutative property in matrices is defined<ref name=McCoy>Neal H. McCoy. [http://www.ams.org/journals/tran/1934-036-02/S0002-9947-1934-1501746-8/S0002-9947-1934-1501746-8.pdf On quasi-commutative matrices. ''Transactions of the American Mathematical Society, 36''(2), 327–340].</ref> as follows. Given two non-commutable matrices ''x'' and ''y''\n:<math> xy - yx = z</math>\nsatisfy the quasi-commutative property whenever ''z'' satisfies the following properties:\n:<math> xz = zx </math>\n:<math> yz = zy </math>\n\nAn example is found in the [[matrix mechanics]] introduced by [[Werner Heisenberg|Heisenberg]] as a version of [[quantum mechanics]]. In this mechanics, ''p'' and ''q'' are infinite matrices corresponding respectively to the momentum and position variables of a particle.<ref name=McCoy/> These matrices are written out at [[Matrix mechanics#Harmonic oscillator]], and z = iħ times the infinite [[Identity matrix|unit matrix]], where ħ is the [[reduced Planck constant]].\n\n==Applied to functions==\n\nA function ''f'', defined as follows:\n:<math>f: X \\times Y \\rightarrow X</math>\nis said to be quasi-commutative<ref>Benaloh, J., & De Mare, M. (1994, January). [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.13.8287&rep=rep1&type=pdf One-way accumulators: A decentralized alternative to digital signatures]. In ''Advances in Cryptology—EUROCRYPT’93'' (pp. 274–285). Springer Berlin Heidelberg.</ref> if for all <math>x \\in X</math> and for all <math>y_1, y_2 \\in Y</math>,\n:<math>f(f(x,y_1),y_2) = f(f(x,y_2),y_1)</math>\n\n==See also==\n* [[Commutative property]]\n* [[Accumulator (cryptography)]]\n\n==References==\n{{reflist|30em}}\n\n[[Category:Mathematical relations]]\n[[Category:Binary operations]]"
    },
    {
      "title": "Relational operator",
      "url": "https://en.wikipedia.org/wiki/Relational_operator",
      "text": "{{redirect|Comparison (computer programming)|comparison of files|File comparison}}\n<!-- content well understood. -->{{Use mdy dates|date=June 2014}}\nIn [[computer science]], a '''relational operator''' is a [[programming language]] construct or [[Operator (programming)|operator]] that tests or defines some kind of [[relation (mathematics)|relation]] between [[Binary function|two entities]]. These include numerical [[equality (mathematics)|equality]] (''e.g.'', <!--can't use = within template-->{{nowrap|5 &#61; 5}}) and [[inequality (mathematics)|inequalities]] (''e.g.'', {{nowrap|4 ≥ 3}}).\n\nIn programming languages that include a distinct [[boolean data type]] in their [[type system]], like [[Pascal (programming language)|Pascal]], [[Ada (programming language)|Ada]], or [[Java (programming language)|Java]], these operators usually evaluate to true or false, depending on if the conditional relationship between the two [[operand]]s holds or not. In languages such as [[C (programming language)|C]], relational operators return the integers 0 or 1, where 0 stands for false and any non-zero value stands for true.\n\nAn [[expression (programming)|expression]] created using a relational operator forms what is termed a ''relational expression'' or a ''condition''. Relational operators can be seen as special cases of logical [[Predicate (mathematical logic)|predicates]].\n\n==Equality==\n\n===Usage===\nEquality is used in many programming-language constructs and data types. It is used to test if an element already exists in a [[set (computer science)|set]], or to access to a value through a key. It is used in [[switch statement]]s to dispatch the control flow to the correct branch, and during the unification process in logic programming.\n\nOne possible meaning of equality is that \"if a equals to b, then we can use either a or b interchangeably in any context without noticing any difference\". But this statement does not necessarily hold, particularly when taking into account mutability together with content equality.\n\n===Sameness (object identity) vs. content equality ===\nSometimes, particularly in [[object-oriented programming]], the comparison raises questions of [[data type]]s and [[inheritance (computer science)|inheritance]], [[equality (mathematics)|equality]], and [[identity (mathematics)|identity]]. It is often necessary to distinguish between:\n* two different objects of the same type, ''e.g.'', two hands\n* two objects being equal but distinct, ''e.g.'', two $10 banknotes\n* two objects being equal but have different representation, ''e.g.'', a $1 bill and a $1 coin\n* two different references to the same object, ''e.g.'', two nicknames for the same person\n\nIn many modern programming languages, objects and data structures are accessed through [[Reference (computer science)|references]]. In such languages, there becomes a need to test for two different types of equality:\n* Physical equality: if two references (A and B) reference the same object. Interactions with the object through A are indistinguishable from the same interactions through B, and in particular changes to the object through A are reflected through B. Physical identity is not applicable when talking about values instead of objects.\n* Semantic equality: if the objects referenced by two references, or if two values, are equivalent in some sense:\n:* Structural equality (''i.e.'', their contents are the same). which may be either shallow (testing only immediate subparts), or deep (testing for equality of subparts recursively). A simple way to achieve this is through representational equality: checking that the values have the same representation.\n:* Some other tailor-made equality, preserving the external behavior. For example, 1/2 and 2/4 are considered equal when seen as a rational number. A possible requirement would be that \"A = B if and only if all operations on objects A and B will have the same result\", in addition to [[reflexive relation|reflexivity]], [[symmetry]], and [[transitive relation|transitivity]].\n\nThe first type of equality usually implies the second (except for things like ''not a number'' ([[NaN]]) which are unequal to themselves), but the converse is not necessarily true. For example, two [[String (computer science)|string]] objects may be distinct objects (unequal in the first sense) but contain the same sequence of characters (equal in the second sense). See [[identity (object-oriented programming)|identity]] for more of this issue.\n\nReal numbers, including many simple [[Fraction (mathematics)|fractions]], cannot be represented exactly in [[floating-point arithmetic]], and it may be necessary to test for equality within a given tolerance. Such tolerance, however, can easily break desired properties such as transitivity, whereas reflexivity breaks too: the [[IEEE floating point]] standard requires that <code>Nan ≠ NaN</code> holds.\n\nOther programming elements such as computable functions, may either have no sense of equality, or an equality that is uncomputable. For these reasons, some languages define an explicit notion of \"comparable\", in the form of a base class, an interface, a trait or a protocol, which is used either explicitly, by declaration in source code, or implicitly, via the structure of the type involved.\n\n===Comparing values of different types===\nIn [[JavaScript]], [[PHP]], [[VBScript]] and a few other [[Type system#DYNAMIC|dynamically typed]] languages, the standard equality operator evaluates to ''true'' if two values are equal, even if they have different types, making the number 4 compare equal to the text string \"4\", for instance. A typed equality operator is often available also, in such languages, returning true only for values with identical or equivalent types (in PHP 5, <code>4 === \"4\"</code> is false although <code>4 == \"4\"</code> is true).<ref>{{cite web|url=http://www.php.net/manual/en/language.oop5.object-comparison.php |title=Comparing Objects |work=PHP Manual |publisher=PHP Group |date= |accessdate=June 29, 2014 |author=[http://www.php.net/manual/en/preface.php#contributors Contributors]}}</ref><ref name=\"php\">{{cite web|url=http://php.net/manual/en/language.operators.comparison.php |title=PHP: Comparison Operators - Manual|accessdate=July 31, 2008}}</ref> For languages where the number 0 may be interpreted as ''false'', this operator may simplify things such as checking for zero (as <code>x == 0</code> would be true for x being either 0 or \"0\" using the type agnostic equality operator).\n\n== Ordering ==\n''Greater than'' and ''less than'' comparison of non-numeric data is performed according to a sort convention (such as, for text strings, [[lexicographical order]]) which may be built into the programming language and/or configurable by a programmer.\n\nWhen it is desired to associate a numeric value with the result of a comparison between two data items, say ''a'' and ''b'', the usual convention is to assign &minus;1 if a < b, 0 if a = b and 1 if a > b. For example, the C function <code>[[strcmp]]</code> performs a [[three-way comparison]] and returns &minus;1, 0, or 1 according to this convention, and [[qsort]] expects the comparison function to return values according to this convention. In [[sorting algorithm]]s, the efficiency of comparison code is critical since it is one of the major factors contributing to sorting performance.\n\nComparison of programmer-defined [[data type]]s (data types for which the programming language has no in-built understanding) may be carried out by custom-written or library functions (such as <code>strcmp</code> mentioned above), or, in some languages, by ''[[Operator overloading|overloading]]'' a comparison operator – that is, assigning a programmer-defined meaning that depends on the data types being compared. Another alternative is using some convention such as memberwise comparison.\n\n==Logical equivalence==\nThough perhaps unobvious at first, like the [[Boolean logic|boolean]] [[logical operator]]s XOR, AND, OR, and NOT, relational operators can be designed to have [[logical equivalence]], such that they can all be defined in terms of one another. The following four conditional statements all have the same logical equivalence ''E'' (either all true or all false) for any given ''x'' and ''y'' values:\n:<math>\nE = \\begin{cases}\nx < y \\\\\ny > x \\\\\nx \\ngeq y \\\\\ny \\nleq x\n\\end{cases}</math>\n\nThis relies on the domain being [[well ordered]].\n\n==Standard relational operators==\n\nThe most common numerical relational operators used in programming languages are shown below.\n\n{| class=\"wikitable\" style=\"text-align: center;\"\n|+ Common relational operators\n! Convention\n! ''equal to''\n! ''not equal to''\n! ''greater than''\n! ''less than''\n! ''greater than<br>or equal to''\n! ''less than<br>or equal to''\n|-\n! In print\n| =\n| ≠\n| &gt;\n| &lt;\n| ≥\n| ≤\n|-\n![[FORTRAN]]<ref group=\"note\">Including FORTRAN II, III, IV, 66 and 77.</ref> \n| <code>.EQ.</code>\n| <code>.NE.</code>\n| <code>.GT.</code>\n| <code>.LT.</code>\n| <code>.GE.</code>\n| <code>.LE.</code>\n|-\n! rowspan=3 | [[ALGOL 68]]<ref group=\"note\">[[ALGOL 68]]: ''[[Stropping (syntax)|stropping]]'' regimes are used in code on platforms with limited character sets (''e.g.'', use <code>>=</code> or <code>GE</code> instead of <code>≥</code>), platforms with no <code>'''bold'''</code> [[Emphasis (typography)|emphasis]] (use <code>'ge'</code>), or platforms with only [[UPPERCASE]] (use <code>.GE</code> ''or'' <code>'GE'</code>).</ref>\n|rowspan=2| <code>=</code>\n| <code>≠</code>\n|rowspan=2| <code>&gt;</code>\n|rowspan=2| <code>&lt;</code>\n| <code>≥</code>\n| <code>≤</code>\n|-\n| <code>/=</code>\n| <code>&gt;=</code>\n| <code>&lt;=</code>\n|-\n| <code>'''eq'''</code>\n| <code>'''ne'''</code>\n| <code>'''gt'''</code>\n| <code>'''lt'''</code>\n| <code>'''ge'''</code>\n| <code>'''le'''</code>\n|-\n! [[APL (programming language)|APL]]\n| <code>=</code>\n| <code>≠</code>\n| <code>&gt;</code>\n| <code>&lt;</code>\n| <code>≥</code>\n| <code>≤</code>\n|-\n! [[BASIC]], [[ML (programming language)|ML]], [[Pascal (programming language)|Pascal]]<ref group=\"note\">Including [[ALGOL]], [[Simula]], [[Modula-2]], [[Eiffel (programming language)|Eiffel]], [[SQL]], [[spreadsheet formulas]], and others.</ref>\n| <code>=</code>\n| <code>&lt;&gt;</code>\n| <code>&gt;</code>\n| <code>&lt;</code>\n| <code>&gt;=</code>\n| <code>&lt;=</code>\n|-\n! [[MUMPS]]\n| <code>=</code>\n| <code>'=</code>\n| <code>&gt;</code>\n| <code>&lt;</code>\n| <code>'&lt;</code>\n| <code>'&gt;</code>\n|-\n! [[Lua (programming language)|Lua]]\n| <code>==</code>\n| <code>~=</code>\n| <code>&gt;</code>\n| <code>&lt;</code>\n| <code>&gt;=</code>\n| <code>&lt;=</code>\n|-\n! [[C (programming language)|C-like]]<ref group=\"note\">Including [[C (programming language)|C]], [[C++]], [[C Sharp (programming language)|C#]], [[Go (programming language)|Go]], [[Java (programming language)|Java]], [[JavaScript]], [[Perl]] (numerical comparison only), [[PHP]], [[Python (programming language)|Python]], [[Ruby (programming language)|Ruby]], and [[R (programming language)|R]].</ref>\n| <code>==</code>\n| <code>!=</code>\n| <code>&gt;</code>\n| <code>&lt;</code>\n| <code>&gt;=</code>\n| <code>&lt;=</code>\n|-\n! rowspan=\"2\" |[[Erlang (programming language)|Erlang]]\n|<code>==</code>\n|<code>!=</code>\n| rowspan=\"2\" |<code>&gt;</code>\n| rowspan=\"2\" |<code>&lt;</code>\n| rowspan=\"2\" |<code>&gt;=</code>\n| rowspan=\"2\" |<code>=<</code>\n|-\n|<code>=:=</code>\n|<code>=/=</code>\n|-\n! [[Unix shell#Bourne shell compatible|Bourne-like]] [[Shell (computing)|shells]]<ref group=\"note\">Including [[Bourne shell]], [[Bash (Unix shell)|Bash]], [[Korn shell]], and [[Windows PowerShell]]. The symbols <code>&lt;</code> and <code>&gt;</code> are usually used in a shell for [[Redirection (computing)|redirection]], so other symbols must be used. Without the hyphen, is used in [[Perl]] for string comparison.</ref>\n| <code>-eq</code>\n| <code>-ne</code>\n| <code>'''-gt'''</code>\n| <code>-lt</code>\n| <code>-ge</code>\n| <code>-le</code>\n|-\n! [[Batch file]]\n| <code>EQU</code>\n| <code>NEQ</code>\n| <code>GTR</code>\n| <code>LSS</code>\n| <code>GEQ</code>\n| <code>LEQ</code>\n|-\n! rowspan=\"2\" |[[MATLAB]]<ref group=\"note\">MATLAB, although in other respects using similar syntax as C, does not use <code>!=</code>, as <code>!</code> in MATLAB sends the following text as a command line to the [[operating system]]. The first form is also used in [[Smalltalk]], with the exception of equality, which is <code>=</code>.</ref>\n| <code>==</code>\n| <code>~=</code>\n| <code>&gt;</code>\n| <code>&lt;</code>\n| <code>&gt;=</code>\n| <code>&lt;=</code>\n|-\n| <code>eq(x,y)</code>\n| <code>ne(x,y)</code>\n| <code>gt(x,y)</code>\n| <code>lt(x,y)</code>\n| <code>ge(x,y)</code>\n| <code>le(x,y)</code>\n|-\n! [[Fortran 90]]<ref group=\"note\">Including FORTRAN 95, 2003, 2008 and 2015.</ref>, [[Haskell (programming language)|Haskell]]\n| <code>==</code>\n| <code>/=</code>\n| <code>&gt;</code>\n| <code>&lt;</code>\n| <code>&gt;=</code>\n| <code>&lt;=</code>\n|-\n! rowspan=\"2\" |[[Mathematica]]<ref>[http://reference.wolfram.com/mathematica/tutorial/RelationalAndLogicalOperators.html Relational and Logical Operators] of [[Mathematica]]</ref>\n| <code>==</code>\n| <code>!=</code>\n| <code>&gt;</code>\n| <code>&lt;</code>\n| <code>&gt;=</code>\n| <code>&lt;=</code>\n|-\n| <code>Equal[x,y]</code>\n| <code>Unequal[x,y]</code>\n| <code>Greater[x,y]</code>\n| <code>Less[x,y]</code>\n| <code>GreaterEqual[x,y]</code>\n| <code>LessEqual[x,y]</code>\n|}\n{{Reflist|group=\"note\"}}\n\nOther conventions are less common: [[Common Lisp]] and [[Macsyma]]/[[Maxima (software)|Maxima]] use Basic-like operators except for inequality, which is <code>/=</code> in Common Lisp and <code>#</code> in Macsyma/Maxima. Older [[Lisp (programming language)|Lisps]] used <code>equal</code>, <code>greaterp</code>, and <code>lessp</code>; and negated them using <code>not</code> for the remaining operators.\n\n==Syntax==\nRelational operators are also used in technical literature instead of words. Relational operators are usually written in [[infix notation]], if supported by the programming language, which means that they appear between their operands (the two expressions being related). For example, an expression in Python will print the message if the ''x'' is less than ''y'':\n<source lang=\"python\">\nif x < y:\n    print(\"x is less than y in this example\")\n</source>\n\nOther programming languages, such as [[Lisp (programming language)|Lisp]], use [[prefix notation]], as follows:\n<source lang=\"lisp\">\n(>= X Y)\n</source>\n\n===Operator chaining===\nIn mathematics, it is common practice to chain relational operators, such as in 3 < x < y < 20 (meaning 3 < x ''and'' x < y ''and'' y < 20). The syntax is clear since these relational operators in mathematics are transitive.\n\nHowever, many recent programming languages would see an expression like 3 < x < y as consisting of two left (or right-) associative operators, interpreting it as something like <code>(3 < x) < y</code>. If we say that x=4, we then get <code>(3 < 4) < y</code>, and evaluation will give <code>true < y</code> which generally does not make sense. However, it does compile in C/C++ and some other languages, yielding surprising result (as ''true'' would be represented by the number 1 here).\n\nIt is possible to give the expression <code>x < y < z</code> its familiar mathematical meaning, and some programming languages such as Python and Perl 6 do that. Others, such as C# and Java, do not, partly because it would differ from the way most other infix operators work in C-like languages. The D programming language do not do that since it maintains some compatibility with C, and \"Allowing C expressions but with subtly different semantics (albeit arguably in the right direction) would add more confusion than convenience\".<ref>{{cite book |last=Alexandrescu |first=Andrei |date= |title=The D Programming Language |url= |location= |publisher=Addison Wesley |page=58 |isbn=978-0-321-63536-5}}</ref>\n\nSome languages, like [[Common Lisp]], use multiple argument predicates for this. In Lisp <code>(<= 1 x 10)</code> is true when x is between 1 and 10.\n\n===Confusion with assignment operators===\n{{See also|Assignment (computer science)#Assignment versus equality}}\nEarly FORTRAN (1956–57) was bounded by heavily restricted character sets where <code>=</code> was the only relational operator available. There were no <code><</code> or <code>></code> (and certainly no <code>≤</code> or <code>≥</code>). This forced the designers to define symbols such as <code>.GT.</code>, <code>.LT.</code>, <code>.GE.</code>, <code>.EQ.</code> etc. and subsequently made it tempting to use the remaining <code>=</code> character for copying, despite the obvious incoherence with mathematical usage (<code>X=X+1</code> should be impossible).\n\nInternational Algebraic Language (IAL, [[ALGOL 58]]) and [[ALGOL]] (1958 and 1960) thus introduced <code>:=</code> for assignment, leaving the standard <code>=</code> available for equality, a convention followed by [[CPL (programming language)|CPL]], [[ALGOL W]], [[ALGOL 68]], Basic Combined Programming Language ([[BCPL]]), [[Simula]], SET Language ([[SETL]]), [[Pascal (programming language)|Pascal]], [[Smalltalk]], [[Modula-2]], [[Ada (programming language)|Ada]], [[Standard ML]], [[OCaml]], [[Eiffel (programming language)|Eiffel]], [[Object Pascal]] ([[Delphi (programming language)|Delphi]]), [[Oberon]], [[Dylan (programming language)|Dylan]], VHSIC Hardware Description Language ([[VHDL]]), and several other languages.\n\n====B and C====\nThis uniform de facto standard among most programming languages was eventually changed, indirectly, by a minimalist compiled language named [[B (programming language)|B]]. Its sole intended application was as a vehicle for a first port of (a then very primitive) [[Unix]], but it also evolved into the very influential [[C (programming language)|C]] language.\n\nB started off as a syntactically changed variant of the systems programming language [[BCPL]], a simplified (and typeless) version of [[CPL (programming language)|CPL]]. In what has been described as a \"strip-down\" process, the <code>and</code> and <code>or</code> operators of BCPL<ref>Used not only in ALGOL-like languages, but also in FORTRAN and BASIC</ref> were replaced with <code>&</code> and <code>|</code> (which would later become <code>&&</code> and <code>||</code>, respectively.<ref>As some programmers were confused by the dual meanings (bitwise operator, and logical connective) of these new symbols (according to [[Dennis Ritchie]]). Only the bitwise meaning of & and | were kept.</ref>). In the same process, the ALGOL style <code>:=</code> of BCPL was replaced by <code>=</code> in B. The reason for all this being unknown.<ref>Although [[Dennis Ritchie]] has suggested that this may have had to do with \"economy of typing\" as updates of variables may be more frequent than comparisons in certain types of programs</ref> As variable updates had no special syntax in B (such as <code>let</code> or similar) and were allowed in expressions, this non standard meaning of the equal sign meant that the traditional semantics of the equal sign now had to be associated with another symbol. [[Ken Thompson]] used the ad hoc <code>==</code> combination for this.\n\nAs a small type system was later introduced, B then became C. The popularity of this language along with its association with Unix, led to Java, C#, and many other languages following suit, syntactically, despite this needless conflict with the mathematical meaning of the equal sign.\n\n====Languages====\nAssignments in C have a [[value (programming)|value]] and since any non-zero scalar value is interpreted as ''true'' in [[Conditional (programming)|conditional expression]]s,<ref>A zero scalar value is interpreted as false while any non-zero scalar value is interpreted as true; this is typically used with integer types, similar to [[assembly language]] idioms.</ref> the code <code>if (x = y)</code> is legal, but has a very different meaning from <code>if (x == y)</code>. The former code fragment means \"assign ''y'' to ''x'', and if the new value of ''x'' is not zero, execute the following statement\". The latter fragment means \"[[if and only if]] ''x'' is equal to ''y'', execute the following statement\".<ref name=\"kandr\">{{cite book |title=The C Programming Language |author=Brian Kernighan and Dennis Ritchie |publisher=Prentice Hall |origyear=1978 |year=1988 |edition=Second}}, 19</ref>\n<source lang=\"c\">\n  int x = 1;\n  int y = 2;\n  if (x = y) {\n      /* This code will always execute if y is anything but 0*/\n      printf(\"x is %d and y is %d\\n\", x, y);\n  }\n</source>\n\nThough [[Java (programming language)|Java]] and [[C Sharp (programming language)|C#]] have the same operators as C, this mistake usually causes a compile error in these languages instead, because the if-condition must be of type <code>boolean</code>, and there is no implicit way to convert from other types (''e.g.'', numbers) into <code>boolean</code>s. So unless the variable that is assigned to has type <code>boolean</code> (or wrapper type <code>Boolean</code>), there will be a compile error.\n\nIn ALGOL-like languages such as Pascal, Delphi, and Ada (in the sense that they allow [[nested function definition]]s), and in [[Python (programming language)|Python]], and many functional languages, among others, assignment operators cannot appear in an [[expression (programming)|expression]] (including <code>if</code> clauses), thus precluding this class of error. Some compilers, such as [[GNU Compiler Collection]] (GCC), provide a warning when compiling code containing an assignment operator inside an if statement, though there are some legitimate uses of an assignment inside an if-condition. In such cases, the assignment must be wrapped in an extra pair of parentheses explicitly, to avoid the warning.\n\nSimilarly, some languages, such as [[BASIC]] use just the <code>=</code> symbol for both assignment ''and'' equality, as they are syntactically separate (as with Pascal, Ada, Python, etc., assignment operators cannot appear in expressions).\n\nSome programmers get in the habit of writing comparisons against a constant in the reverse of the usual order:\n\n<source lang=\"c\">\n  if (2 == a) {   /* Mistaken use of = versus == would be a compile-time error */\n  }\n</source>\n\nIf <code>=</code> is used accidentally, the resulting code is invalid because 2 is not a variable. The compiler will generate an error message, on which the proper operator can be substituted. This coding style is termed left-hand comparison, or [[Yoda conditions]].\n\nThis table lists the different mechanisms to test for these two types of equality in various languages:\n{| class=\"wikitable\"\n|-\n! Language !! Physical equality !! Structural equality !! Notes\n|-\n| [[ALGOL 68]] || <code>a :=: b</code> ''or'' <code>a ''is'' b</code> || <code>a = b</code> || when <code>a</code> and <code>b</code> are pointers\n|-\n| [[C (programming language)|C]], [[C++]] || <code>a == b</code> || <code>*a == *b</code> || when <code>a</code> and <code>b</code> are pointers\n|-\n| [[C Sharp (programming language)|C#]] || <code>object.ReferenceEquals(a, b)</code> || <code>a.Equals(b)</code> || The <code>==</code> operator defaults to <code>ReferenceEquals</code>, but can be [[Operator overloading|overloaded]] to perform <code>Equals</code> instead.\n|-\n| [[Common Lisp]] || <code>(eq a b)</code> || <code>(equal a b)</code> || \n|-\n|[[Erlang (programming language)|Erlang]]\n|<code>a =:= b</code>\n|<code>a == b</code>\n|when a and b are numbers\n|-\n| [[Go (programming language)|Go]] || <code>a == b</code> || <code>reflect.DeepEqual(*a, *b)</code> || when a and b are pointers\n|-\n| [[Java (programming language)|Java]] || <code>a == b</code> || <code>a.equals(b)</code> ||\n|-\n| [[JavaScript]] || <code>a === b</code> || <code>a == b</code> || when a and b are two string objects containing equivalent characters, the === operator will still return true.\n|-\n| [[OCaml]], [[Smalltalk]] || <code>a == b</code> || <code>a = b</code> || \n|-\n| [[Pascal (programming language)|Pascal]] || <code>a^ = b^</code> ||<code>a = b</code> || \n|-\n| [[Perl]] || <code>$a == $b</code> || <code>$$a == $$b</code> || when <code>$a</code> and <code>$b</code> are references to scalars\n|-\n| [[PHP]]5 || <code>$a === $b</code> || <code>$a == $b</code> || when <code>$a</code> and <code>$b</code> are objects\n|-\n| [[Python (programming language)|Python]] || <code>a is b</code> || <code>a == b</code> || \n|-\n| [[Ruby (programming language)|Ruby]] || <code>a.equal?(b)</code> || <code>a == b</code> || \n|-\n| [[Scheme (programming language)|Scheme]] || <code>(eq? a b)</code> || <code>(equal? a b)</code> || \n|-\n| [[Swift (programming language)|Swift]] || <code>a === b</code> || <code>a == b</code> || when a and b have class type\n|-\n| [[Visual Basic .NET]]<ref group=\"inequality\">Patent application: On May 14, 2003, {{US patent application|20040230959}} \"IS NOT OPERATOR\" was filed for the <code>ISNOT</code> operator by employees of [[Microsoft]].  This patent was granted on November 18, 2004.</ref>|| <code>a Is b</code> or <code>object.ReferenceEquals(a, b)</code> || <code>a = b</code> or <code>a.Equals(b)</code> || Same as C#\n|-\n| [[Objective-C]] ([[Cocoa (API)|Cocoa]], [[GNUstep]]) || <code>a == b</code> || <code>[a isEqual:b]</code> || when <code>a</code> and <code>b</code> are pointers to objects that are instances of <code>NSObject</code>\n|}\n{{Reflist|group=\"inequality\"}}\n\nRuby uses <code>a === b</code> to mean \"b is a member of the set a\", though the details of what it means to be a member vary considerably depending on the data types involved.  <code>===</code> is here known as the \"case equality\" or \"case subsumption\" operator.\n\n==See also==\n* [[Binary relation]]\n* [[Common operator notation]]\n* [[Equality (mathematics)]]\n* [[Equals sign]]\n* [[Logical operator]]\n* [[Operation (mathematics)]]\n* [[Operator (mathematics)]]\n* [[Operator (computer programming)]]\n* [[Spaceship operator]]\n* [[Triadic relation]]\n\n==Notes and references==\n{{Reflist}}\n\n[[Category:Operators (programming)]]\n[[Category:Binary operations]]\n[[Category:Comparison (mathematical)]]"
    },
    {
      "title": "Replacement product",
      "url": "https://en.wikipedia.org/wiki/Replacement_product",
      "text": "In [[graph theory]], the '''replacement product''' of two graphs is a [[graph product]] that can be used to reduce the [[degree (graph theory)|degree]] of a graph while maintaining its [[connectivity (graph theory)|connectivity]].<ref>{{cite journal|last1=Hoory|first1=Shlomo|last2=Linial|first2=Nathan|last3=Wigderson|first3=Avi|title=Expander graphs and their applications|journal=Bulletin of the American Mathematical Society|date=7 August 2006|volume=43|issue=4|pages=439–562|doi=10.1090/S0273-0979-06-01126-8}}</ref>\n\nSuppose ''G'' is a ''d''-[[regular graph]] and ''H'' is an ''e''-regular graph with vertex set {0,&nbsp;&hellip;,&nbsp;''d''&nbsp;&minus;&nbsp;1}. Let ''R'' denote the replacement product of ''G'' and ''H''. The vertex set of ''R'' is the [[Cartesian product]] ''V''(''G'')&nbsp;&times;&nbsp;''V''(''H''). For each vertex ''u'' in ''V''(''G'') and for each edge (''i'',&nbsp;''j'') in ''E''(''H''), the vertex (''u'',&nbsp;''i'') is adjacent to (''u'',&nbsp;''j'') in ''R''. Furthermore, for each edge (''u'',&nbsp;''v'') in ''E''(''G''), if ''v'' is the ''i''th neighbor of ''u'' and ''u'' is the ''j''th neighbor of ''v'', the vertex (''u'',&nbsp;''i'') is adjacent to (''v'',&nbsp;''j'') in&nbsp;''R''.\n\nIf ''H'' is an ''e''-regular graph, then ''R'' is an (''e''&nbsp;+&nbsp;1)-regular graph.\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* {{cite web |url=https://lucatrevisan.wordpress.com/2011/03/07/cs359g-lecture-17-the-zig-zag-product/ |title=CS359G Lecture 17: The Zig-Zag Product |last1=Trevisan |first1=Luca |date=7 March 2011 |website= |publisher= |accessdate=16 December 2014}} \n\n[[Category:Binary operations]]\n[[Category:Graph products]]\n\n{{Combin-stub}}"
    },
    {
      "title": "Schouten–Nijenhuis bracket",
      "url": "https://en.wikipedia.org/wiki/Schouten%E2%80%93Nijenhuis_bracket",
      "text": "In [[differential geometry]], the '''Schouten–Nijenhuis bracket''', also known as the '''Schouten bracket''', is a type of [[graded Lie algebra|graded Lie bracket]] defined on [[multivector]] [[vector field|fields]] on a [[smooth manifold]] extending the [[Lie bracket of vector fields]].  There are two  different versions, both rather confusingly called by the same name. The most common version is  defined on alternating multivector fields and makes them into a [[Gerstenhaber algebra]], but there is also another version defined on symmetric multivector fields, which is more or less the same as the [[Poisson bracket]] on the [[cotangent bundle]].  It was discovered by  [[Jan Arnoldus Schouten]] (1940, 1953) and its properties were investigated by his student [[Albert Nijenhuis]] (1955). It is related to but not the same as the [[Nijenhuis–Richardson bracket]] and the [[Frölicher–Nijenhuis bracket]].\n\n==Definition and properties==\nAn alternating multivector field is a section of the [[exterior algebra]] ∧<sup>∗</sup>T''M'' over the [[tangent bundle]] of a manifold ''M''.  The alternating multivector fields form a graded supercommutative ring with the product of ''a'' and ''b'' written as ''ab'' (some authors use ''a''∧''b'').  This is dual to the usual algebra of [[differential form]]s Ω<sup>∗</sup>''M'' by the pairing on homogeneous elements:\n: <math>\\omega(a_1a_2 \\dots a_p)=\\left\\{\n\\begin{matrix}\n\\omega(a_1,\\dots,a_p)&(\\omega\\in \\Omega^pM)\\\\\n0&(\\omega\\not\\in\\Omega^pM)\n\\end{matrix}\\right.\n</math>\nThe '''degree''' of a multivector ''A'' in ∧<sup>''p''</sup>T''M'' is defined to be |''A''| = ''p''.\n\nThe skew symmetric Schouten–Nijenhuis bracket is the unique extension of the [[Lie bracket of vector fields]] to a graded bracket on the space of alternating multivector fields that makes the alternating multivector fields into a [[Gerstenhaber algebra]].\nIt is given in terms of the Lie bracket of vector fields by \n:<math>[a_1\\cdots a_m,b_1\\cdots b_n]=\\sum_{i,j}(-1)^{i+j}[a_i,b_j]a_1\\cdots a_{i-1}a_{i+1}\\cdots a_mb_1\\cdots b_{j-1}b_{j+1}\\cdots b_n</math>\nfor vector fields ''a''<sub>''i''</sub>, ''b''<sub>''j''</sub> and\n:<math>[f,a_1\\cdots a_m] = -i_{df}(a_1 \\cdots a_m)</math>\nfor vector fields ''a''<sub>''i''</sub> and smooth function ''f'', where ''i''<sub>''df''</sub>  is the common [[interior product]] operator.   \nIt has  the following properties.\n*|''ab''| = |''a''| + |''b''|      (The product has degree 0)\n*|[''a'',''b'']| = |''a''| + |''b''| &minus; 1  (The Schouten–Nijenhuis bracket has degree &minus;1)\n*(''ab'')''c'' = ''a''(''bc''), ''ab'' = (&minus;1)<sup>|''a''||''b''|</sup>''ba''  (the product is associative and (super) commutative)\n*[''a'',&nbsp;''bc''] = [''a'',&nbsp;''b'']''c'' + (&minus;1)<sup>|''b''|(|''a''|&nbsp;&minus;&nbsp;1)</sup>''b''[''a'',&nbsp;''c''] (Poisson identity)\n*[''a'',''b''] = &minus;(&minus;1)<sup>(|''a''|&nbsp;&minus;&nbsp;1)(|''b''|&nbsp;&minus;&nbsp;1)</sup> [''b'',''a''] (Antisymmetry of Schouten–Nijenhuis bracket)\n*[[''a'',''b''],''c''] = [''a'',[''b'',''c'']] &minus; (&minus;1)<sup>(|''a''|&nbsp;&minus;&nbsp;1)(|''b''|&nbsp;&minus;&nbsp;1)</sup>[''b'',[''a'',''c'']]  (Jacobi identity for Schouten–Nijenhuis bracket)\n* If ''f'' and ''g'' are functions (multivectors homogeneous of degree 0), then [''f'',''g''] = 0.\n* If ''a'' is a vector field, then [''a'',''b''] = '''L'''<sub>''a''</sub>''b'' is the usual [[Lie derivative]] of the multivector field ''b'' along ''a'', and in particular if ''a'' and ''b'' are vector fields then the Schouten–Nijenhuis bracket is the usual Lie bracket of vector fields.\n\nThe Schouten–Nijenhuis bracket makes the multivector fields into a Lie superalgebra if the grading \nis changed to the one of opposite parity (so that the even and odd subspaces are switched), though\nwith this new grading it is no longer a supercommutative ring.  Accordingly, the Jacobi identity may also be expressed in the symmetrical form\n:<math>(-1)^{(|a|-1)(|c|-1)}[a,[b,c]]+(-1)^{(|b|-1)(|a|-1)}[b,[c,a]]+(-1)^{(|c|-1)(|b|-1)}[c,[a,b]] = 0.\\,</math>\n\n==Generalizations==\nThere is a common generalization of the Schouten–Nijenhuis bracket for alternating multivector fields and the [[Frölicher–Nijenhuis bracket]] due to Vinogradov (1990).\n\nA version of the Schouten–Nijenhuis bracket can also be defined for symmetric multivector fields in a similar way. The symmetric multivector fields can be identified with  functions on the cotangent space ''T''<sup>*</sup>(''M'') of ''M'' that are polynomial in the fiber, and under this identification the symmetric Schouten–Nijenhuis bracket corresponds to the [[Poisson bracket]] of functions on the [[symplectic manifold]] ''T''<sup>*</sup>(''M'').\nThere is a common generalization of the Schouten–Nijenhuis bracket for symmetric multivector fields and the [[Frölicher–Nijenhuis bracket]] due to Dubois-Violette and [[Peter W. Michor]] (1995).\n\n==References==\n*{{Cite journal |first=Michel |last=Dubois-Violette |first2=Peter W. |last2=Michor |title=A common generalization of the Frölicher–Nijenhuis bracket and the Schouten bracket for symmetric multi vector fields |arxiv=alg-geom/9401006  |journal=Indag. Mathem. |volume=6 |issue=1 |year=1995 |pages=51–66 |doi= 10.1016/0019-3577(95)98200-u}}\n*{{Cite journal |first=Charles-Michel |last=Marle |url=http://charles-michel.marle.pagesperso-orange.fr/publications/schouten.pdf |title=The Schouten-Nijenhuis bracket and interior products |journal=Journal of Geometry and Physics |volume=23 |issue= 3–4|pages=350–359 |year=1997 |doi= 10.1016/s0393-0440(97)80009-5|bibcode=1997JGP....23..350M |citeseerx=10.1.1.27.5358 }}\n*{{Cite journal |first=A. |last=Nijenhuis |title=Jacobi-type identities for bilinear differential concomitants of certain tensor fields I |journal=Indagationes Math. |volume=17 |issue= |year=1955 |pages=390–403 |doi= 10.1016/S1385-7258(55)50054-0}}\n*{{Cite journal |first=J. A. |last=Schouten |title=Über Differentialkonkomitanten zweier kontravarianten Grössen |journal=Indag. Math. |volume=2 |issue= |year=1940 |pages=449–452 }}\n*{{Cite book |first=J. A. |last=Schouten |chapter=On the differential operators of the first order in tensor calculus |title=Convegno Int. Geom. Diff. Italia |year=1953 |editor=Cremonese |pages=1–7 }}\n*{{Cite journal |first=A. M. |last=Vinogradov |title=Unification of Schouten–Nijenhuis and Frölicher–Nijenhuis brackets, cohomology and super differential operators |journal=Sov. Math. Zametki |volume=47 |issue= |year=1990 |pages= |doi= }}\n\n==External links==\n*Nicola Ciccoli [https://web.archive.org/web/20070927223438/http://toknotes.mimuw.edu.pl/sem4/online/node9.html ''Schouten–Nijenhuis bracket''] in notes on [https://web.archive.org/web/20070927223836/http://toknotes.mimuw.edu.pl/sem4/online/fpqg.html ''From Poisson to Quantum Geometry'']\n\n{{DEFAULTSORT:Schouten-Nijenhuis bracket}}\n[[Category:Binary operations]]\n[[Category:Bilinear operators]]\n[[Category:Differential geometry]]"
    },
    {
      "title": "Smash product",
      "url": "https://en.wikipedia.org/wiki/Smash_product",
      "text": "{{for|the smash product in the theory of Hopf algebras|Hopf smash product}}\n\nIn [[mathematics]], the '''smash product''' of two [[pointed space]]s (i.e. [[topological space]]s with distinguished basepoints) (''X,'' ''x''<sub>0</sub>) and (''Y, y<sub>0</sub>'') is the [[Quotient space (topology)|quotient]] of the [[product space]] ''X'' &times; ''Y'' under the identifications (''x'',&nbsp;''y''<sub>0</sub>)&nbsp;∼&nbsp;(''x''<sub>0</sub>,&nbsp;''y'') for all ''x''&nbsp;∈&nbsp;''X'' and ''y''&nbsp;∈&nbsp;''Y''.  The smash product is itself a pointed space, with basepoint being the equivalence class of  (''x''<sub>0</sub>, ''y''<sub>0</sub>). The smash product is usually denoted ''X''&nbsp;∧&nbsp;''Y'' or ''X''&nbsp;⨳&nbsp;''Y''. The smash product depends on the choice of basepoints (unless both ''X'' and ''Y'' are [[homogeneous space|homogeneous]]).\n\nOne can think of ''X'' and ''Y'' as sitting inside ''X'' &times; ''Y'' as the [[subspace (topology)|subspaces]] ''X'' &times; {''y''<sub>0</sub>} and {''x''<sub>0</sub>} &times; ''Y''. These subspaces intersect at a single point: (''x''<sub>0</sub>, ''y''<sub>0</sub>), the basepoint of ''X'' &times; ''Y''. So the union of these subspaces can be identified with the [[wedge sum]] ''X'' ∨ ''Y''. The smash product is then the quotient\n:<math>X \\wedge Y = (X \\times Y) / (X \\vee Y). </math>\n\nThe smash product shows up in [[homotopy theory]], a branch of [[algebraic topology]]. In homotopy theory, one often works with a different [[category (mathematics)|category]] of spaces than the category of all topological spaces. In some of these categories the definition of the smash product must be modified slightly. For example, the smash product of two [[CW complex]]es is a CW complex if one uses the product of CW complexes in the definition rather than the product topology. Similar modifications are necessary in other categories.\n\n==Examples==\n*The smash product of any pointed space ''X'' with a [[0-sphere]] (a discrete space with two points) is homeomorphic to ''X''.\n*The smash product of two circles is a quotient of the [[torus]] homeomorphic to the 2-sphere.\n*More generally, the smash product of two spheres ''S''<sup>''m''</sup> and ''S''<sup>''n''</sup> is [[homeomorphic]] to the sphere ''S''<sup>''m''+''n''</sup>.\n*The smash product of a space ''X'' with a circle is homeomorphic to the [[reduced suspension]] of ''X'': \n*:<math> \\Sigma X \\cong X \\wedge S^1. </math>\n*The ''k''-fold iterated reduced suspension of ''X'' is homeomorphic to the smash product of ''X'' and a ''k''-sphere\n*:<math> \\Sigma^k X \\cong X \\wedge S^k. </math>\n* In [[domain theory]], taking the product of two domains (so that the product is strict on its arguments).\n\n==As a symmetric monoidal product==\nFor any pointed spaces ''X'', ''Y'', and ''Z'' in an appropriate \"convenient\" category (e.g., that of [[compactly generated space]]s), there are natural (basepoint preserving) [[homeomorphism]]s\n:<math>\\begin{align}\nX \\wedge Y &\\cong Y\\wedge X, \\\\\n(X\\wedge Y)\\wedge Z &\\cong X \\wedge (Y\\wedge Z).\n\\end{align}</math>\nHowever, for the naive category of pointed spaces, this fails, as shown by the counterexample <math>X=Y=\\mathbb{Q}</math> and <math>Z=\\mathbb{N}</math> found by Dieter Puppe.<ref>{{cite journal|first=Dieter|last=Puppe|authorlink=Dieter Puppe |title=Homotopiemengen und ihre induzierten Abbildungen. I. | journal=[[Mathematische Zeitschrift]] |volume=69 |year=1958|pages=299–344|mr=0100265|doi=10.1007/BF01187411}} (p. 336)</ref> A proof due to Kathleen Lewis that Puppe's counterexample is indeed a counterexample can be found in the book of Johann Sigurdsson and [[J. Peter May]].<ref>{{cite book|first1=J. Peter|last1=May|authorlink1=J. Peter May| first2=Johann|last2=Sigurdsson| title=Parametrized Homotopy Theory|series=Mathematical Surveys and Monographs|volume=132|publisher=[[American Mathematical Society]]|location= Providence, RI| year=2006|isbn=978-0-8218-3922-5|at=section 1.5|mr=2271789}}</ref>\n\nThese isomorphisms make the appropriate [[category of pointed spaces]] into a [[symmetric monoidal category]] with the smash product as the monoidal product and the pointed [[0-sphere]] (a two-point discrete space) as the unit object. One can therefore think of the smash product as a kind of [[tensor product]] in an appropriate category of pointed spaces.\n\n==Adjoint relationship==\n[[Adjoint functors]] make the analogy between the [[Tensor product of modules|tensor product]] and the smash product more precise. In the category of [[module (mathematics)|''R''-modules]] over a [[commutative ring]] ''R'', the tensor functor <math>(- \\otimes_R A)</math> is left adjoint to the internal [[Hom functor]] <math>\\mathrm{Hom}(A,-)</math> so that:\n:<math>\\mathrm{Hom}(X\\otimes A,Y) \\cong \\mathrm{Hom}(X,\\mathrm{Hom}(A,Y)).</math>\nIn the [[category of pointed spaces]], the smash product plays the role of the tensor product in this formula. In particular, if ''A'' is [[locally compact Hausdorff]] then we have an adjunction\n:<math>\\mathrm{Maps_*}(X\\wedge A,Y) \\cong \\mathrm{Maps_*}(X,\\mathrm{Maps_*}(A,Y))</math>\n\nwhere <math>\\operatorname{Maps_*}</math>denotes continuous maps that send basepoint to basepoint, and  <math>\\mathrm{Maps_*}(A,Y)</math> carries the [[compact-open topology]].\n\nIn particular, taking <math>A</math> to be the [[unit circle]] <math>S^1</math>, we see that the suspension functor <math>\\Sigma</math> is left adjoint to the [[loop space]] functor <math>\\Omega</math>:\n:<math>\\mathrm{Maps_*}(\\Sigma X,Y) \\cong \\mathrm{Maps_*}(X,\\Omega Y).</math>\n\n==Notes==\n{{Reflist}}\n\n==References==\n*{{Hatcher AT}}\n\n{{DEFAULTSORT:Smash Product}}\n[[Category:Topology]]\n[[Category:Homotopy theory]]\n[[Category:Binary operations]]"
    },
    {
      "title": "Subtraction",
      "url": "https://en.wikipedia.org/wiki/Subtraction",
      "text": "{{refimprove|date=May 2018}}\n[[File:Subtraction01.svg|right|thumb|180px|\"{{nowrap|5 − 2}} = 3\" (verbally, \"five minus two equals three\")]]\n[[File:Vertical subtraction example.svg|right|thumb|180px|An example problem]]\n[[File:Shop placard showing 20% reduction.JPG|thumb|Placard outside a shop in [[Bordeaux]] advertising subtraction of 20% from the price of a second perfume.]]\n'''Subtraction''' is an [[arithmetic operation]] that represents the operation of removing objects from a collection. The result of a subtraction is called a '''difference'''. Subtraction is signified by the [[minus sign]] (−). For example, in the adjacent picture, there are {{nowrap|5 − 2}} apples—meaning 5 apples with 2 taken away, which is a total of 3 apples. Therefore, the ''difference'' of 5 and 2 is 3, that is, {{nowrap|1=5 − 2 = 3}}. Subtraction represents removing or decreasing physical and abstract quantities using different kinds of objects including [[negative number]]s, [[Fraction (mathematics)|fractions]], [[irrational number]]s, [[Euclidean vector|vectors]], decimals, functions, and matrices.\n\nSubtraction follows several important patterns. It is [[anticommutative]], meaning that changing the order changes the sign of the answer. It is also not [[associativity|associative]], meaning that when one subtracts more than two numbers, the order in which subtraction is performed matters. Because {{num|0}} is the [[additive identity]], subtraction of it does not change a number. Subtraction also obeys predictable rules concerning related operations such as [[addition]] and [[multiplication]]. All of these rules can be [[mathematical proof|proven]], starting with the subtraction of [[integers]] and generalizing up through the [[real number]]s and beyond. General [[binary operations]] that continue these patterns are studied in [[abstract algebra]].\n\nPerforming subtraction is one of the simplest numerical tasks. Subtraction of very small numbers is accessible to young children. In [[primary education]], students are taught to subtract numbers in the [[decimal]] system, starting with single digits and progressively tackling more difficult problems.\n\nIn advanced algebra and in [[computer algebra]], an expression involving subtraction like {{nowrap|''A'' − ''B''}} is generally treated as a shorthand notation for the addition {{nowrap|''A'' + (−''B'')}}. Thus, {{nowrap|''A'' − ''B''}} contains two terms, namely ''A'' and −''B''. This allows an easier use of [[associativity]] and [[commutativity]].\n\n==Notation and terminology==\n[[File:Subtraction chart.png|thumb|180px|Subtraction of numbers 0–10. Line labels = minuend. X axis = subtrahend. Y axis = difference.]]\nSubtraction is written using the [[Plus and minus signs|minus sign]] \"−\" between the terms; that is, in [[infix notation]]. The result is expressed with an [[equals sign]]. For example,\n:<math>2 - 1 = 1 </math> (verbally, \"two minus one equals one\")\n:<math>4 - 2 = 2 </math> (verbally, \"four minus two equals two\")\n:<math>6 - 3 = 3 </math> (verbally, \"six minus three equals three\")\n:<math>4 - 6 = -2 </math> (verbally, \"four minus six equals negative two\")\n\nThere are also situations where subtraction is \"understood\" even though no symbol appears:\n* A column of two numbers, with the lower number in red, usually indicates that the lower number in the column is to be subtracted, with the difference written below, under a line. This is most common in accounting.\n\nFormally, the number being subtracted is known as the ''subtrahend'',<ref name=\"Schmid_1974\">{{cite book |title=Decimal Computation |first=Hermann |last=Schmid<!--General Electric Company, Binghamton, NY--> |author-link=Hermann Schmid (computer scientist) |date=1974 |edition=1 |publisher=[[John Wiley & Sons]] |location=Binghamton, NY |isbn=978-0-471-76180-8}}</ref><ref name=\"Schmid_1983\">{{cite book |title=Decimal Computation |first=Hermann |last=Schmid<!--General Electric Company, Binghamton, NY--> |author-link=Hermann Schmid (computer scientist) |orig-year=1974 |date=1983 |edition=1 (reprint) |publisher=Robert E. Krieger Publishing Company |location=Malabar, FL |isbn=978-0-89874-318-0}}</ref> while the number it is subtracted from is the ''minuend''.<ref name=\"Schmid_1974\"/><ref name=\"Schmid_1983\"/> The result is the ''difference''.<ref name=\"Schmid_1974\"/><ref name=\"Schmid_1983\"/>\n\nAll of this terminology derives from [[Latin]]. \"[[wikt:subtraction|Subtraction]]\" is an [[English language|English]] word derived from the Latin [[verb]] ''subtrahere'', which is in turn a [[compound (linguistics)|compound]] of ''sub'' \"from under\" and ''trahere'' \"to pull\"; thus to ''subtract'' is to ''draw from below, take away''.<ref>{{OED|Subtraction}}</ref> Using the [[gerundive]] [[Affix|suffix]] ''-nd'' results in \"subtrahend\", \"thing to be subtracted\".<ref group=lower-alpha>\"Subtrahend\" is shortened by the inflectional Latin suffix -us, e.g. remaining un-declined as in ''numerus subtrahendus'' \"the number to be subtracted\".</ref> Likewise from ''minuere'' \"to reduce or diminish\", one gets \"minuend\", \"thing to be diminished\".\n\n==Of integers and real numbers==\n===Integers===\n[[File:Line Segment jaredwf.svg|left| ]]\nImagine a [[line segment]] of [[length]] ''b'' with the left end labeled ''a'' and the right end labeled ''c''.\nStarting from ''a'', it takes ''b'' steps to the right to reach ''c''. This movement to the right is modeled mathematically by [[addition]]:\n:''a'' + ''b'' = ''c''.\n\nFrom ''c'', it takes ''b'' steps to the ''left'' to get back to ''a''. This movement to the left is modeled by subtraction:\n:''c'' − ''b'' = ''a''.\n\n[[File:Subtraction line segment.svg|left| ]]\nNow, a line segment labeled with the numbers {{num|1}}, {{num|2}}, and {{num|3}}. From position 3, it takes no steps to the left to stay at 3, so {{nowrap|1=3 − 0 = 3}}. It takes 2 steps to the left to get to position 1, so {{nowrap|1=3 − 2 = 1}}. This picture is inadequate to describe what would happen after going 3 steps to the left of position 3. To represent such an operation, the line must be extended.\n\nTo subtract arbitrary [[natural number]]s, one begins with a line containing every natural number (0, 1, 2, 3, 4, 5, 6, ...). From 3, it takes 3 steps to the left to get to 0, so {{nowrap|1=3 − 3 = 0}}. But {{nowrap|3 − 4}} is still invalid since it again leaves the line. The natural numbers are not a useful context for subtraction.\n\nThe solution is to consider the [[integer]] [[number line]] (..., −3, −2, −1, 0, 1, 2, 3, ...). From 3, it takes 4 steps to the left to get to −1:\n:{{nowrap|1=3 − 4 = −1}}.\n\n===Natural numbers===\nSubtraction of [[natural numbers]] is not [[Closure (mathematics)|closed]]. The difference is not a natural number unless the minuend is greater than or equal to the subtrahend. For example, 26 cannot be subtracted from 11 to give a natural number. Such a case uses one of two approaches:\n# Say that 26 cannot be subtracted from 11; subtraction becomes a [[partial function]].\n# Give the answer as an [[integer]] representing a [[negative number]], so the result of subtracting 26 from 11 is −15.\n\n===Real numbers===\nSubtraction of real numbers is defined as addition of signed numbers. Specifically, a number is subtracted by adding its [[additive inverse]]. Then we have {{nowrap|1=3 − π = 3 + (−π)}}. This helps to keep the [[ring (mathematics)|ring]] of real numbers \"simple\" by avoiding the introduction of \"new\" operators such as subtraction. Ordinarily a ring only has two operations defined on it; in the case of the integers, these are addition and multiplication. A ring already has the concept of additive inverses, but it does not have any notion of a separate subtraction operation, so the use of signed addition as subtraction allows us to apply the ring axioms to subtraction without needing to prove anything.\n\n==Properties==\n===Anticommutativity===\nSubtraction is [[anti-commutative]], meaning that if one reverses the terms in a difference left-to-right, the result is the negative of the original result. Symbolically, if ''a'' and ''b'' are any two numbers, then\n:''a'' − ''b'' = −(''b'' − ''a)''.\n\n===Non-associativity===\nSubtraction is [[associativity|non-associative]], which comes up when one tries to define repeated subtraction. Should the expression\n:\"''a'' − ''b'' − ''c''\"\nbe defined to mean (''a'' − ''b'') − ''c'' or ''a'' − (''b'' − ''c'')? These two possibilities give different answers. To resolve this issue, one must establish an [[order of operations]], with different orders giving different results.\n\n===Predecessor===\nIn the context of integers, subtraction of [[1 (number)|one]] also plays a special role: for any integer ''a'', the integer {{nowrap|(''a'' − 1)}} is the largest integer less than ''a'', also known as the predecessor of ''a''.\n\n==Units of measurement==\nWhen subtracting two numbers with units of measurement such as kilograms or pounds, they must have the same unit. In most cases the difference will have the same unit as the original numbers.\n\n===Percentages===\nChanges in [[percentage]]s can be reported in at least two forms, [[percentage change]] and [[percentage point]] change. Percentage change represents the [[relative change]] between the two quantities as a percentage, while [[percentage point]] change is simply the number obtained by subtracting the two percentages.<ref>Paul E. Peterson, Michael Henderson, Martin R. West (2014) ''Teachers Versus the Public: What Americans Think about Schools and How to Fix Them'' Brookings Institution Press, p.163</ref><ref>Janet Kolodzy (2006) ''Convergence Journalism: Writing and Reporting across the News Media'' Rowman & Littlefield Publishers, p.180</ref><ref>David Gillborn (2008) ''Racism and Education: Coincidence Or Conspiracy?'' Routledge p.46</ref>\n\nAs an example, suppose that 30% of widgets made in a factory are defective. Six months later, 20% of widgets are defective. The percentage change is {{sfrac|−33|1|3}}%, while the percentage point change is −10 percentage points.\n\n==In computing==\nThe [[method of complements]] is a technique used to subtract one number from another using only addition of positive numbers. This method was commonly used in [[mechanical calculator]]s and is still used in modern [[computers]].\n\n{| class=\"wikitable floatright\"\n! Binary<br>digit\n! Ones' <br>complement\n|-align=\"center\"\n| 0\n| 1\n|-align=\"center\"\n| 1\n| 0\n|}\nTo subtract a binary number ''y'' (the subtrahend) from another number ''x'' (the minuend), the ones' complement of ''y'' is added to ''x'' and one is added to the sum. The leading digit \"1\" of the result is then discarded.\n\nThe method of complements is especially useful in binary (radix 2) since the ones' complement is very easily obtained by inverting each bit (changing \"0\" to \"1\" and vice versa). And adding 1 to get the two's complement can be done by simulating a carry into the least significant bit. For example:\n\n   01100100  (x, equals decimal 100)\n - 00010110  (y, equals decimal 22)\n\nbecomes the sum:\n\n   01100100  (x)\n + 11101001  (ones' complement of y)\n +        1  (to get the two's complement)\n ——————————\n  101001110\n\nDropping the initial \"1\" gives the answer: 01001110 (equals decimal 78)\n\n==The teaching of subtraction in schools==\nMethods used to teach subtraction to [[elementary school]] vary from country to country, and within a country, different methods are in fashion at different times. In what is, in the United States, called [[traditional mathematics]], a specific process is taught to students at the end of the 1st&nbsp;year or during the 2nd&nbsp;year for use with multi-digit whole numbers, and is extended in either the fourth or fifth grade to include decimal representations of fractional numbers.\n\n===In America===\nAlmost all American schools currently teach a method of subtraction using borrowing or regrouping (the decomposition algorithm) and a system of markings called crutches.<ref name=\"Klapper1916\">{{cite book |author=Paul Klapper |title=The Teaching of Arithmetic: A Manual for Teachers |url=https://books.google.com/books?id=yjH8U_DswkwC&pg=PA80 |year=1916 |pages=80–}}</ref><ref>Susan Ross and Mary Pratt-Cotter. 2000. \"Subtraction in the United States:  An Historical Perspective,\" ''The Mathematics Educator'' 8(1):4–11. p. 8: \"This new version of the decomposition algorithm [i.e., using Brownell's crutch] has so completely dominated the field that it is rare to see any other algorithm used to teach subtraction today [in America].\"</ref> Although a method of borrowing had been known and published in textbooks previously, the use of crutches in American schools spread after [[William A. Brownell]] published a study claiming that crutches were beneficial to students using this method.<ref>{{cite journal |title=Subtraction From a Historical Perspective |journal=School Science and Mathematics |year=1999 |last=Ross |first=Susan C. |last2=Pratt-Cotter |first2=Mary |volume=99 |issue=7 |pages=389–393 }}</ref> This system caught on rapidly, displacing the other methods of subtraction in use in America at that time.\n\n===In Europe===\nSome European schools employ a method of subtraction called the Austrian method, also known as the additions method. There is no borrowing in this method. There are also crutches (markings to aid memory), which vary by country.<ref>Klapper 1916, pp. 177–.</ref><ref name=\"Smith1913\">{{cite book |author=David Eugene Smith |title=The Teaching of Arithmetic |url=https://books.google.com/books?id=A7NJAAAAIAAJ&pg=PA77 |year=1913 |publisher=Ginn |pages=77–}}</ref>\n\n===Comparing the two main methods===\nBoth these methods break up the subtraction as a process of one digit subtractions by place value. Starting with a least significant digit, a subtraction of subtrahend:\n:''s''<sub>''j''</sub> ''s''<sub>''j''−1</sub> ... ''s''<sub>1</sub>\nfrom minuend\n:''m''<sub>''k''</sub> ''m''<sub>''k''−1</sub> ... ''m''<sub>1</sub>,\nwhere each ''s''<sub>''i''</sub> and ''m''<sub>''i''</sub> is a digit, proceeds by writing down {{nowrap|''m''<sub>1</sub> − ''s''<sub>1</sub>}}, {{nowrap|''m''<sub>2</sub> − ''s''<sub>2</sub>}}, and so forth, as long as ''s''<sub>''i''</sub> does not exceed ''m''<sub>''i''</sub>. Otherwise, ''m''<sub>''i''</sub> is increased by 10 and some other digit is modified to correct for this increase. The American method corrects by attempting to decrease the minuend digit ''m''<sub>''i''+1</sub> by one (or continuing the borrow leftwards until there is a non-zero digit from which to borrow). The European method corrects by increasing the subtrahend digit ''s''<sub>''i''+1</sub> by one.\n\n'''Example:''' 704 − 512.\n\n{{equation|\n   \\begin{array}{rrrr}\n        & \\color{Red}-1 \\\\\n        & C & D & U  \\\\\n        & 7 & 0 & 4 \\\\   \n        & 5 & 1 & 2 \\\\\n      \\hline\n        & 1 & 9 & 2 \\\\\n   \\end{array}\n   \\begin{array}{l}\n       { \\color{Red}\\longleftarrow \\rm carry }\\\\\n       \\\\\n       \\longleftarrow \\; \\rm Minuend\\\\\n       \\longleftarrow \\; \\rm Subtrahend\\\\\n       \\longleftarrow \\rm{Rest \\; or \\; Difference}\\\\\n   \\end{array}\n}}\n\nThe minuend is 704, the subtrahend is 512. The minuend digits are {{nowrap|1=''m''<sub>3</sub> = 7}}, {{nowrap|1=''m''<sub>2</sub> = 0}} and {{nowrap|1=''m''<sub>1</sub> = 4}}. The subtrahend digits are {{nowrap|1=''s''<sub>3</sub> = 5}}, {{nowrap|1=''s''<sub>2</sub> = 1}} and {{nowrap|1=''s''<sub>1</sub> = 2}}. Beginning at the one's place, 4 is not less than 2 so the difference 2 is written down in the result's one's place. In the ten's place, 0 is less than 1, so the 0 is increased by 10, and the difference with 1, which is 9, is written down in the ten's place. The American method corrects for the increase of ten by reducing the digit in the minuend's hundreds place by one. That is, the 7 is struck through and replaced by a 6. The subtraction then proceeds in the hundreds place, where 6 is not less than 5, so the difference is written down in the result's hundred's place. We are now done, the result is 192.\n\nThe Austrian method does not reduce the 7 to 6. Rather it increases the subtrahend hundred's digit by one. A small mark is made near or below this digit (depending on the school). Then the subtraction proceeds by asking what number when increased by 1, and 5 is added to it, makes 7. The answer is 1, and is written down in the result's hundred's place.\n\nThere is an additional subtlety in that the student always employs a mental subtraction table in the American method. The Austrian method often encourages the student to mentally use the addition table in reverse. In the example above, rather than adding 1 to 5, getting 6, and subtracting that from 7, the student is asked to consider what number, when increased by 1, and 5 is added to it, makes 7.\n\n==Subtraction by hand==\n===Austrian method===\nExample:\n<gallery>\nFile:Vertical Subtraction Method B Step 1.JPG|1 + ... = 3\nFile:Vertical Subtraction Method B Step 2.JPG|The difference is written under the line.\nFile:Vertical Subtraction Method B Step 3.JPG|9 + ... = 5<br>The required sum (5) is too small.\nFile:Vertical Subtraction Method B Step 4.JPG|So, we add 10 to it and put a 1 under the next higher place in the subtrahend.\nFile:Vertical Subtraction Method B Step 5.JPG|9 + ... = 15<br>Now we can find the difference like before.\nFile:Vertical Subtraction Method B Step 6.JPG|(4 + 1) + ... = 7\nFile:Vertical Subtraction Method B Step 7.JPG|The difference is written under the line.\nFile:Vertical Subtraction Method B Step 8.JPG|The total difference.\n</gallery>\n\n===Subtraction from left to right===\nExample:\n<gallery>\nFile:LeftToRight Subtraction Step 1.JPG|7 − 4 = 3<br>This result is only penciled in.\nFile:LeftToRight Subtraction Step 2.JPG|Because the next digit of the minuend is smaller than the subtrahend, we subtract one from our penciled-in-number and mentally add ten to the next.\nFile:LeftToRight Subtraction Step 3.JPG|15 − 9 = 6\nFile:LeftToRight Subtraction Step 4.JPG|Because the next digit in the minuend is not smaller than the subtrahend, We keep this number.\nFile:LeftToRight Subtraction Step 5.JPG|3 − 1 = 2\n</gallery>\n\n===American method===\nIn this method, each digit of the subtrahend is subtracted from the digit above it starting from right to left. If the top number is too small to subtract the bottom number from it, we add 10 to it; this 10 is \"borrowed\" from the top digit to the left, which we subtract 1 from. Then we move on to subtracting the next digit and borrowing as needed, until every digit has been subtracted.\nExample:\n<gallery>\nFile:Vertical Subtraction Method A Step 1.JPG|3 − 1 = ...\nFile:Vertical Subtraction Method A Step 2.JPG|We write the difference under the line.\nFile:Vertical Subtraction Method A Step 3.JPG|5 − 9 = ...<br> The minuend (5) is too small!\nFile:Vertical Subtraction Method A Step 4.JPG|So, we add 10 to it. The 10 is \"borrowed\" from the digit on the left, which goes down by 1.\nFile:Vertical Subtraction Method A Step 5.JPG|15 − 9 = ...<br> Now the subtraction works, and we write the difference under the line.\nFile:Vertical Subtraction Method A Step 6.JPG|6 − 4 = ...\nFile:Vertical Subtraction Method A Step 7.JPG|We write the difference under the line.\nFile:Vertical Subtraction Method A Step 8.JPG|The total difference.\n</gallery>\n\n===Trade first===\nA variant of the American method where all borrowing is done before all subtraction.<ref>[https://sites.google.com/a/oswego308.org/msimester/home/math/algorithms/subtraction The Many Ways of Arithmetic in UCSMP Everyday Mathematics] Subtraction: Trade First</ref>\n\nExample:\n<gallery>\nFile:Trade First Subtraction Step 1.JPG|1 − 3 = not possible.<br>We add a 10 to the 1. Because the 10 is \"borrowed\" from the nearby 5, the 5 is lowered by 1.\nFile:Trade First Subtraction Step 2.JPG|4 − 9 = not possible.<br>So we proceed as in step 1.\nFile:Trade First Subtraction Step 3.JPG|Working from right to left:<br>11 − 3 = 8\nFile:Trade First Subtraction Step 4.JPG|14 − 9 = 5\nFile:Trade First Subtraction Step 5.JPG|6 − 4 = 2\n</gallery>\n\n===Partial differences===\nThe partial differences method is different from other vertical subtraction methods because no borrowing or carrying takes place. In their place, one places plus or minus signs depending on whether the minuend is greater or smaller than the subtrahend. The sum of the partial differences is the total difference.<ref>[http://ouronlineschools.org/Schools/NC/Demoschool/4thGrade/Math/PartialDifferences.htm Partial-Differences Subtraction] {{Webarchive|url=https://web.archive.org/web/20140623021239/http://ouronlineschools.org/Schools/NC/Demoschool/4thGrade/Math/PartialDifferences.htm |date=2014-06-23 }}; [https://sites.google.com/a/oswego308.org/msimester/home/math/algorithms/subtraction The Many Ways of Arithmetic in UCSMP Everyday Mathematics] Subtraction: Partial Differences</ref>\n\nExample:\n<gallery>\nFile:Partial-Differences Subtraction Step 1.JPG|The smaller number is subtracted from the greater:<br>700 − 400 = 300<br>Because the minuend is greater than the subtrahend, this difference has a plus sign.\nFile:Partial-Differences Subtraction Step 2.JPG|The smaller number is subtracted from the greater:<br>90 − 50 = 40<br>Because the minuend is smaller than the subtrahend, this difference has a minus sign.\nFile:Partial-Differences Subtraction Step 3.JPG|The smaller number is subtracted from the greater:<br>3 − 1 = 2<br>Because the minuend is greater than the subtrahend, this difference has a plus sign.\nFile:Partial-Differences Subtraction Step 4.JPG|+300 − 40 + 2 = 262\n</gallery>\n\n===Nonvertical methods===\n====Counting up====\nInstead of finding the difference digit by digit, one can count up the numbers between the subtrahend and the minuend.<ref>[https://sites.google.com/a/oswego308.org/msimester/home/math/algorithms/subtraction The Many Ways of Arithmetic in UCSMP Everyday Mathematics] Subtraction: Counting Up</ref>\n\nExample:\n1234 − 567 = can be found by the following steps:\n* {{nowrap|1=567 + '''3''' = 570}}\n* {{nowrap|1=570 + '''30''' = 600}}\n* {{nowrap|1=600 + '''400''' = 1000}}\n* {{nowrap|1=1000 + '''234''' = 1234}}\nAdd up the value from each step to get the total difference: {{nowrap|1=3 + 30 + 400 + 234 = 667}}.\n\n====Breaking up the subtraction====\nAnother method that is useful for mental arithmetic is to split up the subtraction into small steps.<ref>[https://sites.google.com/a/oswego308.org/msimester/home/math/algorithms/subtraction The Many Ways of Arithmetic in UCSMP Everyday Mathematics] Subtraction: Left to Right Subtraction</ref>\n\nExample:\n1234 − 567 = can be solved in the following way:\n* 1234 − '''500''' = 734\n* 734 − '''60''' = 674\n* 674 − '''7''' = 667\n\n====Same change====\nThe same change method uses the fact that adding or subtracting the same number from the minuend and subtrahend does not change the answer. One adds the amount needed to get zeros in the subtrahend.<ref>[https://sites.google.com/a/oswego308.org/msimester/home/math/algorithms/subtraction The Many Ways of Arithmetic in UCSMP Everyday Mathematics] Subtraction: Same Change Rule</ref>\n\nExample:\n\n\"1234 − 567 =\" can be solved as follows:\n* {{nowrap|1=1234 − 567 = 1237 − 570 =}} {{nowrap|1=1267 − 600 = 667}}\n\n==See also==\n* [[wikt:decrement|Decrement]]\n* [[Elementary arithmetic]]\n* [[Method of complements]]\n* [[Negative number]]\n\n==Notes==\n{{notelist}}\n\n==References==\n{{Reflist}}\n\n==Bibliography==\n* Brownell, W.A. (1939). Learning as reorganization: An experimental study in third-grade arithmetic, Duke University Press.\n* [http://math.coe.uga.edu/TME/Issues/v10n2/5ross.pdf Subtraction in the United States: An Historical Perspective, Susan Ross, Mary Pratt-Cotter, ''The Mathematics Educator'', Vol. 8, No. 1 (original publication) and Vol. 10, No. 1 (reprint.)] [[PDF]]\n\n==External links==\n{{Wiktionary}}\n{{commons category}}\n* {{springer|title=Subtraction|id=p/s091050}}\n* Printable Worksheets: [https://web.archive.org/web/20121119024904/http://www.math-drills.com/subtraction.shtml Subtraction Worksheets], [http://www.kwiznet.com/p/takeQuiz.php?ChapterID=1214&CurriculumID=2&Method=Worksheet&NQ=24&NQ4P=3 One Digit Subtraction], [http://www.kwiznet.com/p/takeQuiz.php?ChapterID=1202&CurriculumID=2&Method=Worksheet&NQ=24&NQ4P=3 Two Digit Subtraction], [http://www.kwiznet.com/p/takeQuiz.php?ChapterID=1273&CurriculumID=3&Method=Worksheet&NQ=24&NQ4P=3 Four Digit Subtraction], and [http://www.dadsworksheets.com/worksheets/subtraction.html More Subtraction Worksheets]\n* [http://www.cut-the-knot.org/Curriculum/Arithmetic/SubtractionGame.shtml Subtraction Game] at [[Cut-the-Knot|cut-the-knot]]\n* [http://webhome.idirect.com/~totton/abacus/pages.htm#Subtraction1 Subtraction on a Japanese abacus] selected from [http://webhome.idirect.com/~totton/abacus/ Abacus: Mystery of the Bead]\n\n{{Elementary arithmetic}}\n{{Hyperoperations}}\n\n[[Category:Elementary arithmetic]]\n[[Category:Binary operations]]\n[[Category:Subtraction| ]]"
    },
    {
      "title": "Tensor product",
      "url": "https://en.wikipedia.org/wiki/Tensor_product",
      "text": "{{Technical|date=March 2019}}{{Short description|Concept in linear algebra, generalized throughout mathematics}}{{For|generalizations of this concept|Tensor product of modules|Tensor product (disambiguation)}}\n\nIn [[mathematics]], the '''tensor product''' {{math|''V'' ⊗ ''W''}}  (Latex:  {{code|V \\otimes W}}) of two [[vector space]]s {{math|''V''}} and {{math|''W''}} (over the same [[Field_(mathematics)|field]]) is itself a vector space, endowed with the operation of [[Bilinear map|bilinear composition]], denoted by {{math|⊗}}, from ordered pairs in the [[Cartesian product]] {{math|''V'' × ''W''}} onto {{math|''V'' ⊗ ''W''}} in a way that generalizes the [[outer product]].\n\nEssentially the difference between a tensor product of two vectors and an ordered pair of vectors is that if one vector is multiplied by a nonzero scalar and the other is multiplied by the reciprocal of that scalar, the result is a different ordered pair of vectors, but the same tensor product of two vectors.\n\nThe tensor product of {{math|''V''}} and {{math|''W''}} is the [[vector space]] generated by the symbols {{math|''v'' ⊗ ''w''}}, with {{math|''v'' ∈ ''V''}} and {{math|''w'' ∈ ''W''}}, in which the relations of bilinearity are imposed for the product operation {{math|⊗}}, ''and no other relations'' are assumed to hold. The tensor product space is thus the \"[[free object|freest]]\" (or most general) such vector space, in the sense of having the fewest constraints.\n\nThe tensor product of (finite-dimensional) vector spaces has dimension equal to the product of the dimensions of the two factors:\n\n:<math>\\dim(V\\otimes W) = \\dim V \\times \\dim W.</math>\n\nIn particular, this distinguishes the tensor product from the [[direct sum]] vector space, whose dimension is the sum of the dimensions of the two summands:\n\n:<math>\\dim(V\\oplus W) = \\dim V + \\dim W.</math>\n\nMore generally, the tensor product can be extended to other [[category (mathematics)|categories]] of mathematical objects in addition to vector spaces, such as to [[matrix (mathematics)|matrices]], [[tensors]], [[algebra over a field|algebras]], [[topological vector spaces]], and [[module (mathematics)|modules]]. In each such case the tensor product is characterized by a similar [[#Universal property|universal property]]: it is the freest [[bilinear operator|bilinear operation]]. The general concept of a \"tensor product\" is captured by [[monoidal category|monoidal categories]]; that is, the class of all things that have a tensor product is a monoidal category.\n\n==Intuitive motivation and the concrete tensor product==\nThe intuitive motivation for the tensor product relies on the concept of '''tensors''' more generally. In particular, a tensor is an object which can be considered a special type of [[multilinear map]], which takes in a certain number of vectors (its ''order'') and outputs a scalar. Such objects are useful in a number of areas of application, such as [[Riemannian geometry]], famous for its use in [[Albert Einstein]]'s [[general theory of relativity]] in [[modern physics]], where the [[metric tensor]] is a fundamental concept: in particular, the metric tensor takes in two vectors, conceived of roughly as small arrows emanating from a specific point within a curved space, or [[manifold]], and returns a ''local'' [[dot product]] of them relative to that particular point—an operation which encodes roughly the vectors' [[length]]s as well as the [[angle]] between them. As the dot product is a scalar, the metric tensor is thus seen to deserve its name. There is one metric tensor at each point of the manifold, and variation in the metric tensor thus encodes how that distance and angle concepts, and so the laws of [[analytic geometry]], vary throughout the manifold.\n\nOne can think of the tensor product of two vector spaces, <math>V</math> and <math>W</math> as representing the set of all tensors that take a vector from <math>V</math> and <math>W</math> and output a scalar within their common base field (and thus can only be defined if they have such a base field). The two spaces may be the same—in the above they are vectors in the ''tangent space'' at a point: roughly the flat space a tiny piece of the manifold most \"looks like\" when you zoom very, very close into a particular point therein, and thus the metric tensor lives in the tensor product of that space with itself. But they may also be different.\n\nIf we have a [[basis (linear algebra)|basis]] for the vector spaces, and the vector space is finite-dimensional, we can represent the vectors in terms of components under those basis vectors:\n\n:<math>\\mathbf{v} = \\begin{bmatrix}v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n\\end{bmatrix},\\ \\mathbf{w} = \\begin{bmatrix}w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_m\\end{bmatrix}.</math>\n\nwhere each notation stands for the sum <math>\\mathbf{v} = v_1 \\mathbf{e}_1 + v_2 \\mathbf{e}_2 + \\cdots + v_n \\mathbf{e}_n</math>.\n\nA tensor is then a map <math>T(\\mathbf{v}, \\mathbf{w})</math> that works as above, returning a scalar and is linear in both of its arguments. Such a tensor can be represented using a matrix multiplication:\n\n:<math>T(\\mathbf{v}, \\mathbf{w}) = \\mathbf{v}^T \\mathbf{T} \\mathbf{w}</math>\n\nwhere the superscripted T denotes the [[matrix transpose]] which sends the vector <math>\\mathbf{v}</math> to its [[dual space|dual vector]].\n\nGiven two vectors, we can form a tensor of their own from them rather naturally using the '''outer product''', which is denoted <math>\\mathbf{v} \\otimes \\mathbf{w}</math> and equals <math>\\mathbf{v} \\mathbf{w}^T</math>. This tensor comes out as the matrix\n\n:<math>\\mathbf{v} \\otimes \\mathbf{w} = \\begin{bmatrix} v_1 w_1 && v_1 w_2 && \\cdots && v_1 w_m \\\\ v_2 w_1 && v_2 w_2 && \\cdots && v_2 w_m \\\\ \\vdots && \\vdots && \\ddots && \\vdots \\\\ v_n w_1 && v_n w_2 && \\cdots && v_n w_m \\end{bmatrix}</math>\n\nand this matrix corresponds to the tensor by the prior construction, which is reminiscent of how it corresponds to a linear map (by multiplying on one side only). These tensors themselves generate a vector space by adding them together and multiplying them by scalars in the usual ways that we do for matrices and functions, and the collection of all such tensors so formed is the ''tensor product'' <math>V \\otimes W</math> of the two vector spaces themselves. In fact, this space is equivalent to the space of maps represented by every possible matrix of the above size, as can be seen by noting that the simple tensor products <math>\\mathbf{e}_i \\otimes \\mathbf{f}_j</math> (here <math>\\mathbf{f}_j</math> is the basis of the other vector space, <math>W</math>) have a \"1\" in the <math>(i, j)</math>-th position and \"0\"s everywhere else, which allows them to be multiplied by any number and then added up to get a matrix with arbitrary entries.\n\nThe purpose of the succeeding sections is to find a definition that is equivalent to this where it is applicable but which does not require a specific choice of basis and which can also more easily be applied to [[infinite-dimensional]] settings where the usual basis concepts ([[Hamel basis]]) may be ill-behaved. Not requiring a specific basis is useful from a theoretical point of view since while every vector space has a basis, not all bases are necessarily constructible, and moreover that result itself depends on the acceptance of the [[axiom of choice]] which may be rejected in some systems of mathematics. Also, it is useful to find an abstract construction for analysis from the point of view of [[category theory]], the theory of the very zoomed-out \"big picture of maths\" and how all mathematical objects relate to each other in a very general sense. A very important real-life use for having such a definition can be found in another field of modern physics called [[quantum mechanics]]: the tensor product in this form allows us to talk of the [[wave function]] of a system of two particles as an abstract [[Hilbert space]] vector without having to specify a specific basis of [[observable#Quantum mechanics|observables]].\n\n==Baby step towards the abstract tensor product: the free vector space==\nThe first step we will consider involves introducing something which is called a \"[[free vector space]]\" over a given set. The thrust behind this idea basically consists of what we said in the last point: since a tensor <math>T</math> can be written by the double sum\n\n:<math>T = \\sum_{i=1}^{n} \\sum_{j=1}^{m} (v_i w_j) (\\mathbf{e}_i \\otimes \\mathbf{f}_j)</math>\n\nthe most natural way to approach this problem is somehow to figure out how we can \"forget\" about the specific choice of bases <math>\\mathbf{e}</math> and <math>\\mathbf{f}</math> that are used here. In mathematics, the way we \"forget\" about representational details of something is to establish an identification that tells us that two different things which are to be considered representations of the same thing are in fact such, i.e. which, given those says either \"yes, they are\" or \"no, they aren't\", and then \"lump together\" all representations as constituting the \"thing represented\" without reference to any one in particular by packaging them all together into a single set. In formal terms, we first build an [[equivalence relation]], and then take the [[quotient set]] by that relation.\n\nBut before we can do that, we first need to develop what we are going to take the equivalence relation over. The way we do that is to approach this the other way around, from the \"bottom up\": since we are not guaranteed a, at least constructible, basis when starting from arbitrary vector spaces, we might instead try to start by guaranteeing we have one—that is, we will start first by considering a \"basis\", on its own, as given, and then building the vector space on top. To that end, we accomplish the following: suppose that <math>B</math> is some set, which we could call an ''abstract basis set''. Now consider all ''formal'' expressions of the form\n\n:<math>\\mathbf{v} = a_1 \\beta_1 + a_2 \\beta_2 + \\cdots + a_n \\beta_n</math>\n\nof arbitrary, but finite, length <math>n</math> and for which <math>a_j</math> are scalars and <math>\\beta_j</math> are members of <math>B</math>. Intuitively, this is a linear combination of the basis vectors in the usual sense of expanding an element of a vector space. We call this a \"formal expression\" because technically it is illegal to multiply <math>a_j \\beta_j</math> since there is no defined multiplication operation by default on an arbitrary set and arbitrary field of scalars. Instead, we will \"pretend\" (similar to defining the [[imaginary number]]s) that this refers to something, and then will go about manipulating it according to the rules we expect for a vector space, e.g. the sum of two such strings of the same length is\n\n:<math>(a_1 \\beta_1 + a_2 \\beta_2 + \\cdots + a_n \\beta_n) + (b_1 \\beta_1 + b_2 \\beta_2 + \\cdots + b_n \\beta_n) = (a_1 + b_1) \\beta_1 + (a_2 + b_2) \\beta_2 + \\cdots + (a_n + b_n) \\beta_n</math>\n\nwhere we have used the [[associative]], [[commutative]], and [[Distributive property|distributive]] laws to rearrange the first sum into the second. Continuing this way for scalar multiples and all different-length combinations of vectors allows us to build up a vector addition and scalar multiplication on this set of formal expressions, and we call it the '''free vector space''' over <math>B</math>, writing <math>F(B)</math>. Note that the elements of <math>B</math>, considered as length-one formal expressions with coefficient 1 out front, form a [[Hamel basis]] for this space.\n\nThe tensor product expression is then abstracted by considering that if <math>\\beta_j</math> and <math>\\gamma_j</math> represent \"abstract basis vectors\" from two sets <math>B</math> and <math>G</math>, i.e. that \"<math>\\beta_j = \\mathbf{e}_j</math>\" and \"<math>\\gamma_j = \\mathbf{f}_j</math>\", then pairs of these in the Cartesian product <math>B \\times G</math>, i.e. <math>(\\beta_i, \\gamma_j)</math> are taken as standing for the tensor products <math>\\mathbf{e}_i \\otimes \\mathbf{f}_j</math>. (Note that the tensor products in the expression are in some sense \"[[atomic expression|atomic]]\", i.e. additions and scalar multiplications do not split them up into anything else, so we can replace them with something different without altering the mathematical structure.) With such an identification, we can thus define the tensor product of two free vector spaces <math>F(B)</math> and <math>F(G)</math> as being something (yet to be decided) which is isomorphic to <math>F(B \\times G)</math>.\n\n==Using the free vector space to \"forget\" about the basis==\nThe above definition will actually work for any vector space in which we ''can'' specify a basis, since we can just rebuild it as the free vector space over that basis: the above construction exactly mirrors how you represent vectors via the Hamel basis construction by design. In effect, we haven't gained anything ... until we do this.\n\nSince we are assuming we don't actually have access to a basis for each vector space <math>V</math> and <math>W</math> that we in general want to form the tensor product <math>V \\otimes W</math> of, we will instead do the next best thing and in some sense the one thing we are ''guaranteed'' able to do, regardless of any concerns or problematics in finding a specific basis: take '''all''' of <math>V</math> and <math>W</math> as \"basis\" to build up the tensors—which corresponds to actually what we did in the last part of the \"Intuitive motivation\" section where we considered adding together arbitrary outer products <math>\\mathbf{v} \\otimes \\mathbf{w}</math> of arbitrary vectors taken from the two spaces. The only difference here is that if we use the free vector space construction and form the obvious <math>F(V) \\otimes F(W) = F(V \\times W)</math> it will have many redundant versions of what should be the same tensor, i.e. going back to our basisful case if we consider the very specific example where <math>V = W = \\R^2</math> in the standard basis, which is manageably small yet nontrivial, we may consider that the tensor formed by the vectors <math>\\mathbf{v} = \\begin{bmatrix} 0 && 3 \\end{bmatrix}^T</math> and <math>\\mathbf{w} = \\begin{bmatrix} 5 && -3 \\end{bmatrix}^T</math>, i.e.\n\n:<math>T := \\mathbf{v} \\otimes \\mathbf{w} = \\begin{bmatrix} 0 && 0 \\\\ 15 && -9 \\end{bmatrix}</math>\n\ncould ''also'' be represented by other sums, such as the sum using individual basic tensors <math>\\mathbf{e}_i \\otimes \\mathbf{e}_j</math>, e.g.\n\n:<math>T = 0(\\mathbf{e}_1 \\otimes \\mathbf{e}_1) + 0(\\mathbf{e}_1 \\otimes \\mathbf{e}_2) + 15(\\mathbf{e}_2 \\otimes \\mathbf{e}_1) - 9(\\mathbf{e}_2 \\otimes \\mathbf{e}_2).</math>\n\nThese, while equal expressions in the concrete case, would correspond to distinct elements of the free vector space <math>F(V \\times W)</math>, namely\n\n:<math>T = (v, w)</math>\n\nin the first case and\n\n:<math>T = 0(e_1, e_1) + 0(e_1, e_2) + 15(e_2, e_1) - 9(e_2, e_2)</math>\n\nin the second case. Thus we must condense them—this is where the equivalence relation comes into play. The trick to building it is to note that given any vector <math>\\mathbf{v}</math> in a vector space, it is always possible to represent it as the sum of two other vectors <math>\\mathbf{a}</math> and <math>\\mathbf{b}</math> not equal to the original. If nothing else, let <math>\\mathbf{a}</math> be any vector and then take <math>\\mathbf{b} := \\mathbf{v} - \\mathbf{a}</math>—which also shows that if we are given one vector and then a second vector, we can write the first vector in terms of the second together with a suitable third vector (indeed in many ways—just consider scalar multiples of the second vector in the same subtraction.).\n\nThis is useful to us because the outer product satisfies the following linearity properties, which can be proven by simple algebra on the corresponding matrix expressions (the vectors below are generic, not the example ones above):\n\n:<math>(\\mathbf{u} \\otimes \\mathbf{v})^T = (\\mathbf{v} \\otimes \\mathbf{u})</math>\n:<math>(\\mathbf{v} + \\mathbf{w}) \\otimes \\mathbf{u} = \\mathbf{v} \\otimes \\mathbf{u} + \\mathbf{w} \\otimes \\mathbf{u}</math>\n:<math>\\mathbf{u} \\otimes (\\mathbf{v} + \\mathbf{w}) = \\mathbf{u} \\otimes \\mathbf{v} + \\mathbf{u} \\otimes \\mathbf{w}</math>\n:<math>c (\\mathbf{v} \\otimes \\mathbf{u}) = (c\\mathbf{v}) \\otimes \\mathbf{u} = \\mathbf{v} \\otimes (c\\mathbf{u})</math>\n\nIf we want to relate the outer product <math>\\mathbf{v} \\otimes \\mathbf{w}</math> to, say, <math>\\mathbf{e_1} \\otimes \\mathbf{w}</math>, we can use the first relation above together with a suitable expression of <math>\\mathbf{v}</math> as a sum of some vector and some scalar multiple of <math>\\mathbf{e_1}</math>.\n\nEquality between two concrete tensors is then obtained if using the above rules will permit us to rearrange one sum of outer products into the other by suitably decomposing vectors—regardless of if we have a set of actual basis vectors. Applying that to our example above, we see that of course we have\n\n:<math>\\mathbf{v} = 0 \\mathbf{e}_1 + 3 \\mathbf{e}_2</math>\n:<math>\\mathbf{w} = 5 \\mathbf{e}_1 - 3 \\mathbf{e}_2</math>\n\nfor which substitution in\n\n:<math>T = \\mathbf{v} \\otimes \\mathbf{w}</math>\n\ngives us\n\n:<math>T = (0 \\mathbf{e}_1 + 3 \\mathbf{e}_2) \\otimes (5 \\mathbf{e}_1 - 3 \\mathbf{e}_2)</math>\n\nand judicious use of the distributivity properties lets us rearrange to the desired form. Likewise, there is a corresponding \"mirror\" manipulation in terms of the free vector space elements <math>(v, w)</math> and <math>(e_1, e_1)</math>, <math>(e_1, e_2)</math>, etc, and this finally leads us to the formal definition of the tensor product.\n\n==The definition of the abstract tensor product==\nThe abstract '''tensor product''' of two vector spaces <math>V</math> and <math>W</math> over a common base field is the [[quotient vector space]]\n\n:<math>V \\otimes W := F(V \\times W)/{\\sim}</math>\n\nwhere <math>\\sim</math> is the [[equivalence relation]] of ''formal equality'' which is generated by assuming that for each <math>(v, w)</math> and <math>(v', w')</math> taken as formal expressions in the free vector space the following hold:\n\n:''Identity''. <math>(v, w) \\sim (v, w).</math>\n:''Distributivity.'' <math>(v, w) + (v', w) \\sim (v + v', w)</math> and <math>(v, w) + (v, w') \\sim (v, w + w').</math>\n:''Scalar multiples.'' <math>c(v, w) \\sim (cv, w)</math> and <math>c(v, w) \\sim (v, cw).</math>\n\nand then testing equivalence of generic formal expressions through suitable manipulations based thereupon. Arithmetic is defined on the tensor product by choosing representative elements, applying the arithmetical rules, and finally taking the equivalence class. Moreover, given any two vectors <math>v \\in V</math> and <math>w \\in W</math>, the equivalence class <math>[(v, w)]</math> is denoted <math>v \\otimes w</math>.\n\n==Properties==\n===Notation===\nElements of {{math|''V'' ⊗ ''W''}} are often referred to as ''tensors'', although this term refers to many other related concepts as well.<ref>See [[Tensor]] or [[Tensor (intrinsic definition)]].</ref> If {{math|''v''}} belongs to {{math|''V''}} and {{math|''w''}} belongs to {{math|''W''}}, then the equivalence class of {{math|(''v'', ''w'')}} is denoted by {{math|''v'' ⊗ ''w''}}, which is called the tensor product of {{math|''v''}} with {{math|''w''}}. In physics and engineering, this use of the {{math| \"⊗\"}} symbol refers specifically to the [[outer product]] operation; the result of the outer product {{math|''v'' ⊗ ''w''}} is one of the standard ways of representing the equivalence class {{math|''v'' ⊗ ''w''}}.<ref>This similar to how [[Modulo operation|the engineering use]] of \"<math>(\\bmod n)</math>\" specifically returns the remainder, one of the many elements of the <math>(\\bmod n)</math> equivalence class.</ref> An element of {{math|''V'' ⊗ ''W''}} that can be written in the form {{math|''v'' ⊗ ''w''}} is called a ''pure'' or ''[[simple tensor]]''. In general, an element of the tensor product space is not a pure tensor, but rather a finite linear combination of pure tensors. For example, if {{math|''v''<sub>1</sub>}} and {{math|''v''<sub>2</sub>}} are [[linearly independent]], and {{math|''w''<sub>1</sub>}} and {{math|''w''<sub>2</sub>}} are also linearly independent, then {{math|''v''<sub>1</sub> ⊗ ''w''<sub>1</sub> + ''v''<sub>2</sub> ⊗ ''w''<sub>2</sub>}} cannot be written as a pure tensor. The number of simple tensors required to express an element of a tensor product is called the [[tensor rank]] (not to be confused with [[tensor order]], which is the number of spaces one has taken the product of, in this case 2; in notation, the number of indices), and for linear operators or matrices, thought of as {{math|(1, 1)}} tensors (elements of the space {{math|''V'' ⊗ ''V''<sup>∗</sup>}}), it agrees with [[matrix rank]].\n\n===Dimension===\nGiven bases {{math|{''v<sub>i</sub>''} }} and {{math|{''w<sub>j</sub>''} }} for {{math|''V''}} and {{math|''W''}} respectively, the tensors {{math|{''v<sub>i</sub>'' ⊗ ''w<sub>j</sub>''}<nowiki/>}} form a basis for {{math|''V'' ⊗ ''W''}}. Therefore, if {{math|''V''}} and {{math|''W''}} are finite-dimensional, the dimension of the tensor product is the product of dimensions of the original spaces; for instance {{math|'''R'''<sup>''m''</sup> ⊗ '''R'''<sup>''n''</sup>}} is isomorphic to {{math|'''R'''<sup>''mn''</sup>}}.\n\n===Tensor product of linear maps===\nThe tensor product also operates on [[linear map]]s between vector spaces. Specifically, given two linear maps {{math|''S'' : ''V'' → ''X''}} and {{math|''T'' : ''W'' → ''Y''}} between vector spaces, the ''tensor product of the two linear maps'' {{math|''S''}} and {{math|''T''}} is a linear map\n:<math>S\\otimes T:V\\otimes W\\to X\\otimes Y</math>\ndefined by\n:<math>(S\\otimes T)(v\\otimes w)=S(v)\\otimes T(w).</math>\n\nIn this way, the tensor product becomes a [[bifunctor]] from the category of vector spaces to itself, [[Functor#Covariance and contravariance|covariant]] in both arguments.<ref>{{cite book| last1=Hazewinkel|first1=Michiel|last2=Gubareni|first2=Nadezhda Mikhaĭlovna| last3=Gubareni|first3=Nadiya|last4=Kirichenko|first4=Vladimir V.|title=Algebras, rings and modules|page=100|\npublisher=Springer|year=2004|isbn=978-1-4020-2690-4}}</ref>\n\nIf {{math|''S''}} and {{math|''T''}} are both injective, surjective, or continuous then {{math|''S'' ⊗ ''T''}} is, respectively, injective, surjective, continuous.\n\nBy choosing bases of all vector spaces involved, the linear maps {{math|''S''}} and {{math|''T''}} can be represented by [[matrix (mathematics)|matrices]]. Then, the matrix describing the tensor product {{math|''S'' ⊗ ''T''}} is the [[Kronecker product]] of the two matrices. For example, if {{math|''V'', ''X'', ''W''}}, and {{math|''Y''}} above are all two-dimensional and bases have been fixed for all of them, and {{math|''S''}} and {{math|''T''}} are given by the matrices\n\n:<math>\\begin{bmatrix}\n a_{1,1} & a_{1,2} \\\\\n a_{2,1} & a_{2,2} \\\\\n \\end{bmatrix}, \\qquad \\begin{bmatrix}\n b_{1,1} & b_{1,2} \\\\\n b_{2,1} & b_{2,2} \\\\\n \\end{bmatrix}, </math>\n\nrespectively, then the tensor product of these two matrices is\n\n:<math>\n \\begin{bmatrix}\n a_{1,1} & a_{1,2} \\\\\n a_{2,1} & a_{2,2} \\\\\n \\end{bmatrix}\n\\otimes\n \\begin{bmatrix}\n b_{1,1} & b_{1,2} \\\\\n b_{2,1} & b_{2,2} \\\\\n \\end{bmatrix}\n=\n \\begin{bmatrix}\n a_{1,1} \\begin{bmatrix}\n b_{1,1} & b_{1,2} \\\\\n b_{2,1} & b_{2,2} \\\\\n \\end{bmatrix} & a_{1,2} \\begin{bmatrix}\n b_{1,1} & b_{1,2} \\\\\n b_{2,1} & b_{2,2} \\\\\n \\end{bmatrix} \\\\\n & \\\\\n a_{2,1} \\begin{bmatrix}\n b_{1,1} & b_{1,2} \\\\\n b_{2,1} & b_{2,2} \\\\\n \\end{bmatrix} & a_{2,2} \\begin{bmatrix}\n b_{1,1} & b_{1,2} \\\\\n b_{2,1} & b_{2,2} \\\\\n \\end{bmatrix} \\\\\n \\end{bmatrix}\n=\n \\begin{bmatrix}\n a_{1,1} b_{1,1} & a_{1,1} b_{1,2} & a_{1,2} b_{1,1} & a_{1,2} b_{1,2} \\\\\n a_{1,1} b_{2,1} & a_{1,1} b_{2,2} & a_{1,2} b_{2,1} & a_{1,2} b_{2,2} \\\\\n a_{2,1} b_{1,1} & a_{2,1} b_{1,2} & a_{2,2} b_{1,1} & a_{2,2} b_{1,2} \\\\\n a_{2,1} b_{2,1} & a_{2,1} b_{2,2} & a_{2,2} b_{2,1} & a_{2,2} b_{2,2} \\\\\n \\end{bmatrix}.\n</math>\n\nThe resultant rank is at most 4, and thus the resultant dimension is 4. Note that ''rank'' here denotes the [[tensor rank]] i.e. the number of requisite indices (while the [[matrix rank]] counts the number of degrees of freedom in the resulting array).\n\nA [[dyadic product]] is the special case of the tensor product between two vectors of the same dimension.\n\n===Universal property===\n[[File:Another universal tensor prod.svg|right|thumb|200px|This [[commutative diagram]] presents the universal property of tensor product. Here <math>\\varphi</math> and <math>h</math> are bilinear, whereas <math>\\tilde{h}</math> is linear.]]\n\nIn the context of vector spaces, the tensor product <math>V \\otimes W</math> and the associated bilinear map <math>\\varphi: V \\times W \\to V \\otimes W</math> are characterized up to isomorphism by a [[universal property]] regarding [[bilinear map|bilinear maps]]. (Recall that a bilinear map is a function that is ''separately'' linear in each of its arguments.) Informally, <math>\\varphi</math> is the most general bilinear map out of <math>V \\times W</math>.\n\n{{gbq|\nThe vector space <math>V \\otimes W</math> and the associated bilinear map <math>\\varphi: V \\times W \\to V \\otimes W</math> have the property that any bilinear map <math>h: V \\times W \\to Z</math> from <math>V \\times W</math> to any vector space <math>Z</math> factors through <math>\\varphi</math> uniquely. By saying \"<math>h</math> factors through <math>\\varphi</math> uniquely,\" we mean that there is a unique linear map <math>\\tilde{h}: V \\otimes W \\to Z</math> such that <math>h = \\tilde{h} \\circ \\varphi</math>.\n}}\n\nThis characterization can simplify proofs about the tensor product. For example, the tensor product is symmetric, meaning there is a [[canonical isomorphism]]:\n:<math>V \\otimes W \\cong W \\otimes V.</math>\nTo construct, say, a map from <math>V \\otimes W</math> to <math>W \\otimes V</math>, it suffices to give a bilinear map <math>h: V \\times W \\to W \\otimes V</math> that maps <math>(v,w)</math> to <math>w \\otimes v</math>. Then the universal property of <math>V \\otimes W</math> means <math>h</math> factors into a map <math>\\tilde{h}:V \\otimes W \\to W \\otimes V</math>.\nA map <math>\\tilde{g}:W \\otimes V \\to V \\otimes W</math> in the opposite direction is similarly defined, and one checks that the two linear maps <math>\\tilde{h}</math> and <math>\\tilde{g}</math> are inverse to one another by again using their universal properties.\n\nSimilar reasoning can be used to show that the tensor product is associative, that is, there are natural isomorphisms\n:<math>V_1\\otimes(V_2\\otimes V_3)\\cong (V_1\\otimes V_2)\\otimes V_3.</math>\nTherefore, it is customary to omit the parentheses and write <math>V_1 \\otimes V_2 \\otimes V_3</math>.\n\nThe category of vector spaces with tensor product is an example of a [[symmetric monoidal category]].\n\nThe universal-property definition of a tensor product is valid in more categories than just the category of vector spaces. Instead of using multilinear (bilinear) maps, the general tensor product definition uses multimorphisms.<ref>{{cite web |url=https://ncatlab.org/nlab/show/tensor+product |title=Archived copy |accessdate=2017-09-02 |deadurl=no |archiveurl=https://web.archive.org/web/20170902184851/https://ncatlab.org/nlab/show/tensor+product |archivedate=2017-09-02 |df= }}{{user-generated source|date=September 2017}}</ref>\n\n===Tensor powers and braiding===\nLet {{math|''n''}} be a non-negative integer. The {{math|''n''}}th '''tensor power''' of the vector space {{math|''V''}} is the {{math|''n''}}-fold tensor product of {{math|''V''}} with itself. That is\n:<math>V^{\\otimes n} \\;\\overset{\\mathrm{def}}{=}\\; \\underbrace{V\\otimes\\cdots\\otimes V}_{n}.</math>\n\nA [[permutation]] {{math|''σ''}} of the set {{math|{1, 2, ..., ''n''} }} determines a mapping of the {{math|''n''}}th Cartesian power of {{math|''V''}} as follows:\n\n:<math>\\begin{cases} \\sigma : V^n\\to V^n \\\\ \\sigma(v_1,v_2,\\ldots,v_n) = \\left (v_{\\sigma(1)}, v_{\\sigma(2)},\\ldots,v_{\\sigma(n)} \\right ) \\end{cases}</math>\n\nLet\n\n:<math>\\varphi : V^n \\to V^{\\otimes n}</math>\n\nbe the natural multilinear embedding of the Cartesian power of {{math|''V''}} into the tensor power of {{math|''V''}}. Then, by the universal property, there is a unique isomorphism\n\n:<math>\\tau_\\sigma : V^{\\otimes n} \\to V^{\\otimes n}</math>\n\nsuch that\n\n:<math>\\varphi\\circ\\sigma = \\tau_\\sigma\\circ\\varphi.</math>\n\nThe isomorphism {{math|''τ<sub>σ</sub>''}} is called the '''braiding map''' associated to the permutation {{math|''σ''}}.\n\n==Product of tensors==\n{{See also|Classical treatment of tensors}}\nFor non-negative integers {{math|''r''}} and {{math|''s''}} a type {{math|(''r'',''s'')}} [[tensor]] on a vector space {{math|''V''}} is an element of\n\n:<math> T^r_s(V) = \\underbrace{ V\\otimes \\dots \\otimes V}_{r} \\otimes \\underbrace{ V^*\\otimes \\dots \\otimes V^*}_{s} = V^{\\otimes r}\\otimes V^{*\\otimes s}.</math>\n\nHere {{math|''V''<sup>∗</sup>}} is the [[dual vector space]] (which consists of all [[linear map]]s {{math|''f''}} from {{math|''V''}} to the ground field {{math|''K''}}).\n\nThere is a product map, called the ''(tensor) product of tensors''{{refn|{{harvp|Bourbaki|1989|p=244}} defines the usage \"tensor product of ''x'' and ''y''\", elements of the respective modules.}}\n:<math>T^r_s (V) \\otimes_K T^{r'}_{s'} (V) \\to T^{r+r'}_{s+s'}(V).</math>\nIt is defined by grouping all occurring \"factors\" {{math|''V''}} together: writing {{math|''v''<sub>''i''</sub>}} for an element of {{math|''V''}} and {{math|''f''<sub>''i''</sub>}} for elements of the dual space,\n:<math>(v_1 \\otimes f_1) \\otimes (v'_1) = v_1 \\otimes v'_1 \\otimes f_1.</math>\n\nPicking a basis of {{math|''V''}} and the corresponding [[dual basis]] of {{math|''V''<sup>∗</sup>}} naturally induces a basis for {{math|''T''{{su|b=''s''|p=''r''}}(''V'')}} (this basis is described in the [[Kronecker product#Relation to the abstract tensor product|article on Kronecker products]]). In terms of these bases, the [[Coordinate vector|components]] of a (tensor) product of two (or more) [[tensor]]s can be computed. For example, if {{math|''F''}} and {{math|''G''}} are two [[covariance and contravariance of vectors|covariant]] tensors of rank {{math|''m''}} and {{math|''n''}} respectively (i.e. {{math|''F'' ∈ ''T''{{su|b=''m''|p= 0}}}}, and {{math|''G'' ∈ ''T''{{su|b=''n''|p= 0}}}}), then the components of their tensor product are given by\n\n:<math>(F\\otimes G)_{i_1i_2\\ldots i_{m+n}} = F_{i_{1}i_{2}\\ldots i_{m}}G_{i_{m+1}i_{m+2}i_{m+3}\\ldots i_{m+n}}.</math>\n<ref>Analogous formulas also hold for [[covariance and contravariance of vectors|contravariant]] tensors, as well as tensors of mixed variance. Although in many cases such as when there is an [[inner product]] defined, the distinction is irrelevant.</ref>\nThus, the components of the tensor product of two tensors are the ordinary product of the components of each tensor. Another example: let {{math|'''U'''}} be a tensor of type {{math|(1, 1)}} with components {{math|''U<sup>α</sup><sub>β</sub>''}}, and let {{math|'''V'''}} be a tensor of type {{math|(1, 0)}} with components {{math|''V'' <sup>''γ''</sup>}}. Then\n:<math> U^\\alpha {}_\\beta V^\\gamma = (U \\otimes V)^\\alpha {}_\\beta {}^\\gamma </math>\nand\n:<math> V^\\mu U^\\nu {}_\\sigma = (V \\otimes U)^{\\mu \\nu} {}_\\sigma. </math>\n\nProducts of tensors form an [[algebra over a field|algebra]], called the [[tensor algebra]].\n\n==Relation to dual space==\nA particular example is the tensor product of some vector space {{math|''V''}} with its [[dual vector space]] {{math|''V''<sup>∗</sup>}} (which consists of all [[linear map]]s {{math|''f''}} from {{math|''V''}} to the ground field {{math|''K''}}). In this case, there is a canonical '''evaluation map'''\n\n:<math>V \\otimes V^* \\to K</math>\n\nwhich on elementary tensors is defined by\n\n:<math>v \\otimes f \\mapsto f(v).</math>\n\nThe resulting map\n\n:<math>T^r_s (V) \\to T^{r-1}_{s-1}(V)</math>\n\nis called [[tensor contraction]] (for {{math|''r'', ''s'' > 0}}).\n\nOn the other hand, if {{math|''V''}} is ''finite-dimensional'', there is a canonical map in the other direction (called the '''coevaluation map''')\n\n:<math>\\begin{cases} K \\to V \\otimes V^* \\\\ \\lambda \\mapsto \\sum_i \\lambda v_i \\otimes v^*_i \\end{cases}</math>\n\nwhere {{math|''v''<sub>1</sub>, ..., ''v''<sub>''n''</sub>}} is any basis of {{math|''V''}}, and {{math|''v''<sub>''i''</sub><sup>∗</sup>}} is its dual basis. Surprisingly, this map does not depend on our choice of basis.<ref>{{Cite web| url= https://unapologetic.wordpress.com/2008/11/13/the-coevaluation-on-vector-spaces/|title=The Coevaluation on Vector Spaces|date=2008-11-13| website=The Unapologetic Mathematician|access-date=2017-01-26| deadurl=no |archiveurl =https://web.archive.org/web/20170202080439/https://unapologetic.wordpress.com/2008/11/13/the-coevaluation-on-vector-spaces/| archivedate =2017-02-02|df=}}</ref>\n\nThe interplay of evaluation and coevaluation map can be used to characterize finite-dimensional vector spaces without referring to bases.<ref>See [[Compact closed category]].</ref>\n\n===Tensor product vs. Hom===\nGiven two finite dimensional vector spaces {{math|''U''}}, {{math|''V''}} over the same field {{math|''K''}}, denote the [[dual space]] of {{math|''U''}} as {{math|''U*''}}, and {{math|''K''}}-vector space of all linear maps from {{math|''U''}} to {{math|''V''}} as {{math|Hom(''U'',''V'')}}. We have the following relation:\n\n:<math> U^* \\otimes V \\cong \\mathrm{Hom}(U, V),</math>\n\nan isomorphism can be defined by <math>\\alpha: U^* \\otimes V \\to \\mathrm{Hom} (U, V)</math>, when acting on pure tensors \n\n:<math> u^* \\otimes v \\mapsto (u^* \\otimes v)(u) = u^*(u) v,</math> \n\nits \"inverse\" can be defined in a similar manner as [[Tensor product#Relation to dual space|above (Relation to dual space)]] using [[dual basis]] <math>\\{u^*_i\\}</math>, \n\n:<math>\\begin{cases} \\mathrm{Hom} (U,V) \\to U^* \\otimes V \\\\ f \\mapsto \\sum_i u^*_i \\otimes f(u_i) \\end{cases}</math>\n\nThis result implies \n\n:<math> \\dim( U \\otimes V ) =\\dim(U)\\dim(V)</math>\n\nwhich automatically gives the important fact that <math>\\{u_i\\otimes v_j\\} </math> forms a basis for <math>U \\otimes V</math> where <math> \\{u_i\\}, \\{v_j\\} </math> are bases of {{math|''U''}} and {{math|''V''}}.\n\nFurthermore, given three vector spaces {{math|''U''}}, {{math|''V''}}, {{math|''W''}} the tensor product is linked to the vector space of ''all'' linear maps, as follows:\n:<math> \\mathrm{Hom} (U \\otimes V, W) \\cong \\mathrm{Hom} (U, \\mathrm{Hom}(V, W)).</math>\nThis is an example of [[adjoint functor]]s: the tensor product is \"left adjoint\" to Hom.\n\n===Adjoint representation===\nThe tensor <math>T^r_s(V) </math> may be naturally viewed as a module for the [[Lie algebra]] {{math|End(''V'')}} by means of the diagonal action: for simplicity let us assume {{math|1=''r'' = ''s'' = 1}}, then, for each {{math|''u'' ∈ End(''V'')}},\n\n:<math> u(a \\otimes b) = u(a) \\otimes b - a \\otimes u^*(b),</math>\n\nwhere {{math|''u''<sup>∗</sup>}} in {{math|End(''V''<sup>∗</sup>)}} is the [[transpose]] of {{math|''u''}}, that is, in terms of the obvious pairing on {{math|''V'' ⊗ ''V''<sup>∗</sup>}},\n\n:<math>\\langle u(a), b \\rangle = \\langle a, u^*(b) \\rangle</math>.\n\nThere is a canonical isomorphism <math>T^1_1(V) \\to \\mathrm{End}(V) </math> given by\n\n:<math>(a \\otimes b)(x) = \\langle x, b \\rangle a. </math>\n\nUnder this isomorphism, every {{math|''u''}} in {{math|End(''V'')}} may be first viewed as an endomorphism of <math>T^1_1(V)</math> and then viewed as an endomorphism of {{math|End(''V'')}}. In fact it is the [[Adjoint representation of a Lie algebra|adjoint representation]] {{math|ad(''u'')}} of {{math|End(''V'')}}.\n\n==Tensor products of modules over a ring==\n{{main article|Tensor product of modules}}\nThe tensor product of two [[module (mathematics)|modules]] {{math|''A''}} and {{math|''B''}} over a ''[[commutative ring|commutative]]'' [[ring (mathematics)|ring]] {{math|''R''}} is defined in exactly the same way as the tensor product of vector spaces over a field:\n:<math>A \\otimes_R B := F (A \\times B) / G</math>\nwhere now {{math|''F''(''A'' × ''B'')}} is the [[free module|free {{math|''R''}}-module]] generated by the cartesian product and {{math|''G''}} is the {{math|''R''}}-module generated by [[Tensor product#The definition of the abstract tensor product|the same relations as above]].\n\nMore generally, the tensor product can be defined even if the ring is non-commutative ({{math|''ab'' ≠ ''ba''}}). In this case {{math|''A''}} has to be a right-{{math|''R''}}-module and {{math|''B''}} is a left-{{math|''R''}}-module, and instead of the last two relations above, the relation\n:<math>(ar,b)-(a,rb)</math>\nis imposed. If {{math|''R''}} is non-commutative, this is no longer an {{math|''R''}}-module, but just an [[abelian group]].\n\nThe universal property also carries over, slightly modified: the map {{math|''φ'' : ''A'' × ''B'' → ''A'' ⊗<sub>''R''</sub> ''B''}} defined by {{math|(''a'', ''b'') ↦ ''a'' ⊗ ''b''}} is a [[Tensor product of modules#Balanced product|middle linear map]] (referred to as \"the canonical middle linear map\".<ref>\n{{cite book|last=Hungerford|first=Thomas W.|title=Algebra|\npublisher=Springer|year=1974|isbn=0-387-90518-9}}</ref>); that is,<ref name=chen>\n{{citation|last=Chen|first=Jungkai Alfred|title=Advanced Algebra II|chapter=Tensor product|chapter-url=http://www.math.ntu.edu.tw/~jkchen/S04AA/S04AAL10.pdf|type=lecture notes|date=Spring 2004|place=National Taiwan University|deadurl=no|archiveurl=https://web.archive.org/web/20160304040639/http://www.math.ntu.edu.tw/~jkchen/S04AA/S04AAL10.pdf|archivedate=2016-03-04|df=}}</ref> it satisfies:\n\n:<math> \\begin{align}\n\\phi(a+a',b)=\\phi(a,b)+\\phi(a',b) \\\\\n\\phi(a,b+b')=\\phi(a,b)+\\phi(a,b') \\\\\n\\phi(ar,b)=\\phi(a,rb)\n\\end{align} </math>\n\nThe first two properties make {{math|''φ''}} a bilinear map of the [[abelian group]] {{math|''A'' × ''B''}}. For any middle linear map {{math|''ψ''}} of {{math|''A'' × ''B''}}, a unique group homomorphism {{math|''f''}} of {{math|''A'' ⊗<sub>''R''</sub> ''B''}} satisfies {{math|1=''ψ'' = ''f'' ∘ ''φ''}}, and this property determines <math> \\phi </math> within group isomorphism. See the [[tensor product of modules|main article]] for details.\n\n===Tensor product of modules over a non-commutative ring===\n\nLet A be a right R-module and B be a left R-module B. Then the tensor product of A and B is an abelian group defined by\n:<math>A \\otimes_R B := F (A \\times B) / G</math>\nwhere <math>F (A \\times B)</math> is a [[free abelian group]] over <math>A \\times B </math> and G is a subgroup of <math>F (A \\times B)</math> generated by relations\n:<math>\\begin{align}\n&\\forall a, a_1, a_2 \\in A, \\forall b, b_1, b_2 \\in B, \\forall r \\in R:\\\\\n&(a_1,b) + (a_2,b) - (a_1 + a_2,b),\\\\\n&(a,b_1) + (a,b_2) - (a,b_1+b_2),\\\\\n&(ar,b) - (a,rb).\\\\\n\\end{align}</math>\n\nThe universal property can be stated as follows. Let ''G'' be an abelian group with a map <math>q:A\\times B \\to G</math> which is bilinear, in the sense that\n\n:<math>\\begin{align}\nq(a_1+a_2,b) &=q(a_1,b)+q(a_2,b),\\\\\nq(a,b_1+b_2) &=q(a,b_1)+q(a,b_2),\\\\\nq(ar,b) &=q(a,rb).\n\\end{align}</math>\n\nThen there is a unique map <math>\\overline{q}:A\\otimes B \\to G</math> such that <math>\\overline{q}(a\\otimes b)=q(a,b)</math> for every <math>a\\in A, b\\in B</math>.\n\nFurthermore, we can give <math>A \\otimes_R B</math> a module structure under some extra conditions.\n:1) If A was a (S,R)-bimodule, then <math>A \\otimes_R B</math> is a left S-module where <math>s(a\\otimes b):=(sa)\\otimes b</math>.\n:2) If B was a (R,S)-bimodule, then <math>A \\otimes_R B</math> is a right S-module where <math>(a\\otimes b)s:=a\\otimes (bs)</math>.\n:3) If R was a commutative ring, then A and B are (R,R)-bimodules where <math>ra:=ar</math> and <math>br:=rb</math>. By 1), <math>A \\otimes_R B</math> is a left R-module. By 2), <math>A \\otimes_R B</math> is a right R-module. So we can conclude <math>A \\otimes_R B</math> is a (R,R)-bimodule.\n\n===Computing the tensor product===\nFor vector spaces, the tensor product {{math|''V'' ⊗ ''W''}} is quickly computed since bases of {{math|''V''}} of {{math|''W''}} immediately determine a basis of {{math|''V'' ⊗ ''W''}}, as was mentioned above. For modules over a general (commutative) ring, not every module is free. For example, {{math|'''Z'''/''n'''''Z'''}} is not a free abelian group (= {{math|'''Z'''}}-module). The tensor product with {{math|'''Z'''/''n'''''Z'''}} is given by\n\n:<math>M \\otimes_\\mathbf Z \\mathbf Z/n\\mathbf Z = M/nM.</math>\n\nMore generally, given a [[presentation of a module|presentation]] of some {{math|''R''}}-module {{math|''M''}}, that is, a number of generators {{math|''m''<sub>''i''</sub> ∈ ''M'', ''i'' ∈ ''I''}} together with relations \n\n:<math>\\sum_{j \\in J} a_{ji} m_i = 0, \\qquad a_{ij} \\in R,</math>\n\nthe tensor product can be computed as the following [[cokernel]]:\n\n:<math>M \\otimes_R N = \\operatorname{coker} (N^J \\to N^I)</math>\n\nHere {{math|1=''N''<sup>''J''</sup> := ⨁<sub>''j'' ∈ ''J''</sub> ''N''}} and the map is determined by sending some {{math|''n'' ∈ ''N''}} in the {{math|''j''}}th copy of {{math|''N''<sup>''J''</sup>}} to {{math|''a''<sub>''ji''</sub>''n''}} (in {{math|''N''<sup>''I''</sup>}}). Colloquially, this may be rephrased by saying that a presentation of {{math|''M''}} gives rise to a presentation of {{math|''M'' ⊗<sub>''R''</sub> ''N''}}. This is referred to by saying that the tensor product is a [[right exact functor]]. It is not in general left exact, that is, given an injective map of {{math|''R''}}-modules {{math|''M''<sub>1</sub> → ''M''<sub>2</sub>}}, the tensor product\n\n:<math>M_1 \\otimes_R N \\to M_2 \\otimes_R N</math>\n\nis not usually injective. For example, tensoring the (injective) map given by multiplication with {{math|''n''}}, {{math|''n'' : '''Z''' → '''Z'''}} with {{math|'''Z'''/''n'''''Z'''}} yields the zero map {{math|0 : '''Z'''/''n'''''Z''' → '''Z'''/''n'''''Z'''}}, which is not injective. Higher [[Tor functor]]s measure the defect of the tensor product being not left exact. All higher Tor functors are assembled in the [[derived tensor product]].\n\n==Tensor product of algebras==\n{{main article|Tensor product of algebras}}\nLet {{math|''R''}} be a commutative ring. The tensor product of {{math|''R''}}-modules applies, in particular, if {{math|''A''}} and {{math|''B''}} are [[Algebra (ring theory)|{{math|''R''}}-algebras]]. In this case, the tensor product {{math|''A'' ⊗<sub>''R''</sub> ''B''}} is an {{math|''R''}}-algebra itself by putting\n:<math>(a_1 \\otimes b_1) \\cdot (a_2 \\otimes b_2) = (a_1 \\cdot a_2) \\otimes (b_1 \\cdot b_2).</math>\nFor example,\n:<math>R[x] \\otimes_R R[y] \\cong R[x, y].</math>\n\nA particular example is when {{math|''A''}} and {{math|''B''}} are fields containing a common subfield {{math|''R''}}. The [[tensor product of fields]] is closely related to [[Galois theory]]: if, say, {{math|1=''A'' = ''R''[''x''] / ''f''(''x'')}}, where {{math|''f''}} is some [[irreducible polynomial]] with coefficients in {{math|''R''}}, the tensor product can be calculated as\n:<math>A \\otimes_R B \\cong B[x] / f(x)</math>\nwhere now {{math|''f''}} is interpreted as the same polynomial, but with its coefficients regarded as elements of {{math|''B''}}. In the larger field {{math|''B''}}, the polynomial may become reducible, which brings in Galois theory. For example, if {{math|1=''A'' = ''B''}} is a [[Galois extension]] of {{math|''R''}}, then\n:<math>A \\otimes_R A \\cong A[x] / f(x)</math>\nis isomorphic (as an {{math|''A''}}-algebra) to the {{math|''A''<sup>deg(''f'')</sup>}}.\n\n==Eigenconfigurations of tensors==\nSquare [[Matrix (mathematics)|matrices]] {{math|''A''}} with entries in a [[Field (mathematics)|field]] {{math|''K''}} represent [[linear maps]] of [[vector spaces]], say <math>K^n \\to K^n</math>, and thus linear maps <math>\\psi : \\mathbb{P}^{n-1} \\to \\mathbb{P}^{n-1}</math> of [[projective spaces]] over <math>K</math>. If {{math|''A''}} is [[Invertible matrix|nonsingular]] then <math>\\psi</math> is [[well-defined]] everywhere, and the [[Eigenvalues and eigenvectors|eigenvectors]] of <math>A</math> correspond to the fixed points of <math>\\psi</math>. The ''eigenconfiguration'' of {{math|''A''}} consists of <math>n</math> points in <math>\\mathbb{P}^{n-1}</math>, provided <math>A</math> is generic and {{math|''K''}} is [[Algebraically closed field|algebraically closed]]. The fixed points of nonlinear maps are the eigenvectors of tensors. Let <math>A = (a_{i_1 i_2 \\cdots i_d})</math> be a <math>d</math>-dimensional tensor of format <math>n \\times n \\times \\cdots \\times n</math> with entries <math>(a_{i_1 i_2 \\cdots i_d})</math> lying in an algebraically closed field <math>K</math> of [[Characteristic (algebra)|characteristic]] zero. Such a tensor <math>A \\in (K^{n})^{\\otimes d}</math> defines [[Morphism of algebraic varieties|polynomial maps]] <math>K^n \\to K^n</math> and <math>\\mathbb{P}^{n-1} \\to \\mathbb{P}^{n-1}</math> with coordinates\n \n:<math>\\psi_i(x_1, ..., x_n) = \\sum_{j_2=1}^n \\sum_{j_3=1}^n \\cdots \\sum_{j_d = 1}^n a_{i j_2 j_3 \\cdots j_d} x_{j_2} x_{j_3}\\cdots x_{j_d} \\;\\; \\mbox{for } i = 1,...,n </math>\n\nThus each of the <math>n</math> coordinates of <math>\\psi</math> is a [[homogeneous polynomial]] <math> \\psi_i </math> of degree <math>d-1</math> in <math>\\mathbf{x} = (x_1, \\ldots, x_n)</math>. The eigenvectors of <math>A</math> are the solutions of the constraint \n\n:<math>\\mbox{rank} \\begin{pmatrix}x_1 & x_2 & \\cdots & x_n \\\\ \\psi_1(\\mathbf{x}) & \\psi_2(\\mathbf{x}) & \\cdots & \\psi_n(\\mathbf{x}) \\end{pmatrix} \\leq 1 </math> \n\nand the eigenconfiguration is given by the [[Algebraic variety|variety]] of the <math>2 \\times 2</math> [[Minor (linear algebra)|minors]] of this matrix.<ref>Abo, H.; Seigal, A.; Sturmfels B. arXiv:1505.05729 [math.AG]</ref>\n\n==Other examples of tensor products==\n\n===Tensor product of Hilbert spaces===\n{{main article|Tensor product of Hilbert spaces}}\n\n[[Hilbert space]]s generalize finite-dimensional vector spaces to [[countable|countably-infinite]] dimensions. The tensor product is still defined; it is the [[tensor product of Hilbert spaces]].\n\n===Topological tensor product===\n{{main article|Topological tensor product}}\n\nWhen the basis for a vector space is no longer countable, then the appropriate axiomatic formalization for the vector space is that of a [[topological vector space]]. The tensor product is still defined, it is the [[topological tensor product]].\n\n===Tensor product of graded vector spaces===\n{{main article|Graded vector space#Operations on graded vector spaces}}\n\nSome vector spaces can be decomposed into [[direct sum]]s of subspaces. In such cases, the tensor product of two spaces can be decomposed into sums of products of the subspaces (in analogy to the way that multiplication distributes over addition).\n\n===Tensor product of representations===\n{{main|Tensor product of representations}}\n\nVector spaces endowed with an additional multiplicative structure are called [[algebra over a field|algebras]]. The tensor product of such algebras is described by the [[Littlewood–Richardson rule]].\n\n===Tensor product of quadratic forms===\n{{main article|Tensor product of quadratic forms}}\n\n===Tensor product of multilinear forms===\nGiven two [[multilinear form]]s <math>f(x_1,\\dots,x_k)</math> and <math>g (x_1,\\dots, x_m)</math> on a vector space <math>V</math> over the field <math>K</math> their tensor product is the multilinear form \n\n:<math> (f \\otimes g) (x_1,\\dots,x_{k+m}) = f(x_1,\\dots,x_k) g(x_{k+1},\\dots,x_{k+m}).</math><ref name=\"An Introduction to Manifolds\">{{cite book |title=An Introduction to Manifolds | first=L. W. | last=Tu |series=Universitext |publisher=Springer | page=25 | isbn=978-1-4419-7399-3 | year=2010}}</ref>\n\nThis is a special case of the [[Tensor product#Product of tensors|product of tensors]] if they are seen as multilinear maps (see also [[Tensor#As multilinear maps|tensors as multilinear maps]]). Thus the components of the tensor product of multilinear forms can be computed by the [[Kronecker product]].\n\n===Tensor product of sheaves of modules===\n{{main article|Sheaf of modules}}\n\n===Tensor product of line bundles===\n{{main article|Vector bundle#Operations on vector bundles}}\n\n{{See also|tensor product bundle}}\n\n===Tensor product of fields===\n{{main article|Tensor product of fields}}\n\n===Tensor product of graphs===\n{{main article|Tensor product of graphs}}\nIt should be mentioned that, though called \"tensor product\", this is not a tensor product of graphs in the above sense; actually it is the [[Product (category theory)|category-theoretic product]] in the category of graphs and [[graph homomorphism]]s. However it is actually the [[Kronecker product|Kronecker tensor product]] of the [[adjacency matrix|adjacency matrices]] of the graphs. Compare also the section [[tensor product#Tensor product of linear maps|Tensor product of linear maps]] above.\n\n===Monoidal categories===\n\nThe most general setting for the tensor product is the [[monoidal category]]. It captures the algebraic essence of tensoring, without making any specific reference to what is being tensored. Thus, all tensor products can be expressed as an application of the monoidal category to some particular setting, acting on some particular objects.\n\n==Quotient algebras==\nA number of important subspaces of the [[tensor algebra]] can be constructed as [[quotient space (linear algebra)|quotients]]: these include the [[exterior algebra]], the [[symmetric algebra]], the [[Clifford algebra]], the [[Weyl algebra]], and the [[universal enveloping algebra]] in general. \n\nThe exterior algebra is constructed from the [[exterior product]]. Given a vector space {{math|''V''}}, the exterior product <math>V \\wedge V</math> is defined as\n:<math>V \\wedge V := V \\otimes V/(v\\otimes v \\text{ for all } v\\in V).</math>\nNote that when the underlying field of {{math|''V''}} does not have characteristic 2, then this definition is equivalent to\n:<math>V \\wedge V := V \\otimes V / (v_1 \\otimes v_2 + v_2 \\otimes v_1 \\text{ for all } v_1, v_2 \\in V).</math>\nThe image of <math>v_1 \\otimes v_2</math> in the exterior product is usually denoted <math>v_1 \\wedge v_2</math> and satisfies, by construction, <math>v_1 \\wedge v_2 = - v_2 \\wedge v_1</math>. Similar constructions are possible for <math>V \\otimes \\dots \\otimes V</math> ({{math|''n''}} factors), giving rise to <math>\\Lambda^n V</math>, the {{math|''n''}}th [[exterior power]] of {{math|''V''}}. The latter notion is the basis of [[differential form|differential {{math|''n''}}-forms]].\n\nThe symmetric algebra is constructed in a similar manner, from the [[symmetric tensor#symmetric product|symmetric product]]\n:<math>V \\odot V := V \\otimes V / (v_1 \\otimes v_2 - v_2 \\otimes v_1 \\text{ for all } v_1, v_2 \\in V).</math>\nMore generally\n:<math>\\operatorname{Sym}^n V := \\underbrace{V \\otimes \\dots \\otimes V}_n / (\\dots \\otimes v_i \\otimes v_{i+1} \\otimes \\dots - \\dots \\otimes v_{i+1} \\otimes v_{i} \\otimes \\dots)</math>\nThat is, in the symmetric algebra two adjacent vectors (and therefore all of them) can be interchanged. The resulting objects are called [[symmetric tensor]]s.\n\nAdditional algebras result from quotienting by other polynomials; the general case is given by the [[universal enveloping algebra]]s.\n\n==Tensor product in programming==\n\n===Array programming languages===\n[[Array programming languages]] may have this pattern built in. For example, in [[APL programming language|APL]] the tensor product is expressed as <code>○.×</code> (for example <code>A ○.× B</code> or <code>A ○.× B ○.× C</code>). In [[J programming language|J]] the tensor product is the dyadic form of <code>*/</code> (for example <code>a */ b</code> or <code>a */ b */ c</code>).\n\nNote that J's treatment also allows the representation of some tensor fields, as <code>a</code> and <code>b</code> may be functions instead of constants. This product of two functions is a derived function, and if <code>a</code> and <code>b</code> are [[Differentiable function|differentiable]], then <code>a */ b</code> is differentiable.\n\nHowever, these kinds of notation are not universally present in array languages. Other array languages may require explicit treatment of indices (for example, [[MATLAB]]), and/or may not support [[higher-order function]]s such as the [[Jacobian matrix and determinant|Jacobian derivative]] (for example, [[Fortran]]/APL).\n\n==See also==\n{{wiktionary}}\n*[[Dyadic product]]\n*[[Extension of scalars]]\n*[[Tensor algebra]]\n*[[Tensor contraction]]\n*[[Topological tensor product]]\n*[[Monoidal category]]\n\n==Notes==\n{{Reflist}}\n\n==References==\n*{{cite book |first = Nicolas|last=Bourbaki|authorlink=Nicolas Bourbaki | title = Elements of mathematics, Algebra I| publisher = Springer-Verlag | year = 1989|isbn=3-540-64243-9}}\n*{{cite web|last=Gowers|first=Timothy|authorlink=Tim Gowers|url=https://www.dpmms.cam.ac.uk/~wtg10/tensors3.html|title=How to lose your fear of tensor products}}\n*{{cite book |first=Pierre A.|last=Grillet|title=Abstract Algebra|year=2007|publisher=Springer Science+Business Media, LLC| isbn= 0387715673}}\n*{{cite book |authorlink=Paul Halmos|first=Paul|last=Halmos|title=Finite dimensional vector spaces| year=1974 |publisher= Springer |isbn= 0-387-90093-4}}\n*{{cite book |first=Thomas W.|last=Hungerford|authorlink=Thomas W. Hungerford| title=Algebra |year=2003 |publisher= Springer |isbn= 0387905189}}\n*{{Lang Algebra|edition=3r}}\n*{{cite book |first1=S.|last1=Mac Lane|authorlink1=Saunders Mac Lane|authorlink2=Garrett Birkhoff |last2= Birkhoff |first2= G. |title= Algebra|publisher=AMS Chelsea|year=1999|isbn=0-8218-1646-2}}\n*{{cite book |first1=M.|last1=Aguiar|first2=S.|last2=Mahajan| title = Monoidal functors, species and Hopf algebras|publisher = CRM Monograph Series Vol 29 |year=2010|isbn=0-8218-4776-7}}\n*{{cite web |url=http://pages.bangor.ac.uk/~mas010/nonabtens.html |title=Bibliography on the nonabelian tensor product of groups }}\n\n{{tensors}}\n\n{{DEFAULTSORT:Tensor Product}}\n[[Category:Binary operations]]\n[[Category:Bilinear operators]]"
    },
    {
      "title": "Tensor product of modules",
      "url": "https://en.wikipedia.org/wiki/Tensor_product_of_modules",
      "text": "In [[mathematics]], the '''tensor product of modules''' is a construction that allows arguments about [[bilinear map|bilinear]] maps (e.g. multiplication) to be carried out in terms of [[module homomorphism|linear map]]s. The module construction is analogous to the construction of the [[tensor product]] of [[vector space]]s, but can be carried out for a pair of [[module (mathematics)|modules]] over a [[commutative ring]] resulting in a third module, and also for a pair of a right-module and a left-module over any [[ring (mathematics)|ring]], with result an [[abelian group]]. Tensor products are important in areas of [[abstract algebra]], [[homological algebra]], [[algebraic topology]], [[algebraic geometry]], [[operator algebras]] and [[noncommutative geometry]]. The [[universal property]] of the tensor product of vector spaces extends to more general situations in abstract algebra. It allows the study of bilinear or multilinear operations via [[linear operator|linear operations]]. The tensor product of an algebra and a module can be used for [[extension of scalars]]. For a commutative ring, the tensor product of modules can be iterated to form the [[tensor algebra]] of a module, allowing one to define multiplication in the module in a universal way.\n\n==Balanced product==\n{{main|pairing}}\nFor a ring ''R'', a right ''R''-module ''M'', a left ''R''-module ''N'', and an abelian group ''G'', a map {{nowrap|''φ'': ''M'' × ''N'' → ''G''}} is said to be '''''R''-balanced''', '''''R''-middle-linear''' or an '''''R''-balanced product''' if for all ''m'', ''m''′ in ''M'', ''n'', ''n''′ in ''N'', and ''r'' in ''R'' the following hold:\n\n:<math>\\begin{align}\n\\phi (m, n+n') &= \\phi (m, n) + \\phi (m, n') && \\text{Dl}_{\\phi} \\\\\n\\phi (m +m', n) &= \\phi (m, n) + \\phi (m', n) && \\text{Dr}_{\\phi} \\\\\n\\phi (m \\cdot r, n) &= \\phi (m, r \\cdot n) && \\text{A}_{\\phi} \\\\\n\\end{align}</math>\n\nThe set of all such balanced products over ''R'' from {{nowrap|''M'' × ''N''}} to ''G'' is denoted by {{nowrap|L<sub>''R''</sub>(''M'', ''N''; ''G'')}}.\n\nIf ''φ'', ''ψ'' are balanced products, then each of the operations {{nowrap|''φ'' + ''ψ''}} and −''φ'' defined [[pointwise]] is a balanced product. This turns the set {{nowrap|L<sub>''R''</sub>(''M'', ''N''; ''G'')}} into an abelian group.\n\nFor ''M'' and ''N'' fixed, the map {{nowrap|''G'' ↦ L<sub>''R''</sub>(''M'', ''N''; ''G'')}} is a [[functor]] from the [[category of abelian groups]] to the [[category of sets]]. The morphism part is given by mapping a group homomorphism {{nowrap|''g'' : ''G'' → ''G''′}} to the function {{nowrap|''φ'' ↦ ''g'' ∘ ''φ''}}, which goes from {{nowrap|L<sub>''R''</sub>(''M'', ''N''; ''G'')}} to {{nowrap|L<sub>''R''</sub>(''M'', ''N''; ''G''′)}}.\n\n;Remarks:\n#Property (Dl) states the left and property (Dr) the right [[Distributive property|distributivity]] of ''φ'' over addition.\n#Property (A) resembles some [[associative property]] of ''φ''.\n#Every ring ''R'' is an ''R''-''R''-[[bimodule]]. So the ring multiplication {{nowrap|(''r'', ''r''′) ↦ ''r'' ⋅ ''r''′}} in ''R'' is an ''R''-balanced product {{nowrap|''R'' × ''R'' → ''R''}}.\n\n==Definition==\nFor a ring ''R'', a right ''R''-module ''M'', a left ''R''-module ''N'', the '''tensor product''' over ''R''\n\n:<math>M \\otimes_R N</math>\n\nis an [[abelian group]] together with a balanced product (as defined above)\n\n:<math>\\otimes : M \\times N \\to M \\otimes_{R} N</math>\n\nwhich is [[universal property|universal]] in the following sense:<ref>Hazewinkel, et al. (2004), [https://books.google.com/books?id=AibpdVNkFDYC&pg=PA95 p. 95], Prop. 4.5.1</ref>\n\n[[File:Tensor product of modules2.svg|200px|right]]\n:For every abelian group ''G'' and every balanced product\n::<math>f: M \\times N \\to G</math>\n:there is a '''unique''' group homomorphism\n::<math> \\tilde{f}: M \\otimes_R N \\to G</math>\n:such that\n::<math>\\tilde{f} \\circ \\otimes = f.</math>\n\nAs with all [[Universal property#Existence and uniqueness|universal properties]], the above property defines the tensor product uniquely [[up to]] a unique isomorphism: any other object and balanced product with the same properties will be isomorphic to {{nowrap|''M'' ⊗<sub>''R''</sub> ''N''}} and ⊗. Indeed, the mapping ⊗ is called ''canonical'', or more explicitly: the canonical mapping (or balanced product) of the tensor product.<ref>{{harvnb|Bourbaki|loc=ch. II §3.1}}</ref>\n\nThe definition does not prove the existence of {{nowrap|''M'' ⊗<sub>''R''</sub> ''N''}}; see below for a construction.\n\nThe tensor product can also be defined as a [[representable functor|representing object]] for the functor {{nowrap|''G'' → L<sub>''R''</sub>(''M'',''N'';''G'')}}; explicitly, this means there is a [[natural isomorphism]]:\n\n:<math>\\begin{cases}\\operatorname{Hom}_{\\Z} (M \\otimes_R N, G) \\simeq \\operatorname{L}_R(M, N; G) \\\\ g \\mapsto g \\circ \\otimes \\end{cases}</math>\n\nThis is a succinct way of stating the universal mapping property given above. (if a priori one is given this is natural isomorphism, then <math>\\otimes</math> can be recovered by taking <math>G = M \\otimes_R N</math> and then mapping the identity map.)\n\nSimilarly, given the natural identification <math>\\operatorname{L}_R(M, N; G) = \\operatorname{Hom}_R(M, \\operatorname{Hom}_{\\Z}(N, G))</math>,<ref>First, if <math>R=\\Z,</math> then the claimed identification is given by <math>f \\mapsto f'</math> with <math>f'(x)(y) = f(x, y)</math>. In general, <math>\\operatorname{Hom}_{\\Z }(N, G)</math> has the structure of a right ''R''-module by <math>(g \\cdot r)(y) = g(r y)</math>. Thus, for any <math>\\Z</math>-bilinear map ''f'', ''f''′ is ''R''-linear <math>\\Leftrightarrow f'(xr) = f'(x) \\cdot r \\Leftrightarrow f(xr, y) = f(x, ry).</math></ref> one can also define {{nowrap|''M'' ⊗<sub>''R''</sub> ''N''}} by the formula\n\n:<math>\\operatorname{Hom}_{\\Z} (M \\otimes_R N, G) \\simeq \\operatorname{Hom}_R(M, \\operatorname{Hom}_{\\Z}(N, G)).</math>\n\nThis is known as the [[tensor-hom adjunction]]; see also {{section link||Properties}}.\n\nFor each ''x'' in ''M'', ''y'' in ''N'', one writes\n:''x'' ⊗ ''y''\nfor the image of (''x'', ''y'') under the canonical map <math>\\otimes: M \\times N \\to M \\otimes_R N</math>. It is often called a [[pure tensor]]. Strictly speaking, the correct notation would be ''x'' ⊗<sub>''R''</sub> ''y'' but it is conventional to drop ''R'' here. Then, immediately from the definition, there are relations:\n:{|\n|-\n| style=\"width:24em;\" | ''x'' ⊗ (''y'' + ''y''′) = ''x'' ⊗ ''y'' + ''x'' ⊗ ''y''′ || (Dl<sub>⊗</sub>)\n|-\n| (''x'' + ''x''′) ⊗ ''y'' = ''x'' ⊗ ''y'' + ''x''′ ⊗ ''y'' || (Dr<sub>⊗</sub>)\n|-\n| (''x'' ⋅ ''r'') ⊗ ''y'' = ''x'' ⊗ (''r'' ⋅ ''y'') || (A<sub>⊗</sub>)\n|}\n\nThe universal property of a tensor product has the following important consequence:\n\n{{math_theorem|name=Proposition|Every element of <math>M \\otimes_R N</math> can be written, non-uniquely, as\n:<math>\\sum_i x_i \\otimes y_i, \\, x_i \\in M, y_i \\in N.</math>\nIn other words, the image of <math>\\otimes</math> generates <math>M \\otimes_R N</math>. Furthermore, if ''f'' is a function defined on elements <math>x \\otimes y</math> with values in an abelian group ''G'', then ''f'' extends uniquely to the homomorphism defined on the whole <math>M \\otimes_R N</math> if and only if <math>f(x \\otimes y)</math> is <math>\\Z</math>-bilinear in ''x'' and ''y''.}}\n\nProof: For the first statement, let ''L'' be the subgroup of <math>M \\otimes_R N</math> generated by elements of the form in question, <math>Q = (M \\otimes_R N) / L</math> and ''q'' the quotient map to ''Q''. We have: <math>0 = q \\circ \\otimes</math> as well as <math>0 = 0 \\circ \\otimes</math>. Hence, by the uniqueness part of the universal property, ''q'' = 0. The second statement is because to define a [[module homomorphism]], it is enough to define it on the generating set of the module. <math>\\square</math>\n\nThe proposition says that one can work with explicit elements of the tensor products instead of invoking the universal property directly each time. This is very convenient in practice. For example, if ''R'' is commutative, then <math>M \\otimes_R N</math> can naturally be furnished with the ''R''-scalar multiplication by extending\n\n:<math>r \\cdot (x \\otimes y) := rx \\otimes y = x \\otimes ry</math>,\n\nto the whole <math>M \\otimes_R N</math> by the previous proposition (strictly speaking, what is needed is a bimodule structure not commutativity; see a paragraph below). Equipped with this ''R''-module structure, <math>M \\otimes_R N</math> satisfies a universal property similar to the above: for any ''R''-module ''G'', there is a natural isomorphism:\n\n:<math>\\begin{cases} \\operatorname{Hom}_R(M \\otimes_R N, G) \\simeq \\{R\\text{-bilinear maps } M \\times N \\to G \\}, \\\\ g \\mapsto g \\circ \\otimes \\end{cases}</math>\n\nIf ''R'' is not necessarily commutative but if ''M'' has a left action by a ring ''S'' (for example, ''R''), then <math>M \\otimes_R N</math> can be given the left ''S''-module structure, like above, by the formula\n\n:<math>s \\cdot (x \\otimes y) := sx \\otimes y.</math>\n\nIf ''N'' has a right action by a ring ''S'', then, in the analogous way, <math>M \\otimes_R N</math> becomes a right ''S''-module.<!-- Doesn't seem correct; see the example below. Strictly speaking, the ring used to form the tensor should be indicated: most modules can be considered as modules over several different rings or over the same ring with a different actions of the ring on the module elements. For example, it can be shown that {{nowrap|'''R''' ⊗<sub>'''R'''</sub> '''R'''}} and {{nowrap|'''R''' ⊗<sub>'''Z'''</sub> '''R'''}} are completely different from each other. However in practice, whenever the ring is clear from context, the subscript denoting the ring may be dropped.-->\n\n===Tensor product of linear maps and a change of base ring===\nGiven linear maps <math>f: M \\to M'</math> of right modules over a ring ''R'' and <math>g: N \\to N'</math> of left modules, there is a unique group homomorphism\n\n:<math>\\begin{cases}f \\otimes g: M \\otimes _R N \\to M' \\otimes_R N' \\\\ x \\otimes y \\mapsto f(x) \\otimes g(y) \\end{cases}</math>\n\nThe construction has a consequence that tensoring is a functor: each right ''R''-module ''M'' determines the functor\n\n:<math>M \\otimes_R -: R\\text{-Mod} \\longrightarrow \\text{Ab}</math>\n\nfrom the [[category of modules|category of left modules]] to the category of abelian groups that sends ''N'' to {{nowrap|''M'' ⊗ ''N''}} and a module homomorphism ''f'' to the group homomorphism {{nowrap|1 ⊗ ''f''}}.<!-- unfortunately, this doesn't quite work as written unless ''S'' is an ''R''-algebra (say ''S'' = ''R''). '''Example''': Let ''M'' be a right ''R''-module. A left action on ''M'' by a ring ''S'' is the same thing as a group homomorphism:\n:<math>S \\otimes_R M \\to M</math>.\nTensoring this with a left ''R''-module ''N'' results in\n:<math>(S \\otimes_R M) \\otimes_R N \\to M \\otimes_R N</math>.\nHere, the group on the left is really <math>S \\otimes_R (M \\otimes_R N)</math> by associativity (see below) and so this shows <math>M \\otimes_R N</math> is a left ''S''-module.-->\n\nIf <math>f: R \\to S</math> is a ring homomorphism and if ''M'' is a right ''S''-module and ''N'' a left ''S''-module, then there is the canonical ''surjective'' homomorphism:\n\n:<math>M \\otimes_R N \\to M \\otimes_S N</math>\n\ninduced by \n\n:<math>M \\times N \\overset{\\otimes_S} \\longrightarrow M \\otimes_S N.</math><ref>{{harvnb|Bourbaki|loc=ch. II §3.2.}}</ref> \n\nThe resulting map is surjective since pure tensors {{nowrap|''x'' ⊗ ''y''}} generate the whole module. In particular, taking ''R'' to be <math>\\Z</math> this shows every tensor product of modules is a quotient of a tensor product of abelian groups.\n\nSee also: {{section link|Tensor product|Tensor product of linear maps}}.\n\n===Several modules===\n(This section need to be updated. For now, see {{section link||Properties}} for the more general discussion.)\n\nIt is possible to extend the definition to a tensor product of any number of modules over the same commutative ring. For example, the universal property of\n\n:''M''<sub>1</sub> ⊗ ''M''<sub>2</sub> ⊗ ''M''<sub>3</sub>\n\nis that each trilinear map on\n\n:''M''<sub>1</sub> × ''M''<sub>2</sub> × ''M''<sub>3</sub> → ''Z''\n\ncorresponds to a unique linear map\n\n:''M''<sub>1</sub> ⊗ ''M''<sub>2</sub> ⊗ ''M''<sub>3</sub> → ''Z''.\n\nThe binary tensor product is associative: (''M''<sub>1</sub> ⊗ ''M''<sub>2</sub>) ⊗ ''M''<sub>3</sub> is naturally isomorphic to ''M''<sub>1</sub> ⊗ (''M''<sub>2</sub> ⊗ ''M''<sub>3</sub>). The tensor product of three modules defined by the universal property of trilinear maps is isomorphic to both of these iterated tensor products.\n\n==Properties==\n\n===Modules over general rings===\nLet ''R''<sub>1</sub>, ''R''<sub>2</sub>, ''R''<sub>3</sub>, ''R'' be rings, not necessarily commutative.\n\n*For an ''R''<sub>1</sub>-''R''<sub>2</sub>-[[bimodule]] ''M''<sub>12</sub> and a left ''R''<sub>2</sub>-module ''M''<sub>20</sub>, <math>M_{12}\\otimes_{R_2} M_{20}</math> is a left ''R''<sub>1</sub>-module.\n\n*For a right ''R''<sub>2</sub>-module ''M''<sub>02</sub> and an ''R''<sub>2</sub>-''R''<sub>3</sub>-[[bimodule]] ''M''<sub>23</sub>, <math>M_{02}\\otimes_{R_2} M_{23}</math> is a right ''R''<sub>3</sub>-module.\n\n*(associativity) For a right ''R''<sub>1</sub>-module ''M''<sub>01</sub>, an ''R''<sub>1</sub>-''R''<sub>2</sub>-bimodule ''M''<sub>12</sub>, and a left ''R''<sub>2</sub>-module ''M''<sub>20</sub> we have:<ref>{{harvnb|Bourbaki|loc=ch. II §3.8}}</ref>\n::<math>\\left (M_{01} \\otimes_{R_1} M_{12} \\right ) \\otimes_{R_2} M_{20} = M_{01} \\otimes_{R_1} \\left (M_{12}\\otimes_{R_2} M_{20} \\right ).</math>\n\n*Since ''R'' is an ''R''-''R''-bimodule, we have <math>R\\otimes_R R = R</math> with the ring multiplication <math>mn =: m\\otimes_R n</math> as its canonical balanced product.\n\n===Modules over commutative rings===\nLet ''R'' be a commutative ring, and ''M'', ''N'' and ''P'' be ''R''-modules. Then\n\n*(identity) <math>R \\otimes_R M = M.</math>\n\n*(associativity) <math>(M \\otimes_R N) \\otimes_R P = M \\otimes_R (N \\otimes_R P).</math><ref>The first three properties (plus identities on morphisms) say that the category of ''R''-modules, with ''R'' commutative, forms a [[symmetric monoidal category]].</ref> Thus <math>M \\otimes_R N \\otimes_R P</math> is well-defined.\n\n*(symmetry) <math>M \\otimes_R N = N \\otimes_R M.</math> In fact, for any permutation ''σ'' of the set {1, ..., ''n''}, there is a unique isomorphism:\n::<math>\\begin{cases} M_1 \\otimes_R \\cdots \\otimes_R M_n \\longrightarrow M_{\\sigma(1)} \\otimes_R \\cdots \\otimes_R M_{\\sigma(n)} \\\\ x_1 \\otimes \\cdots \\otimes x_n \\longmapsto x_{\\sigma(1)} \\otimes \\cdots \\otimes x_{\\sigma(n)} \\end{cases}</math>\n\n*(distributive property) <math>M \\otimes_R (N \\oplus P) = (M \\otimes_R N) \\oplus (M \\otimes_R P).</math> In fact,\n::<math>M \\otimes_R \\left (\\bigoplus\\nolimits_{i \\in I} N_i \\right ) = \\bigoplus\\nolimits_{i \\in I}  \\left ( M \\otimes_R N_i \\right ),</math>\n:for an [[index set]] ''I'' of arbitrary [[cardinality]].\n\n*(commutes with finite product) for any finitely many <math>N_i</math>,\n::<math>M \\otimes_R \\prod_{i = 1}^n N_i = \\prod_{i = 1}^nM \\otimes_R N_i.</math>\n\n*(commutes with [[localization of a module|localization]]) for any multiplicatively closed subset ''S'' of ''R'',\n::<math>S^{-1}(M \\otimes_R N) = S^{-1}M \\otimes_{S^{-1}R} S^{-1}N</math>\n:as <math>S^{-1} R</math>-module. Since <math>S^{-1} R</math> is an ''R''-algebra and <math>S^{-1} - = S^{-1} R \\otimes_R -</math>, this is a special case of:\n\n*(commutes with base extension) If ''S'' is an ''R''-algebra, writing <math>-_{S} = S \\otimes_R -</math>,\n::<math>(M \\otimes_R N)_S = M_S \\otimes_S N_S;</math><ref>Proof: (using associativity in a general form) <math>(M \\otimes_R N)_S = (S \\otimes_R M) \\otimes_R N = M_S \\otimes_R N = M_S \\otimes_S S \\otimes_R N = M_S \\otimes_S N_S</math></ref>\n:cf. {{section link||Extension of scalars}}.\n\n*(commutes with direct limit) for any direct system of ''R''-modules ''M''<sub>''i''</sub>,\n::<math>(\\varinjlim M_i) \\otimes_R N = \\varinjlim (M_i \\otimes_R N).</math>\n\n*(tensoring is right exact) if \n::<math>0 \\to N' \\overset{f}\\to N \\overset{g}\\to N'' \\to 0</math> \n:is an exact sequence of ''R''-modules, then\n::<math>M \\otimes_R N' \\overset{1 \\otimes f}\\to M \\otimes_R N \\overset{1 \\otimes g}\\to M \\otimes_R N'' \\to 0</math>\n:is an exact sequence of ''R''-modules, where <math>(1 \\otimes f)(x \\otimes y) = x \\otimes f(y).</math> This is a consequence of:\n\n*([[tensor-hom adjunction|adjoint relation]]) <math>\\operatorname{Hom}_R(M \\otimes_R N, P) = \\operatorname{Hom}_R(M, \\operatorname{Hom}_R(N, P))</math>.\n\n*(tensor-hom relation) there is a canonical ''R''-linear map:\n::<math>\\operatorname{Hom}_R(M, N) \\otimes P \\to \\operatorname{Hom}_R(M, N \\otimes P),</math>\n:which is an isomorphism if either ''M'' or ''P'' is a [[finitely generated projective module]] (see {{section link||As linearity-preserving maps}} for the non-commutative case);<ref>{{harvnb|Bourbaki|loc=ch. II §4.4}}</ref> more generally, there is a canonical ''R''-linear map:\n::<math>\\operatorname{Hom}_R(M, N) \\otimes \\operatorname{Hom}_R(M', N') \\to \\operatorname{Hom}_R(M \\otimes M', N \\otimes N')</math>\n:which is an isomorphism if either <math>(M, N)</math> or <math>(M, M')</math> is a pair of finitely generated projective modules.\n\nTo give a practical example, suppose ''M'', ''N'' are free modules with bases <math>e_i, i \\in I</math> and <math>f_j, j \\in J</math>. Then ''M'' is the [[direct sum of modules|direct sum]] <math>M = \\bigoplus_{i \\in I} R e_i</math>\nand the same for ''N''. By the distributive property, one has:\n\n:<math>M \\otimes_R N = \\bigoplus_{i, j} R(e_i \\otimes f_j)</math>;\n\ni.e., <math>e_i \\otimes f_j, \\, i \\in I, j \\in J</math> are the ''R''-basis of <math>M \\otimes_R N</math>. Even if ''M'' is not free, a [[free presentation]] of ''M'' can be used to compute tensor products.\n\nThe tensor product, in general, does not commute with [[inverse limit]]: on the one hand,\n\n:<math>\\Q \\otimes_{\\Z} \\Z /p^n = 0</math>\n\n(cf. \"examples\"). On the other hand,\n\n:<math> \\left (\\varprojlim \\Z /p^n \\right ) \\otimes_{\\Z} \\Q = \\Z_p \\otimes_{\\Z} \\Q = \\Z_p \\left [p^{-1} \\right ] = \\Q_p</math>\n\nwhere <math>\\Z_p, \\Q_p</math> are the [[ring of p-adic integers]] and the [[field of p-adic numbers]]. See also \"[[profinite integer]]\" for an example in the similar spirit.\n\nIf ''R'' is not commutative, the order of tensor products could matter in the following way: we \"use up\" the right action of ''M'' and the left action of ''N'' to form the tensor product <math>M \\otimes_R N</math>; in particular, <math>N \\otimes_R M</math> would not even be defined. If ''M'', ''N'' are bi-modules, then <math>M \\otimes_R N</math> has the left action coming from the left action of ''M'' and the right action coming from the right action of ''N''; those actions need not be the same as the left and right actions of <math>N \\otimes_R M</math>.\n\nThe associativity holds more generally for non-commutative rings: if ''M'' is a right ''R''-module, ''N'' a (''R'', ''S'')-module and ''P'' a left ''S''-module, then\n\n:<math>(M \\otimes_R N) \\otimes_S P = M \\otimes_R (N \\otimes_S P)</math>\n\nas abelian group.\n\nThe general form of adjoint relation of tensor products says: if ''R'' is not necessarily commutative, ''M'' is a right ''R''-module, ''N'' is a (''R'', ''S'')-module, ''P'' is a right ''S''-module, then as abelian group\n\n:<math>\\operatorname{Hom}_S(M \\otimes_R N, P) = \\operatorname{Hom}_R(M, \\operatorname{Hom}_S(N, P)), \\, f \\mapsto f'</math><ref>{{harvnb|Bourbaki|loc=ch.II §4.1 Proposition 1}}</ref>\n\nwhere <math>f'</math> is given by <math>f'(x)(y) = f(x \\otimes y).</math> See also: [[tensor-hom adjunction]].\n\n===Tensor product of an ''R''-module with the fraction field===\nLet ''R'' be an integral domain with [[Field of fractions|fraction field]] ''K''.\n\n*For any ''R''-module ''M'', <math>K \\otimes_R M \\cong K \\otimes_R (M / M_{\\operatorname{tor}})</math> as ''R''-modules, where <math>M_{\\operatorname{tor}}</math> is the torsion submodule of ''M''.\n\n*If ''M'' is a torsion ''R''-module then <math>K \\otimes_R M = 0</math> and if ''M'' is not a torsion module then <math>K \\otimes_R M \\ne 0</math>.\n\n*If ''N'' is a submodule of ''M'' such that <math>M/N</math> is a torsion module then <math>K \\otimes_R N \\cong K \\otimes_R M</math> as ''R''-modules by <math>x \\otimes n \\mapsto x \\otimes n</math>.\n\n*In <math>K \\otimes_R M</math>, <math>x \\otimes m = 0</math> if and only if <math>x = 0</math> or <math>m \\in M_{\\operatorname{tor}}</math>. In particular, <math>M_{\\operatorname{tor}} = \\operatorname{ker}(M \\to K \\otimes_R M)</math> where <math>m \\mapsto 1 \\otimes m</math>.\n\n*<math>K \\otimes_R M \\cong M_{(0)}</math> where <math>M_{(0)}</math> is the [[localization of a module|localization of the module]] <math>M</math> at the prime ideal <math>(0)</math> (i.e., the localization with respect to the nonzero elements).\n\n===Extension of scalars===\n{{main|extension of scalars}}{{See also|Weil restriction}}\n\nThe adjoint relation in the general form has an important special case: for any ''R''-algebra ''S'', ''M'' a right ''R''-module, ''P'' a right ''S''-module, using <math>\\operatorname{Hom}_S (S, -) = -</math>, we have the natural isomorphism:\n\n:<math>\\operatorname{Hom}_S (M \\otimes_R S, P) = \\operatorname{Hom}_R (M, \\operatorname{Res}_R(P)).</math>\n\nThis says that the functor <math>-\\otimes_R S</math> is a [[left adjoint]] to the forgetful functor <math>\\operatorname{Res}_R</math>, which restricts an ''S''-action to an ''R''-action. Because of this, <math>- \\otimes_R S</math> is often called the [[extension of scalars]] from ''R'' to ''S''. In the [[representation theory]], when ''R'', ''S'' are group algebras, the above relation becomes the [[Frobenius reciprocity]].\n\n====Examples====\n\n*<math>R^n \\otimes_R S = S^n,</math> for any ''R''-algebra ''S'' (i.e., a free module remains free after extending scalars.)\n\n*For a commutative ring <math>R</math> and a commutative ''R''-algebra ''S'', we have:\n::<math>S \\otimes_R R[x_1, \\dots, x_n] = S[x_1, \\dots, x_n];</math>\n:in fact, more generally,\n::<math>S \\otimes_R (R[x_1, \\dots, x_n]/I) = S[x_1, \\dots, x_n]/ IS[x_1, \\dots, x_n],</math> \n:where <math>I</math> is an ideal.\n\n*Using <math>\\Complex = \\R [x]/(x^2 + 1),</math> the previous example and the [[Chinese remainder theorem]], we have as rings\n::<math>\\Complex \\otimes_{\\R} \\Complex = \\Complex [x]/(x^2 + 1) = \\Complex [x]/(x+i) \\times \\Complex[x]/(x-i) = \\Complex^2.</math>\n:This gives an example when a tensor product is a [[direct product]].\n\n*<math>\\R \\otimes_{\\Z} \\Z[i] = \\R[i] = \\Complex.</math>\n\n==Examples==\nLet ''G'' be an abelian group in which every element has finite order (that is ''G'' is a [[torsion abelian group]]; for example ''G'' can be a finite abelian group or <math>\\Q/\\Z</math>). Then:<ref>Example 3.6 of http://www.math.uconn.edu/~kconrad/blurbs/linmultialg/tensorprod.pdf</ref>\n\n:<math>\\Q \\otimes_{\\Z} G = 0.</math>\n\nIndeed, any <math>x \\in \\Q \\otimes_{\\Z} G</math> is of the form\n\n:<math>x = \\sum_i r_i \\otimes g_i, \\qquad r_i \\in \\Q , g_i \\in G.</math>\n\nIf <math>n_i</math> is the order of <math>g_i</math>, then we compute:\n\n:<math>x = \\sum (r_i / n_i )n_i \\otimes g_i = \\sum r_i / n_i \\otimes n_i g_i = 0.</math>\n\nSimilarly, one sees\n\n:<math>\\Q /\\Z \\otimes_{\\Z} \\Q /\\Z = 0.</math><!--\nConsider the [[rational number]]s, '''Q''', and the [[modular arithmetic|integers modulo ''n'']], '''Z'''<sub>''n''</sub>. As with any abelian group, both can be considered as modules over the [[integer]]s, '''Z'''.\nLet {{nowrap|''B'' : '''Q''' × '''Z'''<sub>''n''</sub> → ''M''}} be a '''Z'''-bilinear operator. Then {{nowrap|1=''B''(''q'', ''k'') = ''B''(''q''/''n'', ''nk'') = ''B''(''q''/''n'', 0) = 0}}, so every bilinear operator is identically zero. Therefore, if we define {{nowrap|'''Q''' ⊗<sub>'''Z'''</sub> '''Z'''<sub>''n''</sub>}} to be the trivial module, and ⊗ to be the zero bilinear function, then we see that the properties for the tensor product are satisfied. Therefore, the tensor product of '''Q''' and '''Z'''<sub>''n''</sub> is {0}.<ref>Hazewinkel, et al. (2004), [https://books.google.com/books?id=AibpdVNkFDYC&pg=PA97 p. 97], Ex. 4.5.1.</ref>\n\nSince an [[abelian group]] is the same thing as a '''Z'''-module,<ref>{{cite book |first=Nathan |last=Jacobson |title=Basic Algebra |volume=I |edition=2nd |publisher=Dover |year=2009 |page=164 }}</ref> the tensor product of '''Z'''-modules is the same thing as the '''tensor product of abelian groups'''.-->\n\nHere are some useful identities: Let ''R'' be a commutative ring, ''I'', ''J'' ideals, ''M'', ''N'' ''R''-modules. Then\n#<math>R/I \\otimes_R M = M/IM</math>. If ''M'' is [[flat module|flat]], <math>IM = I \\otimes_R M</math>.\n#<math>M/IM \\otimes_{R/I} N/IN = M \\otimes_R N \\otimes_R R/I.</math>\n#<math>R/I \\otimes_R R/J = R/(I + J)</math>.\n'''Proof:''' Tensoring with ''M'' the exact sequence <math>0 \\to I \\to R \\to R/I \\to 0</math> gives\n:<math>I \\otimes_R M \\overset{f}\\to R \\otimes_R M = M \\to R/I \\otimes_R M \\to 0</math>\nwhere ''f'' is given by <math>i \\otimes x \\mapsto ix</math>. Since the image of ''f'' is ''IM'', we get the first part of 1. If ''M'' is flat, ''f'' is injective and so is an isomorphism onto its image. 2. follows from 1. and 3. is because\n:<math>R/I \\otimes_R R/J = {R/J \\over I(R/J) }= {R/J \\over (I + J)/J} = R/(I+J)</math>. <math>\\square</math>\n\n'''Example:''' If ''G'' is an abelian group, <math>G \\otimes_{\\Z } \\Z /n = G/nG</math>; this follows from 1.\n\n'''Example:''' <math>\\Z /n \\otimes_{\\Z } \\Z /m = \\Z /{\\gcd(n, m)}</math>; this follows from 3.\n\n'''Example:''' Let <math>\\mu_n</math> be the group of ''n''-th roots of unity. It is a [[cyclic group]] and cyclic groups are classified by orders. Thus, non-canonically, <math>\\mu_n \\approx \\Z /n</math> and thus, when ''g'' is the gcd of ''n'' and ''m'',\n:<math>\\mu_n \\otimes_{\\Z } \\mu_m \\approx \\mu_g.</math>\n\n'''Example:''' Consider <math>\\Q \\otimes_{\\Z} \\Q.</math> Since <math>\\Q \\otimes_{\\Q } \\Q</math> is obtained from <math>\\Q \\otimes_{\\Z } \\Q </math> by imposing <math>\\Q </math>-linearity on the middle, we have the surjection\n\n:<math>\\Q \\otimes_{\\Z } \\Q \\to \\Q \\otimes_{\\Q } \\Q </math>\n\nwhose kernel is generated by elements of the form <math>{r \\over s} x \\otimes y - x \\otimes {r \\over s} y</math>\nwhere ''r'', ''s'', ''x'', ''u'' are integers and ''s'' is nonzero. Since\n\n:<math>{r \\over s} x \\otimes y = {r \\over s} x \\otimes {s \\over s} y = x \\otimes {r \\over s} y,</math>\n\nthe kernel actually vanishes; hence, <math>\\Q \\otimes_{\\Z } \\Q = \\Q \\otimes_{\\Q } \\Q = \\Q .</math>\n\n'''Example:''' We propose to compare <math>\\R \\otimes_{\\Z} \\R </math> and <math>\\R \\otimes_{\\R } \\R </math>. Like in the previous example, we have: <math>\\R \\otimes_{\\Z} \\R = \\R \\otimes_{\\Q} \\R </math> as abelian group and thus as <math>\\Q</math>-vector space (any <math>\\Z</math>-linear map between <math>\\Q</math>-vector spaces is <math>\\Q</math>-linear). As <math>\\Q</math>-vector space, <math>\\R </math> has dimension (cardinality of a basis) of [[Cardinality of the continuum|continuum]]. Hence, <math>\\R \\otimes_{\\Q } \\R </math> has a <math>\\Q</math>-basis indexed by a product of continuums; thus its <math>\\Q</math>-dimension is continuum. Hence, for dimension reason, there is a non-canonical isomorphism of <math>\\Q</math>-vector spaces:\n\n:<math>\\R \\otimes_{\\Z } \\R \\approx \\R \\otimes_{\\R } \\R </math>.\n<!-- but they are not isomorphic as rings since the ring on the left is not even a field. -->\n\nConsider the modules <math>M=\\Complex [x,y,z]/(f),N=\\Complex [x,y,z]/(g)</math> for <math>f,g\\in \\Complex[x,y,z]</math> irreducible polynomials such that <math>\\gcd(f,g)=1.</math> Then,\n\n:<math>\\frac{\\Complex [x,y,z]}{(f)}\\otimes_{\\Complex [x,y,z]}\\frac{\\Complex [x,y,z]}{(g)} \\cong \\frac{\\Complex [x,y,z]}{(f,g)}</math>\n\nAnother useful family of examples comes from changing the scalars. Notice that\n\n:<math>\\frac{\\Z [x_1,\\ldots,x_n]}{(f_1,\\ldots,f_k)} \\otimes_\\Z R \\cong \\frac{R[x_1,\\ldots,x_n]}{(f_1,\\ldots,f_k)}</math>\n\nGood examples of this phenomenon to look at are when <math>R = \\Q, \\Complex, \\Z/(p^k), \\Z_p, \\Q_p.</math>\n\n==Construction==\nThe construction of {{nowrap|''M'' ⊗ ''N''}} takes a quotient of a [[free abelian group]] with basis the symbols {{nowrap|''m'' ∗ ''n''}}, used here to denote the [[ordered pair]] {{nowrap|(''m'', ''n'')}}, for ''m'' in ''M'' and ''n'' in ''N'' by the subgroup generated by all elements of the form\n# −''m'' ∗ (''n'' + ''n''′) + ''m'' ∗ ''n'' + ''m'' ∗ ''n''′\n# −(''m'' + ''m''′) ∗ ''n'' + ''m'' ∗ ''n'' + ''m''′ ∗ ''n''\n# (''m'' · ''r'') ∗ ''n'' − ''m'' ∗ (''r'' · ''n'')\nwhere ''m'', ''m''′ in ''M'', ''n'', ''n''′ in ''N'', and ''r'' in ''R''. The quotient map which takes {{nowrap|''m'' ∗ ''n'' {{=}}(''m'', ''n'')}} to the coset containing {{nowrap|''m'' ∗ ''n''}}; that is,\n\n:<math>\\otimes: M \\times N \\to M \\otimes_R N, \\, (m, n) \\mapsto [m * n]</math>\n\nis balanced, and the subgroup has been chosen minimally so that this map is balanced. The universal property of ⊗ follows from the universal properties of a free abelian group and a quotient.\n\nMore category-theoretically, let σ be the given right action of ''R'' on ''M''; i.e., σ(''m'', ''r'') = ''m'' · ''r'' and τ the left action of ''R'' of ''N''. Then the tensor product of ''M'' and ''N'' over ''R'' can be defined as the [[coequalizer]]:\n:<math>M \\times R \\times N {{{} \\atop \\overset{\\sigma \\times 1}\\to}\\atop{\\underset{1 \\times \\tau} \\to \\atop {}}} M \\times N \\overset{\\otimes}\\to M \\otimes_R N,</math>\ntogether with the requirements\n:<math>m \\otimes (n + n') = m \\otimes n + m \\otimes n',</math>\n:<math>(m + m') \\otimes n = m \\otimes n + m' \\otimes n.</math>\n\nIf ''S'' is a subring of a ring ''R'', then <math>M \\otimes_R N</math> is the quotient group of <math>M \\otimes_S N</math> by the subgroup generated by <math>xr \\otimes_S y - x \\otimes_S ry, \\, r \\in R, x \\in M, y \\in N</math>, where <math>x \\otimes_S y</math> is the image of <math>(x, y)</math> under <math>\\otimes: M \\times N \\to M \\otimes_{S} N.</math> In particular, any tensor product of ''R''-modules can be constructed, if so desired, as a quotient of a tensor product of abelian groups by imposing the ''R''-balanced product property.\n\nIn the construction of the tensor product over a commutative ring ''R'', the ''R''-module structure can be built in from the start by forming the quotient of a free ''R''-module by the submodule generated by the elements given above for the general construction, augmented by the elements {{nowrap|''r'' ⋅ (''m'' ∗ ''n'') − ''m'' ∗ (''r'' ⋅ ''n'')}}. Alternately, the general construction can be given a Z(''R'')-module structure by defining the scalar action by {{nowrap|1=''r'' ⋅ (''m'' ⊗ ''n'') = ''m'' ⊗ (''r'' ⋅ ''n'')}} when this is well-defined, which is precisely when ''r'' ∈ Z(''R''), the [[Center (algebra)|centre]] of ''R''.\n\nThe [[direct product]] of ''M'' and ''N'' is rarely isomorphic to the tensor product of ''M'' and ''N''. When ''R'' is not commutative, then the tensor product requires that ''M'' and ''N'' be modules on opposite sides, while the direct product requires they be modules on the same side. In all cases the only function from {{nowrap|''M'' × ''N''}} to ''G'' that is both linear and bilinear is the zero map.\n\n==As linear maps==\nIn the general case, not all the properties of a [[tensor product of vector spaces]] extend to modules. Yet, some useful properties of the tensor product, considered as [[module homomorphism]]s, remain.\n\n===Dual module===\n{{see also|Duality (mathematics)#Dual objects}}\nThe '''dual module''' of a right ''R''-module ''E'', is defined as {{nowrap|Hom<sub>''R''</sub>(''E'', ''R'')}} with the canonical left ''R''-module structure, and is denoted ''E''<sup>∗</sup>.<ref>{{harvnb|Bourbaki|loc=ch. II §2.3}}</ref> The canonical structure is the [[pointwise]] operations of addition and scalar multiplication. Thus, ''E''<sup>∗</sup> is the set of all ''R''-linear maps {{nowrap|''E'' → ''R''}} (also called ''linear forms''), with operations\n:<math>(\\phi + \\psi)(u) = \\phi(u) + \\psi(u), \\quad \\phi, \\psi \\in E^*, u \\in E</math>\n:<math>(r \\cdot \\phi) (u) = r \\cdot \\phi(u), \\quad \\phi \\in E^*, u \\in E, r \\in R,</math>\nThe dual of a left ''R''-module is defined analogously, with the same notation.\n\nThere is always a canonical homomorphism {{nowrap|''E'' → ''E''<sup>∗∗</sup>}} from ''E'' to its second dual. It is an isomorphism if ''E'' is a free module of finite rank. In general, ''E'' is called a [[reflexive module]] if the canonical homomorphism is an isomorphism.\n\n===Duality pairing===\nWe denote the [[natural pairing]] of its dual ''E''<sup>∗</sup> and a right ''R''-module ''E'', or of a left ''R''-module ''F'' and its dual ''F''<sup>∗</sup> as\n:<math> \\langle \\cdot , \\cdot \\rangle : E^* \\times E \\to R : (e', e) \\mapsto \\langle e', e \\rangle = e'(e) </math>\n:<math> \\langle \\cdot , \\cdot \\rangle : F \\times F^* \\to R : (f, f') \\mapsto \\langle f, f' \\rangle = f'(f) .</math>\nThe pairing is left ''R''-linear in its left argument, and right ''R''-linear in its right argument:\n:<math> \\langle r \\cdot g, h \\cdot s \\rangle = r \\cdot \\langle g, h \\rangle \\cdot s, \\quad r, s \\in R .</math>\n\n===An element as a (bi)linear map===\nIn the general case, each element of the tensor product of modules gives rise to a left ''R''-linear map, to a right ''R''-linear map, and to an ''R''-bilinear form. Unlike the commutative case, in the general case the tensor product is not an ''R''-module, and thus does not support scalar multiplication.\n\n* Given right ''R''-module ''E'' and right ''R''-module ''F'', there is a canonical homomorphism {{nowrap|''θ'' : ''F'' ⊗<sub>''R''</sub> ''E''<sup>∗</sup> → Hom<sub>''R''</sub>(''E'', ''F'')}} such that {{nowrap|''θ''(''f'' ⊗ ''e''′)}} is the map {{nowrap|''e'' ↦ ''f'' ⋅ {{langle}}''e''′, ''e''{{rangle}}}}.<ref>{{harvnb|Bourbaki|loc=ch. II §4.2 eq. (11)}}</ref><!-- Thus, an element of a tensor product ''η'' ∈ ''F'' ⊗<sub>''R''</sub> ''E''<sup>∗</sup> acts as a right ''R''-linear map ''η'' : ''E'' → ''F''.-->\n\n* Given left ''R''-module ''E'' and right ''R''-module ''F'', there is a canonical homomorphism {{nowrap|''θ'' : ''F'' ⊗<sub>''R''</sub> ''E'' → Hom<sub>''R''</sub>(''E''<sup>∗</sup>, ''F'')}} such that {{nowrap|''θ''(''f'' ⊗ ''e'')}} is the map {{nowrap|''e''′ ↦ ''f'' ⋅ {{langle}}''e'', ''e''′{{rangle}}}}.<ref>{{harvnb|Bourbaki|loc=ch. II §4.2 eq. (15)}}</ref><!--Thus, an element of a tensor product ''ξ'' ∈ ''F'' ⊗<sub>''R''</sub> ''E'' acts as a right ''R''-linear map ''E''<sup>∗</sup> → ''F'', and by similarity, as a left ''R''-linear map ''F''<sup>∗</sup> → ''E''. -->\n\nBoth cases hold for general modules, and become isomorphisms if the modules ''E'' and ''F'' are restricted to being [[finitely generated projective module]]s (in particular free modules of finite ranks). Thus, an element of a tensor product of modules over a ring ''R'' maps canonically onto an ''R''-linear map, though as with vector spaces, constraints apply to the modules for this to be equivalent to the full space of such linear maps.\n\n* Given right ''R''-module ''E'' and left ''R''-module ''F'', there is a canonical homomorphism {{nowrap|''θ'' : ''F''<sup>∗</sup> ⊗<sub>''R''</sub> ''E''<sup>∗</sup> → L<sub>''R''</sub>(''F'' × ''E'', ''R'')}} such that {{nowrap|''θ''(''f''′ ⊗ ''e''′)}} is the map {{nowrap|(''f'', ''e'') ↦ {{langle}}''f'', ''f''′{{rangle}} ⋅ {{langle}}''e''′, ''e''{{rangle}}}}.{{citation needed|date=April 2015}} Thus, an element of a tensor product ''ξ'' ∈ ''F''<sup>∗</sup> ⊗<sub>''R''</sub> ''E''<sup>∗</sup> may be thought of giving rise to or acting as an ''R''-bilinear map {{nowrap|''F'' × ''E'' → ''R''}}.\n\n===Trace===\nLet ''R'' be a commutative ring<!-- the non-commutative case seems unclear; any source anyone? --> and ''E'' an ''R''-module. Then there is a canonical ''R''-linear map:\n\n:<math>E^* \\otimes_R E \\to R</math>\n\ninduced through linearity by <math>\\phi \\otimes x \\mapsto \\phi(x)</math>; it is the unique ''R''-linear map corresponding to the natural pairing.\n\nIf ''E'' is a finitely generated projective ''R''-module, then one can identify <math>E^* \\otimes_R E = \\operatorname{End}_R(E)</math> through the canonical homomorphism mentioned above and then the above is the '''trace map''':\n\n:<math>\\operatorname{tr}: \\operatorname{End}_R(E) \\to R.</math>\n\nWhen ''R'' is a field, this is the usual [[Trace (linear algebra)|trace]] of a linear transformation.\n\n==Example from differential geometry: tensor field==\nThe most prominent example of a tensor product of modules in differential geometry is the tensor product of the spaces of vector fields and differential forms. More precisely, if ''R'' is the (commutative) ring of smooth functions on a smooth manifold ''M'', then one puts\n\n:<math>\\mathfrak{T}^p_q = \\Gamma(M, T M)^{\\otimes p} \\otimes_R \\Gamma(M, T^* M)^{\\otimes q}</math>\n\nwhere Γ means the [[space of sections]] and the superscript <math>\\otimes p</math> means tensoring ''p'' times over ''R''. By definition, an element of <math>\\mathfrak{T}^p_q</math> is a [[tensor field]] of type (''p'', ''q'').\n\nAs ''R''-modules, <math>\\mathfrak{T}^q_p</math> is the dual module of <math>\\mathfrak{T}^p_q.</math><ref>{{harvnb|Helgason|loc=Lemma 2.3'}}</ref>\n\nTo lighten the notation, put <math>E = \\Gamma(M, T M)</math> and so <math>E^* = \\Gamma(M, T^* M)</math>.<ref>This is actually the ''definition'' of differential one-forms, global sections of <math>T^*M</math>, in Helgason, but is equivalent to the usual definition that does not use module theory.</ref> When ''p'', ''q'' ≥ 1, for each (''k'', ''l'') with 1 ≤ ''k'' ≤ ''p'', 1 ≤ ''l'' ≤ ''q'', there is an ''R''-multilinear map:\n\n:<math>E^p \\times {E^*}^q \\to E^{p-1} \\times {E^*}^{q-1}, \\, (X_1, \\dots, X_p, \\omega_1, \\dots, \\omega_q) \\mapsto \\langle X_k, \\omega_l \\rangle (X_1, \\dots, \\widehat{X_l}, \\dots, X_p, \\omega_1, \\dots, \\widehat{\\omega_l}, \\dots, \\omega_q)</math>\n\nwhere <math>E^p</math> means <math>\\prod_1^p E</math> and the hat means a term is omitted. By the universal property, it corresponds to a unique ''R''-linear map:\n\n:<math>C^k_l: \\mathfrak{T}^p_q \\to \\mathfrak{T}^{p-1}_{q-1}.</math>\n\nIt is called the [[tensor contraction|contraction]] of tensors in the index (''k'', ''l''). Unwinding what the universal property says one sees:\n\n:<math>C^k_l(X_1 \\otimes \\cdots \\otimes X_p \\otimes \\omega_1 \\otimes \\cdots \\otimes \\omega_q) = \\langle X_k, \\omega_l \\rangle X_1 \\otimes \\cdots \\widehat{X_l} \\cdots \\otimes X_p \\otimes \\omega_1 \\otimes \\cdots \\widehat{\\omega_l} \\cdots \\otimes \\omega_q.</math>\n\n'''Remark''': The preceding discussion is standard in textbooks on differential geometry (e.g., Helgason<!-- perhaps Kobayashi-Nomizu? -->). In a way, the sheaf-theoretic construction (i.e., the language of [[sheaf of modules]]) is more natural and increasingly more common; for that, see the section {{section link||Tensor product of sheaves of modules}}.\n\n==Relationship to flat modules==\nIn general, \n\n:<math>-\\otimes_R-:\\text{Mod-}R\\times R\\text{-Mod}\\longrightarrow \\mathrm{Ab}</math> \n\nis a [[bifunctor]] which accepts a right and a left ''R'' module pair as input, and assigns them to the tensor product in the [[category of abelian groups]].\n\nBy fixing a right ''R'' module ''M'', a functor \n\n:<math>M\\otimes_R-:R\\text{-Mod} \\longrightarrow \\mathrm{Ab}</math> \n\narises, and symmetrically a left ''R'' module ''N'' could be fixed to create a functor \n\n:<math>-\\otimes_R N:\\text{Mod-}R \\longrightarrow \\mathrm{Ab}.</math>\n\nUnlike the [[Hom bifunctor]] <math>\\mathrm{Hom}_R(-,-),</math> the tensor functor is [[covariant functor|covariant]] in both inputs.\n\nIt can be shown that <math>M\\otimes_R-</math> and <math>-\\otimes_R N</math> are always [[right exact functor]]s, but not necessarily left exact (<math>0\\to \\Z\\to \\Z\\to \\Z_n\\to 0,</math> where the first map is multiplication by <math>n</math>, is exact but not after taking the tensor with <math>\\Z_n</math>). By definition, a module ''T'' is a [[flat module]] if <math>T\\otimes_R-</math> is an exact functor.\n\nIf <math> \\{m_i \\mid i\\in I \\}</math> and <math> \\{n_j \\mid j \\in J\\}</math> are generating sets for ''M'' and ''N'', respectively, then <math> \\{m_i \\otimes n_j \\mid i\\in I, j \\in J\\}</math> will be a generating set for <math>M\\otimes_R N.</math> Because the tensor functor <math>M\\otimes_R-</math> sometimes fails to be left exact, this may not be a minimal generating set, even if the original generating sets are minimal. If ''M'' is a [[flat module]], the functor <math>M\\otimes_R-</math> is exact by the very definition of a flat module. If the tensor products are taken over a field ''F'', we are in the case of vector spaces as above. Since all ''F'' modules are flat, the [[bifunctor]] <math>-\\otimes_R-</math> is exact in both positions, and the two given generating sets are bases, then <math> \\{m_i \\otimes n_j \\mid i\\in I, j \\in J\\}</math> indeed forms a basis for <math>M\\otimes_F N.</math>\n\n{{See also|pure submodule}}\n\n==Additional structure==\nIf ''S'' and ''T'' are commutative ''R''-algebras, then {{nowrap|''S'' ⊗<sub>''R''</sub> ''T''}} will be a commutative ''R''-algebra as well, with the multiplication map defined by {{nowrap|1=(''m''<sub>1</sub> ⊗ ''m''<sub>2</sub>) (''n''<sub>1</sub> ⊗ ''n''<sub>2</sub>) = (''m''<sub>1</sub>''n''<sub>1</sub> ⊗ ''m''<sub>2</sub>''n''<sub>2</sub>)}} and extended by linearity. In this setting, the tensor product become a [[fibered coproduct]] in the category of ''R''-algebras.<!--Note that any ring is a '''Z'''-algebra, so we may always take {{nowrap|''M'' ⊗<sub>'''Z'''</sub> ''N''}}.-->\n\nIf ''M'' and ''N'' are both ''R''-modules over a commutative ring, then their tensor product is again an ''R''-module. If ''R'' is a ring, ''<sub>R</sub>M'' is a left ''R''-module, and the [[commutator]]\n\n:''rs'' − ''sr''\n\nof any two elements ''r'' and ''s'' of ''R'' is in the [[Annihilator (ring theory)|annihilator]] of ''M'', then we can make ''M'' into a right ''R'' module by setting\n\n:''mr'' = ''rm''.\n\nThe action of ''R'' on ''M'' factors through an action of a quotient commutative ring. In this case the tensor product of ''M'' with itself over ''R'' is again an ''R''-module. This is a very common technique in commutative algebra.\n\n==Generalization==\n\n===Tensor product of complexes of modules===\nIf ''X'', ''Y'' are complexes of ''R''-modules (''R'' a commutative ring), then their tensor product is the complex given by\n\n:<math>(X \\otimes_R Y)_n = \\sum_{i + j = n} X_i \\otimes_R Y_j,</math>\n\nwith the differential given by: for ''x'' in ''X''<sub>''i''</sub> and ''y'' in ''Y''<sub>''j''</sub>,\n\n:<math>d_{X \\otimes Y} (x \\otimes y) = d_X(x) \\otimes y + (-1)^i x \\otimes d_Y(y).</math><ref>{{harvnb|May|ch. 12 §3}}</ref>\n\nFor example, if ''C'' is a chain complex of flat abelian groups and if ''G'' is an abelian group, then the homology group of <math>C \\otimes_{\\Z } G</math> is the homology group of ''C'' with coefficients in ''G'' (see also: [[universal coefficient theorem]].)\n\n===Tensor product of sheaves of modules===\n{{main|Sheaf of modules}}\n\nIn this setup, for example, one can define a [[tensor field]] on a smooth manifold ''M'' as a (global or local) section of the tensor product (called '''tensor bundle''')\n\n:<math>(T M)^{\\otimes p} \\otimes_{O} (T^* M)^{\\otimes q}</math>\n\nwhere ''O'' is the [[sheaf of rings]] of smooth functions on ''M'' and the bundles <math>TM, T^*M</math> are viewed as [[locally free sheaf|locally free sheaves]] on ''M''.<ref>See also [https://www.encyclopediaofmath.org/index.php/Tensor_bundle Encyclopedia of Mathematics - Tensor bundle]</ref>\n\nThe '''exterior bundle''' on ''M'' is the [[subbundle]] of the tensor bundle consisting of all antisymmetric covariant tensors. [[Section (fiber bundle)|Section]]s of the exterior bundle are [[differential forms]] on ''M''.\n\n{{See also|Tensor product bundle}}\n\nOne important case when one forms a tensor product over a sheaf of non-commutative rings appears in theory of [[D-modules|''D''-modules]]; that is, tensor products over the [[sheaf of differential operators]].\n\n==See also==\n*[[Tor functor]]\n*[[Tensor product of algebras]]\n*[[Tensor product of fields]]\n*[[derived tensor product]]\n\n==Notes==\n<references/>\n\n==References==\n* Bourbaki, ''Algebra''\n*{{citation|first=Sigurdur|last=Helgason|title=Differential geometry, Lie groups and symmetric spaces|year=1978|publisher=Academic Press| isbn=0-12-338460-5}}\n*{{citation|first1=D.G.|last1=Northcott|authorlink1=Douglas Northcott|title=Multilinear Algebra|publisher=Cambridge University Press| year=1984|isbn=613-0-04808-4}}.\n*{{citation|first1=Michiel|last1=Hazewinkel|authorlink1=Michiel Hazewinkel|first2=Nadezhda Mikhaĭlovna| last2=Gubareni |authorlink2 =Nadezhda Mikhaĭlovna|first3=Nadiya|last3=Gubareni|authorlink3=Nadiya Gubareni|first4=Vladimir V.| last4=Kirichenko |authorlink4= Vladimir V. Kirichenko|title=Algebras, rings and modules|publisher=Springer|year=2004|isbn=978-1-4020-2690-4}}.\n* Peter May (1999), [http://www.maths.ed.ac.uk/~aar/papers/maybook.pdf ''A concise course in algebraic topology''], University of Chicago Press.\n\n{{tensors}}\n\n[[Category:Module theory]]\n[[Category:Multilinear algebra]]\n[[Category:Homological algebra]]\n[[Category:Binary operations]]"
    },
    {
      "title": "Tor functor",
      "url": "https://en.wikipedia.org/wiki/Tor_functor",
      "text": "In [[mathematics]], the '''Tor functors''' are the [[derived functor]]s of the [[tensor product of modules]] over a [[ring (mathematics)|ring]]. Along with the [[Ext functor]], Tor is one of the central concepts of [[homological algebra]], in which ideas from [[algebraic topology]] are used to construct invariants of algebraic structures. The [[group cohomology#Group homology|homology of groups]], [[Lie algebra homology|Lie algebra]]s, and [[Hochschild homology|associative algebras]] can all be defined in terms of Tor. The name comes from a relation between the first Tor group Tor<sub>1</sub> and the [[torsion subgroup]] of an [[abelian group]].\n\nIn the special case of abelian groups, Tor was introduced by [[Eduard Čech]] (1935) and named by [[Samuel Eilenberg]] around 1950.<ref>Weibel (1999).</ref> It was first applied to the [[Künneth theorem]] and [[universal coefficient theorem]] in topology. For modules over any ring, Tor was defined by [[Henri Cartan]] and Eilenberg in their 1956 book ''Homological Algebra''.<ref>Cartan & Eilenberg (1956), section VI.1.</ref>\n\n==Definition==\nLet ''R'' be a [[ring (mathematics)|ring]]. Write ''R''-Mod for the [[category theory|category]] of [[module (mathematics)|left ''R''-modules]] and Mod-''R'' for the category of right ''R''-modules. (If ''R'' is [[commutative ring|commutative]], the two categories can be identified.) For a fixed left ''R''-module ''B'', let ''T''(''A'') = ''A'' ⊗<sub>''R''</sub> ''B'' for ''A'' in Mod-''R''. This is a [[right exact functor]] from Mod-''R'' to the [[category of abelian groups]] Ab, and so it has left [[derived functor]]s ''L<sub>i</sub>T''. The Tor groups are the abelian groups defined by\n\n: <math>\\operatorname{Tor}_i^R(A,B) = (L_iT)(A),</math>\n\nfor an [[integer]] ''i''. By definition, this means: take any [[projective module#Projective resolutions|projective resolution]]\n\n: <math>\\cdots\\to P_2 \\to P_1 \\to P_0 \\to A\\to 0,</math>\n\nremove the term ''A'', and form the [[chain complex]]:\n\n: <math>\\cdots \\to P_2\\otimes_R B \\to P_1\\otimes_R B \\to P_0\\otimes_R B \\to 0</math>\n\nFor each integer ''i'', Tor{{supsub|''R''|''i''}}(''A'', ''B'') is the [[chain complex|homology]] of this complex at position ''i''. It is zero for ''i'' negative. For example, Tor{{supsub|''R''|0}}(''A'', ''B'') is the [[cokernel]] of the map ''P''<sub>1</sub> ⊗<sub>''R''</sub> ''B'' → ''P''<sub>0</sub> ⊗<sub>''R''</sub> ''B'', which is [[isomorphic]] to ''A'' ⊗<sub>''R''</sub> ''B''.\n\nAlternatively, one can define Tor by fixing ''A'' and taking the left derived functors of the right exact functor ''G''(''B'') = ''A'' ⊗<sub>''R''</sub> ''B''. That is, tensor ''A'' with a projective resolution of ''B'' and take homology. Cartan and Eilenberg showed that these constructions are independent of the choice of projective resolution, and that both constructions yield the same Tor groups.<ref>Weibel (1994), section 2.4 and Theorem 2.7.2.</ref> Moreover, for a fixed ring ''R'', Tor is a functor in each variable (from ''R''-modules to abelian groups).\n\nFor a commutative ring ''R'' and ''R''-modules ''A'' and ''B'', Tor{{supsub|''R''|''i''}}(''A'', ''B'') is an ''R''-module (using that ''A'' ⊗<sub>''R''</sub> ''B'' is an ''R''-module in this case). For a non-commutative ring ''R'', Tor{{supsub|''R''|''i''}}(''A'', ''B'') is only an abelian group, in general. If ''R'' is an [[algebra over a ring|algebra]] over a ring ''S'' (which means in particular that ''S'' is commutative), then Tor{{supsub|''R''|''i''}}(''A'', ''B'') is at least an ''S''-module.\n\n==Properties==\nHere are some of the basic properties and computations of Tor groups.<ref>Weibel (1994), Chapters 2 and 3.</ref>\n\n*Tor{{supsub|''R''|0}}(''A'', ''B'') ≅ ''A'' ⊗<sub>''R''</sub> ''B'' for any right ''R''-module ''A'' and left ''R''-module ''B''.\n\n*Tor{{su|b=''i''|p=''R''}}(''A'', ''B'') = 0 for all ''i'' > 0 if either ''A'' or ''B'' is [[flat module|flat]] (for example, [[free module|free]]) as an ''R''-module. In fact, one can compute Tor using a flat resolution of either ''A'' or ''B''; this is more general than a projective (or free) resolution.<ref>Weibel (1994), Lemma 3.2.8.</ref>\n\n*There are converses to the previous statement:\n**If Tor{{su|b=1|p=''R''}}(''A'', ''B'') = 0 for all ''B'', then ''A'' is flat (and hence Tor{{su|b=''i''|p=''R''}}(''A'', ''B'') = 0 for all ''i'' > 0).\n**If Tor{{su|b=1|p=''R''}}(''A'', ''B'') = 0 for all ''A'', then ''B'' is flat (and hence Tor{{su|b=''i''|p=''R''}}(''A'', ''B'') = 0 for all ''i'' > 0).\n\n*By the general properties of derived functors, every [[short exact sequence]] 0 → ''K'' → ''L'' → ''M'' → 0 of right ''R''-modules induces a [[long exact sequence]] of the form<ref>Weibel (1994), Definition 2.1.1.</ref>\n::<math>\\cdots \\to \\operatorname{Tor}_2^R(M,B) \\to \\operatorname{Tor}_1^R(K,B) \\to \\operatorname{Tor}_1^R(L,B) \\to \\operatorname{Tor}_1^R (M,B) \\to K\\otimes_R B\\to L\\otimes_R B\\to M\\otimes_R B\\to 0,</math>\n:for any left ''R''-module ''B''. The analogous exact sequence also holds for Tor with respect to the second variable.\n\n*Symmetry: for a commutative ring ''R'', there is a [[natural isomorphism]] Tor{{su|b=''i''|p=''R''}}(''A'', ''B'') ≅ Tor{{su|b=''i''|p=''R''}}(''B'', ''A'').<ref>Weibel (1994), Remark in section 3.1.</ref> (For ''R'' commutative, there is no need to distinguish between left and right ''R''-modules.)\n\n*If ''R'' is a commutative ring and ''u'' in ''R'' is not a [[zero divisor]], then for any ''R''-module ''B'',\n::<math>\\operatorname{Tor}^R_i(R/(u),B)\\cong\\begin{cases} B/uB & i=0\\\\ B[u] & i=1\\\\ 0 &\\text{otherwise}\\end{cases}</math>\n:where \n::<math>B[u] = \\{x \\in B : ux =0 \\}</math>\n:is the ''u''-torsion subgroup of ''B''. This is the explanation for the name Tor. Taking ''R'' to be the ring <math>\\Z</math> of integers, this calculation can be used to compute <math>\\operatorname{Tor}^{\\Z}_1(A,B)</math> for any [[finitely generated abelian group]] ''A''.\n\n*Generalizing the previous example, one can compute Tor groups that involve the quotient of a commutative ring by any [[regular sequence]], using the [[Koszul complex]].<ref>Weibel (1994), section 4.5.</ref> For example, if ''R'' is the [[polynomial ring]] ''k''[''x''<sub>1</sub>, ..., ''x''<sub>''n''</sub>] over a field ''k'', then <math>\\operatorname{Tor}_*^R(k,k)</math> is the [[exterior algebra]] over ''k'' on ''n'' generators in Tor<sub>1</sub>.\n\n* <math>\\operatorname{Tor}^{\\Z}_i(A,B)=0</math> for all ''i'' ≥ 2. The reason: every [[abelian group]] ''A'' has a free resolution of length 1, since every subgroup of a [[free abelian group]] is free abelian.\n\n*For any ring ''R'', Tor preserves [[direct sum of modules|direct sums]] (possibly infinite) and [[filtered colimit]]s in each variable.<ref>Weibel (1994), Corollary 2.6.17.</ref> For example, in the first variable, this says that\n::<math>\\begin{align}\n\\operatorname{Tor}_i^R \\left (\\bigoplus_{\\alpha} M_{\\alpha}, N \\right ) &\\cong \\bigoplus_{\\alpha} \\operatorname{Tor}_i^R(M_{\\alpha},N) \\\\\n\\operatorname{Tor}_i^R \\left (\\varinjlim_{\\alpha} M_{\\alpha}, N \\right ) &\\cong \\varinjlim_{\\alpha} \\operatorname{Tor}_i^R(M_{\\alpha},N)\n\\end{align}</math>\n\n*Flat base change: for a commutative flat ''R''-algebra ''T'', ''R''-modules ''A'' and ''B'', and an integer ''i'',<ref>Weibel (1994), Corollary 3.2.10.</ref>\n::<math>\\mathrm{Tor}_i^R(A,B)\\otimes_R T \\cong \\mathrm{Tor}_i^T(A\\otimes_R T,B\\otimes_R T).</math>\n:It follows that Tor commutes with [[localization of a ring|localization]]. That is, for a [[multiplicatively closed set]] ''S'' in ''R'',\n::<math>S^{-1} \\operatorname{Tor}_i^R(A, B) \\cong \\operatorname{Tor}_i^{S^{-1} R} \\left (S^{-1} A, S^{-1} B \\right ).</math>\n\n*For a commutative ring ''R'' and commutative ''R''-algebras ''A'' and ''B'', Tor{{supsub|''R''|*}}(''A'',''B'') has the structure of a [[graded-commutative]] algebra over ''R''. Moreover, elements of odd degree in the Tor algebra have square zero, and there are [[divided power]] operations on the elements of positive even degree.<ref>Avramov & Halperin (1986), section 2.16; {{Citation | title=Stacks Project, Tag 09PQ | url=http://stacks.math.columbia.edu/tag/09PQ}}.</ref>\n\n==Important special cases==\n\n*[[Group homology]] is defined by <math>H_*(G,M)=\\operatorname{Tor}^{\\Z[G]}_*(\\Z, M),</math> where ''G'' is a group, ''M'' is a [[group representation|representation]] of ''G'' over the integers, and <math>\\Z[G]</math> is the [[group ring]] of ''G''.\n\n*For an [[algebra over a field|algebra]] ''A'' over a field ''k'' and an ''A''-[[bimodule]] ''M'', [[Hochschild homology]] is defined by\n::<math>HH_*(A,M)=\\operatorname{Tor}_*^{A\\otimes_k A^{\\text{op}}}(A, M).</math>\n\n*[[Lie algebra homology]] is defined by <math>H_*(\\mathfrak g,M)=\\operatorname{Tor}_*^{U\\mathfrak g}(R,M)</math>, where <math>\\mathfrak g</math> is a [[Lie algebra]] over a commutative ring ''R'', ''M'' is a <math>\\mathfrak g</math>-module, and <math>U\\mathfrak g</math> is the [[universal enveloping algebra]].\n\n*For a commutative ring ''R'' with a homomorphism onto a field ''k'', <math>\\operatorname{Tor}_*^R(k,k)</math> is a graded-commutative [[Hopf algebra]] over ''k''.<ref>Avramov & Halperin (1986), section 4.7.</ref> (If ''R'' is a [[Noetherian local ring]] with residue field ''k'', then the dual Hopf algebra to <math>\\operatorname{Tor}_*^R(k,k)</math> is [[Ext functor#Important special cases|Ext]]{{supsub|*|''R''}}(''k'',''k'').) As an algebra, <math>\\operatorname{Tor}_*^R(k,k)</math> is the free graded-commutative divided power algebra on a graded vector space π<sub>*</sub>(''R'').<ref>Gulliksen & Levin (1969), Theorem 2.3.5; Sjödin (1980), Theorem 1.</ref> When ''k'' has [[characteristic of a field|characteristic]] zero, π<sub>*</sub>(''R'') can be identified with the [[André-Quillen homology]] ''D''<sub>*</sub>(''k''/''R'',''k'').<ref>Quillen (1970), section 7.</ref>\n\n==See also==\n*[[flat morphism]]\n*[[Serre's intersection formula]]\n*[[derived tensor product]]\n*[[Eilenberg–Moore spectral sequence]]\n\n==Notes==\n{{reflist|30em}}\n\n==References==\n*{{Citation | author1-last=Avramov | author1-first=Luchezar | author1-link=Luchezar L. Avramov | author2-last=Halperin | author2-first=Stephen | author2-link=Stephen Halperin | chapter=Through the looking glass: a dictionary between rational homotopy theory and local algebra | title=Algebra, algebraic topology, and their interactions (Stockholm, 1983) | editor=J.-E. Roos | series=Lecture Notes in Mathematics | volume=1183 | publisher=[[Springer Nature]] | year=1986 | pages=1–27 | isbn=978-3-540-16453-1 | doi=10.1007/BFb0075446 | mr=0846435}}\n*{{Citation | author1-last=Cartan | author1-first=Henri | author1-link=Henri Cartan | author2-last=Eilenberg | author2-first=Samuel | author2-link=Samuel Eilenberg | title=Homological algebra | origyear=1956 | year=1999 | publisher=[[Princeton University Press]] | location=Princeton | mr=0077480 | isbn=0-691-04991-2}}\n*{{Citation | author1-last=Čech | author1-first=Eduard | author1-link=Eduard Čech | title=Les groupes de Betti d'un complexe infini | journal=[[Fundamenta Mathematicae]] | volume=25 | year=1935 | pages=33–44 | doi=10.4064/fm-25-1-33-44 | jfm=61.0609.02}}\n*{{Citation | author1-last=Gulliksen | author1-first=Tor | author2-last=Levin | author2-first=Gerson | title=Homology of local rings | series=Queen's Papers in Pure and Applied Mathematics | publisher=Queen's University | volume=20 | year=1969 | mr=0262227}}\n*{{Citation | last1=Quillen | first1=Daniel | author1-link=Daniel Quillen | chapter=On the (co-)homology of commutative rings | title=Applications of categorical algebra | pages=65–87 | series=Proc. Symp. Pure Mat. | volume=17 | publisher=[[American Mathematical Society]] | year=1970 | mr=0257068}}\n*{{Citation | author1-last=Sjödin | author1-first=Gunnar | title=Hopf algebras and derivations | journal=[[Journal of Algebra]] | volume=64 |year=1980 | pages=218–229 | doi=10.1016/0021-8693(80)90143-X | mr=0575792}} \n* {{Weibel IHA}}\n*{{Citation | author1-last=Weibel | author1-first=Charles | author1-link=Charles Weibel | chapter=History of homological algebra | title=History of topology | pages=797—836 | publisher=North-Holland | location=Amsterdam | year=1999 | mr=1721123 | url=http://sites.math.rutgers.edu/~weibel/HA-history.pdf}}\n\n==External links==\n*{{Citation | author1=The Stacks Project Authors | title=The Stacks Project  | url=http://stacks.math.columbia.edu/}}\n\n[[Category:Homological algebra]]\n[[Category:Binary operations]]"
    },
    {
      "title": "Union (set theory)",
      "url": "https://en.wikipedia.org/wiki/Union_%28set_theory%29",
      "text": "[[File:Venn0111.svg|thumb|200px|Union of two sets:<br><math>~A \\cup B</math>]]\n[[File:Venn 0111 1111.svg|thumb|200px|Union of three sets:<br><math>~A \\cup B \\cup C</math>]]\n[[File:Example of a non pairwise disjoint family of sets.svg|200px|thumb|The union of A, B, C, D, and E is everything except the white area.]]\nIn [[set theory]], the '''union''' (denoted by ∪) of a collection of sets is the set of all [[element (set theory)|element]]s in the collection.<ref>{{cite web|url=http://mathworld.wolfram.com/Union.html|title=Union|author=Weisstein, Eric W|publisher=Wolfram's Mathworld|accessdate=2009-07-14|deadurl=no|archiveurl=https://web.archive.org/web/20090207202412/http://mathworld.wolfram.com/Union.html|archivedate=2009-02-07|df=}}</ref> It is one of the fundamental operations through which sets can be combined and related to each other.\n\nFor explanation of the symbols used in this article, refer to the [[List of mathematical symbols|table of mathematical symbols]].\n\n== Union of two sets ==\nThe union of two sets ''A'' and ''B'' is the set of elements which are in ''A'', in ''B'', or in both ''A'' and ''B''.  In symbols,\n\n:<math>A  \\cup B = \\{ x: x \\in A \\text{  or  } x \\in B\\}</math>.<ref name=\":0\">{{Cite book|url=https://books.google.com/books?id=LBvpfEMhurwC|title=Basic Set Theory|last=Vereshchagin|first=Nikolai Konstantinovich|last2=Shen|first2=Alexander|date=2002-01-01|publisher=American Mathematical Soc.|isbn=9780821827314|language=en}}</ref>\n\nFor example, if ''A'' = {1, 3, 5, 7} and ''B'' = {1, 2, 4, 6} then ''A'' ∪ ''B'' = {1, 2, 3, 4, 5, 6, 7}.  A more elaborate example (involving two infinite sets) is:\n: ''A'' = {''x'' is an even [[integer]] larger than 1}\n: ''B'' = {''x'' is an odd integer larger than 1}\n: <math>A \\cup B = \\{2,3,4,5,6, \\dots\\}</math>\n\nAs another example, the number 9 is ''not'' contained in the union of the set of [[prime number]]s {2, 3, 5, 7, 11, …} and the set of [[even number]]s {2, 4, 6, 8, 10, …}, because 9 is neither prime nor even.\n\nSets cannot have duplicate elements,<ref name=\":0\" /><ref>{{Cite book|url=https://books.google.com/books?id=2hM3-xxZC-8C&pg=PA24|title=Applied Mathematics for Database Professionals|last=deHaan|first=Lex|last2=Koppelaars|first2=Toon|date=2007-10-25|publisher=Apress|isbn=9781430203483|language=en}}</ref> so the union of the sets {1, 2, 3} and {2, 3, 4} is {1, 2, 3, 4}. Multiple occurrences of identical elements have no effect on the [[cardinality]] of a set or its contents.\n\n== Algebraic properties ==\nBinary union is an [[associative]] operation; that is, for any sets ''A'', ''B'', and ''C'',\n\n:<math>A \\cup (B \\cup C) = (A \\cup B) \\cup C.</math>\n\nThe operations can be performed in any order, and the parentheses may be omitted without ambiguity (i.e., either of the above can be expressed equivalently as ''A'' ∪ ''B'' ∪ ''C''). Similarly, union is [[commutative]], so the sets can be written in any order.<ref>{{Cite book|url=https://books.google.com/books?id=jV_aBwAAQBAJ|title=Naive Set Theory|last=Halmos|first=P. R.|date=2013-11-27|publisher=Springer Science & Business Media|isbn=9781475716450|language=en}}</ref>\n\nThe [[empty set]] is an [[identity element]] for the operation of union. That is, ''A'' ∪ ∅ = ''A'', for any set ''A.'' This follows from analogous facts about [[logical disjunction]].\n\nSince sets with unions and intersections form a [[Boolean algebra (structure)|Boolean algebra]], [[Intersection (set theory)|intersection]] distributes over union\n:<math>A \\cap (B \\cup C) = (A \\cap B)\\cup(A \\cap C)</math> \nand union distributes over intersection\n:<math>A \\cup (B \\cap C) = (A \\cup B) \\cap (A \\cup C)</math> .\n\nWithin a given [[universe (mathematics)|universal set]], union can be written in terms of the operations of intersection and [[complement (set theory)|complement]] as\n:<math>A \\cup B = \\left(A^C \\cap B^C \\right)^C</math>\nwhere the superscript <sup>C</sup> denotes the complement with respect to the universal set.\n\n== Finite unions ==\nOne can take the union of several sets simultaneously.  For example, the union of three sets ''A'', ''B'', and ''C'' contains all elements of ''A'', all elements of ''B'', and all elements of ''C'', and nothing else. Thus, ''x'' is an element of ''A'' ∪ ''B'' ∪ ''C'' if and only if ''x'' is in at least one of ''A'', ''B'', and ''C''.\n\nA '''finite union''' is the union of a finite number of sets; the phrase does not imply that the union set is a [[finite set]].<ref>{{Cite book|url=https://books.google.com/books?id=u06-BAAAQBAJ|title=Set Theory: With an Introduction to Real Point Sets|last=Dasgupta|first=Abhijit|date=2013-12-11|publisher=Springer Science & Business Media|isbn=9781461488545|language=en}}</ref><ref>{{cite web|url=https://proofwiki.org/wiki/Finite_Union_of_Finite_Sets_is_Finite|title=Finite Union of Finite Sets is Finite - ProofWiki|author=|date=|website=proofwiki.org|accessdate=29 April 2018|deadurl=no|archiveurl=https://web.archive.org/web/20140911224545/https://proofwiki.org/wiki/Finite_Union_of_Finite_Sets_is_Finite|archivedate=11 September 2014|df=}}</ref>\n\n== Arbitrary unions ==\n\nThe most general notion is the union of an arbitrary collection of sets, sometimes called an ''infinitary union''. If '''M''' is a set or [[Class (set theory)|class]] whose elements are sets, then ''x'' is an element of the union of '''M''' [[if and only if]] there is [[existential quantification|at least one]] element ''A'' of '''M''' such that ''x'' is an element of ''A''.<ref name=\":1\">{{Cite book|url=https://books.google.com/books?id=DOUbCgAAQBAJ|title=A Transition to Advanced Mathematics|last=Smith|first=Douglas|last2=Eggen|first2=Maurice|last3=Andre|first3=Richard St|date=2014-08-01|publisher=Cengage Learning|isbn=9781285463261|language=en}}</ref> In symbols:\n: <math>x \\in \\bigcup \\mathbf{M} \\iff \\exists A \\in \\mathbf{M},\\ x \\in A.</math>\nThis idea subsumes the preceding sections—for example, ''A'' ∪ ''B'' ∪ ''C'' is the union of the collection {''A'', ''B'', ''C''}. Also, if '''M''' is the empty collection, then the union of '''M''' is the empty set.\n\n=== Notations ===\nThe notation for the general concept can vary considerably.  For a finite union of sets <math>S_1, S_2, S_3, \\dots , S_n</math> one often writes <math>S_1 \\cup S_2 \\cup S_3 \\cup \\dots \\cup S_n</math> or <math>\\bigcup_{i=1}^n S_i</math>.  Various common notations for arbitrary unions include  <math>\\bigcup \\mathbf{M}</math>, <math>\\bigcup_{A\\in\\mathbf{M}} A</math>, and <math>\\bigcup_{i\\in I} A_{i}</math>, the last of which refers to the union of the collection <math>\\left\\{A_i : i \\in I\\right\\}</math> where ''I'' is an [[index set]] and <math>A_i</math> is a set for every <math>i \\in I</math>. In the case that the index set ''I'' is the set of [[natural number]]s, one uses a notation <math>\\bigcup_{i=1}^{\\infty} A_{i}</math> analogous to that of the [[series (mathematics)|infinite series]].<ref name=\":1\" />\n\nWhen the symbol \"∪\" is placed before other symbols instead of between them, it is usually rendered as a larger size.\n\n== See also ==\n{{Portal|Set theory|Discrete mathematics|Mathematics}}\n* [[Alternation (formal language theory)]], the union of sets of strings\n* [[Disjoint union]]\n* [[Intersection (set theory)]]\n* [[Iterated binary operation]]\n* [[Naive set theory]]\n* [[Symmetric difference]]\n\n==Notes ==\n<references/>\n\n== External links ==\n{{commons category}}\n*{{MathWorld |title=Union |id=Union }}\n*{{springer|title=Union of sets|id=p/u095390}}\n*[http://www.apronus.com/provenmath/sum.htm Infinite Union and Intersection at ProvenMath] De Morgan's laws formally proven from the axioms of set theory.\n\n{{Set theory}}\n\n[[Category:Basic concepts in set theory]]\n[[Category:Binary operations]]"
    }
  ]
}