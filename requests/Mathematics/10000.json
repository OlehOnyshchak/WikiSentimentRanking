{
  "pages": [
    {
      "title": "Faber polynomials",
      "url": "https://en.wikipedia.org/wiki/Faber_polynomials",
      "text": "In mathematics, the '''Faber polynomials''' ''P''<sub>''m''</sub> of a [[Laurent series]]\n:<math>\\displaystyle f(z)=z^{-1}+a_0+a_1z+\\cdots</math>\nare the polynomials such that\n:<math>\\displaystyle P_m(f)-z^{-m}</math>\nvanishes at ''z''=0. They were introduced by {{harvs |txt |last=Faber |authorlink=Georg Faber |year1=1903 |year2=1919}} and studied by {{harvs|txt|last=Grunsky|authorlink=Helmut Grunsky|year=1939}} and {{harvs|txt|last=Schur|authorlink=Issai Schur|year=1945}}.\n\n==References==\n\n*{{Citation | last1=Curtiss | first1=J. H. | title=Faber Polynomials and the Faber Series | jstor=2316567 | publisher=[[Mathematical Association of America]] | year=1971 | journal=[[American Mathematical Monthly|The American Mathematical Monthly]] | issn=0002-9890 | volume=78 | issue=6 | pages= 577–596 | doi=10.2307/2316567}}\n*{{Citation | last1=Faber | first1=Georg | title=Über polynomische Entwickelungen | doi=10.1007/BF01444293 | publisher=Springer Berlin / Heidelberg | year=1903 | journal=[[Mathematische Annalen]] | issn=0025-5831 | volume=57 | pages=389–408}}\n*{{Citation | last1=Faber | first1=G. | title=Über Tschebyscheffsche Polynome. | url=http://resolver.sub.uni-goettingen.de/purl?GDZPPN00216874X | language=German | jfm=47.0315.01 | year=1919 | journal=[[Journal für die reine und angewandte Mathematik]] | issn=0075-4102 | volume=150 | pages=79–106}}\n*{{Citation | last1=Grunsky | first1=Helmut | title=Koeffizientenbedingungen für schlicht abbildende meromorphe Funktionen | doi=10.1007/BF01580272 | year=1939 | journal=[[Mathematische Zeitschrift]] | issn=0025-5874 | volume=45 | issue=1 | pages=29–61}}\n*{{Citation | last1=Schur | first1=Issai | title=On Faber polynomials | jstor=2371913 | mr=0011740 | year=1945 | journal=[[American Journal of Mathematics]] | issn=0002-9327 | volume=67 | pages=33–41 | doi=10.2307/2371913}}\n*{{Citation | last1=Suetin | first1=P. K. | title=Series of Faber polynomials | origyear=1984 | url=https://books.google.com/books?id=uZZS0Eeh5RMC | publisher=Gordon and Breach Science Publishers | location=New York | series=Analytical Methods and Special Functions | isbn=978-90-5699-058-9 | mr=1676281 | year=1998 | volume=1}}\n*{{eom|id=f/f038010|first=P. K.|last= Suetin}}\n\n[[Category:Polynomials]]"
    },
    {
      "title": "Factorization of polynomials",
      "url": "https://en.wikipedia.org/wiki/Factorization_of_polynomials",
      "text": "{{For|an elementary treatment|Factorization}}\n{{See also|Factorization of polynomials over finite fields}}\n\nIn [[mathematics]] and [[computer algebra]], '''factorization of polynomials''' or '''polynomial factorization'''  expresses a [[polynomial]] with coefficients in a given [[field (mathematics)|field]] or in the [[integers]] as the product of [[irreducible polynomial|irreducible factors]] with coefficients in the same domain. Polynomial factorization is one of the fundamental components of [[computer algebra system]]s.\n\nThe first polynomial factorization algorithm was published by [[Theodor von Schubert]] in 1793.<ref>FT Schubert: ''De Inventione Divisorum'' Nova Acta Academiae Scientiarum Petropolitanae v.11, p. 172-182(1793)</ref> [[Leopold Kronecker]] rediscovered Schubert's algorithm in 1882 and extended it to multivariate polynomials and coefficients in an algebraic extension. But most of the knowledge on this topic is not older than circa 1965 and the first [[computer algebra systems]]:\n\n<blockquote>\nWhen the long-known finite step algorithms were first put on computers, they turned out to be highly inefficient. The fact that almost any uni- or multivariate polynomial of degree up to 100 and with coefficients of a moderate size (up to 100 bits) can be factored by modern algorithms in a few minutes of computer time indicates how successfully this problem has been attacked during the past fifteen years. (Erich Kaltofen, 1982)\n</blockquote>\n\nNowadays, modern algorithms and computers can quickly factor [[#Factoring univariate polynomials over the integers|univariate polynomials]] of degree more than 1000 having coefficients with thousands of digits.<ref>An example of degree 2401, taking 7.35 seconds, is found in Section 4 in: Hart, van Hoeij, Novocin: ''Practical Polynomial Factoring in Polynomial Time'' ISSAC'2011 Proceedings, p. 163-170 (2011).</ref>\n\n==Formulation of the question==\n\n[[Polynomial ring]]s over the integers or over a field are [[unique factorization domain]]s. This means that every element of these rings is a product of a constant and a product of [[irreducible polynomial]]s (those that are not the product of two non-constant polynomials). Moreover, this decomposition is unique up to multiplication of the factors by invertible constants.\n\nFactorization depends on the base field.  For example, the [[fundamental theorem of algebra]], which states that every polynomial with [[complex number|complex]] coefficients has complex roots, implies that a polynomial with integer coefficients can be factored (with [[root-finding algorithms]]) into [[linear function|linear factor]]s over the complex field '''C'''. Similarly, over the [[field of reals]], the irreducible factors have degree at most two, while there are polynomials of any degree that are irreducible over the [[field of rationals]] '''Q'''.\n\nThe question of polynomial factorization makes sense only for coefficients in a ''computable field'' whose every element may be represented in a computer and for which there are algorithms for the arithmetic operations. Fröhlich and Shepherson give examples of such fields for which no factorization algorithm can exist.\n\nThe fields of coefficients for which factorization algorithms are known include [[prime field]]s (i.e., the [[field of rationals]] and prime [[modular arithmetic]]) and their [[finitely generated field extension]]s. Integer coefficients are also tractable. Kronecker's classical method is interesting only from a historical point of view; modern algorithms proceed by a succession of:\n\n* Square-free factorization\n* Factorization over finite fields\nand reductions:\n* From the [[multivariate polynomial|multivariate]] case to the [[univariate]] case.\n* From coefficients in a [[purely transcendental extension]] to the multivariate case over the ground field (see [[#Primitive part–content factorization|below]]).\n* From coefficients in an algebraic extension to coefficients in the ground field (see [[#Factoring over algebraic extensions (Trager's method)|below]]).\n* From rational coefficients to integer coefficients (see [[#Primitive part–content factorization|below]]).\n* From integer coefficients to coefficients in a prime field with ''p'' elements, for a well chosen ''p'' (see [[#Factoring univariate polynomials over the integers|below]]).\n\n==Primitive part–content factorization==<!-- There are two links here from a preceding section-->\n{{See also|Content (algebra)|Gauss's lemma (polynomial)}}\n\nIn this section, we show that factoring over '''Q''' (the rational numbers) and over '''Z''' (the integers) is essentially the same problem.\n\nThe ''content'' of a polynomial ''p'' ∈ '''Z'''[''X''], denoted \"cont(''p'')\",  is, [[up to]] its sign, the [[greatest common divisor]] of its coefficients. The ''primitive part'' of ''p'' is primpart(''p'')=''p''/cont(''p''), which is a [[primitive polynomial (ring theory)|primitive polynomial]] with integer coefficients. This defines a factorization of ''p'' into the product of an integer and a primitive polynomial. This factorization is unique up to the sign of the content. It is a usual convention to choose the sign of the content such that the leading coefficient of the primitive part is positive.\n\nFor example,\n\n:<math>\n-10x^2 + 5x + 5 = (-5) (2x^2 - x - 1) \\,\n</math>\nis a factorization into content and primitive part.\n\nEvery polynomial ''q'' with rational coefficients may be written\n:<math>q = \\frac{p}{c},</math>\nwhere ''p'' ∈ '''Z'''[''X''] and ''c'' ∈ '''Z''': it suffices to take for ''c'' a multiple of all denominators of the coefficients of ''q'' (for example their product) and ''p'' = ''cq''. The ''content'' of ''q'' is defined as:\n:<math>\\text{cont} (q) =\\frac{\\text{cont} (p)}{c},</math>\nand the ''primitive part'' of ''q'' is that of ''p''. As for the polynomials with integer coefficients, this defines a factorization into a rational number and a primitive polynomial with integer coefficients. This factorization is also unique up to the choice of a sign.\n\nFor example,\n:<math>\n\\frac{x^5}{3} + \\frac{7x^2}{2} + 2x + 1 = \\frac{2x^5 + 21x^2 + 12x + 6}{6}</math>\nis a factorization into content and primitive part.\n\n[[Carl Friedrich Gauss|Gauss]] proved that the product of two primitive polynomials is also primitive ([[Gauss's lemma (polynomial)|Gauss's lemma]]). This implies that a primitive polynomial is irreducible over the rationals if and only if it is irreducible over the integers. This implies also that the factorization over the rationals of a polynomial with rational coefficients is the same as the factorization over the integers of its primitive part. Similarly, the factorization over the integers of a polynomial with integer coefficients is the product of the factorization of its primitive part by the factorization of its content.\n\nIn other words, an integer GCD computation reduces the factorization of a polynomial over the rationals to the factorization of a primitive polynomial with integer coefficients, and the factorization over the integers to the factorization of an integer and a primitive polynomial.\n\nEverything that precedes remains true if '''Z''' is replaced by a polynomial ring over a field ''F'' and '''Q''' is replaced by a [[field of rational functions]] over ''F'' in the same variables, with the only difference that \"up to a sign\" must be replaced by \"up to the multiplication by an invertible constant in ''F''\". This reduces the factorization over a [[purely transcendental]] field extension of ''F'' to the factorization of [[multivariate polynomial]]s over ''F''.\n\n==Square-free factorization==\n\n{{Main|Square-free polynomial}}\n\nIf two or more factors of a polynomial are identical, then the polynomial is a multiple of the square of this factor. In the case of univariate polynomials, this results in [[Multiplicity (mathematics)#Multiplicity of a root of a polynomial|multiple roots]]. In this case, then the multiple factor is also a factor of the polynomial's [[formal derivative|derivative]] (with respect to any of the variables, if several). In the case of univariate polynomials over the rationals (or more generally over a field of [[characteristic (algebra)|characteristic]] zero), [[Square-free polynomial#Yun's algorithm|Yun's algorithm]] exploits this to efficiently factorize the polynomial into square-free factors, that is, factors that are not a multiple of a square. To factorize the initial polynomial, it suffices to factorize each square-free factor. Square-free factorization is therefore the first step in most polynomial factorization algorithms.\n\nYun's algorithm extends this to the multivariate case by considering a multivariate polynomial as an univariate polynomial over a polynomial ring.\n\nIn the case of a polynomial over a finite field, Yun's algorithm applies only if the degree is smaller than the characteristic, because, otherwise, the derivative of a non-zero polynomial may be zero (over the field with ''p'' elements, the derivative of a polynomial in ''x''<sup>''p''</sup> is always zero). Nevertheless, a succession of GCD computations, starting from the polynomial and its derivative, allows one to compute the square-free decomposition; see [[Polynomial factorization over finite fields#Square-free factorization]].\n\n==Classical methods==\nThis section describes textbook methods that can be convenient when computing by hand. These methods are not used for computer computations because they use [[integer factorization]], which is currently slower than polynomial factorization.\n\n===Obtaining linear factors===\n\nAll linear factors with rational coefficients can be found using the [[rational root test]]. If the polynomial to be factored is <math>a_nx^n + a_{n-1}x^{n-1} + \\cdots + a_1x + a_0</math>, then all possible linear factors are of the form <math>b_1x-b_0</math>, where <math>b_1</math> is an integer factor of <math>a_n</math> and <math>b_0</math> is an integer factor of <math>a_0</math>. All possible combinations of integer factors can be tested for validity, and each valid one can be factored out using [[polynomial long division]]. If the original polynomial is the product of factors at least two of which are of degree 2 or higher, this technique only provides a partial factorization; otherwise the factorization is complete. In particular, if there is exactly one non-linear factor, it will be the polynomial left after all linear factors have been factorized out. In the case of a [[Cubic function|cubic polynomial]], if the cubic is factorizable at all, the rational root test gives a complete factorization, either into a linear factor and an irreducible quadratic factor, or into three linear factors.\n\n===Kronecker's method===\n\nSince integer polynomials must factor into integer polynomial factors, and evaluating integer polynomials at integer values must produce integers, the integer values of a polynomial can be factored in only a finite number of ways, and produce only a finite number of possible polynomial factors.\n\nFor example, consider\n\n:<math>f(x) = x^5 + x^4 + x^2 + x + 2</math>.\n\nIf this polynomial factors over '''Z''', then at least one of its factors must be of degree two or less.  We need three values to uniquely fit a second degree polynomial.  We'll use <math>f(0) = 2</math>, <math>f(1) = 6</math> and <math>f(-1) = 2</math>. If one of those values is 0, then we have found a root (and so a factor). If none are 0, then each one has a finite number of divisors.  Now, 2 can only factor as\n\n:1×2, 2×1, (−1)×(−2), or (−2)×(−1).\n\nTherefore, if a second degree integer polynomial factor exists, it must take one of the values\n\n:1, 2, −1, or −2\n\nat <math>x=0</math>, and likewise at <math>x=-1</math>.  There are eight different ways to factor 6 (one for each divisor of 6), so there are\n\n:4×4×8 = 128\n\npossible combinations, of which half can be discarded as the negatives of the other half, corresponding to 64 possible second degree integer polynomials that must be checked.  These are the only possible integer polynomial factors of <math>f(x)</math>.  Testing them exhaustively reveals that\n\n:<math>p(x) = x^2 + x + 1</math>\n\nconstructed from <math>p(0)=1</math>, <math>p(1)=3</math> and <math>p(-1)=1</math>, factors <math>f(x)</math>.\n\nDividing <math>f</math> by <math>p</math> gives the other factor <math>q(x) = x^3 - x + 2</math>, so that <math>f = pq</math>.\nNow one can test recursively to find factors of <math>p</math> and <math>q</math>.  It turns out they both are irreducible over the integers, so that the irreducible factorization of <math>f</math> is <ref>''Van der Waerden'', Sections 5.4 and 5.6</ref>\n\n:<math>f(x) = p(x)q(x) = (x^2 + x + 1)(x^3 - x + 2). </math>\n\n==Modern methods==\n\n===Factoring over finite fields===\n{{Main|Factorization of polynomials over finite fields|Berlekamp's algorithm|Cantor–Zassenhaus algorithm}}\n\n===Factoring univariate polynomials over the integers===\nIf <math>f(x)</math> is a univariate polynomial over the integers, assumed\nto be [[#Primitive part–content factorization|content-free]]\nand [[#Square-free factorization|square-free]], one starts by computing a bound <math>B</math>\nsuch that any factor <math>g(x)</math> has coefficients of\nabsolute value bounded by <math>B</math>. This way, if <math>m</math> is\nan integer larger than <math>2B</math>, and if <math>g(x)</math> is known modulo\n<math>m</math>, then <math>g(x)</math> can be reconstructed from its image mod <math>m</math>.\n\nThe [[Hans Zassenhaus|Zassenhaus]] algorithm proceeds as follows. First, choose a prime\nnumber <math>p</math> such that the image of <math>f(x)</math> mod <math>p</math>\nremains [[#Square-free factorization|square-free]], and of the same degree as <math>f(x)</math>.\nThen factor <math>f(x)</math> mod <math>p</math>. This produces integer polynomials <math>f_1(x),...,f_r(x)</math> whose product matches <math>f(x)</math> mod <math>p</math>. Next, apply [[Hensel's lemma|Hensel lifting]]; this updates the <math>f_i(x)</math> in such a way that their product matches <math>f(x)</math> mod <math>p^a</math>,  where <math>a</math> is chosen in such a way that <math>p^a</math> is larger than <math>2B</math>. Modulo <math>p^a</math>, the polynomial <math>f(x)</math> has (up to units) <math>2^r</math> factors: for each subset of <math>{f_1(x),...,f_r(x)}</math>, the product is a factor of <math>f(x)</math> mod <math>p^a</math>.  However, a factor modulo <math>p^a</math> need not correspond to a so-called \"true factor\": a factor of <math>f(x)</math> in <math>Z[x]</math>.  For each factor mod <math>p^a</math>, we can test if it corresponds to a \"true\" factor, and if so, find that \"true\" factor, provided that <math>p^a</math> exceeds <math>2B</math>.\nThis way, all irreducible \"true\" factors can be found by checking at most <math>2^r</math> cases. This is reduced to <math>2^{r-1}</math> cases by skipping complements. If <math>f(x)</math> is reducible, the number of cases is reduced further by removing those <math>f_i(x)</math> that appear in an already found \"true\" factor. Zassenhaus algorithm processes each case (each subset) quickly, however, in the worst case, it considers an exponential number of cases.\n\nThe first [[polynomial time]] algorithm for factoring rational polynomials was discovered by Lenstra, Lenstra and Lovász and is an application of [[Lenstra–Lenstra–Lovász lattice basis reduction algorithm]], the \"LLL algorithm\".{{harv|Lenstra|Lenstra|Lovász|1982}}\nA simplified version of the LLL factorization algorithm is as follows: calculate a complex (or ''p''-adic) root α of the polynomial <math>f(x)</math> to high precision, then use the [[Lenstra–Lenstra–Lovász lattice basis reduction algorithm]] to find an approximate [[Integer relation algorithm|linear relation]] between 1, α, α<sup>2</sup>, α<sup>3</sup>, ... with integer coefficients, which might be an exact linear relation and a polynomial factor of <math>f(x)</math>. One can determine a bound for the precision that guarantees that this method produces either a factor, or an irreducibility proof.  Although this method is polynomial time, it is not used in practice because the lattice has high dimension and huge entries, which makes the computation slow.\n\nThe exponential complexity in the algorithm of Zassenhaus comes from a combinatorial problem: how to select the right subsets of <math>f_1(x),...,f_r(x)</math>. State of the art factoring implementations work in a manner similar to Zassenhaus, except that the combinatorial problem is translated to a lattice problem that is then solved by LLL.<ref>M. van Hoeij: ''Factoring polynomials and the knapsack problem.'' Journal of Number Theory, 95, 167-189, (2002).</ref> In this approach, LLL is not used to compute coefficients of factors, instead, it is used to compute vectors with <math>r</math> entries in {0,1} that encode the subsets of <math>f_1(x),...,f_r(x)</math> that correspond to the irreducible \"true\" factors.\n\n===Factoring over algebraic extensions (Trager's method)===\nWe can factor a polynomial <math>p(x) \\in K[x] </math>, where the field <math>K</math> is a finite extension of <math>\\mathbb{Q}</math>.  First, using [[#Square-free factorization|square-free factorization]], we may suppose that the polynomial is square-free.  Next we write <math>L= K[x]/p(x)</math> explicitly as an algebra over <math>\\mathbb{Q}</math>.  We next pick a random element <math>\\alpha \\in L</math>.  By the primitive element theorem, <math>\\alpha</math> generates <math>L</math> over <math>\\mathbb{Q}</math> with high probability.  If this is the case, we can compute the minimal polynomial, <math>q(y)\\in \\mathbb{Q}[y]</math> of <math>\\alpha</math> over <math>\\mathbb{Q}</math>.  Factoring\n\n:<math>q(y) = \\prod_{i=1}^{n} q_i(y)</math>\n\nover <math>\\mathbb{Q}[y]</math>, we determine that\n\n:<math>L = \\mathbb{Q}[\\alpha] = \\mathbb{Q}[y]/q(y) = \\prod_{i=1}^n \\mathbb{Q}[y]/q_i(y)</math>\n\n(notice that <math>L</math> is a [[reduced ring]] since <math>p(x)</math> is square-free), where <math>\\alpha</math> corresponds to the element <math>(y,y,\\ldots,y)</math>.  Note that this is the unique decomposition of <math>L</math> as a product of fields.  Hence this decomposition is the same as\n\n:<math>\\prod_{i=1}^m K[x]/p_i(x)</math>\n\nwhere\n\n:<math>p(x) = \\prod_{i=1}^m p_i(x)</math>\n\nis the factorization of <math>p(x)</math> over <math>K[x]</math>.  By writing <math>x\\in L</math> and generators of <math>K</math> as a polynomials in <math>\\alpha</math>, we can determine the embeddings of <math>x</math> and <math>K</math> into the components <math>\\mathbb{Q}[y]/q_i(y)=K[x]/p_i(x)</math>.  By finding the minimal polynomial of <math>x</math> in this ring, we have computed <math>p_i(x)</math>, and thus factored <math>p(x)</math> over <math>K.</math>\n\n==See also==\n\n* {{slink|Factorization|Polynomials}}, for elementary heuristic methods and explicit formulas\n\n==Bibliography==\n{{Reflist}}\n*{{citation |author1=Fröhlich, A.|author2=Shepherson, J. C.|title = On the factorisation of polynomials in a finite number of steps|journal = Mathematische Zeitschrift|volume = 62|issue=1|year = 1955|issn = 0025-5874|doi=10.1007/BF01180640|pages=331–334}}\n*{{citation |author1=Trager, B.M.|title = Algebraic Factoring and Rational Function Integration|journal = Proc. SYMSAC 76|pages = 219–226|url= http://dl.acm.org/citation.cfm?id=806338|doi = 10.1145/800205.806338|year = 1976|series = Symsac '76}}\n* {{Cite journal|title=Quantitative Estimates for Polynomials in One or Several Variables: From Analysis and Number Theory to Symbolic and Massively Parallel Computation\n|author=Bernard Beauzamy, [[Per Enflo]], Paul Wang\n|journal=Mathematics Magazine\n|volume=67\n|issue=4\n|date=October 1994\n|pages=243–257\n|jstor=2690843|ref=harv\n|doi=10.2307/2690843}} (accessible to readers with undergraduate mathematics)\n*{{Cite book | last1=Cohen | first1=Henri | title=A course in computational algebraic number theory | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Graduate Texts in Mathematics | isbn=978-3-540-55640-4 | mr=1228206  | year=1993 | volume=138 | ref=harv | postscript=<!-- Bot inserted parameter. Either remove it; or change its value to \".\" for the cite to end in a \".\", as necessary. -->}}\n* {{Citation|first1=Erich|last1=Kaltofen|chapter=Factorization of polynomials|title=Computer Algebra|pages=95–113|publisher=Springer Verlag|year=1982|editor1 =B. Buchberger|editor2=R. Loos|editor3=G. Collins|citeseerx = 10.1.1.39.7916|accessdate=<!-- September 20, 2012 -->}}\n*{{cite book\n|author=[[Donald E. Knuth|Knuth, Donald E]]\n|chapter=4.6.2 Factorization of Polynomials\n|title=Seminumerical Algorithms\n|series=[[The Art of Computer Programming]]\n|volume=2\n|edition=Third\n|location=Reading, Massachusetts\n|publisher=Addison-Wesley\n|year=1997\n|pages=439–461, 678–691<!--   xiv+762 -->\n|isbn=978-0-201-89684-8}}\n*{{Cite journal | last1=Lenstra | first1=A. K. | author1-link=A. K. Lenstra | last2=Lenstra  | first2=H. W. | last3=Lovász | first3=László | author3-link=László Lovász | title=Factoring polynomials with rational coefficients | doi=10.1007/BF01457454 | mr=682664  | year=1982 | journal=[[Mathematische Annalen]] | issn=0025-5831 | volume=261 | issue=4 | pages=515–534 | ref=harv | postscript=<!-- Bot inserted parameter. Either remove it; or change its value to \".\" for the cite to end in a \".\", as necessary. -->| citeseerx=10.1.1.310.318 }}\n* [[Bartel Leendert van der Waerden|Van der Waerden]], ''Algebra'' (1970), trans. Blum and Schulenberger, Frederick Ungar.\n\n==Further reading==\n* {{Citation|first1=Erich|last1=Kaltofen|chapter=Polynomial Factorization 1982-1986|title=Computers in Mathematics|series=Lecture Notes in Pure and Applied Mathematics|volume=125|publisher=Marcel Dekker, Inc.|year=1990|editor1 =D. V. Chudnovsky|editor2=R. D. Jenks|citeseerx = 10.1.1.68.7461|accessdate=<!-- October 14, 2012 -->}}\n* {{Citation|first1=Erich|last1=Kaltofen|chapter=Polynomial Factorization 1987–1991|title=Proceedings of Latin '92|series=Springer Lect. Notes Comput. Sci.|volume=583|publisher=Springer|year=1992 |chapter-url=http://www4.ncsu.edu/~kaltofen/bibliography/92/Ka92_latin.pdf|accessdate=October 14, 2012}}\n* {{Citation|first1=Gabor|last1=Ivanyos|first2=Karpinski|last2=Marek|first3=Nitin|last3=Saxena|title=Schemes for Deterministic Polynomial Factoring|journal=Proc. ISSAC 2009|year=2009|pages=191–198|doi=10.1145/1576702.1576730|arxiv=0804.1974|isbn=9781605586090}}\n\n{{DEFAULTSORT:Factorization Of Polynomials}}\n[[Category:Articles with inconsistent citation formats]]\n[[Category:Polynomials]]\n[[Category:Computer algebra]]"
    },
    {
      "title": "Faddeev–LeVerrier algorithm",
      "url": "https://en.wikipedia.org/wiki/Faddeev%E2%80%93LeVerrier_algorithm",
      "text": "[[Image:Urbain Le Verrier.jpg|220px|thumb|right|[[Urbain Le Verrier]] (1811&ndash;1877)<br> The discoverer of [[Neptune]].]]\n\nIn mathematics ([[linear algebra]]), the '''Faddeev–LeVerrier algorithm''' is a [[Recurrence relation|recursive]] method to calculate the coefficients of the  [[characteristic polynomial]] <math>p(\\lambda)=\\det (\\lambda I_n - A)</math> of a  square [[Matrix (mathematics)|matrix]], {{mvar|A}}, named after  [[Dmitry Konstantinovich Faddeev]] and [[Urbain Le Verrier]]. Calculation of this polynomial yields the  [[eigenvalue]]s  of {{mvar|A}} as its roots; as a matrix polynomial in  the matrix {{mvar|A}} itself, it vanishes by the fundamental [[Cayley–Hamilton theorem]]. Calculating determinants, however, is computationally cumbersome, whereas this efficient algorithm is computationally significantly more efficient (in [[NC (complexity)|NC complexity class]]).\n\nThe algorithm has been independently rediscovered several times, in some form or another. It was first published in 1840 by  [[Urbain Le Verrier]], subsequently redeveloped by P. Horst, [[Jean-Marie Souriau]], in its present form here by Faddeev and Sominsky, and further by  J. S. Frame, and others.<ref>[[Urbain Le Verrier]]: ''Sur les variations séculaires des éléments des orbites pour les sept planètes principales'', ''J. de Math.'' (1) '''5''', 230 (1840), [http://gallica.bnf.fr/ark:/12148/bpt6k163849/f228n35.capture# Online]</ref><ref>Paul Horst: ''A method of determining the coefficients of a characteristic equation''. ''Ann. Math. Stat.'' '''6''' 83-84 (1935), {{DOI|10.1214/aoms/1177732612}}</ref><ref>[[Jean-Marie Souriau]], ''Une méthode pour la décomposition spectrale et l'inversion des matrices'', ''Comptes Rend.''  '''227''', 1010-1011 (1948).</ref><ref>D. K. Faddeev, and I. S. Sominsky, ''Sbornik zadatch po vyshej algebra'' ([http://www.isinj.com/aime/Problems%20in%20Higher%20Algebra%20-%20Faddeev,%20Sominskii%20(MIR,1972).pdf Problems in higher algebra], Mir publishers, 1972),  Moskow-Leningrad (1949). Problem '''979'''.</ref><ref>J. S. Frame: ''A simple recursion formula for inverting a matrix (abstract)'', ''Bull. Am. Math. Soc.'' '''55''' 1045 (1949), {{DOI|10.1090/S0002-9904-1949-09310-2}}</ref> (For historical points, see Householder.<ref>\n{{cite book|ref=harv|first=Alston S.|last=Householder|title=The Theory of Matrices in Numerical Analysis |publisher=Dover Books on Mathematics|year=2006|authorlink=Alston Scott Householder | isbn=0486449726}}</ref>  An elegant shortcut to the proof, bypassing [[Newton polynomial]]s, was introduced by Hou.<ref>Hou, S. H. (1998). [http://epubs.siam.org/doi/pdf/10.1137/S003614459732076X \"Classroom Note: A Simple Proof of the Leverrier--Faddeev Characteristic Polynomial Algorithm\"] ''SIAM review''  '''40(3)''' 706-709, {{doi|10.1137/S003614459732076X}} .</ref> The bulk of the presentation here follows Gantmacher, p.&nbsp;88.<ref>{{cite book|ref=harv| last= Gantmacher|first=F.R. | title=The Theory of Matrices  |year=1960| publisher= Chelsea Publishing|location= NY | ISBN = 0-8218-1376-5 }}</ref>)\n\n==The Algorithm==\nThe objective is to calculate the coefficients {{math|''c<sub>k</sub>''}} of the characteristic polynomial of the {{math|''n''×''n''}} matrix {{mvar|A}},\n::<math>p(\\lambda)\\equiv \\det(\\lambda I_n-A)=\\sum_{k=0}^{n} c_k \\lambda^k~,</math>\nwhere, evidently,  {{math|''c<sub>n</sub>''}} = 1 and  {{math|''c''}}<sub>0</sub> = (−1)<sup>''n''</sup> det {{mvar|A}}.\n\nThe coefficients are determined recursively from the top down, by dint of the auxiliary matrices {{mvar|M}}, \n:<math> \\begin{align}\nM_0 &\\equiv 0      & c_n &= 1                                                               \\qquad &(k=0) \\\\\nM_k &\\equiv AM_{k-1} + c_{n-k+1} I \\qquad \\qquad  & c_{n-k} &= -\\frac 1 k \\mathrm{tr}(AM_k) \\qquad &k=1,\\ldots ,n   ~.\n\\end{align}</math>\n\nThus, \n:<math>                                                             \nM_1=   I ~, \\quad   c_{n-1} = - \\mathrm{tr} A =-c_n  \\mathrm{tr} A ; </math>\n:<math>M_2=   A-I\\mathrm{tr} A , \\quad c_{n-2}=-\\frac{1}{2}\\Bigl(\\mathrm{tr} A^2 -(\\mathrm{tr} A)^2\\Bigr) =\n-\\frac{1}{2} (c_n  \\mathrm{tr} A^2 +c_{n-1} \\mathrm{tr} A) ; \n</math>\n:<math>M_3=   A^2-A\\mathrm{tr} A -\\frac{1}{2}\\Bigl(\\mathrm{tr} A^2 -(\\mathrm{tr} A)^2\\Bigr)  I,</math>\n::<math>c_{n-3}=- \\tfrac{1}{6}\\Bigl( (\\operatorname{tr}A)^3-3\\operatorname{tr}(A^2)(\\operatorname{tr}A)+2\\operatorname{tr}(A^3)\\Bigr)=-\\frac{1}{3}(c_n  \\mathrm{tr} A^3+c_{n-1} \\mathrm{tr} A^2 +c_{n-2}\\mathrm{tr} A); </math>\netc.,<ref>Zadeh, Lotfi A. and  Desoer, Charles A. (1963, 2008).  ''Linear System Theory: The State Space Approach'' (Mc Graw-Hill; Dover Civil and Mechanical Engineering) {{ISBN|9780486466637}} , pp 303&ndash;305; \n</ref><ref>Abdeljaoued, Jounaidi  and  Lombardi, Henri (2004). ''Méthodes matricielles - Introduction à la complexité algébrique'',\n (Mathématiques et Applications, 42)  Springer,  {{ISBN|3540202471}} .</ref>\n&nbsp;  ...; \n:<math>M_m= \\sum_{k=1}^{m} c_{n-m+k}  A^{k-1}   ~,</math>\n:<math>c_{n-m}= -\\frac{1}{m}(c_n  \\mathrm{tr} A^m+c_{n-1} \\mathrm{tr} A^{m-1}+...+c_{n-m+1}\\mathrm{tr} A)= -\\frac{1}{m}\\sum_{k=1}^{m} c_{n-m+k}  \\mathrm{tr} A^k   ~  ;  ...</math>\n\nObserve {{math|''A<sup>−1</sup> {{=}} − M<sub>n</sub> /c<sub>0</sub>'' {{=}} (−1)<sup>''n''−1</sup>''M<sub>n</sub>''/det''A''}} terminates the recursion at {{mvar| λ}}.  This could be used to obtain the inverse or the determinant of {{mvar|A}}.\n\n==Derivation==\nThe proof  relies on the modes of the [[adjugate matrix]], {{math|''B<sub>k</sub> ≡ M<sub>n−k</sub>''}}, the auxiliary matrices encountered.  &nbsp; \nThis matrix is defined by \n::<math>(\\lambda I-A) B = I ~ p(\\lambda)</math> \nand is thus proportional to the [[Resolvent formalism|resolvent]] \n:<math>B =   p(\\lambda) \\frac{I}{\\lambda I-A } ~. </math>\n\nIt is evidently a matrix polynomial in {{math|''λ''}} of degree {{math|''n−1''}}. Thus,\n::<math>B\\equiv \\sum_{k=0}^{n-1}  \\lambda^k~ B_k=  \\sum_{k=0}^{n}  \\lambda^k~ M_{n-k}  ,</math>\nwhere one may define the harmless {{math|''M''<sub>0</sub>}}≡0.\n\nInserting the explicit polynomial forms into the defining equation for the adjugate, above, \n:<math>\\sum_{k=0}^{n}  \\lambda^{k+1} M_{n-k} - \\lambda^k (AM_{n-k} +c_k I)=  0~.</math>\n\nNow, at the highest order, the first term vanishes by {{math|''M''<sub>0</sub>}}=0; whereas  at the bottom order (constant in  {{mvar|λ}}, from the defining equation of the adjugate, above),\n:<math>M_n A= B_0 A=c_0~,</math> \nso that shifting the dummy indices of the first term yields \n:<math>\\sum_{k=1}^{n}  \\lambda^{k} \\Big ( M_{1+n-k} - AM_{n-k} +c_k I\\Big )=  0~,</math>\nwhich thus dictates the recursion\n:<math>\\therefore   \\qquad M_{m} =A M_{m-1} +c_{n-m+1} I ~,</math>\nfor {{mvar|m}}=1,...,{{mvar|n}}. Note that ascending index amounts to descending in powers of {{mvar|λ}}, but the polynomial coefficients {{mvar|c}} are yet to be determined in terms of the {{mvar|M}}s and {{mvar|A}}.\n\nThis can be easiest achieved through the following auxiliary equation (Hou, 1998),\n::<math>\\lambda \\frac{\\partial p(\\lambda) }{ \\partial \\lambda} -n p =\\operatorname{tr} AB~.</math> \nThis is but the trace of the defining equation for  {{mvar|B}} by dint of  [[Jacobi's formula]],\n:<math>\\frac{\\partial p(\\lambda)}{\\partial \\lambda}= p(\\lambda) \\sum^\\infty _{m=0}\\lambda ^{-(m+1)}  \\operatorname{tr}A^m =  \np(\\lambda) ~  \\operatorname{tr} \\frac{I}{\\lambda I -A}\\equiv\\operatorname{tr} B~. </math>\n\nInserting the polynomial mode forms in this auxiliary equation yields\n:<math>\\sum_{k=1}^{n}  \\lambda^{k} \\Big ( k c_k - n c_k - \\operatorname{tr}A M_{n-k}\\Big )=  0~,</math>\nso that \n:<math>\\sum_{m=1}^{n-1}  \\lambda^{n-m} \\Big ( m c_{n-m}  + \\operatorname{tr}A M_{m}\\Big )=  0~,</math>\nand finally \n:<math>\\therefore  \\qquad c_{n-m} = -\\frac{1}{m} \\operatorname{tr}A M_{m}  ~.</math>\nThis completes the recursion of the previous section, unfolding in descending powers of {{mvar|λ}}.\n\nFurther note in the algorithm that, more directly, \n:<math> M_{m} =A M_{m-1} - \\frac{1}{m-1} (\\operatorname{tr}A M_{m-1}) I   ~,</math>\nand, in comportance with the [[Cayley–Hamilton theorem]],\n:<math> \\operatorname{adj}(A) =(-)^{n-1} M_{n}=(-)^{n-1} (A^{n-1}+c_{n-1}A^{n-2}+ ...+c_2 A+ c_1 I)=(-)^{n-1}   \\sum_{k=1}^n  c_k A^{k-1}~.</math>\n\n\n\nThe final solution might be more conveniently expressed in terms of complete exponential [[Bell polynomials]] as \n:<math> c_{n-k} = \\frac{(-1)^{n-k}}{k!} {\\mathcal B}_k \\Bigl ( \\operatorname{tr}A , -1! ~  \\operatorname{tr}A^2, 2! ~\\operatorname{tr}A^3, \\ldots, (-1)^{k-1}(k-1)! ~ \\operatorname{tr}A^k\\Bigr ) .</math>\n\n==Example==\n:<math>{\\displaystyle A=\\left[{\\begin{array}{rrr}3&1&5\\\\3&3&1\\\\4&6&4\\end{array}}\\right]}</math>\n<math>{\\displaystyle {\\begin{aligned}M_{0}&=\\left[{\\begin{array}{rrr}0&0&0\\\\0&0&0\\\\0&0&0\\end{array}}\\right]\\quad &&&c_{3}&&&&&=&1\\\\M_{\\mathbf {\\color {blue}1} }&=\\left[{\\begin{array}{rrr}1&0&0\\\\0&1&0\\\\0&0&1\\end{array}}\\right] &A~M_{1}&=\\left[{\\begin{array}{rrr}\\mathbf {\\color {red}3} &1&5\\\\3&\\mathbf {\\color {red}3} &1\\\\4&6&\\mathbf {\\color {red}4} \\end{array}}\\right]  &c_{2}&&&=-{\\frac {1}{\\mathbf {\\color {blue}1} }} \\mathbf {\\color {red}10} &&=&-10\\\\M_{\\mathbf {\\color {blue}2} }&=\\left[{\\begin{array}{rrr}-7&1&5\\\\3&-7&1\\\\4&6&-6\\end{array}}\\right]\\qquad &A~M_{2}&=\\left[{\\begin{array}{rrr}\\mathbf {\\color {red}2} &26&-14\\\\-8&\\mathbf {\\color {red}-12} &12\\\\6&-14&\\mathbf {\\color {red}2} \\end{array}}\\right]\\qquad &c_{1}&&&=-{\\frac {1}{\\mathbf {\\color {blue}2} }} \\mathbf {\\color {red}(-8)} &&=&4\\\\M_{\\mathbf {\\color {blue}3} }&=\\left[{\\begin{array}{rrr}6&26&-14\\\\-8&-8&12\\\\6&-14&6\\end{array}}\\right]\\qquad &A~M_{3}&=\\left[{\\begin{array}{rrr}\\mathbf {\\color {red}40} &0&0\\\\0&\\mathbf {\\color {red}40} &0\\\\0&0&\\mathbf {\\color {red}40} \\end{array}}\\right]\\qquad &c_{0}&&&=-{\\frac {1}{\\mathbf {\\color {blue}3} }}\\mathbf {\\color {red}120} &&=&-40\\end{aligned}}}  </math>\n\nFurthermore,  <math>{\\displaystyle M_{4}=A~M_{3}+c_{0}~I=0}</math>, which  confirms the  above calculations.\n\nThe characteristic polynomial of matrix {{mvar|A}} is thus <math>{\\displaystyle p_{A}(\\lambda )=\\lambda ^{3}-10\\lambda ^{2}+4\\lambda -40}</math>; the determinant of {{mvar|A}} is <math>{\\displaystyle \\det(A)=(-1)^{3} c_{0}=40}</math>; the trace is 10=−''c''<sub>2</sub>; and the inverse of {{mvar|A}} is \n:<math>{\\displaystyle A^{-1}=-{\\frac {1}{c_{0}}}~ M_{3}={\\frac {1}{40}} \\left[{\\begin{array}{rrr}6&26&-14\\\\-8&-8&12\\\\6&-14&6\\end{array}}\\right]=\\left[{\\begin{array}{rrr}0{.}15&0{.}65&-0{.}35\\\\-0{.}20&-0{.}20&0{.}30\\\\0{.}15&-0{.}35&0{.}15\\end{array}}\\right]} </math>.\n\n==An equivalent but distinct expression==\nA compact determinant of an {{mvar|m}}×{{mvar|m}}-matrix solution for the above Jacobi's formula may alternatively determine the coefficients {{mvar|c}},<ref>Brown, Lowell S. (1994). ''Quantum Field Theory'', Cambridge University Press. {{ISBN|978-0-521-46946-3}}, p. 54; Also see,  Curtright, T. L. and Fairlie, D. B. (2012). \"A Galileon Primer\", arXiv:1212.6972 , section 3.</ref>\n:<math>c_{n-m} = \\frac{(-1)^m}{m!}\n\\begin{vmatrix}  \\operatorname{tr}A  &   m-1 &0&\\cdots\\\\\n\\operatorname{tr}A^2  &\\operatorname{tr}A&   m-2 &\\cdots\\\\\n \\vdots & \\vdots & & & \\vdots    \\\\\n\\operatorname{tr}A^{m-1} &\\operatorname{tr}A^{m-2}& \\cdots & \\cdots & 1    \\\\\n\\operatorname{tr}A^m  &\\operatorname{tr}A^{m-1}& \\cdots & \\cdots & \\operatorname{tr}A \\end{vmatrix} ~.</math>\n\n\n{{see also|Exterior algebra#Leverrier's Algorithm}}\n==References==\n{{reflist}}\n\n{{DEFAULTSORT:Faddeev-LeVerrier algorithm}}\n[[Category:Polynomials]]\n[[Category:Matrix theory]]\n[[Category:Linear algebra]]\n[[Category:Mathematical physics]]\n[[Category:Determinants]]\n[[Category:Homogeneous polynomials]]"
    },
    {
      "title": "FGLM algorithm",
      "url": "https://en.wikipedia.org/wiki/FGLM_algorithm",
      "text": "'''FGLM''' is one of the main [[algorithm]]s in [[Symbolic computation|computer algebra]], named after its designers, [[Jean-Charles Faugère|Faugère]], Gianni, [[Daniel Lazard|Lazard]] and [[Teo Mora|Mora]]. They introduced their algorithm in 1993. The input of the algorithm is a [[Gröbner basis]] of a zero-dimensional [[Ideal (ring theory)|ideal]] in the ring of [[polynomial]]s over a [[Field (mathematics)|field]] with respect to a [[monomial order]] and a second [[monomial order]]; As its output, it returns a Gröbner basis of the ideal with respect to the second ordering. The algorithm is a fundamental tool in computer algebra and has been implemented in most of the [[computer algebra system]]s. The [[Complexity of algorithms|complexity]] of FGLM is ''O''(''nD''<sup>3</sup>), where ''n'' is the number of variables of the polynomials and D is the degree of the ideal.<ref>{{Cite journal|author1=J.C. Faugère |author2=P. Gianni |author3=D. Lazard |author4=T. Mora |year=1993|title=Efficient Computation of Zero-dimensional Gröbner Bases by Change of Ordering|url=|journal=Journal of Symbolic Computation|volume=16|issue=4|pages=329–344|via=|doi=10.1006/jsco.1993.1051}}</ref> There are several generalization and various application for FGLM.<ref>{{Cite journal|last=Middeke|first=Johannes|date=2012-01-01|title=A Computational View on Normal Forms of Matrices of Ore Polynomials|journal=ACM Commun. Comput. Algebra|volume=45|issue=3/4|pages=190–191|doi=10.1145/2110170.2110182|issn=1932-2240}}</ref><ref>{{Cite journal|last=Gerdt|first=V. P.|last2=Yanovich|first2=D. A.|date=2003-03-01|title=Implementation of the FGLM Algorithm and Finding Roots of Polynomial Involutive Systems|journal=Programming and Computer Software|language=en|volume=29|issue=2|pages=72–74|doi=10.1023/A:1022992514981|issn=0361-7688}}</ref><ref>{{Cite journal|last=Faugère|first=Jean-Charles|last2=Mou|first2=Chenqi|date=2017-05-01|title=Sparse FGLM algorithms|url=http://www.sciencedirect.com/science/article/pii/S0747717116300700|journal=Journal of Symbolic Computation|volume=80, Part 3|pages=538–569|doi=10.1016/j.jsc.2016.07.025|arxiv=1304.1238}}</ref><ref>{{Cite book|last=Licciardi|first=Sandra|last2=Mora|first2=Teo|date=1994-01-01|title=Implicitization of Hypersurfaces and Curves by the Primbasissatz and Basis Conversion|journal=Proceedings of the International Symposium on Symbolic and Algebraic Computation|series=ISSAC '94|location=New York, NY, USA|publisher=ACM|pages=191–196|doi=10.1145/190347.190416|isbn=978-0897916387}}</ref><ref>{{Cite book|last=Borges-Quintana|first=M.|last2=Borges-Trenard|first2=M. A.|last3=Martínez-Moro|first3=E.|date=2006-02-20|title=A General Framework for Applying FGLM Techniques to Linear Codes|journal=Applied Algebra, Algebraic Algorithms and Error-Correcting Codes|volume=3857|language=en|pages=76–86|doi=10.1007/11617983_7|arxiv=math/0509186|series=Lecture Notes in Computer Science|isbn=978-3-540-31423-3}}</ref>\n\n== References ==\n{{Reflist}}\n\n[[Category:Computer algebra]]\n[[Category:Commutative algebra]]\n[[Category:Polynomials]]\n[[Category:Algorithms]]\n\n{{algebra-stub}}"
    },
    {
      "title": "Gauss's lemma (polynomial)",
      "url": "https://en.wikipedia.org/wiki/Gauss%27s_lemma_%28polynomial%29",
      "text": "{{About|Gauss's lemma for polynomials||Gauss's lemma (disambiguation)}}\nIn [[algebra]], '''Gauss's lemma''',<ref>Article 42 of [[Carl Friedrich Gauss]]'s ''[[Disquisitiones Arithmeticae]]'' (1801)</ref> named after [[Carl Friedrich Gauss]], is a statement about [[polynomial]]s over the [[integer]]s, or, more generally, over a [[unique factorization domain]] (that is, a [[ring (mathematics)|ring]] that has a unique  factorization property similar to the [[fundamental theorem of arithmetic]]). Gauss's lemma underlies all the theory of [[factorization of polynomials|factorization]] and [[polynomial greatest common divisor|greatest common divisors]] of such polynomials.\n\nGauss's lemma asserts that the product of two [[primitive part and content|primitive polynomial]]s is primitive (a polynomial with integer coefficients is ''primitive'' if it has 1 as a greatest common divisor of its coefficients).\n\nA corollary of Gauss's lemma, sometimes also called ''Gauss's lemma'', is that a primitive polynomial is [[irreducible polynomial|irreducible]] over the integers if and only if it is irreducible over the [[rational number]]s. More generally, a primitive polynomial has the same complete factorization over the integers and over the rational numbers. In the case of coefficients in a unique factorization domain {{mvar|R}}, \"rational numbers\" must be replaced by \"[[field of fractions]] of {{mvar|R}}\". This implies that, if {{mvar|R}} is either a [[field (mathematics)|field]], the ring of integers, or a unique factorization domain, then every [[polynomial ring]] (in one or several indeterminates) over {{mvar|R}} is a unique factorization domain. Another consequence is that factorization and greatest common divisor computation of polynomials with integers or rational coefficients may be reduced to similar computations on integers and primitive polynomials. This is systematically used (explicitly or implicitly) in all implemented algorithms (see [[Polynomial greatest common divisor]] and [[Factorization of polynomials]]).\n\nGauss's lemma, and all its consequences that do not involve the existence of a complete factorization remain true over any [[GCD domain]] (an [[integral domain]]s over which greatest common divisors exist). In particular, a polynomial ring over a GCD domain is also a GCD domain. If one calls ''primitive'' a polynomial such that the coefficients  generate the [[unit ideal]], Gauss's lemma is true over every [[commutative ring]].<ref name=\"Atiyah primitive\">{{harvnb|Atiyah|MacDonald|loc=Ch. 1., Exercise 2. (iv) and Exercise 3.}}</ref> However, some care must be taken, when using this definition of ''primitive'', as, over a unique factorization domain that is not a [[principal ideal ring]], there are polynomials that are primitive in the above sense and not primitive in this new sense.\n\n== The lemma over the integers ==\nIf <math>F(X) = a_0 + a_1 X + \\dots + a_n X^n</math> is a polynomial with integer coefficients, then <math>F</math> is called ''[[primitive part and content|primitive]]'' if the greatest common divisor of all the coefficients <math>a_0, a_1, \\dots, a_n</math> is 1; in other words, no [[prime number]] divides all the coefficients.\n\n{{math_theorem|name=Gauss's lemma (primitivity)|math_statement=If ''P'' and ''Q'' are primitive polynomials over the integers, then product ''PQ'' is also primitive.}}\n\n'''Proof:'''  Clearly the product ''f''(''x'').''g''(''x'') of two primitive polynomials has integer coefficients.  Therefore, if it is not primitive, there must be a prime ''p'' which is a common divisor of all its coefficients. But ''p'' can not divide all the coefficients of either ''f''(''x'') or ''g''(''x'') (otherwise they would not be primitive).  Let ''a<sub>r</sub>x<sup>r</sup>'' be the first coefficient of ''f''(''x'') not divisible by ''p'' and let ''b<sub>s</sub>x<sup>s</sup>'' be the first coefficient of ''g''(''x'') not divisible by ''p''.  Now consider the term ''x<sup>r&nbsp;+&nbsp;s</sup>'' in the product.  Its coefficient must take the form:\n\n:<math>a_r b_s + a_{r+1} b_{s-1} + a_{r+2} b_{s-2} + \\cdots + a_{r-1} b_{s+1} + a_{r-2} b_{s+2} + \\cdots\\,</math>\n\nThe first term is not divisible by ''p'' (because ''p'' is prime), yet all the remaining ones are, so the entire sum cannot be divisible by ''p''.  By assumption all coefficients in the product are divisible by ''p'', leading to a contradiction. Therefore, the coefficients of the product can have no common divisor and are thus primitive. <math>\\square</math>\n\n{{math_theorem|name=Gauss's lemma (irreducibility)|math_statement=A non-constant polynomial in '''Z'''[''X''] is irreducible in '''Z'''[''X''] if and only if it is both irreducible in '''Q'''[''X''] and primitive in '''Z'''[''X''].}}\n\nThe proof is given below for the more general case. Note that an irreducible element of '''Z''' (a prime number) is still irreducible when viewed as constant polynomial in '''Z'''[''X'']; this explains the need for \"non-constant\" in the statement.\n\n== Statements for unique factorization domains ==\n{{main|Primitive part and content}}\n\nGauss's lemma holds more generally over arbitrary [[unique factorization domain]]s. There the ''[[content (algebra)|content]]'' {{math|''c''(''P'')}} of a polynomial {{math|''P''}} can be defined as the [[greatest common divisor]] of the coefficients of {{math|''P''}} (like the gcd, the content is actually a class of [[associate elements]]). A polynomial {{math|''P''}} with coefficients in a UFD is then said to be '''primitive''' if the only elements of ''R'' that divide all coefficients of ''P'' at once are the invertible elements of ''R''; i.e., the gcd of the coefficients is one.\n\n'''Primitivity statement:''' If ''R'' is a UFD, then the set of primitive polynomials in {{math|''R''[''X'']}} is closed under multiplication. More generally, the content of a product <math>fg</math> of polynomials is the product <math>c(f)c(g)</math> of their contents.\n\n'''Irreducibility statement:''' Let ''R'' be a unique factorization domain and ''F'' its [[field of fractions]]. A non-constant polynomial <math>f</math> in <math>R[x]</math> is irreducible in <math>R[x]</math> if and only if it is both irreducible in <math>F[x]</math> and primitive in <math>R[x]</math>.\n\n(For the proofs, see [[#General version]] below.)\n\nLet <math>R</math> be a unique factorization domain with field of fractions <math>F</math>. If <math>f \\in F[x]</math> is a polynomial over <math>F</math>, then, for some <math>d \\in R</math>, <math>df</math> has coefficients in <math>R</math> and so, factoring-out the gcd <math>q</math> of the coefficients, we can write: <math>df = qf'</math> for some primitive polynomial <math>f' \\in R[x]</math>. As one can check, this polynomial <math>f'</math> is unique up to the multiplication by a unit element and is called the '''[[primitive part]]''' (or '''primitive representative''') of <math>f</math> and is denoted by <math>\\operatorname{pp}(f)</math>. The procedure is compatible with product: <math>\\operatorname{pp}(fg) = \\operatorname{pp}(f)\\operatorname{pp}(g)</math>.\n\nThe construct can be used to show the statement:\n*A polynomial ring over a UFD is a UFD.\nIndeed, by induction, it is enough to show <math>R[x]</math> is a UFD when <math>R</math> is a UFD. Let <math>f \\in R[x]</math> be a nonzero polynomial. Now, <math>F[x]</math> is a unique factorization domain (since it is a principal ideal domain) and so, as a polynomial in <math>F[x]</math>, <math>f</math> can be factorized as:\n:<math>f = g_1 g_2 \\dots g_r</math>\nwhere <math>g_i</math> are irreducible polynomials of <math>F[x]</math>. Now, we write <math>f = c f'</math> for the gcd <math>c</math> of the coefficients of <math>f</math> (and <math>f'</math> is the primitive part) and then:\n:<math>f = c f' = c \\operatorname{pp}(g_1) \\operatorname{pp}(g_2) \\cdots \\operatorname{pp}(g_r).</math>\nNow, <math>c</math> is a product of prime elements of <math>R</math> (since <math>R</math> is a UFD) and a prime element of <math>R</math> is a prime element of <math>R[x]</math>, as <math>R[x]/(p) \\simeq R/(p)[x]</math> is an integral domain. Hence, <math>c</math> admits a prime factorization (or a unique factorization into irreducibles). Next, observe that <math>f' = \\operatorname{pp}(g_1) \\cdots \\operatorname{pp}(g_r)</math> is a unique factorization into irreducible elements of <math>R[x]</math>, as (1) each <math>\\operatorname{pp}(g_i)</math> is irreducible by the irreducibility statement and (2) it is unique since the factorization of <math>f'</math> can also be viewed as a factorization in <math>F[x]</math> and factorization there is unique. Since <math>c, f'</math> are uniquely determined by <math>f</math>, up to unit elements, the above factorization of <math>f</math> is a unique factorization into irreducible elements. <math>\\square</math>\n\nThe condition that \"''R'' is a unique factorization domain\" is not superfluous because it implies that every irreducible element of this ring is also a [[prime element]], which in turn implies that every nonzero element of ''R'' has at most one factorization into a product of irreducible elements and a unit up to order and associate relationship. In a ring where factorization is not unique, say {{math|''pa'' {{=}} ''qb''}} with ''p'' and ''q'' irreducible elements that do not divide any of the factors on the other side, the product {{math|(''p'' + ''qX'')(''a'' + ''qX'') {{=}} ''pa'' + (''p''+''a'')''qX'' + ''q<sup>2</sup>X<sup>2</sup>'' {{=}} ''q''(''b'' + (''p''+''a'')''X'' + ''qX<sup>2</sup>'')}} shows the failure of the primitivity statement. For a concrete example one can take {{math|''R'' {{=}} '''Z'''[''i''√''5'']}}, {{math|''p'' {{=}} ''1'' + ''i''√''5''}}, {{math|''a'' {{=}} ''1'' - ''i''√''5''}}, {{math|''q'' {{=}} ''2''}}, {{math|''b'' {{=}} ''3''}}. In this example the polynomial {{math|''3'' + ''2X'' + ''2X<sup>2</sup>''}} (obtained by dividing the right hand side by {{math|''q'' {{=}} ''2''}}) provides an example of the failure of the irreducibility statement (it is irreducible over ''R'', but reducible over its field of fractions {{math|'''Q'''[''i''√''5'']}}). Another well known example is the polynomial {{math|''X<sup>2</sup>'' − ''X'' − ''1''}}, whose roots are the [[golden ratio]] {{math|φ {{=}} (''1'' + √''5'')/''2''}} and its conjugate {{math|(''1'' − √''5'')/''2''}} showing that it is reducible over the field {{math|'''Q'''[√''5'']}}, although it is irreducible over the non-UFD {{math|'''Z'''[√''5'']}} which has {{math|'''Q'''[√''5'']}} as field of fractions. In  the latter example the ring can be made into an UFD by taking its [[integral closure]] {{math|'''Z'''[φ]}} in {{math|'''Q'''[√''5'']}} (the ring of [[Dirichlet integers]]), over which {{math|''X<sup>2</sup>'' − ''X'' − ''1''}} becomes reducible, but in the former example ''R'' is already integrally closed.\n\n== General version ==\nLet <math>R</math> be a commutative ring. If <math>f</math> is a polynomial in <math>R[x_1, \\dots, x_n]</math>, then we write <math>\\operatorname{cont}(f)</math> for the ideal of <math>R</math> generated by all the coefficients of <math>f</math>; it is called the content of <math>f</math>. Note that <math>\\operatorname{cont}(af) = a\\operatorname{cont}(f)</math> for each <math>a \\in R</math>. The next proposition states a more substantial property.\n\n{{math_theorem|name=Proposition<ref>{{harvnb|Eisenbud|loc=Exercise 3.4. (a)}}</ref> |math_statement=For each pair of polynomials <math>f, g</math> in <math>R[x_1, \\dots, x_n]</math>,\n:<math>\\operatorname{cont}(fg) \\subset \\operatorname{cont}(f)\\operatorname{cont}(g) \\subset \\sqrt{\\operatorname{cont}(fg)}</math>\nwhere <math>\\sqrt{\\cdot}</math> denotes the [[radical of an ideal]]. Moreover, if <math>R</math> is a GCD domain (e.g., a unique factorization domain), then\n:<math>\\operatorname{gcd}(\\operatorname{cont}(fg)) = \\operatorname{gcd}(\\operatorname{cont}(f)) \\operatorname{gcd}(\\operatorname{cont}(g))</math>\nwhere <math>\\operatorname{gcd}(I)</math> denotes the unique minimal principal ideal containing a finitely generated ideal <math>I</math>.<ref group=note>A generator of the principal ideal is a gcd of some generators of ''I'' (and it exists because <math>R</math> is a GCD domain).</ref>}}\n\nA polynomial <math>f</math> is said to be ''primitive'' if <math>\\operatorname{cont}(f) = (1)</math> is the unit ideal.<ref>{{harvnb|Atiyah|MacDonald|loc=Ch. 1., Exercise 2. (iv}})</ref> When <math>R = \\mathbb{Z}</math> (or more generally a [[Bézout domain]]), this agrees with the usual definition of a primitive polynomial. (But if <math>R</math> is only a UFD, this definition is inconsistent with the definition of primitivity in [[#Statements for unique factorization domains]].)\n\n{{math_theorem|name=Corollary<ref name=\"Atiyah primitive\" />|math_statement=Two polynomials <math>f, g</math> are primitive if and only if the product <math>fg</math> is primitive.}}\n\n''Proof:'' This is easy using the fact<ref>{{harvnb|Atiyah|MacDonald|loc=Ch. 1., Exercise 1.13.}}</ref> that <math>\\sqrt{I} = (1)</math> implies <math>I = (1).</math> <math>\\square</math>\n\n{{math_theorem|name=Corollary<ref>{{harvnb|Eisenbud|loc=Exercise 3.4.c|ps=; The case when ''R'' is a UFD.}}</ref>|Suppose <math>R</math> is a GCD domain (e.g., a unique factorization domain) with the field of fractions <math>F</math>. Then a non-constant polynomial <math>f</math> in <math>R[x]</math> is irreducible if and only if it is irreducible in <math>F[x]</math> and the gcd of the coefficients of <math>f</math> is 1.}}\n\n''Proof:'' (<math>\\Rightarrow</math>) First note that the gcd of the coefficients of <math>f</math> is 1 since, otherwise, we can factor out some element <math>c \\in R</math> from the coefficients of <math>f</math> to write <math>f = cf'</math>, contradicting the irreducibility of <math>f</math>. Next, suppose <math>f = gh</math> for some non-constant polynomials <math>g, h</math> in <math>F[x_1, \\dots, x_n]</math>. Then, for some <math>d \\in R</math>, the polynomial <math>dg</math> has coefficients in <math>R</math> and so, by factoring out the gcd <math>q</math> of the coefficients, we write <math>dg = qg'</math>. Do the same for <math>h</math> and we can write <math>f = c g'h'</math> for some <math>c \\in F</math>. Now, let <math>c = a/b</math> for some <math>a, b \\in R</math>. Then <math>bf = a g'h'</math>. From this, using the proposition, we get:\n:<math>(b) \\supset \\operatorname{gcd}(\\operatorname{cont}(bf)) = (a)</math>.\nThat is, <math>b</math> divides <math>a</math>. Thus, <math>c \\in R</math> and then the factorization <math>f = c g'h'</math> constitutes a contradiction to the irreducibility of <math>f</math>.\n\n(<math>\\Leftarrow</math>) If <math>f</math> is irreducible over <math>F</math>, then either it is irreducible over <math>R</math> or it contains a constant polynomial as a factor, the second possibility is ruled out by the assumption. <math>\\square</math>\n\n''Proof of the proposition:'' Clearly, <math>\\operatorname{cont}(fg) \\subset \\operatorname{cont}(f) \\operatorname{cont}(g)</math>. If <math>\\mathfrak{p}</math> is a prime ideal containing <math>\\operatorname{cont}(fg)</math>, then <math>fg \\equiv 0</math> modulo <math>\\mathfrak{p}</math>. Since <math>R/\\mathfrak{p}[x_1, \\dots, x_n]</math> is a polynomial ring over an integral domain and thus is an integral domain, this implies either <math>f \\equiv 0</math> or <math>g \\equiv 0</math> modulo <math>\\mathfrak{p}</math>. Hence, either <math>\\operatorname{cont}(f)</math> or <math>\\operatorname{cont}(g)</math> is contained  in <math>\\mathfrak{p}</math>. Since <math>\\sqrt{\\operatorname{cont}(fg)}</math> is the intersection of all prime ideals that contain <math>\\operatorname{cont}(fg)</math> and the choice of <math>\\mathfrak p</math> was arbitrary, <math>\\operatorname{cont}(f) \\operatorname{cont}(g) \\subset \\sqrt{\\operatorname{cont}(fg)}</math>.\n\nWe now prove the \"moreover\" part. Factoring out the gcd’s from the coefficients, we can write <math>f = a f'</math> and <math>g = b g'</math> where the gcds of the coefficients of <math>f', g'</math> are both 1. Clearly, it is enough to prove the assertion when <math>f, g</math> are replaced by <math>f', g'</math>; thus, we assume the gcd's of the coefficients of <math>f, g</math> are both 1. The rest of the proof is easy and transparent if <math>R</math> is a unique factorization domain; thus we give the proof in that case here (and see <ref group=note>''Proof for the GCD case'': The proof here is adopted from {{cite book |first=R. |last=Mines |first2=F. |last2=Richman |first3=W. |last3=Ruitenburg |title=A Course in Constructive Algebra |series=Universitext |publisher=Springer-Verlag |year=1988 |isbn=0-387-96640-4 }} We need the following simple lemma about gcd:\n* If <math>\\gcd(a, b) = \\gcd(a, c) = 1</math>, then <math>\\gcd(a, bc) = 1</math>.\n(The proof of the lemma is not trivial but is by elementary algebra.)\n\nWe argue by induction on the sum of the numbers of the terms in <math>f, g</math>; that is, we assume the proposition has been established for any pair of polynomials with one less total number of the terms. Let <math>(c) = \\gcd(\\operatorname{cont}(fg))</math>; i.e., <math>c</math> is the gcd of the coefficients of <math>fg</math>. Assume <math>(c) \\ne (1)</math>; otherwise, we are done. Let <math>f_0, g_0</math> denote the highest-degree terms of <math>f, g</math> in terms of [[monomial order#Lexicographic order|lexicographical monomial ordering]]. Then <math>f_0g_0</math> is precisely the leading term of <math>fg</math> and so <math>c</math> divides the (unique) coefficient of <math>f_0g_0</math> (since it divides all the coefficients of <math>fg</math>). Now, if <math>c</math> does not have a common factor with the (unique) coefficient of <math>f_0</math> and does not have a common factor with that of <math>g_0</math>, then, by the above lemma, <math>\\gcd(c, \\operatorname{cont}(f_0g_0)) = (1)</math>. But <math>c</math> divides the coefficient of <math>f_0g_0</math>; so this is a contradiction. Thus, either <math>c</math> has a common factor with the coefficient of <math>f_0</math> or does with that of <math>g_0</math>; say, the former is the case. Let <math>(d) = \\operatorname{gcd}(c, \\operatorname{cont}(f_0))</math>. Since <math>d</math> divides the coefficients of <math>fg - f_0g = (f - f_0)g</math>, by inductive hypothesis,\n:<math>(d) \\supset \\operatorname{gcd}(\\operatorname{cont}((f - f_0)g)) = \\operatorname{gcd}(\\operatorname{cont}(f - f_0)) \\operatorname{gcd}(\\operatorname{cont}(g)) = \\operatorname{gcd}(\\operatorname{cont}(f - f_0))</math>.\nSince <math>(d)</math> contains <math>\\operatorname{cont}(f_0)</math>, it contains <math>\\operatorname{cont}(f)</math>; i.e., <math>(d) = (1)</math>, a contradiction. <math>\\square</math></ref> for the proof for the GCD case). If <math>\\gcd(\\operatorname{cont}(fg)) = (1)</math>, then there is nothing to prove. So, assume otherwise; then there is a non-unit element dividing the coefficients of <math>fg</math>. Factorizing that element into a product of prime elements, we can take that element to be a prime element <math>\\pi</math>. Now, we have:\n:<math>(\\pi) = \\sqrt{(\\pi)} \\supset \\sqrt{\\operatorname{cont}(fg)} \\supset \\operatorname{cont}(f) \\operatorname{cont}(g)</math>.\nThus, either <math>(\\pi)</math> contains <math>\\operatorname{cont}(f)</math> or <math>\\operatorname{cont}(g)</math>; contradicting the gcd's of the coefficients of <math>f, g</math> are both 1. <math>\\square</math>\n\n*'''Remark''': Over a GCD domain (e.g., a unique factorization domain), the gcd of all the coefficients of a polynomial <math>f</math>, unique up to unit elements, is also called the content of <math>f</math>.\n\n== Applications ==\nIt follows from Gauss's lemma that for each [[unique factorization domain]] <math>R</math>, the polynomial ring <math>R[X_1, X_2,. .., X_n]</math> is also a unique factorization domain (see [[#Statements for unique factorization domains]]). Gauss's lemma can also be used to show [[Eisenstein's irreducibility criterion]]. Finally, it can be used to show that [[cyclotomic polynomials]] (unitary units with integer coefficients) are irreducible.\n\nGauss's lemma implies the following statement:\n*If <math>f(x)</math> is a monic polynomial in one variable with coefficients in a unique factorization domain <math>R</math> (or more generally a GCD domain), then a root of <math>f</math> that is in the field of fractions <math>F</math> of <math>R</math> is in <math>R</math>.<ref group=note>In other words, it says that a unique factorization domain is [[integrally closed domain|integrally closed]].</ref>\nIf <math>R = \\mathbb{Z}</math>, then it says a rational root of a monic polynomial over integers is an integer (cf. the [[rational root theorem]]). To see the statement, let <math>a/b</math> be a root of <math>f</math> in <math>F</math> and assume <math>a, b</math> are relatively prime. In <math>F[x]</math>, we can write <math>f = (x - a/b)g</math> with <math>cg \\in R[x]</math> for some <math>c \\in R</math>. Then\n:<math>cb f = (bx - a)cg</math>,\nis a factorization in <math>R[x]</math>. But <math>bx - a</math> is primitive (in the UFD sense) and thus <math>cb</math> divides the coefficients of <math>cg</math> by Gauss's lemma and so\n:<math>f = (bx - a)h</math>,\nwith <math>h</math> in <math>R[x]</math>. Since <math>f</math> is monic, this is possible only when <math>b</math> is a unit.\n\nA similar argument shows:\n*Let <math>R</math> be a GCD domain with the field of fractions <math>F</math> and <math>f \\in R[x]</math>. If <math>f = gh</math> for some polynomial <math>g \\in R[x]</math> that is primitive in the UFD sense and <math>h \\in F[x]</math>, then <math>h \\in R[x]</math>.\n\nThe irreducibility statement also implies that the [[Minimal polynomial (field theory)|minimal polynomial]] over the rational numbers of an [[algebraic integer]] has integer coefficients.<!-- true but should be expanded. -->\n\n== Notes and references ==\n{{reflist|group=note}}\n{{reflist}}\n\n*{{Citation | last1=Atiyah | first1=Michael Francis | author1-link=Michael Atiyah | last2=Macdonald | first2=I.G. | author2-link=Ian G. Macdonald | title=Introduction to Commutative Algebra | publisher=Westview Press | isbn=978-0-201-40751-8 | year=1969}}\n*{{Citation | last1=Eisenbud | first1=David | author1-link=David Eisenbud | title=Commutative algebra | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Graduate Texts in Mathematics | isbn=978-0-387-94268-1| id={{ISBN|978-0-387-94269-8}} | mr=1322960  | year=1995 | volume=150 | doi=10.1007/978-1-4612-5350-1}}\n\n{{DEFAULTSORT:Gauss's Lemma (Polynomial)}}\n[[Category:Polynomials]]\n[[Category:Lemmas]]"
    },
    {
      "title": "Generalized Appell polynomials",
      "url": "https://en.wikipedia.org/wiki/Generalized_Appell_polynomials",
      "text": "In [[mathematics]], a [[polynomial sequence]] <math>\\{p_n(z) \\}</math> has a '''generalized Appell representation''' if the [[generating function]] for the [[polynomial]]s takes on a certain form:\n\n:<math>K(z,w) = A(w)\\Psi(zg(w)) = \\sum_{n=0}^\\infty p_n(z) w^n\n</math>\nwhere the generating function or [[kernel (category theory)|kernel]] <math>K(z,w)</math> is composed of the series \n\n:<math>A(w)= \\sum_{n=0}^\\infty a_n w^n \\quad</math> with <math>a_0 \\ne 0 </math>\n\nand \n:<math>\\Psi(t)= \\sum_{n=0}^\\infty \\Psi_n t^n \\quad</math> and all <math>\\Psi_n \\ne 0 </math>\n\nand\n:<math>g(w)= \\sum_{n=1}^\\infty g_n w^n \\quad</math> with <math>g_1 \\ne 0.</math>\n\nGiven the above, it is not hard to show that <math>p_n(z)</math> is a [[Degree of a polynomial|polynomial of degree]] <math>n</math>.\n\n[[Boas–Buck polynomials]] are a slightly more general class of polynomials.\n\n==Special cases==\n* The choice of <math>g(w)=w</math> gives the class of [[Brenke polynomials]].\n* The choice of <math>\\Psi(t)=e^t</math> results in the [[Sheffer sequence]] of polynomials, which include the [[general difference polynomials]], such as the [[Newton polynomials]].\n* The combined choice of <math>g(w)=w</math> and <math>\\Psi(t)=e^t</math> gives the [[Appell sequence]] of polynomials.\n\n==Explicit representation==\nThe generalized Appell polynomials have the explicit representation\n\n:<math>p_n(z) = \\sum_{k=0}^n z^k \\Psi_k h_k.</math>\n\nThe constant is\n\n:<math>h_k=\\sum_{P} a_{j_0} g_{j_1} g_{j_2} \\cdots g_{j_k} </math>\n\nwhere this sum extends over all [[Composition (combinatorics)|compositions]] of <math>n</math> into <math>k+1</math> parts; that is, the sum extends over all <math>\\{j\\}</math> such that\n\n:<math>j_0+j_1+ \\cdots +j_k = n.\\,</math>\n\nFor the Appell polynomials, this becomes the formula \n\n:<math>p_n(z) = \\sum_{k=0}^n \\frac {a_{n-k} z^k} {k!}.</math>\n\n==Recursion relation==\nEquivalently, a necessary and sufficient condition that the kernel <math>K(z,w)</math> can be written as <math>A(w)\\Psi(zg(w))</math> with <math>g_1=1</math> is that\n\n:<math>\\frac{\\partial K(z,w)}{\\partial w} = \nc(w) K(z,w)+\\frac{zb(w)}{w} \\frac{\\partial K(z,w)}{\\partial z}</math>\n\nwhere <math>b(w)</math> and <math>c(w)</math> have the power series\n\n:<math>b(w) = \\frac{w}{g(w)} \\frac {d}{dw} g(w)\n= 1 + \\sum_{n=1}^\\infty b_n w^n</math>\n\nand \n\n:<math>c(w) = \\frac{1}{A(w)} \\frac {d}{dw} A(w)\n= \\sum_{n=0}^\\infty c_n w^n.</math>\n\nSubstituting \n\n:<math>K(z,w)= \\sum_{n=0}^\\infty p_n(z) w^n</math>\n\nimmediately gives the [[recursion relation]]\n\n:<math> z^{n+1} \\frac {d}{dz} \\left[ \\frac{p_n(z)}{z^n} \\right]= \n-\\sum_{k=0}^{n-1} c_{n-k-1} p_k(z) \n-z \\sum_{k=1}^{n-1} b_{n-k} \\frac{d}{dz} p_k(z).\n</math>\n\nFor the special case of the Brenke polynomials, one has <math>g(w)=w</math> and thus all of the <math>b_n=0</math>, simplifying the recursion relation significantly.\n\n==See also==\n{{portal|Mathematics}}\n* [[q-difference polynomial]]s\n\n==References==\n* Ralph P. Boas, Jr. and R. Creighton Buck, ''Polynomial Expansions of Analytic Functions (Second Printing Corrected)'', (1964) Academic Press Inc., Publishers New York, Springer-Verlag, Berlin. Library of Congress Card Number 63-23263.\n* {{cite journal|first1=William C.|last1= Brenke|title=On generating functions of polynomial systems|year= 1945|journal=American Mathematical Monthly|volume = 52|number=6|pages=297–301|doi=10.2307/2305289}}\n* {{cite journal|first1=W. N.|last1= Huff|title=The type of the polynomials generated by f(xt) φ(t)|year=1947|journal=Duke Mathematical Journal|volume=14|number=4|pages=1091–1104|doi=10.1215/S0012-7094-47-01483-X}}\n\n[[Category:Polynomials]]"
    },
    {
      "title": "Geometrical properties of polynomial roots",
      "url": "https://en.wikipedia.org/wiki/Geometrical_properties_of_polynomial_roots",
      "text": "{{short description|Geometry of the location of polynomial roots}}\n{{for|the computation of polynomial roots|Real-root isolation|Root-finding algorithm#Roots of polynomials}}\nIn [[mathematics]], a [[univariate polynomial]] of degree {{mvar|n}} with real or complex coefficients has {{mvar|n}} complex '''[[zero of a function|roots]]''', if counted with their [[multiplicities]]. They form a set of {{mvar|n}} points in the [[complex plane]]. This article concerns the '''geometry''' of these points, that is the information about their localization in the complex plane that can be deduced from the degree and the coefficients of the polynomial.\n\nSome of these geometrical properties are related to a single polynomial, such as upper bounds on the absolute values of the roots, which define a disk containing all roots, or lower bounds on the distance between two roots. Such bounds are widely used for [[root-finding algorithm]]s for polynomials, either for tuning them, or for computing their [[computational complexity]]\n\nSome other properties are probabilistic, such as the expected number of real roots of a random polynomial of degree {{mvar|n}} with real coefficients, which is less than  <math> 1+\\frac 2\\pi \\ln (n) </math> for {{mvar|n}} sufficiently large.\n\nIn this article, a polynomial that is considered is always denoted \n:<math>\n  p(x)=a_0 + a_1 x + \\cdots + a_n x^n,\n</math>\nwhere <math>a_0, \\dots, a_n</math> are real or complex numbers and <math>a_n \\neq 0</math>; thus {{mvar|n}} is the degree of the polynomial.\n\n==Continuous dependence on coefficients==\nThe {{math|''n''}} roots of a polynomial of degree {{math|''n''}} depend [[continuous function|continuously]] on the coefficients. For simple roots, this results immediately from the [[implicit function theorem]]. This is true also for multiple roots, but some care is needed for the proof.\n\nA small change of coefficients may induce a dramatic change of the roots, including the change of a real root into a complex root with a rather large imaginary part (see [[Wilkinson's polynomial]]). A consequence is that, for classical numeric [[root-finding algorithm]]s, the problem of approximating the roots given the coefficients is [[ill-conditioned]].\n\n==Conjugation==\n\nThe [[complex conjugate root theorem]] states that if the coefficients\nof a polynomial are real, then the non-real roots appear in pairs of the form  {{math|(''a'' + ''ib'', ''a'' – ''ib'')}}.\n\nIt follows that the roots of a polynomial with real coefficients are [[reflection symmetry|mirror-symmetric]] with respect to the real axis.\n\nThis can be extended to [[algebraic conjugate|algebraic conjugation]]: the roots of a polynomial with [[rational number|rational]] coefficients are ''conjugate'' (that is, invariant) under the action of the [[Galois group]] of the polynomial. However, this symmetry can rarely be interpreted geometrically.\n\n==Bounds on all roots==\nUpper bounds on the absolute values of polynomial roots are widely used for [[root-finding algorithm]]s, either for limiting the regions where roots should be searched, or for the computation of the [[computational complexity]] of these algorithms. \n\nMany such bounds have been given, and the sharper one depends generally of the specific sequence of coefficient that are considered. Most bounds are greater or equal to one, and are thus not sharp for a polynomial which have only roots of absolute values lower than one. However, such polynomials are very rare, as shown below.\n\nAny upper bound on the absolute values of roots provides a corresponding lower bound. In fact, if <math>a_n\\ne 0,</math> and {{mvar|U}} is an upper bound of the absolute values of the roots of \n:<math>\na_0 + a_1 x + \\cdots + a_n x^n,\n</math>\nthen {{math|1/''U''}} is a lower bound of the absolute values of \n:<math>\na_n + a_{n-1} x + \\cdots + a_0 x^n,\n</math>\nsince the roots of either polynomial are the multiplicative inverses of the roots of the other. ''Therefore, in the remainder of the article lower bounds will not be given explicitly''.\n\n===Lagrange's and Cauchy's bounds===\n[[Lagrange]] and [[Cauchy]] were the firsts to provide upper bounds on all complex roots.<ref name=Hirst1997>{{cite journal |first=Holly P. |last=Hirst |first2=Wade T. |last2=Macey |title=Bounding the Roots of Polynomials |journal=[[The College Mathematics Journal]] |volume=28 |issue=4 |year=1997 |pages=292–295 |jstor=2687152 }}</ref> Lagrange's bound is<ref>Lagrange J–L (1798) Traité de la résolution des équations numériques. Paris.</ref>\n <math>\\max\\left\\{1,\\sum_{i=0}^{n-1} \\left|\\frac{a_i}{a_n}\\right|\\right\\},</math>\nand Cauchy's bound is<ref name=Cauchy1829>Cauchy Augustin-Louis (1829). ''Exercices de mathématique''. Œuvres 2 (9) p.122</ref>\n <math>1+\\max\\left\\{ \\left|\\frac{a_{n-1}}{a_n}\\right|, \\left|\\frac{a_{n-2}}{a_n}\\right|, \\ldots, \\left|\\frac{a_0}{a_n}\\right|\\right\\}</math>\n\nLagrange's bound is sharper (smaller) than Cauchy's bound only when 1 is larger than the sum of all <math>\\left|\\frac{a_i}{a_n}\\right|</math> but the largest. This is relatively rare in practice, and explains why Cauchy's bound is more widely used than Lagrange's.\n\nBoth bounds result from the [[Gershgorin circle theorem]] applied to the [[companion matrix]] of the polynomial and its [[transpose]]. They can also be proved by elementary methods.\n\n{{cot|Proof of Lagrange's and Cauchy's bounds}}\nIf {{mvar|z}} is a root of the polynomial, and {{math|{{!}}''z''{{!}} ≥ 1}} one has \n:<math>|a_n||z^n| = \\left|\\sum_{i=0}^{n-1}a_iz^i\\right|\n\\le \\sum_{i=0}^{n-1}|a_iz^i|\n\\le \\sum_{i=0}^{n-1}|a_i||z|^{n-1}.\n</math>\n\nDividing by <math>|a_n||z|^{n-1},</math> one gets \n:<math>|z|\\le \\sum_{i=0}^{n-1}\\frac{|a_i|}{|a_n|},</math>\nwhich is Lagrange's bound when there is at least one root of absolute value larger than 1. Otherwise, 1 is a bound on the roots, and is not larger than Lagrange's bound.\n\nSimilarly, for Cauchy's bound, one has, if {{math|{{!}}''z''{{!}} ≥ 1}},\n:<math>|a_n||z^n| = \\left|\\sum_{i=0}^{n-1}a_iz^i\\right|\n\\le \\sum_{i=0}^{n-1}|a_iz^i|\n\\le \\max |a_i|\\sum_{i=0}^{n-1}|z|^i\n= \\frac{|z|^n-1}{|z|-1}\\max |a_i|\n\\le  \\frac{|z|^n}{|z|-1}\\max |a_i|.\n</math>\nThus \n:<math>\n|a_n|(|z|-1) \\le \\max |a_i|.\n</math>\nSolving in {{math|{{!}}''z''{{!}}}}, one gets Cauchy's bound if there is a root of absolute value larger than 1. Otherwise the bound is also correct, as Cauchy's bound is larger than 1.\n{{cob}}\n\nThese bounds are not invariant by scaling. That is, the roots of the polynomial {{math|''p''(''sx'')}} are the quotient by {{mvar|s}} of the root of {{mvar|p}}, and the bounds given for the roots of {{math|''p''(''sx'')}} are not the quotient by {{mvar|s}} of the bounds of {{mvar|p}}. Thus, one may get sharper bounds by minimizing over possible scalings. This gives \n <math>\\min_{s\\in \\mathbb R_+}\\left(\\max\\left\\{ s,\\sum_{i=0}^{n-1} \\left|\\frac{a_i}{a_n}\\right|s^{i-n+1}\\right\\}\\right),</math>\nand\n <math>\\min_{s\\in \\mathbb R_+}\\left(s+\\max_{1\\le i\\le n}\\left(\\left|\\frac{a_i}{a_n}\\right| s^{i-n+1}\\right)\\right),</math>\nfor Lagrange's and Cauchy's bounds respectively.\n\nAnother bound, originally given by Lagrange, but attributed to Zassenhaus by [[Donald Knuth]], is  <ref name =yap145>{{harvnb|Yap|2000|loc=§VI.2}}</ref>\n <math>2\\max\\left\\{ \\left|\\frac{a_{n-1}}{a_n}\\right|, \\left|\\frac{a_{n-2}}{a_{n}}\\right|^{1/2}, \\ldots, \\left|\\frac{a_0}{a_n}\\right|^{1/n}\\right\\}.</math>\nThis bound is invariant by scaling.\n\n{{cot|Proof of the preceding bound}}\nLet {{mvar|A}} be the largest <math>\\left|\\frac{a_i}{a_n}\\right|^\\frac 1{n-i}</math> for {{math|0 ≤ ''i'' < ''n''}}. Thus one has\n:<math>\\frac {|a_{i}|}{|a_n|} \\le A^{n-i}</math>\nfor <math>0\\le i <n.</math>\nIf {{mvar|z}} is a root of {{mvar|p}}, one has \n:<math>-a_nz^n=\\sum_{i=0}^{n-1}a_i z^i,</math>\nand thus, after dividing by <math>a_n,</math>\n:<math>\\begin{align}\n|z|^n&\\le\\sum_{i=0}^{n-1}A^{n-i}|z|^i\\\\\n&=A\\frac{|z|^{n}-A^{n}}{|z|-A}.\n\\end{align}</math>\nAs we want to prove {{math|{{!}}''z''{{!}} ≤ 2''A''}}, we may suppose that {{math|{{!}}''z''{{!}} > A}} (otherwise there is nothing to prove).\nThus \n:<math>|z|^n \\le A\\frac{|z|^n}{|z|-A},</math>\nwhich gives the result, since <math>|z|>A.</math>\n{{cob}}\n\nLagrange has improved this latter bound into the sum of the two largest values (possibly equal) in the sequence<ref name =yap145/>\n <math>\\left[ \\left|\\frac{a_{n-1}}{a_n}\\right|, \\left|\\frac{a_{n-2}}{a_{n}}\\right|^{1/2}, \\ldots, \\left|\\frac{a_0}{a_n}\\right|^{1/n}\\right].</math>\n\nLagrange provided also the bound {{cn|date=March 2019}}\n <math>\\sum_i \\left|\\frac{a_i}{a_{i+1}}\\right|,</math>\nwhere <math>a_i</math> denotes the {{mvar|i}}th ''nonzero'' coefficient when the terms of the polynomials are sorted by increasing degrees.\n\n===Using Hölder's inequality===\n[[Hölder's inequality]] allows extends Lagrange's and Cauchy's bounds to every [[p-norm|{{mvar|h}}-norm]]. The {{mvar|h}}-norm of a sequence\n:<math>s=(a_0, \\ldots, a_n)</math>\nis \n:<math>\\|s\\|_h = \\left(\\sum_{i=1}^n |a_i|^h\\right)^{1/h},</math>\nfor any real number {{math|''h'' ≥ 1}}, and \n:<math>\\|s\\|_\\infty = \\textstyle{\\max_{i=1}^n} |a_i|.</math>\n\nIf <math>\\frac 1h+ \\frac 1k=1,</math> with {{math|1 ≤ ''h'', ''k'' ≤ ∞}}, and {{math|1=1 / ∞ = 0}}, an upper bound on the absolute values of the roots of {{mvar|p}} is \n <math>\\frac 1{|a_n|}\\left\\|(|a_n|, \\left\\|(|a_{n-1}|, \\ldots, |a_0| \\right)\\|_h\\right)\\|_k.</math>\n\nFor {{math|1=''k'' = 1}} and {{math|1=''k'' = ∞}}, one gets respectively Cauchy's and Lagrange's bounds.\n\nFor {{math|1=''h'' = ''k'' = 1/2}}, one has the bound\n <math>\\frac 1{|a_n|}\\sqrt{|a_n|^2+|a_{n-1}|^2+ \\cdots, +|a_0|^2 }.</math>\nThis is not only a bound of the absolute values of the roots, but also a bound of the product of their absolute values larger than 1; see {{slink||Landau's inequality}}, below.\n\n{{cot|Proof}}\nLet {{mvar|z}} be a root of the polynomial\n\n:<math>p(x)=a^n+a_{n-1}x^{n-1}+\\cdots+a_1x +a_0.</math>\n\nSetting \n:<math>A=\\left(\\frac{|a_{n-1}|}{|a_n|},\\ldots, \\frac{|a_1|}{|a_n|},\\frac{|a_0|}{|a_n|}\\right),</math>\nwe have to prove that every root {{mvar|z }} of {{mvar|p}} satisfies\n:<math>z\\le \\left\\|(1, \\left\\|A\\right\\|_h\\right)\\|_k.</math>\n\nIf <math>|z| \\le 1,</math> the inequality is true; so, one may suppose <math>|z| > 1</math> for the remainder of the proof.\n\nWriting the equation as\n:<math>-z^n=\\frac{a_{n-1}}{a_n}z^{n-1}+\\cdots+\\frac{a_1}{a_n}z+a_0,</math>\nthe [[Hölder's inequality]] implies\n:<math>|z|^n\\leq \\|A\\|_h \\cdot\\left \\|(z^{n-1},\\ldots,z, 1) \\right \\|_k.</math>\n\nIf {{math|''k'' {{=}} 1}}, this is\n:<math>|z|^n\\leq\\|A\\|_1\\max \\left \\{|z|^{n-1},\\ldots,|z|,1 \\right \\} =\\|A\\|_1|z|^{n-1}.</math>\nThus\n:<math>|z|\\leq \\max\\{1,\\|A\\|_1\\}.</math>\n\nIn the case {{math|1 < ''k'' ≤ ∞}}, the summation formula for a [[geometric progression]], gives\n\n:<math>|z|^n\\leq \\|A\\|_h \\left(|z|^{k(n-1)}+\\cdots+|z|^k +1\\right)^{\\frac{1}{k}}=\\|A\\|_h \\left(\\frac{|z|^{kn}-1}{|z|^k-1}\\right)^{\\frac{1}{k}}\\leq\\|A\\|_h \\left(\\frac{|z|^{kn}}{|z|^k-1}\\right)^\\frac{1}{k}.</math>\nThus\n:<math>|z|^{kn}\\leq \\left(\\|A\\|_h\\right)^k \\frac{|z|^{kn}}{|z|^k-1},</math>\nwhich simplifies to\n:<math>|z|^k\\leq 1+\\left(\\|A\\|_h\\right)^k.</math>\n\nThus, in all cases\n:<math>|z|\\leq \\left \\| \\left (1,\\|A\\|_h \\right ) \\right \\|_k,</math>\nwhich finishes the proof.\n{{cob}}\n\n===Other bounds===\nMany other upper bounds for the magnitudes of all roots have been given.<ref>{{cite book |first=M. |last=Marden |title=Geometry of Polynomials |publisher=Amer. Math. Soc. |year=1966 |isbn=0-8218-1503-2 }}</ref>\n\nFujiwara's bound<ref name=Fujiwara1916>{{cite journal |last=Fujiwara |first=M. |year=1916 |title=Über die obere Schranke des absoluten Betrages der Wurzeln einer algebraischen Gleichung |journal=[[Tohoku Mathematical Journal]] |series=First series |volume=10 |issue= |pages=167–171 |url=https://www.jstage.jst.go.jp/article/tmj1911/10/0/10_0_167/_pdf }}</ref>\n <math>2\\, \\max \\left\\{ \\left|\\frac{a_{n-1}}{a_n}\\right|, \\left|\\frac{a_{n-2}}{a_n}\\right|^{\\frac{1}{2}}, \\ldots, \\left|\\frac{a_1}{a_n}\\right|^\\frac{1}{n-1}, \\left|\\frac{a_0}{2a_n}\\right|^\\frac 1 n\\right\\}, </math> \nimproves slightly an above given bound by dividing by two the last argument of the maximum.\n\nKojima's bound is<ref name=Kojima1917>{{cite journal |last=Kojima |first=T. |year=1917 |title=On the limits of the roots of an algebraic equation |journal=Tohoku Mathematical Journal |series=First series |volume=11 |issue= |pages=119–127 |doi= |url=https://www.jstage.jst.go.jp/article/tmj1911/11/0/11_0_119/_pdf }}</ref>{{check|date=March 2019}}\n <math>2\\,\\max \\left\\{ \\left|\\frac{a_{n-1}}{a_n}\\right|,\\left|\\frac{a_{n-2}}{a_{n-1}}\\right|, \\ldots, \\left|\\frac{a_0}{2a_1}\\right|\\right\\},</math>\nwhere <math>a_i</math> denotes the {{mvar|i}}th ''nonzero'' coefficient when the terms of the polynomials are sorted by increasing degrees. If all coefficients are nonzero, Fujiwara's bound is sharper, since\neach element in Fujiwara's bound is the [[geometric mean]] of first elements in Kojima's bound.\n\nSun and Hsieh obtained another improvement on Cauchy's bound.<ref name=Sun1996>{{cite journal |last=Sun |first=Y. J. |last2=Hsieh |first2=J. G. |year=1996 |title=A note on circular bound of polynomial zeros |journal=IEEE Trans Circuits Syst. I |volume=43 |issue=6 |pages=476–478 |doi=10.1109/81.503258 }}</ref> Assume the polynomial is monic with general term {{mvar|a<sub>i</sub>x<sup>i</sup>}}. Sun and Hsieh showed that upper bounds {{math|1 + ''d''<sub>1</sub>}} and {{math|1 + ''d''<sub>2</sub>}} could be obtained from the following equations.\n\n:<math>d_1 = \\tfrac{1}{2} \\left((| a_{n-1}| - 1) + \\sqrt{(|a_{n-1}| - 1 )^2 + 4a } \\right), \\qquad a = \\max \\{ |a_i | \\}.</math>\n\n{{math|''d''<sub>2</sub>}} is the positive root of the cubic equation\n\n:<math>Q(x) = x^3 + (2 - |a_{n-1}|) x^2 + (1 - |a_{n-1}| - |a_{n-2}| ) x - a, \\qquad a = \\max \\{ |a_i | \\}</math>\n\nThey also noted that {{math|''d''<sub>2</sub> ≤ ''d''<sub>1</sub>}}\n\n==Landau's inequality==\nThe previous bounds are upper bounds for each root separately. '''Landau's inequality''' provides an upper bound for the absolute values of the product of the roots that have an absolute value greater than one. This inequality, discovered in 1905 by [[Edmund Landau]]<ref>E. Landeau, Sur quelques th&or&mes de M. Petrovic relatifs aux zéros des fonctions analytiques, ''Bull. Sot. Math. France'' 33 (1905), 251-261.</ref> has been forgotten and rediscovered at least three times during the 20th century.<ref>M. Mignotte. An inequality about factors of polynomials, ''Math. Comp.'' 28 (1974). 1153-1157.</ref><ref>W. Specht, Abschätzungen der Wurzeln algebraischer Gleichungen, Math. Z. 52 (1949). 310-321.</ref><ref>J. Vincente Gonçalves, L’inégalité de W. Specht. ''Univ. Lisboa Revista Fac. Ci A. Ci. Mat.'' 1 (195O), 167-171.</ref>\n\nThis bound of the product of roots is not much greater than the best preceding bounds of each root separately.<ref>{{cite book |last=Mignotte |first=Maurice |chapter=Some useful bounds |title=Computer Algebra : Symbolic and Algebraic Computation |pages=259–263 |publisher=Springer |location=Vienna |year=1983 |isbn=0-387-81776-X |chapterurl=https://books.google.com/books?id=qCX4CAAAQBAJ&pg=PA259 }}</ref>\nLet <math>z_1, \\ldots, z_n</math> be the {{mvar|n}} roots of the polynomial {{mvar|p}}. If\n:<math>M(p)=|a_n|\\prod_{j=1}^n \\max(1,|z_j|)</math>\nis the [[Mahler measure]] of {{mvar|p}},\nthen \n:<math>M(p)\\le \\sqrt{\\sum_{k=0}^n |a_k|^2}.</math>\n\nSurprisingly, this bound of the product of the absolute values larger than 1 of the roots is not much larger than the best bounds of ''one'' root that have been given above for a single root. This bound is even exactly equal to one of the bounds that are obtained [[#Using Hölder's inequality|using Hölder's inequality]].\n\nThis bound is also useful to bound the coefficients of a divisor of a polynomial with integer coefficients:<ref>Mignotte, M. (1988). An inequality about irreducible factors of integer polynomials. ''Journal of number theory'', 30(2), 156-166.</ref>\nif\n:<math>q= \\sum_{k=0}^m  b_k x^k</math>\nis a divisor of {{math|''p''}}, then\n:<math>|b_m|\\le|a_n|,</math>\nand, by [[Vieta's formulas]],\n:<math>\\frac{|b_i|}{|b_m|}\\le \\binom mi \\frac{M(p)}{|a_n|},</math>\nfor {{math|1=''i'' = 0, ..., ''m''}}, where <math>\\binom mi</math> is a [[binomial coefficient]]. Thus \n:<math>|b_i|\\le \\binom mi M(p)\\le \\binom mi \\sqrt{\\sum_{k=0}^n |a_k|^2},</math>\nand \n:<math>\\sum_{i=0}^m |b_k| \\le 2^m M(p) \\le 2^m \\sqrt{\\sum_{k=0}^n |a_k|^2}.</math>\n\n==Discs containing some roots==\n\n===From Rouché theorem===\n\n[[Rouché's theorem]] allows defining discs centered at zero and containing a given number of roots. More precisely, if there is a positive real number {{mvar|''R''}} and an integer {{math|0 ≤ ''k'' ≤ ''n''}} such that\n:<math>|a_k| R^k > |a_0|+\\cdots+|a_{k-1}| R^{k-1}+|a_{k+1}| R^{k+1}+\\cdots+|a_n| R^n,</math>\nthen there are exactly {{math|''k''}} roots, counted with multiplicity, of absolute value less than {{math|''R''}}. \n{{cot|Proof}}\nIf <math>|z|=R,</math> then\n:<math>\\begin{align}\n|a_0 &+\\cdots+a_{k-1}z^{k-1}+a_{k+1} z^{k+1}+\\cdots+a_n z^n|\\\\\n&\\le |a_0|+\\cdots+|a_{k-1}| R^{k-1}+|a_{k+1}| R^{k+1}+\\cdots+|a_n| R^n\\\\\n&\\le |a_k| R^k \\le |a_k z^k|.\n\\end{align}</math>\nBy Rouché's theorem, this implies directly that <math>p(z)</math> and <math>z^k</math> have the same number of roots of absolute values less than {{mvar|R}}, counted with multiplicities. As this number is {{mvar|k}}, the result is proved.\n{{cob}}\n\nThe above result may be applied if the polynomial\n:<math>h_k(x)=|a_0| +\\cdots+|a_{k-1}| x^{k-1}-|a_k|x^k+|a_{k+1}| x^{k+1}+\\cdots+|a_n| x^n.</math>\ntakes a negative value for some positive real value of {{mvar|x}}.\n\nIn the remaining of the section, with suppose that {{math|''a''<sub>''n''</sub> ≠ 0}}. If it is not the case, zero is a root, and the localization of the other roots may be studied by dividing the polynomial by a power of the indeterminate, for getting a polynomial with a nonzero constant term.\n\nFor {{math|1=''k'' = 0}} and {{math|1=''k'' = ''n''}}, [[Descartes' rule of signs]] shows that the polynomial has exactly one positive real root. If <math>R_0</math> and <math>R_n</math> are these root, the above result shows that all the roots verifies \n:<math>R_0\\le |z| \\le R_1.</math>\nA these inequalities apply also to <math>h_0</math> and <math>h_n,</math> these bounds are optimal for polynomials with a given sequence of the absolute values of their coefficients. They are thus sharper than all bounds given in the preceding sections.\n\nFor {{math|0 < ''k'' < ''n''}}, Descartes' rule of signs implies that <math>h_k(x)</math> either has two positive real roots that are not multiple, or is nonnegative for every positive value of {{mvar|x}}. So, the above result may be applied only in the first case. If <math>R_{k,1}<R_{k,2}</math> are these two roots, the above result implies that\n:<math>|z| \\le R_{k,1}</math> \nfor {{mvar|k}} roots of {{mvar|p}}, and that \n:<math>|z| \\ge R_{k,2}</math> \nfor the {{math|''n'' – ''k''}} other roots.\n\nInstead of computing explicitly <math>R_{k,1}</math> and <math>R_{k,2},</math> it is generally sufficient to compute a value <math>R_k</math> such that <math>h_k(R_k)<0</math> (necessarily <math>R_{k,1}<R_k<R_{k,2}</math>). These  <math>R_k</math> have the property of separating roots in terms of their absolute values: if, for {{math|''h'' < ''k''}}, both <math>R_h</math> and <math>R_k</math> exist, there are exactly {{math|''k'' – ''h''}} roots {{mvar|z}} such that <math>R_h < |z| < R_k.</math>\n\nFor computing <math>R_k,</math> one can use that <math>\\frac{h(x)}{x^k}</math> is a [[convex function]] (its second derivative is positive). Thus <math>R_k</math> exists if and only if <math>\\frac{h(x)}{x^k}</math> is negative at its unique minimum. For computing this minimum, one can use any [[optimization]] method, or, alternatively, [[Newton's method]] for computing the unique positive zero of the derivative of <math>\\frac{h(x)}{x^k}</math> (it converges rapidly, as the derivative is a [[monotonic function]]).\n\nOne can increase the number of existing <math>R_k</math>'s by applying the root squaring operation of the [[Graeffe's method|Dandelin–Graeffe iteration]]. If the roots have distinct absolute values, one can eventually separate completely the roots in terms of their absolute values, that is compute {{math|''n'' + 1}} positive numbers <math>R_0 < R_1 <\\dots <R_n</math> such there is exactly one root with an absolute value in the open interval <math>(R_{k-1},R_k),</math> for {{math|1=''k'' = 1, ..., ''n''}}.\n\n===From Gershgorin circle theorem ===\nThe [[Gershgorin circle theorem]] applied the [[companion matrix]] of the polynomial on a basis related to [[Lagrange interpolation]] provides discs centered at the interpolation points and each containing a root of the polynomial; see {{slink|Durand–Kerner method|Root inclusion via Gerschgorin's circles}} for details.\n\nIf the interpolation points are close to the roots of the roots of the polynomial, the radiuses of the discs are small, and this is a key ingredient of Durand–Kerner method for computing polynomial roots.\n\n==Bounds of real roots==\n\nFor polynomials with real coefficients, it is often useful to bound only the real roots. It suffices to bound the positive roots, as the negative roots of {{math|''p''(''x'')}} are the positive roots of {{math|''p''(–''x'')}}.\n\nClearly, every bound of all roots applies also for real roots. But in some contexts, tighter bounds of real roots are useful. For example, the efficiency of the [[Real-root isolation#Method of continued fractions|method of continued fractions]] for [[real-root isolation]] strongly depends on tightness of a bound of positive roots. This has led to establish new bounds that are tighter than the general bounds of all roots. These bounds are generally expressed not only in terms of the absolute values of the coefficients, but also in terms of their signs.\n\nOther bounds apply only to polynomials whose all roots are reals (see below).\n\n===Bounds of positive real roots===\n\nFor giving a bound of the positive roots, one can suppose <math>a_n >0</math> without loss of generality, as changing the signs of all coefficients does not change the roots.\n\nEvery upper bound of the positive roots of \n:<math>q(x)=a_nx^n + \\sum_{i=0}^{n-1} \\min(0,a_i)x^i</math>\nis also a bound  for the real zeros of \n:<math>p(x)=\\sum_{i=0}^n a_ix^i</math>. \nIn fact, if {{mvar|B}} is such a bound, for all {{math|''x'' > ''B''}}, one has\n{{math|''p''(''x'') ≥ ''q''(''x'') > 0}}.\n\nApplied to Cauchy's bound, this gives the upper bound \n:<math>1+{\\textstyle\\max_{i=0}^{n-1}} \\frac{-a_i}{a_n}</math>\nfor the real roots of a polynomial with real coefficients. If this bound is not greater than {{val|1}}, this means that all nonzero coefficients have the same sign, and that there is no positive root.\n\nSimilarly, another upper bound of the positive roots is \n:<math>2\\,{\\max_{a_ia_n<0}}\\left(\\frac{-a_i}{a_n}\\right)^{\\frac 1{n-i}}.</math>\nIf all nonzero coefficients have the same sign, there is no positive root, and the maximum must be defined as being zero.\n\nOther bounds have been recently developed, mainly for the need of the [[Real-root isolation#Method of continued fractions|method of continued fractions]] for [[real-root isolation]].<ref>{{cite journal |last=Akritas |first=Alkiviadis G. |first2=A. W. |last2=Strzeboński |first3=P. S. |last3=Vigklas |title=Improving the performance of the continued fractions method using new bounds of positive roots |journal=Nonlinear Analysis: Modelling and Control |year=2008 |volume=13 |pages=265–279 |url=http://www.lana.lt/journal/30/Akritas.pdf}}</ref><ref>Ştefănescu, D. ''Bounds for Real Roots and Applications to Orthogonal Polynomials''. In: V. G. Ganzha, E. W. Mayr and E. V. Vorozhtsov (Editors): Proceedings of the 10th International Workshop on Computer Algebra in Scientific Computing, CASC 2007, pp. 377 – 391, Bonn, Germany, September 16-20, 2007. LNCS 4770, Springer Verlag, Berlin, Heidelberg.</ref>\n\n===Polynomials whose roots are all real===\n\nIf all roots of a polynomial are real, [[Edmond Laguerre|Laguerre]] proved the following lower and upper bounds of the roots, by using what is now called [[Samuelson's inequality]].<ref name=Laguerre1880>{{cite journal | author = Laguerre E | title = Sur une méthode pour obtenir par approximation les racines d'une équation algébrique qui a toutes ses racines réelles | pages = 161–172, 193–202 | year = 1880 | journal = [[Nouvelles Annales de Mathématiques]] | series = 2 | volume = 19 | url = http://www.numdam.org/numdam-bin/browse?id=NAM_1880_2_19_ }}.</ref>\n\nLet <math>\\sum_{k=0}^n a_k x^k</math> be a polynomial with all real roots. Then its roots are located in the interval with endpoints\n:<math>-\\frac{a_{n-1}}{na_n} \\pm \\frac{n-1}{na_n}\\sqrt{a^2_{n-1} - \\frac{2n}{n-1}a_n a_{n-2}}.</math>\n\nFor example, the roots of the polynomial <math>x^4+5x^3+5x^2-5x-6=(x+3)(x+2)(x+1)(x-1)</math> satisfy\n:<math>-3.8118<-\\frac{5}{4} - \\frac{3}{4}\\sqrt{\\frac{35}{3}}\\le x\\le -\\frac{5}{4} + \\frac{3}{4}\\sqrt{\\frac{35}{3}}<1.3118.</math>\n\n==Root separation==\n\nThe '''root separation''' of a polynomial is the minimal distance between two roots, that is the minimum of the absolute values of the difference of two roots:\n:<math>\\operatorname{sep}(p) = \\min\\{|\\alpha-\\beta|\\;;\\; \\alpha \\neq \\beta \\text{ and } p(\\alpha)=p(\\beta)=0\\}</math>\n\nThe root separation is a fundamental parameter of the [[computational complexity]] of [[root-finding algorithm]]s for polynomials. In fact, the root separation determines the precision of number representation that is needed for being sure of distinguishing different roots. Also, for [[real-root isolation]], it allows bounding the number of interval divisions that are needed for isolating all roots.\n\nFor polynomials with real or complex coefficients is not possible to express a lower bound of the root separation in terms of the degree and the absolute values of the coefficients only, because a small change on a single coefficient transforms a polynomial with multiple roots in a [[square-free polynomial]] with a small root separation, and essentially the same absolute values of the coefficient. However, involving the [[discriminant]] of the polynomial allows a lower bound.\n\nFor square-free polynomials with integer coefficients, the discriminant is an integer, and has thus an absolute value  that is not lower than {{val|1}}. This allows lower bounds for root separation that are independent from the discriminant.\n\nMignotte's separation bound is<ref>{{harvnb|Yap|2000|loc=§ VI.7, Proposition 29}}</ref><ref>{{cite journal|first=George E.|last=Collins|authorlink=George E. Collins|title=Polynomial minimum root separation|journal=Journal of Symbolic Computation|year=2001|volume= 32|pages= 467–473| doi=10.1006/jsco.2001.0481|url=https://core.ac.uk/download/pdf/82808521.pdf }}</ref>\n\n:<math>\\operatorname{sep}(p) >  \\frac {\\sqrt 3\\Delta(p)}{n^{n/2+1}(\\|p\\|_2)^{n-1}},</math>\nwhere <math>\\Delta(p)</math> is the discriminant, and <math>\\textstyle\\|p\\|_2=\\sqrt{a_0^2+a_1^2+\\dots+a_n^2}.</math> \n\nFor a square free polynomial with integer coefficients, this implies \n:<math>\\operatorname{sep}(p) >  \\frac {\\sqrt 3}{n^{n/2+1}(\\|p\\|_2)^{n-1}}> \\frac 1{2^{2s^2}},</math>\nwhere {{mvar|s}} is the [[bit]] size of {{mvar|p}}, that is the sum of the bitsize of its coefficients.\n\n==Gauss–Lucas theorem==\n{{main|Gauss–Lucas theorem}}\nThe Gauss–Lucas theorem states that the [[convex hull]] of the roots of a polynomial contains the roots of the [[derivative]] of the polynomial.\n\nA sometimes useful corollary is that, if all roots of a polynomial have positive real part, then so do the roots of all derivatives of the polynomial.\n\nA related result is [[Bernstein's inequality in mathematical analysis|Bernstein's inequality]]. It states that for a polynomial ''P'' of degree ''n'' with derivative ''P′'' we have\n:<math>\\max_{|z| \\leq 1} \\big|P'(z)\\big| \\le n \\max_{|z| \\leq 1} \\big|P(z)\\big|.</math>\n\n==Statistical distribution of the roots==\n\nIf the coefficients {{math|''a''<sub>i</sub>}} of a random polynomial are independently and identically distributed with a [[mean]] of zero, most complex roots are on the unit circle or close to it. In particular, the real roots are mostly located near {{math|±1}}, and, moreover, their expected number is, for a large degree, less than the [[natural logarithm]] of the degree.\n\nIf the coefficients are [[Gaussian distribution|Gaussian distributed]] with a mean of zero and [[variance]] of ''σ'' then the mean density of real roots is given by the Kac formula<ref name=Kac1943>{{cite journal |last=Kac |first=M. |authorlink=Mark Kac |year=1943 |title=On the average number of real roots of a random algebraic equation |journal=[[Bulletin of the American Mathematical Society]] |volume=49 |issue=4 |pages=314–320 |doi=10.1090/S0002-9904-1943-07912-8 }}</ref><ref name=Kac1948>{{cite journal |last=Kac |first=M. |year=1948 |title=On the Average Number of Real Roots of a Random Algebraic Equation (II) |journal=[[Proceedings of the London Mathematical Society]] |series=Second Series |volume=50 |issue=1 |pages=390–408 |doi=10.1112/plms/s2-50.5.390 }}</ref>\n\n: <math> m( x ) = \\frac { \\sqrt{ A( x ) C( x ) - B( x )^2 }} {\\pi A( x )} </math>\n\nwhere\n\n: <math> \\begin{align}\nA(x) &= \\sigma \\sum x^{2i} = \\sigma \\frac{x^{2n} - 1}{x-1}, \\\\\nB(x) &= \\frac 1 2 \\frac{ d } { dx } A( x ), \\\\\nC(x) &= \\frac 1 4 \\frac{d^2} {dx^2} A(x) + \\frac 1 { 4x } \\frac d {dx} A(x).\n\\end{align} </math>\n\nWhen the coefficients are Gaussian distributed with a non-zero mean and variance of ''σ'', a similar but more complex formula is known.{{Citation needed|date=June 2013}}\n\n===Real roots===\nFor large {{math|''n''}}, the mean density of real roots near {{mvar|x}} is asymptotically\n\n: <math> m( x ) = \\frac{ 1 } { \\pi | 1 - x^2 | } </math>\nif <math>x^2-1\\ne 0,</math>\nand\n: <math> m(\\pm 1) = \\frac 1 \\pi \\sqrt {\\frac{n^2 - 1}{12}} </math>\n\nIt follows that the expected number of real roots is, using [[big O notation|big {{mvar|O}} notation]]\n: <math> N_n = \\frac 2 \\pi \\ln n + C + \\frac 2 {\\pi n} +O( n^{ -2 } ) </math>\nwhere {{math|''C''}} is a constant approximately equal to {{val|0.6257358072}}.<ref name=ek>{{cite journal|last1=Edelman|first1=Alan|last2=Kostlan|first2=Eric|title=How many zeros of a random polynomial are real?| journal=Bulletin of the American Mathematical Society|year=1995|volume=32|pp=1-37|doi=10.1090/S0273-0979-1995-00571-9 |issue=1|url=https://www.ams.org/journals/bull/1995-32-01/S0273-0979-1995-00571-9/S0273-0979-1995-00571-9.pdf}}</ref>\n\nIn other words, ''the expected number of real roots of a random polynomial of high degree is lower than the [[natural logarithm]] of the degree''.\n\nKac, Erdös and others and other have shown that these results are insensitive to the distribution of the coefficients, if they are independent and have the same distribution with mean zero. However, if the  variance of the {{mvar|i}}th coefficient equal to <math>\\binom ni,</math> the expected number of real roots is <math>\\sqrt n.</math><ref name=ek/>\n\n==See also==\n* [[Descartes' rule of signs]]\n* [[Marden's theorem]]\n* [[Newton's identities]]\n* [[Quadratic function#Upper bound on the magnitude of the roots]]\n* [[Real-root isolation]]\n* [[Root-finding algorithm#Roots of polynomials]]\n* [[Square-free polynomial]]\n* [[Vieta's formulas]]\n\n==Notes==\n{{Reflist|30em}}\n\n==References==\n* {{cite book | last1=Rahman | first1=Q. I. | last2=Schmeisser | first2=G. | title=Analytic theory of polynomials | series=London Mathematical Society Monographs. New Series | volume=26 | location=Oxford | publisher=[[Oxford University Press]] | year=2002 | isbn=0-19-853493-0 | zbl=1072.30006 }}\n* {{cite book | last=Yap | first= Chee-Keng| title=Fundamental problems of algorithmic algebra| publisher=[[Oxford University Press]]| year = 2000|isbn=978-0195125160|ref=harv|\nurl=http://160592857366.free.fr/joe/ebooks/ShareData/Fundamental%20Problems%20in%20Algorithmic%20Algebra%201993%20Yap.pdf}}.\n\n==External links==\n*[http://math.ucr.edu/home/baez/roots/ The beauty of the roots], a visualization of the distribution of all roots of all polynomials with degree and integer coefficients in some range.\n\n[[Category:Polynomials]]"
    },
    {
      "title": "Gould polynomials",
      "url": "https://en.wikipedia.org/wiki/Gould_polynomials",
      "text": "In [[mathematics]] the '''Gould polynomials''' ''G''<sub>''n''</sub>(''x''; ''a'',''b'') are [[polynomial]]s introduced by H. W. Gould and named by Roman in 1984.<ref>{{Citation | last1=Roman | first1=Steven | title=The umbral calculus | url=https://books.google.com/books?id=JpHjkhFLfpgC | publisher=Academic Press Inc. [[Harcourt (publisher)|Harcourt Brace Jovanovich Publishers]] | location=London | series=Pure and Applied Mathematics | isbn=978-0-12-594380-2 | mr=741185 |id=Reprinted by Dover, 2005 | year=1984 | volume=111}}</ref>\nThey are given by <ref>Gould, H. W. (1961), \"A series transformation for finding convolution identities\", ''Duke Math. J.'' Volume 28, Number 2, 193-202.</ref>\n:<math> \\displaystyle \\exp(xf(t)) = \\sum_nG_n(x;a,b)t^n/n!</math>\nwhere\n:<math>f(t)=\\sum_{k\\ge 1} \\frac{1}{b}\\binom{-(b+ak)/b}{k-1}\\frac{t^k}{k!}</math>\n\n==References==\n<references/>\n\n\n[[Category:Polynomials]]"
    },
    {
      "title": "Grace–Walsh–Szegő theorem",
      "url": "https://en.wikipedia.org/wiki/Grace%E2%80%93Walsh%E2%80%93Szeg%C5%91_theorem",
      "text": "In mathematics, the '''Grace–Walsh–Szegő coincidence theorem'''<ref>\"A converse to the Grace–Walsh–Szegő theorem\", ''Mathematical Proceedings of the Cambridge Philosophical Society'', August 2009, 147(02):447–453. DOI:10.1017/S0305004109002424</ref><ref>J. H. Grace, \"The zeros of a polynomial\", ''Proceedings of the Cambridge Philosophical Society'' 11 (1902), 352–357.</ref> is a result named after [[John Hilton Grace]], [[Joseph L. Walsh]], and [[Gábor Szegő]].\n\n== Statement ==\n\nSuppose ''ƒ''(''z''<sub>1</sub>,&nbsp;...,&nbsp;''z''<sub>''n''</sub>) is a [[polynomial]] with [[complex number|complex]] coefficients, and that it is\n* symmetric, i.e. invariant under [[permutation]]s of the variables, and\n* multi-affine, i.e. affine in each variable separately.\nLet ''A'' be a circular region in the complex plane.  If either ''A'' is [[convex set|convex]] or the degree of ''ƒ'' is ''n'', then for every <math>\\zeta_1,\\ldots,\\zeta_n\\in A</math> there exists <math>\\zeta\\in A</math> such that\n\n: <math> f(\\zeta_1,\\ldots,\\zeta_n) = f(\\zeta,\\ldots,\\zeta). </math>\n\n== Notes and references ==\n\n{{reflist}}\n\n{{DEFAULTSORT:Grace-Walsh-Szego theorem}}\n[[Category:Theorems in complex analysis]]\n[[Category:Polynomials]]\n\n\n{{mathanalysis-stub}}"
    },
    {
      "title": "Graph polynomial",
      "url": "https://en.wikipedia.org/wiki/Graph_polynomial",
      "text": "In mathematics, a '''graph polynomial''' is a [[Graph property|graph invariant]] whose values are [[polynomial]]s. Invariants of this type are studied in [[algebraic graph theory]].{{r|sdlg}}\nImportant graph polynomials include:\n*The [[characteristic polynomial]], based on the graph's [[adjacency matrix]].\n*The [[chromatic polynomial]], a polynomial whose values at integer arguments give the number of colorings of the graph with that many colors.\n*The [[dichromatic polynomial]], a 2-variable generalization of the chromatic polynomial\n*The [[flow polynomial]], a polynomial whose values at integer arguments give the number of [[nowhere-zero flow]]s with integer flow amounts modulo the argument.\n*The (inverse of the) [[Ihara zeta function]], defined as a product of binomial terms corresponding to certain closed walks in a graph.\n*The [[Martin polynomial]], used by Pierre Martin to study [[Euler tour]]s\n*The [[matching polynomial]]s, several different polynomials defined as the [[generating function]] of the [[Matching (graph theory)|matchings]] of a graph.\n*The [[reliability polynomial]], a polynomial that describes the probability of remaining connected after independent edge failures\n*The [[Tutte polynomial]], a polynomial in two variables that can be defined (after a small change of variables) as the generating function of the numbers of connected components of [[induced subgraph]]s of the given graph, parameterized by the number of vertices in the subgraph.\n\n==See also==\n*[[Knot polynomial]]\n\n==References==\n{{reflist|refs=\n\n<ref name=sdlg>{{citation\n | last1 = Shi | first1 = Yongtang\n | last2 = Dehmer | first2 = Matthias\n | last3 = Li | first3 = Xueliang\n | last4 = Gutman | first4 = Ivan\n | isbn = 9781498755917\n | publisher = CRC Press\n | series = Discrete Mathematics and Its Applications\n | title = Graph Polynomials\n | year = 2016}}</ref>\n\n}}\n\n[[Category:Polynomials]]\n[[Category:Graph invariants]]\n\n{{sia}}"
    },
    {
      "title": "Heine–Stieltjes polynomials",
      "url": "https://en.wikipedia.org/wiki/Heine%E2%80%93Stieltjes_polynomials",
      "text": "{{about||the orthogonal polynomials|Stieltjes-Wigert polynomial|the polynomials associated to a family of orthogonal polynomials|Stieltjes polynomials}}\n\nIn mathematics, the '''Heine–Stieltjes polynomials''' or '''Stieltjes polynomials''', introduced by {{harvs|txt|authorlink=T. J. Stieltjes|first=T. J. |last=Stieltjes|year=1885}}, are polynomial solutions of a second-order [[Fuchsian equation]], a [[differential equation]] all of whose singularities are [[regular singularity|regular]].  The Fuchsian equation has the form\n\n:<math>\\frac{d^2 S}{dz^2}+\\left(\\sum _{j=1}^N \\frac{\\gamma _j}{z - a_j} \\right) \\frac{dS}{dz} + \\frac{V(z)}{\\prod _{j=1}^N (z - a_j)}S = 0</math>\n\nfor some polynomial ''V''(''z'') of degree at most ''N''&nbsp;&minus;&nbsp;2, and if this has a polynomial solution ''S'' then ''V'' is called a Van Vleck polynomial (after [[Edward Burr Van Vleck]]) and ''S'' is called a Heine–Stieltjes polynomial.\n\n[[Heun polynomial]]s are the special cases of Stieltjes polynomials when the differential equation has four singular points.\n\n==References==\n*{{Citation | last1=Marden | first1=Morris | title=On Stieltjes Polynomials | jstor=1989516 | publisher=[[American Mathematical Society]] | location=Providence, R.I. | year=1931 | journal=[[Transactions of the American Mathematical Society]] | issn=0002-9947 | volume=33 | issue=4 | pages=934–944 | doi=10.2307/1989516}}\n*{{dlmf|id=31.15|title=Stieltjes Polynomials|first=B. D.|last=Sleeman|first2=V. B. |last2=Kuznetzov}}\n*{{Citation | last1=Stieltjes | first1=T. J. | title= Sur certains polynômes qui vérifient une équation différentielle linéaire du second ordre et sur la theorie des fonctions de Lamé  | doi=10.1007/BF02400421 | year=1885 | journal=\tActa Mathematica | volume=6 | issue=1 | pages=321–326}}\n\n{{DEFAULTSORT:Heine-Stieltjes polynomials}}\n[[Category:Polynomials]]"
    },
    {
      "title": "Hilbert's thirteenth problem",
      "url": "https://en.wikipedia.org/wiki/Hilbert%27s_thirteenth_problem",
      "text": "'''Hilbert's thirteenth  problem''' is one of the 23 [[Hilbert problems]] set out in a celebrated list compiled in 1900 by [[David Hilbert]].  It entails proving whether a solution exists for all [[Septic equation|7th-degree equations]] using algebraic (variant: continuous) [[mathematical function|functions]] of two [[mathematical argument|arguments]]. It was first presented in the context of [[nomograph]]y, and in particular \"nomographic construction\" — a process whereby a function of several variables is constructed using functions of two variables. Hilbert's conjecture, that it is not always possible to find such a solution, was disproven in 1957.\n\n==Introduction==\nHilbert considered the seventh-degree equation\n\n:<math>x^7 + ax^3 + bx^2 + cx + 1 = 0</math>\n\nand asked whether its solution, ''x'', considered as a function of the three variables ''a'', ''b'' and ''c'', can be expressed as the [[function composition|composition]] of a finite number of two-variable functions.\n\n==History==\nHilbert originally posed his problem for algebraic functions (Hilbert 1927,  \"...Existenz von algebraischen Funktionen...\", i.e., \"...existence of algebraic functions...\"; also see Abhyankar 1997,  Vitushkin 2004).  However, Hilbert also asked in a later version of this problem whether there is a solution in the class of [[continuous function|continuous functions]]. \n\nA generalization of the second (\"continuous\") variant of the problem is the following question: can every continuous function of three variables be expressed as a [[function composition|composition]] of finitely many continuous functions of two variables?  The affirmative answer to this general question was given in 1957 by [[Vladimir Arnold]], then only nineteen years old and a student of [[Andrey Kolmogorov]]. Kolmogorov had shown in the previous year that any function of several variables can be constructed with a finite number of three-variable functions. Arnold then expanded on this work to show that only two-variable functions were in fact required, thus answering the Hilbert's question when posed for the class of continuous functions.\n\nArnold later returned to the algebraic version of the problem, jointly with [[Goro Shimura]] (Arnold and Shimura 1976).\n\n==See also==\n*[[Kolmogorov–Arnold representation theorem]]\n\n==References==\n\n* Shreeram S. Abhyankar, \"[http://www.emis.de/journals/SC/1997/2/pdf/smf_sem-cong_2_1-11.pdf  Hilbert's Thirteenth Problem]\", ''Algèbre non commutative, groupes quantiques et invariants'' (Reims, 1995), 1–11, ''Sémin. Congr.'', 2, Soc. Math. France, Paris, 1997.\n* V. I. Arnold and G. Shimura, ''Superposition of algebraic functions'' (1976), in ''Mathematical Developments Arising From Hilbert Problems'', Volume 1, Proceedings of Symposia in Pure Mathematics 28 (1976), pp. 45-46.\n* D. Hilbert, \"Über die Gleichung neunten Grades\", Math. Ann. 97 (1927), 243–250\n* G. G. Lorentz, ''Approximation of Functions'' (1966), Ch. 11\n* A. G. Vitushkin, \"[http://www.iop.org/EJ/article/0036-0279/59/1/R03/RMS_59_1_R03.pdf?request-id=ef17fbdb-1a1c-4250-ae5f-0e1885b837fa  On Hilbert's thirteenth problem and related questions]\", ''Uspekhi Mat. Nauk'' 59:1 (2004), 11&nbsp;24. (Translation in  Russian Math. Surveys 59 (2004), no. 1, 11–25 )\n\n\n{{Hilbert's problems}}\n\n[[Category:Polynomials]]\n[[Category:Hilbert's problems|#13]]\n[[Category:Disproved conjectures]]"
    },
    {
      "title": "HOMFLY polynomial",
      "url": "https://en.wikipedia.org/wiki/HOMFLY_polynomial",
      "text": "{{Use American English|date=January 2019}}{{Short description|Polynomials arising in knot theory\n}}\nIn the [[mathematics|mathematical]] field of [[knot theory]], the '''HOMFLY polynomial''', sometimes called the '''HOMFLY-PT''' polynomial or the generalized [[Jones polynomial]], is a 2-variable [[knot polynomial]], i.e. a [[knot invariant]] in the form of a [[polynomial]] of variables ''m'' and ''l''.   \n\nA central question in the [[knot theory|mathematical theory of knots]] is whether two [[knot diagram]]s represent the same knot. One tool used to answer such questions is a knot polynomial, which is computed from a diagram of the knot and can be shown to be an [[knot invariant|invariant of the knot]], i.e. diagrams representing the same knot have the same [[polynomial]]. The converse may not be true. The HOMFLY polynomial is one such invariant and it generalizes two polynomials previously discovered, the [[Alexander polynomial]] and the [[Jones polynomial]], both of which can be obtained by appropriate substitutions from HOMFLY. The HOMFLY polynomial is also a [[quantum invariant]].\n\nThe name  ''HOMFLY'' combines the initials of its co-discoverers: [[Jim Hoste]], [[Adrian Ocneanu]], [[Kenneth Millett]], [[Peter J. Freyd]], [[W. B. R. Lickorish]], and David N. Yetter.<ref>{{cite journal|authors= Freyd, P., Yetter, D., Hoste, J., Lickorish, W.B.R., Millett, K., and Ocneanu, A.|title = A New Polynomial Invariant of Knots and Links|journal = Bulletin of the American Mathematical Society|volume = 12|issue = 2|year = 1985|pages = 239–246|doi = 10.1090/S0273-0979-1985-15361-3}}</ref> The addition of ''PT'' recognizes independent work  carried out by [[Józef H. Przytycki]] and Paweł Traczyk.\n\n==Definition==\nThe polynomial is defined using [[skein relation]]s:\n\n: <math>P( \\mathrm{unknot} ) = 1,\\,</math>\n\n: <math>\\ell P(L_+) + \\ell^{-1}P(L_-) + mP(L_0)=0,\\,</math>\n\nwhere <math>L_+, L_-, L_0</math> are links formed by crossing and smoothing changes on a local region of a link diagram, as indicated in the figure.  [[Image:Skein (HOMFLY).svg|200px|center]]\n\nThe HOMFLY polynomial of a link ''L'' that is a split union of two links <math>L_1</math> and <math>L_2</math> is given by\n\n: <math>P(L) = \\frac{-(\\ell+\\ell^{-1})}{m} P(L_1)P(L_2).</math>\n\nSee the page on [[skein relation]] for an example of a computation using such relations.\n\n==Other HOMFLY skein relations==\nThis polynomial can be obtained also using other skein relations:\n: <math>\\alpha P(L_+) - \\alpha^{-1}P(L_-) = zP(L_0),\\,</math>\n: <math>xP(L_+) + yP(L_-) + zP(L_0)=0,\\,</math>\n\n==Main properties==\n: <math>P(L_1 \\# L_2)=P(L_1)P(L_2),\\,</math>, where # denotes the [[knot sum]]; thus the HOMFLY polynomial of a [[composite knot]] is the product of the HOMFLY polynomials of its components.\n\n: <math>P_K(\\ell,m)=P_{\\text{Mirror Image}(K)}(\\ell^{-1},m),\\,</math>, so the HOMFLY polynomial can often be used to distinguish between two knots of different [[chirality]]. However there exist chiral pairs of knots that have the same HOMFLY polynomial, e.g. knots 9<sub>42</sub> and 10<sub>71</sub><ref>https://arxiv.org/pdf/hep-th/9401095.pdf</ref>\n\nThe Jones polynomial, ''V''(''t''), and the Alexander polynomial, <math>\\Delta(t)\\,</math> can be computed in terms of the HOMFLY polynomial (the version in <math>\\alpha</math> and <math>z</math> variables) as follows:\n: <math>V(t)=P(\\alpha=t^{-1},z=t^{1/2}-t^{-1/2}),\\,</math>\n\n: <math>\\Delta(t)=P(\\alpha=1,z=t^{1/2}-t^{-1/2}),\\,</math>\n\n==References==\n{{reflist}}\n\n==Further reading==\n* [[Louis Kauffman|Kauffman, L.H.]], \"Formal knot theory\", Princeton University Press, 1983.\n* [[W. B. R. Lickorish|Lickorish, W.B.R.]] \"An Introduction to Knot Theory\". Springer. {{ISBN|0-387-98254-X}}.\n\n==External links==\n* {{springer|title=Jones-Conway polynomial|id=p/j130040}}\n* {{MathWorld|HOMFLYPolynomial|HOMFLY Polynomial}}\n* {{Knot Atlas|The HOMFLY-PT Polynomial}}\n\n{{Knot theory}}\n\n{{DEFAULTSORT:Homfly Polynomial}}\n[[Category:Knot theory]]\n[[Category:Polynomials]]"
    },
    {
      "title": "Humbert polynomials",
      "url": "https://en.wikipedia.org/wiki/Humbert_polynomials",
      "text": "In [[mathematics]], the '''Humbert polynomials''' π{{su|b=''n'',''m''|p=λ}}(''x'') are a generalization of [[Pincherle polynomials]] introduced by {{harvs|txt|last=Humbert|year=1921|authorlink=Pierre Humbert (mathematician)}} given by the [[generating function]]\n\n:<math>\\displaystyle (1-mxt+t^m)^{-\\lambda}=\\sum^\\infty\n_{n=0}\\pi^\\lambda_{n,m}(x)t^n</math>\n\n{{harvtxt|Boas|Buck|1958|loc=p.58}}.\n\n==See also==\n\n*[[Umbral calculus]]\n\n==References==\n\n*{{Citation | last1=Boas | first1=Ralph P. | last2=Buck | first2=R. Creighton | title=Polynomial expansions of analytic functions | url=https://books.google.com/books?id=eihMuwkh4DsC | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Ergebnisse der Mathematik und ihrer Grenzgebiete. Neue Folge.  | mr=0094466 | year=1958 | volume=19}}\n*{{Citation | last1=Humbert | first1=Pierre | title=Some extensions of Pincherle's Polynomials | doi=10.1017/S0013091500035756 | year=1921 | journal=Proceedings of the Edinburgh mathematics society | volume=39 | pages=21–24}}\n\n[[Category:Polynomials]]"
    },
    {
      "title": "Hurwitz polynomial",
      "url": "https://en.wikipedia.org/wiki/Hurwitz_polynomial",
      "text": "In [[mathematics]], a '''Hurwitz polynomial''', named after [[Adolf Hurwitz]], is a [[polynomial]]  whose [[root of a function|roots]] ([[zero (complex analysis)|zeros]]) are located in the left half-plane of the [[complex number|complex plane]] or on the imaginary  axis, that is, the real part of every root is zero or negative.<ref name=\"Kuo\">{{cite book   \n  | last = Kuo\n  | first = Franklin F.\n  | authorlink = \n  | coauthors = \n  | title = Network Analysis and Synthesis, 2nd Ed.\n  | publisher = John Wiley & Sons\n  | date = 1966\n  | location = \n  | pages = 295–296\n  | url = \n  | doi = \n  | id = \n  | isbn = 0471511188}}</ref>  Such a polynomial must have coefficients that are positive [[real number]]s. The term is sometimes restricted to polynomials whose roots have real parts that are strictly negative, excluding the axis (i.e., a Hurwitz [[stable polynomial]]).<ref name=\" Weisstein\">{{cite web\n  | last =  Weisstein\n  | first = Eric W\n  | title = Hurwitz polynomial\n  | work = Wolfram Mathworld\n  | publisher = Wolfram Research\n  | date = 1999\n  | url = http://mathworld.wolfram.com/HurwitzPolynomial.html\n  | doi = \n  | accessdate = July 3, 2013}}</ref><ref name=\"Reddy\">{{cite conference\n  | first = Hari C.\n  | last = Reddy\n  | title = Theory of two-dimensional Hurwitz polynomials\n  | booktitle = The Circuits and Filters Handbook, 2nd Ed.\n  | pages = 260–263\n  | publisher = CRC Press\n  | date = 2002\n  | location = \n  | url = https://books.google.com/books?id=SmDImt1zHXkC&pg=PA262&dq=hurwitz+polynomial\n  | doi = \n  | isbn = 1420041401\n  | accessdate = July 3, 2013}}</ref>\n\nA polynomial function ''P''(''s'') of a [[complex variable]] ''s'' is said to be Hurwitz if the following conditions are satisfied:\n\n:1. ''P''(''s'') is real when ''s'' is real.\n \n:2. The roots of ''P''(''s'') have real parts which are zero or negative.\n\nHurwitz polynomials are important in [[control system|control systems theory]], because they represent the [[Characteristic polynomial#Characteristic equation|characteristic equations]] of [[Stability theory|stable]] [[linear system]]s.  Whether a polynomial is Hurwitz can be determined by solving the equation to find the roots, or from the coefficients without solving the equation by the [[Routh–Hurwitz stability criterion]].\n\n== Examples ==\n\nA simple example of a Hurwitz polynomial is the following:\n\n:<math>x^2 + 2x + 1.</math>\n\nThe only real solution is &minus;1, as it factors to\n\n:<math>(x+1)^2.</math>\n\nIn general, all second-degree polynomials with positive coefficients are Hurwitz.\nThis follows directly from the [[quadratic formula]]:\n:<math>x=\\frac{-b\\pm\\sqrt{b^2-4ac\\ }}{2a}.</math>\nwhere, if the determinant ''b^2-4ac'' is less than zero, then the polynomial will have two complex-conjugate solutions with real part ''-b/2a'', which is negative for positive ''a'' and ''b''.\nIf it is equal to zero, there will be two coinciding real solutions at ''-b/2a''. Finally, if the determinant is greater than zero, there will be two real negative solutions,\nbecause <math>\\sqrt{b^2-4ac}<b </math> for positive ''a'', ''b'' and ''c''.\n\n== Properties ==\n\nFor a polynomial to be Hurwitz, it is necessary but not sufficient that all of its coefficients be positive (except for second-degree polynomials, which also doesn't imply sufficiency).  A necessary and sufficient condition that a polynomial is Hurwitz is that it passes the [[Routh–Hurwitz stability criterion]].  A given polynomial can be efficiently tested to be Hurwitz or not by using the Routh continued fraction expansion technique.\n\nThe properties of Hurwitz polynomials are:\n\n# All the [[pole (complex analysis)|poles]] and [[zero (complex analysis)|zeros]] are in the left half plane or on its boundary, the imaginary axis.\n# Any poles and zeros on the imaginary axis are simple (have a multiplicity of one).\n# Any poles on the imaginary axis have real strictly positive residues, and similarly at any zeros on the imaginary axis, the function has a real strictly positive derivative.\n# Over the right half plane, the minimum value of the real part of a PR function occurs on the imaginary axis (because the real part of an analytic function constitutes a harmonic function over the plane, and therefore satisfies the maximum principle).\n# The polynomial should not have missing powers of s.\n\n== References ==\n{{reflist}}\n* Wayne H. Chen (1964) ''Linear Network Design and Synthesis'', page 63, [[McGraw Hill]].\n\n{{DEFAULTSORT:Hurwitz Polynomial}}\n[[Category:Polynomials]]"
    },
    {
      "title": "Integer-valued polynomial",
      "url": "https://en.wikipedia.org/wiki/Integer-valued_polynomial",
      "text": "In [[mathematics]], an '''integer-valued polynomial''' (also known as a '''numerical polynomial''') ''P''(''t'') is a [[polynomial]] whose value ''P''(''n'') is an [[integer]] for every integer ''n''. Every polynomial with integer [[coefficient]]s is integer-valued, but the converse is not true. For example, the polynomial\n\n:<math> \\frac{1}{2} t^2 + \\frac{1}{2} t=\\frac{1}{2}t(t+1)</math>\n\ntakes on integer values whenever ''t'' is an integer.  That is because one of ''t'' and ''t'' + 1 must be an [[even number]]. (The values this polynomial takes are the [[triangular number]]s.)\n\nInteger-valued polynomials are objects of study in their own right in algebra, and frequently appear in [[algebraic topology]].<ref>{{citation|title=Commutative Algebra: Recent Advances in Commutative Rings, Integer-Valued Polynomials, and Polynomial Functions|editor1-first=Marco|editor1-last=Fontana|editor2-first=Sophie|editor2-last=Frisch|editor3-first=Sarah|editor3-last=Glaz|editor3-link=Sarah Glaz|publisher=Springer|year=2014|isbn=9781493909254|contribution=Stable homotopy theory, formal group laws, and integer-valued polynomials|first=Keith|last=Johnson|url=https://books.google.com/books?id=ZZEpBAAAQBAJ&pg=PA213|pages=213–224}}. See in particular pp.&nbsp;213–214.</ref>\n\n==Classification==\nThe class of integer-valued polynomials was described fully by {{harvtxt|Pólya|1915}}. Inside the [[polynomial ring]] '''Q'''[''t''] of polynomials with [[rational number]] coefficients, the [[subring]] of integer-valued polynomials is a [[free abelian group]]. It has as [[basis (linear algebra)|basis]] the polynomials\n\n:''P<sub>k</sub>''(''t'') = ''t''(''t'' &minus; 1)...(''t'' &minus; ''k'' + 1)/''k''!\n\nfor ''k'' = 0,1,2, ..., i.e., the [[binomial coefficient]]s.  In other words, every integer-valued polynomial can be written as an integer [[linear combination]] of binomial coefficients in exactly one way.  The proof is by the method of [[Difference operator|discrete Taylor series]]: binomial coefficients are integer-valued polynomials, and conversely, the discrete difference of an integer series is an integer series, so the discrete Taylor series of an integer series generated by a polynomial has integer coefficients (and is a finite series).\n\n==Fixed prime divisors==\nInteger-valued polynomials may be used effectively to solve questions about fixed divisors of polynomials. For example, the polynomials ''P'' with integer coefficients that always take on even number values are just those such that ''P''/2 is integer valued. Those in turn are the polynomials that may be expressed as a linear combination with even integer coefficients of the binomial coefficients.\n\nIn questions of prime number theory, such as [[Schinzel's hypothesis H]] and the [[Bateman–Horn conjecture]], it is a matter of basic importance to understand the case when ''P'' has no fixed prime divisor (this has been called ''Bunyakovsky's property''{{Citation needed|date=January 2013}}, after [[Viktor Bunyakovsky]]). By writing ''P'' in terms of the binomial coefficients, we see the highest fixed prime divisor is also the highest prime [[common factor]] of the coefficients in such a representation. So Bunyakovsky's property is equivalent to coprime coefficients.\n\nAs an example, the pair of polynomials ''n'' and ''n''<sup>2</sup> + 2 violates this condition at ''p'' = 3: for every ''n'' the product\n\n:''n''(''n''<sup>2</sup> + 2)\n\nis divisible by 3. Consequently, there cannot be infinitely many prime pairs ''n'' and ''n''<sup>2</sup> + 2. The divisibility is attributable to the alternate representation\n\n:''n''(''n'' + 1)(''n'' &minus; 1) + 3''n''.\n\n==Other rings==\nNumerical polynomials can be defined over other rings and fields, in which case the integer-valued polynomials above are referred to as '''classical numerical polynomials'''.{{citation needed|date=April 2012}}\n\n==Applications==\nThe [[topological K-theory|K-theory]] of [[Classifying space for U(n)|BU(''n'')]] is numerical (symmetric) polynomials.\n\nThe [[Hilbert polynomial]] of a polynomial ring in ''k''&nbsp;+&nbsp;1 variables is the numerical polynomial <math>\\binom{t+k}{k}</math>.\n\n==References==\n{{Reflist}}\n\n===Algebra===\n* {{citation\n|last1=Cahen\n|first1=P-J.\n|last2=Chabert\n|first2=J-L.\n|title=Integer-valued polynomials\n|series=Mathematical Surveys and Monographs\n|volume=48\n|publisher=American Mathematical Society\n|location=Providence, RI\n|year=1997\n}}\n* {{citation | last=Pólya | first=G. | authorlink=George Pólya | title=Über ganzwertige ganze Funktionen | language=German | journal=Palermo Rend. | volume=40 | pages=1–16 | year=1915 | issn=0009-725X | jfm=45.0655.02 }}\n\n===Algebraic topology===\n* {{citation\n|author1=A. Baker |author2=F. Clarke |author3=N. Ray |author4=L. Schwartz |title=On the Kummer congruences and the stable homotopy of ''BU''\n|journal=Trans. Amer. Math. Soc.\n|volume=316\n|issue=2\n|year=1989\n|pages=385–432\n|doi=10.2307/2001355\n|publisher=Transactions of the American Mathematical Society, Vol. 316, No. 2\n|jstor=2001355\n}}\n\n* {{citation\n|author=T. Ekedahl\n|title=On minimal models in integral homotopy theory\n|journal=Homology Homotopy Appl.\n|volume=4\n|issue=2\n|year=2002\n|pages=191–218\n|url=http://projecteuclid.org/euclid.hha/1139852462\n|zbl = 1065.55003}}\n\n* {{citation\n|author=J. Hubbuck\n|title=Numerical forms\n|journal=J. London Math. Soc. |series=Series 2\n|volume=55\n|year=1997\n|issue=1\n|pages=65–75\n|doi=10.1112/S0024610796004395\n}}\n\n==Further reading==\n* {{cite book | last=Narkiewicz | first=Władysław | title=Polynomial mappings | series=Lecture Notes in Mathematics | volume=1600 | location=Berlin | publisher=[[Springer-Verlag]] | year=1995 | isbn=3-540-59435-3 | issn=0075-8434 | zbl=0829.11002 }}\n\n{{DEFAULTSORT:Integer-Valued Polynomial}}\n[[Category:Polynomials]]\n[[Category:Number theory]]\n[[Category:Commutative algebra]]\n[[Category:Ring theory]]"
    },
    {
      "title": "Jacobian conjecture",
      "url": "https://en.wikipedia.org/wiki/Jacobian_conjecture",
      "text": "{{Infobox mathematical statement\n| name = Jacobian conjecture\n| image = \n| caption = \n| field = [[Algebraic geometry]]\n| conjectured by = [[Ott-Heinrich Keller]]\n| conjecture date = 1939\n| first proof by = \n| first proof date = \n| open problem = \n| known cases =\n| implied by =\n| equivalent to = [[Dixmier conjecture]]\n| generalizations = \n| consequences =\n}}\n\nIn [[mathematics]], the '''Jacobian conjecture''' is a famous unsolved problem on [[polynomial]]s in several [[Variable (mathematics)|variables]]. It states that if a polynomial function from an ''n''-dimensional space to itself has Jacobian determinant which is a constant, then the function has a polynomial inverse. It was first conjectured in 1939 by [[Ott-Heinrich Keller]], and widely publicized by [[Shreeram Abhyankar]], as an example of a difficult question in [[algebraic geometry]] that can be understood using little beyond a knowledge of [[calculus]].\n\nThe Jacobian conjecture is notorious for the large number of attempted proofs that turned out to contain subtle errors. As of 2018, there are no plausible claims to have proved it. Even the two-variable case has resisted all efforts. There are no known compelling reasons for believing it to be true, and according to {{harvtxt|van den Essen|1997}} there are some suspicions that the conjecture is in fact false for large numbers of variables.  The Jacobian conjecture is number 16 in [[Smale's problems|Stephen Smale's 1998 list of Mathematical Problems for the Next Century]].\n\n==The Jacobian determinant==\nLet ''N'' > 1 be a fixed integer and consider  polynomials ''f''<sub>1</sub>, ..., ''f''<sub>''N''</sub> in variables ''X''<sub>1</sub>, ..., ''X''<sub>''N''</sub> with [[coefficient]]s  in a field ''k''. Then we define a [[vector-valued function]] ''F'': ''k<sup>N</sup>'' → ''k''<sup>''N''</sup> by setting:\n\n: ''F''(''X''<sub>1</sub>, ..., ''X''<sub>''N''</sub>) = (''f''<sub>1</sub>(''X''<sub>1</sub>, ...,''X''<sub>''N''</sub>),..., ''f''<sub>''N''</sub>(''X''<sub>1</sub>,...,''X''<sub>''N''</sub>)).\n\n(''F'' is a [[polynomial mapping]].)\n\nThe [[Jacobian matrix and determinant|Jacobian determinant]] of ''F'', denoted by ''J<sub>F</sub>'', is defined as the [[determinant]] of the ''N'' × ''N'' [[Jacobian matrix]] consisting of the [[partial derivative]]s of ''f<sub>i</sub>'' with respect to ''X<sub>j</sub>'':\n\n:<math>J_F = \\left | \\begin{matrix} \\frac{\\partial f_1}{\\partial X_1} & \\cdots & \\frac{\\partial f_1}{\\partial X_N} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial f_N}{\\partial X_1} & \\cdots & \\frac{\\partial f_N}{\\partial X_N} \\end{matrix} \\right |,</math>\n\nthen ''J<sub>F</sub>'' is itself a polynomial function of the ''N'' variables ''X''<sub>1</sub>, ..., ''X<sub>N</sub>''.\n\n==Formulation of the conjecture==\nIt follows from the multivariable chain rule that if ''F'' has a polynomial inverse function ''G'': ''k<sup>N</sup>'' → ''k<sup>N</sup>'', then ''J<sub>F</sub>'' has a polynomial reciprocal, so is a nonzero constant. The Jacobian conjecture is the following partial converse:\n\n<blockquote>'''Jacobian conjecture:''' Let ''k'' have [[Characteristic (algebra)|characteristic]] 0. If ''J<sub>F</sub>'' is a non-zero constant, then ''F'' has an inverse function ''G'': ''k<sup>N</sup>'' → ''k<sup>N</sup>'' which is [[regular map (algebraic geometry)|regular]], meaning its components are polynomials.</blockquote>\n\nAccording to {{Harvtxt|van den Essen|1997}}, the problem was first conjectured by Keller in 1939 for the limited case of two variables and integer coefficients (which has been proved — see {{Section link||Results}}).  \n\nThe obvious analogue of the Jacobian conjecture fails if ''k'' has characteristic ''p''&nbsp;>&nbsp;0 even for 1 variable. The characteristic of a field must be prime, so it is at least 2. The polynomial {{math|''x'' − ''x''<sup>''p''</sup>}} has derivative {{math|1 − ''p x''<sup>''p''−1</sup>}} which is 1 (because ''px'' is 0) but it has no inverse function. However, {{harvtxt|Adjamagbo|1995}} suggested extending the Jacobian conjecture to characteristic {{nowrap|''p'' > 0}} by adding the hypothesis that ''p'' does not divide the degree of the field extension {{nowrap|''k''(''X'') / ''k''(''F'')}}.\n\nThe condition ''J<sub>F</sub>'' ≠ 0 is related to the [[inverse function theorem]] in [[multivariable calculus]]. In fact for smooth functions (and so in particular for polynomials) a smooth local inverse function to ''F'' exists at every point where ''J<sub>F</sub>'' is non-zero. For example, the map x → ''x''&nbsp;+&nbsp;''x''<sup>3</sup> has a smooth global inverse, but the inverse is not polynomial.\n\n==Results==\n{{harvtxt|Wang|1980}} proved the Jacobian conjecture  for polynomials of [[degree of a polynomial|degree]] 2, and {{harvtxt|Bass|Connell|Wright|1982}} showed that the general case follows from the special case where the polynomials are of degree&nbsp;3, or even  more specifically, of cubic homogeneous type, meaning  of the form ''F''&nbsp;=&nbsp;(''X''<sub>1</sub>&nbsp;+&nbsp;''H''<sub>1</sub>,&nbsp;...,&nbsp;''X''<sub>''n''</sub>&nbsp;+&nbsp;''H''<sub>''n''</sub>), where each ''H''<sub>''i''</sub> is either zero or a homogeneous cubic. {{harvtxt|Drużkowski|1983}}  showed that one may further assume that  the map is of cubic linear type, meaning that the nonzero ''H''<sub>''i''</sub> are cubes of homogeneous linear polynomials.  These reductions introduce additional variables and so are not available for fixed ''N''.\n\n{{harvtxt|Connell|van den Dries|1983}} proved that if the Jacobian conjecture is false, then it has a counterexample with integer coefficients and Jacobian determinant 1.  In consequence, the Jacobian conjecture is true either for all fields of characteristic 0 or for none.  For fixed ''N'', it is true if it holds for at least one algebraically closed field of characteristic 0.\n\nLet ''k''[''X''] denote the polynomial ring {{nowrap|''k''[''X''<sub>1</sub>, ..., ''X''<sub>''n''</sub>]}} and ''k''[''F''] denote the ''k''-subalgebra generated by ''f''<sub>1</sub>, ..., ''f''<sub>''n''</sub>.  For a given ''F'', the Jacobian conjecture is true if, and only if,  {{nowrap|''k''[''X''] {{=}} ''k''[''F'']}}. Keller (1939) proved the birational case, that is, where the two fields ''k''(''X'') and ''k''(''F'') are equal. The case where ''k''(''X'') is a Galois extension of ''k''(''F'') was proved by {{harvtxt|Campbell|1973}} for complex maps and in general by {{harvtxt|Razar|1979}} and, independently, {{harvtxt|Wright|1981}}. {{harvtxt|Moh|1983}} checked the conjecture for polynomials of degree at most 100 in two variables.\n\n{{harvtxt|de Bondt|van den Essen|2005, 2005}} and {{harvtxt|Drużkowski|2005}} independently showed that it is enough to prove the Jacobian Conjecture for complex maps of cubic homogeneous type  with a symmetric Jacobian matrix, and further showed that the conjecture holds for maps of cubic linear type with a symmetric Jacobian matrix, over any field of characteristic 0.\n\nThe strong real Jacobian conjecture was that a real polynomial map with a nowhere vanishing Jacobian determinant has a smooth global inverse.  That is equivalent to asking whether such a map is topologically a proper map, in which case it is a covering map of a simply connected manifold, hence invertible. {{harvs|txt|last=Pinchuk|first= Sergey|year=1994}} constructed two variable counterexamples of total degree 25 and higher.\n\nIt is well-known that the [[Dixmier conjecture]] implies the Jacobian conjecture (see Bass et al. 1982). Conversely, it is shown by [[#{{harvid|Tsuchimoto|2005}}|Yoshifumi Tsuchimoto (2005)]], and independently by {{harvs|txt | last1=Belov-Kanel | first1=Alexei | last2=[[Maxim Kontsevich|Kontsevich]] | first2=Maxim |year=2007 }}, that the Jacobian conjecture for 2N variables implies the [[Dixmier conjecture]] for N dimension. A self-contained and purely algebraic proof of the last implication is also given by  {{harvs|txt|first=P. K.|last1= Adjamagbo|authorlink1=Pascal Kossivi Adjamagbo|first2= A.|last2= van den Essen|year=2007}} who also proved in the same paper that these two conjectures is equivalent to Poisson conjecture.\n\n==References==\n*{{citation|mr=1352692\n|last=Adjamagbo|first= Kossivi\n|chapter=On separable algebras over a U.F.D. and the Jacobian conjecture in any characteristic|title= Automorphisms of affine spaces (Curaçao, 1994)|pages= 89–103|publisher= Kluwer Acad. Publ.|place= Dordrecht|year= 1995}}\n*{{citation|last= Adjamagbo|first=P. K.|title= A proof of the equivalence of the Dixmier, Jacobian and Poisson conjectures|url=http://journals.math.ac.vn/acta/pdf/0702205.pdf|year=2007|last2= van den Essen|first2= A.|journal= Acta Math. Vietnam.|volume= 32 |pages= 205–214|mr=2368008}}\n*{{Citation | last1=Bass | first1=Hyman | last2=Connell | first2=Edwin H. | last3=Wright | first3=David | title=The Jacobian conjecture: reduction of degree and formal expansion of the inverse | doi=10.1090/S0273-0979-1982-15032-7 | issn=1088-9485| mr=663785 | year=1982 | journal=American Mathematical Society. Bulletin. New Series   | volume=7 | issue=2 | pages=287–330}}\n*{{Citation | last1=Belov-Kanel | first1=Alexei | last2=Kontsevich | first2=Maxim | title=The Jacobian conjecture is stably equivalent to the Dixmier conjecture | arxiv=math/0512171 | mr=2337879 | year=2007 | journal=Moscow Mathematical Journal   | volume=7 | issue=2 | pages=209–218 | bibcode=2005math.....12171B | doi=10.17323/1609-4514-2007-7-2-209-218 }}\n* {{citation |last=Campbell |first=L. Andrew |title=A condition for a polynomial map to be invertible |journal=Math. Ann. |volume=205 |issue=3 |year=1973 |pages=243–248 |mr=0324062 |doi=10.1007/bf01349234}} (48 #2414)\n* {{citation|last1=Connell|first1=E.|last2=van den Dries|first2=L.|title=Injective polynomial maps and the Jacobian conjecture|journal=J. Pure Appl. Algebra|volume=28|year=1983|issue=3|pages=235–239|mr=0701351|doi=10.1016/0022-4049(83)90094-4}}\n* {{citation|last1=de Bondt|first1=Michiel|last2=van den Essen|first2=Arno|title=A reduction of the Jacobian conjecture to the symmetric case|journal=Proc. Amer. Math. Soc.|volume=133|year=2005|issue=8|pages=2201–2205 (electronic)|mr=2138860|doi= 10.1090/S0002-9939-05-07570-2}}\n* {{citation|last1=de Bondt|first1=Michiel|last2=van den Essen|first2=Arno|title=The Jacobian conjecture for symmetric Drużkowski mappings|journal=Ann. Polon. Math. |volume=86|year=2005|issue=1|pages=43–46|mr=2183036|doi=10.4064/ap86-1-5}}\n* {{citation|last=Drużkowski|first=Ludwik M.|title=An effective approach to Keller's Jacobian conjecture|journal=Math. Ann.|volume= 264|year=1983|issue=3|pages=303–313|mr=0714105|doi=10.1007/bf01459126}}\n* {{citation|last=Drużkowski|first=Ludwik M.|title=The Jacobian conjecture: symmetric reduction and solution in the symmetric cubic linear case|journal=Ann. Polon. Math.|volume=87|year=2005|pages=83–92|mr=2208537|doi=10.4064/ap87-0-7}}\n*{{Citation | last1=Keller | first1=Ott-Heinrich | title=Ganze Cremona-Transformationen | doi=10.1007/BF01695502 | year=1939 | journal=Monatshefte für Mathematik und Physik | issn=0026-9255 | volume=47 | issue=1 | pages=299–306}}\n*{{Citation | last1=Moh | first1=T. T. | title=On the Jacobian conjecture and the configurations of roots | url=http://resolver.sub.uni-goettingen.de/purl?GDZPPN002200376 |mr=691964 | year=1983 | journal=[[Journal für die reine und angewandte Mathematik]] | issn=0075-4102 | volume=340 | issue=340 | pages=140–212 | doi=10.1515/crll.1983.340.140}}\n*{{Citation | last1=Moh | first1=T. T. | title=On the global Jacobian conjecture for polynomials of degree less than 100|series=preprint}}\n*{{citation|last=Pinchuk|first= Sergey|title= A counterexample to the strong real Jacobian conjecture|journal= Math. Z.|volume= 217 |year=1994|issue= 1|pages= 1–4|mr=1292168|doi=10.1007/bf02571929}}\n* {{cite journal | last1 = Razar | first1 = Michael | year = 1979 | title = Polynomial maps with constant Jacobian | url = | journal = Israel J. Math. | volume = 32 | issue = 2–3| pages = 97–106 | mr=0531253 | doi=10.1007/bf02764906}} (80m:14009)\n*{{citation|last=van den Essen|first= Arno|title= Polynomial automorphisms and the Jacobian conjecture|series= Progress in Mathematics|volume= 190|publisher= Birkhäuser Verlag|place= Basel|year= 2000|isbn= 978-3-7643-6350-5 |mr=1790619|doi=10.1007/978-3-0348-8440-2}}\n*{{citation|last=van den Essen|first= Arno|chapter= Polynomial automorphisms and the Jacobian conjecture|title= Algèbre non commutative, groupes quantiques et invariants (Reims, 1995)|pages= 55–81|series= Sémin. Congr.|volume= 2|publisher= Soc. Math. France|place= Paris|year= 1997|mr=1601194|chapter-url=http://emis.mi.ras.ru/journals/SC/1997/2/pdf/smf_sem-cong_2_55-81.pdf}}\n*{{Cite journal|last=Tsuchimoto|first=Yoshifumi|year=2005|title=Endomorphisms of Weyl algebra and $p$-curvatures|url=http://projecteuclid.org/euclid.ojm/1153494387|journal=Osaka Journal of Mathematics|volume=42|issue=2|pages=435–452|issn=0030-6126|ref={{harvid|Tsuchimoto|2005}}}}\n*{{Citation | last1=Wang | first1=Stuart Sui-Sheng | title=A Jacobian criterion for separability | journal=Journal of Algebra | volume=65 | issue=2 | pages=453–494 | url= |date=August 1980 | doi=10.1016/0021-8693(80)90233-1}}\n* {{cite journal | last1 = Wright | first1 = David | year = 1981 | title = On the Jacobian conjecture | url = | journal = Illinois J. Math. | volume = 25 | issue = 3| pages = 423–440 |mr=0620428 }} (83a:12032)\n\n==External links==\n*[http://www.math.purdue.edu/~ttm/jacobian.html Web page of T. T. Moh on the conjecture]\n\n{{DEFAULTSORT:Jacobian Conjecture}}\n[[Category:Polynomials]]\n[[Category:Algebraic geometry]]\n[[Category:Conjectures]]"
    },
    {
      "title": "Jones polynomial",
      "url": "https://en.wikipedia.org/wiki/Jones_polynomial",
      "text": "In the mathematical field of [[knot theory]], the '''Jones polynomial''' is a [[knot polynomial]] discovered by [[Vaughan Jones]] in 1984.<ref>{{Cite journal|last=Jones |first=Vaughan F.R. |authorlink=Vaughan Jones| title=A polynomial invariant for knots via von Neumann algebra | year=1985 | journal=[[Bulletin of the American Mathematical Society]]|series= (N.S.) | volume=12 | pages=103–111|mr=0766964 | doi=10.1090/s0273-0979-1985-15304-2}}</ref>,<ref>{{Cite journal|last=Jones |first=Vaughan F.R. |authorlink=Vaughan Jones| mr=0908150| title= Hecke algebra representations of braid groups and link polynomials|journal= Annals of Mathematics| series=(2) | volume= 126 |year=1987|issue= 2|pages= 335–388|doi=10.2307/1971403|jstor=1971403 }}</ref>.  Specifically, it is an [[knot invariant|invariant]] of an oriented [[knot (mathematics)|knot]] or [[link (knot theory)|link]] which assigns to each oriented knot or link a [[Laurent polynomial]] in the variable <math>t^{1/2}</math> with integer coefficients.<ref>{{cite web |title=Jones Polynomials, Volume and Essential Knot Surfaces: A Survey |url=https://math.byu.edu/~jpurcell/papers/fkp-survey7.pdf }}</ref>\n\n==Definition by the bracket==\n[[Image:Reidemeister move 1.png|thumb|upright|Type I Reidemeister move]]\nSuppose we have an [[Link (knot theory)|oriented link]] <math>L</math>, given as a [[knot diagram]].  We will define the Jones polynomial, <math>V(L)</math>, using Kauffman's [[bracket polynomial]], which we denote by <math>\\langle~\\rangle</math>.  Note that here the bracket polynomial is a Laurent polynomial in the variable <math>A</math> with integer coefficients.\n\nFirst, we define the auxiliary polynomial (also known as the normalized bracket polynomial) \n:<math>X(L) = (-A^3)^{-w(L)}\\langle L \\rangle, </math>\n\nwhere <math>w(L)</math> denotes the [[writhe]] of <math>L</math> in its given diagram.  The writhe of a diagram is the number of positive crossings (<math>L_{+}</math> in the figure below) minus the number of negative crossings (<math>L_{-}</math>). The writhe is not a knot invariant.\n\n<math>X(L)</math> is a knot invariant since it is invariant under changes of the diagram of <math>L</math> by the three [[Reidemeister move]]s.  Invariance under type II and III Reidemeister moves follows from invariance of the bracket under those moves.  The bracket polynomial is known to change by multiplication by <math>-A^{\\pm 3}</math> under a type I Reidemeister move.  The definition of the <math>X</math> polynomial given above is designed to nullify this change, since the writhe changes appropriately by <math>+1</math> or <math>-1</math> under type I moves.\n\nNow make the substitution <math>A = t^{-1/4} </math> in <math>X(L)</math> to get the Jones polynomial <math>V(L)</math>.  This results in a Laurent polynomial with integer coefficients in the variable <math>t^{1/2}</math>.\n\n===Jones polynomial for tangles===\n\nThis construction of the Jones polynomial for [[Tangle (mathematics)|tangles]] is a simple generalization of the [[Bracket polynomial|Kauffman bracket]] of a link. The construction was developed by [[Vladimir Turaev]] and published in 1990.<ref>{{cite journal|last1=Turaev|first1=Vladimir G.|authorlink=Vladimir Turaev| title=Jones-type invariants of tangles|journal=Journal of Mathematical Sciences| date=1990|volume=52|pages=2806–2807|doi=10.1007/bf01099242}}</ref>\n\nLet <math>k</math> be a non-negative integer and <math>S_k</math> denote the set of all isotopic types of tangle diagrams, with <math>2k</math> ends, having no crossing points and no closed components (smoothings). Turaev's construction makes use of the previous construction for the Kauffman bracket and associates to each <math>2k</math>-end oriented tangle an element of the free <math>\\mathrm{R}</math>-module <math>\\mathrm{R}[S_k]</math>, where  <math>\\mathrm{R}</math> is the [[Ring (mathematics)|ring]] of [[Laurent polynomial]]s with integer coefficients in the variable <math>t^{1/2}</math>.\n\n==Definition by braid representation==\n\nJones' original formulation of his polynomial came from his study of operator algebras.  In Jones' approach, it resulted from a kind of \"trace\" of a particular braid representation into an algebra which originally arose while studying certain models, e.g. the [[Potts model]], in [[statistical mechanics]].\n\nLet a link ''L'' be given.  A [[Alexander's theorem|theorem of Alexander's]] states that it is the trace closure of a braid, say with ''n'' strands.<!-- trace closure here is the one that is NOT the plat closure -->  Now define a representation <math>\\rho</math> of the [[braid group]] on ''n'' strands, ''B<sub>n</sub>'', into the [[Temperley–Lieb algebra]] <math>\\operatorname{TL}_n</math> with coefficients in <math>\\Z [A, A^{-1}]</math> and <math>\\delta = -A^2 - A^{-2}</math>.<!-- the defn of the algebra here is not the same as currently in the Temperly–Lieb article, but is another standard one; that article should either be changed or mention the alternative -->  The standard braid generator <math>\\sigma_i</math> is sent to <math>A\\cdot e_i + A^{-1}\\cdot 1</math>, where <math>1, e_1, \\dots, e_{n-1}</math> are the standard generators of the Temperley–Lieb algebra.  It can be checked easily that this defines a representation.\n\nTake the braid word <math>\\sigma</math> obtained previously from <math>L</math> and compute <math>\\delta^{n-1} \\operatorname{tr} \\rho(\\sigma)</math> where <math>\\operatorname{tr}</math> is the [[Markov trace]]. This gives <math>\\langle L \\rangle</math>, where <math>\\langle</math> <math>\\rangle</math> is the bracket polynomial.  This can be seen by considering, as [[Louis Kauffman]] did, the Temperley–Lieb algebra as a particular diagram algebra.<!-- Diagram algebra is what Kauffman says in his article, but I think by now there is a more standard name for this...maybe Kauffman diagrams? -->\n\nAn advantage of this approach is that one can pick similar representations into other algebras, such as the ''R''-matrix representations, leading to \"generalized Jones invariants\".\n\n==Properties==\nThe Jones polynomial is characterized by taking the value 1 on any diagram of the unknot and satisfies the following [[skein relation]]:\n\n::<math> (t^{1/2} - t^{-1/2})V(L_0)  = t^{-1}V(L_{+}) - tV(L_{-}) \\,</math>\n\nwhere <math>L_{+}</math>, <math>L_{-}</math>, and <math>L_{0}</math> are three oriented link diagrams that are identical except in one small region where they differ by the crossing changes or smoothing shown in the figure below:\n\n[[Image:Skein (HOMFLY).svg|center|200px]]\n\nThe definition of the Jones polynomial by the bracket makes it simple to show that for a knot <math>K</math>, the Jones polynomial of its mirror image is given by substitution of <math>t^{-1}</math> for <math>t</math> in <math>V(K)</math>. Thus, an '''[[Chiral knot|amphicheiral knot]]''', a knot equivalent to its mirror image, has [[palindromic]] entries in its Jones polynomial. See the article on [[skein relation]] for an example of a computation using these relations.\n\nAnother remarkable property of this invariant states that the Jones polynomial of an alternating link is an alternating polynomial. This property was proved by [[Morwen Thistlethwaite]] <ref>{{cite journal|last=Thistlethwaite|first=Morwen B.|authorlink=Morwen Thistlethwaite| title=A spanning tree expansion of the Jones polynomial|journal=[[Topology (journal)|Topology]]| date=1987|volume=26|issue=3|pages=297–309|url=http://www.sciencedirect.com/science/article/pii/0040938387900036|doi=10.1016/0040-9383(87)90003-6}}</ref> in 1987. Another proof of this last property is due to [[Hernando Burgos-Soto]], who also gave an extension to tangles<ref>{{cite journal|last=Burgos-Soto|first=Hernando|authorlink=Hernando Burgos-Soto | title=The Jones polynomial and the planar algebra of alternating links|journal=Journal of Knot Theory and its Ramifications|date=2010|volume=19|issue=11|pages=1487–1505|doi=10.1142/s0218216510008510|arxiv=0807.2600}}</ref> of the property.\n\n== Colored Jones polynomial ==\n[[File:NcoloredJonesPolynomial.png|thumb|320px|N colored Jones Polynomial: The ''N'' cables of ''L'' are parallel with each other along the knot ''L'' and each is colored a different color.]]\n\nFor a positive integer ''N'' a ''N''-colored Jones polynomial <math>V_N(L,t)</math> can be defined as the Jones polynomial for ''N'' cables of the knot <math>L</math> as depicted in the right figure. It is associated with an <math>(N + 1)</math>-dimensional [[irreducible representation]] of <math>\\operatorname{SU}(2)</math> . The label ''N'' stands for coloring. Like the ordinary Jones polynomial it can be defined by [[Skein relation]] and is a [[Laurent polynomial]] in one variable ''t'' . The ''N''-colored Jones polynomial <math>V_N(L,t)</math> has the following properties:\n:*<math>V_{X\\oplus Y}(L,t)=V_X(L,t)+V_Y(L,t)</math> where <math>X,Y</math> are two representation space.\n:*<math>V_{X\\otimes Y}(L,t)</math> equals the Jones polynomial of the 2-cables of L with two components labeled by <math>X</math> and <math>Y</math> . So the ''N''-colored Jones polynomial equals the original Jones polynomial of the ''N'' cables of <math>L</math> .\n:*The original Jones polynomial appears as a special case: <math>V(L,t)=V_1(L,t)</math> .\n\n==Relationship to other theories==\n\n===Link with Chern–Simons theory===\nAs first shown by [[Edward Witten]], the Jones polynomial of a given knot <math>\\gamma</math> can be obtained by considering  [[Chern–Simons theory]] on the three-sphere with [[gauge group]] <math>\\mathrm{SU}(2)</math>, and computing the [[vacuum expectation value]] of a [[Wilson loop]] <math>W_F(\\gamma)</math>, associated to <math>\\gamma</math>, and the [[fundamental representation]] <math>F</math> of <math>\\mathrm{SU}(2)</math>.\n\n===Link with quantum knot invariants===\n\nBy substituting <math>e^h</math> the variable <math>t</math> of the Jones polynomial and expanding it as the series of h each of the coefficients turn to be the [[Vassiliev invariant]] of the knot K. In order to unify the Vassiliev invariants (or, finite type invariants), [[Maxim Kontsevich]] constructed the [[Kontsevich integral]]. The value of the Kontsevich integral, which is the infinite sum of 1, 3-valued chord diagrams, named the Jacobi chord diagrams, reproduces the Jones polynomial along with the <math>\\mathfrak{sl}_2</math> weight system studied by [[Dror Bar-Natan]].\n\n===Link with the volume conjecture===\n\nBy numerical examinations on some hyperbolic knots, [[Rinat M. Kashaev]] discovered that substituting the [[primitive n-th root of unity|''n''-th root of unity]] into the parameter of the [[#Colored Jones polynomial|colored Jones polynomial]] corresponding to the ''n''-dimensional representation, and limiting it as ''n'' grows to infinity, the limit value would give the [[hyperbolic volume]] of the [[knot complement]]. (See [[Volume conjecture]].)\n\n===Link with Khovanov homology===\n\nIn 2000 [[Mikhail Khovanov]] constructed a certain chain complex for knots and links and showed that the homology induced from it is a knot invariant (see [[Khovanov homology]]). The Jones polynomial is described as the [[Euler characteristic]] for this homology.\n\n==Open problems==\n*Is there a nontrivial knot with Jones polynomial equal to that of the [[unknot]]?  It is known that there are nontrivial ''links'' with Jones polynomial equal to that of the corresponding [[unlink]]s by the work of [[Morwen Thistlethwaite]].\n\n\n;Problem（Extension of Jones polynomial togeneral 3-manifolds）　\n``The original Jones polynomial was defined for 1-links in the 3-sphere(the 3-ball, the 3-space R3). Can you define the Jones polynomial for 1-links in any 3-manifold?’’\n\nSee section 1.1 of this paper \n<ref>\n{{citation |first=L.H |last=Kauffman \n|first2=E |last2=Ogasa \n|first3=J |last3=Shcneider \n|arxiv=1808.03023|\ntitle= A spinning construction for virtual 1-knots and 2-knots, and the fiberwise and welded equivalence of virtual 1-knots|year=2018}} \n</ref>\nfor the background and the history of this problem. \nKauffman submitted a solution in the case of the product manifold of closed oriented surface and the closed interval, by introducing virtual 1-knots\n<ref>\n{{citation|first=L.E. |last=Kauffman \n|arxiv=math/9811028| \ntitle=  Talks at MSRI Meeting in January 1997, AMS Meeting at University of Maryland, College Park in March 1997, Isaac Newton Institute Lecture in November 1997, Knots in Hellas Meeting in Delphi, Greece in July 1998, APCTP-NANKAI Symposium on Yang-Baxter Systems, Non-Linear Models and Applications at Seoul, Korea in October 1998, Virtual knot theory, European J. Combin. 20 (1999) 663-690 \n|year=1998 \n}} \n</ref>.\nIt is open in the other cases. Witten’s path integral for Jones polynomial is written for links in any compact 3-manifold formally, but the calculus is not done even in physics level in any case other than the 3-sphere (the 3-ball, the 3-space R3). This problem is also open in physics level. In the case of Alexander polynomial, this problem is solved.\n\n==See also==\n*[[HOMFLY polynomial]]\n*[[Khovanov homology]]\n*[[Alexander polynomial]]\n\n==Notes==\n{{Reflist}}\n\n==References==\n*[[Vaughan Jones]], [http://math.berkeley.edu/~vfr/jones.pdf ''The Jones Polynomial'']\n*[[Colin Adams (mathematician)|Colin Adams]], ''The Knot Book'', American Mathematical Society, {{isbn|0-8050-7380-9}}\n*{{cite journal|last=Kauffman|first=Louis H.|authorlink=Louis Kauffman|title=State models and the Jones polynomial|journal=Topology|year=1987|volume=26|issue=3|pages=395–407|doi=10.1016/0040-9383(87)90009-7|url=http://www.sciencedirect.com/science/article/pii/0040938387900097|accessdate=22 December 2012}} (explains the definition by bracket polynomial and its relation to Jones' formulation by braid representation)\n*{{cite book|last=Lickorish|first=W. B. Raymond|authorlink=W. B. R. Lickorish| title=An introduction to knot theory|year=1997|publisher=Springer|location=New York; Berlin; Heidelberg; Barcelona; Budapest; Hong Kong; London; Milan; Paris; Santa Clara; Singapore; Tokyo|isbn=978-0-387-98254-0|page=175|url=https://www.springer.com/mathematics/geometry/book/978-0-387-98254-0}} \n*{{cite journal|last=Thistlethwaite|first=Morwen|authorlink=Morwen Thistlethwaite|title=Links with trivial Jones polynomial|journal=[[Journal of Knot Theory and Its Ramifications]]|year=2001|volume=10|issue=4|pages=641–643|doi=10.1142/S0218216501001050}}\n*{{cite journal|last=Eliahou|first=Shalom|author2=Kauffman, Louis H. |author3=Thistlethwaite, Morwen B. |title=Infinite families of links with trivial Jones polynomial|journal=Topology|year=2003|volume=42|issue=1|pages=155–169|doi=10.1016/S0040-9383(02)00012-5|url=http://www.sciencedirect.com/science/article/pii/S0040938302000125}}\n\n==External links==\n* {{springer|title=Jones-Conway polynomial|id=p/j130040}}\n* [http://www.math.uic.edu/~kauffman/tj.pdf Links with trivial Jones polynomial] by [[Morwen Thistlethwaite]]\n* {{Knot Atlas|The Jones Polynomial}}\n\n{{Knot theory}}\n\n[[Category:Knot theory]]\n[[Category:Polynomials]]"
    },
    {
      "title": "Kauffman polynomial",
      "url": "https://en.wikipedia.org/wiki/Kauffman_polynomial",
      "text": "{{distinguish|Kauffman bracket}}\nIn [[knot theory]], the '''Kauffman polynomial''' is a 2-variable [[knot polynomial]] due to [[Louis Kauffman]].<ref>{{Cite journal|last=Kauffman|first=Louis|authorlink=Louis Kauffman|date=1990|title=An invariant of regular isotopy| url=http://homepages.math.uic.edu/~kauffman/IRH.pdf|journal=[[Transactions of the American Mathematical Society]]|volume=318|issue=2|pages=417–471|doi=10.1090/S0002-9947-1990-0958895-7|mr=0958895}}</ref> It is initially defined on a [[link (knot theory)|link]] diagram as\n\n:<math>F(K)(a,z)=a^{-w(K)}L(K)\\,</math>,\n\nwhere <math>w(K)</math> is the [[writhe]] of the link diagram and <math>L(K)</math> is a polynomial in ''a'' and ''z'' defined on link diagrams by the following properties:\n\n*<math>L(O) = 1</math> (O is the unknot).\n*<math>L(s_r)=aL(s), \\qquad L(s_\\ell)=a^{-1}L(s).</math>\n*''L'' is unchanged under type II and III [[Reidemeister move]]s.\n\nHere <math>s</math> is a strand and <math>s_r</math> (resp. <math>s_\\ell</math>) is the same strand with a right-handed (resp. left-handed) curl added (using a type I Reidemeister move).\n\nAdditionally ''L'' must satisfy Kauffman's [[skein relation]]:\n\n:[[Image:Kauffman poly.png|400px]]\n\nThe pictures represent the ''L'' polynomial of the diagrams which differ inside a disc as shown but are identical outside.\n\nKauffman showed that ''L'' exists and is a [[regular isotopy]] invariant of unoriented links.  It follows easily that ''F'' is an [[ambient isotopy]] invariant of oriented links.\n\nThe [[Jones polynomial]] is a special case of the Kauffman polynomial, as the ''L'' polynomial specializes to the [[bracket polynomial]]. The Kauffman polynomial is related to [[Chern–Simons theory|Chern–Simons gauge theories]] for SO(N) in the same way that the [[HOMFLY polynomial]] is related to Chern–Simons gauge theories for SU(N).<ref>{{Cite journal|last=Witten|first=Edward|authorlink=Edward Witten|date=1989|title=Quantum field theory and the Jones polynomial| url=https://projecteuclid.org/euclid.cmp/1104178138|journal=[[Communications in Mathematical Physics]]|volume=121|issue=3|pages=351–399|doi=10.1007/BF01217730|mr=0990772}}</ref>\n\n== References ==\n<references />\n\n==Further reading==\n*{{Cite book|last=Kauffman|first=Louis|authorlink=Louis Kauffman |title=On Knots|year=1987|isbn=0-691-08435-1|series=Annals of Mathematics Studies|volume= 115|publisher= [[Princeton University Press]]|location= Princeton, NJ|mr=0907872}}\n\n==External links==\n*{{cite web|url=http://eom.springer.de/k/k120040.htm| publisher=Springer EoM| title= Entry for Kauffman polynomial}}\n*{{Knot Atlas|The Kauffman Polynomial}}\n\n{{Knot theory}}\n\n[[Category:Knot theory]]\n[[Category:Polynomials]]\n\n{{knottheory-stub}}"
    },
    {
      "title": "Kharitonov region",
      "url": "https://en.wikipedia.org/wiki/Kharitonov_region",
      "text": "{{multiple issues|\n{{Orphan|date=February 2013}}\n{{Expert-subject|Mathematics|date=February 2009}}\n}}\n\nA '''Kharitonov region''' is a concept in [[mathematics]].  It arises in the study of the [[Stable polynomial|stability of polynomials]].\n\nLet <math>D</math> be a [[simply-connected set]] in the [[complex plane]] and let <math>P</math> be the polynomial family.\n\n<math>D</math> is said to be a '''Kharitonov region''' if\n\n:<math>V_T^n(V_S^n)</math>\n\nis a subset of <math>P.</math> Here, <math>V_T^n</math>  denotes the set of all [[vertex polynomial]]s of complex interval polynomials <math>(T^n)</math> and <math>V_S^n</math> denotes the set of all vertex polynomials of real interval polynomials <math>(S^n).</math>\n\n==See also==\n*[[Kharitonov's theorem]]\n\n==References==\n{{reflist}}\n* Y C Soh and Y K Foo (1991), “Kharitonov Regions: It Suffices to Check a Subset of Vertex Polynomials”, IEEE Trans. on Aut. Cont., 36, 1102 – 1105.\n\n[[Category:Polynomials]]\n\n\n{{algebra-stub}}"
    },
    {
      "title": "Kharitonov's theorem",
      "url": "https://en.wikipedia.org/wiki/Kharitonov%27s_theorem",
      "text": "{{no footnotes|date=March 2012}}\n'''Kharitonov's theorem''' is a result used in [[control theory]] to assess the [[stability (mathematics)|stability]] of a [[dynamical system]] when the physical parameters of the system are not known precisely. When the coefficients of the [[characteristic polynomial]] are known, the [[Routh-Hurwitz stability criterion]] can be used to check if the system is stable (i.e. if all [[root]]s have negative real parts). Kharitonov's theorem can be used in the case where the coefficients are only known to be within specified ranges. It provides a test of stability for a so-called [[interval polynomial]], while Routh-Hurwitz is concerned with an ordinary [[polynomial]].\n\n==Definition== \nAn interval polynomial is the family of all polynomials\n:<math>\n                        p(s)= a_0 + a_1 s^1 + a_2 s^2 + ... + a_n s^n\n</math>\nwhere each coefficient <math>a_i \\in R</math> can take any value in the specified intervals\n:<math>\n                           l_i \\le a_i \\le u_i.\n</math>\nIt is also assumed that the leading coefficient cannot be zero: <math>0 \\notin [l_n, u_n]</math>.\n\n==Theorem==\nAn interval polynomial is stable (i.e. all members of the family are stable) if and only if the four so-called '''Kharitonov polynomials'''\n:<math>k_1(s) = l_0 + l_1 s^1 + u_2 s^2 +  u_3 s^3 + l_4 s^4 + l_5 s^5 + \\cdots </math>\n:<math>k_2(s) = u_0 + u_1 s^1 + l_2 s^2 +  l_3 s^3 + u_4 s^4 + u_5 s^5 + \\cdots </math>\n:<math>k_3(s) = l_0 + u_1 s^1 + u_2 s^2 +  l_3 s^3 + l_4 s^4 + u_5 s^5 + \\cdots </math>\n:<math>k_4(s) = u_0 + l_1 s^1 + l_2 s^2 +  u_3 s^3 + u_4 s^4 + l_5 s^5 + \\cdots </math>\nare stable.\n\nWhat is somewhat surprising about Kharitonov's result is that although in principle we are testing an infinite number of polynomials for stability, in fact we need to test only four.  This we can do using Routh-Hurwitz or any other method.  So it only takes four times more work to be informed about the stability of an interval polynomial than it takes to test one ordinary polynomial for stability.\n\nKharitonov's theorem is useful in the field of [[robust control]], which seeks to design systems that will work well despite uncertainties in component behavior due to [[measurement error]]s, changes in operating conditions, equipment wear and so on.\n\n==References==\n\n*''[[V. L. Kharitonov]], \"Asymptotic stability of an equilibrium position of a family of systems of differential equations\", ''Differentsialnye uravneniya'', 14 (1978), 2086-2088. {{ru icon}}\n*[http://www.apmath.spbu.ru/en/staff/kharitonov/index.html Academic home page of Prof. V. L. Kharitonov]\n\n[[Category:Control theory]]\n[[Category:Polynomials]]\n[[Category:Theorems in dynamical systems]]\n[[Category:Circuit theorems]]"
    },
    {
      "title": "Knot polynomial",
      "url": "https://en.wikipedia.org/wiki/Knot_polynomial",
      "text": "[[Image:Skein (HOMFLY).svg|thumb|Many knot polynomials are computed using [[skein relations]], which allow one to change the different crossings of a knot to get simpler knots.]]\n\nIn the [[mathematics|mathematical]] field of [[knot theory]], a '''knot polynomial''' is a [[knot invariant]] in the form of a [[polynomial]] whose coefficients encode some of the properties of a given [[knot (mathematics)|knot]].\n\n==History==\nThe first knot polynomial, the [[Alexander polynomial]], was introduced by [[James Waddell Alexander II]] in 1923, but other knot polynomials were not found until almost 60 years later.\n\nIn the 1960s, [[John Horton Conway|John Conway]] came up with a [[skein relation]] for a version of the Alexander polynomial, usually referred to as the [[Alexander–Conway polynomial]]. The significance of this skein relation was not realized until the early 1980s, when [[Vaughan Jones]] discovered the [[Jones polynomial]]. This led to the discovery of more knot polynomials, such as the so-called [[HOMFLY polynomial]].\n\nSoon after Jones' discovery, [[Louis Kauffman]] noticed the Jones polynomial could be computed by means of a [[Partition function (statistical mechanics)|partition function]] (state-sum model), which involved the [[bracket polynomial]], an invariant of [[Framed knot|framed knots]].  This opened up avenues of research linking knot theory and [[statistical mechanics]].\n\nIn the late 1980s, two related breakthroughs were made. [[Edward Witten]] demonstrated that the Jones polynomial, and similar Jones-type invariants, had an interpretation in [[Chern–Simons theory]]. [[Victor Anatolyevich Vasilyev|Viktor Vassiliev]] and [[Mikhail Goussarov]] started the theory of [[finite type invariant]]s of knots.  The coefficients of the previously named polynomials are known to be of finite type (after perhaps a suitable \"change of variables\").\n\nIn recent years, the Alexander polynomial has been shown to be related to [[Floer homology]].  The graded [[Euler characteristic]] of the [[Heegaard Floer homology|knot Floer homology]] of [[Peter Ozsváth]] and [[Zoltán Szabó (mathematician)|Zoltan Szabó]] is the Alexander polynomial.\n\n==Example==\n{| class=\"wikitable\"\n|-\n! [[Knot theory#Alexander.E2.80.93Briggs notation|Alexander–Briggs notation]] !! [[Alexander polynomial]] <math>\\Delta(t)</math> !!  [[Alexander polynomial#Alexander.E2.80.93Conway polynomial|Conway polynomial]] <math>\\nabla(z) </math> !! [[Jones polynomial]]  <math>V(q)</math> !! [[HOMFLY polynomial]] <math>H(a,z)</math>\n|-\n|<math>0_1</math> ([[Unknot]])|| <math>1</math> || <math>1</math> || <math>1</math> || <math>1</math>\n|-\n|<math>3_1</math> ([[Trefoil knot|Trefoil Knot]])|| <math>t - 1 + t^{-1}</math> || <math>z^2 + 1</math> || <math>q^{-1} + q^{-3} - q^{-4}</math> || <math>-a^{4}+a^{2}z^{2}+2a^{2}</math>\n|-\n| <math>4_1</math> ([[Figure-eight knot (mathematics)|Figure-eight Knot]]) || <math>-t + 3 - t^{-1}</math> || <math>-z^2+1</math> || <math>q^2 - q + 1 - q^{-1} + q^{-2}</math> || <math>a^{2}+a^{-2}-z^{2}-1</math>\n|-\n| <math>5_1</math> ([[Cinquefoil knot|Cinquefoil Knot]]) || <math>t^2 - t + 1 - t^{-1} + t^{-2}</math> || <math>z^4 + 3z^2 + 1</math> || <math>q^{-2} + q^{-4} - q^{-5} + q^{-6} - q^{-7}</math> || <math>-a^{6}z^{2}-2a^{6}+a^{4}z^{4}+4a^{4}z^{2}+3a^{4}</math>\n|-\n| <math>-</math> ([[Granny knot (mathematics)|Granny Knot]]) || <math>\\left(t - 1 + t^{-1}\\right)^2</math> || <math>\\left(z^2 + 1\\right)^2</math> || <math>\\left(q^{-1} + q^{-3} - q^{-4}\\right)^2</math> || <math>\\left(-a^{4}+a^{2}z^{2}+2a^{2}\\right)^2</math>\n|-\n| <math>-</math> ([[Square knot (mathematics)|Square Knot]]) || <math>\\left(t - 1 + t^{-1}\\right)^2</math> || <math>\\left(z^2 + 1\\right)^2</math> || <math>\\left(q^{-1} + q^{-3} - q^{-4}\\right)\\left(q + q^{3} - q^{4}\\right)</math> || <math>\\left(-a^{4}+a^{2}z^{2}+2a^{2}\\right) \\times</math> <br/> <math>\\left(-a^{-4}+a^{-2}z^{-2}+2a^{-2}\\right)</math>\n|}\n\n[[Knot theory#Alexander.E2.80.93Briggs notation|Alexander–Briggs notation]] is a notation that simply organizes knots by their crossing number. The order of Alexander–Briggs notation of [[prime knot]] is usually sured. (See [[List of prime knots]].)\n\n[[Alexander polynomial]]s and [[Alexander polynomial#Alexander.E2.80.93Conway polynomial|Conway polynomial]]s can ''not'' recognize the difference of left-trefoil knot and right-trefoil knot.\n<gallery widths=\"60px\" heights=\"60px\" align=\"center\">\nImage:Trefoil knot left.svg|The left-trefoil knot.\nImage:TrefoilKnot_01.svg|The right-trefoil knot.\n</gallery>\nSo we have the same situation as the granny knot and square knot since the [[Knot theory#Adding knots|addition]] of knots in <math>\\mathbb{R}^3</math> is the product of knots in [[Knot theory#Knot polynomials|knot polynomials]].\n\n==See also==\n\n===Specific knot polynomials===\n*[[Alexander polynomial]]\n*[[Bracket polynomial]]\n*[[HOMFLY polynomial]]\n*[[Jones polynomial]]\n*[[Kauffman polynomial]]\n\n===Related topics===\n*[[Graph polynomial]], a similar class of polynomial invariants in graph theory\n*[[Tutte polynomial]], a special type of graph polynomial related to the Jones polynomial\n*[[Skein relation]] for a formal definition of the Alexander polynomial, with a worked-out example.\n\n==Further reading==\n*{{cite book |first=Colin |last=Adams |title=The Knot Book |publisher=American Mathematical Society |isbn=0-8050-7380-9 }}\n*{{cite book |first=W. B. R. |last=Lickorish |authorlink=W. B. R. Lickorish |title=An Introduction to Knot Theory |series=[[Graduate Texts in Mathematics]] |volume=175 |publisher=Springer-Verlag |location=New York |year=1997 |isbn=0-387-98254-X }}\n\n{{Knot theory|state=collapsed}}\n\n[[Category:Knot invariants]]\n[[Category:Polynomials]]"
    },
    {
      "title": "Lagrange's theorem (number theory)",
      "url": "https://en.wikipedia.org/wiki/Lagrange%27s_theorem_%28number_theory%29",
      "text": "{{for|Lagrange's theorem|Lagrange's theorem (disambiguation)}}\nIn [[number theory]], '''Lagrange's theorem''' is a statement named after [[Joseph-Louis Lagrange]] about how frequently a polynomial over the integers may evaluate to a multiple of a fixed prime. More precisely, it states that if ''p'' is a [[prime number]] and <math>\\textstyle f(x) \\in \\mathbb{Z}[x]</math> is a polynomial with integer coefficients, then either:\n* every coefficient of ''f''(''x'') is divisible by ''p'', or\n* {{math|''f''(''x'') &equiv; 0 (mod ''p'')}} has at most {{math|deg ''f''(''x'')}} incongruent solutions.\n\nSolutions are \"incongruent\" if they do not differ by a multiple of ''p''. If the [[Modular arithmetic|modulus]] is not prime, then it is possible for there to be more than deg ''f''(''x'') solutions.\n\n==A proof of Lagrange's theorem==\n\nThe two key ideas are the following. Let {{math|''g''(''x'') &isin; ('''Z'''/''p'')[''x'']}} be the polynomial obtained from {{math|''f''(''x'')}} by taking the coefficients {{math|mod ''p''}}. Now:\n\n# {{math|''f''(''k'')}} is divisible by {{mvar|p}} if and only if {{math|''g''(''k'') {{=}} 0}}; and\n# {{math|''g''(''x'')}} has no more than {{math|deg ''g''(''x'')}} roots.\n\nMore rigorously, start by noting that {{math|''g''(''x'') {{=}} 0}} if and only if each coefficient of {{math|''f''(''x'')}} is divisible by {{mvar|p}}. Assume {{math|''g''(''x'') ≠ 0}}; its degree is thus well-defined. It's easy to see {{math|deg ''g''(''x'') ≤ deg ''f''(''x'')}}. To prove (1), first note that we can compute {{math|''g''(''k'')}} either directly, i.e. by plugging in (the residue class of) {{mvar|k}} and performing arithmetic in {{math|'''Z'''/''p''}}, or by reducing {{math|''f''(''k'') mod ''p''}}. Hence {{math|''g''(''k'') {{=}} 0}} if and only if {{math|''f''(''k'') &equiv; 0 (mod ''p'')}}, i.e. if and only if {{math|''f''(''k'')}} is divisible by {{mvar|p}}. To prove (2), note that {{math|'''Z'''/''p''}} is a field, which is a standard fact; a quick proof is to note that since {{mvar|p}} is prime, {{math|'''Z'''/''p''}} is a finite integral domain, hence is a field. Another standard fact is that a non-zero polynomial over a field has at most as many roots as its degree; this follows from the division algorithm.\n\nFinally, note that two solutions {{math|''f''(''k''{{sub|1}}) &equiv; ''f''(''k''{{sub|2}}) &equiv; 0 (mod ''p'')}} are incongruent if and only if <math> k_1 \\not\\equiv k_2 </math> {{math|(mod ''p'')}}. Putting it all together, the number of incongruent solutions by (1) is the same as the number of roots of {{math|''g''(''x'')}}, which by (2) is at most {{math|deg ''g''(''x'')}}, which is at most {{math|deg ''f''(''x'')}}.\n\n==References==\n* {{cite book | last = LeVeque | first = William J. | authorlink = William J. LeVeque | title = Topics in Number Theory, Volumes I and II | publisher = Dover Publications | location = New York | year = 2002 |origyear = 1956 | isbn = 978-0-486-42539-9 | zbl=1009.11001 | page=42 }}\n* {{cite book | title=Elementary Number Theory in Nine Chapters | first=James J. | last=Tattersall | edition=2nd | publisher=[[Cambridge University Press]] | year=2005 | isbn=0-521-85014-2 | zbl=1071.11002  | page=198 }}\n\n{{DEFAULTSORT:Lagrange's Theorem (Number Theory)}}\n[[Category:Theorems about prime numbers]]\n[[Category:Polynomials]]"
    },
    {
      "title": "Laurent polynomial",
      "url": "https://en.wikipedia.org/wiki/Laurent_polynomial",
      "text": "{{Refimprove|date=July 2009}}\n\nIn [[mathematics]], a '''Laurent polynomial''' (named\nafter [[Pierre Alphonse Laurent]]) in one variable over a [[Field (mathematics)|field]] <math>\\mathbb{F}</math> is a [[linear combination]] of positive and negative powers of the variable with coefficients in <math>\\mathbb{F}</math>. Laurent polynomials in ''X'' form a [[Ring (mathematics)|ring]] denoted <math>\\mathbb{F}</math>[''X'', ''X''<sup>&minus;1</sup>].<ref>{{MathWorld|urlname=LaurentPolynomial|title=Laurent Polynomial}}</ref> They differ from ordinary [[polynomial]]s in that they may have terms of negative degree. The construction of Laurent polynomials may be iterated, leading to the ring of Laurent polynomials in several variables.  Laurent polynomials are of particular importance in the study of [[several complex variables|complex variables]].\n\n== Definition ==\nA '''Laurent polynomial''' with coefficients in a field <math>\\mathbb{F}</math> is an expression of the form\n\n: <math> p = \\sum_k p_k X^k, \\quad p_k\\in \\mathbb{F}</math>\n\nwhere ''X'' is a formal variable, the summation index ''k'' is an [[integer]] (not necessarily positive) and only finitely many coefficients ''p''<sub>''k''</sub> are non-zero. Two Laurent polynomials are equal if their coefficients are equal. Such expressions can be added, multiplied, and brought back to the same form by reducing similar terms. Formulas for addition and multiplication are exactly the same as for the ordinary polynomials, with the only difference that both positive and negative powers of ''X'' can be present:\n\n:<math>\\left(\\sum_i a_iX^i\\right) + \\left(\\sum_i b_iX^i\\right) = \n\\sum_i (a_i+b_i)X^i</math>\n\nand\n\n:<math>\\left(\\sum_i a_iX^i\\right) \\cdot \\left(\\sum_j b_jX^j\\right) = \n\\sum_k \\left(\\sum_{i,j: i + j = k} a_i b_j\\right)X^k.</math>\n\nSince only finitely many coefficients ''a''<sub>''i''</sub> and ''b''<sub>''j''</sub> are non-zero, all sums in effect have only finitely many terms, and hence represent Laurent polynomials.\n\n== Properties ==\n\n* A Laurent polynomial over '''C''' may be viewed as a [[Laurent series]] in which only finitely many coefficients are non-zero.\n* The ring of Laurent polynomials ''R''[''X'', ''X''<sup>−1</sup>] is an extension of the [[polynomial ring]] ''R''[''X''] obtained by \"inverting ''X''\". More rigorously, it is the [[localization of a ring|localization]] of the [[polynomial ring]] in the multiplicative set consisting of the non-negative powers of ''X''. Many properties of the Laurent polynomial ring follow from the general properties of localization.\n* The ring of Laurent polynomials is a subring of the [[rational function]]s.\n* The ring of Laurent polynomials over a field is [[Noetherian ring|Noetherian]] (but not [[Artinian ring|Artinian]]).\n* If ''R'' is an integral domain, the units of the Laurent polynomial ring ''R''[''X'', ''X''<sup>−1</sup>] have the form ''uX''<sup>''k''</sup>, where ''u'' is a unit of ''R'' and ''k'' is an integer. In particular, if ''K'' is a [[field (mathematics)|field]] then the units of ''K''[''X'', ''X''<sup>−1</sup>] have the form ''aX''<sup>''k''</sup>, where ''a'' is a non-zero element of ''K''.\n* The Laurent polynomial ring ''R''[''X'', ''X''<sup>−1</sup>] is isomorphic to the [[group ring]] of the group '''Z''' of [[integer]]s over ''R''. More generally, the Laurent polynomial ring in ''n'' variables is isomorphic to the group ring of the [[free abelian group]] of rank ''n''. It follows that the Laurent polynomial ring can be endowed with a structure of a commutative, [[cocommutative]] [[Hopf algebra]].\n\n==See also==\n*[[Jones polynomial]]\n\n==References==\n* Lang, Serge (2002), Algebra, Graduate Texts in Mathematics, 211 (Revised third ed.), New York: Springer-Verlag, {{ISBN|978-0-387-95385-4}}, MR 1878556\n<references/>\n\n{{DEFAULTSORT:Laurent Polynomial}}\n[[Category:Commutative algebra]]\n[[Category:Polynomials]]\n[[Category:Ring theory]]"
    },
    {
      "title": "Lehmer's conjecture",
      "url": "https://en.wikipedia.org/wiki/Lehmer%27s_conjecture",
      "text": "{{Short description|Mathematical conjecture}}\n{{about||Lehmer's conjecture about the non-vanishing of τ(''n'')|Ramanujan's tau function|Lehmer's conjecture about Euler's totient function|Lehmer's totient problem}}\n\n'''Lehmer's conjecture''', also known as the '''Lehmer's Mahler measure problem,''' is a problem in [[number theory]] raised by [[Derrick Henry Lehmer]].<ref name=lmr>{{cite journal | zbl=0007.19904 | last=Lehmer | first=D.H. | authorlink=Derrick Henry Lehmer | title=Factorization of certain cyclotomic functions | journal=Ann. Math. |series=2 | volume=34 | pages=461–479 | year=1933 | issn=0003-486X | doi=10.2307/1968172}}</ref>  The conjecture asserts that there is an absolute constant <math>\\mu>1</math> such that every [[polynomial]] with integer coefficients <math>P(x)\\in\\mathbb{Z}[x]</math> satisfies one of the following properties:\n\n* The [[Mahler measure]] <math>\\mathcal{M}(P(x))</math> of <math>P(x)</math> is greater than or equal to <math>\\mu</math>.\n* <math>P(x)</math> is an integral multiple of a product of cyclotomic polynomials or the monomial <math>x</math>, in which case <math>\\mathcal{M}(P(x))=1</math>. (Equivalently, every complex root of <math>P(x)</math> is a root of unity or zero.)\n\nThere are a number of definitions of the Mahler measure, one of which is to factor <math>P(x)</math> over <math>\\mathbb{C}</math> as\n\n:<math>P(x)=a_0 (x-\\alpha_1)(x-\\alpha_2)\\cdots(x-\\alpha_D),</math>\n\nand then set\n\n:<math>\\mathcal{M}(P(x)) = |a_0| \\prod_{i=1}^{D} \\max(1,|\\alpha_i|).</math>\n\nThe smallest known Mahler measure (greater than 1) is for \"Lehmer's polynomial\"\n\n:<math>P(x)= x^{10}+x^9-x^7-x^6-x^5-x^4-x^3+x+1 \\,,</math>\n\nfor which the Mahler measure is the [[Salem number]]<ref>{{cite book | last=Borwein | first=Peter | authorlink=Peter Borwein | title=Computational Excursions in Analysis and Number Theory | series=CMS Books in Mathematics | publisher=[[Springer-Verlag]] | year=2002 | isbn=0-387-95444-9 | zbl=1020.12001 | page=16 }}</ref>\n\n:<math>\\mathcal{M}(P(x))=1.176280818\\dots \\ .</math>\n\nIt is widely believed that this example represents the true minimal value: that is, <math>\\mu=1.176280818\\dots</math> in Lehmer's conjecture.<ref name=S234>Smyth (2008) p.324</ref><ref name=EPSW30>{{cite book | last1=Everest | first1=Graham | last2=van der Poorten | first2=Alf | author2-link=Alfred van der Poorten | last3=Shparlinski | first3=Igor | last4=Ward | first4=Thomas | title=Recurrence sequences | series=Mathematical Surveys and Monographs | volume=104 | location=[[Providence, RI]] | publisher=[[American Mathematical Society]] | year=2003 | isbn=0-8218-3387-1 | zbl=1033.11006 | page=30 }}</ref>\n\n== Motivation ==\nConsider Mahler measure for one variable and [[Jensen's formula]] shows that if <math>P(x)=a_0 (x-\\alpha_1)(x-\\alpha_2)\\cdots(x-\\alpha_D)</math> then  \n:<math>\\mathcal{M}(P(x)) = |a_0| \\prod_{i=1}^{D} \\max(1,|\\alpha_i|).</math>\nIn this paragraph denote　<math>m(P)=\\log(\\mathcal{M}(P(x))</math> , which is also called [[Mahler measure]].\n\nIf <math>P</math> has integer coefficients, this shows that <math>\\mathcal{M}(P)</math> is an [[algebraic number]] so <math>m(P)</math> is the logarithm of an algebraic integer. It also shows that <math>m(P)\\ge0</math> and that if <math>m(P)=0</math> then <math>P</math> is a product of [[cyclotomic polynomial]]s i.e. monic polynomials whose all roots are roots of unity, or a monomial polynomial of <math>x</math> i.e. a power <math>x^n</math> for some <math>n</math> .\n\nLehmer noticed<ref name=lmr /><ref>David Boyd (1981). \"Speculations concerning the range of Mahler's measure\" Canad. Math. Bull. Vol. 24(4)</ref> that <math>m(P)=0</math> is an important value in the study of the integer sequences <math>\\Delta_n=\\text{Res}(P(x), x^n-1)=\\prod^D_{i=1}(\\alpha_i^n-1)</math> for monic <math>P</math> . If <math>P</math> does not vanish on the circle then <math>\\lim|\\Delta_n|^{1/n}=\\mathcal{M}(P)</math> and this statement might be true even if  <math>P</math> does vanish on the circle. By this he was led to ask\n:whether there is a constant <math>c>0</math> such that <math>m(P)>c</math> provided <math>P</math> is not cyclotomic?,\nor\n:given <math>c>0</math>, are there <math>P</math> with integer coefficients for which <math> 0<m(P)<c </math>?\nSome positive answers have been provided as follows, but Lehmer's conjecture is not yet completely proved and is still a question of much interest.\n\n==Partial results==\n\nLet <math>P(x)\\in\\mathbb{Z}[x]</math> be an irreducible monic polynomial of degree <math>D</math>.\n\nSmyth <ref>{{cite journal | first=C. J. | last=Smyth | title=On the product of the conjugates outside the unit circle of an algebraic integer | journal=Bulletin of the London Mathematical Society | volume=3 | year=1971 | pages=169–175 | zbl=1139.11002 | doi=10.1112/blms/3.2.169}}</ref> proved that Lehmer's conjecture is true for all polynomials that are not [[Reciprocal polynomial|reciprocal]], i.e., all polynomials satisfying <math>x^DP(x^{-1})\\ne P(x)</math>.\n\nBlanksby and [[Hugh Montgomery (mathematician)|Montgomery]]<ref>{{cite journal | first1=P. E. | last1=Blanksby | first2=H. L. | last2=Montgomery | author2-link=Hugh Montgomery (mathematician) | title=Algebraic integers near the unit circle | journal=Acta Arith. | volume=18 | year=1971 | pages=355–369 | zbl=0221.12003 }}</ref> and Stewart<ref>{{cite journal | first=C. L. | last=Stewart | title=Algebraic integers whose conjugates lie near the unit circle | journal=Bull. Soc. Math. France | volume=106 | year=1978 | pages=169–176 }}</ref> independently proved that there is an absolute constant <math>C>1</math> such that either <math>\\mathcal{M}(P(x))=1</math> or<ref name=S235>Smyth (2008) p.325</ref>\n\n:<math>\\log\\mathcal{M}(P(x))\\ge \\frac{C}{D\\log D}. </math>\n\nDobrowolski <ref>{{cite journal | first=E. | last=Dobrowolski | title=On a question of Lehmer and the number of irreducible factors of a polynomial | journal=Acta Arith. | volume=34 | year=1979 | pages=391–401 }}</ref> improved this to\n\n:<math>\\log\\mathcal{M}(P(x))\\ge C\\left(\\frac{\\log\\log D}{\\log D}\\right)^3.</math>\n\nDobrowolski obtained the value ''C'' ≥ 1/1200 and asymptotically C > 1-ε for all sufficiently large ''D''.  Voutier in 1996 obtained ''C'' ≥ 1/4 for ''D'' ≥ 2.<ref>P. Voutier, [https://arxiv.org/pdf/1211.3110v1.pdf An effective lower bound for the height of algebraic numbers], Acta Arith. 74 (1996), 81–95.</ref>\n\n==Elliptic analogues==\n\nLet <math>E/K</math> be an [[elliptic curve]] defined over a number field <math>K</math>, and let <math>\\hat{h}_E:E(\\bar{K})\\to\\mathbb{R}</math> be the [[canonical height]] function. The canonical height is the analogue for elliptic curves of the function <math>(\\deg P)^{-1}\\log\\mathcal{M}(P(x))</math>. It has the property that <math>\\hat{h}_E(Q)=0</math> if and only if <math>Q</math> is a [[torsion point]] in <math>E(\\bar{K})</math>. The '''elliptic Lehmer conjecture''' asserts that there is a constant <math>C(E/K)>0</math> such that\n\n:<math>\\hat{h}_E(Q) \\ge \\frac{C(E/K)}{D}</math> for all non-torsion points <math>Q\\in E(\\bar{K})</math>,\n\nwhere <math>D=[K(Q):K]</math>. If the elliptic curve ''E'' has [[complex multiplication]], then the analogue of Dobrowolski's result holds:\n\n:<math>\\hat{h}_E(Q) \\ge  \\frac{C(E/K)}{D} \\left(\\frac{\\log\\log D}{\\log D}\\right)^3 ,</math>\n\ndue to Laurent.<ref name=S327>Smyth (2008) p.327</ref> For arbitrary elliptic curves, the best known result is\n\n:<math>\\hat{h}_E(Q) \\ge  \\frac{C(E/K)}{D^3(\\log D)^2},</math>\n\ndue to [[David Masser|Masser]].<ref>{{cite journal | zbl=0723.14026 | last=Masser | first=D.W. | authorlink=David Masser | title=Counting points of small height on elliptic curves | journal=Bull. Soc. Math. Fr. | volume=117 | number=2 | pages=247–265 | year=1989 }}</ref> For elliptic curves with non-integral [[j-invariant]], this has been improved to\n\n:<math>\\hat{h}_E(Q) \\ge  \\frac{C(E/K)}{D^2(\\log D)^2},</math>\n\nby Hindry and [[Joseph H. Silverman|Silverman]].<ref>{{cite book | zbl=0741.14013 | last1=Hindry | first1=Marc | last2=Silverman | first2=Joseph H. | author2-link=Joseph_H._Silverman | chapter=On Lehmer's conjecture for elliptic curves | title=Sémin. Théor. Nombres, Paris/Fr. 1988-89 | series=Prog. Math. | volume=91 | pages=103–116 | year=1990 | editor-last=Goldstein | editor-first=Catherine|editor-link=Catherine Goldstein | isbn=0-8176-3493-2 }}</ref>\n\n==Restricted results==\nStronger results are known for restricted classes of polynomials or algebraic numbers.\n\nIf ''P''(''x'') is not reciprocal then\n\n:<math>M(P) \\ge M(x^3 -x - 1) \\approx 1.3247 </math>\n\nand this is clearly best possible.<ref name=S328>Smyth (2008) p.328</ref>  If further all the coefficients of ''P'' are odd then<ref name=S329/>\n\n:<math>M(P) \\ge M(x^2 -x - 1) \\approx 1.618 . </math>\n\nFor any algebraic number ''α'', let <math>M(\\alpha)</math> be the Mahler measure of the minimal polynomial <math>P_\\alpha</math> of ''α''. If the field '''Q'''(''α'') is a [[Galois extension]] of '''Q''', then Lehmer's conjecture holds for <math>P_\\alpha</math>.<ref name=S329>Smyth (2008) p.329</ref>\n\n==References==\n{{Reflist}}\n* {{cite book | first=Chris | last=Smyth | chapter=The Mahler measure of algebraic numbers: a survey | pages=322–349 | editor1-first=James | editor1-last=McKee | editor2-last=Smyth | editor2-first=Chris | title=Number Theory and Polynomials | series=London Mathematical Society Lecture Note Series | volume=352 | publisher=[[Cambridge University Press]] | year=2008 | isbn=978-0-521-71467-9 }}\n\n==External links==\n*http://www.cecm.sfu.ca/~mjm/Lehmer/ is a nice reference about the problem.\n*{{MathWorld|urlname=LehmersMahlerMeasureProblem|title=Lehmer's Mahler Measure Problem}}\n\n[[Category:Polynomials]]\n[[Category:Theorems in number theory]]\n[[Category:Conjectures]]"
    },
    {
      "title": "Lill's method",
      "url": "https://en.wikipedia.org/wiki/Lill%27s_method",
      "text": "In [[mathematics]], '''Lill's method''' is a visual method of finding the [[real number|real]] [[zero of a function|roots]] of [[polynomial]]s of any [[degree of a polynomial|degree]].<ref>{{cite book |title=Uncommon Mathematical Excursions: Polynomia and Related Realms |author=Dan Kalman |publisher=AMS |year=2009 |isbn=978-0-88385-341-2 |pages=13–22}}</ref>  It was developed by Austrian engineer [[Eduard Lill]] in 1867.<ref>{{cite journal|title=Résolution graphique des équations numériques de tous degrés à une seule inconnue, et description d'un instrument inventé dans ce but |author= M. E. Lill |journal=[[Nouvelles Annales de Mathématiques]] |series=2 |volume=6 |year=1867 |pages=359–362}} ([https://eudml.org/doc/98167 online copy])</ref>  A later paper by Lill dealt with the problem of [[complex numbers|complex]] roots.<ref>{{cite journal|title=Résolution graphique des équations algébriques qui ont des racines imaginaires |author= M. E. Lill|journal=[[Nouvelles Annales de Mathématiques]] |series=2 |volume=7 |year=1868 |pages=363–367}} ([https://eudml.org/doc/98262 online copy])</ref>\n\nLill's method involves expressing the coefficients of a polynomial as magnitudes of segments at right angles to each other, starting from the origin, creating a path to a terminus, then finding a non-right angle path from the start to the terminus reflecting or refracting on the lines of the first path.\n\n==Description of the method==\n[[File:LillsMethod.svg|thumb|right|250px|Finding roots of the cubic 4''x''<sup>3</sup>+2''x''<sup>2</sup>−2''x''−1 using Lill's method. Roots are −1/2, −1/{{radic|2}}, 1/{{radic|2}}. Numbers on black segments are distances (coefficients in the equation), while a number shown on a colored line is the negative of the slope and hence a real root of the polynomial. ]]\n\nTo employ the method a diagram is drawn starting at the origin. A line segment is drawn rightwards by the magnitude of the first coefficient (the coefficient of the highest-power term)  (so that with a negative coefficient the segment will end left of the origin). From the end of the first segment another segment is drawn upwards by the magnitude of the second coefficient, then left by the magnitude of the third, and down by the magnitude of the fourth, and so on.  The sequence of directions (not turns) is always rightward, upward, leftward, downward, then repeating itself. Thus each turn is counterclockwise. The process continues for every coefficient of the polynomial including zeroes, with negative coefficients \"walking backwards\". The final point reached, at the end of the segment corresponding to the equation's constant term, is the terminus.\n\nA line is then launched from the origin at some angle {{mvar|θ}}, reflected off of each line segment at a right angle (not necessarily the \"natural\" angle of reflection), and [[Refraction|refracted]] at a right angle through the line through each segment (including a line for the zero coefficients) when the angled path does not hit the line segment on that line.<ref>Phillips Verner Bradford, Sc.D.. ''[http://www.concentric.net/~pvb/ALG/rightpaths.html Visualizing solutions to n-th degree algebraic equations using right-angle geometric paths.]''  {{webarchive |url=https://web.archive.org/web/20100502013959/http://www.concentric.net/~pvb/ALG/rightpaths.html |date=May 2, 2010 }}</ref> The vertical and horizontal lines are reflected off or refracted through in the following sequence: the line containing the segment corresponding to the coefficient of <math>x^{n-1},</math> then of <math>x^{n-2},</math> etc. Choosing {{mvar|θ}} so that the path lands on the terminus, the negative of the tangent of {{mvar|θ}} is a root of this polynomial. For every real zero of the polynomial there will be one unique initial angle and path that will land on the terminus. A quadratic with two real roots, for example, will have exactly two angles that satisfy the above conditions.\n\nThe construction in effect evaluates the polynomial according to [[Horner's method]]. For the polynomial <math>a_n x^n+a_{n-1}x^{n-1}+a_{n-2}x^{n-2}+ \\cdots</math> the values of <math>a_n x</math>, <math>(a_n x+a_{n-1})x</math>, <math>((a_n x+a_{n-1})x+a_{n-2})x,\\ \\dots</math> are successively generated. A solution line giving a root is similar to the Lill's construction for the polynomial with that root removed.\n\nIn 1936 Margharita P. Beloch showed how Lill's method could be adapted to solve cubic equations using [[paper folding]].<ref>{{cite journal |title=Solving Cubics With Creases: The Work of Beloch and Lill |author=Thomas C. Hull|url=http://mars.wne.edu/~thull/papers/amer.math.monthly.118.04.307-hull.pdf|journal=American Mathematical Monthly |date=April 2011|pages=307–315|doi=10.4169/amer.math.monthly.118.04.307}}</ref> If simultaneous folds are allowed then any ''n''th degree equation with a real root can be solved using ''n''–2 simultaneous folds.<ref>{{cite journal |title=One-, Two-, and Multi-Fold Origami Axioms|url=http://www.math.sjsu.edu/~alperin/AlperinLang.pdf |author1=Roger C. Alperin |author2=Robert J. Lang |journal=4OSME|publisher=A K Peters |year=2009}}</ref>\n\n== See also ==\n*[[Carlyle circle]], which is based on a slightly modified version of Lill's method for a normed quadratic.\n\n==References==\n{{Reflist}}\n\n==External links==\n{{commonscat}}\n*  {{cite web|last=Bradford|first=Phillips Verner|title=Extending Lill's Method of 1867|url=http://www.concentric.net/~pvb/ALG/rightpaths.html|work=Visualizing solutions to n-th degree algebraic equations using right-angle geometric paths|publisher=www.concentric.net|accessdate=3 February 2012|deadurl=yes|archiveurl=https://web.archive.org/web/20100502013959/http://www.concentric.net/~pvb/ALG/rightpaths.html|archivedate=2 May 2010|df=}}\n* [http://dankalman.net/ume/lill/ Animation for Lill's Method]\n* [https://www.youtube.com/watch?v=IUC-8P0zXe8 Mathologer video: \"Solving equations by shooting turtles with lasers\"]\n\n[[Category:Geometry]]\n[[Category:Paper folding]]\n[[Category:Polynomials]]"
    },
    {
      "title": "Lindsey–Fox algorithm",
      "url": "https://en.wikipedia.org/wiki/Lindsey%E2%80%93Fox_algorithm",
      "text": "{{orphan|date=September 2012}}\n\nThe '''Lindsey–Fox algorithm''', named after Pat Lindsey and Jim Fox, is a numerical [[algorithm]] for finding the roots or zeros of a high-degree [[polynomial]] with real coefficients over the [[complex field]].  It is particularly designed for random coefficients but also works well on polynomials with coefficients from samples of speech, seismic signals, and other measured phenomena. A [[Matlab]] implementation of this has factored polynomials of degree over a million on a desk top computer.\n\n== The Lindsey–Fox algorithm==\nThe Lindsey–Fox algorithm uses the [[Fast Fourier transform|FFT]] (fast Fourier transform) to very efficiently conduct a grid search in the complex plane to find accurate approximations to the ''N'' roots (zeros) of an ''N''th-degree polynomial. The power of this grid search allows a new [[Factorization of polynomials|polynomial factoring]] strategy that has proven to be very effective for a certain class of polynomials. This algorithm was conceived of by Pat Lindsey and implemented by Jim Fox in a package of computer programs created to factor high-degree polynomials. It was originally designed and has been further developed to be particularly suited to polynomials with real, random coefficients. In that form, it has proven to be very successful by factoring thousands of polynomials of degrees from one thousand to hundreds of thousand as well as several of degree one million and one each of degree two million and four million. In addition to handling very high degree polynomials, it is accurate, fast, uses minimum memory, and is programmed in the widely available language, Matlab. There are practical applications, often cases where the coefficients are samples of some natural signal such as speech or seismic signals, where the algorithm is appropriate and useful. However, it is certainly possible to create special, ill-conditioned polynomials that it cannot factor, even low degree ones. The basic ideas of the algorithm were first published by Lindsey and Fox in 1992<ref>J. P. Lindsey and James W. Fox. “A method of factoring long z-transform polynomials”, Computational Methods in Geosciences, SIAM, pp. 78–90, 1992.</ref> and reprinted in 1996.<ref>Osman Osman (editor), Seismic Source Signature Estimation and Measurement, Geophysics Reprint Series #18, Society of Exploration Geophysicists (SEG), 1996, pp. 712–724.</ref>&nbsp; After further development, other papers were published in 2003<ref>Gary A. Sitton, C. Sidney Burrus, James W. Fox, and Sven Treitel. “Factoring very high degree polynomials”. IEEE Signal Processing Magazine, 20(6):27–42, November 2003.</ref><ref>C. S. Burrus, J. W. Fox, G. A. Sitton, and S. Treitel, “Factoring High Degree Polynomials in Signal Processing”, Proceedings of the IEEE DSP Workshop, Taos, NM, Aug. 3, 2004, pp. 156–157.</ref> and an on-line booklet.<ref>{{cite web|url=http://cnx.org/content/col10494/latest/|author=C. Sidney Burrus|title=Factoring Polynomials of High Degree|work=Connexions|date=Apr 1, 2012|publisher=Rice University|quote=Accepted by the IEEE Signal Processing Society Lens|accessdate=23 July 2012}}</ref>&nbsp; The program was made available to the public in March 2004 on the Rice University web site.<ref>{{cite web|url=http://www-dsp.rice.edu/software/fvhdp.shtml|title=Factoring Very High Degree Polynomials|date=March 10, 2006|author1=C. S. Burrus |author2=J. W. Fox |author3=G. A. Sitton |author4=and S. Treitel |publisher=Rice University|accessdate = 23 July 2012}}{{failed verification|date=July 2012}}</ref>&nbsp; A more robust version-2 was released in March 2006 and updated later in the year.\n\n==The three stages of the Lindsey–Fox program==\nThe strategy implemented in the Lindsey–Fox algorithm to factor polynomials is organized in three stages. The first evaluates the polynomial over a grid on the complex plane and conducts a direct search for potential zeros. The second stage takes these potential zeros and “polishes” them by applying [[Laguerre's method]] to bring them close to the actual zeros of the polynomial. The third stage multiplies these zeros together or “unfactors” them to create a polynomial that is verified against the original. If some of the zeros were not found, the original polynomial is “deflated” by dividing it by the polynomial created from the found zeros. This quotient polynomial will generally be of low order and can be factored by conventional methods with the additional, new zeros added to the set of those first found. If there are still missing zeros, the deflation is carried out until all are found or the whole program needs to be restarted with a finer grid. This system has proven to be fast, accurate, and robust on the class of polynomials with real, random coefficients and other similar, well-conditioned polynomials.\n\n=== Stage one: the grid search for prospective zeros ===\n# Construct a [[polar coordinate]] grid on the complex plane with spacing derived from the degree of the polynomial being factored\n# Use the FFT to evaluate the polynomial at each node along the concentric circles of the grid.\n# Search over each 3x3 set of values for relative minima. If the center value is less than the edge values, it is a prospective zero by the [[Maximum modulus principle|Minimum Modulus Theorem]] of complex analysis.\n\n=== Stage two: polish the prospective zeros ===\n# Apply Laguerre’s algorithm to each prospective zero, correcting it to a better approximation of the “true” zero of the polynomial\n# Test the set of polished zeros for uniqueness and discard any duplicates to give a set of candidate zeros\n\n=== Stage three: unfactor, perhaps deflate, and verify ===\n# Unfactor the polished zeros i.e., reconstruct a candidate polynomial in coefficient form from the polished candidate zeros\n# If the degree of the reconstructed polynomial is the same as that of the original polynomial and differences in their coefficients are small, the factoring is successful and finished\n# If some zeros were missed, deflate and factor the quotient polynomial. If that does not find all of the missed zeros, deflate and factor again until all are found or until no new ones are found\n# If deflation finds all the zeros that it can, and it still has not found them all, design a new grid with a finer spacing and return to stage one. If four restarts do not find them all and/or the reconstruction error is not small, declare failure.\n\n== Description of the three stages ==\n'''Stage one''' is the reason this algorithm is so efficient and is what sets it apart from most other [[Factorization|factoring]] algorithms. Because the FFT (fast Fourier transform) is used to evaluate the polynomial, a fast evaluation over a dense grid in the complex plane is possible. In order to use the FFT, the grid is structured in polar coordinates. In the first phase of this stage, a grid is designed with concentric circles of a particular radius intersected by a set of radial lines. The positions and spacing of the radial lines and the circles are chosen to give a grid that will hopefully separate the expected roots. Because the zeros of a polynomial with random coefficients have a fairly uniform angular distribution and are clustered close to the unit circle, it is possible to design an evaluation grid that fits the expected root density very well. In the second phase of this stage, the polynomial is evaluated at the nodes of the grid using the fast Fourier transform (FFT). It is because of the extraordinary efficiency and accuracy of the FFT that a direct evaluation is possible. In the third phase of this first stage, a search is conducted over all of the 3 by 3 node cells formed in the grid. For each 3 by 3 cell (see Figure below), if the value of the polynomial at the center node of the cell (the \"x\") is less than the values at all 8 of the nodes on the edges of the cell (the \"o's\"), the center is designated a candidate zero. This rule is based on the “Minimum Modulus Theorem” which states that if a relative minimum of the absolute value of an analytic function over an open region exists, the minimum must be a zero of the function. Finally, this set of prospective zeros is passed to the second stage. The number is usually slightly larger than the degree because some were found twice or mistakes were made. The number could be less if some zeros were missed. \n[[File:FFT Cell.png|thumb|Figure 1: Section of the polar coordinate grid showing a 3-node by 3-node cell]]\n\n'''Stage two''' is more traditional than the other two. It “polishes” each of the prospective zeros found by the grid search. The first phase consists of applying an iterative algorithm to improve the accuracy of the location found by the grid search. In earlier versions of the program, Newton’s method was used but analysis and experiment showed that Laguerre's method was both more robust and more accurate. Even though it required more calculation than Newton’s method for each iteration, it converged in fewer iterations. The second phase of the second stage checks for duplications. A “fuzzy” uniqueness test is applied to each zero to eliminate any cases where on two or more prospective zeros, iterations converged to the same zero. If the number of unique, polished zeros is less than the degree of the polynomial, deflation later will be necessary. If the number is greater, some error has occurred. This stage consumes the largest part of the execution time of the total factorization, but it is crucial to the final accuracy of the roots. One of the two criteria for success in factoring a polynomial is that each root must have been successfully polished against the original polynomial.\n\n'''Stage three''' has several phases and possible iterations or even restarting. The first phase of the third stage takes all of the unique, polished zeros that were found in the first two stages and multiplies them together into the coefficient form of a candidate polynomial (“unfactors” the zeros). If the degree of this reconstructed polynomial is the same as that of the original polynomial and if the difference in their coefficients is small, the factorization is considered successful. Often, however, several zeros were missed by the grid search and polish processes of stage one and two, or the uniqueness test discarded a legitimate zero (perhaps it is multiple), so the original polynomial is “deflated” (divided) by the reconstructed polynomial and the resulting (low degree) quotient is factored for the missing zeros. If that doesn’t find them all, the deflation process is repeated until they are all found. This allows the finding of multiple roots (or very tightly clustered roots), even if some of them were discarded earlier. If, in the unusual case, these further iterations of deflation do not find all of the missing zeros, a new, finer grid is constructed and the whole process started again at stage one.  More details on the third stage are in another module.\n\n'''Multiple order''' and clustered roots are unusual in random coefficient polynomials. But, if they happen or if factoring an ill-conditioned polynomial is attempted, the roots will be found with the Lindsey–Fox program in most cases but with reduced accuracy. If there are multiple order zeros (Mth order with M not too high), the grid search will find them, but with multiplicity one. The polishing will converge to the multiple order root but not as fast as to a distinct root. In the case of a cluster with ''Q'' zeros that fall within a single cell, they are erroneously identified as a single zero and the polishing will converge to only one of them. In some cases, two zeros can be close to each other in adjacent cells and polish to the same point. In all of these cases, after the uniqueness test and deflation, the quotient polynomial will contain a ''M''&nbsp;&minus;&nbsp;1 order zero and/or ''Q''&nbsp;&minus;&nbsp;1 zeros clustered together. Each of these zeros will be found after ''M''&nbsp;&minus;&nbsp;1 or ''Q''&nbsp;&minus;&nbsp;1 deflations. There can be problems here because Laguerre’s polishing algorithm is not as accurate and does not converge as fast for a multiple zero and it may even diverge when applied to tightly clustered zeros. Also, the condition of the quotient polynomial will be poorer when multiple and clustered zeros are involved. If multiple order zeros are extremely far from the unit circle, the special methods for handling multiple roots developed by Zhonggang Zeng  are used. Zeng’s method is powerful but slow, and hence only used in special cases [6].  References\n\n'''Successful completion''' of the factoring of a polynomial requires matching zeros on the complex plane measured by the convergence of Laguerre’s algorithm on each of the zeros. It also requires matching the polynomial reconstructed from the found zeros with the original polynomial by measuring the maximum difference in each coefficient.\n\n== Characteristics of the Lindsey–Fox algorithm ==\n\nBecause the FFT is such an efficient means of evaluating the polynomial, a very fine grid can be used which will separate all or almost all of the zeros in a reasonable time. And because of the fineness of the grid, the starting point is close to the actual zero and the polishing almost always converges in a small number of steps (convergence is often a serious problem in traditional approaches). And because the searching and polishing is done on the original polynomial, they can be done on each root simultaneously on a parallel architecture computer. Because the searching is done on a 3 by 3 cell of the grid, no more that three concentric circles of the grid need be kept in memory at a time, i.e., it is not necessary to have the entire grid in memory. And, some parallelization of the FFT calculations can be done.\n\nDeflation is often a major source of error or failure in a traditional iterative algorithm. Here, because of the good starting points and the powerful polisher, very few stages of deflation are generally needed and they produce a low order quotient polynomial that is generally well-conditioned. Moreover, to control error, the unfactoring (multiplying the found roots together) is done in the FFT domain (for degree larger than 500) and the deflation is done partly in the FFT domain and partly in the coefficient domain, depending on a combination of stability, error accumulation, and speed factors.\n\nFor random coefficient polynomials, the number of zeros missed by the grid search and polish stages ranges from 0 to 10 or occasionally more. In factoring one 2 million degree polynomial, the search and polish stages found all 2 million zeros in one grid search and required no deflation which shows the power of the grid search on this class of polynomial. When deflation is needed, one pass is almost always sufficient. However, if you have a multiple zero or two (or more) very, very closely spaced zeros, the uniqueness test will discard a legitimate zero but it will be found by later deflation. Stage three has enough tests and alternatives to handle almost all possible conditions. But, by the very definition of random coefficients, it is hard to absolutely guarantee success.\n\nThe timings of the Lindsey–Fox program and examples of root distributions of polynomials with random coefficients are [http://cnx.org/content/col10494/latest/ here].\n\n== References ==\n{{reflist}}\n\n{{DEFAULTSORT:Lindsey-Fox algorithm}}\n[[Category:Polynomials]]"
    },
    {
      "title": "Linear equation",
      "url": "https://en.wikipedia.org/wiki/Linear_equation",
      "text": "{{refimprove|date=January 2016}}\n[[File:Linear Function Graph.svg|thumb|300px|Two Graphs of linear equations in two variables]]\nIn [[mathematics]], a '''linear equation''' is an [[equation]] that may be put in the form\n:<math>a_1x_1+\\cdots +a_nx_n+b=0,</math>\nwhere <math>x_1, \\ldots, x_n</math> are the [[variable (mathematics)|variables]] (or [[unknown (mathematics)|unknowns]] or [[indeterminate (variable)|indeterminates]]), and <math>b, a_1, \\ldots, a_n</math> are the [[coefficient]]s, which are often [[real number]]s. The coefficients may be considered as [[parameter]]s of the equation, and may be arbitrary [[expression (mathematics)|expressions]], provided they do not contain any of the variables. To yield a meaningful equation for non-zero values of <math>b,</math> the coefficients are required to not be all zero.\n\nIn the words of algebra, a linear equation is obtained by equating to zero a [[linear polynomial]] over some [[field (mathematics)|field]], from which the coefficients are taken, and that does not contain the symbols for the indeterminates.\n\nThe [[solution (equation)|solutions]] of such an equation are the values that, when substituted for the unknowns, make the equality true.\n\nThe case of just one variable is particularly important, and frequently the term ''linear equation'' refers implicitly to this particular case, in which the name ''unknown'' for the variable is sensibly used.\n\nAll the pairs of numbers that are solutions of a linear equation in two variables form a [[line (geometry)|line]] in the [[Euclidean plane]], and every non-vertical line may be defined as the solutions of a linear equation. This is the origin of the term ''linear'' for describing this type of equation. More generally, the solutions of a linear equation in {{mvar|n}} variables form a [[hyperplane]] (a subspace of dimension {{math|''n'' – 1}}) in the [[Euclidean space]] of dimension {{mvar|n}}.\n\nLinear equations occur frequently in all mathematics and their applications in [[physics]] and [[engineering]], partly because [[non-linear system]]s are often well approximated by linear equations.\n \nThis article considers the case of a single equation with coefficients from the field of [[real number]]s, for which one studies the real solutions. All of its content applies to [[complex number|complex]] solutions and, more generally, for linear equations with coefficients and solutions in any [[field (mathematics)|field]]. For the case of several simultaneous linear equations, see [[system of linear equations]].\n\n== One variable ==\nFrequently the term ''linear equation'' refers implicitly to the case of just one variable. This case, in which the name ''unknown'' for the variable is sensibly used, is of particular importance, since it offers a unique value as solution to the equation. According to the above definition such an equation has the form\n:<math>ax+b=0,</math>\n\nand, for {{math|''a'' ≠ 0,}} a unique value as solution \n:<math>x=-\\frac ba.</math>\n\nIn the case of <math>a = 0</math>, two possibilities emerge:\n#<math>b = 0:</math> Every value of <math>x</math> is a solution to the equation <math>0\\cdot x + 0 = 0,</math> and\n#<math>b\\ne 0:</math> There is no solution for the equation <math>0\\cdot x + b = 0,</math> the equation is said to be [[inconsistent equations|inconsistent]].\n\n==Two variables==\nIn the case of just two variables the indexed variable names <math>x_1</math> and <math>x_2</math> and the respective coefficients <math>a_1</math> and <math>a_2</math> are often replaced, for the convenience of not having to deal with indices, by <math>x</math>, <math>y</math>, <math>a</math> and <math>b</math>, respectively. As a consequence, the so called ''constant term'', named {{nowrap|coefficient <math>b</math>}} in the above notation, must also be renamed; <math>c</math> suggests itself. A linear equation in two variables is then denoted as\n:<math>ax+by+c=0.</math>\n\nAny change to such an equation that does not alter the set of solutions, i.e., the set of pairs <math>(x,y)</math>, that satisfy this equation (i.e., make it an [[identity (mathematics)|identity]]), generates an [[equivalence relation|''equivalent'']] equation. It is immediate that changing the involved names (e.g. capitalizing names or using other letters) and also reordering the equation (for example, by moving terms to the other side), does not change this set of solutions, and thus results in an equivalent equation, like, \n:<math>Ax + By = C, \\quad</math> with <math>\\quad A = a,\\;B=b\\quad</math> and <math>\\quad C=-c.</math>\n\nThese equivalent variants are sometimes given generic names, such as ''general form'' or ''standard form'',<ref>{{harvnb|Barnett|Ziegler|Byleen|2008|loc = pg. 15}}</ref> but contribute no new concepts.\n\nThe set of solutions also does not change when both sides of the equation are multiplied by the same non-zero number. According to the above definition, <math>a</math> and <math>b</math> {{nowrap|(identically <math>A</math> and <math>B</math>)}} are not both zero, so multiplying the equation by the reciprocal of one of these non-zero coefficients, results in an equivalent equation {{nowrap|with <math>+1</math>}} as the coefficient of one variable. This variable can be isolated on the left hand side, leaving an expression, possibly containing the other variable on the right hand side. This leads to either\n:<math>b\\ne 0:\\quad y = mx + y_0,\\quad\\;</math> with <math>\\quad m = -\\frac a b\\quad\\;</math> and <math>\\quad y_0  = -\\frac c b,\\quad</math> or\n:<math>a\\ne 0:\\quad x = m'y + x_0,\\quad</math> with <math>\\quad m' = -\\frac b a \\quad</math> and <math>\\quad x_0  = -\\frac c a.</math>\n\nWhen both coefficients <math>a</math> and <math>b</math> are not zero, then both forms exist, and, assuming real numbers as coefficients as well as the domain of the variables, the set of solutions for both equations can then be denoted as\n:<math>S=\\{(x,mx+y_0)|\\;\\forall x\\in\\mathbb R\\},\\quad</math> which is equal to the set <math>\\quad S=\\{(m'y+x_0,y)|\\;\\forall y\\in\\mathbb R\\}.</math>\n\nIn this case both components of the pairs in the set <math>S</math> vary over all real numbers, thereby depending in a so called ''linear affine'' manner on the respective other.\n\nWhen exactly one coefficient, either <math>a</math> or <math> b</math>, is not zero, then one equation remains, which is either\n:<math> y = y_0,\\quad</math> for <math>\\quad a=0,\\;b\\ne 0,\\quad</math> with the set of solutions <math>\\quad S_x=\\{(x,y_0)|\\;\\forall x\\in\\mathbb R\\},\\quad </math> or\n:<math> x = x_0,\\quad</math> for <math>\\quad b=0,\\;a\\ne 0,\\quad</math> with the set of solutions <math>\\quad S^y=\\{(x_0,y)|\\;\\forall y\\in\\mathbb R\\}.</math>\n\nFor both alternatives this is a set of pairs of numbers, where either the second component is a constant, and the first varies over all the reals (<math>S_x</math>), or the first is a constant, and the second varies over all the reals (<math>S^y</math>).\n\n===In Cartesian coordinates===\n\nEvery single solution of a linear equation in two variables can be interpreted as two coordinate values, fixing a [[Point (geometry)|point]] in the [[Euclidean plane]] with a [[Cartesian coordinate system]]. The sets of solutions of such an equation form a [[line (geometry)|line]] in this plane. Conventionally, the first component of a solution <math>(x,y)</math>, the <math>x</math>-value, is assigned to a horizontally drawn <math>x</math>-axis, and the second component, the <math>y</math>-value, to a vertical <math>y</math>-axis.\n\n[[File:x is a.svg|thumb|Vertical Line <math> x = x_0 </math><br> <math>( \\text{in the picture:}\\;x_0\\mapsto a)</math>]]\n\nIn the case of <math>a\\ne0,\\;b= 0</math> the equation is <math> x = x_0,</math> and the set of its solutions <math>S^y=\\{(x_0,y)|\\;\\forall y\\in\\mathbb R\\}</math> has a vertical line as its graph, as shown in the figure to the right. The value <math>x_0= -\\tfrac c a,</math> where the line intersects the <math>x</math>-axis in the point <math>(x_0,0)</math>, is called an [[x-intercept|{{nowrap|<math>x</math>''-intercept''.}}]] Except for <math>x_0 = 0,</math> when the graph coincides with the {{nowrap|<math>y</math>-axis,}} graphs of this kind do not intersect the {{nowrap|<math>y</math>-axis,}} they have no [[y-intercept|{{nowrap|<math>y</math>-''intercept''.}}]]\n\nThe set of solutions defines a [[Function (mathematics)|function]] <math>f(t)</math> and, simultaneously, the [[graph of a function|graph of this function]], by interpreting the pairs <math>(x,y)</math> as <math>(t,f(t)),</math> provided that any two such solutions that differ in their second value {{nowrap|(<math>y=f(t)</math>),}} also differ in their respective first values {{nowrap|(<math>x=t</math>).}} The set <math>S^y=\\{(x_0,y)|\\;\\forall y\\in\\mathbb R\\}</math> violates this condition: all real values <math>y</math> in the second component have the same first component <math>x_0.</math> Nevertheless, a graph for this set may be drawn, but it is not a graph of a function under the conventional assignment of axes, it obviously fails the [[vertical line test]]. This is the only type of straight line which is not the graph of any function <math>f(t)</math>.\n\n[[File:y is b.svg|thumb|Horizontal Line <math> y = y_0</math><br> <math>( \\text{in the picture:}\\;y_0\\mapsto b)</math>]]\n\nThe sets <math>S_x</math> and <math>S</math> satisfy the above condition, and the graph of <math>S_x=\\{(x,y_0)|\\;\\forall x\\in\\mathbb R\\}</math> is shown to the right. In this case of <math>a=0,\\;b\\ne 0</math> the graph of the constant function <math> f(x) = y = y_0</math> is a horizontal line. The value <math>y_0= -\\tfrac c b,</math> where the line intersects the <math>y</math>-axis, is called {{nowrap|<math>y</math>-intercept.}} Except for <math>y_0 = 0,</math> where the graph coincides with the <math>x</math>-axis, graphs of this kind have no {{nowrap|<math>x</math>-intercept.}}\n\nIn the case of <math>a\\ne0\\ne b</math> with the equation <math>y=mx+y_0</math> the set of solutions is <math>S=\\{(x,mx+y_0)|\\;\\forall x\\in\\mathbb R\\}.</math> It consists of pairs of numbers, with the first component varying over all the reals, and the other being calculated by a simple expression, representing a [[linear map]] {{nowrap|(<math>x\\mapsto mx</math>)}} and adding a {{nowrap|constant (<math>y_0</math>).}} This is sometimes called a ''linear affine'' function, or simply also [[linear function (calculus)|linear function]], slightly abusing the strict term ''linear''. Also in this case the graph of a linear equation in two variables is a [[line (geometry)|straight line]] (see figure at the top) that intersects the {{nowrap|<math>x</math>-axis}} at {{nowrap|<math>x</math>-intercept <math>x_0= -\\tfrac c a</math>}} (i.e., <math>(x_0,0)</math> is a solution) and the {{nowrap|<math>y</math>-axis}} at the {{nowrap|<math>y</math>-intercept <math>y_0= -\\tfrac c b</math>}} (i.e., <math>(0,y_0)</math> is a solution).\n\nBesides the intercepts being obvious from graphing the solutions of a linear equation in two variables, also their ratio (if it exists) can be graphically interpreted as determining the incline of the considered line (and all lines parallel to it). The ''[[slope]]'' of a straight line, usually introduced as ''[[rise over run]]'', is the negative ratio of the ''rise'', the {{nowrap|<math>y</math>-intercept,}} to the ''run'', the {{nowrap|<math>x</math>-intercept.}} The negative sign accommodates for a positive slope, when the line rises for increasing {{nowrap|<math>x</math>-values.}} Immediately\n:<math>-\\frac {y_0}{x_0}= -\\frac {-\\tfrac c b}{-\\tfrac c a} = -\\frac a b = m,</math>\n\nwhich holds if both intercepts exist. If the {{nowrap|<math>x</math>-intercept}} does not exist (<math>a=0</math>), the slope <math>m</math> equals <math>0,</math> belonging to a horizontal line.\n\nSince ''rise'' and ''run'' of a straight line can be determined not only between the intercept points and the origin (<math>x_0 - 0</math> and <math>y_0 - 0</math>), but also between arbitrary points <math>(x_1,y_1)</math> and <math>(x_2,y_2)</math> on the line, the slope may also be determined by\n:<math>m= \\frac {y_2 - y_1}{x_2 - x_1} =  \\frac {y_1 - y_2}{x_1 - x_2}.</math>\n\nDenoting the angle enclosed by the <math>x</math>-axis and the line as <math>\\varphi,</math> then\n:<math>\\tan\\varphi = m = -\\frac{a}{b}.</math>\n\nFor <math>b= 0</math>  the slope is undefined (<math> \\varphi = \\pi/2</math>). \n\nThis shows that only two of <math>x_0,\\;y_0</math> and <math>m</math> can be selected independently.\n\nWith the premise that at least one axis is intersected, and since both intercept values may range over the whole real number line,  all parallels to both axes as well as all oblique straight lines, i.e., in fact all straight lines in the Euclidean plane, can be expressed by linear equations in two variables, and all such equations denote either oblique or axis-parallel straight lines. Therefore all equations, equivalent to one of the above forms are often referred to as \"equations of a line\". They are adjusted to fit best to specific tasks, and are given therefore specific names, described below. In what follows, <math>x,\\;y,\\;t,\\;\\theta</math> are the names of variables, and other letters denote [[Constant term|constants]] (fixed numbers) as coefficients.\n\n===Slope–intercept form===\nThis form relies on the habit of writing <math>y=f(x)</math> and the conventional way of assigning the variables of the linear equation to the axes of a Cartesian coordinate system, drawn in the conventional manner as described above. This form exists only for <math>b\\ne 0,</math> allowing to isolate <math>y</math> on the left hand side<ref>{{harvnb|Larson|Hostetler|2007|loc=p. 25}}</ref>\n:<math>y = mx + y_0.</math>\n\nThis way the slope <math>m=-\\tfrac a b </math> describes the inclination of the straight line which is the graph of this equation. The slope is positive for a line ascending to the right and negative, if the line ascends to the left. A zero-slope <math>m=0</math> belongs to a horizontal line.\n\nThe <math>y</math>-intercept <math>y_0 = -\\tfrac c b</math> fixes the point <math>(0,y_0),</math> where the line crosses the <math>y</math>-axis, and <math>y_0 = 0</math> characterizes lines that cross the origin <math>(0,0).</math>\n\nRecalling the <math>x</math>-intercept as <math>x_0 = -\\tfrac c a,</math> the above slope-intercept form, employing the slope <math>m</math> and the <math>y</math>-intercept, can be transformed to\n:<math>y = -\\frac a b x -\\frac c b = -\\frac a b(x+\\frac b a\\cdot\\frac c b) = m(x- x_0),</math>\n\ninvolving the slope <math>m</math> and the <math>x</math>-intercept <math>x_0</math>.\n\nIn the case of <math>b=0,</math> there is no slope-intercept form in the above way, because a slope does not exist for <math>\\varphi=\\pi/2</math>.\n\nFor <math>a\\ne 0\\ne b</math> it is possible to express the inverse functions <math>f^{-1}</math> in the slope-intercept form as\n:<math>x = m'y + x_0,\\quad</math> with <math>m'= \\tfrac b a.</math> \n\nThe graph of this equation, having the same set of solutions, is necessarily identical to the above graph, but depicting it under exchanged assignment of the variables to the coordinate-axes (<math>(x,y)\\mapsto (y\\text{-axis},\\;x\\text{-axis})</math>), yields the usual <math>f^{-1}</math>-graph for inverse functions, the <math>f</math>-graph mirrored along <math>y=x.</math> This holds for both <math>(a=0,\\;b\\ne0)</math> and <math>(b=0,\\;a\\ne0).</math>\n\nThe graph of a vertical line (<math>b=0</math>) with no existing slope and the equation <math>x=d</math> changes under this inverted assignment to the graph of the function <math>y=d</math> with zero-slope (<math>d</math> an arbitrary constant), and vice versa.\n\nThe slope–intercept form of a line can be computed from the value of the function at any two points: the slope can be calculated as <math>m = (y_2 - y_1)/(x_2 - x_1),</math> and then the intercept as <math>a = y_1 - m x_1.</math> This is a special case of the [[unisolvence theorem]] for polynomials: the coefficients of a polynomial of degree at most {{tmath|d}} can be computed from the value at {{tmath|d + 1}} distinct points.\n\n===Point–slope form===\nA line is uniquely defined by its slope and an arbitrary point on it. In the slope-intersect form this point on the line is either taken as the intersection <math>(0,y_0)</math> with the <math>y</math>-axis, or the intersection <math>(x_0,0)</math> with the <math>x</math>-axis and is combined with the slope <math>m</math>, provided its existence, to establish the equation for the according line. Generalizing this approach to an arbitrary point with coordinates <math>(x_1,y_1)</math> yields:<ref>{{harvnb|Larson|Hostetler|2007|loc=p. 29}}</ref>\n:<math>y - y_1 = m( x - x_1 ).</math>\n\nThe point-slope form expresses the fact that the differences of the coordinates of an arbitrary point <math>(x,y)</math> and the point <math>(x_1,y_1),</math> both on a straight line are proportional to each other. More precisely, as long as <math>x \\ne x_1</math> and <math>y \\ne y_1,</math> the nonzero differences <math>x - x_1</math> and <math>y - y_1</math> are proportional and the proportionality constants are, respectively, <math>m</math> and <math>1/m.</math>\n\n===Intercept form===\nFor a straight line that crosses both coordinate axes outside the origin, both intercept values exist and are non-zero. This implies that also <math>c</math> is nonzero, and such lines can be specified via the intercept form, that employs these two intercept values to specify an appropriate equation<ref name=WilsonTracey>{{harvnb|Wilson|Tracey|1925|loc=pp. 52-53}}</ref>\n:<math>\\frac{x}{x_0} + \\frac{y}{y_0} = 1.</math>\n\nThe intercept form results from moving <math>c</math> in the equation <math>ax+by+c=0</math> to the right side, and then multiplying both sides of the equation with <math>-1/c,</math> yielding\n:<math>(-\\frac a c) x + (-\\frac b c) y = \\frac 1{x_0}x + \\frac 1 {y_0}y = 1,</math>\n\nwhich is identical to the above form. The intercept form also works conveniently in higher dimensions for specifying (hyper)planes, when their intersections with all coordinate axes exist and are known.\n\n===Two-point form===\nTwo points <math>(x_1,y_1)</math> and <math>(x_2,y_2)</math> with <math>x_1\\ne x_2</math> (no vertical lines!) determine the slope of the line through these points. This slope, calculated as above, can be used with either point to employ the point-slope form, thereby establishing appropriate equations for this line, based on two points with different <math>x</math>-values. This yields <ref name=WilsonTracey /> \n:<math>y - y_j = \\frac{y_2 - y_1}{x_2 - x_1} (x - x_j),\\quad</math> for <math>j=1,2.</math>\n\nIn the rest of this paragraph <math>j=1</math> is used. \n\n====Expanded form====\nExpanding, regrouping, and appropriately factoring the products leads to\n:<math>(y_1-y_2)x + (x_2-x_1)y + (x_1y_2 - x_2y_1) =0,</math>\n\nidentifying: <math>\\quad a=(y_1-y_2),\\quad b= (x_2-x_1),\\quad</math> and <math>\\quad c= (x_1y_2 - x_2y_1). </math>\n\n====Symmetric form====\nMultiplying both sides of the 2-point form by <math>(x_2-x_1)</math> yields an equation in a  symmetric form\n:<math>(x_2 - x_1)(y - y_1)=(y_2 - y_1)(x - x_1).</math>\n\nThis form also works in the case of a non-existing slope (<math>x_1=x_2</math>), but requires <math>y_1\\ne y_2</math> in this case: {{nowrap|it correctly delivers <math>\\quad x= x_1.</math>}}\n\n====Determinant form====\nThe products in the above equation result also from the evaluation of a 2-rowed [[determinant]], inducing this form of the linear equation:\n:<math>\\begin{vmatrix}x-x_1&y-y_1\\\\x_2-x_1&y_2-y_1\\end{vmatrix}=0.</math>\n\n====Mnemonic determinant====\nThe products on the left hand side of the expanded version can be reproduced by evaluating the easily memorized 3-rowed determinant, which can be justified by the theory of [[projective geometry]]:\n:<math>\\begin{vmatrix}\nx&y&1\\\\\nx_1&y_1&1\\\\\nx_2&y_2&1\n\\end{vmatrix}=0.</math>\n\n====Vectorial treatment====\nAny pair of numbers <math>(x,y)</math> may be treated as a vector, given by two components with respect to a Cartesian coordinate system. A (naive) vector starts at the origin <math>(0,0)</math>, and ends at the given coordinates. Any two [[collinearity|non-collinear]] vectors <math>(a_1,a_2)</math> and <math>(b_1,b_2)</math> span a parallelogram, with these three points. The area <math>A</math> of this parallelogramm can be calculated as the magnitude of the [[exterior algebra|exterior product]] (2dim-cross product, geometric product, ...) of these vectors. In components this can be done by evaluating the absolute value of a determinant with the components:\n:<math>A= \\left|\\begin{vmatrix}a_1 & a_2 \\\\b_1 & b_2\\end{vmatrix}\\right |. </math>\n\nTwo given points <math>P_1=(x_1,y_1),\\;P_2=(x_2,y_2)</math> and an arbitrary third point <math>P=(x,y)</math> are on one straight line (collinear), if, e.g., the vector from <math>P_1</math> to <math>P_2</math> and the vector from <math>P_1</math> to <math>P</math> span ''no'' parallelogram, i.e., a parallelogram with area zero, i.e., also the vectors are collinear.\n\nThe vector from point <math>P_1</math> to point <math>P_2</math> can be expressed as\n:<math>P_{12}= P_2 - P_1 = (x_2,y_2) - (x_1,y_1) = (x_2-x_1,y_2-y_1)</math>\nand similarly the vector from point <math>P_1</math> to an arbitrary point <math>P</math> is\n:<math>P_{1.}= P - P_1 = (x,y) - (x_1,y_1) = (x-x_1,y-y_1).</math>\n\nEquating the exterior product of these two vectors, as specified above, to zero, yields a linear equation\n:<math>\\begin{vmatrix}x-x_1&y-y_1\\\\x_2-x_1&y_2-y_1\\end{vmatrix} = 0,</math>\n\nwhich is identical to the determinant form above.\n\n===Matrix form===\nWriting a linear equation in two unknowns in the form\n:<math>Ax + By = C,</math>\n\nand considering the collection of coefficients <math>(A,B)</math> as a <math>(1,2)</math>-matrix, and the collection of variables <math>\\begin{pmatrix}x\\\\y\\end{pmatrix} </math> as {{nowrap|a <math>(2,1)</math>-matrix,}} then their [[matrix product]] equals the <math>(1,1)</math>-matrix <math>\\begin{pmatrix} C \\end{pmatrix}: </math>\n:<math>\\begin{pmatrix} A&B \\end{pmatrix}\\begin{pmatrix}x\\\\y\\end{pmatrix} = \\begin{pmatrix}C\\end{pmatrix}.</math>\n\nThis notation can easily expanded to more linear equations in more than two variables. For example, a system of two equations in two variables\n: <math>A_1x + B_1y = C_1,\\,</math>\n: <math>A_2x + B_2y = C_2,\\,</math>\n\ncan be denoted with a <math>(2,2)</math>-matrix and a <math>(2,1)</math>-matrix for the coefficients, by equaling the matrix product of the <math>(2,2)</math>-coefficient matrix with the <math>(2,1)</math>-variable matrix to the <math>(2,1)</math>-matrix of the constant terms:\n:<math>\n\\begin{pmatrix}\nA_1&B_1\\\\\nA_2 & B_2\n\\end{pmatrix}\n\\begin{pmatrix}\nx\\\\y\n\\end{pmatrix} = \n\\begin{pmatrix}\nC_1\\\\\nC_2\n\\end{pmatrix}.</math>\n\nA system of three linear equations in four variables would employ a <math>(3,4)</math>-matrix for the coefficients of the variables, which, multiplied with the <math>(4,1)</math>-(column)-matrix of the variables, is equaled to the <math>(3,1)</math>-matrix of the constant terms. For this ready extendability to higher dimensions, the matrix notation is a common representation tool for a [[system of linear equations]], in [[linear algebra]], and in computer programming. There are named methods for solving [[system of linear equations|systems of linear equations]], like [[Gauss-Jordan]] which can be expressed in matrix elementary row operations.\n\n===Parametric form===\nThe parametric form of a curve is useful to e.g. describe the movement of a point along this curve, and controlling this movement with a single parameter. This setting resembles the task in physics, where a particle starts at time <math>t=0</math> at some point in space, say <math>(h,k)</math>, and travels along the curve, where it reaches point <math>(p,q)</math> at time <math>t=1.</math> With linear equations the curves are restricted to straight lines.\n\nThis task can be solved by adding a motion from <math>h\\to p</math> in the direction of the <math>x</math>-axis and a simultaneous motion from <math>k\\to q</math> in the direction of the <math>y</math>-axis, both motions controlled by the parameter <math>t.</math> The motion in the <math>x</math>-direction can be described as\n:<math>x = (p-h) t + h</math>\nand similarly, the motion in the <math>y</math>-direction can be described as\n:<math>y = (q-k) t + k.</math>\n\nThese two linear equations, with variables <math>(t,x)</math> and <math>(t,y)</math>, make up a ''parametric form'' for a linear equation with variables <math>(x,y)</math> that can be constructed from the two-point form with <math>(h,k)</math> and <math>(p,q)</math> as points.\n\nFor <math>t=0: \\quad (x,y)|_{t=0} = (h,k)\\quad</math> and for <math>t=1: \\quad (x,y))|_{t=1} = (p,q).</math> For all <math>t</math> in the interval <math>[0,1]</math> the point <math>(x,y)|_t</math> is on the straight line segment connecting the points for <math>t=0</math> and <math>t=1.</math> This is sometimes called [[interpolation]]. For values of <math>t</math> outside this interval, points outside of the segment, but still on the line are addressed [[extrapolation]].\n\n=== Connection with linear functions ===\n{{see also|Linearity#In mathematics|linear map}}\nA linear equation, written in the form ''y'' = ''f''(''x'') whose  graph crosses the origin (''x'',''y'') = (0,0), that is, whose ''y''-intercept is 0, has the following properties:\n\n* [[Additive map|Additivity]]: <math> f ( x_1 + x_2 ) = f ( x_1) + f ( x_2 )\\ </math>\n* [[Homogeneous function|Homogeneity]] of degree 1: <math> f ( a x ) = a f ( x ),\\,</math>\n\nwhere ''a'' is any [[scalar (mathematics)|scalar]]. A function which satisfies these properties is called a ''linear function'' (or ''linear operator'', or more generally a ''[[linear map]]''). However, linear equations that have non-zero ''y''-intercepts, when written in this manner, produce functions which will have neither property above and hence are not linear functions in this sense. They are known as [[affine function]]s.\n\n=== Example ===\nAn everyday example of the use of different forms of linear equations is computation of tax with [[tax bracket]]s. This is commonly done with a [[Progressive tax#Computation|progressive tax computation]], using either point–slope form or slope–intercept form.\n\n==More than two variables==\nFor the general case of a linear equation with <math>n>2</math> [[Equation solving|unknowns]] the equation may always be assumed to be denoted as at the top \n:<math>a_1 x_1 + a_2 x_2 + \\cdots + a_n x_n + b=0.</math>\n\nSometimes <math>b</math> is called the ''absolute term'', and the term ''coefficient'' is reserved for the <math>a_i.</math> A variant to denote <math>b,</math> stemming from the use in polynomials, is to write <math>a_0</math> instead, alluding to the zeroth power of any variable, that always reduces to <math>1.</math>\n\nWhen dealing with <math>n=3</math> variables, it is common to use <math>x,\\; y</math> and <math>z</math> instead of indexed variables.\n\nThe set of solutions of such an equation is a set of <math>n</math>-tuples, and each <math>n</math>-tuple makes the equation an identity, when its components are inserted for the respective unknowns. The values of the variables with zero coefficients are taken arbitrarily from the field of coefficients. \n\nFor an equation to have meaningful solutions, at least one coefficient must be non-zero. This can be formulated as \n:<math>a_1^2 + a_2^2 +\\cdots + a_n^2 = \\textstyle\\sum_{i=1}^n a_i^2 > 0. </math>\n\nIf all coefficients <math>a_i</math> equal zero, then, as mentioned for one variable, the equation is either ''inconsistent'' (for <math>b\\ne 0</math>) and there is no solution, or all\n{{nowrap|<math>n</math>-tuples}} are solutions.\n\nThe set of solutions {{nowrap|(<math>n</math>-tuples)}} of a linear equation in {{nowrap|<math>n</math> variables}} is an {{nowrap|<math>(n-1)</math>-dimensional}} [[hyperplane]] in an {{nowrap|<math>n</math>-dimensional}} [[Euclidean space]] (or [[affine space]] if the coefficients are complex numbers or belong to any field). Within the usual setting of real numbers and a [[three-dimensional space]] with Cartesian coordinates, the set of the solutions of a linear equation with three variables describes a plane in the intuitive sense.\n\nA given equation may be solved for all variables with a non-zero coefficient. Let <math>j</math> be an index such that <math>a_j\\ne 0,</math> then\n:<math>x_j = -(\\tfrac b{a_j} +\\tfrac {a_1}{a_j}x_1 + \\cdots + 0\\cdot x_j+ \\cdots +\\tfrac {a_n}{a_j}x_n).</math>\n\nThis way the linear equation can be seen as defining a function of <math>(n-1)</math> variables, which maps, assuming the setting of reals, the set of {{nowrap|<math>(n-1)</math>-tuples<ref>The (n-1)-tuples are ordered to represent the removal of ''j'' from the sequence {1..n}.</ref>}} of reals to the real numbers, i.e.:\n:<math>x_j:\\;\\mathbb R^{n-1}\\to \\mathbb R</math>\n\n== See also ==\n* [[Line (geometry)]]\n* [[Linear function]]\n* [[Linear equation over a ring]]\n* [[Algebraic equation]]\n* [[Linear belief function]]\n* [[Linear inequality]]\n\n== Notes ==\n{{Reflist}}\n\n== References ==\n* {{citation|first1=R.A.|last1=Barnett|first2=M.R.|last2=Ziegler|first3=K.E.|last3=Byleen|title=College Mathematics for Business, Economics, Life Sciences and the Social Sciences|edition=11th|year=2008|publisher=Pearson|place=Upper Saddle River, N.J.|isbn=0-13-157225-3}}\n* {{citation|first1=Ron|last1=Larson|first2=Robert|last2=Hostetler|title=Precalculus:A Concise Course|year=2007|publisher=Houghton Mifflin|isbn=978-0-618-62719-6}}\n* {{citation|first1=W.A.|last1=Wilson|first2=J.I.|last2=Tracey|title=Analytic Geometry|edition=revised|year=1925|publisher=D.C. Heath}}\n\n== External links ==\n* [http://catalog.flatworldknowledge.com/bookhub/reader/128?e=fwk-redden-ch02 Linear Equations and Inequalities] Open Elementary Algebra textbook chapter on linear equations and inequalities.\n* {{springer|title=Linear equation|id=p/l059190}}\n\n{{Polynomials}}\n\n{{DEFAULTSORT:Linear Equation}}\n[[Category:Elementary algebra]]\n[[Category:Equations]]"
    },
    {
      "title": "Linear function",
      "url": "https://en.wikipedia.org/wiki/Linear_function",
      "text": "In [[mathematics]], the term '''linear function''' refers to two distinct but related notions:<ref>\"The term ''linear function'' means a linear form in some textbooks and an affine function in others.\" Vaserstein 2006, p. 50-1</ref>\n* In [[calculus]] and related areas, a linear function is a [[function (mathematics)|function]] whose [[graph of a function|graph]] is a [[straight line]], that is a [[polynomial function]] of [[polynomial degree|degree]] one or zero.<ref>Stewart 2012, p. 23</ref>\n* In [[linear algebra]], [[mathematical analysis]]<ref>{{cite book|author=T. M. Apostol|title=Mathematical Analysis|year=1981|publisher=Addison-Wesley|page=345}}</ref>, and [[functional analysis]], a linear function is a [[linear map]].<ref>Shores 2007, p. 71</ref> In this case, and in case of possible ambiguity, the name [[Affine transformation|affine function]] is often used for the concept above.<ref>{{cite book|author=A. Kurosh|title=Higher Algebra|year=1975|publisher=Mir Publishers|page=214}}</ref>\n\n== As a polynomial function ==\n{{main article|Linear function (calculus)}}\n[[File:Linear Function Graph.svg|thumb|Graphs of two linear (polynomial) functions.]]\n\nIn calculus, [[analytic geometry]] and related areas, a linear function is a polynomial of degree one or less, including the [[zero polynomial]] (the latter not being considered to have degree zero).\n\nWhen the function is of only one [[variable (mathematics)|variable]], it is of the form\n:<math>f(x)=ax+b,</math>\nwhere {{mvar|''a''}} and {{mvar|''b''}} are [[constant (mathematics)|constant]]s, often [[real number]]s. The [[graph of a function|graph]] of such a function of one variable is a nonvertical line. {{mvar|''a''}} is frequently referred to as the slope of the line, and {{mvar|''b''}} as the intercept.\n\nFor a function <math>f(x_1, \\ldots, x_k)</math> of any finite number of [[independent variable]]s, the general formula is\n:<math>f(x_1, \\ldots, x_k) = b + a_1 x_1 + \\ldots + a_k x_k</math>,\nand the graph is a [[hyperplane]] of dimension {{nowrap|''k''}}.\n\nA [[constant function]] is also considered linear in this context, as it is a polynomial of degree zero or is the zero polynomial. Its graph, when there is only one independent variable, is a horizontal line.\n\nIn this context, the other meaning (a linear map) may be referred to as a [[homogeneous function|homogeneous]] linear function or a [[linear form]].  In the context of linear algebra, this meaning (polynomial functions of degree 0 or 1) is a special kind of [[affine map]].\n\n== As a linear map ==\n{{main article|Linear map}}\n[[File:Integral as region under curve.svg|thumb|The [[integral]] of a function is a linear map from the vector space of integrable functions to the real numbers.]]\n\nIn linear algebra, a linear function is a map ''f'' between two [[vector space]]s that preserves [[vector addition]] and [[scalar multiplication]]:\n:<math>f(\\mathbf{x} + \\mathbf{y}) = f(\\mathbf{x}) + f(\\mathbf{y}) </math>\n:<math>f(a\\mathbf{x}) = af(\\mathbf{x}). </math>\nHere {{math|''a''}} denotes a constant belonging to some [[field (mathematics)|field]] {{math|''K''}} of [[Scalar (mathematics)|scalar]]s (for example, the [[real number]]s) and {{math|'''x'''}} and {{math|'''y'''}} are elements of a [[vector space]], which might be {{math|''K''}} itself.\n\nSome authors use \"linear function\" only for linear maps that take values in the scalar field;<ref>Gelfand 1961</ref> these are also called [[linear functional]]s.\n\nThe \"linear functions\" of calculus qualify as \"linear maps\" when (and only when) <math>f([0,\\ldots,0]) = 0</math>, or, equivalently, when the constant <math>b = 0</math>.  Geometrically, the graph of the function must pass through the origin.\n\n== See also ==\n* [[Homogeneous function]]\n* [[Nonlinear system]]\n* [[Piecewise linear function]]\n* [[Linear approximation]]\n* [[Linear interpolation]]\n* [[Discontinuous linear map]]\n\n== Notes ==\n<references/>\n\n== References ==\n* Izrail Moiseevich Gelfand (1961), ''Lectures on Linear Algebra'', Interscience Publishers, Inc., New York. Reprinted by Dover, 1989. {{isbn|0-486-66082-6}}\n* Thomas S. Shores (2007), ''Applied Linear Algebra and Matrix Analysis'', [[Undergraduate Texts in Mathematics]], Springer. {{isbn|0-387-33195-6}}\n*James Stewart (2012), ''Calculus: Early Transcendentals'', edition 7E, Brooks/Cole. {{isbn|978-0-538-49790-9}}\n* Leonid N. Vaserstein (2006), \"Linear Programming\", in Leslie Hogben, ed., ''Handbook of Linear Algebra'', Discrete Mathematics and Its Applications, Chapman and Hall/CRC, chap. 50.  {{isbn|1-584-88510-6}}\n\n== External links ==\n{{Polynomials}}\n\n[[Category:Polynomial functions]]"
    },
    {
      "title": "Linear function (calculus)",
      "url": "https://en.wikipedia.org/wiki/Linear_function_%28calculus%29",
      "text": "{{distinguish|linear functional|linear map}}\n[[Image:wiki linear function.png|thumb|right|Graph of the linear function: {{math|1=''y''(''x'') = −''x'' + 2}}]]<!-- people, find an SVG image please instead of this abomination -->\nIn [[calculus]] and related areas of mathematics, a '''linear function''' from the real numbers to the real numbers is a function whose graph (in [[Cartesian coordinates]] with uniform scales) is a [[line (geometry)|line]] in the plane.<ref>Stewart 2012, p. 23</ref>  \nThe characteristic property of linear functions is that when the input variable is changed, the change in the output is [[Proportionality (mathematics)|proportional]] to the change in the input.\n\nLinear functions are related to [[linear equation]]s.\n\n== Properties ==\nA linear function is a [[polynomial function]] in which the [[variable (mathematics)|variable]] {{mvar|x}} has degree at most one:<ref>Stewart 2012, p. 24</ref>\n:<math>f(x)=ax+b</math>.\nSuch a function is called ''linear'' because its [[graph of a function|graph]], the set of all points <math>(x,f(x))</math> in the [[Cartesian plane]], is a [[line (geometry)|line]]. The coefficient ''a'' is called the ''slope'' of the function and of the line (see below).\n \nIf the slope is <math>a=0</math>, this is a ''constant function'' <math>f(x)=b</math> defining a horizontal line, which some authors exclude from the class of linear functions.<ref>{{harvnb|Swokowski|1983|loc=p. 34}}</ref> With this definition, the degree of a linear polynomial would be exactly one, its graph a diagonal line neither vertical nor horizontal. However, we will not require <math>a\\neq 0</math> in this article, so constant functions will be considered linear.\n\nThe natural [[Domain of a function|domain]] of a linear function <math>f(x)</math>, the set of allowed input values for ''x'', is the entire set of [[real number]]s, <math>x\\in \\mathbb R</math>. One can also consider such functions with ''x'' in an arbitrary [[field (mathematics)|field]], taking the coefficients ''a,b'' in that field. \n\nThe graph <math>y=f(x)=ax+b</math> is a non-vertical line having exactly one intersection with the ''y''-axis, its ''y''-intercept point <math>(x,y)=(0,b)</math>. The ''y''-intercept value <math>y=f(0)=b</math> is also called the ''initial value'' of <math>f(x)</math>. If <math>a\\neq 0</math>, the graph is a non-horizontal line having exactly one intersection with the ''x''-axis, the ''x''-intercept point <math>(x,y)=(-\\tfrac ba,0)</math>. The ''x''-intercept value <math>x=-\\tfrac ba</math>, the solution of the equation <math>f(x)=0</math>, is also called the ''root'' or [[zero of a function|''zero'']] of <math>f(x)</math>.\n\n==Slope==\n[[File:Slope picture.svg|thumb|right|128px|The slope of a line is the ratio <math>\\tfrac{\\Delta y}{\\Delta x}</math> between a change in {{mvar|x}}, denoted {{math|&Delta;''x''}}, and the corresponding change in {{mvar|y}}, denoted {{math|&Delta;''y''}}]]\nThe [[slope (mathematics)|slope]] of a nonvertical line is a number that measures how steeply the line is slanted (rise-over-run). If the line is the graph of the linear function {{math|1=''f''(''x'') = ''ax'' + ''b''}}, this slope is given by the constant {{mvar|a}}. \n\nThe slope measures the constant rate of change of <math>f(x)</math> per unit change in ''x'': whenever the input {{mvar|x}} is increased by one unit, the output changes by {{mvar|a}} units: <math>f(x{+}1)=f(x)+a</math>, and more generally <math>f(x{+}\\Delta x)=f(x)+a\\Delta x</math> for any number <math>\\Delta x</math>. If the slope is positive, <math>a > 0</math>, then the function <math>f(x)</math> is increasing; if <math>a < 0</math>, then <math>f(x)</math> is decreasing\n\nIn [[differential calculus|calculus]], the derivative of a general function measures its rate of change. A linear function <math>f(x)=ax+b</math> has a constant rate of change equal to its slope {{mvar|a}}, so its derivative is the constant function <math>f\\,'(x)=a</math>. \n\nThe fundamental idea of differential calculus is that any [[differentiable function|smooth]] function <math>f(x)</math> (not necessarily linear) can be closely [[linear approximation|approximated]] near a given point <math>x=c</math> by a unique linear function. The [[derivative]] <math>f\\,'(c)</math> is the slope of this linear function, and the approximation is: <math>f(x) \\approx f\\,'(c)(x{-}c)+f(c)</math> for <math>x\\approx c</math>. The graph of the linear approximation is the [[tangent line]] of the graph <math>y=f(x)</math> at the point <math>(c,f(c))</math>. The derivative slope <math>f\\,'(c)</math> generally varies with the point ''c''. Linear functions can be characterized as the only real functions whose derivative is constant: if <math>f\\,'(x)=a</math> for all ''x'', then <math>f(x)=ax+b</math> for <math>b=f(0)</math>.\n\n==Slope-intercept, point-slope, and two-point forms==\nA given linear function <math>f(x)</math> can be written in several standard formulas displaying its various properties. The simplest is the ''slope-intercept form'': \n:<math>f(x)= ax+b</math>, \nfrom which one can immediately see the slope ''a'' and the initial value <math>f(0)=b</math>, which is the ''y''-intercept of the graph <math>y=f(x)</math>. \n\nGiven a slope ''a'' and one known value <math>f(x_0)=y_0</math>, we write the ''point-slope form'': \n:<math>f(x) = a(x{-}x_0)+y_0</math>. \nIn graphical terms, this gives the line <math>y=f(x)</math> with slope ''a'' passing through the point <math>(x_0,y_0)</math>. \n\nThe ''two-point form'' starts with two known values <math>f(x_0)=y_0</math> and <math>f(x_1)=y_1</math>. One computes the slope <math>a=\\tfrac{y_1-y_0}{x_1-x_0}</math> and inserts this into the point-slope form: \n:<math>f(x) = \\tfrac{y_1-y_0}{x_1-x_0}(x{-}x_0\\!) + y_0</math>.\nIts graph <math>y=f(x)</math> is the unique line passing through the points <math>(x_0,y_0\\!), (x_1,y_1\\!)</math>. The equation <math>y=f(x)</math> may also be written to emphasize the constant slope:\n:<math>\\frac{y-y_0}{x-x_0}=\\frac{y_1-y_0}{x_1-x_0}</math>.\n\n==Relationship with linear equations==\n[[Image:wiki linearna funkcija eks1.png|thumb|right]]<!-- are PNG and a translit from a foreign language necessary? --> \nLinear functions commonly arise from practical problems involving variables <math>x,y</math> with a linear relationship, that is, obeying a [[linear equation]] <math>Ax+By=C</math>. If  <math>B\\neq 0</math>, one can solve this equation for ''y'', obtaining\n:<math>y = -\\tfrac{A}{B}x +\\tfrac{C}{B}=ax+b,</math>\nwhere we denote <math>a=-\\tfrac{A}{B}</math> and <math>b=\\tfrac{C}{B}</math>. That is, one may consider ''y'' as a dependent variable (output) obtained from the independent variable (input) ''x'' via a linear function: <math>y = f(x) = ax+b</math>. In the ''xy''-coordinate plane, the possible values of <math>(x,y)</math> form a line, the graph of the function <math>f(x)</math>.  If <math>B=0</math> in the original equation, the resulting line <math>x=\\tfrac{C}{A}</math> is vertical, and cannot be written as <math>y=f(x)</math>.\n\nThe features of the graph <math>y = f(x) = ax+b</math> can be interpreted in terms of the variables ''x'' and ''y''. The ''y''-intercept is the initial value <math>y=f(0)=b</math> at <math>x=0</math>. The slope ''a'' measures the rate of change of the output ''y'' per unit change in the input ''x''. In the graph, moving one unit to the right (increasing ''x'' by 1) moves the ''y''-value up by ''a'': that is, <math>f(x{+}1) = f(x) + a</math>. Negative slope ''a'' indicates a decrease in ''y'' for each increase in ''x''.\n\nFor example, the linear function <math>y = -2x + 4</math> has slope <math>a=-2</math>, ''y''-intercept point <math>(0,b)=(0,4)</math>, and ''x''-intercept point <math>(2,0)</math>.\n\n===Example===\nSuppose salami and sausage cost €6 and €3 per kilogram, and we wish to buy €12 worth. How much of each can we purchase? Letting ''x'' and ''y'' be the weights of salami and sausage, the total cost is: <math>6x + 3y = 12</math>. Solving for ''y'' gives the point-slope form <math>y = -2x + 4</math>, as above. That is, if we first choose the amount of salami ''x'', the amount of sausage can be computed as a function <math>y = f(x) = -2x + 4</math>. Since salami costs twice as much as sausage, adding one kilo of salami decreases the sausage by 2 kilos: <math>f(x{+}1) = f(x) - 2</math>, and the slope is &minus;2. The ''y''-intercept point <math>(x,y)=(0,4)</math> corresponds to buying only 4kg of sausage; while the ''x''-intercept point <math>(x,y)=(2,0)</math> corresponds to buying only 2kg of salami. \n\nNote that the graph includes points with negative values of ''x'' or ''y'', which have no meaning in terms of the original variables (unless we imagine selling meat to the butcher). Thus we should restrict our function <math>f(x)</math> to the domain <math>0\\le x\\le 2</math>. \n\nAlso, we could choose ''y'' as the independent variable, and compute ''x'' by the [[inverse function|inverse]] linear function: <math>x = g(y) = -\\tfrac12 y +2</math> over the domain <math>0\\le y \\le 4</math>.\n\n== Relationship with other classes of functions ==\n\nIf the coefficient of the variable is not zero ({{math|''a'' ≠ 0}}), then a linear function is represented by a [[degree of a polynomial|degree]] 1 [[polynomial]] (also called a ''linear polynomial''), otherwise it is a [[constant function]] – also a polynomial function, but of zero degree.\n\nA straight line, when drawn in a different kind of coordinate system may represent other functions.\n\nFor example, it may represent an [[exponential growth|exponential function]] when its [[codomain|values]] are expressed in the [[logarithmic scale]]. It means that when {{math|[[logarithm|log]](''g''(''x''))}} is a linear function of {{mvar|x}}, the function {{mvar|g}} is exponential. With linear functions, increasing the input by one unit causes the output to increase by a fixed amount, which is the slope of the graph of the function. With exponential functions, increasing the input by one unit causes the output to increase by a fixed multiple, which is known as the base of the exponential function.\n\nIf ''both'' [[domain of a function|arguments]] and values of a function are in the logarithmic scale (i.e., when {{math|[[logarithm|log]](''y'')}} is a linear function of {{math|[[logarithm|log]](''x'')}}), then the straight line represents a [[power law]]:\n:<math>\\log_r y = a \\log_r x + b \\quad\\Rightarrow\\quad y = r^b\\cdot x^a</math>\n\n[[File:Archimedean-Spiral.png|thumb|Archimedean spiral defined by the polar equation r = &frac12;&theta; + 2]]\nOn the other hand, the graph of a linear function in terms of [[polar coordinates]]: \n:<math>r =f(\\theta ) = a\\theta  + b</math>\nis an [[Archimedean spiral]] if <math>a \\neq 0</math> and a [[circle]] otherwise.\n\n== Notes ==\n<references/>\n\n== See also ==\n* [[Affine map]], a generalization\n* [[Arithmetic progression]], a linear function of integer argument\n\n== References ==\n* James Stewart (2012), ''Calculus: Early Transcendentals'', edition 7E, Brooks/Cole. {{isbn|978-0-538-49790-9}}\n* {{citation|first=Earl W.|last=Swokowski|title=Calculus with analytic geometry|edition=Alternate|year=1983|publisher=Prindle, Weber & Schmidt|place=Boston|isbn=0871503417}}\n\n== External links ==\n* https://web.archive.org/web/20130524101825/http://www.math.okstate.edu/~noell/ebsm/linear.html\n* http://www.corestandards.org/assets/CCSSI_Math%20Standards.pdf\n\n{{Polynomials}}\n\n[[Category:Calculus]]\n[[Category:Polynomial functions]]"
    },
    {
      "title": "Linearised polynomial",
      "url": "https://en.wikipedia.org/wiki/Linearised_polynomial",
      "text": "In mathematics, a '''linearised polynomial''' (or ''q''- polynomial) is a [[polynomial]] for which the exponents of all the constituent [[monomial]]s are powers of ''q'' and the coefficients come from some extension field of the [[finite field]] of order ''q''.\n\nWe write a typical example as\n\n:<math>L(x) = \\sum_{i=0}^n a_i x^{q^i}, \\text{ where each } a_i \\text{ is in } F_{q^m} (\\text { = } GF(q^m)) \\text{ for some fixed positive integer }m. </math>\n\nThis special class of polynomials is important from both a theoretical and an applications viewpoint.<ref>{{harvnb|Lidl|Niederreiter|1983|loc=pg.107 (first edition)}}</ref> The highly structured nature of their roots makes these roots easy to determine.\n\n==Properties==\n* The map ''x'' → ''L''(''x'') is a linear map over any field containing '''F'''<sub>''q''</sub>\n* The set of roots of ''L'' is an '''F'''<sub>''q''</sub>-vector space and is closed under the ''q''-[[Frobenius map]]\n* Conversely, if ''U'' is any '''F'''<sub>''q''</sub>-linear subspace of some finite field containing '''F'''<sub>''q''</sub>, then the polynomial that vanishes exactly on ''U'' is a linearised polynomial.\n* The set of linearised polynomials over a given field is closed under addition and composition of polynomials.\n* If ''L'' is a nonzero linearised polynomial over <math>F_{q^n}</math> with all of its roots lying in the field <math>F_{q^s}</math> an extension field of <math>F_{q^n}</math>, then each root of ''L'' has the same multiplicity, which is either 1, or a positive power of ''q''.<ref>{{harvnb|Mullen|Panario|2013|loc=p. 23 (2.1.106)}}</ref>\n\n==Symbolic multiplication==\n \nIn general, the product of two linearised polynomials will not be a linearized polynomial, but since the composition of two linearised polynomials results in a linearised polynomial, composition may be used as a replacement for multiplication and, for this reason, composition is often called '''symbolic multiplication''' in this setting. Notationally, if ''L''<sub>1</sub>(''x'') and ''L''<sub>2</sub>(''x'') are linearised polynomials we define\n::<math>L_1(x) \\otimes L_2(x) = L_1(L_2(x))</math>\nwhen this point of view is being taken. \n\n==Associated polynomials==\nThe polynomials '''L'''(''x'') and\n\n:<math> l(x) = \\sum_{i=0}^n a_i x^i \\  </math>\n\nare ''q - associates'' (note: the exponents \"''q''<sup>''i''</sup> \" of ''L''(''x'') have been replaced by \"''i''\" in ''l''(''x'')). More specifically, ''l(x}'' is called the ''conventional q-associate'' of ''L(x)'', and ''L(x)'' is the ''linearised q-associate'' of ''l(x)''.\n\n==q-polynomials over '''F'''<sub>''q''</sub>==\nLinearised polynomials with coefficients in '''F'''<sub>''q''</sub> have additional properties which make it possible to define symbolic division, symbolic reducibility and symbolic factorization. Two important examples of this type of linearised polynomial are the Frobenius automorphism <math>x \\mapsto x^q</math> and the trace function <math>\\operatorname{Tr}(x) = \\sum_{i=0}^{n-1} x^{q^i}</math>.\n\nIn this special case it can be shown that, as an [[Operation (mathematics)|operation]], symbolic multiplication is [[Commutative property|commutative]], [[associative]] and [[Distributive property|distributes]] over ordinary addition.<ref>{{harvnb|Lidl|Niederreiter|1983|loc=pg. 115 (first edition)}}</ref> Also, in this special case, we can define the operation of '''symbolic division'''. If ''L''(''x'') and ''L''<sub>1</sub>(''x'') are linearised polynomials over '''F'''<sub>''q''</sub>, we say that ''L''<sub>1</sub>(''x'') ''symbolically divides'' ''L''(''x'') if there exists a linearised polynomial ''L''<sub>2</sub>(''x'') over '''F'''<sub>''q''</sub> for which:\n:<math>L(x) = L_1(x) \\otimes L_2(x).</math> \n\nIf ''L''<sub>1</sub>(''x'') and ''L''<sub>2</sub>(''x'') are linearised polynomials over '''F'''<sub>''q''</sub> with conventional q-associates ''l''<sub>1</sub>(''x'') and ''l''<sub>2</sub>(''x'') respectively, then ''L''<sub>1</sub>(''x'') symbolically divides ''L''<sub>2</sub>(''x'') if and only if ''l''<sub>1</sub>(''x'') divides ''l''<sub>2</sub>(''x'').<ref>{{harvnb|Lidl|Niederreiter|1983|loc=pg. 115 (first edition) Corollary 3.60}}</ref> Furthermore, \n''L''<sub>1</sub>(''x'') divides ''L''<sub>2</sub>(''x'') in the ordinary sense in this case.<ref>{{harvnb|Lidl|Neiderreiter|1983|loc=pg. 116 (first edition) Theorem 3.62}}</ref>\n\nA linearised polynomial ''L''(''x'') over '''F'''<sub>''q''</sub> of degree > 1 is ''symbolically irreducible'' over '''F'''<sub>''q''</sub> if the only symbolic decompositions\n::<math>L(x) = L_1(x) \\otimes L_2(x),</math>\nwith ''L''<sub>''i''</sub> over '''F'''<sub>''q''</sub> are those for which one of the factors has degree 1. Note that a symbolically irreducible polynomial is always [[Reducible expression|reducible]] in the ordinary sense since any linearised polynomial of degree > 1 has the nontrivial factor ''x''. A linearised polynomial ''L''(''x'') over '''F'''<sub>''q''</sub> is symbolically irreducible if and only if its conventional ''q''-associate ''l''(''x'') is irreducible over '''F'''<sub>''q''</sub>.\n\nEvery ''q''-polynomial ''L''(''x'') over '''F'''<sub>''q''</sub> of degree > 1 has a ''symbolic factorization'' into symbolically irreducible polynomials over '''F'''<sub>''q''</sub> and this factorization is essentially unique (up to rearranging factors and multiplying by nonzero elements of '''F'''<sub>''q''</sub>.)\n\nFor example,<ref>{{harvnb|Lidl|Neiderreiter|1983|loc=pg. 117 (first edition) Example 3.64}}</ref> consider the 2-polynomial ''L''(''x'') = ''x''<sup>16</sup> + ''x''<sup>8</sup> + ''x''<sup>2</sup> + ''x'' over '''F'''<sub>2</sub> and its conventional 2-associate ''l''(''x'') = ''x''<sup>4</sup> + ''x''<sup>3</sup> + ''x'' + 1. The factorization into irreducibles of ''l''(''x'') = (''x''<sup>2</sup> + ''x'' + 1)(''x'' + 1)<sup>2</sup> in '''F'''<sub>2</sub>[''x''], gives the symbolic factorization \n::<math>L(x) = (x^4 + x^2 + x) \\otimes (x^2 + x) \\otimes (x^2 + x).</math>\n\n==Affine polynomials==\n\nLet ''L'' be a linearised polynomial over <math>F_{q^n}</math>. A polynomial of the form <math>A(x) = L(x) - \\alpha \\text{ for } \\alpha \\in F_{q^n},</math> is an ''affine polynomial'' over <math>F_{q^n}</math>.\n\nTheorem: If ''A'' is a nonzero affine polynomial over <math>F_{q^n}</math> with all of its roots lying in the field <math>F_{q^s}</math> an extension field of <math>F_{q^n}</math>, then each root of ''A'' has the same multiplicity, which is either 1, or a positive power of ''q''.<ref>{{harvnb|Mullen|Panario|2013|loc=p. 23 (2.1.109)}}</ref>\n\n==Notes==\n{{reflist}}\n\n==References==\n* {{cite book | zbl=0866.11069 | last1=Lidl | first1=Rudolf | last2=Niederreiter | first2=Harald | author2-link = Harald Niederreiter | title=Finite fields | edition=2nd | series=Encyclopedia of Mathematics and Its Applications | volume=20 | publisher=[[Cambridge University Press]] | year=1997 | isbn=0-521-39231-4 }} \n* {{citation|first1=Gary L.|last1=Mullen|first2=Daniel|last2=Panario|title=Handbook of Finite Fields|year=2013|publisher=CRC Press|place=Boca Raton|series=Discrete Mathematics and its Applications|isbn=978-1-4398-7378-6}}\n\n[[Category:Polynomials]]"
    },
    {
      "title": "Littlewood polynomial",
      "url": "https://en.wikipedia.org/wiki/Littlewood_polynomial",
      "text": "{{for|the orthogonal polynomials in several variables|Hall–Littlewood polynomials}}\n[[File:Roots of Littlewood polynomials with degree 15.png|thumb|313x313px|Roots of all the Littlewood polynomials of degree 15.]]\nIn [[mathematics]], a '''Littlewood polynomial''' is a [[polynomial]] all of whose coefficients are +1 or &minus;1.\n'''Littlewood's problem''' asks how large the values of such a polynomial must be on the [[unit circle]] in the [[complex plane]].  The answer to this would yield information about the [[autocorrelation]] of binary sequences.\nThey are named for [[J. E. Littlewood]] who studied them in the 1950s.\n\n==Definition==\nA polynomial\n\n:<math> p(x) = \\sum_{i=0}^n a_i x^i \\, </math>\n\nis a ''Littlewood polynomial'' if all the <math>a_i = \\pm 1</math>.  ''Littlewood's problem'' asks for constants ''c''<sub>1</sub> and ''c''<sub>2</sub> such that there are infinitely many Littlewood polynomials ''p''<sub>''n''</sub> , of increasing degree ''n'' satisfying\n\n:<math>c_1 \\sqrt{n+1} \\le | p_n(z) | \\le c_2 \\sqrt{n+1} . \\, </math>\n\nfor all <math>z</math> on the unit circle. The [[Shapiro polynomials|Rudin–Shapiro polynomials]] provide a sequence satisfying the upper bound with <math>c_2 = \\sqrt 2</math>.  No sequence is known (as of 2008) that satisfies the lower bound.\n\n==References==\n*{{cite book | author=Peter Borwein | authorlink=Peter Borwein | title=Computational Excursions in Analysis and Number Theory | series=CMS Books in Mathematics | publisher=[[Springer-Verlag]] | year=2002 | isbn=0-387-95444-9 | pages=2–5,121–132 }}\n*{{cite book | author=J.E. Littlewood | authorlink=John Edensor Littlewood | title=Some problems in real and complex analysis | publisher=D.C. Heath | year=1968 }}\n\n[[Category:Polynomials]]\n[[Category:Conjectures]]"
    },
    {
      "title": "Mahler measure",
      "url": "https://en.wikipedia.org/wiki/Mahler_measure",
      "text": "{{Use American English|date = March 2019}}\n{{Short description|Measure of polynomial height}}\nIn [[mathematics]], the '''Mahler measure''' <math>M(p)</math> '''of a [[polynomial]]''' <math>p(z) </math> with [[complex number|complex]] [[coefficient]]s is defined as\n\n:<math>M(p) = |a|\\prod_{|\\alpha_i| \\ge 1} |\\alpha_i| = |a| \\prod_{i=1}^n \\max\\{1,|\\alpha_i|\\},</math>\nwhere <math>p(z) </math> factorizes over the complex numbers <math>\\mathbb{C}</math> as\n:<math>p(z) = a(z-\\alpha_1)(z-\\alpha_2)\\cdots(z-\\alpha_n).</math>\n\nThe Mahler measure can be viewed as a kind of [[Height of a polynomial|height function]]. Using [[Jensen's formula]], it can be proved that this measure is also equal to the [[geometric mean]] of <math>|p(z)| </math> for <math>z</math> on the [[unit circle]] (i.e., <math>|z| = 1</math>):\n:<math>M(p) = \\exp\\left( \\frac{1}{2\\pi} \\int_{0}^{2\\pi} \\ln(|p(e^{i\\theta})|)\\, d\\theta \\right).</math>\n\nBy extension, the '''Mahler measure of an [[algebraic number]]''' <math>\\alpha</math> is defined as the Mahler measure of the [[Minimal polynomial (field theory)|minimal polynomial]] of <math>\\alpha</math> over <math>\\mathbb{Q}</math>. In particular, if <math>\\alpha</math> is a [[Pisot number]] or a [[Salem number]], then its Mahler measure is simply <math>\\alpha</math>.\n\nThe Mahler measure is named after the German-born Australian [[mathematician]] [[Kurt Mahler]].\n\n== Properties ==\n\n* The '''Mahler measure''' is multiplicative: <math>\\forall p, q, \\,\\, M(p \\cdot q) = M(p) \\cdot M(q).</math>\n* <math>M(p) = \\lim_{\\tau \\rightarrow 0} \\|p\\|_{\\tau}</math> where <math>\\, \\|p\\|_\\tau =\\left( \\frac{1}{2\\pi} \\int_0^{2\\pi} |p(e^{i\\theta})|^\\tau \\, d\\theta \\right)^{1/\\tau}  \\,</math> is the [[Lp norm|<math>L_\\tau</math> norm]] of <math>p</math>.<ref>Although this is not a true norm for values of <math>\\tau < 1</math>.</ref>\n* '''Kronecker's Theorem''': If <math>p</math> is an irreducible monic integer polynomial with <math>M(p) = 1</math>, then either <math>p(z) = z,</math> or <math>p</math> is a [[cyclotomic polynomial]].\n* ''([[Lehmer's conjecture]])'' There is a constant <math>\\mu>1</math> such that if <math>p</math> is an irreducible integer polynomial, then either <math>M(p)=1</math> or <math>M(p)>\\mu</math>.\n* The Mahler measure of a monic integer polynomial is a [[Perron number]].\n\n== Higher-dimensional Mahler measure ==\n\nThe Mahler measure <math>M(p)</math> of a multi-variable polynomial <math>p(x_1,\\ldots,x_n) \\in \\mathbb{C}[x_1,\\ldots,x_n]</math> is defined similarly by the formula<ref name=Sch224>{{harvnb|Schinzel|2000|page=224}}.</ref>\n\n:<math>M(p) = \\exp\\left( \\frac{1}{(2\\pi)^n} \\int_0^{2\\pi} \\int_0^{2\\pi} \\cdots \\int_0^{2\\pi} \\log \\Bigl( \\bigl |p(e^{i\\theta_1}, e^{i\\theta_2}, \\ldots, e^{i\\theta_n}) \\bigr| \\Bigr) \\, d\\theta_1\\, d\\theta_2\\cdots d\\theta_n \\right).</math>\nIt inherits the above three properties of the Mahler measure for a one-variable polynomial.\n\nThe multi-variable Mahler measure has been shown, in some cases, to be related to special values\nof [[zeta function|zeta-functions]] and [[L function|<math>L</math>-functions]]. For example, in 1981, Smyth<ref>{{harvnb|Smyth|2008}}.</ref> proved the formulas\n:<math> m(1+x+y)=\\frac{3\\sqrt{3}}{4\\pi}L(\\chi_{-3},2)</math>\nwhere <math>L(\\chi_{-3},s)</math> is the [[Dirichlet L-function]], and\n:<math> m(1+x+y+z)=\\frac{7}{2\\pi^2}\\zeta(3)</math> ,\nwhere <math>\\zeta</math> is the [[Riemann zeta function]]. Here <math> m(P)=\\log{M(P)}</math> is  called the ''logarithmic Mahler measure''.\n\n===Some results by Lawton and Boyd===\n\nFrom the definition, the Mahler measure is viewed as the integrated values of polynomials over the torus (also see [[Lehmer's conjecture]]). If <math>p</math> vanishes on the torus <math>(S^1)^n</math>, then the convergence of the integral defining <math>M(p)</math> is not obvious, but it is known that <math>M(p)</math> does converge and is equal to a limit of one-variable Mahler measures,<ref name=Law83>{{harvnb|Lawton|1983}}.</ref> which had been conjectured by [[David William Boyd|Boyd]].<ref name=Boyd1981a>{{harvnb|Boyd|1981a}}.</ref><ref name=Boyd1981b>{{harvnb|Boyd|1981b}}.</ref>\n\nThis is formulated as follows: Let <math>\\mathbb{Z}</math> denote the integers and define <math>\\mathbb{Z}^N_+=\\{r=(r_1,\\dots,r_N)\\in\\mathbb{Z}^N:r_j\\ge0\\ \\text{for}\\ 1\\le j\\le N\\}</math> . If <math>Q(z_1,\\dots,z_N)</math> is a polynomial in <math>N</math> variables and <math>r=(r_1,\\dots,r_N)\\in\\mathbb{Z}^N_+</math> define the polynomial <math>Q_r(z)</math> of one variable by\n\n:<math>Q_r(z):=Q(z^{r_1},\\dots,z^{r_N})</math>\n\nand define <math>q(r)</math> by\n\n:<math>q(r):=\\text{min}\\{H(s):s=(s_1,\\dots,s_N)\\in\\mathbb{Z}^N,s\\ne(0,\\dots,0)\\ \\text{and}\\ \\sum^N_{j=1}s_jr_j=0\\}</math>\n\nwhere <math>H(s)=\\text{max}\\{|s_j|:1\\le j\\le N\\}</math> .\n\n'''Theorem (Lawton)''' : Let <math>Q(z_1,\\dots,z_N)</math> be a polynomial in ''N'' variables with complex coefficients. Then the following limit is valid (even if the condition that <math>r_i\\ge0</math> is relaxed):\n:<math> \\lim_{q(r)\\rightarrow\\infty}M(Q_r)=M(Q)</math>\n\n===Boyd's proposal===\n\nBoyd provided more general statements than the above theorem. He pointed out  that the classical [[Kronecker's theorem#A result in diophantine approximation|Kronecker's theorem]], which characterizes monic polynomials with integer coefficients all of whose roots are inside the unit disk, can be regarded as characterizing those polynomials of one variable whose measure is exactly 1, and that this result extends to polynomials in several variables.<ref name=Boyd1981b />\n\nDefine an ''extended cyclotomic polynomial''  to be a polynomial of the form\n:<math>\\Psi(z)=z_1^{b_1} \\dots z_n^{b_n}\\Phi_m(z_1^{v_1}\\dots z_n^{v_n}),</math>\nwhere <math>\\Phi_m(z)</math> is the ''m''-th [[cyclotomic polynomial]], the <math>v_i</math> are integers, and the <math>b_i=\\max(0,-v_i\\deg\\Phi_m)</math> are chosen minimally so that <math>\\Psi(z)</math> is a polynomial in the <math>z_i</math>. Let  <math>K_n</math> be the set of polynomials that are products of monomials <math>\\pm z_1^{c_1}\\dots z_n^{c_n}</math> and extended cyclotomic polynomials.\n\n'''Theorem (Boyd)''' : Let <math> F(z_1,\\dots,z_n)\\in\\mathbb{Z}[z_1,\\ldots,z_n]</math> be a polynomial with integer coefficients. Then <math>M(F)=1</math> if and only if <math>F</math> is an element of <math>K_n</math>.\n\nThis led Boyd to consider the set of values\n:<math>L_n:=\\bigl\\{m(P(z_1,\\dots,z_n)):P\\in\\mathbb{Z}[z_1,\\dots,z_n]\\bigr\\},</math>\nand the union <math>{L}_\\infty=\\bigcup^\\infty_{n=1}L_n</math>. He made the far-reaching conjecture<ref name=Boyd1981a /> that the set of <math>{L}_\\infty</math> is a closed subset of <math>\\mathbb R</math>. An immediate consequence of this conjecture would be the truth of Lehmer's conjecture, albeit without an explicit lower bound. As Smyth's result suggests that <math>L_1\\subsetneqq L_2</math> , Boyd further conjectures that\n:<math>L_1\\subsetneqq L_2\\subsetneqq L_3\\subsetneqq\\ \\cdots\\,.</math>\n\n== See also ==\n\n*[[Bombieri norm]]\n*[[Height of a polynomial]]\n\n== Notes ==\n\n{{reflist}}\n\n== References ==\n\n*{{cite book\n | last = Borwein\n | first = Peter\n | author-link = Peter Borwein\n | title = Computational Excursions in Analysis and Number Theory\n | series = CMS Books in Mathematics\n | volume = 10\n | publisher = [[Springer Science+Business Media|Springer]]\n | year = 2002\n | pages = 3, 15\n | isbn = 978-0-387-95444-8\n | zbl = 1020.12001\n | ref = harv\n}}\n\n*{{cite journal\n | first = David\n | last = Boyd\n | author-link = David William Boyd\n | title = Speculations concerning the range of Mahler's measure\n | pages = 453–469\n | journal = Canad. Math. Bull.\n | volume = 24 | issue = 4 \n | year = 1981a\n | ref = harv\n | doi=10.4153/cmb-1981-069-5\n}}\n\n*{{cite journal\n | first = David\n | last = Boyd\n | author-link = David William Boyd\n | title = Kronecker's Theorem and Lehmer's Problem for Polynomials in Several Variables\n | pages = 116–121\n | journal = [[Journal of Number Theory]]\n | volume = 13\n | year = 1981b\n | ref = harv\n | doi=10.1016/0022-314x(81)90033-0\n}}\n\n*{{cite book\n | first = David\n | last = Boyd\n | author-link = David William Boyd\n | chapter = Mahler's measure and invariants of hyperbolic manifolds\n | title= Number theory for the Millenium\n | editor-first = M. A.\n | editor-last = Bennett\n | publisher = A. K. Peters\n | pages = 127–143\n | year = 2002a\n | ref = harv\n}}\n\n*{{cite journal\n | first = David\n | last = Boyd\n | author-link = David William Boyd\n | title = Mahler's measure, hyperbolic manifolds and the dilogarithm\n | pages = 3–4, 26–28\n | journal = Canadian Mathematical Society Notes\n | volume = 34\n | number = 2\n | year = 2002b\n | ref = harv\n}}\n\n*{{cite journal\n | first1 = David\n | last1 = Boyd\n | author1-link = David William Boyd\n | first2 = F.\n | last2 = Rodriguez Villegas\n | title = Mahler's measure and the dilogarithm, part 1\n | journal = Canadian Journal of Mathematics\n | volume = 54\n | issue = 3\n | pages = 468–492\n | year = 2002\n | ref = harv\n | doi=10.4153/cjm-2002-016-9\n}}\n\n* {{SpringerEOM||id=Mahler_measure&oldid=36175|title=Mahler measure}}.\n*{{cite journal\n | first = J.L.\n | last = Jensen\n | title = Sur un nouvel et important théorème de la théorie des fonctions\n | journal = [[Acta Mathematica]]\n | volume = 22\n | pages = 359–364\n | year = 1899\n | jfm = 30.0364.02\n | doi = 10.1007/BF02417878\n | ref = harv\n}}\n\n*{{cite book\n | last = Knuth\n | first = Donald E.\n | author-link = Donald E. Knuth\n | chapter = 4.6.2 Factorization of Polynomials\n | title = Seminumerical Algorithms\n | series = [[The Art of Computer Programming]]\n | volume = 2\n | edition = 3rd\n | publisher = Addison-Wesley\n | year = 1997\n | pages = 439–461, 678–691<!--   xiv+762 -->\n | isbn = 978-0-201-89684-8\n | ref = harv\n}}\n\n*{{cite journal\n | first = Wayne M.\n | last = Lawton\n | title = A problem of Boyd concerning geometric means of polynomials\n | journal = [[Journal of Number Theory]]\n | volume = 16\n | issue = 3\n | pages = 356–362\n | year = 1983\n | zbl = 0516.12018\n | doi = 10.1016/0022-314X(83)90063-X\n | ref = harv\n}}\n\n*{{cite journal\n | first = M.J.\n | last = Mossinghoff\n | title = Polynomials with Small Mahler Measure\n | journal = [[Mathematics of Computation]]\n | volume = 67\n | issue = 224\n | pages = 1697–1706\n | year = 1998\n | zbl = 0918.11056\n | doi = 10.1090/S0025-5718-98-01006-0\n | ref = harv\n}}\n\n*{{cite book\n | last = Schinzel\n | first = Andrzej\n | author-link = Andrzej Schinzel\n | title = Polynomials with special regard to reducibility\n | series = Encyclopedia of Mathematics and Its Applications\n | volume = 77\n | publisher = [[Cambridge University Press]]\n | year = 2000\n | isbn = 978-0-521-66225-3\n | zbl = 0956.12001\n | ref = harv\n}}\n\n*{{cite book\n | first = Chris\n | last = Smyth\n | chapter = The Mahler measure of algebraic numbers: a survey\n | pages = 322–349\n | editor1-first = James\n | editor1-last = McKee\n | editor2-last = Smyth\n | editor2-first = Chris\n | title = Number Theory and Polynomials\n | series = London Mathematical Society Lecture Note Series\n | volume = 352\n | publisher = [[Cambridge University Press]]\n | year = 2008\n | isbn = 978-0-521-71467-9\n | zbl = 1334.11081\n | ref = harv\n}}\n\n== External links ==\n\n*[http://mathworld.wolfram.com/MahlerMeasure.html Mahler Measure on MathWorld]\n*[http://mathworld.wolfram.com/JensensFormula.html Jensen's Formula on MathWorld]\n\n[[Category:Analytic number theory]]\n[[Category:Polynomials]]"
    },
    {
      "title": "Mahler polynomial",
      "url": "https://en.wikipedia.org/wiki/Mahler_polynomial",
      "text": "In mathematics, the '''Mahler polynomials''' ''g''<sub>''n''</sub>(''x'') are polynomials introduced by {{harvs|txt|last=Mahler|year=1930|authorlink=Kurt Mahler}} in his work on the zeros of the [[incomplete gamma function]].\n\nMahler polynomials are given by the [[generating function]]\n\n:<math>\\displaystyle \\sum g_n(x)t^n/n! = \\exp(x(1+t-e^t)) </math>\n\nMahler polynomials can be given as the [[Sheffer sequence]] for the functional inverse of 1+''t''–''e''<sup>''t''</sup> {{harv|Roman|1984|loc=4.9}}.\n\nThe first few examples are {{OEIS|A008299}}\n:<math>g_0=1;</math>\n:<math>g_1=0;</math>\n:<math>g_2=-x;</math>\n:<math>g_3=-x;</math>\n:<math>g_4=-x+3x^2;</math>\n:<math>g_5=-x+10x^2;</math>\n:<math>g_6=-x+25x^2-15x^3;</math>\n:<math>g_7=-x+56x^2-105x^3;</math>\n:<math>g_8=-x+119x^2-490x^3+105x^4;</math>\n\n==References==\n\n*{{Citation | last1=Mahler | first1=Kurt | title=Über die Nullstellen der unvollständigen Gammafunktionen. | language=German | jfm=56.0310.01 | year=1930 | journal=Rendiconti Palermo | volume=54 | pages=1–41}}\n*{{Citation | last1=Roman | first1=Steven | title=The umbral calculus | url=https://books.google.com/books?id=JpHjkhFLfpgC | publisher=Academic Press Inc. [Harcourt Brace Jovanovich Publishers] | location=London | series=Pure and Applied Mathematics | isbn=978-0-12-594380-2 | mr=741185 | year=1984 | volume=111}} Reprinted by Dover, 2005\n\n[[Category:Polynomials]]"
    },
    {
      "title": "Marden's theorem",
      "url": "https://en.wikipedia.org/wiki/Marden%27s_theorem",
      "text": "[[File:Marden theorem.svg|thumb|A triangle and its Steiner inellipse. The zeroes of {{math|''p''(''z'')}} are the black dots, and the zeroes of {{math|''p'''(''z'')}} are the red dots). The center green dot is the zero of&nbsp;{{math|''p''\"(''z'')}}. Marden's theorem states that the red dots are the foci of the ellipse.]]\nIn [[mathematics]], '''Marden's theorem''', named after Morris Marden but proved much earlier by Jörg Siebeck, gives a geometric relationship between the zeroes of a third-degree [[polynomial]] with [[complex number|complex]] coefficients and the zeroes of its [[derivative]]. See also Geometry of roots of real polynomials.\n\n==Statement of the theorem==\nA cubic polynomial has three zeroes in the complex number plane, which in general form a triangle, and the [[Gauss–Lucas theorem]] states that the roots of its derivative lie within this triangle. Marden's theorem states their location within this triangle more precisely:\n\n:Suppose the zeroes {{math|''z''<sub>1</sub>}}, {{math|''z''<sub>2</sub>}}, and {{math|''z''<sub>3</sub>}} of a third-degree polynomial {{math|''p''(''z'')}} are non-collinear.  There is a unique ellipse inscribed in the [[triangle]] with vertices {{math|''z''<sub>1</sub>}}, {{math|''z''<sub>2</sub>}}, {{math|''z''<sub>3</sub>}} and [[tangent]] to the sides at their [[midpoint]]s: the [[Steiner inellipse]].  The [[focus (geometry)|foci]] of that ellipse are the zeroes of the derivative {{math|''p'''(''z'')}}.\n\n==Additional relations between root locations and the Steiner inellipse==\nBy the [[Gauss–Lucas theorem]], the root of the double derivative {{math|''p''\"(''z'')}} must be the average of the two foci, which is the center point of the ellipse and the [[centroid]] of the triangle.\nIn the special case that the triangle is equilateral (as happens, for instance, for the polynomial {{math|1=''p''(''z'') = ''z''<sup>3</sup> &minus; 1}}) the inscribed ellipse degenerates to a circle, and the derivative of&nbsp;{{math|''p''}} has a [[double root]] at the center of the circle. Conversely, if the derivative has a double root, then the triangle must be equilateral {{harv|Kalman|2008a}}.\n\n==Generalizations==\nA more general version of the theorem, due to {{harvtxt|Linfield|1920}}, applies to polynomials {{math|1=''p''(''z'') = (''z'' &minus; ''a'')<sup>''i''</sup> (''z'' &minus; ''b'')<sup>''j''</sup> (''z'' &minus; ''c'')<sup>''k''</sup>}} whose degree {{math|''i'' + ''j'' + ''k''}} may be higher than three, but that have only three roots {{math|''a''}}, {{math|''b''}}, and {{math|''c''}}. For such polynomials, the roots of the derivative may be found at the multiple roots of the given polynomial (the roots whose exponent is greater than one) and at the foci of an ellipse whose points of tangency to the triangle divide its sides in the ratios {{math|''i'' : ''j''}}, {{math|''j'' : ''k''}}, and {{math|''k'' : ''i''}}.\n\nAnother generalization ({{harvtxt|Parish|2006}}) is to ''n''-gons: some ''n''-gons have an interior ellipse that is tangent to each side at the side's midpoint. Marden's theorem still applies: the foci of this midpoint-tangent inellipse are zeroes of the derivative of the polynomial whose zeroes are the vertices of the ''n''-gon.\n\n==History==\nJörg Siebeck discovered this theorem 81 years before Marden wrote about it.  However, Dan Kalman titled his American Mathematical Monthly paper \"Marden's theorem\" because, as he writes, \"I call this Marden’s Theorem because I first read it in M. Marden’s wonderful book\".\n\n{{harvs|last=Marden|year=1945|year2=1966|txt}} attributes what is now known as Marden's theorem to  {{harvtxt|Siebeck|1864}} and cites nine papers that included a version of the theorem. Dan Kalman won the 2009 [[Lester R. Ford]] Award of the [[Mathematical Association of America]] for his 2008 paper in the [[American Mathematical Monthly]] describing the theorem.\n\nA short and elementary proof of Marden’s theorem is explained in the solution of an exercise in Fritz Carlson’s book “Geometri” (in Swedish, 1943).<ref>{{Cite web|url=http://www.su.se/polopoly_fs/1.229312.1426783194!/menu/standard/file/marden.pdf|title=Carlson’s proof of Marden’s theorem.|last=|first=|date=|website=|publisher=|access-date=}}</ref>\n\n== See also ==\n*[[Bôcher's theorem]] for rational functions\n\n== References ==\n{{Reflist}}\n* {{Citation | last1=Kalman | first1=Dan | title=An Elementary Proof of Marden's Theorem | year=2008a | journal=[[American Mathematical Monthly|The American Mathematical Monthly]] | issn=0002-9890 | volume=115 | pages=330–338|url=http://mathdl.maa.org/mathDL/22/?pa=content&sa=viewDocument&nodeId=3338&pf=1}}\n* {{Citation | last1=Kalman | first1=Dan | title=The Most Marvelous Theorem in Mathematics  | url=http://mathdl.maa.org/mathDL/4/?pa=content&sa=viewDocument&nodeId=1663 | year=2008b | journal=[http://mathdl.maa.org/mathDL/4/ Journal of Online Mathematics and its Applications]}}\n* {{Citation | last=Linfield | first=B. Z. | title=On the relation of the roots and poles of a rational function to the roots of its derivative | journal=Bulletin of the American Mathematical Society | volume=27 | year=1920 | pages=17–21 | doi=10.1090/S0002-9904-1920-03350-1 }}.\n* {{Citation | last1=Marden | first1=Morris | title=A note on the zeroes of the sections of a partial fraction | url=http://www.ams.org/bull/1945-51-12/S0002-9904-1945-08470-5/home.html | year=1945 | journal=[[Bulletin of the American Mathematical Society]] | volume=51 | issue=12 | pages=935–940 | doi=10.1090/S0002-9904-1945-08470-5}}\n* {{Citation | last1=Marden | first1=Morris | title=Geometry of Polynomials | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=Mathematical Surveys | volume=3 | year=1966}}\n* {{Citation | last1=Parish | first1=James L. | title=On the derivative of a vertex polynomial | url=http://forumgeom.fau.edu/FG2006volume6/FG200633.pdf | year=2006 | journal=Forum Geometricorum | volume=6 | pages=285–288: Proposition 5}}\n* {{Citation | last1=Siebeck | first1=Jörg | title=Über eine neue analytische Behandlungweise der Brennpunkte | year=1864 | journal=[[Journal für die reine und angewandte Mathematik]] | issn=0075-4102 | volume=64 | pages=175–182|url=http://gdz.sub.uni-goettingen.de/dms/load/img/?PPN=GDZPPN002152495}}\n\n[[Category:Triangle geometry]]\n[[Category:Polynomials]]\n[[Category:Conic sections]]\n[[Category:Theorems in complex geometry]]"
    },
    {
      "title": "Matching polynomial",
      "url": "https://en.wikipedia.org/wiki/Matching_polynomial",
      "text": "In the [[mathematics|mathematical]] fields of [[graph theory]] and [[combinatorics]], a '''matching polynomial''' (sometimes called an '''acyclic polynomial''') is a [[generating function]] of the numbers of [[Matching (graph theory)|matching]]s of various sizes in a graph. It is one of several [[graph polynomial]]s studied in [[algebraic graph theory]].\n\n==Definition==\nSeveral different types of matching polynomials have been defined. Let ''G'' be a graph with ''n'' vertices and let ''m<sub>k</sub>'' be the number of ''k''-edge matchings.\n\nOne matching polynomial of ''G'' is\n:<math>m_G(x) := \\sum_{k\\geq0} m_k x^k.</math>\n\nAnother definition gives the matching polynomial as\n:<math>M_G(x) := \\sum_{k\\geq0} (-1)^k m_k x^{n-2k}.</math>\n\nA third definition is the polynomial\n:<math>\\mu_G(x,y) := \\sum_{k\\geq0} m_k x^k y^{n-2k}.</math>\n\nEach type has its uses, and all are equivalent by simple transformations.  For instance,\n:<math>M_G(x) = x^n m_G(-x^{-2})</math>\nand\n:<math>\\mu_G(x,y) = y^n m_G(x/y^2).</math>\n\n==Connections to other polynomials==\nThe first type of matching polynomial is a direct generalization of the [[rook polynomial]].\n\nThe second type of matching polynomial has remarkable connections with [[orthogonal polynomials]].  For instance, if ''G''&nbsp;=&nbsp;''K''<sub>''m'',''n''</sub>, the [[complete bipartite graph]], then the second type of matching polynomial is related to the generalized [[Laguerre polynomial]] ''L''<sub>''n''</sub><sup>''&alpha;''</sup>(''x'') by the identity:\n\n: <math>M_{K_{m,n}}(x) = n! L_n^{(m-n)}(x^2). \\, </math>\n\nIf ''G'' is the [[complete graph]] ''K''<sub>''n''</sub>, then ''M''<sub>''G''</sub>(''x'') is an Hermite polynomial:\n: <math>M_{K_n}(x) = H_n(x), \\, </math>\nwhere ''H''<sub>''n''</sub>(''x'') is the \"probabilist's Hermite polynomial\" (1) in the definition of [[Hermite polynomial]]s.  These facts were observed by {{harvtxt|Godsil|1981}}.\n\nIf ''G'' is a [[forest (graph theory)|forest]], then its matching polynomial is equal to the [[characteristic polynomial]] of its [[adjacency matrix]].\n\nIf ''G'' is a [[Path (graph theory)|path]] or a [[Cycle (graph theory)|cycle]], then ''M''<sub>''G''</sub>(''x'') is a [[Chebyshev polynomial]]. In this case\nμ<sub>''G''</sub>(1,''x'') is a [[Fibonacci polynomial]] or [[Lucas polynomial]] respectively.\n\n==Complementation==\nThe matching polynomial of a graph ''G'' with ''n'' vertices is related to that of its complement by a pair of (equivalent) formulas.  One of them is a simple combinatorial identity due to {{harvtxt|Zaslavsky|1981}}.  The other is an integral identity due to {{harvtxt|Godsil|1981}}.\n\nThere is a similar relation for a subgraph ''G'' of ''K''<sub>''m'',''n''</sub> and its complement in ''K''<sub>''m'',''n''</sub>.  This relation, due to Riordan (1958), was known in the context of non-attacking rook placements and rook polynomials.\n\n==Applications in chemical informatics==\nThe [[Hosoya index]] of a graph ''G'', its number of matchings, is used in [[chemoinformatics]] as a structural descriptor of a molecular graph. It may be evaluated as ''m''<sub>''G''</sub>(1) {{harv|Gutman|1991}}.\n\nThe third type of matching polynomial was introduced by {{harvtxt|Farrell|1980}} as a version of the \"acyclic polynomial\" used in [[chemistry]].\n\n==Computational complexity==\nOn arbitrary graphs, or even [[planar graph]]s, computing the matching polynomial is [[Sharp-P-complete|#P-complete]] {{harv|Jerrum|1987}}. However, it can be computed more efficiently when additional structure about the graph is known. In particular,\ncomputing the matching polynomial on ''n''-vertex graphs of [[treewidth]] ''k'' is [[fixed-parameter tractability|fixed-parameter tractable]]: there exists an algorithm whose running time, for any fixed constant ''k'', is a [[polynomial time|polynomial]] in ''n'' with an exponent that does not depend on ''k'' {{harv|Courcelle|Makowsky|Rotics|2001}}.\nThe matching polynomial of a graph with ''n'' vertices and [[clique-width]] ''k'' may be computed in time ''n''<sup>O(''k'')</sup> {{harv|Makowsky|Rotics|Averbouch|Godlin|2006}}\n\n== References ==\n*{{citation\n | last1 = Courcelle | first1 = B. | author1-link = Bruno Courcelle\n | last2 = Makowsky | first2 = J. A.\n | last3 = Rotics | first3 = U.\n | doi = 10.1016/S0166-218X(00)00221-3\n | issue = 1-2\n | journal = Discrete Applied Mathematics\n | pages = 23–52\n | title = On the fixed parameter complexity of graph enumeration problems definable in monadic second-order logic\n | url = http://www.labri.fr/perso/courcell/CoursMaster/CMR-Dam.pdf\n | volume = 108\n | year = 2001}}.\n*{{citation\n | last = Farrell | first = E. J.\n | journal = Ars Combinatoria\n | pages = 221–228\n | title = The matching polynomial and its relation to the acyclic polynomial of a graph\n | volume = 9\n | year = 1980}}.\n*{{citation\n | last = Godsil | first = C.D. | author-link = Chris Godsil\n | doi = 10.1007/BF02579331\n | issue = 3\n | journal = Combinatorica\n | pages = 257–262\n | title = Hermite polynomials and a duality relation for matchings polynomials\n | volume = 1\n | year = 1981}}.\n*{{citation\n | last = Gutman | first = Ivan\n | editor1-last = Bonchev | editor1-first = D.\n | editor2-last = Rouvray | editor2-first = D. H.\n | contribution = Polynomials in graph theory\n | isbn = 978-0-85626-454-2\n | pages = 133–176\n | publisher = Taylor & Francis\n | series = Mathematical Chemistry\n | title = Chemical Graph Theory: Introduction and Fundamentals\n | volume = 1\n | year = 1991}}.\n*{{citation\n | last = Jerrum | first = Mark\n | doi = 10.1007/BF01010403\n | issue = 1\n | journal = Journal of Statistical Physics\n | pages = 121–134\n | title = Two-dimensional monomer-dimer systems are computationally intractable\n | volume = 48\n | year = 1987}}.\n*{{citation\n | last1 = Makowsky | first1 = J. A.\n | last2 = Rotics | first2 = Udi\n | last3 = Averbouch | first3 = Ilya\n | last4 = Godlin | first4 = Benny\n | contribution = Computing graph polynomials on graphs of bounded clique-width\n | doi = 10.1007/11917496_18\n | pages = 191–204\n | publisher = Springer-Verlag\n | series = Lecture Notes in Computer Science\n | title = Proc. 32nd International Workshop on Graph-Theoretic Concepts in Computer Science (WG '06)\n | url = http://www.cs.technion.ac.il/~admlogic/TR/2006/WG06_makowsky.pdf\n | volume = 4271\n | year = 2006}}.\n*{{citation\n | last = Riordan | first = John | author-link = John Riordan (mathematician)\n | location = New York\n | publisher = Wiley\n | title = An Introduction to Combinatorial Analysis\n | year = 1958}}.\n*{{citation\n | last = Zaslavsky | first = Thomas\n | journal = European Journal of Combinatorics\n | pages = 91–103\n | title = Complementary matching vectors and the uniform matching extension property\n | volume = 2\n | year = 1981\n | doi=10.1016/s0195-6698(81)80025-x}}.\n\n[[Category:Algebraic graph theory]]\n[[Category:Matching]]\n[[Category:Polynomials]]\n[[Category:Graph invariants]]"
    },
    {
      "title": "Matrix polynomial",
      "url": "https://en.wikipedia.org/wiki/Matrix_polynomial",
      "text": "{{distinguish|Polynomial matrix}}\n\nIn mathematics, a '''matrix polynomial''' is a polynomial with [[square matrix|square matrices]] as variables. Given an ordinary, scalar-valued polynomial\n: <math>P(x) = \\sum_{i=0}^n{ a_i x^i} =a_0  + a_1 x+ a_2 x^2 + \\cdots + a_n x^n, </math>\nthis polynomial evaluated at a matrix ''A'' is\n:<math>P(A) = \\sum_{i=0}^n{ a_i A^i} =a_0 I + a_1 A + a_2 A^2 + \\cdots + a_n A^n,</math>\nwhere ''I'' is the [[identity matrix]].{{sfn|Horn|Johnson|1990|p=36}}\n\nA '''matrix polynomial equation''' is an equality between two matrix polynomials, which holds for the specific matrices in question.  A '''matrix polynomial identity''' is a matrix polynomial equation which holds for all matrices ''A'' in a specified [[matrix ring]] ''M<sub>n</sub>''(''R'').\n\n== Characteristic and minimal polynomial ==\n\nThe [[characteristic polynomial]] of a matrix ''A'' is a scalar-valued polynomial, defined by <math>p_A(t) = \\det \\left(tI - A\\right)</math>. The [[Cayley–Hamilton theorem]] states that if this polynomial is viewed as a matrix polynomial and evaluated at the matrix ''A'' itself, the result is the zero matrix: <math>p_A(A) = 0</math>. The characteristic polynomial is thus a polynomial which annihilates ''A''. \n\nThere is a unique [[monic polynomial]] of minimal degree which annihilates ''A''; this polynomial is the [[minimal polynomial (linear algebra)|minimal polynomial]]. Any polynomial which annihilates ''A'' (such as the characteristic polynomial) is a multiple of the minimal polynomial.{{sfn|Horn|Johnson|1990|loc=Thm 3.3.1}}\n\nIt follows that given two polynomials ''P'' and ''Q'', we have <math> P(A) = Q(A) </math> if and only if \n:<math> P^{(j)}(\\lambda_i) = Q^{(j)}(\\lambda_i) \\qquad \\text{for } j = 0,\\ldots,n_i-1 \\text{ and } i = 1,\\ldots,s, </math>\nwhere <math> P^{(j)} </math> denotes the ''j''th derivative of ''P'' and <math> \\lambda_1, \\dots, \\lambda_s </math> are the [[eigenvalue]]s of ''A'' with corresponding indices <math> n_1, \\dots, n_s </math> (the index of an eigenvalue is the size of its largest [[Jordan normal form|Jordan block]]).{{sfn|Higham|2000|loc=Thm 1.3}}\n\n==Matrix geometrical series==\n\nMatrix polynomials can be used to sum a matrix geometrical series as one would an ordinary [[geometric series]],\n\n:<math>S=I+A+A^2+\\cdots +A^n</math>\n:<math>AS=A+A^2+A^3+\\cdots +A^{n+1}</math>\n:<math>(I-A)S=S-AS=I-A^{n+1}</math>\n:<math>S=(I-A)^{-1}(I-A^{n+1})</math>\n\nIf ''I''&nbsp;&minus;&nbsp;''A'' is nonsingular one can evaluate the expression for the sum&nbsp;''S''.\n\n==See also==\n*[[Latimer–MacDuffee theorem]]\n*[[Matrix exponential]]\n*[[Matrix function]]\n\n==Notes==\n{{reflist}}\n\n==References==\n* {{cite book | title=Matrix Polynomials | volume=58 | series=Classics in Applied Mathematics | first1=Israel | last1=Gohberg | first2=Peter | last2=Lancaster |author2-link=Peter Lancaster| first3=Leiba | last3=Rodman | publisher=[[Society for Industrial and Applied Mathematics]] | location=Lancaster, PA | year=2009 | origyear=1982 | isbn=0-898716-81-0 | zbl=1170.15300 }}\n* {{cite book | title=Functions of Matrices: Theory and Computation | last=Higham | first=Nicholas J. | year=2000 | publisher=SIAM | isbn=089-871-777-9 | ref=harv}}.\n* {{cite book | last1=Horn | first1=Roger A. | last2=Johnson | first2=Charles R. | title=Matrix Analysis | publisher=[[Cambridge University Press]] | isbn=978-0-521-38632-6 | year=1990 | ref=harv}}.\n\n\n{{DEFAULTSORT:Matrix Polynomial}}\n[[Category:Matrix theory]]\n[[Category:Polynomials]]"
    },
    {
      "title": "Minimal polynomial (field theory)",
      "url": "https://en.wikipedia.org/wiki/Minimal_polynomial_%28field_theory%29",
      "text": "{{Use American English|date = March 2019}}\n{{Short description|Concept in abstract algebra}}\n{{for|the minimal polynomial of a matrix|Minimal polynomial (linear algebra)}}\n\nIn [[field theory (mathematics)|field theory]], a branch of mathematics, the '''minimal polynomial''' of a value ''&alpha;'' is, roughly speaking, the polynomial of lowest [[degree of a polynomial|degree]] having coefficients of a specified type, such that ''&alpha;'' is a root of the polynomial. If the minimal polynomial of ''&alpha;'' exists, it is unique. The coefficient of the highest-degree term in the polynomial is required to be 1, and the specified type for the remaining coefficients could be [[integer]]s, [[rational number]]s, [[real number]]s, or others.\n\nMore formally, a minimal polynomial is defined relative to a [[field extension]] ''E''/''F'' and an element of the extension field ''E''. The minimal polynomial of an element, if it exists, is a member of ''F''[''x''], the  [[Polynomial ring|ring of polynomials]] in the variable ''x'' with coefficients in ''F''. Given an element ''&alpha;'' of ''E'', let ''J''<sub>''&alpha;''</sub> be the set of all polynomials ''f''(''x'') in ''F''[''x''] such that ''f''(''&alpha;'') = 0. The element ''&alpha;'' is called a [[Zero of a function|root or zero]] of each polynomial in ''J''<sub>''&alpha;''</sub>. The set ''J''<sub>''&alpha;''</sub> is so named because it is an [[Ideal (ring theory)|ideal]] of ''F''[''x'']. The zero polynomial, all of whose coefficients are 0, is in every ''J''<sub>''&alpha;''</sub> since 0''&alpha;''<sup>''i''</sup> = 0 for all ''&alpha;'' and ''i''. This makes the zero polynomial useless for classifying different values of ''&alpha;'' into types, so it is excepted. If there are any non-zero polynomials in ''J''<sub>''&alpha;''</sub>, then ''&alpha;'' is called an [[algebraic element]] over ''F'', and there exists a [[monic polynomial]] of least degree in ''J''<sub>''&alpha;''</sub>. This is the minimal polynomial of ''&alpha;'' with respect to ''E''/''F''. It is unique and [[irreducible polynomial|irreducible]] over ''F''. If the zero polynomial is the only member of ''J''<sub>''&alpha;''</sub>, then ''&alpha;'' is called a [[transcendental element]] over ''F'' and has no minimal polynomial with respect to ''E''/''F''.\n\nMinimal polynomials are useful for constructing and analyzing field extensions. When ''&alpha;'' is algebraic with minimal polynomial ''a''(''x''), the smallest field that contains both ''F'' and ''&alpha;'' is [[Ring isomorphism|isomorphic]] to the [[quotient ring]] ''F''[''x'']/⟨''a''(''x'')⟩, where ⟨''a''(''x'')⟩ is the ideal of ''F''[''x''] generated by ''a''(''x''). Minimal polynomials are also used to define [[conjugate elements]].\n\n== Definition ==\n\nLet ''E''/''F'' be a field extension, ''α'' an element of ''E'', and ''F''[''x''] the ring of polynomials in ''x'' over ''F''. The minimal polynomial of ''&alpha;'' is the monic polynomial of least degree among all polynomials in ''F''[''x''] having ''&alpha;'' as a root; it exists when ''α'' is algebraic over ''F'', that is, when ''f''(''α'') = 0 for some non-zero polynomial ''f''(''x'') in ''F''[''x''].\n\n=== Uniqueness ===\n\nLet ''a''(''x'') be the minimal polynomial of ''&alpha;'' with respect to ''E''/''F''. The uniqueness of ''a''(''x'') is established by considering the [[ring homomorphism]] sub<sub>''&alpha;''</sub> from ''F''[''x''] to ''E'' that substitutes ''α'' for ''x'', that is, sub<sub>''&alpha;''</sub>(''f''(''x'')) = ''f''(''α''). The kernel of sub<sub>''&alpha;''</sub>, ker(sub<sub>''&alpha;''</sub>), is the set of all polynomials in ''F''[''x''] that have ''&alpha;'' as a root. That is, ker(sub<sub>''&alpha;''</sub>) = ''J''<sub>''&alpha;''</sub> from above. Since sub<sub>''&alpha;''</sub> is a ring homomorphism, ker(sub<sub>''&alpha;''</sub>) is an ideal of ''F''[''x'']. Since ''F''[''x''] is a [[principal ring]] whenever ''F'' is a field, there is at least one polynomial in ker(sub<sub>''&alpha;''</sub>) that generates ker(sub<sub>''&alpha;''</sub>). Such a polynomial will have least degree among all non-zero polynomials in ker(sub<sub>''&alpha;''</sub>), and ''a''(''x'') is taken to be the unique monic polynomial among these.\n\n====Alternative proof of uniqueness====\n\nSuppose ''p'' and ''q'' are monic polynomials in ''J''<sub>''α''</sub> of minimal degree ''n'' > 0. Since ''p'' − ''q'' ∈ ''J''<sub>''α''</sub> and deg(''p'' − ''q'') < ''n'' it follows that ''p'' − ''q'' = 0, i.e. ''p'' = ''q''.\n\n== Properties ==\n\nA minimal polynomial is irreducible. Let ''E''/''F'' be a field extension over ''F'' as above, ''α'' ∈ ''E'', and ''f'' ∈ ''F''[''x''] a minimal polynomial for ''α''. Suppose ''f'' = ''gh'', where ''g'', ''h'' ∈ ''F''[''x''] are of lower degree than ''f''. Now ''f''(''α'') = 0. Since fields are also [[integral domain]]s, we have ''g''(''α'') = 0 or ''h''(''α'') = 0. This contradicts the minimality of the degree of ''f''. Thus minimal polynomials are irreducible.\n\n== Examples ==\n\nIf ''F'' = '''Q''', ''E'' = '''R''', ''α'' = {{radic|2}}, then the minimal polynomial for ''α'' is ''a''(''x'') = ''x''<sup>2</sup> &minus; 2. The base field ''F'' is important as it determines the possibilities for the coefficients of ''a''(''x'').  For instance, if we take ''F'' = '''R''', then the minimal polynomial for ''α'' = {{radic|2}} is ''a''(''x'') = ''x'' &minus; {{radic|2}}.\n\nIf  ''α'' = {{radic|2}} + {{radic|3}}, then the minimal polynomial in '''Q'''[''x''] is ''a''(''x'') = ''x''<sup>4</sup> &minus; 10''x''<sup>2</sup> + 1 = (''x'' &minus; {{radic|2}} &minus;  {{radic|3}})(''x'' + {{radic|2}} &minus; {{radic|3}})(''x'' &minus; {{radic|2}} + {{radic|3}})(''x'' + {{radic|2}} + {{radic|3}}).\n\nThe minimal polynomial in '''Q'''[''x''] of the sum of the square roots of the first ''n'' prime numbers is constructed analogously, and is called a [[Swinnerton-Dyer polynomial]].\n\nThe minimal polynomials in '''Q'''[''x''] of [[root of unity|roots of unity]] are the [[cyclotomic polynomial]]s.\n\n== References ==\n\n{{Reflist}}\n\n* {{MathWorld|urlname=AlgebraicNumberMinimalPolynomial|title=Algebraic Number Minimal Polynomial}}\n* {{PlanetMath|urlname=MinimalPolynomial|title=Minimal polynomial}}\n* Pinter, Charles C. ''A Book of Abstract Algebra''. Dover Books on Mathematics Series. Dover Publications, 2010, p.&nbsp;270–273. {{isbn|978-0-486-47417-5}}\n\n[[Category:Polynomials]]\n[[Category:Field theory]]"
    },
    {
      "title": "Minimal polynomial (linear algebra)",
      "url": "https://en.wikipedia.org/wiki/Minimal_polynomial_%28linear_algebra%29",
      "text": "{{for|the minimal polynomial of an algebraic element of a field|Minimal polynomial (field theory)}}\n\nIn [[linear algebra]], the '''minimal polynomial''' {{math|''μ<sub>A</sub>''}} of an {{math|''n'' × ''n''}} [[matrix (mathematics)|matrix]] {{mvar|A}} over a [[field (mathematics)|field]] {{math|'''F'''}} is the [[monic polynomial]] {{mvar|P}} over {{math|'''F'''}} of least degree such that {{math|''P''(''A'') {{=}} 0}}.  Any other polynomial {{mvar|Q}} with {{math|''Q''(''A'') {{=}} 0}} is a (polynomial) multiple of {{math|''μ<sub>A</sub>''}}.\n\nThe following three statements are equivalent:\n# {{mvar|λ}} is a root of {{math|''μ<sub>A</sub>''}},\n# {{mvar|λ}} is a root of the [[characteristic polynomial]] {{math|''χ<sub>A</sub>''}} of {{mvar|A}},\n# {{mvar|λ}} is an [[eigenvalue]] of matrix {{mvar|A}}.\n\nThe multiplicity of a root {{mvar|λ}} of {{math|''μ<sub>A</sub>''}} is the largest power {{mvar|m}} such that {{math|ker((''A'' − ''λI<sub>n</sub>'')<sup>''m''</sup>)}} ''strictly'' contains {{math|ker((''A'' − ''λI<sub>n</sub>'')<sup>''m''−1</sup>)}}. In other words, increasing the exponent up to {{mvar|m}} will give ever larger kernels, but further increasing the exponent beyond {{mvar|m}} will just give the same kernel.\n\nIf the field {{math|'''F'''}} is not algebraically closed, then the minimal and characteristic polynomials need not factor according to their roots (in {{math|'''F'''}}) alone, in other words they may have [[irreducible polynomial]] factors of degree greater than {{math|1}}. For irreducible polynomials {{mvar|P}} one has similar equivalences:\n# {{mvar|P}} divides {{math|''μ<sub>A</sub>''}},\n# {{mvar|P}} divides {{math|''χ<sub>A</sub>''}},\n# the kernel of {{math|''P''(''A'')}} has dimension at least {{math|1}}.\n# the kernel of {{math|''P''(''A'')}} has dimension at least {{math|deg(''P'')}}.\n\nLike the characteristic polynomial, the minimal polynomial does not depend on the base field, in other words considering the matrix as one with coefficients in a larger field does not change the minimal polynomial. The reason is somewhat different from for the characteristic polynomial (where it is immediate from the definition of determinants), namely the fact that the minimal polynomial is determined by the relations of [[linear dependence]] between the powers of {{mvar|A}}: extending the base field will not introduce any new such relations (nor of course will it remove existing ones).\n\nThe minimal polynomial is often the same as the characteristic polynomial, but not always. For example, if {{mvar|A}} is a multiple {{math|''aI<sub>n</sub>''}} of the identity matrix, then its minimal polynomial is {{math|''X'' − ''a''}} since the kernel of {{math|''aI<sub>n</sub>'' − ''A'' {{=}} 0}} is already the entire space; on the other hand its characteristic polynomial is {{math|(''X'' − ''a'')<sup>''n''</sup>}} (the only eigenvalue is {{mvar|a}}, and the degree of the characteristic polynomial is always equal to the dimension of the space). The minimal polynomial always divides the characteristic polynomial, which is one way of formulating the [[Cayley–Hamilton theorem]] (for the case of matrices over a field).\n\n== Formal definition ==\nGiven an [[endomorphism]] {{mvar|T}} on a finite-dimensional [[vector space]] {{mvar|V}} over a [[Field (mathematics)|field]] {{math|'''F'''}}, let {{math|''I<sub>T</sub>''}} be the set defined as \n\n:<math> \\mathit{I}_T = \\{ p \\in \\mathbf{F}[t] \\mid p(T) = 0 \\} </math>\n\nwhere {{math|'''F'''[''t'']}} is the space of all polynomials over the field {{math|'''F'''}}. {{math|''I<sub>T</sub>''}} is a [[Ideal (ring theory)|proper ideal]] of {{math|'''F'''[''t'']}}. Since {{math|'''F'''}} is a field, {{math|'''F'''[''t'']}} is a [[principal ideal domain]], thus any ideal is generated by a single polynomial, which is unique up to units in {{math|'''F'''}}. A particular choice among the generators can be made, since precisely one of the generators is [[monic polynomial|monic]]. The '''minimal polynomial''' is thus defined to be the monic polynomial which generates {{math|''I<sub>T</sub>''}}. It is the monic polynomial of least degree in {{math|''I<sub>T</sub>''}}.\n\n== Applications ==\nAn [[endomorphism]] {{mvar|φ}} of a finite dimensional vector space over a field {{math|'''F'''}} is [[diagonalizable]] if and only if its minimal polynomial factors completely over {{math|'''F'''}} into ''distinct'' linear factors. The fact that there is only one factor {{math|''X'' − ''λ''}} for every eigenvalue {{mvar|λ}} means that the [[generalized eigenspace]] for {{mvar|λ}} is the same as the [[eigenspace]] for {{mvar|λ}}: every Jordan block has size {{math|1}}. More generally, if {{mvar|φ}} satisfies a polynomial equation {{math|''P''(''φ'') {{=}} 0}} where {{mvar|P}} factors into distinct linear factors over {{math|'''F'''}}, then it will be diagonalizable: its minimal polynomial is a divisor of {{mvar|P}} and therefore also factors into distinct linear factors. In particular one has:\n\n* {{math|''P'' {{=}} ''X<sup>&thinsp;k</sup>'' − 1}}: finite order endomorphisms of complex vector spaces are diagonalizable. For the special case {{math|''k'' {{=}} 2}} of [[involution (mathematics)|involutions]], this is even true for endomorphisms of vector spaces over any field of [[characteristic (algebra)|characteristic]] other than {{math|2}}, since {{math|''X''<sup>&thinsp;2</sup> − 1 {{=}} (''X'' − 1)(''X'' + 1)}} is a factorization into distinct factors over such a field. This is a part of [[representation theory]] of cyclic groups.\n* {{math|''P'' {{=}} ''X''<sup>&thinsp;2</sup> − ''X'' {{=}} ''X''(''X'' − 1)}}: endomorphisms satisfying {{math|''φ''<sup>2</sup> {{=}} ''φ''}} are called [[Projection (linear algebra)|projections]], and are always diagonalizable (moreover their only eigenvalues are {{math|0}} and {{math|1}}).\n* By contrast if {{math|''μ<sub>φ</sub>'' {{=}} ''X<sup>&thinsp;k</sup>''}}  with {{math|''k'' ≥ 2}} then {{mvar|φ}} (a nilpotent endomorphism) is not necessarily diagonalizable, since {{math|''X<sup>&thinsp;k</sup>''}} has a repeated root {{math|0}}.\n\nThese cases can also be proved directly, but the minimal polynomial gives a unified perspective and proof.\n\n== Computation ==\nFor a vector {{mvar|v}} in {{mvar|V}} define:\n\n:<math> \\mathit{I}_{T, v} = \\{ p \\in \\mathbf{F}[t] \\; | \\; p(T)(v) = 0 \\}.</math>\n\nThis definition satisfies the properties of a proper ideal. Let {{math|''μ''<sub>''T'',''v''</sub>}} be the monic polynomial which generates it.\n\n===Properties===\n{{unordered list\n|1= Since {{math|''I''<sub>''T'',''v''</sub>}} contains the minimal polynomial {{math|''μ<sub>T</sub>''}}, the latter is divisible by {{math|''μ''<sub>''T'',''v''</sub>}}.\n|2= If {{mvar|d}} is the least natural number such that {{math|''v'', ''T''(''v''), ..., ''T<sup>d</sup>''(''v'')}} are [[linearly dependent]], then there exist unique {{math|''a''<sub>0</sub>, ''a''<sub>1</sub>, ..., ''a''<sub>''d''−1</sub>}} in {{math|'''F'''}}, not all zero, such that\n:<math> a_0 v + a_1 T(v) + \\cdots + a_{d-1} T^{d-1} (v) + T^d (v) = 0</math>\nand for these coefficients one has\n:<math> \\mu_{T,v} (t) = a_0  + a_1 t + \\ldots + a_{d-1} t^{d-1} + t^d. </math>\n|3= Let the subspace ''W'' be the image of {{math|''μ''<sub>''T'',''v''</sub>(''T'')}}, which is {{mvar|T}}-stable. Since {{math|''μ''<sub>''T'',''v''</sub>(''T'')}} annihilates at least the vectors {{math|''v'', ''T''(''v''), ..., ''T''<sup>''d''-1</sup>(''v'')}}, the [[codimension]] of {{mvar|W}} is at least {{mvar|d}}.\n|4= The minimal polynomial {{math|''μ<sub>T</sub>''}} is the product of {{math|''μ''<sub>''T'',''v''</sub>}} and the minimal polynomial {{mvar|Q}} of the restriction of {{mvar|T}} to {{mvar|W}}. In the (likely) case that {{mvar|W}} has dimension {{math|0}} one has {{math|''Q'' {{=}} 1}} and therefore {{math|''μ<sub>T</sub>'' {{=}} ''μ''<sub>''T'',''v''</sub>}}; otherwise a recursive computation of {{mvar|Q}} suffices to find {{math|''μ<sub>T</sub>''}}.\n}}\n\n=== Example ===\nDefine {{mvar|T}} to be the endomorphism of {{math|'''R'''<sup>3</sup>}} with matrix, on the canonical basis,\n\n:<math>\\begin{pmatrix} 1 & -1 & -1 \\\\ 1 & -2 & 1 \\\\ 0 & 1 & -3 \\end{pmatrix}.</math>\n\nTaking the first canonical basis vector {{math|''e''<sub>1</sub>}} and its repeated images by {{mvar|T}} one obtains\n\n:<math>  e_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad\n  T\\cdot e_1 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix}. \\quad\nT^2\\cdot e_1 = \\begin{bmatrix} 0 \\\\ -1 \\\\ 1 \\end{bmatrix} \\mbox{ and}\\quad\nT^3\\cdot e_1=\\begin{bmatrix} 0 \\\\ 3 \\\\ -4 \\end{bmatrix}</math>\n\nof which the first three are easily seen to be [[linearly independent]], and therefore span all of {{math|'''R'''<sup>3</sup>}}. The last one then necessarily is a linear combination of the first three, in fact\n\n:{{math|''T''<sup>&thinsp;3</sup>⋅''e''<sub>1</sub> {{=}} −4''T''<sup>&thinsp;2</sup>⋅''e''<sub>1</sub> − ''T''⋅''e''<sub>1</sub> + ''e''<sub>1</sub>}}, \n\nso that: \n\n:{{math|''μ''<sub>''T'',''e''<sub>1</sub></sub> {{=}} ''X''<sup>&thinsp;3</sup> + 4''X''<sup>&thinsp;2</sup> + ''X'' − ''I''}}.\n\nThis is in fact also the minimal polynomial {{math|''μ<sub>T</sub>''}} and the characteristic polynomial {{math|''χ<sub>T</sub>''}}: indeed {{math|''μ''<sub>''T'',''e''<sub>1</sub></sub>}} divides {{math|''μ<sub>T</sub>''}} which divides {{math|''χ<sub>T</sub>''}}, and since the first and last are of degree {{math|3}} and all are monic, they must all be the same. Another reason is that in general if any polynomial in {{mvar|T}} annihilates a vector {{mvar|v}}, then it also annihilates {{math|''T''⋅''v''}} (just apply {{mvar|T}} to the equation that says that it annihilates {{mvar|v}}), and therefore by iteration it annihilates the entire space generated by the iterated images by {{mvar|T}} of {{mvar|v}}; in the current case we have seen that for {{math|''v'' {{=}} ''e''<sub>1</sub>}} that space is all of {{math|'''R'''<sup>3</sup>}}, so {{math|''μ''<sub>''T'',''e''<sub>1</sub></sub>(''T'') {{=}} 0}}. Indeed one verifies for the full matrix that {{math|''T''<sup>&thinsp;3</sup> + 4''T''<sup>&thinsp;2</sup> + ''T'' − ''I''<sub>3</sub>}} is the null matrix:\n\n:<math>\\begin{bmatrix} 0 & 1 & -3 \\\\ 3 & -13 & 23 \\\\ -4 & 19 & -36 \\end{bmatrix}\n +4\\begin{bmatrix} 0 & 0 & 1 \\\\ -1 & 4 & -6 \\\\ 1 & -5 & 10 \\end{bmatrix}\n +\\begin{bmatrix} 1 & -1 & -1 \\\\ 1 & -2 & 1 \\\\ 0 & 1 & -3 \\end{bmatrix}\n +\\begin{bmatrix} -1 & 0 & 0 \\\\ 0 & -1 & 0 \\\\ 0 & 0 & -1 \\end{bmatrix}\n =\\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}</math>\n\n==References==\n* {{Lang Algebra}}\n\n[[Category:Matrix theory]]\n[[Category:Polynomials]]"
    },
    {
      "title": "Mittag-Leffler polynomials",
      "url": "https://en.wikipedia.org/wiki/Mittag-Leffler_polynomials",
      "text": "In mathematics, the '''Mittag-Leffler polynomials''' are the [[polynomial]]s ''g''<sub>''n''</sub>(''z'') studied by {{harvs|txt|last=Mittag-Leffler|year=1891|authorlink=Gösta Mittag-Leffler}}.\n\nThey are given by \n:<math> \\displaystyle (1+t)^z(1-t)^{-z}=\\sum_n g_n(z)t^n. </math>\n\n== See also==\n* [[Bernoulli polynomials of the second kind]]\n* [[Stirling polynomials]]\n* [[Poly-Bernoulli number]]\n\n==References==\n\n*{{Citation | last1=Bateman | first1=H. | title=The polynomial of Mittag-Leffler | jstor=86958 | mr=0002381 | year=1940 | journal=[[Proceedings of the National Academy of Sciences|Proceedings of the National Academy of Sciences of the United States of America]] | issn=0027-8424 | volume=26 | pages=491–496 | doi=10.1073/pnas.26.8.491| url=http://authors.library.caltech.edu/8694/1/BATpnas40.pdf }}\n*{{Citation | last1=Mittag-Leffler | first1=G. | title=Sur la représentasion analytique des intégrales et des invariants d'une équation différentielle linéaire et homogène | language=French | doi=10.1007/BF02392600 | jfm=23.0327.01 | year=1891 | journal=[[Acta Mathematica]] | issn=0001-5962 | volume=XV | pages=1–32}}\n\n[[Category:Polynomials]]"
    },
    {
      "title": "Monic polynomial",
      "url": "https://en.wikipedia.org/wiki/Monic_polynomial",
      "text": "{{refimprove|date=January 2013}}\nIn [[algebra]], a '''monic polynomial''' is a single-variable polynomial (that is, a [[univariate polynomial]]) in which the [[leading coefficient]] (the nonzero coefficient of highest degree) is equal to&nbsp;1. Therefore, a monic polynomial has the form\n\n:<math>x^n+c_{n-1}x^{n-1}+\\cdots+c_2x^2+c_1x+c_0</math>\n\n== Univariate polynomials ==\nIf a [[polynomial]] has only one [[indeterminate (variable)|indeterminate]] ([[univariate polynomial]]), then the terms are usually written either from highest degree to lowest degree (\"descending powers\") or from lowest degree to highest degree (\"ascending powers\").  A univariate polynomial in ''x'' of degree ''n'' then takes the general form displayed above, where\n\n: ''c''<sub>''n''</sub> ≠ 0, ''c''<sub>''n''&minus;1</sub>, ..., ''c''<sub>2</sub>, ''c''<sub>1</sub> and ''c''<sub>0</sub>\n\nare constants, the coefficients of the polynomial.\n\nHere the term ''c''<sub>''n''</sub>''x''<sup>''n''</sup> is called the ''leading term'', and its coefficient ''c''<sub>''n''</sub> the ''leading coefficient''; if the leading coefficient {{nowrap|is 1}}, the univariate polynomial is called '''monic'''.\n\n===Examples===\n* [[Complex quadratic polynomial]]s\n\n=== Properties ===\n\n====Multiplicatively closed====\nThe set of all monic polynomials (over a given (unitary) [[ring (mathematics)|ring]] ''A'' and for a given variable ''x'') is closed under multiplication, since the product of the leading terms of two monic polynomials is the leading term of their product. Thus, the monic polynomials form a multiplicative [[semigroup]] of the [[polynomial ring]] ''A''[''x''].  Actually, since the [[constant polynomial]] 1 is monic, this semigroup is even a [[monoid]].\n\n====Partially ordered====\nThe restriction of the [[divisibility (ring theory)|divisibility]] relation to the set of all monic polynomials (over the given ring) is a [[partial order]], and thus makes this set to a [[poset]].  The reason is that if ''p''(''x'') divides ''q''(''x'') and ''q''(''x'') divides ''p''(''x'') for two monic polynomials ''p'' and ''q'', then ''p'' and ''q'' must be equal.  The corresponding property is not true for polynomials in general, if the ring contains [[invertible element]]s other than 1.\n\n====Polynomial equation solutions====\nIn other respects, the properties of monic polynomials and of their corresponding monic [[polynomial equation]]s depend crucially on the coefficient ring ''A''. If ''A'' is a [[field (algebra)|field]], then every non-zero polynomial ''p'' has exactly one [[associated element|associated]] monic polynomial ''q''; actually, ''q'' is ''p'' divided with its leading coefficient. In this manner, then, any non-trivial polynomial equation ''p''(''x'')&nbsp;=&nbsp;0 may be replaced by an equivalent monic equation ''q''(''x'')&nbsp;=&nbsp;0. E.g., the general real second degree equation\n:<math>\\ ax^2+bx+c = 0</math> (where <math> a \\neq 0</math>)\nmay be replaced by\n:<math>\\ x^2+px+q = 0</math>,\nby putting &nbsp;''p''&nbsp;=&nbsp;''b''/''a''&nbsp; and &nbsp;''q''&nbsp;=&nbsp;''c''/''a''. Thus, the equation\n:<math>2x^2+3x+1 = 0</math>\nis equivalent to the monic equation\n:<math>x^2+\\frac{3}{2}x+\\frac{1}{2}=0.</math>\n\nThe general quadratic solution formula is then the slightly more simplified form of:\n:<math>x = \\frac{1}{2} \\left( -p \\pm \\sqrt{p^2 - 4q} \\right).</math>\n\n=====Integrality=====\nOn the other hand, if the coefficient ring is not a field, there are more essential differences.  E.g., a monic polynomial equation with [[integer]] coefficients cannot have other [[Rational number|rational]] solutions than integer solutions.  Thus, the equation\n:<math>\\ 2x^2+3x+1 = 0</math>\npossibly might have some rational root, which is not an integer, (and incidentally it does have ''inter alia'' the root &minus;1/2); while the equations\n:<math>\\ x^2+5x+6 = 0</math>\nand\n:<math>\\ x^2+7x+8 = 0</math>\nonly may have integer solutions or [[irrational number|irrational]] solutions.\n\nThe roots of monic polynomial with integer coefficients are called [[algebraic integer]]s.\n\nThe solutions to monic polynomial equations over an [[integral domain]] are important in the theory of [[integral extension]]s and [[integrally closed domain]]s, and hence for [[algebraic number theory]].  In general, assume that ''A'' is an integral domain, and also a subring of the integral domain ''B''.  Consider the subset ''C'' of ''B'', consisting of those ''B'' elements, which satisfy monic polynomial equations over ''A'':\n:<math> C := \\{b \\in B : \\exists\\, p(x) \\in A[x]\\,, \\hbox{ which is monic and such that } p(b) = 0\\}\\,.</math>\nThe set ''C'' contains ''A'', since any ''a''&nbsp;∈&nbsp;''A'' satisfies the equation ''x''&nbsp;−&nbsp;''a''&nbsp;=&nbsp;0. Moreover, it is possible to prove that ''C'' is closed under addition and multiplication.  Thus, ''C'' is a subring of ''B''.  The ring ''C'' is called the ''integral closure'' of ''A'' in ''B''; or just the integral closure of ''A'', if ''B'' is the [[fraction field]] of ''A''; and the elements of ''C'' are said to be ''[[integrality|integral]]'' over ''A''.  If here <math>A=\\mathbb{Z}</math> (the ring of [[integer]]s) and <math>B=\\mathbb{C}</math> (the field of [[complex number]]s), then ''C'' is the ring of ''[[algebraic integers]]''.\n\n====Irreduciblity ====\nIf {{mvar|p}} is a [[prime number]], the number of monic [[irreducible polynomial]]s of degree {{mvar|n}} over a [[finite field]] <math>GF(p)</math> with {{mvar|p}} elements is equal to the [[Necklace (combinatorics)|necklace counting function]] {{tmath|N_p(n)}}.{{cn|date=February 2018}} \n\nIf one removes the constraint of being monic, this number becomes {{tmath|(p-1)N_p(n)}}. \n\nThe total number of roots of these monic irreducible polynomials is {{tmath|nN_p(n)}}. This is the number of elements of the field {{tmath|GF(p^n)}} (with {{tmath|p^n}} elements) that do not belong to any smaller field.\n\nFor {{math|1=''p'' = 2}}, such polynomials are commonly used to generate [[pseudorandom binary sequence]]s.{{cn|date=February 2018}}\n\n== Multivariate polynomials ==\nOrdinarily, the term ''monic'' is not employed for polynomials of several variables.  However, a polynomial in several variables may be regarded as a polynomial in only \"the last\" variable, but with coefficients being polynomials in the others.  This may be done in several ways, depending on which one of the variables is chosen as \"the last one\". E.g., the real polynomial\n:<math>\\ p(x,y) = 2xy^2+x^2-y^2+3x+5y-8</math>\nis monic, considered as an element in '''R'''[''y''][''x''], i.e., as a univariate polynomial in the variable ''x'', with coefficients which themselves are univariate polynomials in ''y'':\n:<math>p(x,y) = 1\\cdot x^2 + (2y^2+3) \\cdot x + (-y^2+5y-8)</math>;\nbut ''p''(''x'',''y'') is not monic as an element in  '''R'''[''x''][''y''], since then the highest degree coefficient (i.e., the ''y''<sup>2</sup> coefficient) is &nbsp;2''x''&nbsp;&minus;&nbsp;1.\n\nThere is an alternative convention, which may be useful e.g. in [[Gröbner basis]] contexts: a polynomial is called monic, if its leading coefficient (as a multivariate polynomial) is 1.  In other words, assume that ''p = p''(''x''<sub>1</sub>'',...,x<sub>n</sub>'') is a non-zero polynomial in ''n'' variables, and that there is a given [[monomial order]] on the set of all (\"monic\") monomials in these variables, i.e., a total order of the free commutative [[monoid]] generated by ''x''<sub>1</sub>'',...,x<sub>n</sub>'', with the unit as lowest element, and respecting multiplication.  In that case, this order defines a highest non-vanishing term in ''p'', and ''p'' may be called monic, if that term has coefficient one.\n\n\"Monic multivariate polynomials\" according to either definition share some properties with the \"ordinary\" (univariate) monic polynomials.  Notably, the product of monic polynomials again is monic.\n\n== References ==\n* {{cite book |last=Pinter |first=Charles C. |title=A Book of Abstract Algebra |year=2010 |origyear=Unabridged republication of the 1990 second edition of the work originally published in 1982 by the McGraw–Hill Publishing Company |publisher=Dover |isbn=978-0486474175}}\n\n[[Category:Polynomials]]"
    },
    {
      "title": "Monomial order",
      "url": "https://en.wikipedia.org/wiki/Monomial_order",
      "text": "{{short description|Order for the terms of a polynomial}}\nIn [[mathematics]], a '''monomial order''' (sometimes called a '''term order''' or an '''admissible order''') is a [[total order]] on the set of all ([[Monic polynomial|monic]]) [[monomial]]s in a given [[polynomial ring]], satisfying the property of respecting multiplication, i.e.,\n* If <math>u \\leq v</math> and <math>w</math> is any other monomial, then <math>uw \\leq vw</math>.\n\nMonomial orderings are most commonly used with [[Gröbner basis|Gröbner bases]] and [[multivariate division algorithm|multivariate division]]. In particular, the property of ''being'' a Gröbner basis is always relative to a specific monomial order.\n\n== Definition, details and variations ==\nBesides respecting multiplication, monomial orders are often required to be [[well ordering|well-orders]], since this ensures the multivariate division procedure will terminate. There are however practical applications also for multiplication-respecting order relations on the set of monomials that are not well-orders.\n\nIn the case of finitely many variables, well-ordering of a monomial order is equivalent to the conjunction of the following two conditions:\n# The order is a [[total order]].\n# If ''u'' is any monomial then <math>1 \\leq u</math>.\nSince these conditions may be easier to verify for a monomial order defined through an explicit rule, than to directly prove it is a well-ordering, they are sometimes preferred in definitions of monomial order. <!-- Totality by itself is needed for defining the leading monomial function. -->\n\n<!--\nThese conditions imply that \n* If ''u'' and ''v'' are any monomials then <math>u \\leq uv</math>.\nThey imply also that the ordering is a [[well-order|well ordering]], which means that every strictly decreasing sequence of  monomials is finite, or equivalently that every non-empty set of monomials has a minimal element.\n-->\n=== Leading monomials, terms, and coefficients ===\n\nThe choice of a total order on the monomials allows sorting the terms of a polynomial. The '''leading term''' of a polynomial is thus the term of the largest monomial (for the chosen monomial ordering). \n\nConcretely, let {{math|''R''}} be any ring of polynomials. Then the set {{math|''M''}} of the (monic) monomials in {{math|''R''}} is a [[basis (linear algebra)|basis]] of {{math|''R''}}, considered as a [[vector space]] over the [[field (mathematics)|field]] of the coefficients. Thus, any nonzero polynomial {{math|''p''}} in {{math|''R''}} has a unique expression \n<math> p = \\textstyle\\sum_{u \\in S} c_u u </math> \nas a [[linear combination]] of monomials, where {{math|''S''}} is a finite subset of {{math|''M''}} and the {{math|''c''<sub>''u''</sub>}} are all nonzero. When a monomial order has been chosen, the '''leading monomial''' is the largest {{math|''u''}} in {{math|''S''}}, the '''leading coefficient''' is the corresponding {{math|''c''<sub>''u''</sub>}}, and the '''leading term''' is the corresponding {{math|''c''<sub>''u''</sub>''u''}}. ''Head'' monomial/coefficient/term is sometimes used as a synonym of \"leading\". Some authors use \"monomial\" instead of \"term\" and \"power product\" instead of \"monomial\". In this article, a monomial is assumed to not include a coefficient.\n\nThe defining property of monomial orderings implies that the order of the terms is kept when multiplying a polynomial by a monomial. Also, the leading term of a product of polynomials is the product of the leading terms of the factors.\n\n== Examples ==\nOn the set <math> \\left\\{ x^n \\mid n \\in \\mathbb{N} \\right\\} </math> of powers of any one variable ''x'', the only monomial orders are the natural ordering 1&nbsp;<&nbsp;''x''&nbsp;<&nbsp;x<sup>2</sup>&nbsp;<&nbsp;x<sup>3</sup>&nbsp;<&nbsp;... and its converse, the latter of which is not a well-ordering. Therefore, the notion of monomial order becomes interesting only in the case of multiple variables.\n\nThe monomial order implies an order on the individual indeterminates. One can simplify the classification of monomial orders by assuming that the indeterminates are named ''x''<sub>1</sub>, ''x''<sub>2</sub>, ''x''<sub>3</sub>, ... in decreasing order for the monomial order considered, so that always {{nowrap|1=''x''<sub>1</sub> > ''x''<sub>2</sub> > ''x''<sub>3</sub> > &hellip;}}. (If there should be infinitely many indeterminates, this convention is incompatible with the condition of being a well ordering, and one would be forced to use the opposite ordering; however the case of polynomials in infinitely many variables is rarely considered.) In the example below we use ''x'', ''y'' and ''z'' instead of ''x''<sub>1</sub>, ''x''<sub>2</sub> and ''x''<sub>3</sub>. With this convention there are still many examples of different monomial orders.\n\n=== Lexicographic order ===\n'''Lexicographic order''' (lex) first compares exponents of ''x''<sub>1</sub> in the monomials, and in case of equality compares exponents of ''x''<sub>2</sub>, and so forth. The name is derived from the similarity with the usual [[alphabetical order]] used in [[lexicography]] for dictionaries, if monomials are represented by the sequence of the exponents of the indeterminates. If the number of indeterminates is fixed (as it is usually the case), the [[lexicographical order]] is a [[well-order]], although this is not the case for the lexicographical order applied to sequences of various lengths (see {{slink|Lexicographic order|Ordering of sequences of various lengths}}. For [[Gröbner basis]] computations this ordering tends to be the most costly; thus it should be avoided, as far as possible, except for very simple computations.\n\n=== Graded lexicographic order ===\n'''Graded lexicographic order''' (grlex, or deglex for '''degree lexicographic order''') first compares the total degree (sum of all exponents), and in case of a tie applies lexicographic order. This ordering is not only a well ordering, it also has the property that any monomial is preceded only by a finite number of other monomials; this is not the case for lexicographic order, where all (infinitely many) powers of ''x'' are less than ''y'' (that lexicographic order is nevertheless a well ordering is related to the impossibility of constructing an infinite decreasing chain of monomials). Although very natural, this ordering is rarely used: the [[Gröbner basis]] for the graded reverse lexicographic order, which follows, is easier to compute and provides the same information on the input set of polynomials.\n\n=== Graded reverse lexicographic order ===\n'''Graded reverse lexicographic order''' (grevlex, or degrevlex for '''degree reverse lexicographic order''') compares the total degree first, then uses a reverse lexicographic order as tie-breaker, but it ''reverses the outcome'' of the lexicographic comparison so that lexicographically larger monomials of the same degree are considered to be degrevlex smaller. For the final order to exhibit the conventional ordering {{nowrap|1=''x''<sub>1</sub> > ''x''<sub>2</sub> > &hellip; > ''x''<sub>n</sub>}} of the indeterminates, it is furthermore necessary that the tie-breaker lexicographic order before reversal considers the ''last'' indeterminate ''x''<sub>n</sub> to be the largest, which means it must start with that indeterminate. A concrete recipe for the graded reverse lexicographic order is thus to compare by the total degree first, then compare exponents of the ''last'' indeterminate ''x''<sub>''n''</sub> but ''reversing the outcome'' (so the monomial with smaller exponent is larger in the ordering), followed (as always only in case of a tie) by a similar comparison of ''x''<sub>''n''−1</sub>, and so forth ending with ''x''<sub>1</sub>. <!-- Unlike for graded lexicographic order, the ungraded version of this ordering does not give a monomial ordering, since the (increasing) powers of any single indeterminate would form an infinite decreasing chain. Indeed, thanks to the comparison of total degree first, the reversal of subsequent comparisons can be interpreted informally as follows: the monomial with a smaller power of ''x''<sub>''n''</sub> necessarily has a higher power of some (unspecified) ''x''<sub>''i''</sub> with ''i''&lt;''n'' (indeed it has greater total degree with respect to all indeterminates other than ''x''<sub>''n''</sub>). -->\n\nThe differences between graded lexicographic and graded reverse lexicographic orders are subtle, since they in fact coincide for 1 and 2 indeterminates. The first difference comes for degree 2 monomials in 3 indeterminates, which are graded lexicographic ordered as <math> x_1^2 > x_1 x_2 > x_1 x_3 > x_2^2 > x_2 x_3 > x_3^2 </math> but graded reverse lexicographic ordered as <math> x_1^2 > x_1 x_2 > x_2^2 > x_1 x_3 > x_2 x_3 > x_3^2 </math>. The general trend is that the reverse order exhibits all variables among the small monomials of any given degree, whereas with the non-reverse order the intervals of smallest monomials of any given degree will only be formed from the smallest variables.\n\n=== Elimination order ===\n'''Block order''' or '''elimination order''' (lexdeg) may be defined for any number of blocks but, for sake of simplicity, we consider only the case of two blocks (however, if the number of blocks equals the number of variables, this order is simply the lexicographic order). For this ordering, the variables are divided in two blocks ''x''<sub>1</sub>,..., ''x''<sub>''h''</sub> and ''y''<sub>1</sub>,...,''y''<sub>''k''</sub> and a monomial ordering is chosen for each block, usually the graded reverse lexicographical order. Two monomials are compared by comparing their ''x'' part, and in case of a tie, by comparing their ''y'' part. This ordering is important as it allows ''elimination'', an operation which corresponds to projection in algebraic geometry.\n\n=== Weight order ===\n'''Weight order''' depends on a vector <math>(a_1,\\ldots,a_n)\\in\\R_{\\geq0}^n</math> called the weight vector. It first compares the [[dot product]] of the exponent sequences of the monomials with this weight vector, and in case of a tie uses some other fixed monomial order. For instance, the graded orders above are weight orders for the \"total degree\" weight vector (1,1,...,1).  If the ''a''<sub>''i''</sub> are [[rational dependence|rationally independent]] numbers (so in particular none of them are zero and all fractions <math>\\tfrac{a_i}{a_j}</math> are irrational) then a tie can never occur, and the weight vector itself specifies a monomial ordering. In the contrary case, one could use another weight vector to break ties, and so on; after using ''n'' linearly independent weight vectors, there cannot be any remaining ties. One can in fact define ''any'' monomial ordering by a sequence of weight vectors ([[#cox|Cox]] et al. pp.&nbsp;72–73), for instance (1,0,0,...,0), (0,1,0,...,0), ... (0,0,...,1) for lex, or  (1,1,1,...,1), (1,1,..., 1,0), ... (1,0,...,0) for grevlex.\n\nFor example, consider the monomials <math>xy^2z</math>, <math>z^2</math>, <math>x^3</math>, and <math>x^2z^2</math>; the monomial orders above would order these four monomials as follows:\n\n* Lex: <math>x^3 > x^2z^2 > xy^2z > z^2</math> (power of <math>x</math> dominates).\n* Grlex: <math>x^2z^2 > xy^2z > x^3 > z^2</math> (total degree dominates; higher power of <math>x</math> broke tie among the first two).\n* Grevlex: <math>xy^2z > x^2z^2 > x^3 > z^2</math> (total degree dominates; lower power of <math>z</math> broke tie among the first two).\n* A weight order with weight vector (1,2,4): <math>x^2z^2 > xy^2z > z^2 > x^3</math> (the dot products 10&gt;9&gt;8&gt;3 do not leave any ties to be broken here).\n\n== Related notions ==\n\n* An '''elimination order''' guarantees that a monomial involving any of a set of indeterminates will always be greater than a monomial not involving any of them.\n* A '''product order''' is the easier example of an elimination order. It consists in combining monomial orders on disjoint sets of indeterminates into a monomial order on their union. It simply compares the exponents of the indeterminates in the first set using the first monomial order, then breaks ties using the other monomial ordering on the indeterminates of the second set. This method obviously generalizes to any disjoint union of sets of indeterminates; the lexicographic order can be so obtained from the singleton sets {''x''<sub>1</sub>}, {''x''<sub>2</sub>}, {''x''<sub>3</sub>}, ... (with the unique monomial ordering for each singleton).\n\nWhen using monomial orderings to compute Gröbner bases, different orders can lead to different results, and the difficulty of the computation may vary dramatically.  For example, graded reverse lexicographic order has a reputation for producing, almost always, the Gröbner bases that are the easiest to compute (this is enforced by the fact that, under rather common conditions on the ideal, the polynomials in the Gröbner basis have a degree that is at most exponential in the number of variables; no such complexity result exists for any other ordering). On the other hand, elimination orders are required for [[elimination theory|elimination]] and relative problems.\n\n==References==\n* {{cite book |author1=David Cox |author2=John Little |author3=Donal O'Shea |year=2007 |title=Ideals, Varieties, and Algorithms: An Introduction to Computational Algebraic Geometry and Commutative Algebra |publisher=Springer |isbn=0-387-35650-9|ref=cox}}\n\n[[Category:Order theory]]\n[[Category:Polynomials]]"
    },
    {
      "title": "Mott polynomials",
      "url": "https://en.wikipedia.org/wiki/Mott_polynomials",
      "text": "In mathematics the '''Mott polynomials''' ''s''<sub>''n''</sub>(''x'') are polynomials introduced by {{harvs|txt|authorlink=Nevill Francis Mott|first=N. F. |last=Mott|year=1932|loc=p. 442}} who applied them to a problem in the theory of electrons. \nThey are given by the exponential generating function \n:<math> e^{x(\\sqrt{1-t^2}-1)/t}=\\sum_n s_n(x) t^n/n!.</math>\nBecause the factor in the exponential has the power series\n:<math> \\frac{\\sqrt{1-t^2}-1}{t} = -\\sum_{k\\ge 0} C_k (\\frac{t}{2})^{2k+1}</math>\nin terms of [[Catalan number|Catalan numbers]] <math>C_k</math>, the coefficient in front of <math>x^k</math> of the polynomial can be written as\n:<math>[x^k] s_n(x) =(-1)^k\\frac{n!}{k!2^n}\\sum_{n=l_1+l_2+\\cdots l_k}C_{(l_1-1)/2}C_{(l_2-1)/2}\\cdots C_{(l_k-1)/2}</math>,\naccording to the general formula for [[Generalized Appell polynomials|generalized Appell polynomials]],\nwhere the sum is over all [[Composition (combinatorics)|compositions]] <math>n=l_1+l_2+\\cdots l_k</math> of <math>n</math> into <math>k</math> positive odd integers. The empty product appearing for <math>k=n=0</math> equals 1. Special values, where all contributing Catalan numbers equal 1, are\n:<math> [x^n]s_n(x) = \\frac{(-1)^n}{2^n}.</math>\n:<math> [x^{n-2}]s_n(x) = \\frac{(-1)^n n(n-1)(n-2)}{2^n}.</math>\n<!--\nFor odd <math>n</math> and <math>k=1</math>\n:<math> [x]s_n(x) = -\\frac{n!}{2^n}C_{(n-1)/2}</math>\n-->\nBy differentiation the recurrence for the first derivative becomes\n<!--\n:<math> s'(x) =- \\sum_{m=0}^{n-1} \\frac{n!}{m!2^{n-m}} C_{(n-1-m)/2}s_m(x)</math>,\nwhere the sum is over the <math>m</math> such that <math>(n-1-m)/2</math> is integer.\n-->\n:<math> s'(x) =- \\sum_{k=0}^{\\lfloor (n-1)/2\\rfloor} \\frac{n!}{(n-1-2k)!2^{2k+1}} C_k s_{n-1-2k}(x).</math>\n\nThe first few of them are  {{OEIS|A137378}}\n:<math>s_0(x)=1;</math>\n:<math>s_1(x)=-\\frac{1}{2}x;</math>\n:<math>s_2(x)=\\frac{1}{4}x^2;</math>\n:<math>s_3(x)=-\\frac{3}{4}x-\\frac{1}{8}x^3;</math>\n:<math>s_4(x)=\\frac{3}{2}x^2+\\frac{1}{16}x^4;</math>\n:<math>s_5(x)=-\\frac{15}{2}x-\\frac{15}{8}x^3-\\frac{1}{32}x^5;</math>\n:<math>s_6(x)=\\frac{225}{8}x^2+\\frac{15}{8}x^4+\\frac{1}{64}x^6;</math>\n\nThe polynomials ''s''<sub>''n''</sub>(''x'') form the associated [[Sheffer sequence]] for –2''t''/(1–t<sup>2</sup>) {{harv|Roman|1984|loc=p.130}}.\n{{harvs|txt | last1=Erdélyi | first1=Arthur | last2=Magnus | first2=Wilhelm | author2-link=Wilhelm Magnus | last3=Oberhettinger | first3=Fritz | last4=Tricomi | first4=Francesco G. | title=Higher transcendental functions. Vol. III | publisher=McGraw-Hill Book Company, Inc., New York-Toronto-London | mr=0066496 | year=1955|loc=p. 251|url=https://authors.library.caltech.edu/43491/}} give an explicit expression for them in terms of the [[generalized hypergeometric function]] <sub>3</sub>F<sub>0</sub>:\n:<math>s_n(x)=(-x/2)^n{}_3F_0(-n,\\frac{1-n}{2},1-\\frac{n}{2};;-\\frac{4}{x^2})</math>\n\n==References==\n*{{Citation | last1=Erdélyi | first1=Arthur | last2=Magnus | first2=Wilhelm | author2-link=Wilhelm Magnus | last3=Oberhettinger | first3=Fritz | last4=Tricomi | first4=Francesco G. | title=Higher transcendental functions. Vol. III | publisher=McGraw-Hill Book Company, Inc., New York-Toronto-London | mr=0066496 | year=1955}}\n*{{Citation | last1=Mott | first1=N. F. | title=The Polarisation of Electrons by Double Scattering | jstor=95868 | year=1932 | journal=Proceedings of the Royal Society of London. Series A, Containing Papers of a Mathematical and Physical Character | issn=0950-1207 | volume=135 | issue=827 | pages=429–458 | doi=10.1098/rspa.1932.0044}}\n*{{Citation | last1=Roman | first1=Steven | title=The umbral calculus | url=https://books.google.com/books?id=JpHjkhFLfpgC | publisher=Academic Press Inc. [Harcourt Brace Jovanovich Publishers] | location=London | series=Pure and Applied Mathematics | isbn=978-0-12-594380-2 | mr=741185 |id=Reprinted by Dover, 2005 | year=1984 | volume=111}}\n\n[[Category:Polynomials]]"
    },
    {
      "title": "Multi-homogeneous Bézout theorem",
      "url": "https://en.wikipedia.org/wiki/Multi-homogeneous_B%C3%A9zout_theorem",
      "text": "In [[algebra]] and [[algebraic geometry]], the '''multi-homogeneous Bézout theorem''' is a generalization to multi-homogeneous polynomials of [[Bézout's theorem]], which counts the number of isolated common zeros of a set of [[homogeneous polynomial]]s. This generalization is due to [[Igor Shafarevich]].<ref>I. R. Shafarevich, ''Basic algebraic geometry'', Springer Study Edition, Springer-Verlag, Berlin, 1977, Translated from the Russian by K. A. Hirsch; Revised printing of Grundlehren der mathematischen Wissenschaften, Vol. 213, 1974. </ref>\n\n==Motivation==\nGiven a [[polynomial equation]] or a [[system of polynomial equations]] it is often useful to compute or to bound the number of solutions without computing explicitly the solutions.\n\nIn the case of a single equation, this problem is solved by the [[fundamental theorem of algebra]], which asserts that the number of [[complex number|complex]] solutions is bounded by the [[degree of a polynomial|degree]] of the polynomial, with equality, if the solutions are counted with their [[multiplicities]].\n\nIn the case of a system of {{mvar|n}} polynomial equations in {{mvar|n}} unknowns, the problem is solved by [[Bézout's theorem]], which asserts that, if the number of complex solutions is finite, their number is bounded by the product of the degrees of the solutions. Moreover, if the number of solutions [[point at infinity|at infinity]] is also finite, then the product of the degrees equals the number of solutions counted with multiplicities and including the solutions at infinity.\n\nHowever, it is rather common that the number of solutions at infinity is infinite. In this case, the product of the degrees of the polynomials may be much larger than the number of roots, and better bounds are useful. \n\nMulti-homogeneous Bézout theorem provides such a better root when the unknowns may be split into several subsets such that the degree of each polynomial in each subset is lower than the total degree of the polynomial. For example, let <math>p_1, \\ldots, p_{2n}</math> be polynomials of degree two which are of degree one in {{mvar|n}} indeterminate <math>x_1, \\ldots x_n,</math> and also of degree one in <math>y_1, \\ldots y_n.</math> (that is the polynomials are ''bilinear''. In this case, Bézout's theorem bounds the number of solutions by \n:<math>2^{2n},</math>\nwhile the multi-homogeneous Bézout theorem gives the bound (using [[Stirling's approximation]])\n:<math>\\binom{2n}{n}= \\frac{(2n)!}{(n!)^2}\\sim \\frac{2^{2n}}{\\sqrt{\\pi n}}.</math>\n\n==Statement==\n\nA '''multi-homogeneous polynomial''' is a [[polynomial]] that is [[homogeneous polynomial|homogeneous]] with respect to several sets of variables. \n\nMore precisely, consider {{mvar|k}} positive integers <math>n_1, \\ldots, n_k</math>, and, for {{math|1=''i'' = 1, ..., ''k''}}, the <math>n_i+1</math> [[indeterminate (variable)|indeterminates]] <math>x_{i,0}, x_{i,1}, \\ldots, x_{i,n}.</math> A polynomial in all these indeterminates is multi-homogeneous of '''multi-degree''' <math>d_1, \\ldots, d_k,</math> if it is homogeneous of degree <math>d_i</math> in <math>x_{i,0}, x_{i,1}, \\ldots, x_{i,n}.</math>\n\nA '''multi-projective variety''' is a [[projective variety|projective subvariety]] of the product of [[projective space]]s\n:<math>\\mathbb P_{n_1}\\times \\cdots\\times \\mathbb P_{n_k},</math>\nwhere <math>\\mathbb P_n</math> denote the projective space of dimension {{mvar|n}}. A multi-projective variety may be defined as the set of the common nontrivial zeros of an ideal of multi-homogeneous polynomials, where \"nontrivial\" means that  <math>x_{i,0}, x_{i,1}, \\ldots, x_{i,n}</math> are not simultaneously 0, for each {{mvar|i}}.\n\n[[Bézout's theorem]] asserts that {{mvar|n}} homogeneous polynomials of degree <math>d_1, \\ldots, d_n</math> in {{math|''n'' + 1}} indeterminates define either an [[algebraic set]] of positive [[dimension of an algebraic variety|dimension]], or a zero-dimensional algebraic set consisting of <math>d_1\\cdots d_n</math> points counted with their multiplicities.\n\nFor stating the generalization of Bézout's theorem, it is convenient to introduce new indeterminates <math>t_1, \\ldots, t_k,</math> and to represent the multi-degree <math>d_1, \\ldots, d_k</math> by the linear form <math>\\mathbf d=d_1t_1+\\cdots + d_kt_k.</math> In the following, \"multi-degree\" will refer to this linear form rather than to the sequence of degrees.\n\nSetting <math>n=n_1+\\cdots +n_k,</math> the '''multi-homogeneous Bézout theorem''' is the following.\n\n''With above notation,'' {{math|''n''}} ''multi-homogeneous polynomials of multi-degrees'' <math>\\mathbf d_1, \\ldots, \\mathbf d_n</math> ''define either an multi-projective algebraic set of positive dimension, or a zero-dimensional algebraic set consisting of'' {{mvar|B}} ''points, counted with multiplicities, where'' {{mvar|B}} ''is the coefficient of'' \n:<math>t_1^{n_1}\\cdots t_k^{n_k}</math>\n''in the product of linear forms''\n:<math>\\mathbf d_1 \\cdots \\mathbf d_n.</math>\n\n==Non-homogeneous case==\nThe multi-homogeneous Bézout bound on the number of solutions may be used for non-homogeneous systems of equations, when the polynomials may be (multi)-[[Homogeneous polynomial#Homogenization|homogenized]] without increasing the total degree. However, in this case, the bound may be not sharp, if there are solutions \"at infinity\".\n\nWithout insight on the problem that is studied, it may be difficult to group the variables for a \"good\" multi-homogenization. Fortunately, there are many problems where such a grouping results directly from the problem that is modeled. For example, in [[mechanics]], equations are generally homogeneous or almost homogeneous in the lengths and in the masses.\n\n==References==\n{{reflist}}\n\n{{algebraic-geometry-stub}}\n[[Category:Polynomials]]\n[[Category:Algebraic geometry]]"
    },
    {
      "title": "Multilinear polynomial",
      "url": "https://en.wikipedia.org/wiki/Multilinear_polynomial",
      "text": "In algebra, a '''multilinear polynomial''' is a [[polynomial]] that is [[linear]] in each of its variables. In other words, no variable occurs to a power of 2 or higher; or alternatively, each [[monomial]] is a constant times a product of distinct variables.   For example p(x,y,z) = 3xy + 2.5 y - 7z is a multilinear polynomial with degree 2 (because of the monomial 3xy) whereas p(x,y,z) = x² +4y is not.\n\nMultilinear polynomials are important in the study of [[polynomial identity testing]]. The [[Degree of a polynomial|degree]] of a multilinear polynomial is the maximum number of distinct variables occurring in any monomial.<ref>A. Giambruno, Mikhail Zaicev. ''Polynomial Identities and Asymptotic Methods.'' AMS Bookstore, 2005 {{ISBN|978-0-8218-3829-7}}. Section 1.3.</ref>\n\n==References==\n<references />\n\n[[Category:Polynomials]]"
    },
    {
      "title": "Multiplicative sequence",
      "url": "https://en.wikipedia.org/wiki/Multiplicative_sequence",
      "text": "In [[mathematics]], a '''multiplicative sequence''' or '''''m''-sequence''' is a sequence of [[polynomial]]s associated with a formal [[group theory|group]] structure.  They have application in the [[cobordism|cobordism ring]] in [[algebraic topology]].\n\n==Definition==\nLet ''K''<sub>''n''</sub> be polynomials over a [[ring (mathematics)|ring]] ''A'' in indeterminates ''p''<sub>1</sub>,... weighted so that ''p''<sub>''i''</sub> has weight ''i'' (with ''p''<sub>0</sub> = 1) and all the terms in ''K''<sub>''n''</sub> have weight ''n'' (so that ''K''<sub>''n''</sub> is a polynomial in ''p''<sub>1</sub>,&nbsp;...,&nbsp;''p''<sub>''n''</sub>).  The sequence ''K''<sub>''n''</sub> is ''multiplicative'' if an identity\n\n:<math>\\sum_i p_i z^i = \\sum_i p'_i z^i \\cdot \\sum_i p''_i z^i </math>\n\nimplies\n\n:<math>\\sum_i K_i(p_1,\\ldots,p_i) z^i = \\sum_j K_j(p'_1,\\ldots,p'_j) z^j \\cdot \\sum_k K_k(p''_1,\\ldots,p''_k) z^k </math>\n\nIn other words, <math>p\\mapsto K(p)</math> is required to be an endomomorphism of the multiplicative monoid <math>(A[[X]],\\cdot)</math>.\n\nThe power series\n\n:<math>\\sum K_n(1,0,\\ldots,0) z^n </math>\n\nis the ''characteristic power series'' of the&nbsp;''K''<sub>''n''</sub>.  A multiplicative sequence is determined by its characteristic power series ''Q''(''z''), and every [[power series]] with constant term 1 gives rise to a multiplicative sequence.\n\nTo recover a multiplicative sequence from a characteristic power series ''Q''(''z'') we consider the coefficient of ''z''<sup>''j''</sup> in the product\n\n:<math> \\prod_{i=1}^m Q(\\beta_i z) \\ </math>\n\nfor any&nbsp;''m''&nbsp;>&nbsp;''j''.  This is symmetric in the ''β''<sub>''i''</sub> and homogeneous of weight ''j'': so can be expressed as a polynomial ''K''<sub>''j''</sub>(''p''<sub>1</sub>,&nbsp;...,&nbsp;''p''<sub>''j''</sub>) in the [[elementary symmetric function]]s ''p'' of the&nbsp;''β''.  Then ''K''<sub>''j''</sub> defines a multiplicative sequence.\n\n==Examples==\nAs an example, the sequence ''K''<sub>''n''</sub> = ''p''<sub>''n''</sub> is multiplicative and has characteristic power series&nbsp;1&nbsp;+&nbsp;''z''.\n\nConsider the power series\n\n:<math>Q(z) = \\frac{\\sqrt z}{\\tanh \\sqrt z} = 1 - \\sum_{k=1}^\\infty (-1)^k \\frac{2^{2k}}{(2k)!} B_k z^k \\ </math>\n\nwhere ''B''<sub>''k''</sub> is the ''k''-th [[Bernoulli number]].  The multiplicative sequence with ''Q'' as characteristic power series is denoted ''L''<sub>''j''</sub>(''p''<sub>1</sub>,&nbsp;...,&nbsp;''p''<sub>''j''</sub>).\n\nThe multiplicative sequence with characteristic power series\n\n:<math>Q(z) = \\frac{2\\sqrt z}{\\sinh 2\\sqrt z} \\ </math>\n\nis denoted ''A''<sub>''j''</sub>(''p''<sub>1</sub>,...,''p''<sub>''j''</sub>).\n\nThe multiplicative sequence with characteristic power series\n\n:<math>Q(z) = \\frac{z}{1-\\exp(-z)}  = 1 + \\frac{x}{2} - \\sum_{k=1}^\\infty (-1)^k \\frac{B_k}{(2k)!} z^{2k} \\ </math>\n\nis denoted ''T''<sub>''j''</sub>(''p''<sub>1</sub>,...,''p''<sub>''j''</sub>): these are the ''[[Todd polynomial]]s''.\n\n==Genus==\n{{main|Genus of a multiplicative sequence}}\n\nThe ''genus'' of a multiplicative sequence is  a [[ring homomorphism]], from the  [[cobordism|cobordism ring]] of smooth oriented [[compact manifold]]s to another [[ring (mathematics)|ring]], usually the ring of [[rational number]]s.\n\nFor example, the [[Todd genus]] is associated to the Todd polynomials with characteristic power series <math>\\frac{z}{1-\\exp(-z)}</math>.\n\n==References==\n* {{cite book | zbl=0843.14009 | last=Hirzebruch | first=Friedrich | authorlink=Friedrich Hirzebruch | title=Topological methods in algebraic geometry | others=Translation from the German and appendix one by R. L. E. Schwarzenberger. Appendix two by A. Borel | edition=Reprint of the 2nd, corr. print. of the 3rd | origyear=1978 | series=Classics in Mathematics | location=Berlin | publisher=[[Springer-Verlag]] | year=1995 | isbn=3-540-58663-6 }}\n\n[[Category:Polynomials]]\n[[Category:Topological methods of algebraic geometry]]"
    },
    {
      "title": "Narumi polynomials",
      "url": "https://en.wikipedia.org/wiki/Narumi_polynomials",
      "text": "{{Orphan|date=September 2011}}\n\nIn mathematics, the '''Narumi polynomials''' ''s''<sub>''n''</sub>(''x'') are polynomials introduced by {{harvtxt|Narumi|1929}} given by the [[generating function]]\n\n:<math>\\displaystyle \\sum s_n(x)t^n/n! = \\left(\\frac{t}{\\log(1+t)}\\right)^a(1+t)^x</math>\n\n{{harv|Roman|1984|loc=4.4}}, {{harv|Boas|Buck|1958|loc=p.37}}\n\n==See also==\n*[[Umbral calculus]]\n\n==References==\n\n*{{Citation | last1=Boas | first1=Ralph P. | last2=Buck | first2=R. Creighton | title=Polynomial expansions of analytic functions | url=https://books.google.com/books?id=eihMuwkh4DsC | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Ergebnisse der Mathematik und ihrer Grenzgebiete. Neue Folge.  | mr=0094466 | year=1958 | volume=19}}\n*{{Citation | last1=Narumi | first1=S. | title=On a power series having only a finite number of algebraico logarithmic singularities on its circle of convergence. | url=http://www.journalarchive.jst.go.jp/english/jnlabstract_en.php?cdjournal=tmj1911&cdvol=30&noissue=0&startpage=185 | jfm=55.0185.03 | year=1929 | journal=[[Tohoku Mathematical Journal]] | volume=30 | pages=185–201}}\n*{{Citation | last1=Roman | first1=Steven | title=The umbral calculus | url=https://books.google.com/books?id=JpHjkhFLfpgC | publisher=Academic Press Inc. [Harcourt Brace Jovanovich Publishers] | location=London | series=Pure and Applied Mathematics | isbn=978-0-12-594380-2 | mr=741185 | year=1984 | volume=111}} Reprinted by Dover, 2005\n\n[[Category:Polynomials]]"
    },
    {
      "title": "Octic equation",
      "url": "https://en.wikipedia.org/wiki/Octic_equation",
      "text": "{{Confuse|Optic equation}}\n\n[[Image:Polynomial degree 8.png|thumb|right|Graph of a polynomial of degree 8, with 8 [[real number|real]] [[root of a polynomial|roots]] (crossings of the {{math|''x''}} axis) and with 7 [[critical point (mathematics)|critical points]]. In general, depending on the number and vertical location of the local [[minimum|maxima and minima]], the number of real roots could be 8, 6, 4, 2, or 0. The number of [[complex number|complex]] roots equals 8 minus the number of real roots.]]\n\nIn [[algebra]], an '''octic equation'''<ref>[[James Cockle]] proposed the names \"sexic\", \"septic\", \"octic\", \"nonic\", and \"decic\" in 1851. ([https://books.google.com/books?id=cxIFAAAAQAAJ&pg=PP1#v=onepage&q=sexic%20septic%20octic%20nonic%20decic&f=false ''Mechanics Magazine'', Vol. LV, p. 171])</ref> is an [[equation]] of the form\n\n:<math>ax^8+bx^7+cx^6+dx^5+ex^4+fx^3+gx^2+hx+k=0,\\,</math>\n\nwhere {{math|''a'' ≠ 0}}.\n\nAn '''octic function''' is a [[Function (mathematics)|function]] of the form\n\n:<math>f(x)=ax^8+bx^7+cx^6+dx^5+ex^4+fx^3+gx^2+hx+k,</math>\n\nwhere {{math|''a'' ≠ 0}}. In other words, it is a [[polynomial]] of [[Degree of a polynomial|degree]] eight. If {{math|1=''a'' = 0}}, then ''f'' is a [[septic function]] ({{math|''b'' ≠ 0}}), [[sextic function]] ({{math|1=''b'' = 0, ''c ''≠ 0}}), etc.\n\nThe equation may be obtained from the function by setting {{math|1=''f''(''x'') = 0}}.\n\nThe ''coefficients'' {{math|''a'', ''b'', ''c'', ''d'', ''e'', ''f'', ''g'', ''h'', ''k''}} may be either [[integers]], [[rational number]]s, [[real number]]s, [[complex number]]s or, more generally, members of any [[field (mathematics)|field]].\n\nSince an octic function is defined by a polynomial with an even degree, it has the same infinite limit when the argument goes to positive or negative [[infinity]]. If the [[leading coefficient]] {{math|''a''}} is positive, then the function increases to positive infinity at both sides; and thus the function has a global minimum. Likewise, if {{math|''a''}} is negative, the octic function decreases to negative infinity and has a global maximum. The [[derivative]] of an octic function is a [[septic function]].\n\n== Solvable octics ==\n\nBy the [[Abel–Ruffini theorem]], there is no general [[algebraic formula]] for a solution of an octic equation in terms of its parameters. However, some sub-classes of octics do have such formulas.\n\nTrivially, octics of the form\n\n:<math>x^8=a</math>\n\nwith positive ''a'' have the solutions \n\n:<math>x_k=a^{1/8}\\omega_k\\, , \\quad k=1, \\dots , 8,</math>\n\nwhere <math>\\omega_k</math> is the ''k''-th [[roots of unity|eighth root of 1]] in the [[complex plane]].\n\nOctics which can be [[polynomial decomposition|decomposed as a functional composition]] of solvable polynomials can be solved. For example, octics of the form \n:<math>ax^8+ex^4+k=0</math>\nare compositions of a quadratic and {{math|''x''<sup>4</sup>}}. Octics of the form\n:<math>ax^8 +cx^6+ex^4+gx^2+k=0</math> are compositions of a [[quartic equation|quartic]] and {{math|''x''<sup>2</sup>}}.\n\n==Application==\n\nIn some cases some of the quadrisections (partitions into four regions of equal area) of a [[triangle]] by [[perpendicular lines]] are solutions of an octic equation.<ref>http://forumgeom.fau.edu/FG2018volume18/FG201802.pdf Carl Eberhart, “Revisiting the quadrisection problem of Jacob Bernoulli”, ''Forum Geometricorum'' 18, 2018, pp. 7–16 (particularly pp. 14–15).</ref>\n\n==See also==\n*[[Cubic function]]\n*[[Quartic function]]\n*[[Quintic function]]\n\n==References==\n<references/>\n\n{{Polynomials}}\n\n{{DEFAULTSORT:Octic Equation}}\n[[Category:Equations]]\n[[Category:Galois theory]]\n[[Category:Polynomials]]"
    },
    {
      "title": "Order polynomial",
      "url": "https://en.wikipedia.org/wiki/Order_polynomial",
      "text": "{{see also|Order of a polynomial (disambiguation){{!}}Order of a polynomial}}\nThe '''order polynomial''' is a [[polynomial]] studied in mathematics, in particular in [[algebraic graph theory]] and [[algebraic combinatorics]]. The order polynomial counts the number of order-preserving maps from a [[partially ordered set|poset]] to a [[total order|chain]] of length <math>n</math>.  These order-preserving maps were first introduced by [[Richard P. Stanley]] while studying ordered structures and partitions as a Ph.D. student at [[Harvard University]] in 1971 under the guidance of [[Gian-Carlo Rota]]. \n\n== Definition ==\n\nLet <math>P</math> be a [[Finite set|finite]] [[partially ordered set|poset]], <math>x</math> and <math>y</math> be points in <math>P</math> and <math>[n]</math> be a [[total order|chain]] of length <math>n</math>. A map <math> \\phi: P \\to [n] </math> is order-preserving if <math> x \\leq y</math> implies <math>\\phi(x) \\leq \\phi(y)</math>. The number of such maps grows polynomially with <math>n</math>, and the function that counts their number as a function of <math>n</math> is the order polynomial <math>\\Omega(n)</math>.\n\nSimilarly, we can define an order polynomial that counts the number of [[strict]]ly order-preserving maps. A map <math> \\phi: P \\to [n]</math> is strictly order-preserving if <math> x < y</math> implies <math>\\phi(x) < \\phi(y)</math>. The order polynomial that counts the number of these maps is denoted by <math>\\Omega^\\circ (n)</math>.<ref>{{cite book|last1=Stanley|first1=Richard P.|title=Ordered structures and partitions|date=1972|publisher=American Mathematical Society|location=Providence, Rhode Island}}</ref>\n\n== Examples == \n\nLet <math> P </math> be a [[total order|chain]] of length <math>k</math>. Then <math>\\Omega(n) = \\binom{n+k-1}{k}</math> and <math>\\Omega^\\circ(n) = \\binom{n}{k}.</math>\n\nWe can also consider the poset <math> P </math> with <math>p</math> disjoint points. Then the number of order-preserving maps to a [[total order|chain]] of length <math> n </math> is <math>\\Omega^\\circ(n) = n^p</math>, and <math>\\Omega^\\circ(n) = n^p </math>. \n\n==Reciprocity theorem ==\n\nThe reciprocity theorem shows that there is a relation between strictly order-preserving maps and order-preserving maps. In addition, this comes hand in hand with important properties that the [[chromatic polynomial]] and [[Ehrhart polynomial]] share. The relation can be stated as follows:<ref>{{cite journal |last1=Stanley |first1=Richard P. |title=A chromatic-like polynomial for ordered sets |date=1970 |journal=Proc. Second Chapel Hill Conference on Combinatorial Mathematics and Its Appl. |pages=421&ndash;427}}</ref>\n:<math>\\Omega^\\circ(n) = (-1)^{|P|}\\Omega(-n).</math>\nIn the case that <math>P</math> is a chain, this recovers the [[Binomial coefficient#Generalization to negative integers|negative binomial identity]].\n\n== Connections ==\n\n===Chromatic polynomial ===\n\nThe [[chromatic polynomial]] counts the number of proper ways to color a [[graph (discrete mathematics)|graph]]. Let <math>G </math> be a [[finite set|finite]] graph and let <math>\\sigma</math> be an [[acyclic orientation]] of <math>G</math>. Then there is a natural [[binary relation]] on the [[Vertex (graph theory)|vertices]] <math> v,u </math> of <math>G</math> defined as <math> u \\geq v </math> if <math>  u \\rightarrow v </math>. In particular, <math>\\sigma </math> is a [[partial ordering]] on the set of vertices <math>V(G) </math> of <math>G</math>. In addition,  <math>\\phi: V(G) \\rightarrow \\lbrace 1,2,3,\\ldots,n \\rbrace </math> is compatible with <math>\\sigma</math> if <math>\\phi</math> is order-preserving. Then, we can conclude that \n:<math> P(G,n) = \\sum_{\\sigma} \\Omega^\\circ(n),</math> \nwhere <math>\\sigma</math> are all acyclic orientations of ''G''.<ref>{{cite journal|last1=Stanley|first1=Richard P.|title=Acyclic orientations of graphs|journal=Discrete Math|date=1973|volume=5|pages=171–178}}</ref>\n\n===Ehrhart polynomial ===\nThe [[Ehrhart polynomial]] is a polynomial that associates the number of [[integer lattice]] points and the expansion of a [[polytope]] <math>P</math> by a factor of <math>t</math>. In other words, consider the [[Lattice (group)|lattice]] <math>L</math> and a <math>d</math>-dimensional polytope in <math>\\mathbb{R}^n</math> with [[integer]] vertices. Let <math>t</math> be a positive integer, and <math>tP</math> be a dilation of <math>P</math> so \n:<math>L(P,t) = \\#(tP \\cap L)</math> \nis the number of lattice points in <math>tP</math>. [[Eugène Ehrhart]] showed that this was a [[rational function|rational polynomial]] of degree <math>d</math> in <math>t</math>.<ref>{{Cite book |title=Computing the continuous discretely |last=Beck |first=Matthias |last2=Robins |first2=Sinai |publisher=Springer |year=2015 |isbn=978-1-4939-2968-9 |location=New York |pages=64–72}}</ref>\n\n===Order polytope===\nThe '''order polytope''' associates a [[polytope]] with a partial order. Let <math>P</math> be a poset with <math>p</math> elements. Then the order polytope, denoted <math>O(P)</math>, is the set of points in the <math>p</math>-dimensional [[unit cube]] such that the coordinates satisfy the partial order.<ref>{{cite journal |first1=Alexander |last1=Karzanov |first2=Leonid |last2=Khachiyan |title=On the conductance of Order Markov Chains |journal=Order |volume=8 |pages=7&ndash;15 |year=1991 |doi=10.1007/BF00385809}}</ref><ref>{{cite journal |first1=Graham |last1=Brightwell |first2=Peter |last2=Winkler |title=Counting linear extensions |journal=Order |volume=8 |pages=225&ndash;242 |year=1991 |doi=10.1007/BF00383444}}</ref>\n\nMore formally, let <math>\\mathbb{R}^{|P|}</math> be the set of all functions from <math>P</math> to <math>\\mathbb{R}</math>. The order polytope <math>O(P)</math> of the poset <math>P</math> is the set of all maps <math>\\phi</math> in <math>\\mathbb{R}^{|P|}</math> which satisfy the following two conditions. First, <math>0 \\leq \\phi(x) \\leq 1</math> for all <math>x \\in P</math>, and second, <math>f(x) \\leq f(y)</math> if <math>x \\leq y</math> in the ordering of <math>P</math>. Thus, the polytope can be created given a poset and a partial order.<ref name=\"Stanley1986\">{{Cite journal|last=Stanley|first=Richard P.|date=1986|title=Two poset polytopes|url=|journal=Discrete Comput. Geom.|doi=10.1007/BF02187680|pmid=|access-date=}}</ref>\n\nThe volume of the order polytope is given by the leading coefficient of the order polynomial, which is the number of [[linear extension]]s of the poset <math>P</math> divided by <math>p!</math>.<ref name=\"Stanley1986\"/><ref>{{cite journal|first=Nathan|last=Linial|year=1984|title=The information-theoretic bound is good for merging|journal=SIAM J. Comput.|volume=13|pages=795&ndash;801|doi=10.1137/0213049}}<br/>{{cite journal|first1=Jeff|last1=Kahn| first2=Jeong Han|last2=Kim|title=Entropy and sorting|doi=10.1145/129712.129731|journal=Journal of Computer and System Sciences|volume=51|year=1995|pages=390&ndash;399}}</ref>\n\n==References==\n{{reflist}}\n[[Category:Order theory]]\n[[Category:Polynomials]]\n[[Category:Polytopes]]"
    },
    {
      "title": "P-recursive equation",
      "url": "https://en.wikipedia.org/wiki/P-recursive_equation",
      "text": "{{expert needed|Mathematics|reason=to review the article}}\n\nIn mathematics a '''P-recursive equation''' is a linear equation of [[Sequence|sequences]] where the coefficient sequences can be represented as [[Polynomial ring|polynomials]]. P-recursive equations are '''linear recurrence equations''' (or '''linear recurrence relations''' or '''linear difference equations''') '''with polynomial coefficients'''. These equations play an important role in different areas of mathematics, specifically in [[combinatorics]]. The sequences which are solutions of these equations are called [[Holonomic function|holonomic]], P-recursive or D-finite.\n\nFrom the late 1980s on the first algorithms were developed to find solutions for these equations. Sergei A. Abramov, [[Marko Petkovšek]] and Mark van Hoeij described algorithms to find polynomial, rational, hypergeometric and d'Alembertian solutions.\n\n== Definition ==\nLet <math display=\"inline\">\\mathbb{K}</math> be a [[field of characteristic zero]] (for example <math display=\"inline\">\\mathbb{K} = \\mathbb{Q}</math>), <math display=\"inline\">p_k(n) \\in \\mathbb{K} [n]</math> polynomials for <math display=\"inline\">k = 0,\\dots,r</math>,<math display=\"inline\">f \\in \\mathbb{K}^{\\N}</math> a sequence and <math display=\"inline\">y \\in \\mathbb{K}^{\\N}</math> an unknown sequence. The equation<math display=\"block\">\\sum_{k=0}^r p_k(n) \\, y (n+k) = f(n)</math>is called a linear recurrence equation with polynomial coefficients (all recurrence equations in this article are of this form). If <math display=\"inline\">p_0</math> and <math display=\"inline\">p_r</math> are both nonzero, then <math display=\"inline\">r</math> is called the order of the equation. If <math display=\"inline\">f</math> is zero the equation is called homogeneous, otherwise it is called inhomogeneous.\n\nThis can also be written as <math display=\"inline\">L y = f</math> where <math display=\"inline\">L=\\sum_{k=0}^r p_k N^k</math> is a linear recurrence operator with polynomial coefficients and <math display=\"inline\">N</math> is the shift operator, i.e. <math display=\"inline\">N \\, y (n) = y (n+1)</math>.\n\n== Closed form solutions ==\nLet <math display=\"inline\">\\sum_{k=0}^r p_k(n) \\, y (n+k) = f(n)</math> or equivalently <math display=\"inline\">Ly=f</math> be a recurrence equation with polynomial coefficients. There exist several algorithms which compute solutions of this equation. These algorithms can compute polynomial, rational, hypergeometric and d'Alembertian solutions. The solution of a homogeneous equation is given by the [[Kernel (linear algebra)|kernel]] of the linear recurrence operator: <math display=\"inline\">\\ker L = \\{ y \\in \\mathbb{K}^\\N \\, : \\, L y = 0\\}</math>. As a subspace of the space of sequences this kernel has a [[Basis (linear algebra)|basis]].<ref>If sequences are considered equal if they are equal in almost all terms, then this basis is finite. More on this can be found in the book A=B by Petkovšek, Wilf and Zeilberger.</ref> Let <math display=\"inline\">\\{ y^{(1)}, y^{(2)}, \\dots, y^{(m)} \\}</math> be a basis of <math display=\"inline\">\\ker L</math>, then the formal sum <math display=\"inline\">c_1 y^{(1)} + \\dots + c_m y^{(m)}</math> for arbitrary constants <math display=\"inline\">c_1,\\dots,c_m \\in \\mathbb{K} </math> is called the general solution of the homogeneous problem <math display=\"inline\">Ly=0</math>. If <math display=\"inline\">\\tilde{y}</math> is a particular solution of <math display=\"inline\">Ly=f</math>, i.e. <math display=\"inline\">L \\tilde{y}=f</math>, then <math display=\"inline\">c_1 y^{(1)} + \\dots + c_m y^{(m)} + \\tilde{y}</math> is also a solution of the inhomogeneous problem and it is called the general solution of the inhomogeneous problem.\n\n=== Polynomial solutions ===\n{{Main|Polynomial solutions of P-recursive equations}}In the late 1980s Sergei A. Abramov described an algorithm which finds the general polynomial solution of a recurrence equation, i.e. <math display=\"inline\">y (n) \\in \\mathbb{K} [n]</math>, with a polynomial right-hand side<math display=\"inline\">f(n) \\in \\mathbb{K} [n]</math>. He (and a few years later [[Marko Petkovšek]]) gave a degree bound for polynomial solutions. This way the problem can simply be solved by considering a [[system of linear equations]].<ref>{{Cite journal|last=Abramov|first=Sergei A.|date=1989|title=Problems in computer algebra that are connected with a search for polynomial solutions of linear differential and difference equations|url=|journal=Moscow University Computational Mathematics and Cybernetics|volume=3|pages=|via=}}</ref><ref name=\"Petkovšek 1992\">{{Cite journal|last=Petkovšek|first=Marko|date=1992|title=Hypergeometric solutions of linear recurrences with polynomial coefficients|journal=Journal of Symbolic Computation|volume=14|issue=2–3|pages=243–264|doi=10.1016/0747-7171(92)90038-6|issn=0747-7171}}</ref><ref name=\"Petkovšek 1996\">{{Cite book|url=https://www.math.upenn.edu/~wilf/Downld.html|title=A=B|last=Petkovšek|first=Marko|last2=Wilf|first2=Herbert S.|last3=Zeilberger|first3=Doron|date=1996|publisher=A K Peters|others=|isbn=978-1568810638|location=|pages=|oclc=33898705}}</ref> In 1995 Abramov, Bronstein and Petkovšek showed that the polynomial case can be solved more efficiently by considering [[Formal power series|power series]] solution of the recurrence equation in a specific power basis (i.e. not the ordinary basis <math display=\"inline\">(x^n)_{n \\in \\N}</math>).<ref>{{Cite book|last=Abramov|first=Sergei A.|last2=Bronstein|first2=Manuel|last3=Petkovšek|first3=Marko|date=1995|title=On polynomial solutions of linear operator equations|journal=ISSAC '95 Proceedings of the 1995 International Symposium on Symbolic and Algebraic Computation|publisher=ACM|volume=|pages=290–296|doi=10.1145/220346.220384|isbn=978-0897916998|citeseerx=10.1.1.46.9373}}</ref>\n\nThe other algorithms for finding more general solutions (e.g. rational or hypergeometric solutions) also rely on algorithms which compute polynomial solutions.\n\n=== Rational solutions ===\n{{Main|Abramov's algorithm}}In 1989 Sergei A. Abramov showed that a general [[Rational function|rational]] solution, i.e. <math display=\"inline\">y(n) \\in \\mathbb{K} (n)</math>, with polynomial right-hand side <math display=\"inline\">f(n) \\in \\mathbb{K}[n]</math>, can be found by using the notion of a universal denominator. A universal denominator is a polynomial <math display=\"inline\">u</math> such that the denominator of every rational solution divides <math display=\"inline\">u</math>. Abramov showed how this universal denominator can be computed by only using the first and the last coefficient polynomial <math display=\"inline\">p_0</math> and <math display=\"inline\">p_r</math>. Substituting this universal denominator for the unknown denominator of <math>y</math> all rational solutions can be found by computing all polynomial solutions of a transformed equation.<ref>{{Cite journal|last=Abramov|first=Sergei A.|date=1989|title=Rational solutions of linear differential and difference equations with polynomial coefficients|journal=USSR Computational Mathematics and Mathematical Physics|volume=29|issue=6|pages=7–12|doi=10.1016/s0041-5553(89)80002-3|issn=0041-5553}}</ref>\n\n=== Hypergeometric solution ===\n{{Main|Petkovšek's algorithm}}A sequence <math display=\"inline\">y(n)</math> is called [[Hypergeometric identity|hypergeometric]] if the ratio of two consecutive terms is a rational function in <math>n</math>, i.e. <math display=\"inline\">y (n+1) / y(n) \\in \\mathbb{K} (n)</math>. This is the case if and only if the sequence is the solution of a first-order recurrence equation with polynomial coefficients. The set of hypergeometric sequences is not a subspace of the space of sequences as it is not closed under addition.\n\nIn 1992 [[Marko Petkovšek]] gave an [[Petkovšek's algorithm|algorithm]] to get the general hypergeometric solution of a recurrence equation where the right-hand side <math>f</math> is the sum of hypergeometric sequences. The algorithm makes use of the Gosper-Petkovšek normal-form of a rational function. With this specific representation it is again sufficient to consider polynomial solutions of a transformed equation.<ref name=\"Petkovšek 1992\" />\n\nA different and more efficient approach is due to Mark van Hoeij. Considering the roots of the first and the last coefficient polynomial <math display=\"inline\">p_0</math> and <math display=\"inline\">p_r</math> – called singularities – one can build a solution step by step making use of the fact that every hypergeometric sequence <math display=\"inline\">y (n)</math> has a representation of the form<math display=\"block\">y (n) = c \\, r(n)\\, z^n \\, \\Gamma(n-\\xi_1)^{e_1} \\Gamma(n-\\xi_2)^{e_2} \\cdots \\Gamma(n-\\xi_s)^{e_s}</math>for some <math display=\"inline\">c \\in \\mathbb{K}, z \\in \\overline{\\mathbb{K}}, s \\in \\N, r(n) \\in \\overline\\mathbb{K}(n), \\xi_1, \\dots, \\xi_s \\in \\overline{\\mathbb{K}}</math> with <math display=\"inline\">\\xi_i-\\xi_j \\notin \\Z</math> for <math display=\"inline\">i \\neq j</math> and <math display=\"inline\">e_1, \\dots, e_s \\in \\Z</math>. Here <math display=\"inline\">\\Gamma (n)</math> denotes the [[Gamma function]] and <math display=\"inline\">\\overline{\\mathbb{K}}</math> the [[algebraic closure]] of the field <math display=\"inline\">\\mathbb{K}</math>. Then the <math display=\"inline\">\\xi_1, \\dots, \\xi_s </math> have to be singularities of the equation (i.e. roots of <math display=\"inline\">p_0</math> or <math display=\"inline\">p_r</math>). Furthermore one can compute bounds for the exponents <math display=\"inline\">e_i</math>. For fixed values <math display=\"inline\">\\xi_1, \\dots, \\xi_s, e_1, \\dots, e_s </math> it is possible to make an ansatz which gives candidates for <math display=\"inline\">z</math>. For a specific <math display=\"inline\">z</math> one can again make an ansatz to get the rational function <math display=\"inline\">r(n)</math> by Abramov's algorithm. Considering all possibilities one gets the general solution of the recurrence equation.<ref>{{Cite journal|last=van Hoeij|first=Mark|date=1999|title=Finite singularities and hypergeometric solutions of linear recurrence equations|journal=Journal of Pure and Applied Algebra|volume=139|issue=1–3|pages=109–131|doi=10.1016/s0022-4049(99)00008-0|issn=0022-4049}}</ref><ref>{{Cite journal|last=Cluzeau|first=Thomas|last2=van Hoeij|first2=Mark|date=2006|title=Computing Hypergeometric Solutions of Linear Recurrence Equations|journal=Applicable Algebra in Engineering, Communication and Computing|language=en|volume=17|issue=2|pages=83–115|doi=10.1007/s00200-005-0192-x|issn=0938-1279}}</ref>\n\n=== D'Alembertian solutions ===\nA sequence <math>y</math> is called d'Alembertian if <math display=\"inline\">y = h_1 \\sum h_2 \\sum \\cdots \\sum h_k</math> for some hypergeometric sequences <math display=\"inline\">h_1,\\dots,h_k</math> and <math display=\"inline\">y=\\sum x</math> means that <math display=\"inline\">\\Delta y = x</math> where <math display=\"inline\">\\Delta </math> denotes the difference operator, i.e. <math display=\"inline\">\\Delta y = N y - y = y (n+1) - y(n)</math>. This is the case if and only if there are first-order linear recurrence operators <math display=\"inline\">L_1, \\dots, L_k</math> with rational coefficients such that <math display=\"inline\">L_k \\cdots L_1 y = 0</math>.<ref name=\"Petkovšek 1996\" />\n\n1994 Abramov and Petkovšek described an algorithm which computes the general d'Alembertian solution of a recurrence equation. This algorithm computes hypergeometric solutions and reduces the order of the recurrence equation recursively.<ref>{{Cite book|last=Abramov|first=Sergei A.|last2=Petkovšek|first2=Marko|date=1994|title=D'Alembertian solutions of linear differential and difference equations|journal=ISSAC '94 Proceedings of the International Symposium on Symbolic and Algebraic Computation|publisher=ACM|volume=|pages=169–174|doi=10.1145/190347.190412|isbn=978-0897916387}}</ref>\n\n== Examples ==\n\n=== Signed permutation matrices ===\nThe number of [[Generalized permutation matrix|signed permutation matrices]] of size <math>n \\times n</math> can be described by the [[Sequence space|sequence]] <math display=\"inline\">y(n) \\in \\Q^{\\N}</math>. A signed permutation matrix is a square matrix which has exactly one nonzero entry in every row and in every column. The nonzero entries can be <math display=\"inline\">\\pm 1</math>. The sequence is determined by the linear recurrence equation with polynomial coefficients<math display=\"block\">y (n) = 4(n-1)^2 \\, y (n-2) + 2 \\, y (n-1)</math>and the initial values <math display=\"inline\">y(0) = 1, y(1) = 2</math>. Applying an algorithm to find hypergeometric solutions one can find the general hypergeometric solution<math display=\"block\">y (n) = c \\, 2^n n!</math>for some constant <math display=\"inline\">c</math>. Also considering the initial values, the sequence <math display=\"inline\">y (n) = 2^n n!</math> describes the number of signed permutation matrices.<ref>{{Cite web|url=https://oeis.org/A000165|title=A000165 - OEIS|website=oeis.org|access-date=2018-07-02}}</ref>\n\n=== Involutions ===\nThe number of [[Involution (mathematics)|involutions]] <math display=\"inline\">y(n)</math> of a set with <math display=\"inline\">n</math> elements is given by the recurrence equation<math display=\"block\">y (n) = (n-1) \\, y (n-2) + y (n-1).</math>Applying for example [[Petkovšek's algorithm]] it is possible to see that there is no polynomial, rational or hypergeometric solution for this recurrence equation.<ref name=\"Petkovšek 1996\" />\n\n== Applications ==\nA function <math display=\"inline\">F(n,k)</math> is called hypergeometric if <math display=\"inline\">F(n,k+1)/F(n,k), F(n+1,k)/F(n,k) \\in \\mathbb{K}(n,k)</math> where <math display=\"inline\">\\mathbb{K}(n,k)</math> denotes the rational functions in <math display=\"inline\">n</math> and <math display=\"inline\">k</math>. A hypergeometric sum is a finite sum of the form <math display=\"inline\">f(n)=\\sum_k F(n,k)</math> where <math display=\"inline\">F(n,k)</math> is hypergeometric. [[Doron Zeilberger|Zeilberger]]'s creative telescoping algorithm can transform such a hypergeometric sum into a recurrence equation with polynomial coefficients. This equation can then be solved to get for example a linear combination of hypergeometric solutions which is called a closed form solution of <math display=\"inline\">f</math>.<ref name=\"Petkovšek 1996\" />\n\n== References ==\n<references />\n\n\n\n[[Category:Polynomials]]"
    },
    {
      "title": "Padovan polynomials",
      "url": "https://en.wikipedia.org/wiki/Padovan_polynomials",
      "text": "In [[mathematics]], '''Padovan polynomials''' are a generalization of [[Padovan sequence]] numbers.  These [[polynomial]]s are defined by:\n\n:<math>P_n(x)=\\left\\{\\begin{matrix}\n1,\\qquad\\qquad\\qquad\\qquad&\\mbox{if }n=1\\\\\n0,\\qquad\\qquad\\qquad\\qquad&\\mbox{if }n=2\\\\\nx,\\qquad\\qquad\\qquad\\qquad&\\mbox{if }n=3\\\\\nxP_{n-2}(x)+P_{n-3}(x),&\\mbox{ if }n\\ge4.\n\\end{matrix}\\right.</math>\n\nThe first few Padovan polynomials are:\n\n:<math>P_1(x)=1 \\,</math>\n:<math>P_2(x)=0 \\,</math>\n:<math>P_3(x)=x \\,</math>\n:<math>P_4(x)=1 \\,</math>\n:<math>P_5(x)=x^2 \\,</math>\n:<math>P_6(x)=2x \\,</math>\n:<math>P_7(x)=x^3+1 \\,</math>\n:<math>P_8(x)=3x^2 \\,</math>\n:<math>P_9(x)=x^4+3x \\,</math>\n:<math>P_{10}(x)=4x^3+1\\,</math>\n:<math>P_{11}(x)=x^5+6x^2.\\,</math>\n\nThe Padovan numbers are recovered by evaluating the polynomials P<sub>''n''-3</sub>(''x'') at ''x''&nbsp;=&nbsp;1.\n\nEvaluating P<sub>''n''-3</sub>(''x'') at ''x''&nbsp;=&nbsp;2 gives the ''n''th [[Fibonacci sequence|Fibonacci number]] plus (-1)<sup>''n''</sup>.  {{OEIS|id=A008346}}\n\nThe [[Generating function#Ordinary gnerating function|ordinary generating function]] for the sequence is\n\n:<math> \\sum_{n=1}^\\infty P_n(x) t^n = \\frac{t}{1-xt^2-t^3} . </math>\n\n\n==See also==\n*[[Polynomial sequence]]s\n\n[[Category:Polynomials]]"
    },
    {
      "title": "Permutation polynomial",
      "url": "https://en.wikipedia.org/wiki/Permutation_polynomial",
      "text": "In [[mathematics]], a '''permutation polynomial''' (for a given [[ring (mathematics)|ring]]) is a [[polynomial]] that acts as a [[permutation]] of the elements of the ring, i.e. the map <math>x \\mapsto g(x)</math> is a [[bijection]].  In case the ring is a [[finite field]], the [[Dickson polynomials]], which are closely related to the [[Chebyshev polynomials]], provide examples. Over a finite field, every function, so in particular every permutation of the elements of that field, can be written as a polynomial function.\n\nIn the case of finite rings '''Z'''/''n'''''Z''', such polynomials have also been studied and applied in the [[interleaver]] component of [[error detection and correction]] algorithms.<ref name=\"Takeshita1\">{{cite journal |  title=Permutation Polynomial Interleavers: An Algebraic-Geometric Perspective| year=2006 | first1=Oscar | last1= Takeshita |  arxiv = cs/0601048 | doi=10.1109/TIT.2007.896870 | volume=53 | journal=IEEE Transactions on Information Theory | pages=2116–2132}}</ref>\n<ref name=\"Takeshita2\">{{cite arXiv |  title=A New Construction for [[Low-density parity-check code|LDPC Codes]] using Permutation Polynomials over Integer Rings| year=2005 | first1=Oscar | last1= Takeshita |  eprint = cs/0506091 }}</ref>\n\n==Single variable permutation polynomials over finite fields==\nLet {{math|1='''F'''<sub>''q''</sub> = GF(''q'')}} be the finite field of [[Characteristic (field theory)|characteristic]] {{mvar|p}}, that is, the field having {{mvar|q}} elements where {{math|1=''q'' = ''p''<sup>''e''</sup>}} for some prime {{math|''p''}}. A polynomial {{mvar|f}} with coefficients in {{math|'''F'''<sub>''q''</sub>}} (symbolically written as {{math|''f'' ∈ '''F'''<sub>''q''</sub>[''x'']}}) is a ''permutation polynomial'' of {{math|'''F'''<sub>''q''</sub>}} if the function from {{math|'''F'''<sub>''q''</sub>}} to itself defined by <math>c \\mapsto f(c)</math> is a permutation of {{math|'''F'''<sub>''q''</sub>}}.<ref>{{harvnb|Mullen|Panario|2013|loc=p. 215}}</ref>\n\nDue to the finiteness of {{math|'''F'''<sub>''q''</sub>}}, this definition can be expressed in several equivalent ways:<ref>{{harvnb|Lidl|Niederreiter|1983|loc=p. 348}}</ref>\n:* the function  <math> c \\mapsto f(c)</math> is ''onto'' ([[Surjective function|surjective]]);\n:* the function <math>c \\mapsto f(c)</math> is ''one-to-one'' ([[Injective function|injective]]);\n:* {{math|1=''f''(''x'') = ''a''}} has a solution in {{math|'''F'''<sub>''q''</sub>}} for each {{mvar|a}} in {{math|'''F'''<sub>''q''</sub>}};\n:* {{math|1=''f''(''x'') = ''a''}} has a ''unique'' solution in {{math|'''F'''<sub>''q''</sub>}} for each {{mvar|a}} in {{math|'''F'''<sub>''q''</sub>}}.\n\nA characterization of which polynomials are permutation polynomials is given by\n \n(''[[Hermite]]'s Criterion'')<ref>{{harvnb|Lidl|Niederreiter|1997|loc= p. 349}}</ref><ref name=\"MP216\">{{harvnb|Mullen|Panario|2013|loc=p. 216}}</ref>  {{math|''f'' ∈ '''F'''<sub>''q''</sub>[''x'']}} is a permutation polynomial of {{math|'''F'''<sub>''q''</sub>}} if and only if the following two conditions hold:\n# {{mvar|f}} has exactly one root in {{math|'''F'''<sub>''q''</sub>}};\n# for each integer {{math|t}} with {{math|1 ≤ ''t'' ≤ ''q'' − 2}} and <math>t \\not \\equiv 0 \\!\\pmod p</math>, the reduction of {{math|''f''(''x'')<sup>''t''</sup> mod (''x''<sup>''q''</sup> − ''x'')}} has degree {{math|≤ ''q'' − 2}}.\n\nIf {{math|''f''(''x'')}} is a permutation polynomial defined over the finite field {{math|GF(''q'')}},  then so is {{math|1=''g''(''x'') = ''a'' ''f''(''x'' + ''b'') + ''c''}} for all {{math|''a'' ≠ 0, ''b''}} and {{math|''c''}} in {{math|GF(''q'')}}. The permutation polynomial {{math|''g''(''x'')}} is in '''normalized form''' if {{math|''a'', ''b''}} and {{math|''c''}}{{math|}} are chosen so that {{math|''g''(''x'')}} is [[Monic polynomial|monic]], {{math|1=''g''(0) = 0}} and (provided the characteristic {{math|''p''}} does not divide the degree {{math|''n''}} of the polynomial) the coefficient of {{math|1=''x''<sup>''n''-1</sup>}} is 0.\n\nThere are many open questions concerning permutation polynomials defined over finite fields (see {{harvtxt|Lidl|Mullen|1988}} and {{harvtxt|Lidl|Mullen|1993}}).\n\n===Small degree===\nHermite's criterion is computationally intensive and can be difficult to use in making theoretical conclusions. However, [[Leonard Eugene Dickson|Dickson]] was able to use it to find all permutation polynomials of degree at most five over all finite fields. These results are:<ref>{{harvnb|Dickson|1958|loc = pg. 63}}</ref><ref name=\"MP216\" />\n::{| class=\"wikitable\"\n|-\n!Normalized Permutation Polynomial of {{math|'''F'''<sub>''q''</sub>}}\n!{{mvar|q}}\n|-\n|<math>x</math>\n| any <math>q</math>\n|-\n| <math>x^2</math>\n| <math>q \\equiv 0\\! \\pmod 2</math>\n|- \n| <math>x^3</math>\n| <math>q \\not \\equiv 1 \\! \\pmod 3</math>\n|-\n| <math>x^3 - ax</math> (<math>a</math> not a square)\n| <math>q \\equiv 0 \\! \\pmod 3</math>\n|-\n| <math>x^4 \\pm 3x</math>\n| <math>q = 7</math>\n|-\n| <math>x^4 + a_1 x^2 + a_2 x</math> (if its only root in {{math|'''F'''<sub>''q''</sub>}} is 0)\n| <math>q \\equiv 0 \\! \\pmod 2</math>\n|-\n| <math>x^5</math>\n| <math>q \\not \\equiv 1 \\! \\pmod 5</math>\n|- \n| <math>x^5 - ax</math> (<math>a</math> not a fourth power)\n| <math>q \\equiv 0 \\! \\pmod 5</math>\n|-\n| <math>x^5 + ax \\,(a^2 = 2)</math>\n| <math>q = 9</math>\n|-\n| <math>x^5 \\pm 2x^2</math>\n| <math>q = 7</math>\n|-\n| <math>x^5 + ax^3 \\pm x^2 + 3a^2 x</math> (<math>a</math> not a square)\n| <math>q = 7</math>\n|-\n| <math>x^5 + ax^3 + 5^{-1} a^2 x</math> (<math>a</math> arbitrary)\n| <math>q \\equiv \\pm 2 \\! \\pmod 5</math>\n|-\n| <math>x^5 + ax^3 + 3a^2 x</math> (<math>a</math> not a square)\n| <math>q = 13</math>\n|-\n| <math>x^5 - 2ax^3 + a^2x</math> (<math>a</math> not a square)\n| <math>q \\equiv 0 \\! \\pmod 5</math>\n|}\n\nA list of all monic permutation polynomials of degree six in normalized form can be found in {{harvtxt|Shallue|Wanless|2013}}.<ref>{{harvnb|Mullen|Panario|2013|loc=p. 217}}</ref>\n\n===Some classes of permutation polynomials===\nBeyond the above examples, the following list, while not exhaustive, contains almost all of the known major classes of permutation polynomials over finite fields.<ref>{{harvnb|Lidl|Mullen|1988|loc=p. 244}}</ref>\n\n* {{math|''x''<sup>''n''</sup>}} permutes {{math|GF(''q'')}} if and only if {{mvar|n}} and {{math|''q'' − 1}} are [[Coprime integers|coprime]] (notationally, {{math|1=(''n'', ''q'' − 1) = 1}}).<ref name=\"LN351\">{{harvnb|Lidl|Niederreiter|1983|loc=p. 351}}</ref>\n* If {{math|''a''}} is in {{math|GF(''q'')}} and {{math|''n'' ≥ 1}} then the [[Dickson polynomial]] (of the first kind) {{math|''D''<sub>''n''</sub>(''x'',''a'')}} is defined by\n:<math>D_n(x,a)=\\sum_{j=0}^{\\lfloor n/2\\rfloor}\\frac{n}{n-j} \\binom{n-j}{j} (-a)^j x^{n-2j}. </math>\nThese can also be obtained from the [[Recursive relation|recursion]]\n:<math>D_n(x,a) = xD_{n-1}(x,a)-a D_{n-2}(x,a), </math>\nwith the initial conditions <math>D_0(x,a) = 2</math> and <math>D_1(x,a) = x</math>.\nThe first few Dickson polynomials are:\n:<math> D_2(x,a) = x^2 - 2a </math>\n\n:<math> D_3(x,a) = x^3 - 3ax</math>\n\n:<math> D_4(x,a) = x^4 - 4ax^2 + 2a^2 </math>\n\n:<math> D_5(x,a) = x^5 - 5ax^3 + 5a^2x.</math>\n\nIf {{math|''a'' ≠ 0}} and {{math|''n'' > 1}} then {{math|''D''<sub>''n''</sub>(''x'', ''a'')}} permutes GF(''q'') if and only if {{math|1=(''n'', ''q''<sup>2</sup> − 1) = 1}}.<ref>{{harvnb|Lidl|Niederreiter|1983|loc=p. 356}}</ref> If {{math|1=''a'' = 0}} then {{math|1=''D''<sub>''n''</sub>(''x'', 0) = ''x''<sup>''n''</sup>}} and the previous result holds.\n* If {{math|GF(''q''<sup>''r''</sup>)}} is an [[Field extension|extension]] of {{math|GF(''q'')}} of degree {{math|''r''}}, then the [[linearised polynomial|linearized polynomial]]\n::<math>L(x) = \\Sigma_{s=0}^{r-1} \\alpha_s x^{q^s},</math>\n:with {{math|α<sub>''s''</sub>}} in {{math|GF(''q''<sup>''r''</sup>)}}, is a [[linear operator]] on {{math|GF(''q''<sup>''r''</sup>)}} over {{math|GF(''q'')}}. A linearized polynomial {{math|''L''(''x'')}} permutes {{math|GF(''q''<sup>''r''</sup>)}} if and only if 0 is the only root of {{math|''L''(''x'')}} in {{math|GF(''q''<sup>''r''</sup>)}}.<ref name=\"LN351\" /> This condition can be expressed algebraically as<ref name=\"LN362\">{{harvnb|Lidl|Niederreiter|1983|loc=p. 362}}</ref>\n\n::<math> \\rm{det}\\left ( \\alpha_{i-j}^{q^j} \\right ) \\neq 0 \\quad (i, j= 0,1,\\ldots,r-1).</math>\n\nThe linearized polynomials that are permutation polynomials over {{math|GF(''q''<sup>''r''</sup>)}} form a [[Group (mathematics)|group]] under the operation of composition modulo <math>x^{q^r} - x</math>, which is known as the Betti-Mathieu group, isomorphic to the [[general linear group]] {{math|GL(''r'', '''F'''<sub>''q''</sub>)}}.<ref name=\"LN362\" />\n* If {{math|''g''(''x'')}} is in the polynomial ring {{math|'''F'''<sub>''q''</sub>[''x'']}} and {{math|''g''(''x''<sup>''s''</sup>)}} has no nonzero root in {{math|GF(''q'')}} when {{math|''s''}} divides {{math|''q'' − 1}}, and {{math|''r'' > 1}} is relatively prime (coprime) to {{math|''q'' − 1}}, then {{math|''x''<sup>''r''</sup>(''g''(''x''<sup>''s''</sup>))<sup>(''q'' - 1)/''s''</sup>}} permutes {{math|GF(''q'')}}.<ref name=\"MP216\" />\n* Only a few other specific classes of permutation polynomials over {{math|GF(''q'')}} have been characterized. Two of these, for example, are:\n\n:<math> x^{(q + m - 1)/m} + ax </math>\n\n:where {{math|''m''}} divides {{math|''q'' − 1}}, and\n\n:<math> x^r(x^d - a)^{(p^n - 1)/d}</math>\n\n:where {{math|''d''}} divides {{math|''p''<sup>''n''</sup> − 1}}.\n\n===Exceptional polynomials===\nAn '''exceptional polynomial''' over {{math|GF(''q'')}} is a polynomial in {{math|'''F'''<sub>''q''</sub>[''x'']}} which is a permutation polynomial on {{math|GF(''q''<sup>''m''</sup>)}} for infinitely many {{mvar|m}}.<ref>{{harvnb|Mullen|Panario|2013|loc=p. 236}}</ref>\n\nA permutation polynomial over {{math|GF(''q'')}} of degree at most {{math|''q''<sup>1/4</sup>}} is exceptional over {{math|GF(''q'')}}.<ref name=\"MP238\">{{harvnb|Mullen|Panario|2013|loc=p. 238}}</ref>\n\nEvery permutation of {{math|GF(''q'')}} is induced by an exceptional polynomial.<ref name=\"MP238\" />\n\nIf a polynomial with integer coefficients (i.e., in {{math|ℤ[''x'']}}) is a permutation polynomial over {{math|GF(''p'')}} for infinitely many primes {{mvar|p}}, then it is the composition of linear and Dickson polynomials.<ref>{{harvnb|Mullen|Panario|2013|loc=p. 239}}</ref> (See Shur's conjecture below).\n\n==Geometric examples==\n{{main|Oval (projective plane)}}\n\nIn [[finite geometry]] coordinate descriptions of certain point sets can provide examples of permutation polynomials of higher degree. In particular, the points forming an [[oval (projective plane)|oval]] in a finite [[projective plane]], {{math|PG(2,''q'')}} with {{math|''q''}} a power of 2, can be coordinatized in such a way that the relationship between the coordinates is given by an ''[[o-polynomial]]'', which is a special type of permutation polynomial over the finite field {{math|GF(''q'')}}.\n\n==Computational complexity==\nThe problem of testing whether a given polynomial over a finite field is a permutation polynomial can be solved in [[polynomial time]].<ref>{{cite web|year=2005|id={{ECCC|2005|05|008}}|first=Neeraj|last=Kayal|title=Recognizing permutation functions in polynomial time}} For earlier research on this problem, see: {{cite journal\n | last1 = Ma | first1 = Keju\n | last2 = von zur Gathen | first2 = Joachim | author2-link = Joachim von zur Gathen\n | doi = 10.1007/BF01277957\n | issue = 1\n | journal = Computational Complexity\n | mr = 1319494\n | pages = 76–97\n | title = The computational complexity of recognizing permutation functions\n | volume = 5\n | year = 1995}} {{cite journal\n | last = Shparlinski | first = I. E.\n | doi = 10.1007/BF01202000\n | issue = 2\n | journal = Computational Complexity\n | mr = 1190826\n | pages = 129–132\n | title = A deterministic test for permutation polynomials\n | volume = 2\n | year = 1992}}.</ref>\n\n==Permutation polynomials in several variables over finite fields==\nA polynomial <math>f \\in \\mathbb{F}_q[x_1,\\ldots,x_n]</math> is a '''permutation polynomial in {{mvar|n}} variables over''' <math>\\mathbb{F}_q</math> if the equation <math>f(x_1,\\ldots,x_n) = \\alpha</math> has exactly <math>q^{n-1}</math> solutions in <math>\\mathbb{F}_q^n</math> for each <math>\\alpha \\in \\mathbb{F}_q</math>.<ref>{{harvnb|Mullen|Panario|2013|loc=p. 230}}</ref>\n\n==Quadratic permutation polynomials (QPP) over finite rings ==\n\nFor the [[finite ring]] '''Z'''/''n'''''Z''' one can construct quadratic \npermutation polynomials. Actually it is possible if and only if ''n'' is divisible by ''p<sup>2</sup>'' for some prime number ''p''. \nThe construction is surprisingly simple, nevertheless it can produce  permutations with certain good properties. That is why it has been used in the [[interleaver]] component of   [[turbo codes]] in [[3GPP Long Term Evolution]]  mobile telecommunication standard (see 3GPP technical specification 36.212 <ref>[http://www.3gpp.org/ftp/Specs/html-info/36212.htm 3GPP TS 36.212]</ref> e.g. page 14 in version 8.8.0).\n\n===Simple examples ===\n\nConsider <math> g(x) = 2x^2+x </math> for the ring '''Z'''/''4'''''Z'''.\nOne sees: <math> g(0) = 0;~ g(1) = 3;~ g(2) = 2;~ g(3) = 1  </math>,\nso the polynomial defines  the permutation\n\n: <math>\\begin{pmatrix}\n0 &1 & 2 & 3 \\\\\n0 &3 & 2 & 1\\end{pmatrix} </math>.\n\nConsider the same polynomial <math> g(x) = 2x^2+x </math> for the other ring '''Z'''/''8'''''Z'''.\nOne sees: <math> g(0) = 0;~ g(1) = 3;~ g(2) = 2;~ g(3) = 5;~ g(4) = 4;~ g(5) = 7; ~ g(6) = 6; ~ g(7) = 1; </math>,\nso the polynomial defines  the permutation\n\n: <math>\\begin{pmatrix}\n0 &1 & 2 & 3 & 4 & 5 & 6 & 7 \\\\\n0 &3 & 2 & 5 & 4 & 7 & 6 & 1 \\end{pmatrix} </math>.\n\n=== Rings Z/''p<sup>k</sup>''Z ===\n\nConsider <math> g(x) = ax^2+bx+c </math>  for the ring '''Z'''/''p<sup>k</sup>'''''Z'''.\n\nLemma: for ''k=1'' (i.e. '''Z'''/''p'''''Z''') such polynomial defines a permutation\nonly in the case ''a=0'' and ''b'' not equal to zero. So the polynomial is not quadratic, but linear.\n\nLemma: for ''k>1'', ''p>2''( '''Z'''/''p<sup>k</sup>'''''Z''') such polynomial defines a permutation  if and only if   <math>a = 0~ mod~ p</math> and <math>b \\ne 0~ mod~ p</math>.\n\n=== Rings Z/''n''Z ===\n\nConsider <math>n=p_1^{k_1}p_2^{k_2}...p_l^{k_l}</math>, where  ''p<sub>t</sub>'' are prime numbers.\n\nLemma: any polynomial <math> g(x) = a_0+ \\sum_{0<i \\le M} a_i x^i </math>\ndefines a permutation for the ring  '''Z'''/''n'''''Z''' if and only if all the polynomials <math> g_{p_t}(x) = a_{0,p_t}+ \\sum_{0<i \\le M} a_{i,p_t} x^i </math> defines the permutations for all rings <math>Z/p_t^{k_t}Z</math>, where \n<math>a_{j,p_t}</math> are remainders of <math>a_{j}</math>\nmodulo  <math>p_t^{k_t}</math>.\n\nAs a corollary one can construct plenty quadratic permutation polynomials using the following simple construction. \nConsider <math>n=p_1^{k_1}p_2^{k_2}...p_l^{k_l}</math>, assume that ''k<sub>1</sub> >1''.\n\nConsider <math>ax^2+bx</math>, such that <math> a= 0~ mod~ p_1</math>, but <math> a\\ne 0~ mod~ p_1^{k_1}</math>; assume that <math> a= 0~ mod~ p_i^{k_i}</math>,''i''>1. And  assume that <math>b\\ne 0~ mod~ p_i</math> for all ''i''=1...''l''.\n(For example, one can take \n<math> a=p_1 p_2^{k_2}...p_l^{k_l} </math>\nand  <math>b=1</math>).\nThen such polynomial defines a permutation.\n\nTo see this we observe that for all primes ''p<sub>i</sub>'',''i''>1, the reduction of this quadratic polynomial modulo ''p<sub>i</sub>'' is actually linear polynomial and hence is permutation by trivial reason. For the first prime number we should use \nthe lemma discussed previously to see that it defines the permutation.\n\nFor example, consider '''Z'''/''12'''''Z''' and polynomial <math>6x^2+x</math>.\nIt defines a permutation\n\n: <math>\\begin{pmatrix}\n0 &1 & 2 & 3 & 4 & 5  & 6 & 7 & 8 & ... \\\\\n0 &7 & 2 & 9 & 4 & 11 & 6 & 1 & 8 &  ... \\end{pmatrix} </math>.\n\n== Higher degree polynomials over finite rings==\n\nA polynomial ''g''(''x'') for the ring '''Z'''/''p<sup>k</sup>'''''Z''' is a permutation polynomial if and only if it permutes the [[finite field]] '''Z'''/''p'''''Z''' and <math>g'(x) \\ne 0~ mod~ p</math> for all ''x'' in '''Z'''/''p<sup>k</sup>'''''Z''', where ''g''&prime;(''x'') is the [[formal derivative]] of ''g''(''x'').<ref>{{cite journal | last1 = Sun | first1 = Jing | last2 = Takeshita | first2 = Oscar | year = 2005 | title = Interleaver for Turbo Codes Using Permutation Polynomials Over Integer Rings | url = | journal = IEEE Transactions on Information Theory | volume = 51 | issue = 1| page = 102 }}</ref>\n\n==Schur's conjecture==\nLet ''K'' be an [[algebraic number field]] with ''R'' the [[ring of integers]].  The term \"Schur's conjecture\" refers to the assertion \nthat, if a polynomial ''f'' defined over ''K'' is a permutation polynomial on ''R''/''P'' for infinitely many [[prime ideal]]s ''P'', then ''f'' is the composition of Dickson polynomials, degree-one polynomials, and polynomials of the form ''x''<sup>''k''</sup>.  In fact, [[Issai Schur|Schur]] did not make any conjecture in this direction.  The notion that he did is due to Fried,<ref>{{cite journal | last=Fried | first=M. | title=On a conjecture of Schur | journal=Michigan Math. J. | year=1970 | pages=41–55 }}</ref> who gave a flawed proof of a false version of the result.  Correct proofs have been given by Turnwald <ref>{{cite journal | last=Turnwald | first=G. | title=On Schur's conjecture | journal=J. Austral. Math. Soc. | year=1995 | pages=312–357 }}</ref>\nand Müller.<ref>{{cite journal | last=Müller | first=P. | title=A Weil-bound free proof of Schur's conjecture | journal=Finite Fields and Their Applications | year=1997 | pages=25–32 }}</ref>\n\n==Notes==\n{{reflist}}\n\n==References==\n* {{cite book|last=Dickson|first=L. E.|title=Linear Groups with an Exposition of the Galois Field Theory|year=1958|origyear=1901|publisher=Dover|location=New York}} \n* {{cite journal|last1=Lidl|first1=Rudolf|last2=Mullen| first2=Gary L.|title=When Does a Polynomial over a Finite Field Permute the Elements of the Field?|journal=The American Mathematical Monthly|date=March 1988|volume=95|issue=3|pages=243–246|doi=10.2307/2323626|ref=harv}} \n* {{cite journal|last1=Lidl|first1=Rudolf|last2=Mullen| first2=Gary L.|title=When Does a Polynomial over a Finite Field Permute the Elements of the Field?, II|journal=The American Mathematical Monthly|date=January 1993|volume=100|issue=1|pages=71–74|doi=10.2307/2324822}}\n* {{cite book | zbl=0866.11069 | last1=Lidl | first1=Rudolf | last2=Niederreiter | first2=Harald | author2-link = Harald Niederreiter | title=Finite fields | edition=2nd | series=Encyclopedia of Mathematics and Its Applications | volume=20 | publisher=[[Cambridge University Press]] | year=1997 | isbn=0-521-39231-4 }}  Chapter 7.\n* {{citation|first1=Gary L.|last1=Mullen|first2=Daniel|last2=Panario|title=Handbook of Finite Fields|year=2013|publisher=CRC Press|isbn=978-1-4398-7378-6}} Chapter 8.\n* {{citation|first1=C.J.|last1=Shallue|first2=I.M.|last2=Wanless|title=Permutation polynomials and orthomorphism polynomials of degree six|journal=Finite Fields and Their Applications|volume=20|date= March 2013|pages=84–92|doi=10.1016/j.ffa.2012.12.003}}\n\n[[Category:Polynomials]]\n[[Category:Permutations]]"
    },
    {
      "title": "Peters polynomials",
      "url": "https://en.wikipedia.org/wiki/Peters_polynomials",
      "text": "In mathematics, the '''Peters polynomials''' ''s''<sub>''n''</sub>(''x'') are polynomials studied by {{harvs|txt|last=Peters|year1=1956|year2=1956b}} given by the [[generating function]]\n\n:<math>\\displaystyle \\sum_{n=0}^{+\\infty} s_n(x)\\frac{t^n}{n!} = \\frac{(1+t)^x}{(1+(1+t)^\\lambda)^{\\mu}}</math>\n\n{{harv|Roman|1984|loc=4.4.6}}, {{harv|Boas|Buck|1958|loc=p.37}}. They are a generalization of the [[Boole polynomials]].\n\n==See also==\n*[[Umbral calculus]]\n\n==References==\n\n*{{Citation | last1=Boas | first1=Ralph P. | last2=Buck | first2=R. Creighton | title=Polynomial expansions of analytic functions | url=https://books.google.com/books?id=eihMuwkh4DsC | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Ergebnisse der Mathematik und ihrer Grenzgebiete. Neue Folge.  | mr=0094466 | year=1958 | volume=19}}\n*{{Citation | last1=Peters | first1=George Owen | editor1-last=Schafer | editor1-first=Richard D. | title=Boole polynomials of higher and negative orders | doi=10.1090/S0002-9904-1956-09972-0  | year=1956 | journal=Bulletin of the A.M.S. | volume=62 | issue=1 | pages=7}}\n*{{Citation | last1=Peters | first1=George Owen | editor1-last=Schafer | editor1-first=Richard D. | title=Boole polynomials and numbers of the second kind | doi=10.1090/S0002-9904-1956-10046-3  | year=1956b | journal=Bulletin of the A.M.S. | volume=62 | pages=387}}\n*{{Citation | last1=Roman | first1=Steven | title=The umbral calculus | url=https://books.google.com/books?id=JpHjkhFLfpgC | publisher=Academic Press Inc. [Harcourt Brace Jovanovich Publishers] | location=London | series=Pure and Applied Mathematics | isbn=978-0-12-594380-2 | mr=741185 | year=1984 | volume=111}} Reprinted by Dover, 2005\n\n[[Category:Polynomials]]"
    },
    {
      "title": "Pidduck polynomials",
      "url": "https://en.wikipedia.org/wiki/Pidduck_polynomials",
      "text": "In mathematics, the '''Pidduck polynomials''' ''s''<sub>''n''</sub>(''x'') are polynomials introduced by {{harvs|txt|last=Pidduck|year1=1910|year2=1912}} given by the [[generating function]]\n\n:<math>\\displaystyle \\sum_n \\frac{s_n(x)}{n!}t^n = \\left(\\frac{1+t}{1-t}\\right)^x(1-t)^{-1}</math>\n\n{{harv|Roman|1984|loc=4.4.3}}, {{harv|Boas|Buck|1958|loc=p.38}}\n\n==See also==\n*[[Umbral calculus]]\n\n==References==\n\n*{{Citation | last1=Boas | first1=Ralph P. | last2=Buck | first2=R. Creighton | title=Polynomial expansions of analytic functions | url=https://books.google.com/books?id=eihMuwkh4DsC | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Ergebnisse der Mathematik und ihrer Grenzgebiete. Neue Folge.  | mr=0094466 | year=1958 | volume=19}}\n*{{Citation | last1=Pidduck | first1=F. B. | title=On the Propagation of a Disturbance in a Fluid under Gravity | jstor=92977 | publisher=The Royal Society | year=1910 | journal=Proceedings of the Royal Society of London. Series A, Containing Papers of a Mathematical and Physical Character | issn=0950-1207 | volume=83 | issue=563 | pages=347–356 | doi=10.1098/rspa.1910.0023}}\n*{{Citation | last1=Pidduck | first1=F. B. | title=The Wave-Problem of Cauchy and Poisson for Finite Depth and Slightly Compressible Fluid | jstor=93103 | publisher=The Royal Society | year=1912 | journal=Proceedings of the Royal Society of London. Series A, Containing Papers of a Mathematical and Physical Character | issn=0950-1207 | volume=86 | issue=588 | pages=396–405 | doi=10.1098/rspa.1912.0031}}\n*{{Citation | last1=Roman | first1=Steven | title=The umbral calculus | url=https://books.google.com/books?id=JpHjkhFLfpgC | publisher=Academic Press Inc. [Harcourt Brace Jovanovich Publishers] | location=London | series=Pure and Applied Mathematics | isbn=978-0-12-594380-2 | mr=741185 | year=1984 | volume=111}} Reprinted by Dover, 2005\n\n[[Category:Polynomials]]"
    },
    {
      "title": "Pincherle polynomials",
      "url": "https://en.wikipedia.org/wiki/Pincherle_polynomials",
      "text": "In [[mathematics]], the '''Pincherle polynomials''' P<sub>''n''</sub>(''x'') are [[polynomials]] introduced by {{harvs|txt|first=S.|last=Pincherle|year=1891|authorlink=S. Pincherle}} given by the [[generating function]]\n\n:<math>\\displaystyle (1-3xt+t^3)^{-1/2}=\\sum^\\infty\n_{n=0}P_n(x)t^n</math>\n\n[[Humbert polynomials]] are a generalization of Pincherle polynomials\n\n==References==\n\n*{{Citation | last1=Humbert | first1=Pierre | title=Some extensions of Pincherle's Polynomials | doi=10.1017/S0013091500035756 | year=1921 | journal=Proceedings of the Edinburgh mathematics society | volume=39 | pages=21–24}}\n*{{citation|last=Pincherle|first=Salvatore|year=1891|title=Una nuova estensione delle funzioni sferiche| language=Italian|journal=Memorie della accademia R. di Bologna|volume=I|pages=337–369|jfm=23.0514.01}}\n\n[[Category:Polynomials]]"
    },
    {
      "title": "Polylogarithmic function",
      "url": "https://en.wikipedia.org/wiki/Polylogarithmic_function",
      "text": "{{Distinguish|Polylogarithm}}\nA '''polylogarithmic function''' in ''n'' is a [[polynomial]] in the [[logarithm]] of ''n'',\n\n: <math>a_k (\\log n)^k + \\cdots + a_1(\\log n) + a_0. </math>\n\nIn [[computer science]], polylogarithmic functions occur as the [[Big O notation|order]] of [[time complexity|time]] or [[space complexity|memory used]] by some [[algorithm]]s (e.g., \"it has polylogarithmic order\").\n\nAll polylogarithmic functions  of <math>n</math> are <math>o(n^\\varepsilon)</math> for every exponent ''&epsilon;''&nbsp;>&nbsp;0 (for the meaning of this symbol, see [[small o notation]]), that is, a polylogarithmic function grows more slowly than any positive exponent.  This observation is the basis for the [[Soft O notation#Extensions to the Bachmann–Landau notations|soft O notation]] Õ(''n'').\n\n== References ==\n* {{cite web|url=http://xlinux.nist.gov/dads/HTML/polylogarith.html|title=polylogarithmic|last=Black|first=Paul E.|date=2004-12-17|publisher=U.S. National Institute of Standards and Technology|work=Dictionary of Algorithms and Data Structures|accessdate=2010-01-10}}\n\n[[Category:Mathematical analysis]]\n[[Category:Polynomials]]\n[[Category:Analysis of algorithms]]\n\n{{mathanalysis-stub}}\n{{comp-sci-theory-stub}}"
    },
    {
      "title": "Polynomial decomposition",
      "url": "https://en.wikipedia.org/wiki/Polynomial_decomposition",
      "text": "In mathematics, a '''polynomial decomposition''' expresses a [[polynomial]] ''f'' as the [[functional composition]] <math>g  \\circ h</math> of polynomials ''g'' and ''h'', where ''g'' and ''h'' have [[Degree of a polynomial|degree]] greater than 1.<ref>Composition of polynomials may also be thought of as [[substitution (algebra)|substitution]] of one polynomial as the value of the variable of another.</ref> [[Algorithms]] are known for decomposing polynomials in [[polynomial time]].\n\nPolynomials which are decomposable in this way are '''composite polynomials'''; those which are not are '''prime''' or '''indecomposable polynomials'''<ref name=\"ritt\">J.F. Ritt, \"Prime and Composite Polynomials\", ''Transactions of the American Mathematical Society'' '''23''':1:51–66 (January, 1922) {{doi|10.2307/1988911}} {{jstor|1988911}}</ref> (not to be confused with [[irreducible polynomial]]s, which cannot be [[Factorization of polynomials|factored into products of polynomials]]).\n\n==Examples==\n\nIn the simplest case, one of the polynomials is a [[monomial]]. For example,\n\n:<math>f = x^6 - 3 x^3 + 1</math>\n\ndecomposes into\n\n:<math>g = x^2 - 3 x + 1 \\text{ and } h = x^3</math>\n\nsince\n\n:<math>f(x) = (g  \\circ h)(x) = g(h(x)) = g(x^3) = (x^3)^2 - 3 (x^3) + 1.</math>\n\nLess trivially,\n\n: <math>\n\\begin{align}\n& x^6-6 x^5+21 x^4-44 x^3+68 x^2-64 x+41 \\\\\n= {} & (x^3+9 x^2+32 x+41) \\circ (x^2-2 x).\n\\end{align}\n</math>\n\n==Uniqueness==\n\nA polynomial may have distinct decompositions into indecomposable polynomials where <math>f = g_1 \\circ g_2 \\circ \\cdots \\circ g_m = h_1 \\circ h_2 \\circ \\cdots\\circ h_n</math> where <math>g_i \\neq h_i</math> for some <math>i</math>. The restriction in the definition to polynomials of degree greater than one excludes the infinitely many decompositions possible with linear polynomials.\n\n[[Joseph Ritt]] proved that <math>m = n</math>, and the degrees of the components are the same, but possibly in different order; this is '''Ritt's polynomial decomposition theorem'''.<ref name=\"ritt\"/><ref>Capi Corrales-Rodrigáñez, \"A note on Ritt's theorem on decomposition of polynomials\", ''Journal of Pure and Applied Algebra'' '''68''':3:293–296 (6 December 1990) {{doi|10.1016/0022-4049(90)90086-W}}</ref> For example, <math>x^2 \\circ x^3 = x^3 \\circ x^2</math>.\n\n==Applications==\n\nA polynomial decomposition may enable more efficient evaluation of a polynomial. For example,\n:<math>\n\\begin{align}\n& x^8 + 4 x^7 + 10 x^6 + 16 x^5 + 19 x^4 + 16 x^3 + 10 x^2 + 4 x - 1 \\\\\n= {} & (x^2 - 2) \\circ (x^2) \\circ (x^2 + x + 1)\n\\end{align}\n</math>\ncan be calculated with only 3 multiplications using the decomposition, while [[Horner's method]] would require 7.\n\nA polynomial decomposition enables calculation of symbolic roots using [[Nth root|radicals]], even for some [[irreducible polynomial]]s. This technique is used in many [[computer algebra systems]].<ref>The examples below were calculated using [[Maxima (software)|Maxima]].</ref> For example, using the decomposition\n\n:<math>\n\\begin{align}\n& x^6 - 6 x^5 + 15 x^4 - 20 x^3 + 15 x^2 - 6 x - 1 \\\\\n= {} & (x^3 - 2) \\circ (x^2 - 2 x + 1),\n\\end{align}\n</math>\n\nthe roots of this irreducible polynomial can be calculated as\n\n:<math>1 \\pm 2^{1/6}, 1 \\pm \\frac{\\sqrt{-1 \\pm \\sqrt{3}i}}{2^{1/3}}</math>.<ref name=\"indep\">Where each ± is taken independently.</ref>\n\nEven in the case of [[quartic polynomial]]s, where there is an explicit formula for the roots, solving using the decomposition often gives a simpler form. For example, the decomposition\n\n:<math>\n\\begin{align}\n& x^4 - 8 x^3 + 18 x^2 - 8 x + 2 \\\\\n= {} & (x^2 + 1) \\circ (x^2 - 4 x + 1)\n\\end{align}\n</math>\n\ngives the roots\n\n:<math> 2 \\pm \\sqrt{3 \\pm i} </math><ref name=\"indep\"/>\n\nbut straightforward application of the [[Quartic function#General formula for roots|quartic formula]] gives equivalent results but in a form that is difficult to [[Symbolic computation#Simplification|simplify]] and difficult to understand:\n\n:<math> -{ \\frac{\\sqrt{{{ 9 \\left(\\frac{8 \\sqrt{10} i}{3^{3/2}} + 72\\right)^{2/3} + 36 \\left(\\frac{8 \\sqrt{10} i}{3^{3/2}} + 72\\right)^{1/3} + 156} \\over {\\left({\\frac{8 \\sqrt{10} i}{3^{3/2}}} + 72\\right)^{1/3}}}}} 6}-{{\\sqrt{-\\left(\\frac{8 \\sqrt{10} i}{3^{3/2}} + 72\\right)^{1/3}-{{52}\\over{3 \\left(\\frac{8 \\sqrt{10} i}{3^{3/2}} +72\\right)^{1/3}}} + 8}}\\over 2} + 2. </math>\n\n==Algorithms==\n\nThe first algorithm for polynomial decomposition was published in 1985,<ref name=\"bz\">David R. Barton, Richard Zippel, \"Polynomial Decomposition Algorithms\", ''Journal of Symbolic Computation'' '''1''':159–168 (1985)</ref> though it had been discovered in 1976<ref name=\"z\">Richard Zippel , \"Functional Decomposition\" (1996) [http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.51.3154 full text]</ref> and implemented in the [[Macsyma]] [[computer algebra system]].<ref>Available in its open-source successor, [[Maxima computer algebra system|Maxima]], see the [http://maxima.sourceforge.net/docs/manual/maxima_14.html#polydecomp ''polydecomp'' function]</ref> That algorithm took worst-case exponential time but worked independently of the [[characteristic (algebra)|characteristic]] of the underlying [[field (algebra)|field]].\n\nMore recent algorithms ran in polynomial time but with restrictions on the characteristic.<ref name=\"kz\">Dexter Kozen, Susan Landau, \"Polynomial Decomposition Algorithms\", ''Journal of Symbolic Computation'' '''7''':445–456 (1989)</ref>\n\nThe most recent algorithm calculates a decomposition in polynomial time and without restrictions on the characteristic.<ref>Raoul Blankertz, \"A polynomial time algorithm for computing all minimal decompositions of a polynomial\", ''ACM Communications in Computer Algebra'' '''48''':1 (Issue 187, March 2014) [http://www.sigsam.org/bulletin/articles/187/Polynomial_time_decomposition_pp13-23.pdf full text] {{webarchive|url=https://web.archive.org/web/20150924101735/http://www.sigsam.org/bulletin/articles/187/Polynomial_time_decomposition_pp13-23.pdf |date=2015-09-24 }}</ref>\n\n==Notes==\n<references/>\n\n==References==\n\n* Joel S. Cohen, \"Polynomial Decomposition\", Chapter 5 of ''Computer Algebra and Symbolic Computation'', 2003, {{isbn|1-56881-159-4}}\n\n[[Category:Polynomials]]\n[[Category:Computer algebra]]"
    },
    {
      "title": "Polynomial expansion",
      "url": "https://en.wikipedia.org/wiki/Polynomial_expansion",
      "text": "{{about||locally approximating a function with a polynomial|Taylor series|graphs with sparse shallow minors|Bounded expansion}}\n\nIn [[mathematics]], an '''expansion''' of a product of sums expresses it as a sum of products by using the fact that multiplication [[distributive property|distributes]] over addition. Expansion of a [[polynomial expression]] can be obtained by repeatedly replacing subexpressions that multiply two other subexpressions, at least one of which is an addition, by the equivalent sum of products, continuing until the expression becomes a sum of (repeated) products. During the expansion, simplifications such as grouping of like terms or cancellations of terms may also be applied. Instead of multiplications, the expansion steps could also involve replacing powers of a sum of terms by the equivalent expression obtained from the [[binomial formula]]; this is a shortened form of what would happen if the power were treated as a repeated multiplication, and expanded repeatedly. It is customary to reintroduce powers in the final result when terms involve products of identical symbols.\n\nSimple examples of polynomial expansions are the well known rules\n:<math>(x+y)^2=x^2+2xy+y^2</math>\n:<math>(x+y)(x-y)=x^2-y^2</math>\nwhen used from left to right. A more general single-step expansion will introduce all products of a term of one of the sums being multiplied with a term of the other:\n:<math>(a+b+c+d)(x+y+z)=ax+ay+az+bx+by+bz+cx+cy+cz+dx+dy+dz</math>\nAn expansion which involves multiple nested rewrite steps is that of working out a [[Horner scheme]] to the (expanded) polynomial it defines, for instance\n:<math>1+x(-3+x(4+x(0+x(-12+x\\cdot 2))))=1-3x+4x^2-12x^4+2x^5</math>.\n\nThe opposite process of trying to write an expanded polynomial as a product is called [[polynomial factorization]].\n\n==Expansion of a polynomial written in factored form==\n[[File:Polynomial expansion.png|thumb|Two expressions can be multiplied by using the commutative law, associative law and distributive law. (To multiply more than 2 expressions, just multiply 2 at a time)]]\n\nTo multiply two factors, each term of the first factor must be multiplied by each term of the other factor. If both factors are [[binomial (polynomial)|binomial]]s, the [[FOIL rule]] can be used, which stands for \"'''F'''irst '''O'''uter '''I'''nner '''L'''ast,\" referring to the terms that are multiplied together.  For example, expanding\n\n: <math>(x+2)(2x-5)\\,</math>\n\nyields\n\n: <math>2x^2-5x+4x-10 = 2x^2-x-10.</math>\n\n==Expansion of (x+y)<sup>n</sup>==\n\n:{{main|Binomial theorem}}\n\nWhen expanding <math>(x+y)^n</math>, a special relationship exists between the coefficients of the terms when written in order of descending powers of ''x'' and ascending powers of ''y''.  The coefficients will be the numbers in the (''n'' + 1)th row of [[Pascal's triangle]].\n\nFor example, when expanding <math>(x+y)^6</math>, the following is obtained:\n:<math>{\\color{red}1}x^6+{\\color{red}6}x^5y+{\\color{red}{15}}x^4y^2+{\\color{red}{20}}x^3y^3+{\\color{red}{15}}x^2y^4+{\\color{red}{6}}xy^5+{\\color{red}1}y^6 \\,</math>\n\n==See also==\n*[[Polynomial factorization]]\n*[[Factorization]]\n*[[Multinomial theorem]]\n\n==External links==\n'''Discussion'''\n*[http://www.math.uakron.edu/~dpstory/tutorial/mptii/lesson05.pdf Review of Algebra: Expansion], [[University of Akron]]\n'''Online tools'''\n*[http://www.quickmath.com/webMathematica3/quickmath/page.jsp?s1=algebra&s2=expand&s3=basic Expand page], quickmath.com\n*[http://www.livephysics.com/ptools/online-calculator.php#ocal Online Calculator with Symbolic Calculations], livephysics.com\n\n{{DEFAULTSORT:Polynomial Expansion}}\n[[Category:Polynomials]]\n\n[[de:Ausmultiplizieren]]"
    },
    {
      "title": "Polynomial greatest common divisor",
      "url": "https://en.wikipedia.org/wiki/Polynomial_greatest_common_divisor",
      "text": "{{Use American English|date = March 2019}}\n{{Short description|Greatest common divisor of polynomials}}\n{{refimprove|date=September 2012}}\nIn algebra, the '''greatest common divisor''' (frequently abbreviated as GCD) of two [[polynomial]]s is a polynomial, of the highest possible degree, that is a [[factorization|factor]] of both the two original polynomials. This concept is analogous to the [[greatest common divisor]] of two integers.\n\nIn the important case of [[univariate]] polynomials over a [[field (mathematics)|field]] the polynomial GCD may be computed, like for the integer GCD, by [[Euclid's algorithm]] using [[polynomial long division|long division]]. The polynomial GCD is defined only [[up to]] the multiplication by an invertible constant.\n\nThe similarity between the integer GCD and the polynomial GCD allows us to extend to univariate polynomials all the properties that may be deduced from Euclid's algorithm and [[Euclidean division]]. Moreover, the polynomial GCD has specific properties that make it a fundamental notion in various areas of algebra. Typically, the [[root of a function|root]]s of the GCD of two polynomials are the common roots of the two polynomials, and this allows us to get information on the roots without computing them. For example, the [[multiple root]]s of a polynomial are the roots of the GCD of the polynomial and its derivative, and further GCD computations allow us to compute the [[square-free factorization]] of the polynomial, which provides polynomials whose roots are the roots of a given [[multiplicity (mathematics)|multiplicity]].\n\nThe greatest common divisor may be defined and exists, more generally, for [[multivariate polynomial]]s over a field or the ring of integers, and also over a [[unique factorization domain]]. There exist algorithms to compute them as soon as one has a GCD algorithm in the ring of coefficients. These algorithms proceed by a [[recursion]] on the number of variables to reduce the problem to a variant of Euclid's algorithm. They are a fundamental tool in [[computer algebra]], because [[computer algebra system]]s use them systematically to simplify fractions. Conversely, most of the modern theory of polynomial GCD has been developed to satisfy the need of efficiency of computer algebra systems.\n\n==General definition==\nLet ''p'' and ''q'' be polynomials with [[coefficients]] in an [[integral domain]] ''F'', typically a [[field (mathematics)|field]] or the integers.  \nA '''greatest common divisor''' of ''p'' and ''q'' is a polynomial ''d'' that divides ''p'' and ''q'' and such that every common divisor of ''p'' and ''q'' also divides ''d''. Every pair of polynomials (not both zero) has a GCD if and only  if ''F'' is a [[unique factorization domain]].\n\nIf ''F'' is a field and ''p'' and ''q'' are not both zero, for ''d'' to be a greatest common divisor it is sufficient that it divides both ''p'' and ''q'' and it has the greatest degree among the polynomials having this property. If ''p'' = ''q'' = 0, the GCD is 0. However, some authors consider that it is not defined in this case.\n\nThe greatest common divisor of ''p'' and ''q'' is usually denoted \"gcd(''p'', ''q'')\".\n\nThe greatest common divisor is not unique: if ''d'' is a GCD of ''p'' and ''q'', then the polynomial ''f'' is another GCD if and only if there is an invertible element ''u'' of ''F'' such that \n:<math>f=u d</math>\nand \n:<math>d=u^{-1} f</math>.\nIn other words, the GCD is unique up to the multiplication by an invertible constant.\n\nIn the case of the integers, this indetermination has been settled by choosing, as the GCD, the unique one which is positive (there is another one, which is its opposite). With this convention, the GCD of two integers is also the greatest (for the usual ordering) common divisor. However, since there is no natural [[total order]] for polynomials over an integral domain, one cannot proceed in the same way here. For [[univariate]] polynomials over a field, one can additionally require the GCD to be [[monic polynomial|monic]] (i.e. it has 1 as coefficient of the highest degree), but in more general cases there is no general convention. Therefore, equalities like ''d'' = gcd(''p'', ''q'') or gcd(''p'', ''q'') = gcd(''r'', ''s'') are usual abuses of notation which should be read \"''d'' is a GCD of ''p'' and ''q''\" and \"''p'', ''q'' has the same set of GCD as ''r'', ''s''\". In particular, gcd(''p'', ''q'') = 1 means that the invertible constants are the only common divisors, and thus that ''p'' and ''q'' are [[coprime]].\n<!-- This example seems not really useful. In any case, it should be placed in another section\n\nThese hypotheses for the ring of the coefficients are necessary.  For example, suppose we allow ''F''&nbsp;=&nbsp;'''Z'''/6'''Z''', which is not a field. Then we have\n\n: <math>x(x + 3) = x^2 + 3x,\\ </math>\n\n: <math>x(x + 1) = x^2 + x,\\ </math>\n\nand\n\n: <math>(x + 3)(x + 4) = x^2 + 7x + 12 = x^2 + x,\\ </math>\n\nafter reduction modulo six (see [[modular arithmetic]]). This shows that ''x'' and <math>x+3</math> would both satisfy the definition of\n\n: <math>\\gcd(x^2 + x, x^2 + 3x).\\ </math>\n-->\n\n==Properties==\n\n*As stated above, the GCD of two polynomials exists if the coefficients belong either to a field, the ring of the integers or more generally to a [[unique factorization domain]].\n*If ''c'' is any common divisor of ''p'' and ''q'', then ''c'' divides their GCD.\n*<math>\\gcd(p,q)= \\gcd(q,p).</math>\n*<math>\\gcd(p, q)= \\gcd(q,p+rq)</math> for any polynomial ''r''. This property is at the basis of the proof of Euclid's algorithm.\n*For any invertible element ''k'' of the ring of the coefficients, <math>\\gcd(p,q)=\\gcd(p,kq)</math>.\n*Hence <math>\\gcd(p,q)=\\gcd(a_1p+b_1q,a_2p+b_2q)</math> for any scalars <math>a_1, b_1, a_2, b_2</math> such that <math>a_1 b_2 - a_2 b_1</math> is invertible.\n*If <math>\\gcd(p, r)=1</math>, then <math>\\gcd(p, q)=\\gcd(p, qr)</math>.\n*If <math>\\gcd(q, r)=1</math>, then <math>\\gcd(p, qr)=\\gcd(p, q)\\,\\gcd(p, r)</math>.\n*For two univariate polynomials ''p'' and ''q'' over a field, there exist polynomials ''a'' and ''b'', such that <math>\\gcd(p,q)=ap+bq</math> and <math>\\gcd(p,q)</math> divides every such linear combination of ''p'' and ''q'' ([[Bézout's identity]]).\n*The greatest common divisor of three or more polynomials may be defined similarly as for two polynomials. It may be computed recursively from GCD's of two polynomials by the identities:\n::<math>\\gcd(p, q, r) = \\gcd(p, \\gcd(q, r)),</math>\n: and\n::<math>\\gcd(p_1, p_2, \\dots , p_n) = \\gcd( p_1, \\gcd(p_2, \\dots , p_n)).</math>\n\n==GCD by hand computation==\nThere are several ways to find the greatest common divisor of two polynomials. Two of them are:\n\n#''[[Factorization of polynomials]]'', in which one finds the factors of each expression, then selects the set of common factors held by all from within each set of factors. This method may be useful only in simple cases, as factoring is usually more difficult than computing the greatest common divisor.\n#The ''[[Euclidean algorithm]]'', which can be used to find the GCD of two polynomials in the same manner as for two numbers.\n\n===Factoring===\nTo find the GCD of two polynomials using factoring, simply factor the two polynomials completely.  Then, take the product of all common factors.  At this stage, we do not necessarily have a monic polynomial, so finally multiply this by a constant to make it a monic polynomial.  This will be the GCD of the two polynomials as it includes all common divisors and is monic.\n\nExample one: Find the GCD of {{math|''x''<sup>2</sup> + 7''x'' + 6}} and {{math|''x''<sup>2</sup> − 5''x'' − 6}}.\n\n:{{math|1=''x''<sup>2</sup> + 7''x'' + 6 = (''x'' + 1)(''x'' + 6)}}\n:{{math|1=''x''<sup>2</sup> − 5''x'' − 6 = (''x'' + 1)(''x'' − 6)}}\n\nThus, their GCD is {{math|''x'' + 1}}.\n\n===Euclidean algorithm===\nFactoring polynomials can be difficult, especially if the polynomials have large degree. The [[Euclidean algorithm]] is a method which works for any pair of polynomials. It makes repeated use of [[Euclidean division]]. When using this algorithm on two numbers, the size of the numbers decreases at each stage. With polynomials, the degree of the polynomials decreases at each stage. The last nonzero remainder, made monic if necessary, is the GCD of the two polynomials.\n\nMore specifically, assume we wish to find the gcd of two polynomials {{math|''a''(''x'')}} and {{math|''b''(''x'')}}, where we can suppose \n:<math>\\deg(b(x)) \\le \\deg(a(x)) \\,.</math>\n\nWe can find two polynomials {{math|''q''(''x'')}} and {{math|''r''(''x'')}} which satisfy (see [[Polynomial long division]])\n\n* <math>a(x) = q_0(x)b(x) + r_0(x)</math>\n* <math>\\deg(r_0(x)) < \\deg(b(x))</math>\n\nThe polynomial {{math|''q''<sub>0</sub>(''x'')}} is called the quotient and {{math|''r''<sub>0</sub>(''x'')}} is the remainder. Notice that a polynomial {{math|''g''(''x'')}} divides {{math|''a''(''x'')}} and {{math|''b''(''x'')}} if and only if it divides {{math|''b''(''x'')}} and {{math|''r''<sub>0</sub>(''x'')}}. We deduce \n:<math>\\gcd(a(x), b(x)) = \\gcd(b(x), r_0(x)) \\,.</math>\nThen set \n:<math>a_1(x) = b(x), b_1(x) = r_0(x)\\,.</math>\nRepeat the [[polynomial long division]] to get new polynomials {{math|''q''<sub>1</sub>(''x''), ''r''<sub>1</sub>(''x''), ''a''<sub>2</sub>(''x''), ''b''<sub>2</sub>(''x'')}} and so on. At each stage we have \n:<math>\\deg(a_{k+1})+\\deg(b_{k+1}) < \\deg(a_{k})+\\deg(b_{k})\\ ,</math>\nso the sequence will eventually reach a point at which \n:<math>b_N(x) = 0</math>\nand we will have found our GCD: \n:<math>\\gcd(a,b)=\\gcd(a_1,b_1)=\\cdots=\\gcd(a_N, 0)=a_N \\,.</math>\n\nExample: Find the GCD of {{math|1=<span style=\"color:#0000ff\">''x''<sup>2</sup> + 7''x'' + 6</span>}} and {{math|1=<span style=\"color:#990000\">''x''<sup>2</sup> − 5''x'' − 6</span>}}.\n\n: {{math|1=<span style=\"color:#0000ff\">''x''<sup>2</sup> + 7''x'' + 6</span> = <span style=\"color:#990000\">(''x''<sup>2</sup> − 5''x'' − 6)</span>(1) + <span style=\"color:#009900\">(''x'' + 1)</span>(12)}}\n\n: {{math|1=<span style=\"color:#990000\">''x''<sup>2</sup> − 5''x'' − 6</span> = <span style=\"color:#009900\">(''x'' + 1)</span>(''x'' − 6) + 0}}\n\nSince {{math|1=<span style=\"color:#009900\">''x'' + 1</span>}} is the last nonzero remainder, the GCD of these polynomials is {{math|1=''x''&nbsp;+&nbsp;1}}.\n\nThis method works only if one may test the equality to zero of the elements of the field of the coefficients, so one needs a description of the coefficients as elements of some finitely generated field, for which the generators and relations are known exactly. If the coefficients are floating point numbers, known only approximately, then one uses completely different techniques, usually based on [[singular value decomposition|SVD]].\n\nThis induces a new difficulty: For all these fields except the finite ones, the coefficients are fractions. If the fractions are not simplified during the computation, the size of the coefficients grows exponentially during the computation, which makes it impossible except for very small degrees. On the other hand, it is highly time consuming to simplify the fractions immediately. Therefore, two different alternative methods have been introduced (see below):\n* Pseudo-remainder sequences, especially subresultant sequences\n* Modular GCD algorithm using [[modular arithmetic]]\n\n==Univariate polynomials with coefficients in a field==\n\nThe case of univariate polynomials over a field is specially important for several reasons. Firstly, it is the most elementary case and therefore appear in most first courses in algebra. Secondly, it is very similar to the case of the integers, and this analogy is the source of the notion of [[Euclidean domain]]. A third reason is that the theory and the algorithms for the [[multivariate polynomial|multivariate]] case and for coefficients in a [[unique factorization domain]] are strongly based on this particular case. Last but not least, polynomial GCD algorithms and derived algorithms allow one to get useful information on the roots of a polynomial, without computing them.\n\n===Euclidean division===\nEuclidean division of polynomials, which is used in [[#Euclid's algorithm|Euclid's algorithm]] for computing  GCDs, is very similar to [[Euclidean division]] of integers. Its existence is based on the following theorem: Given two univariate polynomials ''a'' and ''b'' ≠ 0 defined over a field, there exist two polynomials ''q'' (the ''quotient'') and ''r'' (the ''remainder'')  which satisfy\n:<math>a=bq+r</math>\nand\n:<math>\\deg(r)<\\deg(b),</math>\nwhere \"deg(...)\" denotes the degree and the degree of the zero polynomial is defined as being negative. Moreover, ''q'' and ''r'' are uniquely defined by these relations.\n\nThe difference from Euclidean division of the integers is that, for the integers, the degree is replaced by the absolute value, and that to have uniqueness one has to suppose that ''r'' is non-negative. The rings for which such a theorem exists are called [[Euclidean domain]]s.\n\nLike for the integers, the Euclidean division of the polynomials may be computed by the [[polynomial long division|long division]] algorithm. This algorithm is usually presented for paper-and-pencil computation, but it works well on computers, when formalized as follows (note that the names of the variables correspond exactly to the regions of the paper sheet in a pencil-and-paper computation of long division). In the following computation \"deg\" stands for the degree of its argument (with the convention {{nowrap|deg(0) &lt; 0}}), and \"lc\" stands for the leading coefficient, the coefficient of the highest degree of the variable.\n\n{{pre2|1=\n'''Euclidean division'''\n\n'''''Input:''''' ''a'' and ''b'' ≠ 0 two polynomials in the variable ''x'';\n'''''Output:''''' ''q'', the quotient, and ''r'', the remainder;\n\n'''''Begin'''''\n    ''q'' := 0\n    ''r'' := ''a''\n    ''d'' := deg(''b'')\n    ''c'' := lc(''b'')\n    '''''while''''' deg(''r'') ≥ ''d'' '''''do'''''\n        ''s'' := {{sfrac|lc(''r'')|''c''}} ''x''{{sup|deg(''r'')−''d''}}\n        ''q'' := ''q'' + ''s''\n        ''r'' := ''r'' − ''sb''\n    '''''end do'''''\n    '''''return''''' (''q'', ''r'')\n'''''end.'''''\n}}\n\nThe proof of the validity of this algorithm relies on the fact that during the whole \"while\" loop, we have {{math|1=''a'' = ''bq'' + ''r''}} and {{math|deg(''r'')}} is a non-negative integer that decreases at each iteration. Thus the proof of the validity of this algorithm also proves the validity of Euclidean division.\n\n===Euclid's algorithm===\nAs for the integers, Euclidean division allows us to define [[Euclid's algorithm]] for computing  GCDs.\n\nStarting from two polynomials ''a'' and ''b'', Euclid's algorithm consists of recursively replacing the pair {{math|(''a'', ''b'')}} by {{math|(''b'', rem(''a'', ''b''))}} (where \"{{math|rem(''a'', ''b'')}}\" denotes the remainder of the Euclidean division, computed by the algorithm of the preceding section), until ''b'' = 0. The GCD is the last non zero remainder.\n\nEuclid's algorithm may be formalized in the recursive programming style as:\n{{quotation|\n<math>\\gcd(a,b):= \\text{if } b=0 \\text{ then } a \\text{ else } \\gcd(b, \\operatorname{rem}(a,b)).</math>\n}}\nIn the imperative programming style, the same algorithm becomes, giving a name to each intermediate remainder:\n{{pre2|\n{{nowrap|<math>\\begin{align} r_0:=a\\\\ r_1:=b \\end{align}</math>}}\n\n{{nowrap|'''''for''''' (<math>i:=1</math>; <math>r_i \\neq 0</math>; <math>i:=i+1</math>) '''''do'''''}}\n  {{nowrap|<math>\\begin{align} r_{i+1}&:=\\operatorname{rem}(r_{i-1},r_i) \\end{align}</math>}}\n'''''end do'''''\n\n{{nowrap|'''''return''''' <math>(r_{i-1}).</math>}}\n}}\n\nThe sequence of the degrees of the {{math|''r<sub>i</sub>''}} is strictly decreasing. Thus after, at most, {{math|deg(''b'')}} steps, one get a null remainder, say {{math|''r<sub>k</sub>''}}. As {{math|(''a'', ''b'')}} and {{math|(''b'', rem(''a'',''b''))}} have the same divisors, the set of the common divisors is not changed by Euclid's algorithm and thus all pairs {{math|(''r<sub>i</sub>'', ''r''<sub>''i''+1</sub>)}} have the same set of common divisors. The common divisors of {{mvar|a}} and {{mvar|b}} are thus the common divisors of {{math|''r''<sub>''k''−1</sub>}} and 0. Thus {{math|''r''<sub>''k''−1</sub>}} is a GCD of {{mvar|a}} and {{mvar|b}}.\nThis not only proves that Euclid's algorithm computes GCDs, but also proves that GCDs exist.\n\n===Bézout's identity and extended GCD algorithm===\n\n[[Bézout's identity]] is a GCD related theorem, initially proved for the integers, which is valid for every [[principal ideal domain]]. In the case of the univariate polynomials over a field, it may be stated as follows.\n\n{{quotation|\nIf {{mvar|g}} is the greatest common divisor of two polynomials {{mvar|a}} and {{mvar|b}} (not both zero), then there are two polynomials {{mvar|u}} and {{mvar|v}} such that\n:<math>au+bv=g\\quad</math> (Bézout's identity)\nand either {{math|1=''u'' = 1, ''v'' = 0}}, or {{math|1=''u'' = 0, ''v'' = 1}}, or \n:<math>\\deg(u)<\\deg(b)-\\deg(g), \\quad \\deg(v)<\\deg(a)-\\deg(g).</math>\n}}\n\nThe interest of this result in the case of the polynomials is that there is an efficient algorithm to compute the polynomials {{mvar|u}} and {{mvar|v}}, This algorithm differs from Euclid's algorithm by a few more computations done at each iteration of the loop. It is therefore called '''extended GCD algorithm'''. Another difference with Euclid's algorithm is that it also uses the quotient, denoted \"quo\", of the Euclidean division instead of only the remainder. This algorithm works as follows.\n\n{{pre2|1=\n'''Extended GCD''' algorithm\n\n'''''Input:''''' {{mvar|a}}, {{mvar|b}}, univariate polynomials\n\n'''''Output:'''''\n  {{mvar|g}}, the GCD of {{mvar|a}} and {{mvar|b}}\n  {{mvar|u}}, {{mvar|v}}, as in above statement\n  {{math|''a''<sub>1</sub>}}, {{math|''b''<sub>1</sub>}}, such that\n    {{nowrap|<math>\\begin{align}\n    a=ga_1\\\\\n    b=gb_1\n    \\end{align}</math>}}\n'''''Begin'''''\n  {{nowrap|<math>\\begin{array}{ll}\n  r_0  =a & r_1  =b\\\\\n  s_0  =1 & s_1  =0\\\\\n  t_0  =0 & t_1  =1\n  \\end{array}</math>}}\n  \n  '''''for''''' ({{math|1=''i'' = 1}}; {{math|''r<sub>i</sub>'' ≠ 0}}; {{math|1=''i'' = ''i''+1}}) '''''do'''''\n    {{nowrap|<math>q  =\\operatorname{quo}(r_{i-1},r_{i})</math>}}\n    {{nowrap|<math>\\begin{align}\n    r_{i+1} & =r_{i-1}-qr_{i}\\\\\n    s_{i+1} & =s_{i-1}-qs_{i}\\\\\n    t_{i+1} & =t_{i-1}-qt_{i}\n    \\end{align}</math>}}\n  '''''end do'''''\n  {{nowrap|<math>\\begin{align}\n  g & =r_{i-1}\\\\\n  u & =s_{i-1}\\\\\n  v & =t_{i-1}\n  \\end{align}</math>}}\n  {{nowrap|<math>\\begin{align}\n  a_1 & =(-1)^{i-1}t_i\\\\\n  b_1 & =(-1)^i s_i\n  \\end{align}</math>}}\n'''''end.'''''\n}}\n\nThe proof that the algorithm satisfies its output specification relies on the fact that, for every {{mvar|i}} we have \n:<math>r_i=as_i+bt_i</math>\n:<math>s_it_{i+1}-t_is_{i+1}=s_it_{i-1}-t_is_{i-1},</math>\nthe latter equality implying \n:<math>s_it_{i+1}-t_is_{i+1}=(-1)^i.</math>\nThe assertion on the degrees follows from the fact that, at every iteration, the degrees of {{math|''s<sub>i</sub>''}} and {{math|''t<sub>i</sub>''}} increase at most as the degree of {{math|''r<sub>i</sub>''}} decreases.\n\nAn interesting feature of this algorithm is that, when the coefficients of Bezout's identity are needed, one gets for free the quotient of the input polynomials by their GCD.\n\n====Arithmetic of algebraic extensions====\n\nAn important application of extended GCD algorithm is that it allows one to compute division in [[algebraic extension|algebraic field extension]]s.\n\nLet {{mvar|L}} an algebraic extension of a field {{mvar|K}}, generated by an element whose minimal polynomial {{mvar|f}} has degree {{mvar|n}}. The elements of {{mvar|L}} are usually represented by univariate polynomials over {{mvar|K}} of degree less than {{mvar|n}}.\n\nThe addition in {{mvar|L}} is simply the addition of polynomials:\n:<math>a+_Lb=a+_{K[X]}b.</math>\n\nThe multiplication in {{mvar|L}} is the multiplication of polynomials followed by the division by {{mvar|f}}:\n:<math>a\\cdot_Lb=\\operatorname{rem}(a._{K[X]}b,f).</math>\n\nThe inverse of a non zero element {{mvar|a}} of {{mvar|L}} is the coefficient {{mvar|u}} in Bézout's identity {{math|1=''au'' + ''fv'' = 1}}, which may be computed by extended GCD algorithm. (the GCD is 1 because the minimal polynomial {{mvar|f}} is irreducible). The degrees inequality in the specification of extended GCD algorithm shows that a further division by {{mvar|f}} is not needed to get deg({{mvar|u}}) < deg({{mvar|f}}).\n\n===Subresultants===<!-- [[subresultant]] links here -->\n\nIn the case of univariate polynomials, there is a strong relationship between greatest common divisors and [[resultant]]s. In fact the resultant of two polynomials ''P'', ''Q'' is a polynomial function of the coefficients of ''P'' and ''Q'' which has the value zero if and only if the GCD of ''P'' and ''Q'' is not constant.\n\nThe subresultants theory is a generalization of this property that allows characterizing generically the GCD of two polynomials, and the resultant is the 0-th subresultant polynomial.<ref>\n{{cite book\n | author = Saugata Basu |author2=Richard Pollack |author3=Marie-Françoise Roy\n | year = 2006\n | title = Algorithms in real algebraic geometry, chapter 4.2\n | publisher = [[Springer Science+Business Media|Springer-Verlag]]\n | url = http://perso.univ-rennes1.fr/marie-francoise.roy/bpr-ed2-posted1.html\n}}</ref>\n\nThe ''i''-th ''subresultant polynomial'' ''S<sub>i</sub>''(''P'' ,''Q'') of two polynomials ''P'' and ''Q'' is a polynomial of degree at most ''i'' whose coefficients are polynomial functions of the coefficients of ''P'' and ''Q'', and the ''i''-th ''principal subresultant coefficient'' ''s<sub>i</sub>''(''P'' ,''Q'') is the coefficient of degree ''i'' of ''S<sub>i</sub>''(''P'', ''Q''). They have the property that the GCD of ''P'' and ''Q'' has a degree ''d'' if and only if \n:<math>s_0(P,Q)=\\cdots=s_{d-1}(P,Q) =0 \\ , s_d(P,Q)\\neq 0</math>.\n\nIn this case, ''S<sub>d</sub>''(''P'' ,''Q'') is a GCD of ''P'' and ''Q'' and\n\n:<math>S_0(P,Q)=\\cdots=S_{d-1}(P,Q) =0.</math>\n\nEvery coefficient of the subresultant polynomials is defined as the determinant of a submatrix of the [[Sylvester matrix]] of ''P'' and ''Q''. This implies that subresultants \"specialize\" well. More precisely, subresultants are defined for polynomials over any commutative ring ''R'', and have the following property.\n\nLet ''φ'' be a ring homomorphism of ''R'' into another commutative ring ''S''. It extends to another homomorphism, denoted also ''φ'' between the polynomials rings over ''R'' and ''S''. Then, if ''P'' and ''Q'' are univariate polynomials with coefficients in ''R'' such that \n:<math>\\deg(P)=\\deg(\\varphi(P))</math>\nand \n:<math>\\deg(Q)=\\deg(\\varphi(Q)),</math>\nthen the subresultant polynomials and the principal subresultant coefficients of ''φ''(''P'') and ''φ''(''Q'') are the image by ''φ'' of those of ''P'' and ''Q''.\n\nThe subresultants have two important properties which make them fundamental for the computation on computers of the GCD of two polynomials with integer coefficients.\nFirstly, their definition through determinants allows bounding, through [[Hadamard inequality]], the size of the coefficients of the GCD.\nSecondly, this bound and the property of good specialization allow to compute the GCD of two polynomials with integer coefficients through [[modular arithmetic|modular computation]] and [[Chinese remainder theorem]] (see [[#Modular GCD algorithm|below]]).\n\n====Technical definition====\nLet \n:<math>P=p_0+p_1 X+\\cdots +p_m X^m,\\quad Q=q_0+q_1 X+\\cdots +q_n X^n.</math>\nbe two univariate polynomials with coefficients in a field ''K''. Let us denote by <math>\\mathcal{P}_i</math> the ''K'' vector space of dimension ''i'' the polynomials of degree less than ''i''. For non-negative integer ''i'' such that ''i'' ≤ ''m'' and ''i'' ≤ ''n'', let \n:<math>\\varphi_i:\\mathcal{P}_{n-i} \\times \\mathcal{P}_{m-i} \\rightarrow \\mathcal{P}_{m+n-i}</math>\nbe the linear map such that\n:<math>\\varphi_i(A,B)=AP+BQ.</math>\n\nThe [[resultant]] of ''P'' and ''Q'' is the determinant of the [[Sylvester matrix]], which is the (square) matrix of <math>\\varphi_0</math> on the bases of the powers of ''X''. Similarly, the ''i''-subresultant polynomial is defined in term of determinants of submatrices of the matrix of <math>\\varphi_i.</math>\n\nLet us describe these matrices more precisely;\n\nLet ''p''<sub>''i''</sub> = 0 for ''i'' < 0 or ''i'' > ''m'', and ''q''<sub>''i''</sub> = 0 for ''i'' < 0 or ''i'' > ''n''. The [[Sylvester matrix]] is the (''m'' + ''n'') × (''m'' + ''n'')-matrix such that the coefficient of the ''i''-th row and the ''j''-th column is ''p''<sub>''m''+''j''−''i''</sub> for ''j'' ≤ ''n'' and ''q''<sub>''j''−''i''</sub> for ''j'' > ''n'':<ref>Many author define the Sylvester matrix as the transpose of ''S''. This breaks the usual convention for writing the matrix of a linear map.</ref>\n\n:<math>S=\\begin{pmatrix} \np_m     & 0       & \\cdots & 0       & q_n     & 0       & \\cdots & 0       \\\\\np_{m-1} & p_m     & \\cdots & 0       & q_{n-1} & q_n     & \\cdots & 0  \\\\\np_{m-2} & p_{m-1} & \\ddots & 0       & q_{n-2} & q_{n-1} & \\ddots & 0 \\\\\n\\vdots  &\\vdots   & \\ddots & p_m     & \\vdots  &\\vdots   & \\ddots & q_n  \\\\\n\\vdots  &\\vdots   & \\cdots & p_{m-1} & \\vdots  &\\vdots   & \\cdots & q_{n-1}\\\\\np_0     & p_1     & \\cdots & \\vdots  & q_0     & q_1     & \\cdots & \\vdots\\\\\n0       & p_0     & \\ddots &  \\vdots & 0       & q_0     & \\ddots &  \\vdots & \\\\\n\\vdots  & \\vdots  & \\ddots & p_1     & \\vdots  & \\vdots  & \\ddots & q_1   \\\\\n0       & 0       & \\cdots & p_0     & 0       & 0       & \\cdots & q_0   \n\\end{pmatrix}.</math>\n\nThe matrix ''T<sub>i</sub>'' of <math>\\varphi_i</math> is the (''m'' + ''n'' − ''i'') × (''m'' + ''n'' − 2''i'')-submatrix of ''S'' which is obtained by removing the last ''i'' rows of zeros in the submatrix of the columns 1 to ''n'' − ''i'' and ''n'' + 1 to ''m'' + ''n'' − ''i'' of ''S'' (that is removing ''i'' columns in each block and the ''i'' last rows of zeros). The ''principal subresultant coefficient'' ''s<sub>i</sub>'' is the determinant of the ''m'' + ''n'' − 2''i'' first rows of ''T<sub>i</sub>''.\n\nLet ''V<sub>i</sub>'' be the (''m'' + ''n'' − 2''i'') × (''m'' + ''n'' − ''i'') matrix defined as follows. First we add (''i'' + 1) columns of zeros to the right of the (''m'' + ''n'' − 2''i'' − 1) × (''m'' + ''n'' − 2''i'' − 1) [[identity matrix]]. Then we border the bottom of the resulting matrix by a row consisting in (''m'' + ''n'' − ''i'' − 1) zeros followed by ''X<sup>i</sup>'', ''X''<sup>''i''−1</sup>, ..., ''X'', 1:\n:<math>V_i=\\begin{pmatrix} \n1       & 0       & \\cdots & 0       & 0       & 0      & \\cdots & 0 \\\\\n0       & 1       & \\cdots & 0       & 0       & 0      & \\cdots & 0 \\\\\n\\vdots  &\\vdots   & \\ddots & \\vdots  & \\vdots  &\\ddots  & \\vdots & 0 \\\\\n0       & 0       & \\cdots & 1       & 0       & 0      & \\cdots & 0 \\\\\n0       & 0       & \\cdots & 0       & X^i     & X^{i-1}& \\cdots & 1 \n\\end{pmatrix}.</math>\n\nWith this notation, the ''i''-th ''subresultant polynomial'' is the determinant of the matrix product ''V<sub>i</sub>T<sub>i</sub>''. Its coefficient of degree ''j'' is the determinant of the square submatrix of ''T<sub>i</sub>'' consisting in its ''m'' + ''n'' − 2''i'' − 1 first rows and the  (''m'' + ''n'' − ''i'' − ''j'')-th row.\n\n====Sketch of the proof====\nIt is not obvious that, as defined, the subresultants have the desired properties. In fact the proof is rather simple if the properties of linear algebra and those of polynomials are put together.\n\nAs defined, the columns of the matrix ''T<sub>i</sub>'' are the vectors of the coefficients of some polynomials belonging to the image of <math>\\varphi_i</math>. The definition of the ''i''-th subresultant polynomial ''S<sub>i</sub>'' shows that the vector of its coefficients is a linear combination of these column vectors, and thus that ''S<sub>i</sub>'' belongs to the image of <math>\\varphi_i.</math>\n\nIf the degree of the GCD is greater than ''i'', then [[Bézout's identity]] shows that every non zero polynomial in the image of <math>\\varphi_i</math> has a degree larger than ''i''. This implies that ''S<sub>i</sub>''=0.\n\nIf, on the other hand, the degree of the GCD is ''i'', then Bézout's identity again allows proving that the multiples of the GCD that have a degree lower than ''m'' + ''n'' − ''i'' are in the image of <math>\\varphi_i</math>. The vector space of these multiples has the dimension ''m'' + ''n'' − 2''i'' and has a base of polynomials of pairwise different degrees, not smaller than ''i''. This implies that the submatrix of the ''m'' + ''n'' − 2''i'' first rows of the column echelon form of ''T<sub>i</sub>'' is the identity matrix and thus that ''s<sub>i</sub>'' is not 0. Thus ''S<sub>i</sub>'' is a polynomial in the image of <math>\\varphi_i</math>, which is a multiple of the GCD and has the same degree. It is thus a greatest common divisor.\n\n===GCD and root finding===\n\n====Square-free factorization====\n{{main|Square-free factorization}}\n\nMost [[root-finding algorithm]]s behave badly with polynomials that have [[multiple root]]s. It is therefore useful to detect and remove them before calling a root-finding algorithm. A GCD computation allows detection of the existence of multiple roots, because the multiple roots of a polynomial are the roots of the GCD of the polynomial and its [[formal derivative|derivative]].\n\nAfter computing the GCD of the polynomial and its derivative, further GCD computations provide the complete ''square-free factorization'' of the polynomial, which is a factorization \n:<math>f=\\prod_{i=1}^{\\deg(f)} f_i^i</math>\nwhere, for each ''i'', the polynomial ''f''<sub>''i''</sub> either is 1 if ''f'' does not have any root of multiplicity ''i'' or is a square-free polynomial (that is a polynomial without multiple root) whose roots are exactly the roots of multiplicity ''i'' of ''f'' (see [[Square-free polynomial#Yun's algorithm|Yun's algorithm]]).\n\nThus the square-free factorization reduces root finding of a polynomial with multiple roots to root finding of several square-free polynomials of lower degree. The square-free factorization is also the first step in most [[polynomial factorization]] algorithms.\n\n====Sturm sequence====\n{{main|Sturm sequence}}\nThe ''Sturm sequence'' of a polynomial with real coefficients is the sequence of the remainders provided by a variant of Euclid's algorithm applied to the polynomial and its derivative. For getting the Sturm sequence, one simply replaces the instruction \n:<math>r_{i+1}:=\\operatorname{rem}(r_{i-1},r_{i})</math>\nof Euclid's algorithm by \n:<math>r_{i+1}:=-\\operatorname{rem}(r_{i-1},r_{i}).</math>\n\nLet ''V''(''a'') be the number of changes of signs in the sequence, when evaluated at a point ''a''. Sturm's theorem asserts that ''V''(''a'') − ''V''(''b'') is the number of real roots of the polynomial in the interval [''a'', ''b'']. Thus the Sturm sequence allows computing the number of real roots in a given interval. By subdividing the interval until every subinterval contains at most one root, this provides an algorithm that  locates the real roots in intervals of arbitrary small length.\n\n==GCD over a ring and over its field of fractions==\n\nIn this section, we consider polynomials over a [[unique factorization domain]] ''R'', typically the ring of the integers, and over its [[field of fractions]] ''F'', typically the field of the rational numbers, and we denote ''R''[''X''] and ''F''[''X''] the rings of polynomials in a set of variables over these rings.\n\n===Primitive part–content factorization===\n{{see also|Content (algebra)|Gauss's lemma (polynomial)}}\n\nThe ''content'' of a polynomial ''p'' ∈ ''R''[''X''], denoted \"cont(''p'')\",  is the GCD of its coefficients. A polynomial ''q'' ∈ ''F''[''X''] may be written\n\n:<math>q = \\frac{p}{c}</math>\nwhere ''p'' ∈ ''R''[''X''] and ''c'' ∈ ''R'': it suffices to take for ''c'' a multiple of all denominators of the coefficients of ''q'' (for example their product) and ''p'' = ''cq''. The ''content'' of ''q'' is defined as:\n:<math>\\operatorname{cont} (q) =\\frac{\\operatorname{cont} (p)}{c}.</math>\nIn both cases, the content is defined up to the multiplication by a [[unit (ring theory)|unit]] of ''R''.\n\nThe ''primitive part'' of a polynomial in ''R''[''X''] or ''F''[''X''] is defined by \n:<math>\\operatorname{primpart} (p) =\\frac{p}{\\operatorname{cont} (p)}.</math>\n\nIn both cases, it is a polynomial in ''R''[''X''] that is ''primitive'', which means that 1 is a GCD of its coefficients.\n\nThus every polynomial in ''R''[''X''] or ''F''[''X''] may be factorized as \n:<math>p =\\operatorname{cont} (p)\\,\\operatorname{primpart} (p),</math>\nand this factorization is unique up to the multiplication of the content by a unit of ''R'' and of the primitive part by the inverse of this unit.\n\nGauss's lemma implies that the product of two primitive polynomials is primitive. It follows that \n:<math>\\operatorname{primpart} (pq)=\\operatorname{primpart} (p) \\operatorname{primpart}(q)</math>\nand\n:<math>\\operatorname{cont} (pq)=\\operatorname{cont} (p) \\operatorname{cont}(q).</math>\n\n===Relation between the GCD over ''R'' and over ''F''===\n\nThe relations of the preceding section imply a strong relation between the GCD's in ''R''[''X''] and in ''F''[''X'']. In order to avoid ambiguities, the notation \"''gcd''\" will be indexed, in the following, by the ring in which the GCD is computed.\n\nIf ''q''<sub>1</sub> and ''q''<sub>2</sub> belong to ''F''[''X''], then\n:<math>\\operatorname{primpart}(\\gcd_{F[X]}(q_1,q_2))=\\gcd_{R[X]}(\\operatorname{primpart}(q_1),\\operatorname{primpart}(q_2)).</math>\n\nIf ''p''<sub>1</sub> and ''p''<sub>2</sub> belong to ''R''[''X''], then\n:<math>\\gcd_{R[X]}(p_1,p_2)=\\gcd_R(\\operatorname{cont}(p_1),\\operatorname{cont}(p_2)) \\gcd_{R[X]}(\\operatorname{primpart}(p_1),\\operatorname{primpart}(p_2)),</math>\nand\n:<math>\\gcd_{R[X]}(\\operatorname{primpart}(p_1),\\operatorname{primpart}(p_2))=\\operatorname{primpart}(\\gcd_{F[X]}(p_1,p_2)).</math>\n\nThus the computation of polynomial GCD's is essentially the same problem over ''F''[''X''] and over ''R''[''X''].\n\nFor univariate polynomials over the rational numbers one may think that Euclid's algorithm is a convenient method for computing the GCD. However, it involves to simplify a large number of fractions of integers, and the resulting algorithm is not efficient. For this reason, methods have been designed to modify Euclid's algorithm for working only with polynomials over the integers. They consist in replacing Euclidean division, which introduces fractions, by a so-called ''pseudo-division'', and replacing the remainder sequence of Euclid's algorithm by so-called ''pseudo-remainder sequences'' (see [[#Pseudo-remainder sequences|below]]).\n\n===Proof that GCD exists for multivariate polynomials===\n\nIn the previous section we have seen that the GCD of polynomials in ''R''[''X''] may be deduced from GCDs in ''R'' and in ''F''[''X'']. A closer look on the proof shows that this allows us to prove the existence of GCDs in ''R''[''X''], if they exist in ''R'' and in ''F''[''X'']. In particular, if GCDs exist in ''R'', and if ''X'' is reduced to one variable, this proves that GCDs exist in ''R''[''X''] (Euclid's algorithm proves the existence of GCDs in ''F''[''X'']).\n\nA polynomial in ''n'' variables may be considered as a univariate polynomial over the ring of polynomials in (''n'' − 1) variables. Thus a recursion on the number of variables shows that if GCDs exists and may be computed in ''R'', then they exist and may be computed in every multivariate polynomial ring over ''R''. In particular, if ''R'' is either the ring of the integers or a field, then GCDs exist in ''R''[''x''<sub>1</sub>,..., ''x<sub>n</sub>''], and what precedes provides an algorithm to compute them.\n\nThe proof that a polynomial ring over a unique factorization domain is also a unique factorization domain is similar, but it does not provide an algorithm, because there is no general algorithm to factor univariate polynomials over a field (there are examples of fields for which there does not exist any factorization algorithm for the univariate polynomials).\n\n==Pseudo-remainder sequences==\n\nIn this section, we consider an [[integral domain]] ''Z'' (typically the ring '''Z''' of the integers) and its field of fractions ''Q'' (typically the field '''Q''' of the rational numbers). Given two polynomials ''A'' and ''B'' in the univariate polynomial ring ''Z''[''X''], the Euclidean division (over ''Q'') of ''A'' by ''B'' provides a quotient and a remainder which may not belong to ''Z''[''X''].\n\nFor, if one applies Euclid's algorithm to the following polynomials <ref>D.E. Knuth, the Art of Computer Programming II, Addison-Wesley, 1969, pp. 370-371</ref>\n:<math>X^8+X^6-3 X^4-3 X^3+8 X^2+2 X-5</math>\nand\n:<math>3 X^6+5 X^4-4 X^2-9 X+21,</math>\nthe successive remainders of Euclid's algorithm are\n:<math>-\\frac{5}{9}X^4+\\frac{1}{9}X^2-\\frac{1}{3},</math>\n:<math>-\\frac{117}{25}X^2-9X+\\frac{441}{25},</math>\n:<math>\\frac{233150}{19773}X-\\frac{102500}{6591},</math>\n:<math>-\\frac{1288744821}{543589225}.</math>\nOne sees that, despite the small degree and the small size of the coefficients of the input polynomials, one has to manipulate and simplify integer fractions of rather large size.\n\n{{anchor|pseudo-remainder|pseudo-division}}\nThe ''pseudo-division'' has been introduced to allow a variant of Euclid's algorithm for which all remainders belong to ''Z''[''X''].\n\nIf <math>\\deg(A)=a</math> and <math>\\deg(B)=b</math> and ''a'' ≥ ''b'', the '''pseudo-remainder''' of the pseudo-division of ''A'' by ''B'', denoted by prem(''A'',''B'') is \n:<math>\\operatorname{prem}(A,B)=\\operatorname{rem}(\\operatorname{lc}(B)^{a-b+1}A,B),</math>\nwhere lc(''B'') is the leading coefficient of ''B'' (the coefficient of ''X''<sup>''b''</sup>).\n\nThe pseudo-remainder of the pseudo-division of two polynomials in ''Z''[''X''] belongs always to ''Z''[''X''].\n\nA '''pseudo-remainder sequence''' is the sequence of the (pseudo) remainders ''r''<sub>''i''</sub> obtained by replacing the instruction \n:<math>r_{i+1}:=\\operatorname{rem}(r_{i-1},r_{i})</math>\nof Euclid's algorithm by \n:<math>r_{i+1}:=\\frac{\\operatorname{prem}(r_{i-1},r_{i})}{\\alpha},</math>\nwhere ''α'' is an element of ''Z'' that divides exactly every coefficient of the numerator. Different choices of ''α'' give different pseudo-remainder sequences, which are described in the next subsections.\n\nAs the common divisors of two polynomials are not changed if the polynomials are multiplied by invertible constants (in ''Q''), the last non zero term in a pseudo-remainder sequence is a GCD (in ''Q''[''X'']) of the input polynomials. Therefore, pseudo-remainder sequences allows computing GCD's in ''Q''[''X''] without introducing fractions in ''Q''.\n\n===Trivial pseudo-remainder sequence===\n\nThe simplest (to define) remainder sequence consists in taking always ''α''=1. In practice, it is not interesting, as the size of the coefficients grow exponentially with the degree of the input polynomials. This appears clearly on the example of the preceding section, for which the successive pseudo-remainders are \n:<math>-15\\, X^4+3\\, X^2-9,</math>\n:<math>15795\\, X^2+30375\\, X-59535,</math>\n:<math>1254542875143750\\, X-1654608338437500,</math>\n:<math>12593338795500743100931141992187500.</math>\nThe number of digits of the coefficients of the successive remainders is more than doubled at each iteration of the algorithm. This is a typical behavior of the trivial pseudo-remainder sequences.\n\n=== Primitive pseudo-remainder sequence===\n\nThe ''primitive pseudo-remainder sequence'' consists in taking for ''α'' the content of the numerator. Thus all the ''r''<sub>''i''</sub> are primitive polynomials.\n\nThe primitive pseudo-remainder sequence is the pseudo-remainder sequence, which generates the smallest coefficients. However it requires to compute a number of GCD's in ''Z'', and therefore is not sufficiently efficient to be used in practice, especially when ''Z'' is itself a polynomial ring.\n\nWith the same input as in the preceding sections, the successive remainders, after division by their content are \n:<math>-5\\,X^4+X^2-3,</math>\n:<math>13\\,X^2+25\\,X-49,</math>\n:<math>4663\\,X-6150,</math>\n:<math>1.</math>\nThe small size of the coefficients hides the fact that a number of integers GCD and divisions by the GCD have been computed.\n\n=== Subresultant pseudo-remainder sequence===\n\nA subresultant sequence can be also computed with pseudo-remainders. The process consists in choosing ''α'' is such a way that every ''r''<sub>''i''</sub> is a subresultant polynomial. Surprisingly, the computation of ''α'' is very easy (see below). On the other hand, the proof of correctness of the algorithm is difficult, because it should take into account all the possibilities for the difference of degrees of two consecutive remainders.\n\nThe coefficients in the subresultant sequence are rarely much larger than those of the primitive pseudo-remainder sequence. As GCD computations in ''Z'' are not needed, the subresultant sequence with pseudo-remainders gives the most efficient computation.\n\nWith the same input as in the preceding sections, the successive remainders are\n:<math>15\\,X^4-3\\,X^2+9,</math>\n:<math>65\\,X^2+125\\,X-245,</math>\n:<math>9326\\,X-12300,</math>\n:<math>260708.</math>\nThe coefficients have a reasonable size. They are obtained without any GCD computation, only exact divisions. This makes this algorithm more efficient than that of primitive pseudo-remainder sequences.\n\nThe algorithm computing the subresultant  sequence with pseudo-remainders is given below. In this algorithm, the input {{math|(''a'', ''b'')}} is a pair of polynomials in ''Z''[X]. The {{math|''r''<sub>''i''</sub>}} are the successive pseudo remainders in ''Z''[X], the variables ''i'' and {{math|''d''<sub>''i''</sub>}} are non negative integers, and the Greek letters denote elements in ''Z''. The functions deg() and rem() denote the degree of a polynomial and the remainder of the Euclidean division. In the algorithm, this remainder is always in ''Z''[X]. Finally the divisions denoted / are always exact and have their result either in ''Z''[X] or in ''Z''.\n\n{{pre2|\n{{nowrap|<math>\\begin{align}r_0:=a\\\\ r_1:=b \\end{align}</math>}}\n{{nowrap|'''''for''''' (<math>i:=1</math>; <math>r_i \\neq 0</math>; <math>i:=i+1;</math>) '''''do'''''}}\n  {{nowrap|<math>\\begin{align} d_{i}&:=\\deg(r_{i-1})-\\deg(r_{i})\\\\ \\gamma_{i}&:=\\operatorname{lc}(r_{i})\n  \\end{align}</math>}}\n  {{nowrap|'''if''' <math>i:=1</math> '''then'''}}\n    {{nowrap|<math>\\begin{align} \\beta_1&:=(-1)^{d_1+1}\\\\ \\psi_1&:=-1 \\end{align}</math>}}\n  '''else'''\n    {{nowrap|<math>\\begin{align} \\beta_i&:=-\\gamma_{i-1}\\psi_i^{d_i}\\\\ \\psi_{i}&:=\\frac{(-\\gamma_i)^{d_{i-1}}}{\\psi_{i-1}^{d_{i-1}-1}} \\end{align}</math>}}\n  '''end if'''\n  {{nowrap|<math>r_{i+1}:=\\frac{\\operatorname{rem}(\\gamma_i^{d_i +1}r_{i-1},r_{i})}{\\beta_i}</math>}}\n'''''end do.'''''\n}}\nNote: \"lc\" stands for the leading coefficient, the coefficient of the highest degree of the variable.\n\nThis algorithm computes not only the greatest common divisor (the last non zero {{math|''r''<sub>''i''</sub>}}), but also all the subresultant polynomials: The remainder {{math|''r''<sub>''i''</sub>}} is the {{math|(deg(''r''<sub>''i''−1</sub>) − 1)}}-th subresultant polynomial. If {{math|deg(''r''<sub>''i''</sub>) < deg(''r''<sub>''i''−1</sub>) − 1}}, the {{math|deg(''r''<sub>''i''</sub>)}}-th subresultant polynomial is {{math|lc(''r''<sub>''i''</sub>)<sup>deg(''r''<sub>''i''−1</sub>)−deg(''r''<sub>''i''</sub>)−1</sup>''r''<sub>''i''</sub>}}. All the other subresultant polynomials are zero.\n\n===Sturm sequence with pseudo-remainders===\n\nOne may use pseudo-remainders for constructing sequences having the same properties as [[Sturm sequence]]s. This requires to control the signs of the successive pseudo-remainders, in order to have the same signs as in the Sturm sequence. This may be done by defining a modified pseudo-remainder as follows.\n\nIf <math>\\deg(A)=a</math> and <math>\\deg(B)=b</math> and ''a'' ≥ ''b'', the modified pseudo-remainder prem2(''A'', ''B'') of the pseudo-division of ''A'' by ''B'' is \n:<math>\\operatorname{prem2}(A,B)=-\\operatorname{rem}(|\\operatorname{lc}(B)|^{a-b+1}A,B),</math>\nwhere |lc(''B'')| is the absolute value of the leading coefficient of ''B'' (the coefficient of ''X''<sup>''b''</sup>).\n\nFor input polynomials with integers coefficients, this allows retrieval of Sturm sequences consisting of polynomials with integer coefficients. The subresultant pseudo-remainder sequence may be modified similarly, in which case the signs of the remainders '''coincide''' with those computed over the rationals.\n\nNote that the algorithm for computing the subresultant pseudo-remainder sequence given above  will compute wrong subresultant polynomials if one uses <math>-\\mathrm{prem2}(A,B)</math> instead of <math>\\operatorname{prem}(A,B)</math>.\n\n==Modular GCD algorithm==\nIf ''f'' and ''g'' are polynomials in ''F''[''x''] for some finitely generated field ''F'', the Euclidean Algorithm is the most natural way to compute their GCD. However, modern [[computer algebra]] systems only use it if ''F'' is finite because of a phenomenon called [[symbolic computation|intermediate expression swell]]. Although degrees keep decreasing during the Euclidean algorithm, if ''F'' is not [[finite field|finite]] then the bitsize of the polynomials can increase (sometimes dramatically) during the computations because repeated arithmetic operations in ''F'' tends to lead to larger expressions. For example, the addition of two rational numbers whose denominators are bounded by ''b'' leads to a rational number whose denominator is bounded by ''b''<sup>2</sup>, so in the worst case, the bitsize could nearly double with just one operation.\n\nTo expedite the computation, take a ring ''D'' for which ''f'' and ''g'' are in ''D''[''x''], and take an ideal ''I'' such that ''D''/''I'' is a finite ring. Then compute the GCD over this finite ring with the Euclidean Algorithm. Using reconstruction techniques ([[Chinese remainder theorem]], [[Rational reconstruction (mathematics)|rational reconstruction]], etc.) one can recover the GCD of ''f'' and ''g'' from its image modulo a number of ideals ''I''. One can prove<ref>{{citation|title=M. van Hoeij, M.B. Monagan: Algorithms for polynomial GCD computation over algebraic function fields. ISSAC 2004: 297–304}}</ref> that this works provided that one discards modular images with non-minimal degree, and avoids ideals ''I'' modulo which a leading coefficient vanishes.\n\nSuppose <math>F = \\mathbb{Q}(\\sqrt{3})</math>, <math>D = \\mathbb{Z}[\\sqrt{3}]</math>, <math>f = \\sqrt{3}x^3 - 5 x^2 + 4x + 9</math> and <math>g = x^4 + 4 x^2 + 3\\sqrt{3}x - 6</math>. If we take <math>I=(2)</math> then <math>D/I</math> is a [[finite ring]] (not a field since <math>I</math> is not maximal in <math>D</math>). The Euclidean algorithm applied to the images of <math>f,g</math> in <math>(D/I)[x]</math> succeeds and returns 1. This implies that the GCD of <math>f,g</math> in <math>F[x]</math> must be 1 as well. Note that this example could easily be handled by any method because the degrees were too small for expression swell to occur, but it illustrates that if two polynomials have GCD 1, then the modular algorithm is likely to terminate after a single ideal <math>I</math>.\n\n==See also==\n* [[List of polynomial topics]]\n* [[Multivariate division algorithm]]\n\n==References==\n{{reflist|1}}\n*{{cite book|first1=James H.|last1=Davenport|author1-link=James H. Davenport|first2=Yvon|last2=Siret|first3= Évelyne|last3=Tournier|title=Computer algebra: systems and algorithms for algebraic computation|others=Translated from the French by A. Davenport and J.H. Davenport|year=1988|publisher=Academic Press|isbn=978-0-12-204230-0}}\n*{{cite book\n|author=[[Donald E. Knuth|Knuth, Donald E]]\n|title=Seminumerical Algorithms\n|series=The Art of Computer Programming\n|volume=2\n|edition=Third\n|location=Reading, Massachusetts\n|publisher=Addison-Wesley\n|year=1997\n|pages=439–461, 678–691<!--   xiv+762 -->\n|isbn=0-201-89684-2}}\n* {{citation|first1=Rudiger|last1=Loos|chapter=Generalized polynomial remainder sequences|title=Computer Algebra|publisher=Springer Verlag|year=1982|editor1 =B. Buchberger|editor2=R. Loos|editor3=G. Collins}}\n* {{citation|title=S.M.M. Javadi, M.B. Monagan: A sparse modular GCD algorithm for polynomials over algebraic function fields. ISSAC 2007: 187–194}}\n\n{{Polynomials}}\n\n[[Category:Polynomials]]\n[[Category:Computer algebra]]"
    },
    {
      "title": "Polynomial identity testing",
      "url": "https://en.wikipedia.org/wiki/Polynomial_identity_testing",
      "text": "In mathematics, '''polynomial identity testing''' (PIT) is the problem of efficiently determining whether two multivariate [[polynomials]] are identical. More formally, a PIT algorithm is given an [[arithmetic circuit]] that computes a polynomial p in a [[Field (mathematics)|field]], and decides whether p is the zero polynomial. Determining the [[computational complexity theory|computational complexity]] required for polynomial identity testing is one of the most important open problems in algebraic computing complexity.\n\n== Description ==\nThe question \"Does <math>(x+y)(x-y)</math> equal <math>x^2 - y^2 \\, ?</math>\" is a question about whether two polynomials are identical. As with any polynomial identity testing question, it can be trivially transformed into the question \"Is a certain polynomial equal to 0?\"; in this case we can ask \"Does <math>(x+y)(x-y) - (x^2 - y^2) = 0</math>\"? If we are given the polynomial as an algebraic expression (rather than as a black-box), we can confirm that the equality holds through brute-force multiplication and addition, but the [[time complexity]] of the brute-force approach grows as <math>\\tbinom{n+d}{d}</math>, where <math>n</math> is the number of variables (here, <math>n=2</math>: <math>x</math> is the first and <math>y</math> is the second), and <math>d</math> is the [[Degree of a polynomial|degree]] of the polynomial (here, <math>d=2</math>). If <math>n</math> and <math>d</math> are both large, <math>\\tbinom{n+d}{d}</math> grows exponentially.<ref name=saxena>Saxena, Nitin. \"Progress on Polynomial Identity Testing.\" Bulletin of the EATCS 99 (2009): 49-79.</ref>\n\nPIT concerns whether a polynomial is identical to the zero polynomial, rather than whether the function implemented by the polynomial always evaluates to zero in the given domain. For example, the field with two elements, [[GF(2)]], contains only the elements 0 and 1. In GF(2), <math>x^2 - x</math> always evaluates to zero; despite this, PIT does not consider <math>x^2 - x</math> to be equal to the zero polynomial.<ref name=shpilka>Shpilka, Amir, and Amir Yehudayoff. \"Arithmetic circuits: A survey of recent results and open questions.\" Foundations and Trends in Theoretical Computer Science 5.3–4 (2010): 207-388.</ref>\n\nDetermining the computational complexity required for polynomial identity testing is one of the most important open problems in the mathematical subfield known as \"algebraic computing complexity\".<ref name=saxena /><ref>Dvir, Zeev, and Amir Shpilka. \"Locally decodable codes with two queries and polynomial identity testing for depth 3 circuits.\" SIAM Journal on Computing 36.5 (2007): 1404-1434.</ref> The study of PIT is a building-block to many other areas of computational complexity, such as the proof that [[IP (complexity)|IP]]=[[PSPACE]].<ref name=saxena /><ref>[[Adi Shamir]]. \"IP=PSPACE.\" [[Journal of the ACM]] (JACM) 39.4 (1992): 869-877.</ref> In addition, PIT has applications to [[Tutte matrix|Tutte matrices]] and also to [[primality testing]], where PIT techniques led to the [[AKS primality test]], the first deterministic (though impractical) [[polynomial time]] algorithm for primality testing.<ref name=saxena />\n\n== Formal problem statement ==\nGiven an [[arithmetic circuit]] that computes a [[polynomial]] in a [[Field (math)|field]], determine whether the polynomial is equal to the zero polynomial (that is, the polynomial with no nonzero terms).<ref name=saxena />\n\n== Solutions ==\nIn some cases, the specification of the arithmetic circuit is not given to the PIT solver, and the PIT solver can only input values into a \"black box\" that implements the circuit, and then analyze the output. Note that the solutions below assume that any operation (such as multiplication) in the given field takes constant time; further, all black-box algorithms below assume the size of the field is larger than the degree of the polynomial.\n\nThe [[Schwartz–Zippel lemma|Schwartz–Zippel]] algorithm provides a practical probabilistic solution, by simply randomly testing inputs and checking whether the output is zero. It was the first [[randomized polynomial time]] PIT algorithm to be proven correct.<ref name=saxena /> The larger the domain the inputs are drawn from, the less likely Schwartz–Zippel is to fail. If random bits are in short supply, the Chen-Kao algorithm (over the rationals) or the Lewin-Vadhan algorithm (over any field) require fewer random bits at the cost of more required runtime.<ref name=shpilka />\n\nA ''sparse PIT'' has at most <math>m</math> nonzero [[monomial]] terms. A sparse PIT can be deterministically solved in [[polynomial time]] of the size of the circuit and the number <math>m</math> of monomials,<ref name=saxena /> see also.<ref name=gks90>Grigoriev,Dima, Karpinski,Marek, and Singer,Michael F., \"Fast Parallel Algorithms for Sparse Multivariate Polynomial Interpolation over Finite Fields\", SIAM J. Comput., Vol 19, No.6, pp. 1059-1063, December 1990</ref>\n\nA ''low degree PIT'' has an upper bound on the degree of the polynomial. Any low degree PIT problem can be reduced in [[Time_complexity#Sub-exponential_time|subexponential]] time of the size of the circuit to a PIT problem for depth-four circuits; therefore, PIT for circuits of depth-four (and below) is intensely studied.<ref name=saxena /> \n\n==See also==\n* [[Schwartz–Zippel lemma#Applications|Applications of Schwartz–Zippel lemma]]\n\n==External links==\n* [https://nickhar.wordpress.com/2012/02/01/lecture-9-polynomial-identity-testing-by-the-schwartz-zippel-lemma/ Lecture notes] on \"Polynomial Identity Testing by the Schwartz-Zippel Lemma\"\n* {{Youtube|FjW0s9YFf3g|Polynomial Identity Testing by Michael Forbes - MIT}}\n* [https://researchmatters.in/news/prof-nitin-saxena-iit-kanpur-awarded-shanti-swarup-bhatnagar-prize-2018-his-work-algebraic Prize winner for Polynomial Identity Testing]\n\n==References==\n{{reflist|colwidth=25em}}\n\n[[Category:Polynomials]]\n[[Category:Computer algebra]]"
    },
    {
      "title": "Polynomial long division",
      "url": "https://en.wikipedia.org/wiki/Polynomial_long_division",
      "text": "{{Short description|Algorithm for division of polynomials}}\nIn [[algebra]], '''polynomial long division''' is an [[algorithm]] for dividing a [[polynomial]] by another polynomial of the same or lower [[Degree of a polynomial|degree]], a generalised version of the familiar arithmetic technique called [[long division]]. It can be done easily by hand, because it separates an otherwise complex division problem into smaller ones. Sometimes using a shorthand version called [[synthetic division]] is faster, with less writing and fewer calculations.\n\nPolynomial long division is an algorithm that implements the [[Euclidean division of polynomials]], which starting from two polynomials ''A'' (the ''dividend'') and ''B'' (the ''divisor'') produces, if ''B'' is not zero, a ''quotient'' ''Q'' and a ''remainder'' ''R'' such that\n:''A'' = ''BQ'' + ''R'',\nand either ''R'' = 0 or the degree of ''R'' is lower than the degree of ''B''. These conditions uniquely define  ''Q'' and ''R'', which means that ''Q'' and ''R'' do not depend on the method used to compute them.\n\nThe result ''R''=0 occurs [[if and only if]] the polynomial ''A'' has ''B'' as a [[polynomial factorization|factor]]. Thus long division is a means for testing whether one polynomial has another as a factor, and, if it does, for factoring it out. For example, if a [[root of a polynomial|root]] ''r'' of ''A'' is known, it can be factored out by dividing ''A'' by (''x''–''r'').\n\n==Example==\n\nFind the quotient and the remainder of the division of  <math>x^3 - 2x^2 - 4,</math> the ''dividend'', by <math>x-3,</math> the ''divisor''.\n\nThe dividend is first rewritten like this:\n\n:<math>x^3 - 2x^2 + 0x - 4.</math>\n\nThe quotient and remainder can then be determined as follows:\n\n{{Ordered list\n|1=Divide the first term of the dividend by the highest term of the divisor (meaning the one with the highest power of ''x'', which in this case is ''x''). Place the result above the bar (''x''<sup>3</sup> ÷ ''x'' = ''x''<sup>2</sup>).\n:<math>\n\\begin{array}{l}\n{\\color{White} x-3 ) x^3 - 2}x^2\\\\\nx-3\\overline{) x^3 - 2x^2 + 0x - 4}\n\\end{array}\n</math>\n\n|2=Multiply the divisor by the result just obtained (the first term of the eventual quotient). Write the result under the first two terms of the dividend ({{math|1=''x''<sup>2</sup>&nbsp;·&nbsp;(''x''&nbsp;&minus;&nbsp;3) = ''x''<sup>3</sup>&nbsp;&minus;&nbsp;3''x''<sup>2</sup>}}).\n:<math>\n\\begin{array}{l}\n{\\color{White} x-3 ) x^3 - 2}x^2\\\\\nx-3\\overline{) x^3 - 2x^2 + 0x - 4}\\\\\n{\\color{White} x-3 )} x^3 - 3x^2\n\\end{array}\n</math>\n\n|3=Subtract the product just obtained from the appropriate terms of the original dividend (being careful that subtracting something having a minus sign is equivalent to adding something having a plus sign), and write the result underneath ({{math|({{math|1=''x''<sup>3</sup>&nbsp;&minus;&nbsp;2''x''<sup>2</sup>)&nbsp;&minus;&nbsp;(''x''<sup>3</sup>&nbsp;&minus;&nbsp;3''x''<sup>2</sup>) = &minus;2''x''<sup>2</sup>&nbsp;+&nbsp;3''x''<sup>2</sup> = &nbsp;''x''<sup>2</sup>}}}}). Then, \"bring down\" the next term from the dividend.\n\n:<math>\n\\begin{array}{l}\n{\\color{White} x-3 ) x^3 - 2}x^2\\\\\nx-3\\overline{) x^3 - 2x^2 + 0x - 4}\\\\\n{\\color{White} x-3 )} \\underline{x^3 - 3x^2}\\\\\n{\\color{White} x-3 )0x^3} + {\\color{White}}x^2 + 0x\n\\end{array}\n</math>\n\n|4=Repeat the previous three steps, except this time use the two terms that have just been written as the dividend.\n:<math>\n\\begin{array}{r}\n x^2 + {\\color{White}1}x {\\color{White} {} + 3}\\\\\n x-3\\overline{) x^3 - 2x^2 + 0x - 4}\\\\\n \\underline{x^3 - 3x^2 {\\color{White} {} + 0x - 4}}\\\\\n +x^2 + 0x {\\color{White} {} - 4}\\\\\n \\underline{+x^2 - 3x {\\color{White} {} - 4}}\\\\\n +3x - 4\\\\\n\\end{array}\n</math>\n\n|5=Repeat step 4. This time, there is nothing to \"pull down\".\n:<math>\n\\begin{array}{r}\n x^2 + {\\color{White}1}x + 3\\\\\n x-3\\overline{) x^3 - 2x^2 + 0x - 4}\\\\\n \\underline{x^3 - 3x^2 {\\color{White} {} + 0x - 4}}\\\\\n +x^2 + 0x {\\color{White} {} - 4}\\\\\n \\underline{+x^2 - 3x {\\color{White} {} - 4}}\\\\\n +3x - 4\\\\\n \\underline{+3x - 9}\\\\\n +5\n\\end{array}\n</math>\n}}\n\nThe polynomial above the bar is the quotient ''q''(''x''), and the number left over (5) is the remainder ''r''(''x'').\n\n:<math>{x^3 - 2x^2 - 4} = (x-3)\\,\\underbrace{(x^2 + x + 3)}_{q(x)}  +\\underbrace{5}_{r(x)}</math>\n\nThe [[long division]] algorithm for arithmetic is very similar to the above algorithm, in which the variable ''x'' is replaced by the specific number 10.\n\n==Pseudo-code==\n\nThe algorithm can be represented in [[pseudo-code]] as follows, where +, −, and × represent polynomial arithmetic, and / represents simple division of two terms:\n\n function n / d:\n   require d ≠ 0\n   q ← 0\n   r ← n       # At each step n = d × q + r\n   while r ≠ 0 AND degree(r) ≥ degree(d):\n      t ← lead(r)/lead(d)     # Divide the leading terms\n      q ← q + t\n      r ← r − t * d\n   return (q, r)\n\nNote that this works equally well when degree(n) < degree(d); in that case the result is just the trivial (0, n).\n\nThis algorithm describes exactly the above paper and pencil method: d is written on the left of the \")\"; q is written, term after term, above the horizontal line, the last term being the value of t; the region under the horizontal line is used to compute and write down the successive values of r.\n\n== Euclidean division ==\n{{anchor|Division transformation}}\n{{main|Euclidean division of polynomials}}\nFor every pair of polynomials (''A'', ''B'') such that ''B'' ≠ 0, polynomial division provides a ''quotient'' ''Q'' and a ''remainder'' ''R'' such that \n:<math>A=BQ+R,</math>\nand either ''R''=0 or degree(''R'') < degree(''B''). Moreover (''Q'', ''R'') is the unique pair of polynomials having this property.\n\nThe process of getting the uniquely defined polynomials ''Q'' and ''R'' from ''A'' and ''B'' is called ''Euclidean division'' (sometimes ''division transformation''). Polynomial long division is thus an [[algorithm]] for Euclidean division.<ref>{{cite book|author=S. Barnard|title=Higher Algebra|year=2008|publisher=READ BOOKS|isbn=1-4437-3086-6|page=24}}</ref>\n\n==Applications==\n\n===Factoring polynomials===\n\nSometimes one or more roots of a polynomial are known, perhaps having been found using the [[rational root theorem]]. If one root ''r'' of a polynomial ''P''(''x'') of degree ''n'' is known then polynomial long division can be used to factor ''P''(''x'') into the form {{nowrap|(''x'' − ''r'')(''Q''(''x''))}} where ''Q''(''x'') is a polynomial of degree ''n'' − 1.  ''Q''(''x'') is simply the quotient obtained from the division process; since ''r'' is known to be a root of ''P''(''x''), it is known that the remainder must be zero.\n\nLikewise, if more than one root is known, a linear factor {{nowrap|(''x'' − ''r'')}} in one of them (''r'') can be divided out to obtain ''Q''(''x''), and then a linear term in another root, ''s'', can be divided out of ''Q''(''x''), etc. Alternatively, they can all be divided out at once: for example the linear factors {{nowrap|''x'' − ''r''}} and {{nowrap|''x'' − ''s''}} can be multiplied together to obtain the quadratic factor {{nowrap|''x''<sup>2</sup> − (''r'' + ''s'')''x'' + ''rs'',}} which can then be divided into the original polynomial ''P''(''x'') to obtain a quotient of degree {{nowrap|''n'' − 2.}}\n\nIn this way, sometimes all the roots of a polynomial of degree greater than four can be obtained, even though that is not always possible.  For example, if the rational root theorem can be used to obtain a single (rational) root of a [[quintic function|quintic polynomial]], it can be factored out to obtain a quartic (fourth degree) quotient; the explicit formula for the roots of a [[quartic function|quartic polynomial]] can then be used to find the other four roots of the quintic.\n\n===Finding tangents to polynomial functions===\n\nPolynomial long division can be used to find the equation of the line that is [[tangent]] to the [[graph of a function|graph of the function]] defined by the polynomial ''P''(''x'') at a particular point {{nowrap|''x'' {{=}} ''r''.}}<ref>Strickland-Constable, Charles, \"A simple method for finding tangents to polynomial graphs\", ''[[Mathematical Gazette]]'' 89, November 2005: 466-467.</ref> If ''R''(''x'') is the remainder of the division of ''P''(''x'') by {{nowrap|(''x'' – ''r'')<sup>2</sup>,}} then the equation of the tangent line at {{nowrap|''x'' {{=}} ''r''}} to the graph of the function {{nowrap|''y'' {{=}} ''P''(''x'')}} is {{nowrap|''y'' {{=}} ''R''(''x''),}} regardless of whether or not ''r''  is a root of the polynomial.\n\n====Example====\n: Find the equation of the line that is tangent to the following curve at <math>x=1</math>:\n:: <math>y = x^3 - 12x^2 - 42.</math>\n:Begin by dividing the polynomial by <math>(x-1)^2 = x^2-2x+1</math>:\n:: <math>\n\\begin{array}{r}\n x - 10\\\\\n x^2-2x+1\\overline{) x^3 - 12x^2 + 0x - 42}\\\\\n \\underline{x^3 - {\\color{White}0}2x^2 + {\\color{White}1}x} {\\color{White} {} - 42}\\\\\n -10x^2 - {\\color{White}01}x - 42\\\\\n \\underline{-10x^2 + 20x - 10}\\\\\n -21x - 32\n\\end{array}\n</math>\n:The tangent line is <math>y = - 21 x - 32.</math>\n\n===Cyclic redundancy check===\n\nA [[cyclic redundancy check]] uses the remainder of polynomial division to detect errors in transmitted messages.\n\n==See also==\n*[[Polynomial remainder theorem]]\n*[[Synthetic division]], a more concise method of performing Euclidean polynomial division\n*[[Ruffini's rule]]\n*[[Euclidean domain]]\n*[[Gröbner basis]]\n*[[Greatest common divisor of two polynomials]]\n\n==Notes==\n<references/>\n\n{{DEFAULTSORT:Polynomial Long Division}}\n[[Category:Polynomials]]\n[[Category:Computer algebra]]\n[[Category:Division (mathematics)]]"
    },
    {
      "title": "Polynomial matrix",
      "url": "https://en.wikipedia.org/wiki/Polynomial_matrix",
      "text": "{{distinguish|matrix polynomial}}\nIn [[mathematics]], a '''polynomial matrix''' or '''matrix of polynomials''' is a [[matrix (mathematics)|matrix]] whose elements are univariate or multivariate [[polynomial]]s. Equivalently, a polynomial matrix is a polynomial whose coefficients are matrices. \n\nA univariate polynomial matrix ''P'' of degree ''p'' is defined as:\n\n:<math>P = \\sum_{n=0}^p A(n)x^n = A(0)+A(1)x+A(2)x^2+ \\cdots +A(p)x^p</math>\n\nwhere <math>A(i)</math> denotes a matrix of constant coefficients, and <math>A(p)</math> is non-zero. \nAn example 3×3 polynomial matrix, degree 2:\n\n:<math>\nP=\\begin{pmatrix}\n1 & x^2 & x \\\\\n0 & 2x & 2 \\\\\n3x+2 & x^2-1 & 0\n\\end{pmatrix}\n=\\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 0 & 2 \\\\\n2 & -1 & 0\n\\end{pmatrix}\n\n+\\begin{pmatrix}\n0 & 0 & 1 \\\\\n0 & 2 & 0 \\\\\n3 & 0 & 0\n\\end{pmatrix}x+\\begin{pmatrix}\n0 & 1 & 0 \\\\\n0 & 0 & 0 \\\\\n0 & 1 & 0\n\\end{pmatrix}x^2.\n</math>\n\nWe can express this by saying that for a [[ring (mathematics)|ring]] ''R'', the rings <math>M_n(R[X])</math> and\n<math>(M_n(R))[X]</math> are [[Ring homomorphism|isomorphic]].\n\n==Properties==\n*A polynomial matrix over a [[field (mathematics)|field]] with [[determinant]] equal to a non-zero element of that field is called [[unimodular matrix|unimodular]], and has an [[matrix inverse|inverse]] that is also a polynomial matrix. Note that the only scalar unimodular polynomials are polynomials of degree 0 – nonzero constants, because an inverse of an arbitrary polynomial of higher degree is a rational function.\n*The roots of a polynomial matrix over the [[complex numbers]] are the points in the [[complex plane]] where the matrix loses [[rank (linear algebra)|rank]].\n\nNote that polynomial matrices are ''not'' to be confused with [[monomial matrix|monomial matrices]], which are simply matrices with exactly one non-zero entry in each row and column.\n\nIf by λ we denote any element of the [[field (mathematics)|field]] over which we constructed the matrix, by ''I'' the identity matrix, and we let ''A'' be a polynomial matrix, then the matrix λ''I''&nbsp;&minus;&nbsp;''A'' is the '''characteristic matrix''' of the matrix ''A''. Its determinant, |λ''I''&nbsp;&minus;&nbsp;''A''| is the [[characteristic polynomial]] of the matrix&nbsp;''A''.\n\n== References ==\n\n* E.V.Krishnamurthy, Error-free Polynomial Matrix computations, Springer Verlag, New York, 1985\n\n[[Category:Matrices]]\n[[Category:Polynomials]]\n\n{{Linear-algebra-stub}}"
    },
    {
      "title": "Polynomial sequence",
      "url": "https://en.wikipedia.org/wiki/Polynomial_sequence",
      "text": "{{Use American English|date = March 2019}}\n{{Short description|Sequence valued in polynomials}}\nIn [[mathematics]], a '''polynomial sequence''' is a [[sequence]] of [[polynomial]]s indexed by the nonnegative integers 0, 1, 2, 3, ..., in which each [[indexed family|index]] is [[equality (mathematics)|equal]] to the degree of the corresponding polynomial.  Polynomial sequences are a topic of interest in [[enumerative combinatorics]] and [[algebraic combinatorics]], as well as [[applied mathematics]].\n\n==Examples==\n\nSome polynomial sequences arise in [[physics]] and [[approximation theory]] as the solutions of certain [[ordinary differential equation]]s:\n* [[Laguerre polynomials]]\n* [[Chebyshev polynomials]]\n* [[Legendre polynomials]]\n* [[Jacobi polynomials]]\n\nOthers come from [[statistics]]:\n* [[Hermite polynomials]]\n\nMany are studied in algebra and combinatorics:\n* [[Monomial]]s\n* [[Rising factorial]]s\n* [[Falling factorial]]s\n* [[All-one polynomial]]s\n* [[Abel polynomials]]\n* [[Bell polynomials]]\n* [[Bernoulli polynomials]]\n* [[Cyclotomic polynomial]]s\n* [[Dickson polynomial]]s\n* [[Fibonacci polynomials]]\n* [[Lagrange polynomials]]\n* [[Lucas polynomials]]\n* [[Spread polynomials]]\n* [[Touchard polynomials]]\n* [[Rook polynomials]]\n\n==Classes of polynomial sequences==\n* Polynomial sequences of [[binomial type]]\n* [[Orthogonal polynomials]]\n* [[Secondary polynomials]]\n* [[Sheffer sequence]]\n* [[Sturm sequence]]\n* [[Generalized Appell polynomials]]\n\n==See also==\n*[[Umbral calculus]]\n\n==References==\n* Aigner, Martin. \"A course in enumeration\",  GTM Springer, 2007, {{isbn|3-540-39032-4}}  p21.\n* Roman, Steven \"The Umbral Calculus\", Dover Publications, 2005, {{isbn|978-0-486-44139-9}}.\n* Williamson, S. Gill \"Combinatorics for Computer Science\", Dover Publications, (2002) p177.\n\n{{DEFAULTSORT:Polynomial Sequence}}\n[[Category:Polynomials]]\n[[Category:Sequences and series]]"
    },
    {
      "title": "Polynomial solutions of P-recursive equations",
      "url": "https://en.wikipedia.org/wiki/Polynomial_solutions_of_P-recursive_equations",
      "text": "In mathematics a '''[[P-recursive equation]]''' can be solved for '''polynomial solutions'''. Sergei A. Abramov in 1989 and [[Marko Petkovšek]] in 1992 described an [[algorithm]] which finds all [[Polynomial ring|polynomial]] solutions of those recurrence equations with polynomial coefficients.<ref name=\"Abramov 1989\">{{Cite journal|last=Abramov|first=Sergei A.|date=1989|title=Problems in computer algebra that are connected with a search for polynomial solutions of linear differential and difference equations|url=|journal=Moscow University Computational Mathematics and Cybernetics|volume=3|pages=|via=}}</ref><ref name=\"Petkovšek 1992\">{{Cite journal|last=Petkovšek|first=Marko|date=1992|title=Hypergeometric solutions of linear recurrences with polynomial coefficients|journal=Journal of Symbolic Computation|volume=14|issue=2–3|pages=243–264|doi=10.1016/0747-7171(92)90038-6|issn=0747-7171}}</ref> The algorithm computes a ''degree bound'' for the solution in a first step. In a second step an [[ansatz]] for a polynomial of this degree is used and the unknown coefficients are computed by a [[system of linear equations]]. This article describes this algorithm.\n\nIn 1995 Abramov, Bronstein and Petkovšek showed that the polynomial case can be solved more efficiently by considering [[Formal power series|power series]] solution of the recurrence equation in a specific power basis (i.e. not the ordinary basis <math display=\"inline\">(x^n)_{n \\in \\N}</math>).<ref name=\"Abramov 1995\">{{Cite book|last=Abramov|first=Sergei A.|last2=Bronstein|first2=Manuel|last3=Petkovšek|first3=Marko|date=1995|title=On polynomial solutions of linear operator equations|url=http://dl.acm.org/citation.cfm?id=220346.220384|journal=ISSAC '95 Proceedings of the 1995 International Symposium on Symbolic and Algebraic Computation|publisher=ACM|volume=|pages=290–296|doi=10.1145/220346.220384|isbn=978-0897916998|via=|citeseerx=10.1.1.46.9373}}</ref>\n\nOther algorithms which compute [[Abramov's algorithm|rational]] or [[Petkovšek's algorithm|hypergeometric]] solutions of a linear recurrence equation with polynomial coefficients also use algorithms which compute polynomial solutions.\n\n== Degree bound ==\nLet <math display=\"inline\">\\mathbb{K}</math> be a [[Field (mathematics)|field]] of characteristic zero and  <math display=\"inline\">\\sum_{k=0}^r p_k(n) \\, y (n+k) = f(n)</math> a [[P-recursive equation|recurrence equation]] of order <math display=\"inline\">r</math> with polynomial coefficients <math display=\"inline\">p_k \\in \\mathbb{K} [n]</math>, polynomial right-hand side <math display=\"inline\">f \\in \\mathbb{K}[n]</math> and unknown polynomial sequence <math>y(n) \\in \\mathbb{K}[n]</math>. Furthermore <math display=\"inline\">\\deg (p)</math> denotes the degree of a polynomial <math display=\"inline\">p \\in \\mathbb{K}[n]</math> (with <math display=\"inline\">\\deg (0) = - \\infty</math> for the zero polynomial) and <math display=\"inline\">\\text{lc}(p)</math> denotes the leading coefficient of the polynomial. Moreover let<math display=\"block\">\\begin{align}\n    q_i &= \\sum_{k=i}^r \\binom{k}{i} p_k, & b &= \\max_{i=0,\\dots,r}(\\deg (q_i)-i), \\\\\n    \\alpha(n) &= \\sum_{i=0,\\dots,r \\atop \\deg (q_i) - i = b} \\text{lc} (q_i) n^{\\underline{i}}, & d_\\alpha &= \\max \\{n \\in \\N \\, : \\, \\alpha(n) = 0 \\} \\cup \\{ - \\infty\\}\n\\end{align}</math>for <math display=\"inline\">i=0,\\dots,r</math> where <math display=\"inline\">n^{\\underline{i}} = n (n-1) \\cdots (n-i+1)</math> denotes the [[Falling and rising factorials|falling factorial]] and <math display=\"inline\">\\N</math> the set of nonegative integers. Then <math display=\"inline\">\\deg (y) \\leq \\max \\{ \\deg(f) - b, -b-1, d_\\alpha \\}</math>. This is called a degree bound for the polynomial solution <math display=\"inline\">y</math>. This bound was shown by Abramov and Petkovšek.<ref name=\"Abramov 1989\" /><ref name=\"Petkovšek 1992\" /><ref name=\"Abramov 1995\" /><ref>Weixlbaumer, Christian (2001). [https://www.risc.jku.at/research/combinat/software/DiffTools/pub/weixlbaumer01.pdf Solutions of difference equations with polynomial coefficients]. Diploma Thesis, Johannes Kepler Universität Linz</ref>\n\n== Algorithm ==\nThe algorithm consists of two steps. In a first step the ''degree bound'' is computed. In a second step an ''ansatz'' with a polynomial <math display=\"inline\">y</math> of that degree with arbitrary coefficients in <math display=\"inline\">\\mathbb{K}</math> is made and plugged into the recurrence equation. Then the different powers are compared and a system of linear equations for the coefficients of <math display=\"inline\">y</math> is set up and solved. This is called the ''method undetermined coefficients''.<ref>{{Cite book|url=https://www.math.upenn.edu/~wilf/Downld.html|title=A=B|last=Petkovšek|first=Marko|last2=Wilf|first2=Herbert S.|last3=Zeilberger|first3=Doron|date=1996|publisher=A K Peters|others=|isbn=978-1568810638|location=|pages=|oclc=33898705}}</ref> The algorithm returns the general polynomial solution of a recurrence equation.\n '''algorithm''' polynomial_solutions '''is'''\n     '''input:''' Linear recurrence equation <math display=\"inline\">\\sum_{k=0}^r p_k(n) \\, y (n+k) = f(n), p_k, f \\in \\mathbb{K}[n], p_0, p_r \\neq 0</math>.\n     '''output:''' The general polynomial solution <math display=\"inline\">y</math> if there are any solutions, otherwise false.\n \n     '''for''' <math display=\"inline\">i=0,\\dots,r</math> '''do'''\n         <math display=\"inline\">q_i = \\sum_{k=i}^r \\binom{k}{i} p_k</math>\n     '''repeat'''\n     <math display=\"inline\">b=\\max_{i=0,\\dots,r} (\\deg (q_i) - i)</math>\n     <math display=\"inline\">\\alpha(n) = \\sum_{i=0,\\dots,r \\atop \\deg (q_i) - i = b} \\text{lc} (q_i) n^{\\underline{i}}</math>\n     <math display=\"inline\">d_\\alpha = \\max \\{n \\in \\N \\, : \\, \\alpha(n) = 0 \\} \\cup \\{ - \\infty\\}</math>\n     <math display=\"inline\">d = \\max \\{ \\deg (f) - b, -b-1, d_\\alpha\\}</math>\n     <math display=\"inline\">y(n) = \\sum_{j=0}^d y_j n^j</math> with unknown coefficients <math display=\"inline\">y_j \\in \\mathbb{K}</math> for <math display=\"inline\">j=0,\\dots,d</math>\n     Compare coefficients of polynomials <math display=\"inline\">\\sum_{k=0}^r p_k(n) \\, y (n+k)</math> and <math display=\"inline\">f(n)</math> to get possible values for <math display=\"inline\">y_j, j=0,\\dots,d</math>\n     '''if''' there are possible values for <math display=\"inline\">y_j</math> '''then'''\n         '''return''' general solution <math display=\"inline\">y</math>\n     '''else'''\n         '''return''' false\n     '''end if'''\n\n== Example ==\nApplying the formula for the degree bound on the recurrence equation<math display=\"block\">(n^2-2) \\, y (n) + (-n^2+2n) \\, y (n+1)=2n,</math>over <math display=\"inline\">\\Q</math> yields <math display=\"inline\">\\deg (y) \\leq 2</math>. Hence one can use an ansatz with a quadratic polynomial <math display=\"inline\">y(n) =y_2 n^2 + y_1 n + y_0</math> with <math display=\"inline\">y_0, y_1, y_2 \\in \\Q</math>. Plugging this ansatz into the original recurrence equation leads to<math display=\"block\">2n = (n^2-2) \\, y(n) + (-n^2+2n) \\, y (n+1) = (y_1 + y_2) \\, n^2 + (2 y_0 + 2 y_2 ) \\, n - 2 y_0.</math>This is equivalent to the following system of linear equations<math display=\"block\">\\begin{align}\n\\begin{pmatrix}\n0 & 1 & 1 \\\\ 2 & 0 & 2 \\\\ -2 & 0 & 0 \n\\end{pmatrix}\n\\begin{pmatrix}\ny_0 \\\\ y_1 \\\\ y_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0 \\\\ 2 \\\\ 0\n\\end{pmatrix}\n\\end{align}</math>with the solution <math display=\"inline\">y_0 = 0, y_1 = -1, y_2 = 1</math>. Therefore the only polynomial solution is <math display=\"inline\">y (n) = n^2-n</math>.\n\n== References ==\n<references />\n\n\n\n\n[[Category:Polynomials]]"
    },
    {
      "title": "Polynomial Wigner–Ville distribution",
      "url": "https://en.wikipedia.org/wiki/Polynomial_Wigner%E2%80%93Ville_distribution",
      "text": "{{More citations needed|date=August 2015}}\nIn signal processing, the '''polynomial Wigner–Ville distribution''' is a [[quasiprobability distribution]] that generalizes the [[Wigner distribution function]]. It was proposed by Boualem Boashash and Peter O'Shea in 1994.\n\n== Introduction ==\n\nMany signals in nature and in engineering applications can be modeled as <math>z(t)=e^{j2\\pi\\phi(t)}</math>, where <math>\\phi(t)</math> is a polynomial phase and <math>j=\\sqrt{-1}</math>.\n\nFor example, it is important to detect signals of an arbitrary high-order polynomial phase. However, the conventional Wigner–Ville distribution have the limitation being based on the second-order statistics. Hence, the polynomial Wigner–Ville distribution was proposed as a generalized form of the conventional Wigner–Ville distribution, which is able to deal with signals with nonlinear phase.\n\n== Definition ==\n\nThe polynomial Wigner–Ville distribution <math>W^g_z(t, f)</math> is defined as\n\n:<math> W^g_z(t, f)=\\mathcal{F}_{\\tau\\to f}\\left[K^g_z(t, \\tau)\\right] </math>\n\nwhere <math>\\mathcal{F}_{\\tau\\to f}</math> denotes the [[Fourier transform]] with respect to <math>\\tau</math>, and <math>K^g_z(t, \\tau)</math> is the polynomial kernel given by\n\n:<math> K^g_z(t, \\tau)=\\prod_{k=-\\frac{q}{2}}^{\\frac{q}{2}} \\left[z\\left(t+c_k\\tau\\right)\\right]^{b_k} </math>\n\nwhere <math>z(t)</math> is the input signal and <math>q</math> is an even number.\nThe above expression for the kernel may be rewritten in symmetric form as\n\n:<math> K^g_z(t, \\tau)=\\prod_{k=0}^{\\frac{q}{2}} \\left[z\\left(t+c_k\\tau\\right)\\right]^{b_k}\\left[z^*\\left(t+c_{-k}\\tau\\right)\\right]^{-b_{-k}} </math>\n\nThe discrete-time version of the polynomial Wigner–Ville distribution is given by the [[discrete Fourier transform]] of\n\n:<math> K^g_z(n, m)=\\prod_{k=0}^{\\frac{q}{2}} \\left[z\\left(n+c_{k}m\\right)\\right]^{b_k}\\left[z^*\\left(n+c_{-k}m\\right)\\right]^{-b_{-k}} </math>\n\nwhere <math>n=t{f}_s, m={\\tau}{f}_{s},</math> and <math>f_s</math> is the sampling frequency.\nThe conventional [[Wigner distribution function|Wigner–Ville distribution]] is a special case of the polynomial Wigner–Ville distribution with <math>q=2, b_{-1}=-1, b_1=1, b_0=0, c_{-1}=-\\frac{1}{2}, c_0=0, c_1=\\frac{1}{2} </math>\n\n== Example ==\n\nOne of the simplest generalizations of the usual Wigner–Ville distribution kernel can be achieved by taking <math>q=4</math>. The set of coefficients <math>b_k</math> and <math>c_k</math> must be found to completely specify the new kernel. For example, we set\n\n:<math> b_1=-b_{-1}=2, b_2=b_{-2}=1, b_0=0 </math>\n:<math> c_1=-c_{-1}=0.675, c_2=-c_{-2}=-0.85</math>\n\nThe resulting discrete-time kernel is then given by\n\n:<math> K^g_z(n, m)=\\left[z\\left(n+0.675m\\right)z^*\\left(n-0.675m\\right)\\right]^2z^*\\left(n+0.85m\\right)z\\left(n-0.85m\\right) </math>\n\n=== Design of a Practical Polynomial Kernel ===\nGiven a signal <math>z(t)=e^{j2\\pi\\phi(t)}</math>, where <math>\\phi(t)=\\sum_{i=0}^p a_i t^i</math>is a polynomial function, its instantaneous frequency (IF) is <math>\\phi'(t) = \\sum_{i=1}^p ia_it^{i-1}</math>.\n\nFor a practical polynomial kernel <math>K^g_z(t, \\tau)</math>, the set of coefficients <math>q, b_k</math>and <math>c_k</math>should be chosen properly such that\n:<math> \\begin{align}\nK^g_z(t, \\tau) &=\\prod_{k=0}^{\\frac{q}{2}} \\left[z\\left(t+c_k\\tau\\right)\\right]^{b_k}\\left[z^*\\left(t+c_{-k}\\tau\\right)\\right]^{-b_{-k}}\\\\\n&= \\exp(j2\\pi \\sum_{i=1}^pia_it^{i-1}\\tau)\n\\end{align}</math>\n:<math> \\begin{align}\nW_z^g(t,f) &= \\int_{-\\infin}^{\\infin} \\exp(-j2\\pi(f - \\sum_{i=1}^p i a_i t^{i-1}) \\tau)d\\tau\\\\\n&\\cong \\delta (f - \\sum_{i=1}^p i a_i t^{i-1})\n\\end{align}</math>\n\n* When <math>q=2, b_{-1}=-1, b_0=0, b_1=1, p=2</math>,\n:<math> z\\left(t+c_1\\tau\\right)z^*\\left(t+c_{-1}\\tau\\right)=\\exp(j2\\pi \\sum_{i=1}^2 i a_i t^{i-1}\\tau)</math>\n:<math> a_2(t+c_1)^2 + a_1(t+c_1) - a_2(t + c_{-1})^2 - a_1(t + c_{-1}) = 2a_2t\\tau + a_1\\tau</math>\n:<math> \\Rightarrow c_1 - c_{-1} = 1, c_1 + c_{-1} = 0</math>\n:<math> \\Rightarrow c_1=\\frac{1}{2}, c_{-1}=-\\frac{1}{2}</math>\n\n* When <math>q=4, b_{-2}=b_{-1}=-1, b_0=0, b_2=b_1=1, p=3</math>\n:<math> \\begin{align}\n&a_3(t + c_1)^3 + a_2(t+c_1)^2 + a_1(t+c_1) \\\\\n&a_3(t + c_2)^3 + a_2(t+c_2)^2 + a_1(t+c_2) \\\\\n&- a_3(t + c_{-1})^3 - a_2(t + c_{-1})^2 - a_1(t + c_{-1}) \\\\\n&- a_3(t + c_{-2})^3 - a_2(t + c_{-2})^2 - a_1(t + c_{-2}) \\\\\n&= 3a_3t^2\\tau + 2a_2t\\tau + a_1\\tau\n\\end{align}</math>\n:<math> \\Rightarrow \n\\begin{cases} \nc_1 + c_2 - c_{-1} - c_{-2} = 1 \\\\ \nc_1^2 + c_2^2 - c_{-1}^2 - c_{-2}^2 = 0 \\\\ \nc_1^3 + c_2^3 - c_{-1}^3 - c_{-2}^3 = 0 \n\\end{cases}</math>\n\n== Applications ==\n\nNonlinear FM signals are common both in nature and in engineering applications. For example, the sonar system of some bats use hyperbolic FM and quadratic FM signals for echo location. In radar, certain pulse-compression schemes employ linear FM and quadratic signals. The [[Wigner distribution function|Wigner–Ville distribution]] has optimal concentration in the time-frequency plane for linear [[Frequency modulation|frequency modulated]] signals. However, for nonlinear frequency modulated signals, optimal concentration is not obtained, and smeared spectral representations result. The polynomial Wigner–Ville distribution can be designed to cope with such problem.\n\n== References ==\n\n* {{cite journal|last1=Boashash|first1=B.|last2=O'Shea|first2=P.|title=Polynomial Wigner-Ville distributions and their relationship to time-varying higher order spectra|journal=IEEE Transactions on Signal Processing|volume=42|issue=1|year=1994|pages=216–220|issn=1053587X|doi=10.1109/78.258143}}\n* {{cite conference|last1=Luk|first1=Franklin T.|last2=Benidir|first2=Messaoud|last3=Boashash|first3=Boualem|title=Polynomial Wigner-Ville distributions|volume=2563|date=June 1995|pages=69–79|issn=0277786X|doi=10.1117/12.211426|location=San Diego, CA|conference=SPIE Proceedings}}\n* “Polynomial Wigner–Ville distributions and time-varying higher spectra,” in Proc. Time-Freq. Time-Scale Anal., Victoria, B.C., Canada, Oct. 1992, pp.&nbsp;31–34.\n\n{{DEFAULTSORT:Polynomial Wigner-Ville distribution}}\n[[Category:Quantum mechanics]]\n[[Category:Continuous distributions]]\n[[Category:Concepts in physics]]\n[[Category:Mathematical physics]]\n[[Category:Exotic probabilities]]\n[[Category:Polynomials]]"
    },
    {
      "title": "Primitive polynomial (field theory)",
      "url": "https://en.wikipedia.org/wiki/Primitive_polynomial_%28field_theory%29",
      "text": "{{Use American English|date = March 2019}}\n{{Short description|minimal polynomial of some finite field extensions}}\n{{For|polynomials such that the greatest common divisor of the coefficeints is 1|Primitive polynomial (ring theory)}}\n{{More citations needed|date=May 2010}}\nIn [[field theory (mathematics)|field theory]], a branch of [[mathematics]], a '''primitive polynomial''' is the [[minimal polynomial (field theory)|minimal polynomial]] of a [[primitive element (finite field)|primitive element]] of the [[finite field|finite]] [[extension field]] [[Galois Field|GF]](''p''<sup>''m''</sup>). In other words, a polynomial ''F''(''X'') with coefficients in {{nowrap|1=GF(''p'') = '''Z'''/''p'''''Z'''}} is a primitive polynomial if its degree is ''m'' and it has a root ''α'' in GF(''p''<sup>''m''</sup>) such that {{nowrap|{0, 1, ''α'', ''α''{{i sup|2}}, ''α''{{i sup|3}}, ..., ''α''{{i sup|''p''{{sup|''m''}}−2}}}{{null}}}} is the entire field GF(''p''<sup>''m''</sup>). This means also that ''α'' is a [[primitive root of unity|primitive ({{nowrap|''p''<sup>''m''</sup> − 1}})-root of unity]] in GF(''p''<sup>''m''</sup>).\n\n==Properties==\nBecause all minimal polynomials are [[irreducible polynomial|irreducible]], all primitive polynomials are also irreducible.\n\nA primitive polynomial must have a non-zero constant term, for otherwise it will be divisible by&nbsp;''x''. Over [[GF(2)]], {{nowrap|''x'' + 1}} is a primitive polynomial and all other primitive polynomials have an odd number of terms, since any polynomial mod 2 with an even number of terms is divisible by {{nowrap|''x'' + 1}} (it has 1 as a root).\n\nAn [[irreducible polynomial]] ''F''(''x'') of degree ''m'' over GF(''p''), where ''p'' is prime, is a primitive polynomial if the smallest positive integer ''n'' such that ''F''(''x'') divides {{nowrap|''x''<sup>''n''</sup> − 1}} is {{nowrap|1=''n'' = ''p''<sup>''m''</sup> − 1}}.\n\nOver GF(''p''<sup>''m''</sup>) there are exactly {{nowrap|''φ''(''p''<sup>''m''</sup> − 1)/''m''}} primitive polynomials of degree ''m'', where ''φ'' is [[Euler's totient function]].\n\nA primitive polynomial of degree ''m'' has ''m'' different roots in GF(''p''<sup>''m''</sup>), which all have [[order (group theory)|order]] {{nowrap|''p''<sup>''m''</sup> − 1}}. This means that, if ''α'' is such a root, then {{nowrap|1=''α''{{i sup|''p''{{sup|''m''}}−1}} = 1}} and {{nowrap|''α''{{i sup|''i''}} ≠ 1}} for {{nowrap|0 < ''i'' < ''p''<sup>''m''</sup> − 1}}.\n\nThe primitive polynomial ''F''(''x'') of degree ''m'' of a primitive element ''α'' in GF(''p''<sup>''m''</sup>) has explicit form {{nowrap|1=''F''(''x'') = (''x'' − ''α'')(''x'' − ''α''{{i sup|''p''}})(''x'' − ''α''{{i sup|''p''{{sup|2}}}})⋅⋅⋅(''x'' − ''α''{{i sup|''p''{{sup|''m''−1}}}})}}.\n\n==Usage==\n===Field element representation===\nPrimitive polynomials are used in the representation of elements of a [[finite field]].  If ''α'' in GF(''p''<sup>''m''</sup>) is a root of a primitive polynomial ''F''(''x'') then since the order of ''α'' is {{nowrap|''p''<sup>''m''</sup> − 1}} that means that all nonzero elements of GF(''p''<sup>''m''</sup>) can be represented as successive powers of ''α'':\n\n:<math>\n\\mathrm{GF}(p^m) = \\{ 0, 1, \\alpha, \\alpha^2, \\ldots, \\alpha^{p^m-2} \\} .\n</math>\n\nWhen these elements are reduced modulo ''F''(''x''), they provide the [[polynomial basis]] representation of all the elements of the field.\n\nSince the [[multiplicative group]] of a finite field is always a [[cyclic group]], a primitive polynomial ''f'' is a polynomial such that ''x'' is a generator of the multiplicative group in GF(''p'')[''x'']/''f''(''x'')\n\n===Pseudo-random bit generation===\nPrimitive polynomials over GF(2), the field with two elements, can be used for [[Pseudorandom number generator|pseudorandom bit generation]]. In fact, every [[linear feedback shift register]] with maximum cycle length (which is {{nowrap|2<sup>''n''</sup> − 1}}, where ''n'' is the length of the linear feedback shift register) may be built from a primitive polynomial.<ref>C. Paar, J. Pelzl - Understanding Cryptography: A Textbook for Students and Practitioners</ref>\n\nFor example, given the primitive polynomial {{nowrap|''x''<sup>10</sup> + ''x''<sup>3</sup> + 1}}, we start with a user-specified nonzero 10-bit seed occupying bit positions 1 through 10, starting from the least significant bit. (The seed need not randomly be chosen, but it can be).  We then take the 10th and 3rd bits, and create a new 0th bit, so that the [[xor]] of the three bits is 0.  The seed is then shifted left one position so that the 0th bit moves to position 1.  This process can be repeated to generate {{nowrap|1=2<sup>10</sup> − 1 = 1023}} pseudo-random bits.\n\nIn general, for a primitive polynomial of degree ''m'' over GF(2), this process will generate {{nowrap|2<sup>''m''</sup> − 1}} pseudo-random bits before repeating the same sequence.\n\n=== CRC codes ===\n\nThe [[cyclic redundancy check]] (CRC) is an error-detection code that operates by interpreting the message bitstring as the coefficients of a polynomial over GF(2) and dividing it by a fixed generator polynomial also over GF(2); see [[Mathematics of CRC]]. Primitive polynomials, or multiples of them, are sometimes a good choice for generator polynomials because they can reliably detect two bit errors that occur far apart in the message bitstring, up to a distance of {{nowrap|2<sup>''n''</sup> − 1}} for a degree ''n'' primitive polynomial.\n\n==Primitive trinomials==\n\nA useful class of primitive polynomials is the primitive trinomials, those having only three nonzero terms: {{nowrap|''x<sup>r</sup>'' + ''x<sup>k</sup>'' + 1}}.  Their simplicity makes for particularly small and fast [[linear feedback shift register]]s.  A number of results give techniques for locating and testing primitiveness of trinomials.\n\nFor polynomials over GF(2), where {{nowrap|2<sup>''r''</sup> − 1}} is a [[Mersenne prime]], a polynomial of degree ''r'' is primitive if and only if it is irreducible.  (Given an irreducible polynomial, it is ''not'' primitive only if the period of ''x'' is a non-trivial factor of {{nowrap|2<sup>''r''</sup> − 1}}.  Primes have no non-trivial factors.)  Although the [[Mersenne Twister]] pseudo-random number generator does not use a trinomial, it does take advantage of this.\n\n[[Richard Brent (scientist)|Richard Brent]] has been tabulating primitive trinomials of this form, such as {{nowrap|''x''<sup>74207281</sup> + ''x''<sup>30684570</sup> + 1}}.<ref>[http://wwwmaths.anu.edu.au/~brent/trinom.html Search for Primitive Trinomials (mod&nbsp;2)]</ref><ref>{{cite arxiv |title=Twelve new primitive binary trinomials |first1=Richard P. |last1=Brent |authorlink1=Richard Brent (scientist) |first2=Paul |last2=Zimmerman |arxiv=1605.09213 |class=math.NT |date=24 May 2016 }}</ref> This can be used to create a pseudo-random number generator of the huge period {{nowrap|2<sup>74207281</sup> − 1}}, or roughly 10<sup>22338617</sup>.\n\n==References==\n{{reflist}}\n\n==External links==\n* {{MathWorld |urlname=PrimitivePolynomial |title=Primitive Polynomial}}\n\n[[Category:Field theory]]\n[[Category:Polynomials]]"
    },
    {
      "title": "Principal root of unity",
      "url": "https://en.wikipedia.org/wiki/Principal_root_of_unity",
      "text": "{{refimprove|date=August 2016}}\nIn mathematics, a '''principal ''n''-th root of unity''' (where ''n'' is a positive integer) of a [[ring (mathematics)|ring]] is an element <math>\\alpha</math> satisfying the equations\n\n: <math>\n\\begin{align}\n& \\alpha^n = 1 \\\\\n& \\sum_{j=0}^{n-1} \\alpha^{jk} = 0 \\text{ for } 1 \\leq k < n\n\\end{align}\n</math>\n\nIn an integral domain, every primitive ''n''-th root of unity is also a principal <math>n</math>-th root of unity. In any ring, if <math>n</math> is a power of <math>2</math>, then any <math>n/2</math>-th root of <math>-1</math> is a principal <math>n</math>-th root of unity.\n\nA non-example is <math>3</math> in the ring of integers [[modular arithmetic|modulo]] <math>26</math>; while <math>3^3 \\equiv 1 \\pmod{26}</math> and thus <math>3</math> is a cube root of unity, <math>1 + 3 + 3^2 \\equiv 13 \\pmod{26}</math> meaning that it is not a ''principal'' [[cube root of unity]].\n\nThe significance of a root of unity being ''principal'' is that it is a necessary condition for the theory of the [[Discrete Fourier transform (general)|discrete Fourier transform]] to work out correctly.\n\n==References==\n{{reflist}}\n*{{citation|last=Bini|first= D.|last2= Pan|first2= V.|title= Polynomial and Matrix Computations|volume=1|place= Boston, MA|publisher= Birkhäuser|year= 1994|pages=11}}\n\n[[Category:Algebraic numbers]]\n[[Category:Cyclotomic fields]]\n[[Category:Polynomials]]\n[[Category:1 (number)]]\n[[Category:Complex numbers]]"
    },
    {
      "title": "Quadratic equation",
      "url": "https://en.wikipedia.org/wiki/Quadratic_equation",
      "text": "{{short description|A polynomial equation of degree two}}\n{{About|algebraic equations of degree two and their solutions|functions defined by polynomials of degree two|Quadratic function}}\n[[File:Quadratic formula.svg|thumbnail|The [[quadratic formula]] for the roots of the general quadratic equation]]\nIn [[algebra]], a '''quadratic equation''' (from the [[Latin]] {{lang|la|quadratus}} for \"[[Square (algebra)|square]]\") is any equation having the form \n: <math>ax^2 + bx + c = 0,</math>\nwhere {{math|''x''}} represents an unknown, and {{math|''a''}}, {{math|''b''}}, and {{math|''c''}} represent known numbers, with {{math|''a'' ≠ 0}}. If {{math|''a'' {{=}} 0}}, then the equation is [[linear equation|linear]], not quadratic, as there is no <math>ax^2</math> term.  The numbers {{math|''a''}}, {{math|''b''}}, and {{math|''c''}} are the ''[[coefficient]]s'' of the equation and may be distinguished by calling them, respectively, the ''quadratic coefficient'', the ''linear coefficient'' and the ''constant'' or ''free term''.<ref>Protters & Morrey: \"Calculus and Analytic Geometry. First Course\".</ref>\n\nThe values of {{mvar|x}} that satisfy the equation are called ''solutions'' of the equation, and ''[[zero of a function|roots]]'' or ''[[zero of a function|zeros]]'' of its left-hand side. A quadratic equation has at most two solutions. If there is no [[real number|real]] solution, there are two [[complex number|complex]] solutions. If there is only one solution, one says that it is a [[double root]]. So a quadratic equation has always two roots, if complex roots are considered, and if a double root is counted for two.\nIf the two solutions are denoted {{mvar|r}} and {{mvar|s}} (possibly equal), one has \n:<math>ax^2+bx+c=a(x-r)(x-s).</math>\nThus, the process of solving a quadratic equation is also called ''[[Factorization|factorizing]]'' or ''factoring''. [[Completing the square]] is the standard method for that, which results in the [[quadratic formula]], which express the solutions in terms of {{mvar|a}}, {{mvar|b}}, and {{mvar|c}}. [[Graph of a function|Graphing]] may also be used for getting an approximate value of the solutions.  Solutions to problems that may be expressed in terms of  quadratic equations were known as early as 2000 BC.\n\nBecause the quadratic equation involves only one unknown, it is called \"[[univariate]]\". The quadratic equation only contains [[exponentiation|powers]] of {{math|''x''}} that are non-negative integers, and therefore it is a [[polynomial equation]]. In particular, it is a [[degree of a polynomial|second-degree]] polynomial equation, since the greatest power is two.\n\n==Solving the quadratic equation==\n[[File:Quadratic equation coefficients.png|thumb|right|300px|Figure 1. Plots of quadratic function {{nowrap|''y'' {{=}} ''ax''<sup>2</sup> + ''bx'' + ''c''}}, varying each coefficient separately while the other coefficients are fixed (at values ''a''&nbsp;=&nbsp;1, ''b''&nbsp;=&nbsp;0, ''c''&nbsp;=&nbsp;0)|<!-- Note: The unusual spellings in this alt text (for example, \"eh\" for the constant \"a\" ) is intended to aid enunciation by screen readers. Before changing any alt text, please test your changes in multiple screen readers. -->alt=Figure 1. Plots of the quadratic function, y = eh x squared plus b x plus c, varying each coefficient separately while the other coefficients are fixed at values eh = 1, b = 0, c = 0. The left plot illustrates varying c. When c equals 0, the vertex of the parabola representing the quadratic function is centered on the origin, and the parabola rises on both sides of the origin, opening to the top. When c is greater than zero, the parabola does not change in shape, but its vertex is raised above the origin. When c is less than zero, the vertex of the parabola is lowered below the origin. The center plot illustrates varying b. When b is less than zero, the parabola representing the quadratic function is unchanged in shape, but its vertex is shifted to the right of and below the origin. When b is greater than zero, its vertex is shifted to the left of and below the origin. The vertices of the family of curves created by varying b follow along a parabolic curve. The right plot illustrates varying eh. When eh is positive, the quadratic function is a parabola opening to the top. When eh is zero, the quadratic function is a horizontal straight line. When eh is negative, the quadratic function is a parabola opening to the bottom.]]\nA quadratic equation with [[real number|real]] or [[complex number|complex]] [[coefficients]] has two solutions, called ''roots''. These two solutions may or may not be distinct, and they may or may not be real.\n\n===Factoring by inspection===\nIt may be possible to express a quadratic equation {{math|''ax''<sup>2</sup> + ''bx'' + ''c'' {{=}} 0}} as a product {{math|(''px'' + ''q'')(''rx'' + ''s'') {{=}} 0}}. In some cases, it is possible, by simple inspection, to determine values of ''p'', ''q'', ''r,'' and ''s'' that make the two forms equivalent to one another. If the quadratic equation is written in the second form, then the \"Zero Factor Property\" states that the quadratic equation is satisfied if {{math|''px'' + ''q'' {{=}} 0}} or {{math|''rx'' + ''s'' {{=}} 0}}. Solving these two linear equations provides the roots of the quadratic.\n\nFor most students, factoring by inspection is the first method of solving quadratic equations to which they are exposed.<ref name=Washington2000>{{cite book|last=Washington|first=Allyn J.|title=Basic Technical Mathematics with Calculus, Seventh Edition|year=2000|publisher=Addison Wesley Longman, Inc.|isbn=978-0-201-35666-3}}</ref>{{rp|202&ndash;207}} If one is given a quadratic equation in the form {{math|''x''<sup>2</sup> + ''bx'' + ''c'' {{=}} 0}}, the sought factorization has the form {{math|(''x'' + ''q'')(''x'' + ''s'')}}, and one has to find two numbers {{math|''q''}} and {{math|''s''}} that add up to {{math| ''b''}} and whose product is {{math|''c''}} (this is sometimes called \"Vieta's rule\"<ref>{{citation|title=Numbers|series=Graduate Texts in Mathematics|volume=123|first1=Heinz-Dieter|last1=Ebbinghaus|first2=John H.|last2=Ewing|publisher=Springer|year=1991|isbn=9780387974972|page=77|url=https://books.google.com/books?id=OKcKowxXwKkC&pg=PA77}}.</ref> and is related to [[Vieta's formulas]]). As an example, {{math|''x''<sup>2</sup> + 5''x'' + 6}} factors as {{math|(''x'' + 3)(''x'' + 2)}}. The more general case where {{math|''a''}} does not equal {{math|1}} can require a considerable effort in trial and error guess-and-check, assuming that it can be factored at all by inspection.\n\nExcept for special cases such as where {{math|''b'' {{=}} 0}} or {{math|''c'' {{=}} 0}}, factoring by inspection only works for quadratic equations that have rational roots. This means that the great majority of quadratic equations that arise in practical applications cannot be solved by factoring by inspection.<ref name=Washington2000/>{{rp|207}}\n\n===Completing the square===\n{{Main|Completing the square}}\n[[File:Polynomialdeg2.svg|thumb|right|300px|Figure 2. For the [[quadratic function]] {{math|''y'' {{=}} ''x''<sup>2</sup> &minus; ''x'' &minus; 2}}, the points where the graph crosses the {{math|''x''}}-axis, {{math|''x'' {{=}} −1}} and {{math|''x'' {{=}} 2}}, are the solutions of the quadratic equation {{math|''x''<sup>2</sup> &minus; ''x'' &minus; 2 {{=}} 0}}.\n|alt=Figure 2 illustrates an x y plot of the quadratic function f of x equals x squared minus x minus 2. The x-coordinate of the points where the graph intersects the x-axis, x equals &minus;1 and x equals 2, are the solutions of the quadratic equation x squared minus x minus 2 equals zero.]]\nThe process of completing the square makes use of the algebraic identity\n:<math>x^2+2hx+h^2 = (x+h)^2,</math>\nwhich represents a well-defined [[algorithm]] that can be used to solve any quadratic equation.<ref name=Washington2000/>{{rp|207}} Starting with a quadratic equation in standard form, {{math|''ax''<sup>2</sup> + ''bx'' + ''c'' {{=}} 0}} \n#Divide each side by {{math|''a''}}, the coefficient of the squared term.\n#Subtract the constant term {{math|''c''/''a''}} from both sides.\n#Add the square of one-half of {{math|''b''/''a''}}, the coefficient of {{math|''x''}}, to both sides. This \"completes the square\", converting the left side into a perfect square.\n#Write the left side as a square and simplify the right side if necessary.\n#Produce two linear equations by equating the square root of the left side with the positive and negative square roots of the right side.\n#Solve the two linear equations.\n\nWe illustrate use of this algorithm by solving {{math|2''x''<sup>2</sup> + 4''x'' &minus; 4 {{=}} 0}}\n:<math>1) \\ x^2+2x-2=0</math>\n:<math>2) \\ x^2+2x=2</math>\n:<math>3) \\ x^2+2x+1=2+1</math>\n:<math>4) \\ \\left(x+1 \\right)^2=3</math>\n:<math>5) \\ x+1=\\pm\\sqrt{3}</math>\n:<math>6) \\ x=-1\\pm\\sqrt{3}</math>\n\nThe [[plus-minus sign|plus-minus symbol \"±\"]] indicates that both {{math|''x'' {{=}} &minus;1 + {{radic|3}}}} and {{math|''x'' {{=}} &minus;1 &minus; {{radic|3}}}} are solutions of the quadratic equation.<ref>{{Citation|last=Sterling|first=Mary Jane|title=Algebra I For Dummies|year=2010|publisher=Wiley Publishing|isbn=978-0-470-55964-2|url=https://books.google.com/books?id=2toggaqJMzEC&pg=PA219&dq=quadratic+formula#v=onepage&q=quadratic%20formula&f=false|page=219}}</ref>\n\n===Quadratic formula and its derivation===\n{{main|Quadratic formula}}\n[[Completing the square]] can be used to [[Quadratic formula#Derivation of the formula|derive a general formula]] for solving quadratic equations, called the quadratic formula.<ref>{{citation\n|title=Schaum's Outline of Theory and Problems of Elementary Algebra\n|first1=Barnett\n|last1=Rich\n|first2=Philip\n|last2=Schmidt\n|publisher=The McGraw-Hill Companies\n|year=2004\n|isbn=978-0-07-141083-0\n|url=https://books.google.com/books?id=8PRU9cTKprsC}}, [https://books.google.com/books?id=8PRU9cTKprsC&pg=PA291 Chapter 13 §4.4, p. 291]</ref> The [[mathematical proof]] will now be briefly summarized.<ref>Himonas, Alex.  ''[https://books.google.com/books?id=1Mg5u98BnEMC&q=%22left+as+an+exercise%22+and+%22quadratic+formula%22&dq=%22left+as+an+exercise%22+and+%22quadratic+formula%22&hl=en&sa=X&ei=6CJbUu2aFMylkQei6YGABA&ved=0CDMQ6AEwATgK Calculus for Business and Social Sciences]'', p. 64 (Richard Dennis Publications, 2001).</ref>  It can easily be seen, by [[polynomial expansion]], that the following equation is equivalent to the quadratic equation:\n:<math>\\left(x+\\frac{b}{2a}\\right)^2=\\frac{b^2-4ac}{4a^2}.</math>\nTaking the [[square root]] of both sides, and isolating {{math|''x''}}, gives:\n:<math>x=\\frac{-b\\pm\\sqrt{b^2-4ac\\ }}{2a}.</math>\n\nSome sources, particularly older ones, use alternative parameterizations of the quadratic equation such as {{math|''ax''<sup>2</sup> + 2''bx'' + ''c'' {{=}} ''0''}} or {{math|''ax''<sup>2</sup> &minus; 2''bx'' + ''c'' {{=}} 0}}&nbsp;,<ref name=\"kahan\">{{Citation |first=Willian |last=Kahan |title=On the Cost of Floating-Point Computation Without Extra-Precise Arithmetic |url=http://www.cs.berkeley.edu/~wkahan/Qdrtcs.pdf |date=November 20, 2004 |accessdate=2012-12-25}}</ref> where {{math|''b''}} has a magnitude one half of the more common one, possibly with opposite sign. These result in slightly different forms for the solution, but are otherwise equivalent.\n\nA number of [[Quadratic formula#Other derivations|alternative derivations]] can be found in the literature.  These proofs are simpler than the standard completing the square method, represent interesting applications of other frequently used techniques in algebra, or offer insight into other areas of mathematics.\n\nA lesser known quadratic formula, as used in [[Muller's method]] provides the same roots via the equation\n:<math>x = \\frac{2c}{-b \\pm \\sqrt {b^2-4ac\\ }}.</math> \nThis can be deduced from the standard quadratic formula by [[Vieta's formulas]], which assert that the product of the roots is {{math|''c''/''a''}}.\n\nOne property of this form is that it yields one valid root when {{math|''a'' {{=}} 0}}, while the other root contains division by zero, because when {{math|''a'' {{=}} 0}}, the quadratic equation becomes a linear equation, which has one root. By contrast, in this case, the more common formula has a division by zero for one root and an [[indeterminate form]] {{math|0/0}} for the other root. On the other hand, when {{math|''c'' {{=}} 0}}, the more common formula yields two correct roots whereas this form yields the zero root and an indeterminate form {{math|0/0}}.\n\n===Reduced quadratic equation===\nIt is sometimes convenient to reduce a quadratic equation so that its [[leading coefficient]] is one. This is done by dividing both sides by ''a'', which is always possible since ''a'' is non-zero.  This produces the ''reduced quadratic equation'':<ref>Alenit͡syn, Aleksandr and Butikov, Evgeniĭ. ''Concise Handbook of Mathematics and Physics'', p. 38 (CRC Press 1997)</ref>\n\n:<math>x^2+px+q=0,</math>\n\nwhere ''p'' = ''b''/''a'' and ''q'' = ''c''/''a''. This [[monic polynomial|monic equation]] has the same solutions as the original.\n\nThe quadratic formula for the solutions of the reduced quadratic equation, written in terms of its coefficients, is:\n:<math>x = \\frac{1}{2} \\left( - p \\pm \\sqrt{p^2 - 4q} \\right) ,</math>\nor equivalently:\n:<math>x = - \\frac{p}{2} \\pm \\sqrt{\\left(\\frac{p}{2}\\right)^2 - q}.</math>\n\n===Discriminant===\n[[File:Quadratic eq discriminant.svg|thumb|right|Figure 3. Discriminant signs|alt=Figure 3. This figure plots three quadratic functions on a single Cartesian plane graph to illustrate the effects of discriminant values. When the discriminant, delta, is positive, the parabola intersects the {{math|''x''}}-axis at two points. When delta is zero, the vertex of the parabola touches the {{math|''x''}}-axis at a single point. When delta is negative, the parabola does not intersect the {{math|''x''}}-axis at all.]]\nIn the quadratic formula, the expression underneath the square root sign is called the ''[[discriminant]]'' of the quadratic equation, and is often represented using an upper case {{math|''D''}} or an upper case Greek [[Delta (letter)|delta]]:<ref>'''Δ''' is the initial of the [[Greek language|Greek]] word '''Δ'''ιακρίνουσα, ''Diakrínousa'', discriminant.</ref>\n:<math>\\Delta = b^2 - 4ac.</math>\nA quadratic equation with ''real'' coefficients can have either one or two distinct real roots, or two distinct complex roots. In this case the discriminant determines the number and nature of the roots. There are three cases:\n\n*If the discriminant is positive, then there are two distinct roots\n::<math>\\frac{-b + \\sqrt {\\Delta}}{2a} \\quad\\text{and}\\quad \\frac{-b - \\sqrt {\\Delta}}{2a},</math>\n:both of which are real numbers. For quadratic equations with [[rational number|rational]] coefficients, if the discriminant is a [[square number]], then the roots are rational—in other cases they may be [[quadratic irrational]]s.\n\n*If the discriminant is zero, then there is exactly one [[real number|real]] root\n::<math>-\\frac{b}{2a},</math>\n:sometimes called a repeated or [[multiple root|double root]].\n\n*If the discriminant is negative, then there are no real roots. Rather, there are two distinct (non-real) [[complex number|complex]] roots<ref>{{cite book|last=Achatz|first=Thomas|last2=Anderson|first2=John G.|last3=McKenzie|first3=Kathleen|title=Technical Shop Mathematics|year=2005|publisher=Industrial Press|isbn=978-0-8311-3086-2|url=https://books.google.com/?id=YOdtemSmzQQC&pg=PA276&dq=quadratic+formula#v=onepage&q=quadratic%20formula&f=false|page=277}}</ref>\n::<math> -\\frac{b}{2a} + i \\frac{\\sqrt {-\\Delta}}{2a} \\quad\\text{and}\\quad -\\frac{b}{2a} - i \\frac{\\sqrt {-\\Delta}}{2a},</math>\n:which are [[complex conjugate]]s of each other. In these expressions {{math|''i''}} is the [[imaginary unit]].\n\nThus the roots are distinct if and only if the discriminant is non-zero, and the roots are real if and only if the discriminant is non-negative.\n\n===Geometric interpretation===\n{{quadratic_equation_graph_key_points.svg}}\n{{quadratic_function_graph_complex_roots.svg}}\nThe function {{math|''f''(''x'') {{=}} ''ax''<sup>2</sup> + ''bx'' + ''c''}} is the [[quadratic function]].<ref>{{cite book |last=Wharton |first=P. |title=Essentials of Edexcel Gcse Math/Higher |year=2006 |publisher=Lonsdale |isbn=978-1-905-129-78-2|url=https://books.google.com/?id=LMmKq-feEUoC&pg=PA63&dq=%22Quadratic+function%22+%22Quadratic+equation%22#v=onepage&q=%22Quadratic%20function%22%20%22Quadratic%20equation%22&f=false |page=63}}</ref> The graph of any quadratic function has the same general shape, which is called a [[parabola]]. The location and size of the parabola, and how it opens, depend on the values of {{math|''a''}}, {{math|''b''}}, and {{math|''c''}}. As shown in Figure&nbsp;1, if {{math|''a'' &gt; 0}}, the parabola has a minimum point and opens upward. If {{math|''a'' &lt; 0}}, the parabola has a maximum point and opens downward. The extreme point of the parabola, whether minimum or maximum, corresponds to its [[vertex (curve)|vertex]]. The ''{{math|x}}-coordinate'' of the vertex will be located at <math>\\scriptstyle x=\\tfrac{-b}{2a}</math>, and the ''{{math|y}}-coordinate'' of the vertex may be found by substituting this ''{{math|x}}-value'' into the function. The ''{{math|y}}-intercept'' is located at the point {{math|(0, ''c'')}}.\n\nThe solutions of the quadratic equation {{math|''ax''<sup>2</sup> + {{math|''bx''}} + {{math|''c''}} {{=}} 0}} correspond to the [[root of a function|roots]] of the function {{math|''f''(''x'') {{=}} ''ax''<sup>2</sup> + ''bx'' + ''c''}}, since they are the values of {{math|''x''}} for which {{math|''f''(''x'') {{=}} 0}}. As shown in Figure&nbsp;2, if {{math|''a''}}, {{math|''b''}}, and {{math|''c''}} are [[real numbers]] and the [[domain (mathematics)|domain]] of {{math|''f''}} is the set of real numbers, then the roots of {{math|''f''}} are exactly the {{math|''x''}}-[[coordinates]] of the points where the graph touches the {{math|''x''}}-axis. As shown in Figure&nbsp;3, if the discriminant is positive, the graph touches the [[x-axis|{{math|''x''}}-axis]] at two points; if zero, the graph touches at one point; and if negative, the graph does not touch the {{math|''x''}}-axis.\n\n===Quadratic factorization===\nThe term\n:<math>x - r</math>\nis a factor of the polynomial\n: <math>ax^2+bx+c</math>\nif and only if {{math|''r''}} is a [[root of a function|root]] of the quadratic equation\n: <math>ax^2+bx+c=0.</math>\nIt follows from the quadratic formula that\n: <math>ax^2+bx+c = a \\left( x - \\frac{-b + \\sqrt {b^2-4ac}}{2a} \\right) \\left( x - \\frac{-b - \\sqrt {b^2-4ac}}{2a} \\right).</math>\nIn the special case {{math|''b''<sup>2</sup> {{=}} 4''ac''}} where the quadratic has only one distinct root (''i.e.'' the discriminant is zero), the quadratic polynomial can be [[Factorization|factored]] as\n:<math>ax^2+bx+c = a \\left( x + \\frac{b}{2a} \\right)^2.</math>\n\n===Graphical solution===\n[[File:Graphical calculation of root of quadratic equation.png|240px|thumb|Figure 4. Graphing calculator computation of one of the two roots of the quadratic equation {{math|2''x''<sup>2</sup> + 4''x'' &minus; 4 {{=}} 0}}. Although the display shows only five significant figures of accuracy, the retrieved value of {{math|''xc''}} is 0.732050807569, accurate to twelve significant figures.]]\n[[Image:Visual.complex.root.finding.png|240px|right|thumb|A quadratic function without real root: {{nowrap|''y'' {{=}} (''x'' − 5)<sup>2</sup> + 9}}. The \"3\" is the imaginary part of the ''x''-intercept. The real part is the ''x''-coordinate of the vertex. Thus the roots are {{nowrap|5 ± 3''i''}}.]]\n\nThe solutions of the quadratic equation \n:<math>ax^2+bx+c=0</math>\nmay be deduced from the [[graph of a function|graph]] of the [[quadratic function]]\n:<math>y=ax^2+bx+c,</math>\nwhich is a [[parabola]].\n\nIf the parabola intersects the {{mvar|x}}-axis in two points, there are two real [[zero of a function|roots]], which are the {{mvar|x}}-coordinates of these two points (also called {{mvar|x}}-intercept).\n\nIf the parabola is [[tangent]] to the {{mvar|x}}-axis, there is a double root, which is the {{mvar|x}}-coordinate of the contact point between the graph and parabola.\n\nIf the parabola does not intersect the {{mvar|x}}-axis, there are two [[complex conjugate]] roots. Although these roots cannot be visualized on the graph, their [[complex number|real and imaginary parts]] can be.<ref name = \"Norton1984\">{{citation |title=Complex Roots Made Visible |author=Alec Norton, Benjamin Lotto |journal=The College Mathematics Journal |volume=15 |date=June 1984 |pages=248–249 |issue=3 |doi=10.2307/2686333}}</ref>\n\nLet {{mvar|h}} and {{mvar|k}} be respectively the {{mvar|x}}-coordinate and the {{mvar|y}}-coordinate of the vertex of the parabola (that is the point with maximal or minimal {{mvar|y}}-coordinate. The quadratic function may be rewritten\n: <math> y = a(x - h)^2 + k.</math>\nLet {{mvar|d}} be the distance between the point of {{mvar|y}}-coordinate {{math|2''k''}} on the axis of the parabola, and a point on the parabola with the same {{mvar|y}}-coordinate (see the figure; there are two such points, which give the same distance, because of the symmetry of the parabola). Then the real part of the roots is {{mvar|h}}, and their imaginary part are {{math|±''d''}}. That is, the roots are \n:<math>h+id \\quad  \\text{and} \\quad x-id,</math>\nor in the case of the example of the figure\n:<math>5+3i \\quad  \\text{and} \\quad 5-3i.</math>\n\n===Avoiding loss of significance===\nAlthough the quadratic formula provides an exact solution, the result is not exact if [[real number]]s are approximated during the computation, as usual in [[numerical analysis]], where real numbers are approximated by [[floating point number]]s (called \"reals\" in many [[programming language]]s). In this context, the quadratic formula is not completely [[numerical stability|stable]].\n\nThis occurs when the roots have different [[order of magnitude]], or, equivalently, when {{math|''b''<sup>2</sup>}} and  {{math|''b''<sup>2</sup> − 4''ac''}} are close in magnitude. In this case, the subtraction of two nearly equal numbers will cause [[loss of significance]] or [[catastrophic cancellation]] in the smaller root. To avoid this, the root that is smaller in magnitude, {{math|''r''}}, can be computed as <math>(c/a)/R</math> where {{math|''R''}} is the root that is bigger in magnitude.\n\nA second form of cancellation can occur between the terms {{math|''b''<sup>2</sup>}} and {{math|4''ac''}} of the discriminant, that is when the two roots are very close. This can lead to loss of up to half of correct significant figures in the roots.<ref name=\"kahan\"/><ref name=\"Higham2002\">{{Citation |first=Nicholas |last=Higham |title=Accuracy and Stability of Numerical Algorithms |edition=2nd |publisher=SIAM |year=2002 |isbn=978-0-89871-521-7 |page=10 }}</ref>\n\n==Examples and applications==\n[[File:La Jolla Cove cliff diving - 02.jpg|thumb|The trajectory of the cliff jumper is [[parabola|parabolic]] because horizontal displacement is a linear function of time <math>x=v_x t</math>, while vertical displacement is a quadratic function of time <math>y=\\tfrac{1}{2} at^2+v_y t+h</math>. As a result, the path follows quadratic equation <math>y=\\tfrac{a}{2v_x^2} x^2+\\tfrac{v_y}{v_x} x+h</math>, where <math>v_x</math> and <math>v_y</math> are horizontal and vertical components of the original velocity, {{math|a}} is [[Gravity of Earth|gravitational]] [[acceleration]] and {{math|h}} is original height. The {{math|a}} value should be considered negative here, as its direction (downwards) is opposite to the height measurement (upwards).]]\nThe [[golden ratio]] is found as the positive solution of the quadratic equation <math>x^2-x-1=0.</math>\n\nThe equations of the [[circle]] and the other [[conic sections]]&mdash;[[ellipse]]s, [[parabola]]s, and [[hyperbola]]s&mdash;are quadratic equations in two variables.\n\nGiven the [[cosine]] or [[sine]] of an angle, finding the cosine or sine of [[Bisection#Angle bisector|the angle that is half as large]] involves solving a quadratic equation.\n\nThe process of simplifying expressions involving the [[nested radical|square root of an expression involving the square root of another expression]] involves finding the two solutions of a quadratic equation.\n\n[[Descartes' theorem]] states that for every four kissing (mutually tangent) circles, their [[radius|radii]] satisfy a particular quadratic equation.\n\nThe equation given by [[Fuss' theorem]], giving the relation among the radius of a [[bicentric quadrilateral]]'s [[inscribed circle]], the radius of its [[circumscribed circle]], and the distance between the centers of those circles, can be expressed as a quadratic equation for which the distance between the two circles' centers in terms of their radii is one of the solutions. The other solution of the same equation in terms of the relevant radii gives the distance between the circumscribed circle's center and the center of the [[excircle]] of an [[ex-tangential quadrilateral]].\n\n==History==\n[[Babylonian mathematics|Babylonian mathematicians]], as early as 2000 BC (displayed on [[First Babylonian dynasty|Old Babylonian]] [[clay tablet]]s) could solve problems relating the areas and sides of rectangles. There is evidence dating this algorithm as far back as the [[Third Dynasty of Ur]].<ref name=Friberg2009>{{cite journal|last=Friberg|first=Jöran|title=A Geometric Algorithm with Solutions to Quadratic Equations in a Sumerian Juridical Document from Ur III Umma|journal=Cuneiform Digital Library Journal|year=2009|volume=3|url=http://cdli.ucla.edu/pubs/cdlj/2009/cdlj2009_003.html}}</ref> In modern notation, the problems typically involved solving a pair of simultaneous equations of the form:\n:<math> x+y=p,\\ \\ xy=q, </math>\nwhich is equivalent to the statement that {{mvar|x}} and {{mvar|y}} are the roots of the equation:<ref name=Stillwell2004>{{cite book |last=Stillwell |first=John |title=Mathematics and Its History (2nd ed.) |year=2004 |publisher=Springer |isbn=978-0-387-95336-6}}</ref>{{rp|86}}\n:<math>z^2+q=pz.</math>\n\nThe steps given by Babylonian scribes for solving the above rectangle problem, in terms of {{mvar|x}} and {{mvar|y}}, were as follows:\n#Compute half of ''p''.\n#Square the result.\n#Subtract ''q''.\n#Find the (positive) square root using a table of squares.\n#Add together the results of steps (1) and (4) to give {{math|''x''}}. In modern notation this means calculating <math>x = \\frac{p}{2} + \\sqrt{\\left(\\frac{p}{2}\\right)^2 - q}.</math>\n\nGeometric methods were used to solve quadratic equations in Babylonia, Egypt, Greece, China, and India. The Egyptian [[Berlin Papyrus 6619|Berlin Papyrus]], dating back to the [[Middle Kingdom of Egypt|Middle Kingdom]] (2050 BC to 1650 BC), contains the solution to a two-term quadratic equation.<ref>{{cite book|title=The Cambridge Ancient History Part 2 Early History of the Middle East|url=https://books.google.com/books?id=slR7SFScEnwC&pg=PA530|year=1971|publisher=Cambridge University Press|isbn=978-0-521-07791-0|page=530}}</ref> Babylonian mathematicians from circa 400 BC and [[Chinese mathematics|Chinese mathematicians]] from circa 200 BC used [[Dissection problem|geometric methods of dissection]] to solve quadratic equations with positive roots.<ref name=Henderson>{{cite web|last=Henderson|first=David W.|title=Geometric Solutions of Quadratic and Cubic Equations |publisher=Mathematics Department, Cornell University |url=http://www.math.cornell.edu/~dwh/papers/geomsolu/geomsolu.html|accessdate=28 April 2013}}</ref><ref name=Aitken>{{cite web|last=Aitken|first=Wayne|title=A Chinese Classic: The Nine Chapters|url=http://public.csusm.edu/aitken_html/m330/china/ninechapters.pdf|publisher=Mathematics Department, California State University|accessdate=28 April 2013}}</ref> Rules for quadratic equations were given in ''[[The Nine Chapters on the Mathematical Art]]'', a Chinese treatise on mathematics.<ref name=Aitken/><ref>{{cite book|last=Smith|first=David Eugene|title=History of Mathematics|url=https://books.google.com/books?id=uTytJGnTf1kC&pg=PA380|year=1958|publisher=Courier Dover Publications|isbn=978-0-486-20430-7|page=380}}</ref> These early geometric methods do not appear to have had a general formula. [[Euclid]], the [[Greek mathematics|Greek mathematician]], produced a more abstract geometrical method around 300 BC. With a purely geometric approach [[Pythagoras]] and Euclid created a general procedure to find solutions of the quadratic equation. In his work ''[[Arithmetica]]'', the Greek mathematician [[Diophantus]] solved the quadratic equation, but giving only one root, even when both roots were positive.<ref>{{cite book |title=History of Mathematics, Volume 1 |first1=David Eugene |last1=Smith |publisher=Courier Dover Publications |year=1958 |isbn=978-0-486-20429-1 |page=134 |url=https://books.google.com/books?id=12qdOZ0gsWoC}} [https://books.google.com/books?id=12qdOZ0gsWoC&pg=PA134 Extract of page 134]</ref>\n\nIn 628 AD, [[Brahmagupta]], an [[Indian mathematics|Indian mathematician]], gave the first explicit (although still not completely general) solution of the quadratic equation {{math|''ax''<sup>2</sup> + ''bx'' {{=}} ''c''}} as follows: \"To the absolute number multiplied by four times the [coefficient of the] square, add the square of the [coefficient of the] middle term; the square root of the same, less the [coefficient of the] middle term, being divided by twice the [coefficient of the] square is the value.\" (''Brahmasphutasiddhanta'', Colebrook translation, 1817, page 346)<ref name=Stillwell2004/>{{rp|87}} This is equivalent to:\n:<math>x = \\frac{\\sqrt{4ac+b^2}-b}{2a}.</math>\nThe ''[[Bakhshali Manuscript]]'' written in India in the 7th century AD contained an algebraic formula for solving quadratic equations, as well as quadratic [[indeterminate equation]]s (originally of type {{math|''ax''/''c'' {{=}} ''y''}}{{clarify|post-text=: this is linear, not quadratic|date=October 2017}}). [[Muhammad ibn Musa al-Khwarizmi]] ([[Persia]], 9th century), inspired by Brahmagupta,{{original research inline|date=October 2017}} developed a set of formulas that worked for positive solutions. Al-Khwarizmi goes further in providing a full solution to the general quadratic equation, accepting one or two numerical answers for every quadratic equation, while providing geometric [[Mathematical proof|proofs]] in the process.<ref name=Katz2007>{{Cite journal | last1 = Katz | first1 = V. J. | last2 = Barton | first2 = B. | doi = 10.1007/s10649-006-9023-7 | title = Stages in the History of Algebra with Implications for Teaching | journal = Educational Studies in Mathematics | volume = 66 | issue = 2 | pages = 185&ndash;201 | year = 2006 | pmid =  | pmc = }}</ref> He also described the method of completing the square and recognized that the [[discriminant]] must be positive,<ref name=Katz2007/><ref name=Boyer1991/>{{rp|230}} which was proven by his contemporary [['Abd al-Hamīd ibn Turk]] (Central Asia, 9th century) who gave geometric figures to prove that if the discriminant is negative, a quadratic equation has no solution.<ref name=Boyer1991>{{cite book|last=Boyer|first=Carl B.; [[Uta Merzbach|Uta C. Merzbach]], rev. editor|title=A History of Mathematics|year=1991|publisher=John Wiley & Sons, Inc.|isbn=978-0-471-54397-8}}</ref>{{rp|234}} While al-Khwarizmi himself did not accept negative solutions, later [[Mathematics in medieval Islam|Islamic mathematicians]] that succeeded him accepted negative solutions,<ref name=Katz2007/>{{rp|191}} as well as [[irrational number]]s as solutions.<ref>{{MacTutor|class=HistTopics|id=Arabic_mathematics|title=Arabic mathematics: forgotten brilliance?|year=1999}} \"Algebra was a unifying theory which allowed rational numbers, irrational numbers, geometrical magnitudes, etc., to all be treated as \"algebraic objects\".\"</ref> [[Abū Kāmil Shujā ibn Aslam]] (Egypt, 10th century) in particular was the first to accept irrational numbers (often in the form of a [[square root]], [[cube root]] or [[Nth root|fourth root]]) as solutions to quadratic equations or as [[coefficient]]s in an equation.<ref>Jacques Sesiano, \"Islamic mathematics\", p. 148, in {{citation|title=Mathematics Across Cultures: The History of Non-Western Mathematics|editor1-first=Helaine|editor1-last=Selin|editor1-link=Helaine Selin|editor2-first=Ubiratan|editor2-last=D'Ambrosio|editor2-link=Ubiratan D'Ambrosio|year=2000|publisher=[[Springer Science+Business Media|Springer]]|isbn=978-1-4020-0260-1}}</ref> The 9th century Indian mathematician [[Sridhara]] wrote down rules for solving quadratic equations.<ref>{{cite book|last=Smith|first=David Eugene|title=History of Mathematics|url=https://books.google.com/books?id=12qdOZ0gsWoC&pg=PA280|year=1958|publisher=Courier Dover Publications|isbn=978-0-486-20429-1|page=280}}</ref>\n\nThe Jewish mathematician [[Abraham bar Hiyya|Abraham bar Hiyya Ha-Nasi]] (12th century, Spain) authored the first European book to include the full solution to the general quadratic equation.<ref name=Livio2006>{{cite book |last=Livio |first=Mario |title=The Equation that Couldn't Be Solved |year=2006 |publisher=Simon & Schuster |isbn=978-0743258210 |url=https://books.google.com/?id=veQ9a3nixDUC&pg=PA62&lpg=PA62&dq=Abraham+bar+Hiyya+Ha-Nasi+quadratic}}</ref> His solution was largely based on Al-Khwarizmi's work.<ref name=Katz2007/> The writing of the Chinese mathematician [[Yang Hui]] (1238–1298 AD) is the first known one in which quadratic equations with negative coefficients of 'x' appear, although he attributes this to the earlier [[Liu Yi (mathematician)|Liu Yi]].<ref name=Ron>{{cite book|last=Ronan|first=Colin|title=The Shorter Science and Civilisation in China|url=https://books.google.com/books?id=XsMxmS7NyukC&pg=PA15|year=1985|publisher=Cambridge University Press|isbn=978-0-521-31536-4|page=15}}</ref> By 1545 [[Gerolamo Cardano]] compiled the works related to the quadratic equations. The quadratic formula covering all cases was first obtained by [[Simon Stevin]] in 1594.<ref>{{Citation |title=The Principal Works of Simon Stevin, Mathematics |volume=II-B |first1=D. J. |last1=Struik |first2=Simon |last2=Stevin |publisher=C. V. Swets & Zeitlinger |year=1958 |page=470 |url=http://www.dwc.knaw.nl/pub/bronnen/Simon_Stevin-%5bII_B%5d_The_Principal_Works_of_Simon_Stevin,_Mathematics.pdf}}</ref> In 1637 [[René Descartes]] published ''[[La Géométrie]]'' containing the quadratic formula in the form we know today. The first appearance of the general solution in the modern mathematical literature appeared in an 1896 paper by [[Henry Heaton]].<ref name=\"heaton-1896\">{{cite journal | last1 = Heaton | first1 = H | year = 1896 | title = A Method of Solving Quadratic Equations | journal = [[American Mathematical Monthly]] | volume = 3 | issue = 10| pages = 236–237 | jstor=2971099 | doi=10.2307/2971099}}</ref>\n\n==Advanced topics==\n\n===Alternative methods of root calculation===\n\n====Vieta's formulas====\n{{Main|Vieta's formulas}}\n[[File:Excel quadratic error.PNG|thumb|350px|Figure 5. Graph of the difference between Vieta's approximation for the smaller of the two roots of the quadratic equation {{math|''x''<sup>2</sup> + ''bx'' + ''c'' {{=}} 0}} compared with the value calculated using the quadratic formula. Vieta's approximation is inaccurate for small {{math|''b''}} but is accurate for large {{math|''b''}}. The direct evaluation using the quadratic formula is accurate for small {{math|''b''}} with roots of comparable value but experiences loss of significance errors for large {{math|''b''}} and widely spaced roots. The difference between Vieta's approximation ''versus'' the direct computation reaches a minimum at the large dots, and rounding causes squiggles in the curves beyond this minimum.|alt=Figure 5. Graph of the difference between Vieta's approximation for the smaller of the two roots of the quadratic equation x squared plus b x plus c equals zero compared with the value calculated using the quadratic formula. The difference is plotted as a function of b for two different values of c, c equals 4, and c equals 400,000. The graph is a log log graph, with the vertical axis, the difference, ranging from ten to the minus 13 at the bottom to ten to the minus 1 at the top. The horizontal axis, b, ranges from 10 at the left to ten to the eighth at the right. Vieta's approximation for the smaller root is not accurate for small b but is accurate for large b. The direct evaluation of the smaller root using the quadratic formula is accurate for small b with roots of comparable value, but experiences loss of significance errors for large b and widely spaced roots. When c equals 4, Vieta's approximation starts off poorly at the left, but gets better with larger b, the difference between Vieta's approximation and the quadratic formula reaching a minimum at approximately b equals ten to the fifth. Vieta's approximation and the quadratic formula then start diverging again because the quadratic formula experiences loss of significance error. When c equals four hundred thousand, the difference between Vieta's approximation and the quadratic formula reaches a minimum at approximately b equals ten to the seventh. The curves are both straight to the left of the minimum, indicating a simple monomial power relationship between the difference and b. Likewise, the curves are both approximately straight to the right of the minimum, indicating a power relationship, except that the straight lines have squiggles in them due to the loss of significance errors in the quadratic formula.]]\n\nVieta's formulas give a simple relation between the roots of a polynomial and its coefficients. In the case of the quadratic polynomial, they take the following form:\n:<math> x_1 + x_2 = -\\frac{b}{a} </math>\nand\n:<math> x_1 \\ x_2 = \\frac{c}{a}.</math>\nThese results follow immediately from the relation:\n:<math>\\left( x - x_1 \\right) \\ \\left( x-x_2 \\right ) = x^2 \\ - \\left( x_1+x_2 \\right)x +x_1 x_2 = 0,</math>\nwhich can be compared term by term with\n:<math> x^2 + (b/a)x +c/a = 0.</math>\nThe first formula above yields a convenient expression when graphing a quadratic function. Since the graph is symmetric with respect to a vertical line through the [[Quadratic function#Vertex|vertex]], when there are two real roots the vertex's {{math|''x''}}-coordinate is located at the average of the roots (or intercepts). Thus the {{math|''x''}}-coordinate of the vertex is given by the expression\n:<math> x_V = \\frac {x_1 + x_2} {2} = -\\frac{b}{2a}.</math>\nThe {{math|''y''}}-coordinate can be obtained by substituting the above result into the given quadratic equation, giving\n:<math> y_V = - \\frac{b^2}{4a} + c = - \\frac{ b^2 - 4ac} {4a}.</math>\n\nAs a practical matter, Vieta's formulas provide a useful method for finding the roots of a quadratic in the case where one root is much smaller than the other. If {{math|{{!}}&#8239;''x'' <sub>2</sub>{{!}} &lt;&lt; {{!}}&#8239;''x'' <sub>1</sub>{{!}}}}, then {{math|''x'' <sub>1</sub> + ''x'' <sub>2</sub> &asymp; ''x'' <sub>1</sub>}}, and we have the estimate:\n:<math> x_1 \\approx -\\frac{b}{a} .</math>\nThe second Vieta's formula then provides:\n:<math>x_2 = \\frac{c}{a \\ x_1} \\approx -\\frac{c}{b} .</math>\nThese formulas are much easier to evaluate than the quadratic formula under the condition of one large and one small root, because the quadratic formula evaluates the small root as the difference of two very nearly equal numbers (the case of large {{math|''b''}}), which causes [[round-off error]] in a numerical evaluation. Figure&nbsp;5 shows the difference between (i)&nbsp;a direct evaluation using the quadratic formula (accurate when the roots are near each other in value) and (ii)&nbsp;an evaluation based upon the above approximation of Vieta's formulas (accurate when the roots are widely spaced). As the linear coefficient {{math|''b''}} increases, initially the quadratic formula is accurate, and the approximate formula improves in accuracy, leading to a smaller difference between the methods as {{math|''b''}} increases. However, at some point the quadratic formula begins to lose accuracy because of round off error, while the approximate method continues to improve. Consequently, the difference between the methods begins to increase as the quadratic formula becomes worse and worse.\n\nThis situation arises commonly in amplifier design, where widely separated roots are desired to ensure a stable operation (see [[step response]]).\n\n====Trigonometric solution====\nIn the days before calculators, people would use [[mathematical table]]s—lists of numbers showing the results of calculation with varying arguments—to simplify and speed up computation. Tables of logarithms and trigonometric functions were common in math and science textbooks. Specialized tables were published for applications such as astronomy, celestial navigation and statistics. Methods of numerical approximation existed, called [[prosthaphaeresis]], that offered shortcuts around time-consuming operations such as multiplication and taking powers and roots.<ref name=Ballew2007>{{cite web|last=Ballew|first=Pat|title=Solving Quadratic Equations — By analytic and graphic methods; Including several methods you may never have seen|url=http://www.pballew.net/quadsol.pdf|accessdate=18 April 2013}}{{dead link|date=February 2019}}</ref> Astronomers, especially, were concerned with methods that could speed up the long series of computations involved in [[celestial mechanics]] calculations.\n\nIt is within this context that we may understand the development of means of solving quadratic equations by the aid of trigonometric substitution. Consider the following alternate form of the quadratic equation,\n\n'''[1]'''&nbsp;&nbsp; <math>ax^2 + bx \\pm c = 0 ,</math>\n\nwhere the sign of the ± symbol is chosen so that {{math|''a''}} and {{math|''c''}} may both be positive. By substituting\n\n'''[2]'''&nbsp;&nbsp; <math>x = \\sqrt{c/a} \\tan\\theta </math>\n\nand then multiplying through by {{math|cos<sup>2</sup>''&theta;''}}, we obtain\n\n'''[3]'''&nbsp;&nbsp; <math>\\sin^2\\theta + \\frac{b}{\\sqrt {ac}} \\sin\\theta  \\cos\\theta \\pm \\cos^2\\theta = 0 .</math>\n\nIntroducing functions of {{math|2''&theta;''}} and rearranging, we obtain\n\n'''[4]'''&nbsp;&nbsp; <math> \\tan 2 \\theta_n = + 2 \\frac{\\sqrt{ac}}{b} ,</math>\n\n'''[5]'''&nbsp;&nbsp; <math> \\sin 2 \\theta_p = - 2 \\frac{\\sqrt{ac}}{b} ,</math>\n\nwhere the subscripts {{math|''n''}} and {{math|''p''}} correspond, respectively, to the use of a negative or positive sign in equation '''[1]'''. Substituting the two values of {{math|''&theta;''<sub>n</sub>}} or {{math|''&theta;''<sub>p</sub>}} found from equations '''[4]''' or '''[5]''' into '''[2]''' gives the required roots of '''[1]'''. Complex roots occur in the solution based on equation '''[5]''' if the absolute value of {{math|sin 2''&theta;''<sub>p</sub>}} exceeds unity. The amount of effort involved in solving quadratic equations using this mixed trigonometric and logarithmic table look-up strategy was two-thirds the effort using logarithmic tables alone.<ref name=Seares1945>{{cite journal|last=Seares|first=F. H.|title=Trigonometric Solution of the Quadratic Equation|journal=Publications of the Astronomical Society of the Pacific |year=1945 |volume=57 |issue=339 |page=307&ndash;309 |doi=10.1086/125759 |bibcode=1945PASP...57..307S}}</ref> Calculating complex roots would require using a different trigonometric form.<ref name=Aude1938>{{cite journal |last=Aude |first=H. T. R. |title=The Solutions of the Quadratic Equation Obtained by the Aid of the Trigonometry |journal=National Mathematics Magazine |year=1938 |volume=13 |issue=3 |pages=118–121 |doi=10.2307/3028750 |jstor=3028750}}</ref>\n\n:To illustrate, let us assume we had available seven-place logarithm and trigonometric tables, and wished to solve the following to six-significant-figure accuracy:\n:::<math>4.16130x^2 + 9.15933x - 11.4207 = 0</math>\n#A seven-place lookup table might have only 100,000 entries, and computing intermediate results to seven places would generally require interpolation between adjacent entries. \n#<math>\\log a = 0.6192290,  \\log b = 0.9618637, \\log c  = 1.0576927</math>\n#<math>2 \\sqrt{ac}/b = 2 \\times 10^{(0.6192290 + 1.0576927)/2 - 0.9618637} = 1.505314 </math>\n#<math>\\theta = (\\tan^{-1}1.505314) / 2 = 28.20169^{\\circ} \\text{ or } -61.79831^{\\circ} </math>\n#<math>\\log | \\tan \\theta | = -0.2706462 \\text{ or } 0.2706462</math>\n#<math> \\log\\sqrt{c/a} = (1.0576927 - 0.6192290) / 2 = 0.2192318</math>\n#<math>x_1 = 10^{0.2192318 - 0.2706462} = 0.888353</math> (rounded to six significant figures)\n::<math>x_2 = -10^{0.2192318 + 0.2706462} = -3.08943</math>\n\n====Solution for complex roots in polar coordinates====\n\nIf the quadratic equation <math>ax^2+bx+c=0</math> with real coefficients has two complex roots&mdash;the case where <math>b^2-4ac<0,</math> requiring ''a'' and ''c'' to have the same sign as each other&mdash;then the solutions for the roots can be expressed in polar form as<ref>Simons, Stuart, \"Alternative approach to complex roots of real quadratic equations\", ''Mathematical Gazette'' 93, March 2009, 91–92.</ref>\n\n:<math>x_1, \\, x_2=r(\\cos \\theta \\pm i\\sin \\theta), </math>\n\nwhere <math>r=\\sqrt{\\tfrac{c}{a}}</math> and <math>\\theta =\\cos ^{-1}\\left(\\tfrac{-b}{2\\sqrt{ac}}\\right).</math>\n\n====Geometric solution====\n[[File:LillsQuadratic.svg|thumb|180px|Figure 6. Geometric solution of {{math|''ax''<sup>2</sup> + ''bx'' + ''c'' {{=}} 0}} using Lill's method. Solutions are −AX1/SA, −AX2/SA|alt=Figure 6. Geometric solution of eh x squared plus b x plus c = 0 using Lill's method. The geometric construction is as follows: Draw a trapezoid S Eh B C. Line S Eh of length eh is the vertical left side of the trapezoid. Line Eh B of length b is the horizontal bottom of the trapezoid. Line B C of length c is the vertical right side of the trapezoid. Line C S completes the trapezoid. From the midpoint of line C S, draw a circle passing through points C and S. Depending on the relative lengths of eh, b, and c, the circle may or may not intersect line Eh B. If it does, then the equation has a solution. If we call the intersection points X 1 and X 2, then the two solutions are given by negative Eh X 1 divided by S Eh, and negative Eh X 2 divided by S Eh.]]\n\nThe quadratic equation may be solved geometrically in a number of ways. One way is via [[Lill's method]]. The three coefficients {{math|''a''}}, {{math|''b''}}, {{math|''c''}} are drawn with right angles between them as in SA, AB, and BC in Figure&nbsp;6. A circle is drawn with the start and end point SC as a diameter. If this cuts the middle line AB of the three then the equation has a solution, and the solutions are given by negative of the distance along this line from A divided by the first coefficient {{math|''a''}} or SA. If {{math|''a''}} is {{math|1}} the coefficients may be read off directly. Thus the solutions in the diagram are &minus;AX1/SA and &minus;AX2/SA.<ref>{{Citation |title=Graphical Method for finding readily the Real Roots of Numerical Equations of Any Degree |first=William Herbert |last=Bixby |year=1879 |publisher=West Point N. Y.}}</ref>\n\n[[File:CarlyleCircle.svg|thumb|300px|left|Carlyle circle of the quadratic equation ''x''<sup>2</sup>&nbsp;&minus;&nbsp;''sx''&nbsp;+&nbsp;''p''&nbsp;=&nbsp;0.]]\nThe [[Carlyle circle]], named after [[Thomas Carlyle]], has the property that the solutions of the quadratic equation are the horizontal coordinates of the intersections of the circle with the [[horizontal axis]].<ref name=Wolfram>{{cite web|last=Weisstein|first=Eric W|title=Carlyle Circle|url=http://mathworld.wolfram.com/CarlyleCircle.html|work=From MathWorld—A Wolfram Web Resource|accessdate=21 May 2013}}</ref> Carlyle circles have been used to develop [[ruler-and-compass construction]]s of [[regular polygon]]s.\n\n====Alternative quadratic formula====\nThe general quadratic equation can be written in the standard form\n:<math>x^2-4ux+4v^2=0</math>\nwhere {{math|''u''}} and {{math|''v''}} are [[complex number]]s. Then the solutions can be written in the particularly symmetric form\n:<math>x_{1,2}=(\\sqrt{u+v}\\pm\\sqrt{u-v})^2</math>\nor equivalently\n:<math>x_{1,2}=(\\sqrt{u-v}\\pm\\sqrt{u+v})^2.</math>\nThe correctness of the formula can easily be verified by inserting the expression into the equation. This formula has the advantage that it is numerically more stable than the classical quadratic formula. It appeared in.<ref name=Hungerbühler2017>{{cite arxiv|last=Hungerbühler|first=Norbert|title=An alternative quadratic formula|year=2017|eprint=1702.05789|class=math.HO}}</ref> Problems originating from physics or geometry often present themselves in the homogeneous standard form on which the alternative formula is based. In particular, [[René Descartes]]' first example in ''[[La Géométrie]]'', at the very hour of birth of the quadratic formula, was geometric and of this particular homogeneous form.\n\n===Generalization of quadratic equation===\nThe formula and its derivation remain correct if the coefficients {{math|''a''}}, {{math|''b''}} and {{math|''c''}} are [[complex number]]s, or more generally members of any [[field (mathematics)|field]] whose [[characteristic (algebra)|characteristic]] is not {{math|2}}. (In a field of characteristic 2, the element {{math|2''a''}} is zero and it is impossible to divide by it.)\n\nThe symbol\n:<math>\\pm \\sqrt {b^2-4ac}</math>\nin the formula should be understood as \"either of the two elements whose square is {{math|''b''<sup>2</sup> &minus; 4''ac''}}, if such elements exist\". In some fields, some elements have no square roots and some have two; only zero has just one square root, except in fields of characteristic {{math|2}}. Even if a field does not contain a square root of some number, there is always a quadratic [[extension field]] which does, so the quadratic formula will always make sense as a formula in that extension field.\n\n====Characteristic 2====\nIn a field of characteristic {{math|2}}, the quadratic formula, which relies on {{math|2}} being a [[unit (ring theory)|unit]], does not hold. Consider the [[monic polynomial|monic]] quadratic polynomial\n:<math>x^{2} + bx + c</math>\nover a field of characteristic {{math|2}}. If {{math|''b'' {{=}} 0}}, then the solution reduces to extracting a square root, so the solution is\n:<math>x = \\sqrt{c}</math>\nand there is only one root since\n:<math>-\\sqrt{c} = -\\sqrt{c} + 2\\sqrt{c} = \\sqrt{c}.</math>\nIn summary,\n:<math>\\displaystyle x^{2} + c = (x + \\sqrt{c})^{2}.</math>\nSee [[quadratic residue]] for more information about extracting square roots in finite fields.\n\nIn the case that {{math|''b'' &ne; 0}}, there are two distinct roots, but if the polynomial is [[irreducible polynomial|irreducible]], they cannot be expressed in terms of square roots of numbers in the coefficient field. Instead, define the '''2-root''' {{math|''R''(''c'')}} of {{math|''c''}} to be a root of the polynomial {{math|''x''<sup>2</sup> + ''x'' + ''c''}}, an element of the [[splitting field]] of that polynomial. One verifies that {{math|''R''(''c'') + 1}} is also a root. In terms of the 2-root operation, the two roots of the (non-monic) quadratic {{math|''ax''<sup>2</sup> + ''bx'' + ''c''}} are\n:<math>\\frac{b}{a}R\\left(\\frac{ac}{b^2}\\right)</math>\nand\n:<math>\\frac{b}{a}\\left(R\\left(\\frac{ac}{b^2}\\right)+1\\right).</math>\n\nFor example, let {{math|''a''}} denote a multiplicative generator of the group of units of {{math|''F''<sub>4</sub>}}, the [[Galois field]] of order four (thus {{math|''a''}} and {{math|''a'' + 1}} are roots of {{math|''x''<sup>2</sup> + ''x'' + 1}} over {{math|''F''<sub>4</sub>}}. Because {{math|(''a'' + 1)<sup>2</sup> {{=}} ''a''}}, {{math|''a'' + 1}} is the unique solution of the quadratic equation {{math|''x''<sup>2</sup> + ''a'' {{=}} 0}}. On the other hand, the polynomial {{math|''x''<sup>2</sup> + ''ax'' + 1}} is irreducible over {{math|''F''<sub>4</sub>}}, but it splits over {{math|''F''<sub>16</sub>}}, where it has the two roots {{math|''ab''}} and {{math|''ab'' + ''a''}}, where {{math|''b''}} is a root of {{math|''x''<sup>2</sup> + ''x'' + ''a''}} in {{math|''F''<sub>16</sub>}}.\n\nThis is a special case of [[Artin–Schreier theory]].\n\n==See also==\n{{Div col|colwidth=25em}}\n* [[Chakravala method]]\n*[[Completing the square]]\n* [[Cubic function]]\n* [[Fundamental theorem of algebra]]\n* [[Linear equation]]\n* [[Parabola]]\n* [[Periodic points of complex quadratic mappings]]\n* [[Quadratic function]]\n* [[Quadratic polynomial]]\n* [[Quartic function]]\n* [[Quintic function]]\n* [[Solving quadratic equations with continued fractions]]\n{{colend}}\n\n==References==\n{{reflist|30em}}\n\n==External links==\n{{commons category}}\n* {{springer|title=Quadratic equation|id=p/q076050}}\n* {{MathWorld|title=Quadratic equations|urlname=QuadraticEquation}}\n* [http://plus.maths.org/issue29/features/quadratic/index-gifd.html 101 uses of a quadratic equation]\n* [http://plus.maths.org/issue30/features/quadratic/index-gifd.html 101 uses of a quadratic equation: Part II]\n{{Polynomials}}\n\n{{DEFAULTSORT:Quadratic Equation}}\n[[Category:Elementary algebra]]\n[[Category:Equations]]"
    },
    {
      "title": "Quadratic formula",
      "url": "https://en.wikipedia.org/wiki/Quadratic_formula",
      "text": "{{short description|solution of the quadratic equation}}\n[[File:Quadratic roots.svg|alt=Roots of a quadratic function|thumb|231x231px|A quadratic function with roots ''x''&nbsp;=&nbsp;1 and ''x''&nbsp;=&nbsp;4.]]\nIn [[elementary algebra]], the '''quadratic formula''' is the solution of the [[quadratic equation]]. There are other ways to solve the quadratic equation instead of using the quadratic formula, such as [[Factorization|factoring]], [[completing the square]], [[Graph of a function|graphing]] and others. Using the quadratic formula is often the most convenient way.\n\nThe general quadratic equation is\n\n:<math>ax^2+bx+c=0\\ \\ .</math>\n\nHere {{math|''x''}} represents an unknown, while {{math|''a''}}, {{math|''b''}}, and {{math|''c''}} are [[Constant term|constant]]s with {{math|''a''}} not equal to 0. One can verify that the quadratic formula satisfies the quadratic equation by inserting the former into the latter. With the above parameterization, the quadratic formula is: \n\n:<math display=\"block\">x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\\ \\ </math>\n\nEach of the solutions given by the quadratic formula is called a [[Zero of a function|root]] of the quadratic equation. Geometrically, these roots represent the {{math|''x''}} values at which ''any'' [[parabola]], explicitly given as {{math|''y'' {{=}} ''ax''{{sup|2}} + ''bx'' + ''c''}}, crosses the {{math|''x''}}-axis. As well as being a formula that will yield the zeros of any parabola, the quadratic formula will give the axis of symmetry of the parabola, and it can be used to immediately determine how many [[Real number|real]] zeros the quadratic equation has.\n\n==Derivations of the formula==\n===By using the 'completing the square' method===\nThe quadratic formula can be derived with a simple application of technique of [[completing the square]].<ref>{{citation\n|title=Schaum's Outline of Theory and Problems of Elementary Algebra\n|first1=Barnett\n|last1=Rich\n|first2=Philip\n|last2=Schmidt\n|publisher=The McGraw–Hill Companies\n|year=2004\n|isbn=0-07-141083-X\n|url=https://books.google.com/books?id=8PRU9cTKprsC}}, [https://books.google.com/books?id=8PRU9cTKprsC&pg=PA291 Chapter 13 §4.4, p. 291]</ref><ref>Li, Xuhui. ''An Investigation of Secondary School Algebra Teachers' Mathematical Knowledge for Teaching Algebraic Equation Solving'', p. 56 (ProQuest, 2007): \"The quadratic formula is the most general method for solving quadratic equations and is derived from another general method: completing the square.\"</ref><ref>Rockswold, Gary. ''College algebra and trigonometry and precalculus'', p. 178 (Addison Wesley, 2002).</ref><ref>Beckenbach, Edwin et al. ''Modern college algebra and trigonometry'', p. 81 (Wadsworth Pub. Co., 1986).</ref> The two derivations are as follows:\n\n====Method 1====\nDivide the quadratic equation by {{math|''a''}}, which is allowed because {{math|''a''}} is non-zero:\n\n:<math>x^2 + \\frac{b}{a} x + \\frac{c}{a}=0\\ \\ .</math>\n\nSubtract {{math|{{sfrac|''c''|''a''}}}} from both sides of the equation, yielding:\n:<math>x^2 + \\frac{b}{a} x= -\\frac{c}{a}\\ \\ .</math>\n\nThe quadratic equation is now in a form to which the method of [[completing the square]] can be applied. Thus, add a constant to both sides of the equation such that the left hand side becomes a complete square.\n\n:<math>x^2+\\frac{b}{a}x+\\left( \\frac{b}{2a} \\right)^2 =-\\frac{c}{a}+\\left( \\frac{b}{2a} \\right)^2\\ \\ ,</math>\n\nwhich produces:\n\n:<math>\\left(x+\\frac{b}{2a}\\right)^2=-\\frac{c}{a}+\\frac{b^2}{4a^2}\\ \\ .</math>\n\nAccordingly, after rearranging the terms on the right hand side to have a common denominator, we obtain:\n\n:<math>\\left(x+\\frac{b}{2a}\\right)^2=\\frac{b^2-4ac}{4a^2}\\ \\ .</math>\n\nThe square has thus been completed. Taking the [[square root]] of both sides yields the following equation:\n\n:<math>x+\\frac{b}{2a}=\\pm\\frac{\\sqrt{b^2-4ac\\ }}{2a}\\ \\ .</math>\n\nIsolating {{math|''x''}} gives the quadratic formula:\n\n:<math>x=\\frac{-b\\pm\\sqrt{b^2-4ac\\ }}{2a}\\ \\ .</math>\n\nThe [[plus-minus sign|plus-minus symbol \"±\"]] indicates that\n\n:<math> x_1=\\frac{-b + \\sqrt {b^2-4ac}}{2a}\\quad\\text{and}\\quad x_2=\\frac{-b - \\sqrt {b^2-4ac}}{2a}</math>\n\nboth are solutions of the quadratic equation.<ref>{{Citation|last=Sterling|first=Mary Jane|title=Algebra I For Dummies|year=2010|publisher=Wiley Publishing|isbn=978-0-470-55964-2|url=https://books.google.com/books?id=2toggaqJMzEC&pg=PA219&dq=quadratic+formula#v=onepage&q=quadratic%20formula&f=false|page=219}}</ref> There are many alternatives of this derivation with minor differences, mostly concerning the manipulation of {{math|''a''}}.\n\nThe quadratic formula may also be written as:\n\n:<math>x = \\frac{-b}{2a} \\pm \\sqrt{\\frac{b^2-4ac}{4a^2}} \\ \\ ,</math>\n\nwhich may be simplified to:\n\n:<math>x = -\\frac{b}{2a} \\pm \\sqrt{\\left(\\frac{b}{2a}\\right)^2-\\frac{c}{a}} \\ \\ .</math>\n\nThis version of the formula is convenient when complex roots are accepted. Then the expression outside the square root will be the real part and the square root expression will be the imaginary part. The expression inside the square root is a discriminant.\n\nSome sources, particularly older ones, use alternative parameterizations of the quadratic equation such as {{math|1=''ax''<sup>2</sup> − 2''bx'' + ''c'' = 0}}<ref name=\"kahan\">{{Citation |first=Willian |last=Kahan |title=On the Cost of Floating-Point Computation Without Extra-Precise Arithmetic |url=http://www.cs.berkeley.edu/~wkahan/Qdrtcs.pdf |date=November 20, 2004 |accessdate=2012-12-25}}</ref> or {{math|1=''ax''<sup>2</sup> + 2''bx'' + ''c'' = 0}},<ref>{{Citation |url=http://www.proofwiki.org/wiki/Quadratic_Formula |title=Quadratic Formula |journal=Proof Wiki |accessdate=2016-10-08}}</ref> where {{math|''b''}} has a magnitude one half of the more common one. These result in slightly different forms for the solution, but are otherwise equivalent.\n\nA lesser known quadratic formula, as used in [[Muller's method]], and which can be found from [[Vieta's formulas]], provides the same roots via the equation:\n\n:<math>x=\\frac{-2c}{b\\mp\\sqrt{b^2-4ac}} = \\frac{2c}{-b \\pm \\sqrt {b^2-4ac\\ }}\\ \\ .</math>\n\n====Method 2====\nThe majority of algebra texts published over the last several decades teach [[completing the square]] using the sequence presented earlier: (1) divide each side by {{math|''a''}} to make the equation monic, (2) rearrange, (3) then add {{math|({{sfrac|''b''|2''a''}})<sup>2</sup>}} to both sides to complete the square.\n\nAs pointed out by Larry Hoehn in 1975, completing the square can be accomplished by a different sequence that leads to a simpler sequence of intermediate terms: (1) multiply each side by {{math|4''a''}}, (2) rearrange, (3) then add {{math|''b''<sup>2</sup>}}.<ref name=Hoehn1975>{{cite journal |last=Hoehn |first=Larry |title=A More Elegant Method of Deriving the Quadratic Formula |journal=The Mathematics Teacher |year=1975 |volume=68 |issue=5 |page=442&ndash;443}}</ref>\n\nIn other words, the quadratic formula can be derived as follows: \n\n:<math>\\begin{align}\nax^2+bx+c &= 0 \\\\\n4 a^2 x^2 + 4abx + 4ac &= 0 \\\\\n4 a^2 x^2 + 4abx &= -4ac \\\\\n4 a^2 x^2 + 4abx + b^2 &= b^2 - 4ac \\\\\n(2ax + b)^2 &= b^2 - 4ac \\\\\n2ax + b &= \\pm \\sqrt{b^2-4ac} \\\\\n2ax &= -b \\pm \\sqrt{b^2-4ac} \\\\ \nx &= \\frac{-b\\pm\\sqrt{b^2-4ac }}{2a}\\ \\ .\n\\end{align}</math>\n\nThis actually represents an ancient derivation of the quadratic formula and was known to the Hindus at least as far back as 1025.<ref name=Smith1958>{{cite book|last=Smith|first=David E.|title=History of Mathematics, Vol. II|year=1958|publisher=Dover Publications|isbn=0486204308|page=446}}</ref> Compared with the derivation in standard usage, this alternate derivation is shorter, involves fewer computations with literal coefficients, avoids fractions until the last step, has simpler expressions, and uses simpler mathematics. As Hoehn states, \"it is easier 'to add the square of {{math|''b''}}' than it is 'to add the square of half the coefficient of the {{math|''x''}} term'\".<ref name=Hoehn1975/>\n\nMany ''alternative derivations'' of the quadratic formula are in the literature. These derivations may be simpler than the standard completing the square method and represent interesting applications of other algebraic techniques or may offer insight into other areas of mathematics.\n\n===By substitution===\nAnother technique is solution by [[substitution (algebra)|substitution]].<ref>Joseph J. Rotman. (2010). Advanced modern algebra (Vol. 114). American Mathematical Soc. Section 1.1</ref> In this technique, we substitute {{math|1=''x'' = ''y'' + ''m''}} into the quadratic to get:\n\n:<math>a(y+m)^2 + b(y+m) + c =0\\ \\ .</math>\n\nExpanding the result and then collecting the powers of {{math|''y''}} produces:\n\n:<math>ay^2 + y(2am + b) + \\left(am^2+bm+c\\right) = 0\\ \\ .</math>\n\nWe have not yet imposed a second condition on {{math|''y''}} and {{math|''m''}}, so we now choose {{math|''m''}} so that the middle term vanishes. That is, {{math|1=2''am'' + ''b'' = 0}} or {{math|''m'' {{=}} {{sfrac|−''b''|2''a''}}}}. Subtracting the constant term from both sides of the equation (to move it to the right hand side) and then dividing by {{math|''a''}} gives:\n\n:<math>y^2=\\frac{-\\left(am^2+bm+c\\right)}{a}\\ \\ .</math>\n\nSubstituting for {{math|''m''}} gives:\n\n:<math>y^2=\\frac{-\\left(\\frac{b^2}{4a}+\\frac{-b^2}{2a}+c\\right)}{a}=\\frac{b^2-4ac}{4a^2}\\ \\ .</math>\n\nTherefore,\n\n:<math>y=\\pm\\frac{\\sqrt{b^2-4ac}}{2a}</math>\n\nsubstituting {{math|1=''x'' = ''y'' + ''m'' = ''y'' −{{sfrac|''b''|2''a''}}}} provides the quadratic formula\n\n:<math>x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\\ \\ .</math>\n\n===By using algebraic identities===\nThe following method was used by many historical mathematicians:<ref>Debnath, L. (2009). The legacy of Leonhard Euler–a tricentennial tribute. International Journal of Mathematical Education in Science and Technology, 40(3), 353–388. Section 3.6</ref>\n\nLet the roots of the standard quadratic equation be {{math|''r''<sub>1</sub>}} and {{math|''r''<sub>2</sub>}}. The derivation starts by recalling the identity:\n\n:<math>(r_1 - r_2)^2 = (r_1 + r_2)^2 - 4r_1r_2\\ \\ .</math>\n\nTaking the square root on both sides, we get:\n\n:<math>r_1 - r_2 = \\pm\\sqrt{(r_1 + r_2)^2 - 4r_1r_2}\\ \\ .</math>\n\nSince the coefficient {{math|1=''a'' ≠ 0}}, we can divide the standard equation by {{math|''a''}} to obtain a quadratic polynomial having the same roots. Namely,\n\n:<math> x^2 + \\frac{b}{a}x + \\frac{c}{a} = (x - r_1)(x-r_2) = x^2 - (r_1 + r_2)x + r_1 r_2\\ \\ .</math>\n\nFrom this we can see that the sum of the roots of the standard quadratic equation is given by {{math|−{{sfrac|''b''|''a''}}}}, and the product of those roots is given by {{math|{{sfrac|''c''|''a''}}}}.\n<!--\nWe know that the sum of roots of the standard quadratic equation is given by {{math|−{{sfrac|''b''|''a''}}}}:\n\n:<math>\\frac{-b + \\sqrt{b^2 - 4ac}}{2a} + \\frac{-b - \\sqrt{b^2 - 4ac}}{2a} = \\frac{-b - b + \\sqrt{b^2-4ac} - \\sqrt{b^2-4ac}}{2a} = \\frac{-2b}{2a} = -\\frac{b}{a}</math>\\ \\ .\n\nAdditionally, the product is given by {{math|{{sfrac|''c''|''a''}}}}:\n\n:<math>\\frac{-b + \\sqrt{b^2 - 4ac}}{2a} \\cdot \\frac{-b - \\sqrt{b^2 - 4ac}}{2a} = \\frac{(-b + \\sqrt{b^2 - 4ac})(-b - \\sqrt{b^2 - 4ac})}{4a^2} = \\frac{b^2 - (b^2 - 4ac)}{4a^2} = \\frac{4ac}{4a^2} = \\frac{c}{a}\\ \\.</math>\n-->\nHence the identity can be rewritten as:\n\n:<math>r_1 - r_2 = \\pm\\sqrt{\\left(-\\frac{b}{a}\\right)^2-4\\frac{c}{a}} = \\pm\\sqrt{\\frac{b^2}{a^2} - \\frac{4ac}{a^2}} = \\pm\\frac{\\sqrt{b^2-4ac}}{a}\\ \\ .</math>\n\nNow, \n\n:<math>r_1 = \\frac{(r_1 + r_2) + (r_1 - r_2)}{2} = \\frac{-\\frac{b}{a} \\pm \\frac{\\sqrt{b^2 - 4ac}}{a}}{2} = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\\ \\ .</math>\n\nSince {{math|''r''{{sub|2}} {{=}} −''r''{{sub|1}} − {{sfrac|''b''|''a''}}}}, if we take\n\n:<math>r_1 = \\frac{-b + \\sqrt{b^2 - 4ac}}{2a}</math>\n\nthen we obtain\n\n:<math>r_2 = \\frac{-b - \\sqrt{b^2 - 4ac}}{2a}\\ \\ ;</math>\n\nand if we instead take\n\n:<math>r_1 = \\frac{-b - \\sqrt{b^2 - 4ac}}{2a}</math>\n\nthen we calculate that\n\n:<math>r_2 = \\frac{-b + \\sqrt{b^2 - 4ac}}{2a}\\ \\ .</math>\n\nCombining these results by using the standard shorthand ±, we have that the solutions of the quadratic equation are given by:\n\n:<math> x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\\ \\ .</math>\n\n===By Lagrange resolvents===\n{{details|Lagrange resolvents}}\nAn alternative way of deriving the quadratic formula is via the method of [[Lagrange resolvents]],<ref name=Clark>Clark, A. (1984). ''Elements of abstract algebra''. Courier Corporation. p. 146.</ref> which is an early part of [[Galois theory]].<ref name=\"efei\">{{citation\n|title=Elliptic functions and elliptic integrals\n|first1=Viktor\n|last1=Prasolov\n|first2=Yuri\n|last2=Solovyev\n|publisher=AMS Bookstore\n|year=1997\n|isbn=978-0-8218-0587-9\n|url=https://books.google.com/books?id=fcp9IiZd3tQC\n}}, [https://books.google.com/books?id=fcp9IiZd3tQC&pg=PA134#PPA134,M1 §6.2, p. 134]</ref>\nThis method can be generalized to give the roots of [[cubic polynomial]]s and [[quartic polynomial]]s, and leads to Galois theory, which allows one to understand the solution of algebraic equations of any degree in terms of the [[symmetry group]] of their roots, the [[Galois group]].\n\nThis approach focuses on the ''roots'' more than on rearranging the original equation. Given a monic quadratic polynomial\n\n:<math>x^2+px+q\\ \\ ,</math> \n\nassume that it factors as\n\n:<math>x^2+px+q=(x-\\alpha)(x-\\beta)\\ \\ ,</math>\n\nExpanding yields\n\n:<math>x^2+px+q=x^2-(\\alpha+\\beta)x+\\alpha \\beta\\ \\ ,</math>\n\nwhere {{math|''p'' {{=}} −(''α'' + ''β'')}} and {{math|''q'' {{=}} ''αβ''}}.\n\nSince the order of multiplication does not matter, one can switch {{math|''α''}} and {{math|''β''}} and the values of {{math|''p''}} and {{math|''q''}} will not change: one can say that {{math|''p''}} and {{math|''q''}} are [[symmetric polynomials]] in {{math|''α''}} and {{math|''β''}}. In fact, they are the [[elementary symmetric polynomials]] – any symmetric polynomial in {{math|''α''}} and {{math|''β''}} can be expressed in terms of {{math|''α'' + ''β''}} and {{math|''αβ''}} The Galois theory approach to analyzing and solving polynomials is: given the coefficients of a polynomial, which are symmetric functions in the roots, can one \"break the symmetry\" and recover the roots? Thus solving a polynomial of degree {{math|''n''}} is related to the ways of rearranging (\"permuting\") {{math|''n''}} terms, which is called the [[symmetric group]] on {{math|''n''}} letters, and denoted {{math|''S{{sub|n}}''}}. For the quadratic polynomial, the only way to rearrange two terms is to swap them (\"[[Transposition (mathematics)|transpose]]\" them), and thus solving a quadratic polynomial is simple.\n\nTo find the roots {{math|''α''}} and {{math|''β''}}, consider their sum and difference:\n\n:<math>\\begin{align}\nr_1 &= \\alpha + \\beta\\\\\nr_2 &= \\alpha - \\beta\\ \\ .\n\\end{align}</math>\n\nThese are called the ''Lagrange resolvents'' of the polynomial; notice that one of these depends on the order of the roots, which is the key point. One can recover the roots from the resolvents by inverting the above equations:\n\n:<math>\\begin{align}\n\\alpha &= \\textstyle{\\frac{1}{2}}\\left(r_1+r_2\\right)\\\\\n\\beta  &= \\textstyle{\\frac{1}{2}}\\left(r_1-r_2\\right)\\ \\ .\n\\end{align}</math>\n\nThus, solving for the resolvents gives the original roots.\n\nNow {{math|''r''{{sub|1}} {{=}} ''α'' + ''β''}} is a symmetric function in {{math|''α''}} and {{math|''β''}}, so it can be expressed in terms of {{math|''p''}} and {{math|''q''}}, and in fact {{math|''r''{{sub|1}} {{=}} −''p''}} as noted above. But {{math|''r''{{sub|2}} {{=}} ''α'' − ''β''}} is not symmetric, since switching {{math|''α''}} and {{math|''β''}} yields {{math|−''r''{{sub|2}} {{=}} ''β'' − ''α''}} (formally, this is termed a [[Group action (mathematics)|group action]] of the symmetric group of the roots). Since {{math|''r''{{sub|2}}}} is not symmetric, it cannot be expressed in terms of the coefficients {{math|''p''}} and {{math|''q''}}, as these are symmetric in the roots and thus so is any polynomial expression involving them. Changing the order of the roots only changes {{math|''r''{{sub|2}}}} by a factor of −1, and thus the square {{math|''r''{{sub|2}}{{sup|2}} {{=}} (''α'' − ''β''){{sup|2}}}} is symmetric in the roots, and thus expressible in terms of {{math|''p''}} and {{math|''q''}}. Using the equation\n\n:<math>(\\alpha - \\beta)^2 = (\\alpha + \\beta)^2 - 4\\alpha\\beta\\ \\ </math>\n\nyields\n\n:<math>r_2^2 = p^2 - 4q\\ \\ </math>\n\nand thus\n\n:<math>r_2 = \\pm \\sqrt{p^2 - 4q}\\ \\ </math>\n\nIf one takes the positive root, breaking symmetry, one obtains:\n\n:<math>\\begin{align}\nr_1 &= -p\\\\\nr_2 &= \\sqrt{p^2 - 4q}\n\\end{align}</math>\nand thus\n:<math>\\begin{align}\n\\alpha &= \\tfrac12\\left(-p+\\sqrt{p^2 - 4q}\\right)\\\\\n\\beta  &= \\tfrac12\\left(-p-\\sqrt{p^2 - 4q}\\right)\\ \\ .\n\\end{align}</math>\nThus the roots are\n:<math>\\textstyle{\\frac{1}{2}}\\left(-p \\pm \\sqrt{p^2 - 4q}\\right)</math>\nwhich is the quadratic formula. Substituting {{math|''p'' {{=}} {{sfrac|''b''|''a''}}, ''q'' {{=}} {{sfrac|''c''|''a''}}}} yields the usual form for when a quadratic is not monic. The resolvents can be recognized as {{math|{{sfrac|''r''{{sub|1}}|2}} {{=}} {{sfrac|−''p''|2}} {{=}} {{sfrac|−''b''|2''a''}}}} being the vertex, and {{math|''r''{{sub|2}}{{sup|2}} {{=}} ''p''{{sup|2}} − 4''q''}} is the discriminant (of a monic polynomial).\n\nA similar but more complicated method works for [[cubic equation]]s, where one has three resolvents and a quadratic equation (the \"resolving polynomial\") relating {{math|''r''{{sub|2}}}} and {{math|''r''{{sub|3}}}}, which one can solve by the quadratic equation, and similarly for a [[quartic equation]] (degree 4), whose resolving polynomial is a cubic, which can in turn be solved.<ref name=Clark/> The same method for a [[quintic equation]] yields a polynomial of degree 24, which does not simplify the problem, and, in fact, solutions to quintic equations in general cannot be expressed using only roots.\n\n===By extrema===\nKnowing the value of {{math|''x''}} in the functional extreme point makes it possible to solve only for the increase (or decrease) needed in {{math|''x''}} to solve the quadratic equation. This method first uses differentiation to find the {{math|''x''}} value at the extremum, called {{math|''x''<sub>ext</sub>}}. We then solve for the value, {{math|''q''}}, that ensures that {{math|''f''(''x''<sub>ext</sub> + ''q'') {{=}} 0}}. While this may not be the most intuitive method, it ensures that the mathematics is straightforward.\n\n:<math>\\begin{align}\nf(x) & =ax^2+bx+c \\\\[5pt]\n\\frac{\\partial f}{\\partial x} & =2ax+b\\ \\ .\n\\end{align}</math>\n\nSetting the above differential to zero will give us the extrema of the quadratic function\n:<math>x_\\text{ext}=\\frac{-b}{2a}\\ \\ .</math>\n\nWe define {{math|''q''}} as follows:\n\n:<math> q=x_0-x_\\text{ext}\\ \\ ,</math>\n\nHere {{math|''x''<sub>0</sub>}} is the value of {{math|''x''}} that solves the quadratic equation. The sum of {{math|''x''<sub>ext</sub>}} and the variable of interest, {{math|''q''}}, is plugged in to the quadratic equation\n\n:<math>\\begin{align}\na\\left(\\frac{-b}{2a}+q\\right)^2+b\\left(\\frac{-b}{2a}+q\\right)+c&=0 \\\\[5pt]\n\\left(\\frac{-b}{2a}+q\\right)^2+\\frac{b}{a}\\left(\\frac{-b}{2a} + q \\right) +\\frac{c}{a}&=0 &&(a\\neq 0)\\\\[5pt]\n\\frac{b^2}{4a^2}+q^2-\\frac{bq}{a}-\\frac{b^2}{2a^2}+\\frac {bq} a + \\frac{c}{a}&=0 \\\\[5pt]\n\\frac{-b^2}{4a^2}+q^2+\\frac{c}{a}&=0 \\\\[5pt]\nq^2&=\\frac{b^2-4ac}{4a^2} \\\\[5pt]\nq&=\\frac{\\pm\\sqrt{b^2-4ac}}{2a}\\ \\ .\n\\end{align}</math>\n\nThe value of {{math|''x''}} in the extreme point is then added to both sides of the equation\n:<math> x_0=\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}\\ \\ \n.</math>\n\nThis gives the quadratic formula. This way one avoids the technique of completing the square and much more complicated math is not needed. Note this solution is very similar to solving deriving the formula by substitution.\n\n===By splitting into real and imaginary parts===\n{{quadratic_function_graph_complex_roots.svg}}\nConsider the equation\n:<math> a z^2 + b z + c = 0 </math>\nwhere <math>z = x + iy</math> is a complex number and where ''a'', ''b'', and ''c'' are real numbers. Then\n:<math>\\begin{align}\n a (x + iy)^2 + b (x + iy) + c &= 0\\ \\ ,\n\\ \\ a (x^2 - y^2 + i2xy) + b(x + iy) + c = 0\\ \\ . \n\\end{align}\n</math>\nThis splits into two equations, the real part:\n:<math> a(x^2 - y^2) + bx + c = 0</math>\nand the imaginary part:\n:<math> 2axy + by = 0\\ \\ .</math>\n\nAssuming that <math>y \\ne 0</math> then divide the second equation by ''y'':\n:<math> 2ax + b = 0</math>\nand solve for ''x'':\n:<math>x = {-b \\over 2a}\\ \\ . </math>\nSubstitute this value for ''x'' into the first equation and solve for ''y'':\n:<math>\\begin{align}\na \\Big( {b^2 \\over 4 a^2} - y^2 \\Big) - {b^2 \\over 2 a} + c &= 0\\\\\n {b^2 \\over 4a} - ay^2 - {2b^2 \\over 4a} + c &= 0\\\\\n {-b^2 \\over 4a} - ay^2 + c &= 0\\\\\n ay^2 + {b^2 \\over 4a} - c &= 0\\\\\n y^2 &= {c \\over a} - {b^2 \\over 4a^2}\\\\\n y &= \\pm \\sqrt{{c\\over a} - {b^2 \\over 4a^2}} = {\\pm \\sqrt{ca - {b^2 \\over 4}} \\over a} = {\\pm \\sqrt{4ca - b^2} \\over 2a}\\ \\ .\n\\end{align}</math>\n\nSince <math>z = x + iy</math>, then\n:<math>\\begin{align}\n z &= {-b \\over 2a} \\pm {i\\sqrt{4ac - b^2} \\over 2a}\\\\[5pt]\n z &= {-b \\over 2a} \\pm {\\sqrt{b^2 - 4ac} \\over 2a} = {-b \\pm \\sqrt{b^2 - 4ac} \\over 2a}\\ \\ .\n\\end{align}</math>\nEven though ''y'' was assumed to be non-zero, this last formula works for any roots of the original equation, whereas assuming that <math>y = 0</math> turns out to be of not much help (trivial and circular).\n\n==Historical development==\nThe earliest methods for solving quadratic equations were geometric. Babylonian cuneiform tablets contain problems reducible to solving quadratic equations.<ref>{{cite book|last=Irving|first=Ron|title=Beyond the Quadratic Formula|url=https://books.google.com/books?id=CV_UInCRO38C&pg=PA39|year=2013|publisher=MAA|isbn=978-0-88385-783-0|page=34}}</ref> The Egyptian [[Berlin Papyrus 6619|Berlin Papyrus]], dating back to the [[Middle Kingdom of Egypt|Middle Kingdom]] (2050 BC to 1650 BC), contains the solution to a two-term quadratic equation.<ref>{{cite book|title=The Cambridge Ancient History Part 2 Early History of the Middle East|url=https://books.google.com/books?id=slR7SFScEnwC&pg=PA530|year=1971|publisher=Cambridge University Press|isbn=978-0-521-07791-0|page=530}}</ref>\n[[File:Scuola di atene 07.jpg|thumb|250px|[[Euclid]] in [[Raphael|Raphael Sanzio]]'s ''[[The School of Athens|School of Athens]]'']]\nThe Greek mathematician [[Euclid]] (circa 300 BC) used geometric methods to solve quadratic equations in Book 2 of his ''[[Euclid's Elements|Elements]]'', an influential mathematical treatise.<ref name=quad/> Rules for quadratic equations appear in the Chinese ''[[The Nine Chapters on the Mathematical Art]]'' circa 200 BC.<ref name=Aitken>{{cite web|last=Aitken|first=Wayne|title=A Chinese Classic: The Nine Chapters|url=http://public.csusm.edu/aitken_html/m330/china/ninechapters.pdf|publisher=Mathematics Department, California State University|accessdate=28 April 2013}}</ref><ref>{{cite book|last=Smith|first=David Eugene|title=History of Mathematics|url=https://books.google.com/books?id=uTytJGnTf1kC&pg=PA380|year=1958|publisher=Courier Dover Publications|isbn=978-0-486-20430-7|page=380}}</ref> In his work ''[[Arithmetica]]'', the Greek mathematician [[Diophantus]] (circa 250 BC) solved quadratic equations with a method more recognizably algebraic than the geometric algebra of Euclid.<ref name=quad>{{cite book|last=Irving|first=Ron|title=Beyond the Quadratic Formula|url=https://books.google.com/books?id=CV_UInCRO38C&pg=PA39|year=2013|publisher=MAA|isbn=978-0-88385-783-0|page=39}}</ref> His solution gives only one root, even when both roots are positive.<ref>{{cite book|last=Smith|first=David Eugene|title=History of Mathematics|url=https://books.google.com/?id=12qdOZ0gsWoC&pg=PA134&dq#v=onepage&q=&f=false|year=1958|publisher=Courier Dover Publications|isbn=0-486-20429-4|page=134}}</ref>\n\nThe Indian mathematician [[Brahmagupta]] (597–668 AD) explicitly described the quadratic formula in his treatise ''[[Brāhmasphuṭasiddhānta]]'' published in 628 AD,<ref name=Bradley>Bradley, Michael. ''The Birth of Mathematics: Ancient Times to 1300'', p. 86 (Infobase Publishing 2006).</ref> but written in words instead of symbols.<ref>Mackenzie, Dana. ''The Universe in Zero Words: The Story of Mathematics as Told through Equations'', p. 61 (Princeton University Press, 2012).</ref> His solution of the quadratic equation {{math|1=''ax''<sup>2</sup> + ''bx'' = ''c''}} was as follows: \"To the absolute number multiplied by four times the [coefficient of the] square, add the square of the [coefficient of the] middle term; the square root of the same, less the [coefficient of the] middle term, being divided by twice the [coefficient of the] square is the value.\"<ref name=Stillwell2004>{{cite book |last=Stillwell |first=John |title=Mathematics and Its History (2nd ed.) |year=2004 |publisher=Springer |isbn=0-387-95336-1|page=87}}</ref>\nThis is equivalent to:\n\n:<math>x = \\frac{\\sqrt{4ac+b^2}-b}{2a}\\ \\ .</math>\n\nThe 9th-century Persian mathematician [[Muḥammad ibn Mūsā al-Khwārizmī]] solved quadratic equations algebraically.<ref>{{cite book|last=Irving|first=Ron|title=Beyond the Quadratic Formula|url=https://books.google.com/books?id=CV_UInCRO38C&pg=PA39|year=2013|publisher=MAA|isbn=978-0-88385-783-0|page=42}}</ref> The quadratic formula covering all cases was first obtained by [[Simon Stevin]] in 1594.<ref>{{Citation |title=The Principal Works of Simon Stevin, Mathematics |volume=II-B |first1=D. J. |last1=Struik |first2=Simon |last2=Stevin |publisher=C. V. Swets & Zeitlinger |year=1958 |page=470 |url=http://www.dwc.knaw.nl/pub/bronnen/Simon_Stevin-%5bII_B%5d_The_Principal_Works_of_Simon_Stevin,_Mathematics.pdf}}</ref> In 1637 [[René Descartes]] published ''[[La Géométrie]]'' containing special cases of the quadratic formula in the form we know today.{{fact|date=January 2017}} The first appearance of the general solution in the modern mathematical literature appeared in an 1896 paper by [[Henry Heaton]].<ref name=\"heaton-1896\">Heaton, H. (1896) ''[https://www.jstor.org/stable/info/2971099 A Method of Solving Quadratic Equations]'', [[American Mathematical Monthly]] '''3'''(10), 236–237.</ref>\n\n==Significant uses==\n===Geometrical significance===\n{{quadratic_equation_graph_key_points.svg}}\nIn terms of coordinate geometry, a parabola is a curve whose {{math|(''x'', ''y'')}}-coordinates are described by a second-degree polynomial, i.e. any equation of the form:\n\n:<math>y =p(x) = a_2x^2 + a_1x +a_0\\ \\ ,</math>\n\nwhere {{math|''p''}} represents the polynomial of degree 2 and {{math|''a''<sub>0</sub>, ''a''<sub>1</sub>,}} and {{math|1=''a''<sub>2</sub> ≠ 0}} are constant coefficients whose subscripts correspond to their respective term's degree. The geometrical interpretation of the quadratic formula is that it defines the points on the {{math|''x''}}-axis where the parabola will cross the axis. Additionally, if the quadratic formula was looked at as two terms,\n\n:<math>x = \\frac{-b\\pm\\sqrt{b^2-4ac\\ }}{2a}=-\\frac{b}{2a} \\pm\\frac{\\sqrt{b^2-4ac\\ }}{2a}</math>\n\nthe [[axis of symmetry]] appears as the line {{math|1=''x'' = −{{sfrac|''b''|2''a''}}}}. The other term, {{math|{{sfrac|{{sqrt|''b''{{sup|2}} − 4''ac''}}|2''a''}}}}, gives the distance the zeros are away from the axis of symmetry, where the plus sign represents the distance to the right, and the minus sign represents the distance to the left. \n\nIf this distance term were to decrease to zero, the value of the axis of symmetry would be the {{math|''x''}} value of the only zero, that is, there is only one possible solution to the quadratic equation. Algebraically, this means that {{math|{{sqrt|''b''{{sup|2}} − 4''ac''}} {{=}} 0}}, or simply {{math|''b''{{sup|2}} − 4''ac'' {{=}} 0}} (where the left-hand side is referred to as the ''discriminant''). This is one of three cases, where the discriminant indicates how many zeros the parabola will have. If the discriminant is positive, the distance would be non-zero, and there will be two solutions. However, there is also the case where the discriminant is less than zero, and this indicates the distance will be ''imaginary''{{snd}} or some multiple of the complex unit {{math|''i''}}, where {{math|''i'' {{=}} {{sqrt|−1}}}}{{snd}} and the parabola's zeros will be [[complex number]]s. The complex roots will be [[complex conjugate]]s, where the real part of the complex roots will be the value of the axis of symmetry. There will be no real values of {{math|''x''}} where the parabola crosses the {{math|''x''}}-axis.\n\n===Dimensional analysis===\nIf the constants {{math|''a''}}, {{math|''b''}}, and/or {{math|''c''}} are not [[unitless]], then the units of {{math|''x''}} must be equal to the units of {{math|{{sfrac|''b''|''a''}}}}, due to the requirement that {{math|''ax''{{sup|2}}}} and {{math|''bx''}} agree on their units. Furthermore, by the same logic, the units of {{math|''c''}} must be equal to the units of {{math|{{sfrac|''b''{{sup|2}}|''a''}}}}, which can be verified without solving for {{math|''x''}}. This can be a powerful tool for verifying that a quadratic expression of [[physical quantities]] has been set up correctly, prior to solving it.\n\n==See also==\n* [[Discriminant]]\n* [[Fundamental theorem of algebra]]\n\n==References==\n{{Reflist|30em}}\n\n{{Polynomials}}\n\n[[Category:Elementary algebra]]\n[[Category:Equations]]"
    },
    {
      "title": "Quadratic function",
      "url": "https://en.wikipedia.org/wiki/Quadratic_function",
      "text": "In [[algebra]], a '''quadratic function''', a '''quadratic polynomial''', a '''polynomial of degree 2''', or simply a '''quadratic''', is a [[polynomial function]] with one or more variables in which the highest-degree term is of the second degree. For example, a quadratic function in three variables ''x'', ''y,'' and ''z'' contains exclusively terms ''x''<sup>2</sup>, ''y''<sup>2</sup>, ''z''<sup>2</sup>, ''xy'', ''xz'', ''yz'', ''x'', ''y'', ''z'', and a constant:\n\n:<math>f(x,y,z)=ax^2+by^2+cz^2+dxy+exz+fyz+gx+hy+iz +j,</math>\n\nwith at least one of the [[coefficient]]s ''a, b, c, d, e,'' or ''f'' of the second-degree terms being non-zero.\n\n[[Image:Polynomialdeg2.svg|thumb|right|<center><math>x^2 - x - 2\\!</math></center>|frame|A quadratic polynomial with two [[real number|real]] [[root of a polynomial|roots]] (crossings of the ''x'' axis) and hence no [[complex number|complex]] roots. Some other quadratic polynomials have their [[minimum]] above the ''x'' axis, in which case there are no real roots and two complex roots.]]\n\nA ''univariate'' (single-variable) quadratic function has the form<ref name=\"wolfram\">{{cite web | url=http://mathworld.wolfram.com/QuadraticEquation.html | title=Quadratic Equation -- from Wolfram MathWorld | accessdate=January 6, 2013}}</ref>\n\n:<math>f(x)=ax^2+bx+c,\\quad a \\ne 0</math>\nin the single variable ''x''. The [[graph of a function|graph]] of a univariate quadratic function is a [[parabola]] whose axis of symmetry is parallel to the {{math|''y''}}-axis, as shown at right.\n\nIf the quadratic function is set equal to zero, then the result is a [[quadratic equation]]. The solutions to the univariate equation are called the [[root of a function|root]]s of the univariate function.\n\nThe bivariate case in terms of variables ''x'' and ''y'' has the form\n\n:<math> f(x,y) = a x^2 + by^2 + cx y+ d x+ ey + f \\,\\!</math>\n\nwith at least one of ''a, b, c'' not equal to zero, and an equation setting this function equal to zero gives rise to a [[conic section]] (a [[circle]] or other [[ellipse]], a [[parabola]], or a [[hyperbola]]).\n\nIn general there can be an arbitrarily large number of variables, in which case the resulting [[surface (geometry)|surface]] is called a [[quadric]], but the highest degree term must be of degree 2, such as ''x''<sup>2</sup>, ''xy'', ''yz'', etc.\n\n==Etymology==\n\nThe adjective ''quadratic'' comes from the [[Latin]] word ''[[wikt:en:quadratum#Latin|quadrātum]]'' (\"[[square (geometry)|square]]\"). A term like {{math|''x''<sup>2</sup>}} is called a [[square (algebra)|square]] in algebra because it is the area of a ''square'' with side {{math|''x''}}.\n\n==Terminology==\n\n===Coefficients===\nThe [[coefficients]] of a polynomial are often  taken to be real or [[Complex quadratic polynomial|complex number]]s, but in fact, a polynomial may be defined over any [[ring (mathematics)|ring]].\n\n===Degree===\nWhen using the term \"quadratic polynomial\", authors sometimes mean \"having degree exactly 2\", and sometimes \"having degree at most 2\". If the degree is less than 2, this may be called a \"[[Degeneracy (mathematics)|degenerate case]]\". Usually the context will establish which of the two is meant.\n\nSometimes the word \"order\" is used with the meaning of \"degree\", e.g. a second-order polynomial.\n\n===Variables===\n\nA quadratic polynomial may involve a single [[Variable (mathematics)|variable]] ''x'' (the univariate case), or multiple variables such as ''x'', ''y'', and ''z'' (the multivariate case).\n\n====The one-variable case====\n\nAny single-variable quadratic polynomial may be written as \n:<math>ax^2 + bx + c,\\,\\!</math>\nwhere ''x'' is the variable, and ''a'', ''b'', and ''c'' represent the [[coefficient]]s.  In [[elementary algebra]], such polynomials often arise in the form of a [[quadratic equation]] <math>ax^2 + bx + c = 0</math>.  The solutions to this equation are called the [[Root of a function|roots]] of the quadratic polynomial, and may be found through [[factorization]], [[completing the square]], [[Graph of a function|graphing]], [[Newton's method]], or through the use of the [[quadratic formula]].  Each quadratic polynomial has an associated quadratic function, whose [[graph of a function|graph]] is a [[parabola]].\n\n====Bivariate case====\n\nAny quadratic polynomial with two variables may be written as \n:<math> f(x,y) = a x^2 + b y^2 + cxy + dx+ e y + f, \\,\\!</math>\nwhere ''x'' and ''y'' are the variables and ''a'', ''b'', ''c'', ''d'', ''e'', and ''f'' are the coefficients.  Such polynomials are fundamental to the study of [[conic section]]s, which are characterized by equating the expression for ''f'' (''x'', ''y'') to zero.  \nSimilarly, quadratic polynomials with three or more variables correspond to [[quadric]] surfaces and [[hypersurface]]s.  In [[linear algebra]], quadratic polynomials can be generalized to the notion of a [[quadratic form]] on a [[vector space]].\n\n==Forms of a univariate quadratic function==\nA univariate quadratic function can be expressed in three formats:<ref>{{citation\n|title=College Algebra\n|first1=Deborah\n|last1=Hughes-Hallett | author1-link = Deborah Hughes Hallett\n|first2=Eric\n|last2=Connally\n|first3=William G.\n|last3=McCallum | author3-link = William G. McCallum\n|publisher=John Wiley & Sons Inc.\n|year=2007\n|isbn=9780471271758\n|page=205\n|url=https://books.google.com/books?sourceid=navclient&ie=UTF-8&rlz=1T4GGLJ_enBE306BE306&q=%22three+different+forms+for+a+quadratic+expression+are%22}},  [https://books.google.com/books?sourceid=navclient&ie=UTF-8&rlz=1T4GGLJ_enBE306BE306&q=%22three+different+forms+for+a+quadratic+expression+are%22 Search result]\n</ref>\n\n* <math>f(x) = a x^2 + b x + c \\,\\!</math>  is called the '''standard form''',\n* <math>f(x) = a(x - r_1)(x - r_2)\\,\\!</math> is called the '''factored form''', where {{math|''r''<sub>1</sub>}} and {{math|''r''<sub>2</sub>}} are the roots of the quadratic function and the solutions of the corresponding quadratic equation.\n* <math>f(x) = a(x - h)^2 + k \\,\\!</math> is called the '''vertex form''', where {{math|''h''}} and {{math|''k''}} are the {{math|''x''}} and {{math|''y''}} coordinates of the vertex, respectively.\n\nThe coefficient {{math|''a''}} is the same value in all three forms.  To convert the '''standard form''' to '''factored form''', one needs only the [[quadratic formula]] to determine the two roots {{math|''r''<sub>1</sub>}} and {{math|''r''<sub>2</sub>}}. To convert the '''standard form''' to '''vertex form''', one needs a process called [[completing the square]]. To convert the factored form (or vertex form) to standard form, one needs to multiply, expand and/or distribute the factors.\n\n==Graph of the univariate function==\n[[Image:Function ax^2.svg|thumb|350px|<math>f(x) = ax^2 |_{a=\\{0.1,0.3,1,3\\}} \\!</math>]]\n[[Image:Function x^2+bx.svg|thumb|350px|<math>f(x) = x^2 + bx |_{b=\\{1,2,3,4\\}} \\!</math>]]\n[[Image:Function x^2-bx.svg|thumb|350px|<math>f(x) = x^2 + bx |_{b=\\{-1,-2,-3,-4\\}} \\!</math>]]\n\nRegardless of the format, the graph of a univariate quadratic function <math>f(x) = ax^2 + bx + c</math> is a [[parabola]] (as shown at the right). Equivalently, this is the graph of the bivariate quadratic equation <math>y = ax^2 + bx + c</math>.\n\n* If {{math|''a'' &gt; 0}}, the parabola opens upwards.\n* If {{math|''a'' &lt; 0}}, the parabola opens downwards.\n\nThe coefficient {{math|''a''}} controls the degree of curvature of the graph; a larger magnitude of  {{math|''a''}} gives the graph a more closed (sharply curved) appearance.\n\nThe coefficients {{math|''b''}} and {{math|''a''}} together control the location of the axis of symmetry of the parabola (also the {{math|''x''}}-coordinate of the vertex) which is at \n:<math>x = -\\frac{b}{2a}.</math>\n\nThe coefficient {{math|''c''}} controls the height of the parabola; more specifically, it is the height of the parabola  where it intercepts the {{math|''y''}}-axis.\n\n===Vertex===<!-- This section is linked from [[Quadratic equation]] -->\n\nThe '''vertex''' of a parabola is the place where it turns; hence, it is also called the '''turning point'''. If the quadratic function is in vertex form, the vertex is {{math|(''h'', ''k'')}}. Using the method of completing the square, one can turn the standard form\n:<math>f(x) = a x^2 + b x + c \\,\\!</math>\ninto\n: <math>\\begin{align}\n  f(x) &= a x^2 + b x + c \\\\\n       &= a (x - h)^2 + k \\\\\n       &= a\\left(x - \\frac{-b}{2a}\\right)^2 + \\left(c - \\frac{b^2}{4a}\\right),\\\\\n\\end{align}</math>\nso the vertex, {{math|(''h'', ''k'')}}, of the parabola in standard form is\n: <math> \\left(-\\frac{b}{2a}, c - \\frac{b^2}{4a}\\right). </math>\nIf the quadratic function is in factored form\n:<math>f(x) = a(x - r_1)(x - r_2) \\,\\!</math>\nthe average of the two roots, i.e.,\n: <math>\\frac{r_1 + r_2}{2} \\,\\!</math>\nis the {{math|''x''}}-coordinate of the vertex, and hence the vertex {{math|(''h'', ''k'')}} is\n: <math> \\left(\\frac{r_1 + r_2}{2}, f\\left(\\frac{r_1 + r_2}{2}\\right)\\right).\\!</math>\n\nThe vertex is also the maximum point if {{math|''a'' &lt; 0}}, or the minimum point if {{math|''a'' &gt; 0}}.\n\nThe vertical line\n\n: <math> x=h=-\\frac{b}{2a} </math>\n\nthat passes through the vertex is also the '''axis of symmetry''' of the parabola.\n\n====Maximum and minimum points====\n\nUsing [[calculus]], the vertex point, being a [[minima and maxima|maximum or minimum]] of the function, can be obtained by finding the roots of the [[derivative]]:\n:<math>f(x)=ax^2+bx+c \\quad \\Rightarrow \\quad f'(x)=2ax+b \\,\\!.</math>\n{{math|''x''}} is a root of {{math|''f'' '(''x'')}} if {{math|''f'' '(''x'') {{=}} 0}}\nresulting in\n:<math>x=-\\frac{b}{2a}</math>\nwith the corresponding function value\n:<math>f(x) = a \\left (-\\frac{b}{2a} \\right)^2+b \\left (-\\frac{b}{2a} \\right)+c = c-\\frac{b^2}{4a} \\,\\!,</math>\nso again the vertex point coordinates, {{math|(''h'', ''k'')}}, can be expressed as\n:<math> \\left (-\\frac {b}{2a}, c-\\frac {b^2}{4a} \\right). </math>\n\n==Roots of the univariate function==\n{{quadratic_equation_graph_key_points.svg|250px}}\n{{quadratic_function_graph_complex_roots.svg}}\n{{further information|Quadratic equation}}\n\n===Exact roots===\n\nThe [[root of a function|roots]] (or ''zeros''), {{math|''r''<sub>1</sub>}} and {{math|''r''<sub>2</sub>}}, of the univariate quadratic function\n\n: <math>\\begin{align}\n f(x) &= ax^2+bx+c \\\\ \n  &= a(x-r_1)(x-r_2), \\\\ \n\\end{align}</math>\n\nare the values of {{math|''x''}} for which {{math|''f''(''x'') {{=}} 0}}.\n\nWhen the [[coefficient]]s {{math|''a''}}, {{math|''b''}}, and {{math|''c''}}, are [[real numbers|real]] or [[complex numbers|complex]], the roots are\n\n:<math>r_1=\\frac{-b - \\sqrt{b^2-4ac}}{2a}, </math>\n\n:<math>r_2=\\frac{-b + \\sqrt{b^2-4ac}}{2a}. </math>\n\n===Upper bound on the magnitude of the roots===\n\nThe [[absolute value|modulus]] of the roots of a quadratic <math>ax^2+bx+c\\,</math> can be no greater than <math>\\frac{\\max(|a|, |b|, |c|)}{|a|}\\times \\phi,\\, </math> where <math>\\phi</math> is the [[golden ratio]] <math>\\frac{1+\\sqrt{5}}{2}.</math><ref>Lord, Nick, \"Golden bounds for the roots of quadratic equations\", ''Mathematical Gazette'' 91, November 2007, 549.</ref>{{importance inline|<!--Formula doesn't scale under scale of ''x''; a realistic formula should scale by α when b ↦ bα and c ↦cα<sup>2</sup>-->}}\n\n==The square root of a univariate quadratic function==\nThe [[square root]] of a univariate quadratic function gives rise to one of the four conic sections, [[almost always]] either to an [[ellipse]] or to a [[hyperbola]].\n\nIf <math>a>0\\,\\!</math> then the equation <math> y = \\pm \\sqrt{a x^2 + b x + c} </math> describes a hyperbola, as can be seen by squaring both sides. The directions of the axes of the hyperbola are determined by the [[ordinate]] of the [[minimum]] point of the corresponding parabola <math> y_p = a x^2 + b x + c \\,\\!</math>. If the ordinate is negative, then the hyperbola's major axis (through its vertices) is horizontal, while if the ordinate is positive then the hyperbola's major axis is vertical.\n\nIf <math>a<0\\,\\!</math> then the equation <math> y = \\pm \\sqrt{a x^2 + b x + c} </math> describes either a circle or other ellipse or nothing at all.  If the ordinate of the [[maximum]] point of the corresponding parabola\n<math> y_p = a x^2 + b x + c \\,\\!</math> is positive, then its square root describes an ellipse, but if the ordinate is negative then it describes an [[Empty set|empty]] locus of points.\n\n==Iteration==\nTo [[iterated function|iterate a function]] <math>f(x)=ax^2+bx+c</math>, one applies the function repeatedly, using the output from one iteration as the input to the next.\n\nOne cannot always deduce the analytic form of <math>f^{(n)}(x)</math>, which means the ''n''<sup>th</sup> iteration of <math>f(x)</math>. (The superscript can be extended to negative numbers, referring to the iteration of the inverse of <math>f(x)</math> if the inverse exists.) But there are some analytically [[closed-form expression|tractable]] cases.\n\nFor example, for the iterative equation\n\n:<math>f(x)=a(x-c)^2+c</math>\n\none has\n:<math>f(x)=a(x-c)^2+c=h^{(-1)}(g(h(x))),\\,\\!</math>\nwhere\n:<math>g(x)=ax^2\\,\\!</math> and <math>h(x)=x-c.\\,\\!</math>\nSo by induction,\n:<math>f^{(n)}(x)=h^{(-1)}(g^{(n)}(h(x)))\\,\\!</math>\ncan be obtained, where <math>g^{(n)}(x)</math> can be easily computed as\n:<math>g^{(n)}(x)=a^{2^{n}-1}x^{2^{n}}.\\,\\!</math>\nFinally, we have\n:<math>f^{(n)}(x)=a^{2^n-1}(x-c)^{2^n}+c\\,\\!</math>\n\nas the solution.\n\nSee [[Topological conjugacy]] for more detail about the relationship between ''f'' and ''g''. And see [[Complex quadratic polynomial]] for the chaotic behavior in the general iteration.\n\nThe [[logistic map]]\n\n:<math> x_{n+1} = r x_n (1-x_n), \\quad  0\\leq x_0<1</math>\n\nwith parameter 2<''r''<4 can be solved in certain cases, one of which is [[chaos (mathematics)|chaotic]] and one of which is not. In the chaotic case ''r''=4 the solution is\n\n:<math>x_{n}=\\sin^{2}(2^{n} \\theta \\pi)</math>\n\nwhere the initial condition parameter <math>\\theta</math> is given by <math>\\theta = \\tfrac{1}{\\pi}\\sin^{-1}(x_0^{1/2})</math>.  For rational <math>\\theta</math>, after a finite number of iterations <math>x_n</math> maps into a periodic sequence.  But almost all <math>\\theta</math> are irrational, and, for irrational <math>\\theta</math>, <math>x_n</math> never repeats itself &ndash; it is non-periodic and exhibits [[sensitive dependence on initial conditions]], so it is said to be chaotic.\n\nThe solution of the logistic map when ''r''=2 is\n\n<math>x_n = \\frac{1}{2} - \\frac{1}{2}(1-2x_0)^{2^{n}}</math>\n\nfor <math>x_0 \\in [0,1)</math>.  Since <math>(1-2x_0)\\in (-1,1)</math> for any value of <math>x_0</math> other than the unstable fixed point 0, the term <math>(1-2x_0)^{2^{n}}</math> goes to 0 as ''n'' goes to infinity, so <math>x_n</math> goes to the stable fixed point <math>\\tfrac{1}{2}.</math>\n\n==Bivariate (two variable) quadratic function==\n{{further information|Quadric|Quadratic form}}\nA '''bivariate quadratic function''' is a second-degree polynomial of the form\n:<math> f(x,y) = A x^2 + B y^2 + C x + D y + E x y + F \\,\\!</math>\nwhere ''A, B, C, D'', and ''E'' are fixed [[coefficient]]s and ''F'' is the constant term.\nSuch a function describes a quadratic [[Surface (mathematics)|surface]].  Setting <math>f(x,y)\\,\\!</math> equal to zero describes the intersection of the surface with the plane <math>z=0\\,\\!</math>, which is a [[locus (mathematics)|locus]] of points equivalent to a [[conic section]].\n\n===Minimum/maximum===\n\nIf <math> 4AB-E^2 <0 \\,</math> the function has no maximum or minimum; its graph forms an hyperbolic [[paraboloid]].\n\nIf <math> 4AB-E^2 >0 \\,</math> the function has a minimum if ''A''>0, and a maximum if ''A''<0; its graph forms an elliptic paraboloid. In this case the minimum or maximum occurs at <math> (x_m, y_m) \\,</math> where:\n\n:<math>x_m = -\\frac{2BC-DE}{4AB-E^2},</math>\n\n:<math>y_m = -\\frac{2AD-CE}{4AB-E^2}.</math>\n\nIf <math> 4AB- E^2 =0 \\,</math> and <math> DE-2CB=2AD-CE \\ne 0 \\,</math> the function has no maximum or minimum; its graph forms a parabolic [[cylinder (geometry)|cylinder]].\n\nIf <math> 4AB- E^2 =0 \\,</math> and <math> DE-2CB=2AD-CE =0 \\,</math> the function achieves the maximum/minimum at a line—a minimum if ''A''>0 and a maximum if ''A''<0; its graph forms a parabolic cylinder.\n\n==See also==\n* [[Quadratic form]]\n* [[Quadratic equation]]\n* [[Matrix representation of conic sections]]\n* [[Quadric]]\n* [[Periodic points of complex quadratic mappings]]\n* [[List of mathematical functions]]\n\n==References==\n{{reflist}}\n*Algebra 1, Glencoe, {{isbn|0-07-825083-8}}\n*Algebra 2, Saxon, {{isbn|0-939798-62-X}}\n\n==External links==\n* {{MathWorld|title=Quadratic|urlname=Quadratic}}\n\n{{Polynomials}}\n\n{{DEFAULTSORT:Quadratic Function}}\n[[Category:Polynomial functions]]\n[[Category:Parabolas]]"
    },
    {
      "title": "Quartic function",
      "url": "https://en.wikipedia.org/wiki/Quartic_function",
      "text": "{{short description|Polynomial function of degree four}}\n{{Use dmy dates|date=December 2017}}\n{{about|the univariate case|the bivariate case|Quartic plane curve}}\n\n[[File:Polynomialdeg4.svg|thumb|right|233px|Graph of a polynomial of degree 4, with 3 [[critical point (mathematics)|critical points]] and four [[real number|real]] [[root of a polynomial|roots]] (crossings of the ''x'' axis) (and thus no [[complex number|complex]] roots). If one or the other of the local [[minimum|minima]] were above the ''x'' axis, or if the local [[maximum]] were below it, or if there were no local maximum and one minimum below the ''x'' axis, there would only be two real roots (and two complex roots). If all three local extrema were above the ''x'' axis, or if there were no local maximum and one minimum above the ''x'' axis, there would be no real root (and four complex roots).]]\nIn [[algebra]], a '''quartic function'''  is a [[function (mathematics)|function]] of the form\n:<math>f(x)=ax^4+bx^3+cx^2+dx+e,</math>\nwhere ''a'' is nonzero,\nwhich is defined by a [[polynomial]] of [[Degree of a polynomial|degree]] four, called a '''quartic polynomial'''.\n\nSometimes the term '''biquadratic''' is used instead of ''quartic'', but, usually, '''biquadratic function''' refers to a [[quadratic function]] of a square (or, equivalently, to the function defined by a quartic polynomial without terms of odd degree), having the form\n:<math>f(x)=ax^4+cx^2+e.</math>\n\nA '''quartic equation''', or equation of the fourth degree, is an equation that equates a quartic polynomial to zero, of the form\n:<math>ax^4+bx^3+cx^2+dx+e=0 ,</math>\nwhere {{nowrap|''a'' ≠ 0}}.\n\nThe [[derivative]] of a quartic function is a [[cubic function]].\n\nSince a quartic function is defined by a polynomial of even degree, it has the same infinite limit when the argument goes to positive or negative [[infinity]]. If ''a'' is positive, then the function increases to positive infinity at both ends; and thus the function has a [[Maxima and minima|global minimum]]. Likewise, if ''a'' is negative, it decreases to negative infinity and has a global maximum. In both cases it may or may not have another local maximum and another local minimum.\n\nThe degree four (''quartic'' case) is the highest degree such that every polynomial equation can be solved by [[Nth root|radicals]].\n\n==History==\n[[Lodovico Ferrari]] is credited with the discovery of the solution to the quartic in 1540, but since this solution, like all algebraic solutions of the quartic, requires the solution of a [[cubic equation|cubic]] to be found, it could not be published immediately.<ref>{{MacTutor|id=Ferrari|title=Lodovico Ferrari}}</ref> The solution of the quartic was published together with that of the cubic by Ferrari's mentor [[Gerolamo Cardano]] in the book ''[[Ars Magna (Gerolamo Cardano)|Ars Magna]]''.<ref>{{Citation | last = Cardano | first = Gerolamo | author-link = Gerolamo Cardano | year = 1993 | orig-year = 1545 | title = Ars magna or The Rules of Algebra | publisher = Dover | isbn = 0-486-67811-3}}</ref>\n\nThe Soviet historian I. Y. Depman claimed that even earlier, in 1486, Spanish mathematician Valmes was [[burned at the stake]] for claiming to have solved the quartic equation.<ref>{{citation|last=Depman|title=Rasskazy o matematike|publisher=Gosdetizdat|year=1954|place=Leningrad|language=Russian}}</ref> [[Inquisitor General]] [[Tomás de Torquemada]] allegedly told Valmes that it was the will of God that such a solution be inaccessible to human understanding.<ref>{{cite book |author=P. Beckmann |title=A history of π |publisher=Macmillan |year=1971 |page=80 |url=https://books.google.com/books?id=TB6jzz3ZDTEC&pg=PA80}}</ref>  However [[Petr Beckmann|Beckmann]], who popularized this story of Depman in the West, said that it was unreliable and hinted that it may have been invented as Soviet antireligious propaganda.<ref>{{cite book |author=P. Beckmann |title=A history of π |publisher=Macmillan |year=1971 |page=191 |url=https://books.google.com/books?id=TB6jzz3ZDTEC&pg=PA80}}</ref> Beckmann's version of this story has been widely copied in several books and internet sites, usually without his reservations and sometimes with fanciful embellishments. Several attempts to find corroborating evidence for this story, or even for the existence of Valmes, have failed.<ref>{{cite journal|author=P. Zoll | title=Letter to the Editor |journal=American Mathematical Monthly |volume=96 |issue=8 |year=1989 |pages=709–710 |jstor=2324719}}</ref>\n\nThe proof that four is the highest degree of a general polynomial for which such solutions can be found was first given in the [[Abel–Ruffini theorem]] in 1824, proving that all attempts at solving the higher order polynomials would be futile. The notes left by [[Évariste Galois]] prior to dying in a duel in 1832 later led to an elegant [[Galois Theory|complete theory]] of the roots of polynomials, of which this theorem was one result.<ref>Stewart, Ian, ''Galois Theory, Third Edition'' (Chapman & Hall/CRC Mathematics, 2004)</ref>\n\n==Applications==\nEach [[coordinate]] of the intersection points of two [[conic section]]s is a solution of a quartic equation. The same is true for the intersection of a line and a [[torus]]. It follows that quartic equations often arise in [[computational geometry]] and all related fields such as [[computer graphics]], [[computer-aided design]], [[computer-aided manufacturing]] and [[optics]]. Here are examples of other geometric problems whose solution involves solving a quartic equation.\n\nIn [[computer-aided manufacturing]], the torus is a shape that is commonly associated with the [[endmill]] cutter.  To calculate its location relative to a triangulated surface, the position of a horizontal torus on the {{math|''z''}}-axis must be found where it is tangent to a fixed line, and this requires the solution of a general quartic equation to be calculated.<ref>{{Cite web|url=http://people.math.gatech.edu/~etnyre/class/4441Fall16/ShifrinDiffGeo.pdf|title=DIFFERENTIAL GEOMETRY: A First Course in Curves and Surfaces, p. 36|last=|first=|date=|website=math.gatech.edu|archive-url=|archive-date=|dead-url=|access-date=}}</ref>\n\nA quartic equation arises also in the process of solving the [[crossed ladders problem]], in which the lengths of two crossed ladders, each based against one wall and leaning against another, are given along with the height at which they cross, and the distance between the walls is to be found.\n\nIn optics, [[Alhazen's problem]] is \"''Given a light source and a spherical mirror, find the point on the mirror where the light will be reflected to the eye of an observer.''\" This leads to a quartic equation.<ref name=MacTutor>{{MacTutor|id=Al-Haytham|title=Abu Ali al-Hasan ibn al-Haytham}}</ref><ref>{{citation|title=Scientific Method, Statistical Method and the Speed of Light|first1=R. J.|last1=MacKay|first2=R. W.|last2=Oldford|journal=Statistical Science|volume=15|issue=3|date=August 2000|pages=254–78|doi=10.1214/ss/1009212817|MR=1847825}}</ref><ref name=Weiss>{{Citation|last = Neumann|first = Peter M.|author-link = Peter M. Neumann|journal = [[American Mathematical Monthly]]|title = Reflections on Reflection in a Spherical Mirror|year = 1998|volume = 105|issue = 6|pages = 523–528|doi = 10.2307/2589403|jstor = 2589403}}</ref>\n\nFinding the [[distance of closest approach of ellipses and ellipsoids#Distance of closest approach of two ellipses|distance of closest approach of two ellipses]] involves solving a quartic equation.\n\nThe [[eigenvalue]]s of a 4×4 [[matrix (mathematics)|matrix]] are the roots of a quartic polynomial which is the [[characteristic polynomial]] of the matrix.\n\nThe characteristic equation of a fourth-order linear [[difference equation]] or [[differential equation]] is a quartic equation. An example arises in the [[Bending#Timoshenko-Rayleigh theory|Timoshenko-Rayleigh theory]] of beam bending.\n\n[[Intersection (Euclidean geometry)|Intersections]] between spheres, cylinders, or other [[quadric]]s can be found using quartic equations.\n\n==Inflection points and golden ratio==\n\nLetting {{mvar|F}} and {{mvar|G}} be the distinct [[inflection point]]s of a quartic, and letting {{mvar|H}} be the intersection of the inflection [[secant line]] {{mvar|FG}} and the quartic, nearer to {{mvar|G}} than to {{mvar|F}}, then {{mvar|G}} divides {{mvar|FH}} into the [[golden section]]:<ref>{{Citation|last = Aude|first = H. T. R.|journal = [[American Mathematical Monthly]]|year = 1949|issue = 3|volume = 56|title = Notes on Quartic Curves|jstor = 2305030|doi = 10.2307/2305030|pages=165}}</ref>\n\n:<math>\\frac{FG}{GH}=\\frac{1+\\sqrt{5}}{2}= \\varphi \\; (\\text{the golden ratio}).</math>\n\nMoreover, the area of the region between the secant line and the quartic below the secant line equals the area of the region between the secant line and the quartic above the secant line. One of those regions is disjointed into sub-regions of equal area.\n\n==Solution==\n\n===Nature of the roots===\nGiven the general quartic equation\n:<math>ax^4 + bx^3 + cx^2 + dx + e = 0</math>\nwith real coefficients and {{math|''a'' ≠ 0}} the nature of its roots is mainly determined by the sign of its [[discriminant]] \n:<math>\\begin{align} \n\\Delta\\ =\\ &256 a^3 e^3 - 192 a^2 b d e^2 - 128 a^2 c^2 e^2 + 144 a^2 c d^2 e - 27 a^2 d^4 \\\\ \n&+ 144 a b^2 c e^2 - 6 a b^2 d^2 e - 80 a b c^2 d e + 18 a b c d^3 + 16 a c^4 e \\\\\n&- 4 a c^3 d^2 - 27 b^4 e^2 + 18 b^3 c d e - 4 b^3 d^3 - 4 b^2 c^3 e + b^2 c^2 d^2\n\\end{align} </math> \nThis may be refined by considering the signs of four other polynomials:\n:<math>P = 8ac - 3b^2</math>\nsuch that {{math|{{sfrac|''P''|8''a''<sup>2</sup>}}}} is the second degree coefficient of the associated depressed quartic (see [[#Converting_to_a_depressed_quartic|below]]);\n:<math>R= b^3+8da^2-4abc,</math>\nsuch that {{math|{{sfrac|''R''|8''a''<sup>3</sup>}}}} is the first degree coefficient of the associated depressed quartic; \n:<math>\\Delta_0 = c^2 - 3bd + 12ae,</math>\nwhich is 0 if the quartic has a triple root; and\n:<math>D = 64 a^3 e - 16 a^2 c^2 + 16 a b^2 c - 16 a^2 bd - 3 b^4</math>\nwhich is 0 if the quartic has two double roots.\n\nThe possible cases for the nature of the roots are as follows:<ref>{{cite journal|first= E. L.|last=Rees|title=Graphical Discussion of the Roots of a Quartic Equation|journal = The American Mathematical Monthly|volume=29|issue=2|year=1922|pages=51–55|doi=10.2307/2972804|jstor = 2972804}}</ref>\n\n* If {{math|∆ < 0}} then the equation has two distinct real roots and two [[complex conjugate]] non-real roots.\n* If {{math|∆ > 0}} then either the equation's four roots are all real or none is.\n** If {{mvar|P}} < 0 and {{mvar|D}} < 0 then all four roots are real and distinct.\n** If {{mvar|P}} > 0 or {{mvar|D}} > 0 then there are two pairs of non-real complex conjugate roots.<ref>{{Cite journal | last1 = Lazard | first1 = D. | doi = 10.1016/S0747-7171(88)80015-4 | title = Quantifier elimination: Optimal solution for two classical examples | journal = Journal of Symbolic Computation | volume = 5 | pages = 261–266 | year = 1988 | pmid =  | pmc = }}</ref>\n* If {{math|∆ {{=}} 0}} then (and only then) the polynomial has a [[multiplicity (mathematics)|multiple]] root. Here are the different cases that can occur:\n** If {{mvar|P}} < 0 and {{mvar|D}} < 0 and {{math|∆<sub>0</sub> ≠ 0}}, there are a real double root and two real simple roots.\n** If {{mvar|D}} > 0 or ({{mvar|P}} > 0 and ({{mvar|D}} ≠ 0 or {{mvar|R}} ≠ 0)), there are a real double root and two complex conjugate roots.\n** If {{math|∆<sub>0</sub> {{=}} 0}} and {{mvar|D}} ≠ 0, there are a triple root and a simple root, all real.\n** If {{mvar|D}} = 0, then:\n***If {{mvar|P}} < 0, there are two real double roots.\n***If {{mvar|P}} > 0 and {{mvar|R}} = 0, there are two complex conjugate double roots.\n***If {{math|∆<sub>0</sub> {{=}} 0}}, all four roots are equal to {{math|−{{sfrac|''b''|4''a''}}}}\n\nThere are some cases that do not seem to be covered, but they cannot occur.  For example, {{math|∆<sub>0</sub> > 0}}, {{mvar|P}} = 0 and {{mvar|D}} ≤ 0 is not one of the cases.  However, if {{math|∆<sub>0</sub> > 0}} and {{mvar|P}} = 0 then  {{mvar|D}} > 0 so this combination is not possible.\n\n===General formula for roots===\n[[File:Quartic Formula.svg|thumb|600px|right|Solution of <math>x^4+ax^3+bx^2+cx+d=0</math> written out in full. This formula is too unwieldy for general use; hence other methods, or simpler formulas for special cases, are generally used.<ref>http://planetmath.org/QuarticFormula, PlanetMath, quartic formula, 21 October 2012</ref>]]\n\nThe four roots {{math|''x''<sub>1</sub>}}, {{math|''x''<sub>2</sub>}}, {{math|''x''<sub>3</sub>}}, and {{math|''x''<sub>4</sub>}} for the general quartic equation\n:<math>ax^4+bx^3+cx^2+dx+e=0 \\,</math>\nwith {{mvar|a}} ≠ 0 are given in the following formula, which is deduced from the one in the section on [[#Ferrari's solution|Ferrari's method]] by back changing the variables (see {{slink||Converting to a depressed quartic}}) and using the formulas for the [[Quadratic function|quadratic]] and [[Cubic function#General formula for roots|cubic equation]]s.\n:<math>\\begin{align}\nx_{1,2}\\ &= -\\frac{b}{4a} - S \\pm \\frac12\\sqrt{-4S^2 - 2p + \\frac{q}{S}}\\\\\nx_{3,4}\\ &= -\\frac{b}{4a} + S \\pm \\frac12\\sqrt{-4S^2 - 2p - \\frac{q}{S}}\n\\end{align}</math>\n\nwhere {{mvar|p}} and {{mvar|q}} are the coefficients of the second and of the first degree respectively in the [[#Converting to a depressed quartic|associated depressed quartic]]\n:<math>\\begin{align}\np &= \\frac{8ac-3b^2}{8a^2}\\\\\nq &= \\frac{b^3 - 4abc + 8a^2d}{8a^3} \n\\end{align}</math>\n:\nand where\n:<math>\\begin{align}\nS &= \\frac{1}{2}\\sqrt{-\\frac23\\ p+\\frac{1}{3a}\\left(Q + \\frac{\\Delta_0}{Q}\\right)}\\\\\nQ &= \\sqrt[3]{\\frac{\\Delta_1 + \\sqrt{\\Delta_1^2 - 4\\Delta_0^3}}{2}} \n\\end{align}</math>\n(if {{math|''S'' {{=}} 0}} or {{math|''Q'' {{=}} 0}}, see {{slink||Special cases of the formula}}, below)\n\nwith\n:<math>\\begin{align}\n\\Delta_0 &= c^2 - 3bd + 12ae\\\\\n\\Delta_1 &= 2c^3 - 9bcd + 27b^2 e + 27ad^2 - 72ace\n\\end{align}</math>\nand\n:<math>\\Delta_1^2-4\\Delta_0^3 = - 27 \\Delta\\ ,</math> where <math>\\Delta</math> is the aforementioned [[discriminant]]. For the cube root expression for ''Q'', any of the three cube roots in the complex plane can be used, although if one of them is real that is the natural and simplest one to choose. The mathematical expressions of these last four terms are very similar to those of their [[Cubic function#Algebraic solution|cubic counterparts]].\n\n====Special cases of the formula====\n\n*If <math>\\Delta > 0,</math> the value of <math>Q</math> is a non-real complex number. In this case, either all roots are non-real or they are all real. In the latter case, the value of <math>S</math> is also real, despite being expressed in terms of <math>Q;</math> this is [[casus irreducibilis]] of the cubic function extended to the present context of the quartic. One may prefer to express it in a purely real way, by using [[trigonometric functions]], as follows:\n::<math>S = \\frac{1}{2}\\sqrt{-\\frac23\\ p+\\frac{2}{3a}\\sqrt{\\Delta_0}\\cos\\frac{\\phi}{3}}</math>\n:where\n::<math>\\phi = \\arccos\\left(\\frac{\\Delta_1}{2\\sqrt{\\Delta_0^3}}\\right).</math>\n\n*If <math>\\Delta \\neq 0</math> and <math>\\Delta_0 = 0,</math> the sign of <math>\\sqrt{\\Delta_1^2 - 4 \\Delta_0^3}=\\sqrt{\\Delta_1^2} </math>  has to be chosen to have <math>Q \\neq 0,</math> that is one should define <math>\\sqrt{\\Delta_1^2}</math> as <math>\\Delta_1,</math> maintaining the sign of <math>\\Delta_1.</math>\n*If <math>S = 0,</math> then one must change the choice of the cube root in <math>Q</math> in order to have <math>S \\neq 0.</math> This is always possible except if the quartic may be factored into <math>\\left(x+\\tfrac{b}{4a}\\right)^4.</math> The result is then correct, but misleading because it hides the fact that no cube root is needed in this case. In fact this case may occur only if the [[numerator]] of <math>q</math> is zero, in which case the associated [[#Converting to a depressed quartic|depressed quartic]] is biquadratic; it may thus be solved by the method described [[#Biquadratic equation|below]].\n*If <math>\\Delta = 0</math> and <math>\\Delta_0 = 0,</math> and thus also <math>\\Delta_1 = 0,</math> at least three roots are equal to each other, and the roots are [[rational function]]s of the coefficients. The triple root <math>x_0</math> is a common root of the quartic and its second derivative <math>2(6ax^2+3bx+c);</math> it is thus also the unique root of the remainder of the [[Euclidean division]] of the quartic by its second derivative, which is a linear polynomial. The simple root <math>x_1</math> can be deduced from <math>x_1+3x_0=-b/a.</math>\n*If <math>\\Delta=0</math> and <math> \\Delta_0 \\neq 0,</math> the above expression for the roots is correct but misleading, hiding the fact that the polynomial is [[irreducible polynomial|reducible]] and no cube root is needed to represent the roots.\n\n===Simpler cases===\n\n====Reducible quartics====\nConsider the general quartic\n:<math>Q(x) = a_4x^4+a_3x^3+a_2x^2+a_1x+a_0.</math>\nIt is [[irreducible polynomial|reducible]] if {{math|''Q''(''x'') {{=}} ''R''(''x'')×''S''(''x'')}}, where {{math|''R''(''x'')}} and {{math|''S''(''x'')}} are non-constant polynomials with [[rational number|rational]] coefficients (or more generally with coefficients in the same [[field (mathematics)|field]] as the coefficients of {{math|''Q''(''x'')}}). Such a factorization will take one of two forms:\n\n:<math>Q(x) = (x-x_1)(b_3x^3+b_2x^2+b_1x+b_0)</math>\nor\n:<math>Q(x) = (c_2x^2+c_1x+c_0)(d_2x^2+d_1x+d_0).</math>\nIn either case, the roots of {{math|''Q''(''x'')}} are the roots of the factors, which may be computed using the formulas for the roots of a [[quadratic function]] or [[cubic function]].\n\nDetecting the existence of such factorizations can be done [[Resolvent cubic#Factoring quartic polynomials|using the resolvent cubic of {{math|''Q''(''x'')}}]]. It turns out that:\n* if we are working over {{math|'''R'''}} (that is, if coefficients are restricted to be real numbers) (or, more generally, over some [[real closed field]]) then there is always such a factorization;\n* if we are working over {{math|'''Q'''}} (that is, if coefficients are restricted to be rational numbers) then there is an algorithm to determine whether or not {{math|''Q''(''x'')}} is reducible and, if it is, how to express it as a product of polynomials of smaller degree.\n\nIn fact, several methods of solving quartic equations ([[Quartic function#Ferrari's solution|Ferrari's method]], [[Quartic function#Descartes' solution|Descartes' method]], and, to a lesser extent, [[Quartic function#Euler's solution|Euler's method]]) are based upon finding such factorizations.\n\n====Biquadratic equation====\nIf {{math|''a''<sub>3</sub> {{=}} ''a''<sub>1</sub> {{=}} 0}} then the '''biquadratic function'''\n<!--The \\,\\! below is to format this as PNG instead of HTML. Please do not remove it-->\n:<math>\nQ(x) = a_4x^4+a_2x^2+a_0\\,\\!\n</math>\ndefines a '''biquadratic equation''', which is easy to solve.\n\nLet {{math|''z'' {{=}} ''x''<sup>2</sup>}}.\nThen {{math|''Q''(''x'')}} becomes a [[Quadratic function|quadratic]] {{math|''q''}} in {{math|''z''}}: {{math|''q''(''z'') {{=}} ''a''<sub>4</sub>''z''<sup>2</sup> + ''a''<sub>2</sub>''z'' + ''a''<sub>0</sub>}}. Let {{math|''z''<sub>+</sub>}} and {{math|''z''<sub>−</sub>}} be the roots of {{math|''q''(''z'')}}. Then the roots of our quartic {{math|''Q''(''x'')}} are\n:<math>\n\\begin{align}\nx_1&=+\\sqrt{z_+},\n\\\\\nx_2&=-\\sqrt{z_+},\n\\\\\nx_3&=+\\sqrt{z_-},\n\\\\\nx_4&=-\\sqrt{z_-}.\n\\end{align}\n</math>\n\n==== Quasi-palindromic equation ====\nThe polynomial\n: <math>P(x)=a_0x^4+a_1x^3+a_2x^2+a_1 m x+a_0 m^2</math>\nis almost [[Reciprocal polynomial#Palindromic polynomial|palindromic]], as {{math|''P''(''mx'') {{=}} {{sfrac|''x''<sup>4</sup>|''m''<sup>2</sup>}}''P''({{sfrac|''m''|''x''}})}} (it is palindromic if {{math|''m'' {{=}} 1}}). The change of variables {{math|''z'' {{=}} ''x'' + {{sfrac|''m''|''x''}}}} in {{math|{{sfrac|''P''(''x'')|''x''<sup>2</sup>}} {{=}} 0}} produces the [[quadratic equation]]  {{math|''a''<sub>0</sub>''z''<sup>2</sup> + ''a''<sub>1</sub>''z'' + ''a''<sub>2</sub> − 2''ma''<sub>0</sub> {{=}} 0}}. Since {{math|''x''<sup>2</sup> − ''xz'' + ''m'' {{=}} 0}}, the quartic equation {{math|''P''(''x'') {{=}} 0}} may be solved by applying the [[quadratic formula]] twice.\n\n===Solution methods===\n\n====Converting to a depressed quartic====\n\nFor solving purposes, it is generally better to convert the quartic into a '''depressed quartic''' by the following simple change of variable. All formulas are simpler and some methods work only in this case. The roots of the original quartic are easily recovered from that of the depressed quartic by the reverse change of variable.\n\nLet\n:<math> a_4 x^4 + a_3 x^3 + a_2 x^2 + a_1 x + a_0 = 0 </math>\nbe the general quartic equation we want to solve.\n\nDividing by {{math|''a''<sub>4</sub>}}, provides the equivalent equation {{math|''x''<sup>4</sup> + ''bx''<sup>3</sup> + ''cx''<sup>2</sup> + ''dx'' + ''e'' {{=}} 0}}, with {{math|''b'' {{=}} {{sfrac|''a''<sub>3</sub>|''a''<sub>4</sub>}}}}, {{math|''c'' {{=}} {{sfrac|''a''<sub>2</sub>|''a''<sub>4</sub>}}}}, {{math|''d'' {{=}} {{sfrac|''a''<sub>1</sub>|''a''<sub>4</sub>}}}}, and {{math|''e'' {{=}} {{sfrac|''a''<sub>0</sub>|''a''<sub>4</sub>}}}}.\nSubstituting {{math|''y'' − {{sfrac|''b''|4}}}} for {{mvar|x}} gives, after regrouping the terms, the equation {{math|''y''<sup>4</sup> + ''py''<sup>2</sup> + ''qy'' + ''r'' {{=}} 0}},\nwhere\n:<math>\\begin{align}\np&=\\frac{8c-3b^2}{8} =\\frac{8a_2a_4-3{a_3}^2}{8{a_4}^2}\\\\\nq&=\\frac{b^3-4bc+8d}{8} =\\frac{{a_3}^3-4a_2a_3a_4+8a_1{a_4}^2}{8{a_4}^3}\\\\\nr&=\\frac{-3b^4+256e-64bd+16b^2c}{256}=\\frac{-3{a_3}^4+256a_0{a_4}^3-64a_1a_3{a_4}^2+16a_2{a_3}^2a_4}{256{a_4}^4}.\n\\end{align}\n</math>\n\nIf {{math|''y''<sub>0</sub>}} is a root of this depressed quartic, then {{math|''y''<sub>0</sub> − {{sfrac|''b''|4}}}} (that is {{math|''y''<sub>0</sub> − {{sfrac|''a''<sub>3</sub>|4''a''<sub>4</sub>}})}} is a root of the original quartic and every root of the original quartic can be obtained by this process.\n\n====Ferrari's solution====\n\nAs explained in the preceding section, we may start with the ''depressed quartic equation''\n:<math> y^4 + p y^2 + q y + r = 0. </math>\nThis depressed quartic can be solved by means of a method discovered by [[Lodovico Ferrari]]. The depressed equation may be rewritten (this is easily verified by expanding the square and regrouping all terms in the left-hand side) as\n:<math> \\left(y^2 + \\frac p2\\right)^2 = -q y - r + \\frac{p^2}4. </math>\nThen, we introduce a variable {{mvar|m}} into the factor on the left-hand side by adding {{math|2''y''<sup>2</sup>''m'' + ''pm'' + ''m''<sup>2</sup>}} to both sides. After regrouping the coefficients of the power of {{mvar|y}} in the right-hand side, this gives the equation \n{{NumBlk|:|<math> \\left(y^2 + \\frac p2 + m\\right)^2 = 2 m y^2 - q y + m^2 + m p + \\frac{p^2}4 - r, </math>|{{EquationRef|1}}}}\nwhich is equivalent to the original equation, whichever value is given to {{mvar|m}}.\n\nAs the value of {{mvar|m}} may be arbitrarily chosen, we will choose it in order to get a perfect square in the right-hand side. This implies that the [[discriminant]] in {{mvar|y}} of this [[quadratic equation]] is zero, that is {{mvar|m}} is a root of the equation\n:<math> (-q)^2 - 4 (2m)\\left(m^2 + p m + \\frac{p^2}4 - r\\right) = 0,\\,</math>\nwhich may be rewritten as\n\n{{NumBlk|:|<math>8m^3+ 8pm^2 + (2p^2 -8r)m- q^2 =0.</math>|{{EquationRef|1a}}}}\n\nThis is the [[resolvent cubic]] of the quartic equation. The value of {{mvar|m}} may thus be obtained from [[Cubic function#Cardano's method|Cardano's formula]]. When {{mvar|m}} is a root of this equation, the right-hand side of equation (''{{EquationNote|1}}'') is the square\n:<math>\\left(\\sqrt{2m}y-\\frac q{2\\sqrt{2m}}\\right)^2.</math>\nHowever, this induces a division by zero if {{math|''m'' {{=}} 0}}. This implies {{math|''q'' {{=}} 0}}, and thus that the depressed equation is bi-quadratic, and may be solved by an easier method (see above). This was not a problem at the time of Ferrari, when one solved only explicitly given equations with numeric coefficients. For a general formula that is always true, one thus needs to choose a root of the cubic equation such that {{math|''m'' ≠ 0}}. This is always possible except for the depressed equation {{math|''y''<sup>4</sup> {{=}} 0}}.\n\nNow, if {{mvar|m}} is a root of the cubic equation such that {{math|''m'' ≠ 0}}, equation (''{{EquationNote|1}}'') becomes\n:<math> \\left(y^2 + \\frac p2 + m\\right)^2 = \\left(y\\sqrt{2 m}-\\frac{q}{2\\sqrt{2 m}}\\right)^2. </math>\n\nThis equation is of the form {{math|''M''&thinsp;<sup>2</sup> {{=}} ''N''&thinsp;<sup>2</sup>}}, which can be rearranged as {{math|''M''&thinsp;<sup>2</sup> − ''N''&thinsp;<sup>2</sup> {{=}} 0}} or {{math|(''M'' + ''N'')(''M'' − ''N'') {{=}} 0}}. Therefore, equation (''{{EquationNote|1}}'')  may be rewritten as\n:<math> \\left(y^2 + \\frac p2 + m + \\sqrt{2 m}y-\\frac q{2\\sqrt{2 m}}\\right) \\left(y^2 + \\frac p2 + m - \\sqrt{2 m}y+\\frac q{2\\sqrt{2 m}}\\right)=0.</math>\n\nThis equation is easily solved by applying to each factor the [[quadratic formula]]. Solving them we may write the four roots as\n:<math>y={\\pm_1\\sqrt{2 m} \\pm_2 \\sqrt{-\\left(2p + 2m \\pm_1 {\\sqrt 2q \\over \\sqrt{m}} \\right)} \\over 2},</math>\nwhere {{math|±<sub>1</sub>}} and {{math|±<sub>2</sub>}} denote either {{math|+}} or {{math|−}}. As the two occurrences of {{math|±<sub>1</sub>}} must denote the same sign, this leaves four possibilities, one for each root.\n\nTherefore, the solutions of the original quartic equation are\n:<math>x=-{a_3 \\over 4a_4} + {\\pm_1\\sqrt{2 m} \\pm_2 \\sqrt{-\\left(2p + 2m \\pm_1 {\\sqrt2q \\over \\sqrt{m}} \\right)} \\over 2}.</math> \nA comparison with the [[#General_formula_for_roots|general formula]] above shows that {{math|{{sqrt|2''m''}} {{=}} 2''S''}}.\n\n====Descartes' solution====\nDescartes<ref>{{Citation|last = Descartes|first = René|author-link = René Descartes|title = [[La Géométrie|The Geometry of Rene Descartes with a facsimile of the first edition]]|isbn = 0-486-60068-8|publisher = [[Dover Publications|Dover]]|year = 1954|jfm = 51.0020.07|chapter = Book&nbsp;III: On the construction of solid and supersolid problems|origyear = 1637}}</ref> introduced in 1637 the method of finding the roots of a quartic polynomial by factoring it into two quadratic ones. Let\n\n:<math>\n  \\begin{align}\n   x^4 + bx^3 + cx^2 + dx + e & = (x^2 + sx + t)(x^2 + ux + v) \\\\\n   & = x^4 + (s + u)x^3 + (t + v + su)x^2 + (sv + tu)x + tv\n  \\end{align}\n </math>\n\nBy equating coefficients, this results in the following system of equations:\n\n:<math>\n  \\left\\{\\begin{array}{l}\n   b = s + u \\\\\n   c = t + v + su \\\\\n   d = sv + tu \\\\\n   e = tv\n  \\end{array}\\right.\n </math>\n\nThis can be simplified by starting again with the [[#Converting to a depressed quartic|depressed quartic]] {{math|''y''<sup>4</sup> + ''py''<sup>2</sup> + ''qy'' + ''r''}}, which can be obtained by substituting {{math|''y'' − ''b''/4}} for {{math|''x''}}. Since the coefficient of {{math|''y''<sup>3</sup>}} is&nbsp;{{math|0}}, we get {{math|''s'' {{=}} −''u''}}, and:\n\n:<math>\n  \\left\\{\\begin{array}{l}\n   p + u^2 = t + v \\\\\n   q = u (t - v) \\\\\n   r = tv\n  \\end{array}\\right.\n </math>\n\nOne can now eliminate both {{mvar|t}} and {{mvar|v}} by doing the following:\n:<math>\n  \\begin{align}\n   u^2(p + u^2)^2 - q^2 & = u^2(t + v)^2 - u^2(t - v)^2 \\\\\n   & = u^2 [(t + v + (t - v))(t + v - (t - v))]\\\\\n   & = u^2(2t)(2v) \\\\\n   & = 4u^2tv \\\\\n   & = 4u^2r\n  \\end{align}\n </math>\n\nIf we set {{math|''U'' {{=}} ''u''<sup>2</sup>}}, then solving this equation becomes finding the roots of the [[resolvent cubic]]\n\n{{NumBlk|:|<math> U^3 + 2pU^2 + (p^2-4r)U - q^2,</math>|{{EquationRef|2}}}}\n\nwhich is [[Cubic_function#General_solution_to_the_cubic_equation_with_real_coefficients|done elsewhere]]. This resolvent cubic is equivalent to the resolvent cubic given above (equation (1a)), as can be seen by substituting U = 2m.\n\nIf {{math|''u''}} is a square root of a non-zero root of this resolvent (such a non-zero root exists except for the quartic {{math|''x''<sup>4</sup>}}, which is trivially factored),\n\n:<math>\n  \\left\\{\\begin{array}{l}\n  s = -u \\\\\n 2t = p + u^2 + q/u \\\\\n 2v = p + u^2 - q/u\n  \\end{array}\\right.\n </math>\n\nThe symmetries in this solution are as follows. There are three roots of the cubic, corresponding to the three ways that a quartic can be factored into two quadratics, and choosing positive or negative values of {{mvar|u}} for the square root of {{mvar|U}} merely exchanges the two quadratics with one another.\n\nThe above solution shows that a quartic polynomial with rational coefficients and a zero coefficient on the cubic term is factorable into quadratics with rational coefficients if and only if either the resolvent cubic (''{{EquationNote|2}}'') has a non-zero root which is the square of a rational, or {{math|''p''<sup>2</sup> − 4''r''}} is the square of rational and {{math|''q'' {{=}} 0}}; this can readily be checked using the [[rational root test]].<ref name=Brookfield>{{cite journal |author=Brookfield, G. |title=Factoring quartic polynomials: A lost art |journal=[[Mathematics Magazine]] |volume=80 |issue=1 |year=2007 |pages=67–70|url = https://www.maa.org/sites/default/files/Brookfield2007-103574.pdf}}</ref>\n\n====Euler's solution====\nA variant of the previous method is due to [[Leonhard Euler|Euler]].<ref>{{Citation|last = van der Waerden|first=Bartel Leendert|authorlink = Bartel Leendert van der Waerden|title = [[Moderne Algebra|Algebra]]|volume = 1|publisher=[[Springer Science+Business Media|Springer-Verlag]]|edition = 7th|isbn = 0-387-97424-5|year = 1991|section = The Galois theory: Equations of the second, third, and fourth degrees|zbl = 0724.12001}}</ref><ref>{{Citation|last = Euler|first = Leonhard|author-link = Leonhard Euler|title = [[Elements of Algebra]]|chapter= Of a new method of resolving equations of the fourth degree|publisher=[[Springer Science+Business Media|Springer-Verlag]]|origyear = 1765|year = 1984|zbl = 0557.01014|isbn = 978-1-4613-8511-0}}</ref> Unlike the previous methods, both of which use ''some'' root of the resolvent cubic, Euler's method uses all of them. Consider a depressed quartic {{math|''x''<sup>4</sup> + ''px''<sup>2</sup> + ''qx'' + ''r''}}. Observe that, if\n* {{math|''x''<sup>4</sup> + ''px''<sup>2</sup> + ''qx'' + ''r'' {{=}} (''x''<sup>2</sup> + ''sx'' + ''t'')(''x''<sup>2</sup> − ''sx'' + ''v'')}},\n* {{math|''r''<sub>1</sub>}} and {{math|''r''<sub>2</sub>}} are the roots of {{math|''x''<sup>2</sup> + ''sx'' + ''t''}},\n* {{math|''r''<sub>3</sub>}} and {{math|''r''<sub>4</sub>}} are the roots of {{math|''x''<sup>2</sup> − ''sx'' + ''v''}},\nthen\n* the roots of {{math|''x''<sup>4</sup> + ''px''<sup>2</sup> + ''qx'' + ''r''}} are {{math|''r''<sub>1</sub>}}, {{math|''r''<sub>2</sub>}}, {{math|''r''<sub>3</sub>}}, and {{math|''r''<sub>4</sub>}},\n* {{math|''r''<sub>1</sub> + ''r''<sub>2</sub> {{=}} −''s''}},\n* {{math|''r''<sub>3</sub> + ''r''<sub>4</sub> {{=}} ''s''}}.\nTherefore, {{math|(''r''<sub>1</sub> + ''r''<sub>2</sub>)(''r''<sub>3</sub> + ''r''<sub>4</sub>) {{=}} −''s''<sup>2</sup>}}. In other words, {{math|−(''r''<sub>1</sub> + ''r''<sub>2</sub>)(''r''<sub>3</sub> + ''r''<sub>4</sub>)}} is one of the roots of the resolvent cubic (''{{EquationNote|2}}'') and this suggests that the roots of that cubic are equal to {{math|−(''r''<sub>1</sub> + ''r''<sub>2</sub>)(''r''<sub>3</sub> + ''r''<sub>4</sub>)}}, {{math|−(''r''<sub>1</sub> + ''r''<sub>3</sub>)(''r''<sub>2</sub> + ''r''<sub>4</sub>)}}, and {{math|−(''r''<sub>1</sub> + ''r''<sub>4</sub>)(''r''<sub>2</sub> + ''r''<sub>3</sub>)}}. This is indeed true and it follows from [[Vieta's formulas]]. It also follows from Vieta's formulas, together with the fact that we are working with a depressed quartic, that {{math|''r''<sub>1</sub> + ''r''<sub>2</sub> + ''r''<sub>3</sub> + ''r''<sub>4</sub> {{=}} 0}}. (Of course, this also follows from the fact that {{math|''r''<sub>1</sub> + ''r''<sub>2</sub> + ''r''<sub>3</sub> + ''r''<sub>4</sub> {{=}} −''s'' + ''s''}}.) Therefore, if {{math|''α''}}, {{math|''β''}}, and {{math|''γ''}} are the roots of the resolvent cubic, then the numbers {{math|''r''<sub>1</sub>}}, {{math|''r''<sub>2</sub>}}, {{math|''r''<sub>3</sub>}}, and {{math|''r''<sub>4</sub>}} are such that\n:<math>\\left\\{\\begin{array}{l}r_1+r_2+r_3+r_4=0\\\\(r_1+r_2)(r_3+r_4)=-\\alpha\\\\(r_1+r_3)(r_2+r_4)=-\\beta\\\\(r_1+r_4)(r_2+r_3)=-\\gamma\\text{.}\\end{array}\\right.</math>\nIt is a consequence of the first two equations that {{math|''r''<sub>1</sub> + ''r''<sub>2</sub>}} is a square root of {{math|''α''}} and that {{math|''r''<sub>3</sub> + ''r''<sub>4</sub>}} is the other square root of {{math|''α''}}. For the same reason,\n* {{math|''r''<sub>1</sub> + ''r''<sub>3</sub>}} is a square root of {{math|''β''}},\n* {{math|''r''<sub>2</sub> + ''r''<sub>4</sub>}} is the other square root of {{math|''β''}},\n* {{math|''r''<sub>1</sub> + ''r''<sub>4</sub>}} is a square root of {{math|''γ''}},\n* {{math|''r''<sub>2</sub> + ''r''<sub>3</sub>}} is the other square root of {{math|''γ''}}.\nTherefore, the numbers {{math|''r''<sub>1</sub>}}, {{math|''r''<sub>2</sub>}}, {{math|''r''<sub>3</sub>}}, and {{math|''r''<sub>4</sub>}} are such that\n:<math>\\left\\{\\begin{array}{l}r_1+r_2+r_3+r_4=0\\\\r_1+r_2=\\sqrt{\\alpha}\\\\r_1+r_3=\\sqrt{\\beta}\\\\r_1+r_4=\\sqrt{\\gamma}\\text{;}\\end{array}\\right.</math>\nthe sign of the square roots will be dealt with below. The only solution of this system is:\n:<math>\\left\\{\\begin{array}{l}r_1=\\frac{\\sqrt{\\alpha}+\\sqrt{\\beta}+\\sqrt{\\gamma}}2\\\\[2mm]r_2=\\frac{\\sqrt{\\alpha}-\\sqrt{\\beta}-\\sqrt{\\gamma}}2\\\\[2mm]r_3=\\frac{-\\sqrt{\\alpha}+\\sqrt{\\beta}-\\sqrt{\\gamma}}2\\\\[2mm]r_4=\\frac{-\\sqrt{\\alpha}-\\sqrt{\\beta}+\\sqrt{\\gamma}}2\\text{.}\\end{array}\\right.</math>\nSince, in general, there are two choices for each square root, it might look as if this provides {{math|8 ({{=}} 2<sup>3</sup>)}} choices for the set {{math|{''r''<sub>1</sub>, ''r''<sub>2</sub>, ''r''<sub>3</sub>, ''r''<sub>4</sub>}}}, but, in fact, it provides no more than {{math|2}}&nbsp;such choices, because the consequence of replacing one of the square roots by the symmetric one is that the set {{math|{''r''<sub>1</sub>, ''r''<sub>2</sub>, ''r''<sub>3</sub>, ''r''<sub>4</sub>}}} becomes the set {{math|{−''r''<sub>1</sub>, −''r''<sub>2</sub>, −''r''<sub>3</sub>, −''r''<sub>4</sub>}}}.\n\nIn order to determine the right sign of the square roots, one simply chooses some square root for each of the numbers {{math|''α''}}, {{math|''β''}}, and {{math|''γ''}} and uses them to compute the numbers {{math|''r''<sub>1</sub>}}, {{math|''r''<sub>2</sub>}}, {{math|''r''<sub>3</sub>}}, and {{math|''r''<sub>4</sub>}} from the previous equalities. Then, one computes the number {{math|{{sqrt|''α''}}{{sqrt|''β''}}{{sqrt|''γ''}}}}. Note that, since {{math|''α''}}, {{math|''β''}}, and {{math|''γ''}} are the roots of (''{{EquationNote|2}}''), it is a consequence of Vieta's formulas that their product is equal to {{math|''q''<sup>2</sup>}} and therefore that {{math|{{sqrt|''α''}}{{sqrt|''β''}}{{sqrt|''γ''}} {{=}} ±''q''}}. But a straightforward computation shows that\n:{{math|{{sqrt|''α''}}{{sqrt|''β''}}{{sqrt|''γ''}} {{=}} ''r''<sub>1</sub>''r''<sub>2</sub>''r''<sub>3</sub> + ''r''<sub>1</sub>''r''<sub>2</sub>''r''<sub>4</sub> + ''r''<sub>1</sub>''r''<sub>3</sub>''r''<sub>4</sub> + ''r''<sub>2</sub>''r''<sub>3</sub>''r''<sub>4</sub>.}}\nIf this number is {{math|−''q''}}, then the choice of the square roots was a good one (again, by Vieta's formulas); otherwise, the roots of the polynomial will be {{math|−''r''<sub>1</sub>}}, {{math|−''r''<sub>2</sub>}}, {{math|−''r''<sub>3</sub>}}, and {{math|−''r''<sub>4</sub>}}, which are the numbers obtained if one of the square roots is replaced by the symmetric one (or, what amounts to the same thing, if each of the three square roots is replaced by the symmetric one).\n\nThis argument suggests another way of choosing the square roots:\n* pick ''any'' square root {{math|{{sqrt|''α''}}}} of {{math|''α''}} and ''any'' square root {{math|{{sqrt|''β''}}}} of {{math|''β''}};\n* ''define'' {{math|{{sqrt|''γ''}}}} as <math>-\\frac q{\\sqrt{\\alpha}\\sqrt{\\beta}}</math>.\nOf course, this will make no sense if {{math|''α''}} or {{math|''β''}} is equal to {{math|0}}, but {{math|0}} is  a root of (''{{EquationNote|2}}'') only when {{math|''q'' {{=}} 0}}, that is, only when we are dealing with a [[Quartic function#Biquadratic equation|biquadratic equation]], in which case there is a much simpler approach.\n\n====Solving by Lagrange resolvent====\nThe [[symmetric group]] {{math|''S''<sub>4</sub>}} on four elements has the [[Klein four-group]] as a [[normal subgroup]]. This suggests using a '''{{visible anchor|resolvent cubic}}''' whose roots may be variously described as a discrete Fourier transform or a [[Hadamard matrix]] transform of the roots; see [[Lagrange resolvents]] for the general method. Denote by {{math|''x<sub>i</sub>''}}, for {{math|''i''}} from&nbsp;{{math|0}} to&nbsp;{{math|3}}, the four roots of {{math|''x''<sup>4</sup> + ''bx''<sup>3</sup> + ''cx''<sup>2</sup> + ''dx'' + ''e''}}. If we set\n\n: <math> \\begin{align}\ns_0 &= \\tfrac12(x_0 + x_1 + x_2 + x_3), \\\\[4pt]\ns_1 &= \\tfrac12(x_0 - x_1 + x_2 - x_3), \\\\[4pt]\ns_2 &= \\tfrac12(x_0 + x_1 - x_2 - x_3), \\\\[4pt]\ns_3 &= \\tfrac12(x_0 - x_1 - x_2 + x_3),\n\\end{align}</math>\n\nthen since the transformation is an [[Involution (mathematics)|involution]] we may express the roots in terms of the four {{math|''s<sub>i</sub>''}} in exactly the same way. Since we know the value {{math|''s''<sub>0</sub> {{=}} −{{sfrac|''b''|2}}}}, we only need the values for {{math|''s''<sub>1</sub>}}, {{math|''s''<sub>2</sub>}} and {{math|''s''<sub>3</sub>}}. These are the roots of the polynomial\n\n:<math>(s^2 - {s_1}^2)(s^2-{s_2}^2)(s^2-{s_3}^2).</math>\n\nSubstituting the {{math|''s<sub>i</sub>''}} by their values in term of the {{math|''x<sub>i</sub>''}}, this polynomial may be expanded in a polynomial in {{math|''s''}} whose coefficients are [[symmetric polynomial]]s in the {{math|''x<sub>i</sub>''}}. By the [[fundamental theorem of symmetric polynomials]], these coefficients may be expressed as polynomials in the coefficients of the monic quartic. If, for simplification, we suppose that the quartic is depressed, that is {{math|''b'' {{=}} 0}}, this results in the polynomial\n{{NumBlk|:|<math> s^6+2cs^4+(c^2-4e)s^2-d^2 </math>|{{EquationRef|3}}}}\nThis polynomial is of degree six, but only of degree three in {{math|''s''<sup>2</sup>}}, and so the corresponding equation is solvable by the method described in the article about [[cubic function]]. By substituting the roots in the expression of the {{math|''x<sub>i</sub>''}} in terms of the {{math|''s<sub>i</sub>''}}, we obtain expression for the roots. In fact we obtain, apparently, several expressions, depending on the numbering of the roots of the cubic polynomial and of the signs given to their square roots. All these different expressions may be deduced from one of them by simply changing the numbering of the {{math|''x<sub>i</sub>''}}.\n\nThese expressions are unnecessarily complicated, involving the [[root of unity|cubic roots of unity]], which can be avoided as follows. If {{math|''s''}} is any non-zero root of (''{{EquationNote|3}}''), and if we set\n\n:<math>\\begin{align}\nF_1(x) & = x^2 + sx + \\frac{c}{2} + \\frac{s^2}{2} - \\frac{d}{2s} \\\\\nF_2(x) & = x^2 - sx + \\frac{c}{2} + \\frac{s^2}{2} + \\frac{d}{2s}\n\\end{align}</math>\n\nthen\n\n:<math>F_1(x)\\times F_2(x) = x^4 + cx^2 + dx + e.</math>\n\nWe therefore can solve the quartic by solving for {{math|''s''}} and then solving for the roots of the two factors using the [[quadratic formula]].\n\nNote that this gives exactly the same formula for the roots as the one provided by [[Quartic function#Descartes' solution|Descartes' method]].\n\n====Solving with algebraic geometry====\nThere is an alternative solution using algebraic geometry<ref>{{Citation|last = Faucette|first = William M.|journal = [[American Mathematical Monthly]]|pages = 51–57|title = A Geometric Interpretation of the Solution of the General Quartic Polynomial|volume = 103|year = 1996|issue = 1|doi = 10.2307/2975214|jstor = 2975214|mr = 1369151}}</ref> In brief, one interprets the roots as the intersection of two quadratic curves, then finds the three [[degenerate conic|reducible quadratic curves]] (pairs of lines) that pass through these points (this corresponds to the resolvent cubic, the pairs of lines being the Lagrange resolvents), and then use these linear equations to solve the quadratic.\n\nThe four roots of the depressed quartic {{math|''x''<sup>4</sup> + ''px''<sup>2</sup> + ''qx'' + ''r'' {{=}} 0}} may also be expressed as the {{mvar|x}} coordinates of the intersections of the two quadratic equations {{math|''y''<sup>2</sup> + ''py'' + ''qx'' + ''r'' {{=}} 0}} and {{math|''y'' − ''x''<sup>2</sup> {{=}} 0}} i.e., using the substitution {{math|''y'' {{=}} ''x''<sup>2</sup>}} that two quadratics intersect in four points is an instance of [[Bézout's theorem]]. Explicitly, the four points are {{math|''P<sub>i</sub>'' &#8788; (''x<sub>i</sub>'', ''x<sub>i</sub>''<sup>2</sup>)}} for the four roots {{math|''x<sub>i</sub>''}} of the quartic.\n\nThese four points are not collinear because they lie on the irreducible quadratic {{math|''y'' {{=}} ''x''<sup>2</sup>}} and thus there is a 1-parameter family of quadratics (a [[pencil of curves]]) passing through these points. Writing the projectivization of the two quadratics as [[quadratic form]]s in three variables:\n:<math>\\begin{align}\nF_1(X,Y,Z) &:= Y^2 + pYZ + qXZ + rZ^2,\\\\\nF_2(X,Y,Z) &:= YZ - X^2\n\\end{align}</math>\nthe pencil is given by the forms {{math|''λF''<sub>1</sub> + ''μF''<sub>2</sub>}} for any point {{math|[''λ'', ''μ'']}} in the projective line — in other words, where {{math|''λ''}} and {{math|''μ''}} are not both zero, and multiplying a quadratic form by a constant does not change its quadratic curve of zeros.\n\nThis pencil contains three reducible quadratics, each corresponding to a pair of lines, each passing through two of the four points, which can be done <math>\\textstyle{\\binom{4}{2}}</math>&nbsp;=&nbsp;{{math|6}} different ways. Denote these {{math|''Q''<sub>1</sub> {{=}} ''L''<sub>12</sub> + ''L''<sub>34</sub>}}, {{math|''Q''<sub>2</sub> {{=}} ''L''<sub>13</sub> + ''L''<sub>24</sub>}}, and  {{math|''Q''<sub>3</sub> {{=}} ''L''<sub>14</sub> + ''L''<sub>23</sub>}}. Given any two of these, their intersection has exactly the four points.\n\nThe reducible quadratics, in turn, may be determined by expressing the quadratic form {{math|''λF''<sub>1</sub> + ''μF''<sub>2</sub>}} as a {{math|3×3}}&nbsp;matrix: reducible quadratics correspond to this matrix being singular, which is equivalent to its determinant being zero, and the determinant is a homogeneous degree three polynomial in {{math|''λ''}} and {{math|''μ''}} and corresponds to the resolvent cubic.\n\n==See also==\n*[[Linear function]]\n*[[Quadratic function]]\n*[[Cubic function]]\n*[[Quintic function]]\n*[[Polynomial]]\n*[[Newton's method]]\n\n==References==\n{{reflist}}\n\n==Further reading==\n* {{cite journal |author=Carpenter, W. |title=On the solution of the real quartic |journal=[[Mathematics Magazine]] |volume=39 |year=1966 |pages=28–30 |doi=10.2307/2688990}}\n* {{cite journal |author1=Yacoub,M.D.|author2=Fraidenraich, G. |title=A solution to the quartic equation |journal=[[Mathematical Gazette]] |volume=96|year=July 2012|pages=271–275}}\n\n==External links==\n*{{PlanetMath | urlname = QuarticFormula | title = Quartic formula as four single equations }}{{dead link|date=January 2016}}\n*[http://members.tripod.com/l_ferrari/quartic_equation.htm Ferrari's achievement]\n\n{{Polynomials}}\n\n[[Category:Elementary algebra]]\n[[Category:Equations]]\n[[Category:Polynomial functions]]"
    },
    {
      "title": "Quasi-polynomial",
      "url": "https://en.wikipedia.org/wiki/Quasi-polynomial",
      "text": "{{for|quasi-polynomial time complexity of algorithms|Quasi-polynomial time}}\nIn [[mathematics]], a '''quasi-polynomial''' ('''pseudo-polynomial''') is a generalization of [[polynomial]]s. While the coefficients of a polynomial come from a [[ring (mathematics)|ring]], the coefficients of quasi-polynomials are instead [[periodic function]]s with integral period. Quasi-polynomials appear throughout much of [[combinatorics]] as the enumerators for various objects.\n\nA quasi-polynomial can be written as <math>q(k) = c_d(k) k^d + c_{d-1}(k) k^{d-1} + \\cdots + c_0(k)</math>, where <math>c_i(k)</math> is a periodic function with integral period. If <math>c_d(k)</math> is not identically zero, then the degree of <math>q</math> is <math>d</math>. Equivalently, a function <math>f \\colon \\mathbb{N} \\to \\mathbb{N}</math> is a quasi-polynomial if there exist polynomials <math>p_0, \\dots, p_{s-1}</math> such that <math>f(n) = p_i(n)</math> when <math>n \\equiv i \\bmod s</math>. The polynomials <math>p_i</math> are called the constituents of <math>f</math>.\n\n==Examples==\n* Given a <math>d</math>-dimensional [[polytope]] <math>P</math> with [[rational number|rational]] vertices <math>v_1,\\dots,v_n</math>, define <math>tP</math> to be the [[convex hull]] of <math>tv_1,\\dots,tv_n</math>. The function <math>L(P,t) = \\#(tP \\cap \\mathbb{Z}^d)</math> is a quasi-polynomial in <math>t</math> of degree <math>d</math>. In this case, <math>L(P,t)</math> is a function <math>\\mathbb{N} \\to \\mathbb{N}</math>. This is known as the '''Ehrhart quasi-polynomial''', named after [[Eugène Ehrhart]].\n* Given two quasi-polynomials <math>F</math> and <math>G</math>, the [[convolution]] of <math>F</math> and <math>G</math> is\n:: <math>(F*G)(k) = \\sum_{m=0}^k F(m)G(k-m)</math>\nwhich is a quasi-polynomial with degree <math>\\le \\deg F + \\deg G + 1.</math>\n\n==See also==\n* [[Ehrhart polynomial]]\n\n==References==\n* [[Richard P. Stanley|Stanley, Richard P.]] (1997). [http://www-math.mit.edu/~rstan/ec/ ''Enumerative Combinatorics'', Volume 1]. Cambridge University Press.  {{isbn|0-521-55309-1}}, 0-521-56069-1.\n\n[[Category:Polynomials]]\n[[Category:Algebraic combinatorics]]\n\n{{combin-stub}}"
    },
    {
      "title": "Quintic function",
      "url": "https://en.wikipedia.org/wiki/Quintic_function",
      "text": "[[File:Quintic polynomial.svg|thumb|right|233px|Graph of a polynomial of degree 5, with 3 real zeros (roots) and 4 [[critical point (mathematics)|critical points]].]]\n\nIn [[algebra]], a '''quintic function''' is a [[function (mathematics)|function]] of the form\n\n:<math>g(x)=ax^5+bx^4+cx^3+dx^2+ex+f,\\,</math>\n\nwhere {{mvar|a}}, {{mvar|b}}, {{mvar|c}}, {{mvar|d}}, {{mvar|e}} and {{mvar|f}} are members of a [[field (mathematics)|field]], typically the [[rational number]]s, the [[real number]]s or the [[complex number]]s, and {{mvar|a}} is nonzero. In other words, a quintic function is defined by a [[polynomial]] of [[Degree of a polynomial|degree]] five.\n\nIf {{mvar|a}} is zero but one of the coefficients {{mvar|b}}, {{mvar|c}}, {{mvar|d}}, or {{mvar|e}} is non-zero, the function is classified as either a [[quartic function]], [[cubic function]], [[quadratic function]] or [[linear function]].\n\nBecause they have an odd degree, normal quintic functions appear similar to normal [[cubic function]]s when graphed, except they may possess an additional [[Maxima and minima|local maximum]] and local minimum each. The [[derivative]] of a quintic function is a [[quartic function]].\n\nSetting {{math|''g''(''x'') {{=}} 0}} and assuming {{math|''a'' ≠ 0}} produces a '''quintic equation''' of the form:\n:<math>ax^5+bx^4+cx^3+dx^2+ex+f=0.\\,</math> \nSolving quintic equations in terms of radicals was a major problem in algebra from the 16th century, when [[cubic equation|cubic]] and [[quartic equation]]s were solved, until the first half of the 19th century, when the impossibility of such a general solution was proved with the [[Abel–Ruffini theorem]].\n\n==Finding roots of a quintic equation==\n\nFinding the roots of a given polynomial has been a prominent mathematical problem.\n\nSolving [[Linear equation|linear]], [[Quadratic equation|quadratic]], [[Cubic equation|cubic]] and [[quartic equation]]s by [[factorization]] into [[Nth root|radical]]s can always be done, no matter whether the roots are rational or irrational, real or complex; there are formulae that yield the required solutions. However, there is no [[algebraic expression]] for general quintic equations over the rationals in terms of radicals; this statement is known as the [[Abel–Ruffini theorem]], first asserted in 1799 and completely proved in 1824. This result also holds for equations of higher degrees. An example of a quintic whose roots cannot be expressed in terms of radicals is {{math|''x''<sup>5</sup> − ''x'' + 1 {{=}} 0}}. This quintic is in [[Bring–Jerrard normal form]].\n\nSome quintics may be solved in terms of radicals. However, the solution is generally too complex to be used in practice. Instead, numerical approximations are calculated using a [[root-finding algorithm#Finding roots of polynomials|root-finding algorithm for polynomials]].\n\n==Solvable quintics==\n\nSome quintic equations can be solved in terms of radicals. These include the quintic equations defined by a polynomial that is [[irreducible polynomial|reducible]], such as {{math|''x''<sup>5</sup> − ''x''<sup>4</sup> − ''x'' + 1 {{=}} (''x''<sup>2</sup> + 1)(''x'' + 1)(''x'' − 1)<sup>2</sup>}}. For example, it has been shown<ref>Michele Elia and Piero Filipponi. \"Equations of the Bring-Jerrard form, the golden section, and square Fibonacci numbers\", ''Fibonacci Quarterly'' 36, June-July 1998, 282–286. http://www.fq.math.ca/Scanned/36-3/elia.pdf </ref> that \n\n:<math>x^5-x-r=0</math>\n\nhas solutions in radicals if and only if it has an integer solution or ''r'' is one of ±15, ±22440, or ±2759640, in which cases the polynomial is reducible.\n\nAs solving reducible quintic equations reduces immediately to solving polynomials of lower degree, only irreducible quintic equations are considered in the remainder of this section, and the term \"quintic\" will refer only to irreducible quintics. A '''solvable quintic''' is thus an irreducible quintic polynomial whose roots may be expressed in terms of radicals.\n\nTo characterize solvable quintics, and more generally solvable polynomials of higher degree, [[Évariste Galois]] developed techniques which gave rise to [[group theory]] and [[Galois theory]]. Applying these techniques, [[Arthur Cayley]] found a general criterion for determining whether any given quintic is solvable.<ref>A. Cayley. ''On a new auxiliary equation in the theory of equation of the fifth order'', Philosophical Transactions of the Royal Society of London (1861).</ref> This criterion is the following.<ref>This formulation of Cayley's result is extracted from Lazard (2004) paper.</ref>\n\nGiven the equation\n:<math> ax^5+bx^4+cx^3+dx^2+ex+f=0,</math>\nthe [[Tschirnhaus transformation]] {{math|''x'' {{=}} ''y'' − {{sfrac|''b''|5''a''}}}}, which depresses the quintic (that means removes the term of degree four), gives the equation\n\n:<math> y^5+p y^3+q y^2+r y+s=0</math>,\n\nwhere\n\n:<math>\\begin{align}p &= \\frac{5ac-2b^2}{5a^2}\\\\\nq &= \\frac{25a^2d-15abc+4b^3}{25a^3}\\\\\nr &= \\frac{125a^3e-50a^2bd+15ab^2c-3b^4}{125a^4}\\\\\ns &= \\frac{3125 a^4f-625a^3 be+125a^2b^2 d-25ab^3 c+4 b^5}{3125a^5}\\end{align}</math>\n\nBoth quintics are solvable by radicals if and only if either they are factorisable in equations of lower degrees with rational coefficients or the polynomial {{math|''P''<sup>2</sup> − 1024''z''Δ}}, named ''Cayley's resolvent'', has a rational root in {{mvar|z}}, where\n\n:<math>\nP =z^3-z^2(20r+3p^2)- z(8p^2r - 16pq^2- 240r^2 + 400sq - 3p^4)\n</math>\n::<math>\n{} - p^6 + 28p^4r- 16p^3q^2- 176p^2r^2- 80p^2sq + 224prq^2- 64q^4\n</math>\n::<math>\n{} + 4000ps^2 + 320r^3- 1600rsq\n</math>\n\nand\n\n:<math>\\Delta=-128p^2r^4+3125s^4-72p^4qrs+560p^2qr^2s+16p^4r^3+256r^5+108p^5s^2\n</math>\n::<math>\n{} -1600qr^3s+144pq^2r^3-900p^3rs^2+2000pr^2s^2-3750pqs^3+825p^2q^2s^2\n</math>\n::<math>\n{} +2250q^2rs^2+108q^5s-27q^4r^2-630pq^3rs+16p^3q^3s-4p^3q^2r^2.\n</math>\n\nCayley's result allows us to test if a quintic is solvable. If it is the case, finding its roots is a more difficult problem, which consists of expressing the roots in terms of radicals involving the coefficients of the quintic and the rational root of Cayley's resolvent.\n\nIn 1888, [[George Paxton Young]]<ref>George Paxton Young. ''Solvable Quintics Equations with Commensurable Coefficients'' ''American Journal of Mathematics'' '''10''' (1888), 99–130 [https://www.jstor.org/pss/2369502 at JSTOR]</ref> described how to solve a solvable quintic equation, without providing an explicit formula; [[Daniel Lazard]] wrote out a three-page formula (Lazard (2004)).\n\n===Quintics in Bring–Jerrard form===\n\nThere are several parametric representations of solvable quintics of the form {{math|''x''<sup>5</sup> + ''ax'' + ''b'' {{=}} 0}}, called the [[Bring–Jerrard form]].\n\nDuring the second half of 19th century, John Stuart Glashan, George Paxton Young, and [[Carl Runge]] gave such a parameterization: an [[irreducible polynomial|irreducible]] quintic with rational coefficients in Bring–Jerrard form\nis solvable if and only if either {{math|''a'' {{=}} 0}} or it may be written\n:<math>x^5 + \\frac{5\\mu^4(4\\nu + 3)}{\\nu^2 + 1}x + \\frac{4\\mu^5(2\\nu + 1)(4\\nu + 3)}{\\nu^2 + 1} = 0</math>\nwhere {{math|''μ''}} and {{math|''ν''}} are rational.\n\nIn 1994, Blair Spearman and Kenneth S. Williams gave an alternative,\n:<math>x^5 + \\frac{5e^4( 4c + 3)}{c^2 + 1}x + \\frac{-4e^5(2c-11)}{c^2 + 1} = 0.</math>\n\nThe relationship between the 1885 and 1994 parameterizations can be seen by defining the expression\n:<math>b = \\frac{4}{5} \\left(a+20 \\pm 2\\sqrt{(20-a)(5+a)}\\right)</math>\nwhere {{math|''a'' {{=}} {{sfrac|5(4''ν'' + 3)|''ν''<sup>2</sup> + 1}}}}. Using the negative case of the square root yields, after scaling variables, the first parametrization while the positive case gives the second.\n\nThe substitution {{math|''c'' {{=}} {{sfrac|−''m''|''l''<sup>5</sup>}}}}, {{math|''e'' {{=}} {{sfrac|1|''l''}}}} in the Spearman-Williams parameterization allows one to not exclude the special case {{math|''a'' {{=}} 0}}, giving the following result:\n\nIf {{mvar|a}} and {{mvar|b}} are rational numbers, the equation {{math|''x''<sup>5</sup> + ''ax'' + ''b'' {{=}} 0}} is solvable by radicals if either its left-hand side is a product of polynomials of degree less than 5 with rational coefficients or there exist two rational numbers {{mvar|l}} and {{mvar|m}} such that\n:<math>a=\\frac{5 l (3 l^5-4 m)}{m^2+l^{10}}\\qquad b=\\frac{4(11 l^5+2 m)}{m^2+l^{10}}.</math>\n\n===Roots of a solvable quintic===\nA polynomial equation is solvable by radicals if its [[Galois group]] is a [[solvable group]]. In the case of irreducible quintics, the Galois group is a subgroup of the [[symmetric group]] {{math|''S''<sub>5</sub>}} of all permutations of a five element set, which is solvable if and only if it is a subgroup of the group {{math|''F''<sub>5</sub>}}, of order {{math|20}}, generated by the cyclic permutations {{math|(1 2 3 4 5)}} and {{math|(1 2 4 3)}}.\n\nIf the quintic is solvable, one of the solutions may be represented by an [[algebraic expression]] involving a fifth root and at most two square roots, generally [[nested radical|nested]]. The other solutions may then be obtained either by changing of fifth root or by multiplying all the occurrences of the fifth root by the same power of a [[root of unity|primitive 5th root of unity]]\n:<math>\\frac{\\sqrt{-10-2\\sqrt{5}}+\\sqrt{5}-1}{4}.</math>\n\nAll four primitive fifth roots of unity may be obtained by changing the signs of the square roots appropriately, namely:\n:<math>\\frac{\\alpha\\sqrt{-10-2\\beta\\sqrt{5}}+\\beta\\sqrt{5}-1}{4},</math>\n\nwhere <math> \\alpha, \\beta \\in \\{-1,1\\}</math>, yielding the four distinct primitive fifth roots of unity.\n\nIt follows that one may need four different square roots for writing all the roots of a solvable quintic. Even for the first root that involves at most two square roots, the expression of the solutions in terms of radicals is usually huge. However, when no square root is needed, the form of the first solution may be rather simple, as for the equation {{math|''x''<sup>5</sup> − 5''x''<sup>4</sup> + 30''x''<sup>3</sup> − 50''x''<sup>2</sup> + 55''x'' − 21 {{=}} 0}}, for which the only real solution is\n\n: <math>x=1+\\sqrt[5]{2}-\\left(\\sqrt[5]{2}\\right)^2+\\left(\\sqrt[5]{2}\\right)^3-\\left(\\sqrt[5]{2}\\right)^4.</math>\n\nAn example of a more complex (although small enough to be written here) solution is the unique real root of {{math|''x''<sup>5</sup> − 5''x'' + 12 {{=}} 0}}. Let {{math|''a'' {{=}} {{sqrt|2''φ''<sup>−1</sup>}}}}, {{math|''b'' {{=}} {{sqrt|2''φ''}}}}, and {{math|''c'' {{=}} {{radic|5|4}}}}, where {{math|''φ'' {{=}} {{sfrac|1+{{sqrt|5}}|2}}}} is the [[golden ratio]]. Then the only real solution {{math|''x'' {{=}} −1.84208&hellip;}} is given by\n\n: <math>-cx = \\sqrt[5]{(a+c)^2(b-c)} + \\sqrt[5]{(-a+c)(b-c)^2} + \\sqrt[5]{(a+c)(b+c)^2} - \\sqrt[5]{(-a+c)^2(b+c)} \\,,</math>\n\nor, equivalently, by\n\n:<math>x = \\sqrt[5]{y_1}+\\sqrt[5]{y_2}+\\sqrt[5]{y_3}+\\sqrt[5]{y_4}\\,,</math>\n\nwhere the {{math|''y<sub>i</sub>''}} are the four roots of the [[quartic equation]]\n\n:<math>y^4+4y^3+\\frac{4}{5}y^2-\\frac{8}{5^3}y-\\frac{1}{5^5}=0\\,.</math>\n\nMore generally, if an equation {{math|1=''P''(''x'') = 0}} of prime degree {{math|''p''}} with rational coefficients is solvable in radicals, then one can define an auxiliary equation {{math|1=''Q''(''y'') = 0}} of degree {{math|''p'' – 1}}, also with rational coefficients, such that each root of {{math|''P''}} is the sum of {{math|''p''}}-th roots of the roots of {{math|''Q''}}. These {{math|''p''}}-th roots have been introduced by [[Joseph-Louis Lagrange]], and their product by {{math|''p''}} are commonly called [[Lagrange resolvent]]s. The computation of {{math|''Q''}} and its roots can be used to solve {{math|1=''P''(''x'') = 0}}. However these {{math|''p''}}-th roots may not be computed independently (this would provide {{math|''p''<sup>''p''–1</sup>}} roots instead of {{math|''p''}}). Thus a correct solution needs to express all these {{math|''p''}}-roots in term of one of them. Galois theory shows that this is always theoretically possible, even if the resulting formula may be too large to be of any use.\n\nIt is possible that some of the roots of {{math|''Q''}} are rational (as in the first example of this section) or some are zero. In these cases, the formula for the roots is much simpler, as for the solvable [[de Moivre]] quintic{{anchor|de Moivre quintic}}\n\n:<math>x^5+5ax^3+5a^2x+b = 0\\,,</math>\n\nwhere the auxiliary equation has two zero roots and reduces, by factoring them out, to the [[quadratic equation]]\n\n:<math>y^2+by-a^5 = 0\\,,</math>\n\nsuch that the five roots of the de Moivre quintic are given by\n\n:<math>x_k = \\omega^k\\sqrt[5]{y_i} -\\frac{a}{\\omega^k\\sqrt[5]{y_i}},</math>\n\nwhere ''y<sub>i</sub>'' is any root of the auxiliary quadratic equation and ''ω'' is any of the four [[primitive root of unity|primitive 5th roots of unity]].  This can be easily generalized to construct a solvable [[septic equation|septic]] and other odd degrees, not necessarily prime.\n\n===Other solvable quintics===\n\nThere are infinitely many solvable quintics in Bring-Jerrard form which have been parameterized in a preceding section.\n\nUp to the scaling of the variable, there are exactly five solvable quintics of the shape <math>x^5+ax^2+b</math>, which are<ref>http://www.math.harvard.edu/~elkies/trinomial.html</ref> (where ''s'' is a scaling factor):\n:<math>x^5-2s^3x^2-\\frac{s^5}{5} </math>\n:<math> x^5-100s^3x^2-1000s^5</math>\n:<math>x^5-5s^3x^2-3s^5 </math>\n:<math>x^5-5s^3x^2+15s^5 </math>\n:<math> x^5-25s^3x^2-300s^5</math>\n\nPaxton Young (1888) gave a number of examples of solvable quintics:\n:{| <math>x^5+3x^2+2x-1 </math> ||\n|-\n| <math> x^5-10x^3-20x^2-1505x-7412</math> ||\n|-\n| <math>x^5+\\frac{625}{4}x+3750 </math>||\n|-\n| <math>x^5-\\frac{22}{5}x^3-\\frac{11}{25}x^2+\\frac{462}{125}x+\\frac{979}{3125} </math> ||\n|-\n| <math>x^5+20x^3+20x^2+30x+10 </math> || <math>~\\qquad ~</math> Root: <math> \\sqrt[5]{2}-\\sqrt[5]{2}^2+\\sqrt[5]{2}^3-\\sqrt[5]{2}^4</math>\n|-\n|<math>x^5-20x^3+250x-400 </math> ||\n|-\n| <math>x^5-5x^3+\\frac{85}{8}x-\\frac{13}{2} </math> ||\n|-\n|<math> x^5+\\frac{20}{17}x+\\frac{21}{17}</math> ||\n|-\n|<math>x^5-\\frac{4}{13}x+\\frac{29}{65}</math>||\n|-\n|<math> x^5+\\frac{10}{13}x+\\frac{3}{13}\t</math> ||\n|-\n| <math> x^5+110(5x^3+60x^2+800x+8320)</math> ||\n|-\n| <math>x^5-20 x^3 -80 x^2 -150 x -656 </math>||\n|-\n| <math> x^5 -40 x^3 +160 x^2 +1000 x -5888 </math> ||\n|-\n|<math> x^5-50x^3-600x^2-2000x-11200</math> ||\n|-\n| <math>x^5+110(5 x^3 + 20x^2 -360 x +800) </math> ||\n|-\n| <math> x^5-20 x^3 +170 x + 208</math>||\n|}\n\nAn infinite sequence of solvable quintics may be constructed, whose roots are sums of ''n''-th [[roots of unity]], with ''n'' = 10''k'' + 1 being a prime number:\n\n:{|\n|-\n| <math>x^5+x^4-4x^3-3x^2+3x+1</math> || || Roots: <math>2\\cos(\\frac{2k\\pi}{11})</math>\n|-\n| <math> x^5+x^4-12x^3-21x^2+x+5</math>|| || Root: <math> \\sum_{k=0}^5 e^\\frac{2i\\pi 6^k }{31}</math>\n|-\n| <math>x^5+x^4-16x^3+5x^2+21x-9 </math>|| || Root: <math>\\sum_{k=0}^7 e^\\frac{2i\\pi 3^k }{41}</math>\n|-\n| <math>x^5+x^4-24x^3-17x^2+41x-13</math>|| <math>~\\qquad ~</math> || {{nowrap|1= Root: <math>\\sum_{k=0}^{11} e^\\frac{2i\\pi (21)^k }{61}</math>}}\n|-\n| <math>x^5+x^4-28x^3+37x^2+25x+1</math>|| || {{nowrap|1= Root: <math>\\sum_{k=0}^{13} e^\\frac{2i\\pi (23)^k }{71}</math>}}\n|}\n\nThere are also two parameterized families of solvable quintics:\nThe Kondo–Brumer quintic,\n\n:<math>x^5+(a-3)x^4+(-a+b+3)x^3+(a^2-a-1-2b)x^2+bx+a = 0\\,</math>\n\nand the family depending on the parameters <math>a, l, m</math>\n:<math>\tx^5-5p(2x^3 + ax^2 + bx)-pc = 0\\,</math>\n\nwhere\n\n::<math>p=\\frac{l^2(4m^2+a^2)-m^2}{4}, \\qquad \tb=l(4m^2+a^2)-5p-2m^2, \\qquad c=\\frac{b(a+4m)-p(a-4m)-a^2m}{2}</math>\n\n===''Casus irreducibilis''===\n\nAnalogously to [[cubic equation]]s, there are solvable quintics which have five real roots all of whose solutions in radicals involve roots of complex numbers. This is ''[[casus irreducibilis]]'' for the quintic, which is discussed in Dummit.<ref>David S. Dummit [http://www.emba.uvm.edu/~dummit/quintics/solvable.pdf Solving Solvable Quintics]</ref>{{rp|p.17}}\n\n==Beyond radicals==\n\nAbout 1835, [[George Jerrard|Jerrard]] demonstrated that quintics can be solved by using [[ultraradical]]s (also known as [[Bring radical]]s), the unique real root of {{math|''t''<sup>5</sup> + ''t'' − ''a'' {{=}} 0}} for real numbers {{math|''a''}}. In 1858 [[Charles Hermite]] showed that the Bring radical could be characterized in terms of the Jacobi [[theta functions]] and their associated [[elliptic modular function]]s, using an approach similar to the more familiar approach of solving [[cubic equation]]s by means of [[trigonometric function]]s.  At around the same time, [[Leopold Kronecker]], using [[group theory]], developed a simpler way of deriving Hermite's result, as had [[Francesco Brioschi]].  Later, [[Felix Klein]] came up with a method that relates the symmetries of the [[icosahedron]], [[Galois theory]], and the elliptic modular functions that are featured in Hermite's solution, giving an explanation for why they should appear at all, and developed his own solution in terms of [[generalized hypergeometric function]]s.<ref>{{Harv|Klein|1888}}; a modern exposition is given in {{Harv|Tóth|2002|loc=Section 1.6, Additional Topic: Klein's Theory of the Icosahedron, [https://books.google.com/books?id=i76mmyvDHYUC&pg=PA66 p. 66]}}</ref> Similar phenomena occur in degree {{math|7}} ([[septic equation]]s) and {{math|11}}, as studied by Klein and discussed in {{slink|Icosahedral symmetry|Related geometries}}.\n\n=== Solving through Bring radical ===\n{{main article|Bring radical}}\n\nA [[Tschirnhaus transformation]], which may be computed by solving a [[quartic equation]], reduces the general quintic equation of the form \n:<math>x^5 + a_4x^4 + a_3x^3 + a_2x^2 + a_1x + a_0 = 0\\,</math> \nto the [[Bring–Jerrard normal form]] {{math|''x''<sup>5</sup> − ''x'' + ''t'' {{=}} 0}}.\n\nThe roots of this equation cannot be expressed by radicals. However, in 1858, [[Charles Hermite]] published the first known solution of this equation in terms of [[elliptic function]]s.<ref name=\"hermite\">{{cite journal\n | last = Hermite\n | first = Charles\n | year = 1858\n | title =  Sur la résolution de l'équation du cinquième degré \n | journal = Comptes Rendus de l'Académie des Sciences\n | volume = XLVI\n | issue = I\n | pages = 508–515}}</ref>\nAt around the same time [[Francesco Brioschi]]<ref>\n{{cite journal\n | last = Brioschi\n | first = Francesco\n | year = 1858\n | title =  Sul Metodo di Kronecker per la Risoluzione delle Equazioni di Quinto Grado \n | journal = Atti dell'i. R. Istituto Lombardo di scienze, lettere ed arti\n | volume = I\n | pages = 275–282}}</ref> \nand [[Leopold Kronecker]]<ref>\n{{cite journal\n | last = Kronecker\n | first = Leopold\n | year = 1858\n | title =  Sur la résolution de l'equation du cinquième degré, extrait d'une lettre adressée à M. Hermite \n | journal = Comptes Rendus de l'Académie des Sciences\n | volume = XLVI\n | issue = I\n | pages = 1150–1152}}</ref>\ncame upon equivalent solutions.\n\nSee [[Bring radical]] for details on these solutions and some related ones.\n\n==Application to celestial mechanics==\nSolving for the locations of the [[Lagrangian point]]s of an astronomical orbit in which the masses of both objects are non-negligible involves solving a quintic.\n\nMore precisely, the locations of ''L''<sub>2</sub> and ''L''<sub>1</sub> are the solutions to the following equations, where the gravitational forces of two masses on a third (for example, Sun and Earth on satellites such as [[Gaia probe|Gaia]] at ''L''<sub>2</sub> and [[Solar and Heliospheric Observatory|SOHO]] at ''L''<sub>1</sub>) provide the satellite's centripetal force necessary to be in a synchronous orbit with Earth around the Sun:\n\n: <math>\\frac{G m M_S}{(R \\pm r)^2} \\pm \\frac{G m M_E}{r^2} = m \\omega^2 (R \\pm r)</math>\n\nThe ± sign corresponds to ''L''<sub>2</sub> and ''L''<sub>1</sub>, respectively; ''G'' is the [[gravitational constant]], ''ω'' the [[angular velocity]], ''r'' the distance of the satellite to Earth, ''R'' the distance Sun to Earth (that is, the [[semi-major axis]] of Earth's orbit), and ''m'', ''M<sub>S</sub>'', and ''M<sub>E</sub>'' are the respective masses of satellite, [[Earth]], and [[Sun]].\n\nUsing Kepler's Third Law <math>\\omega^2=\\frac{4 \\pi^2}{P^2}=\\frac{G (M_S+M_E)}{R^3}</math> and rearranging all terms yields the quintic\n\n: <math>a r^5 + b r^4 + c r^3 + d r^2 + e r + f = 0</math>\n\nwith <math>a = \\pm (M_S + M_E)</math> , <math>b = + (M_S + M_E) 3 R</math> , <math>c = \\pm (M_S + M_E) 3 R^2</math> , <math>d = + (M_E \\mp M_E) R^3</math> (thus ''d'' = 0 for ''L''<sub>2</sub>), <math>e = \\mp M_E 2 R^4</math> , <math>f = \\mp M_E R^5</math> .\n\nSolving these two quintics yields {{math|1=''r'' = 1.501 x 10<sup>9</sup> ''m''}} for ''L''<sub>2</sub> and {{math|1=''r'' = 1.491 x 10<sup>9</sup> ''m''}} for ''L''<sub>1</sub>. The [[List of objects at Lagrangian points|Sun–Earth Lagrangian points]] ''L''<sub>2</sub> and ''L''<sub>1</sub> are usually given as 1.5 million km from Earth.\n\n==See also==\n*[[Sextic equation]]\n*[[Septic function]]\n*[[Theory of equations]]\n\n==Notes==\n{{reflist}}\n\n==References==\n* Charles Hermite, \"Sur la résolution de l'équation du cinquème degré\", ''Œuvres de Charles Hermite'', t.2, pp.&nbsp;5–21, Gauthier-Villars, 1908.\n* Felix Klein, [https://archive.org/details/cu31924059413439 ''Lectures on the Icosahedron and the Solution of Equations of the Fifth Degree''], trans. George Gavin Morrice, Trübner & Co., 1888. {{isbn|0-486-49528-0}}.\n* Leopold Kronecker, \"Sur la résolution de l'equation du cinquième degré, extrait d'une lettre adressée à M. Hermite\", ''Comptes Rendus de l'Académie des Sciences'', t. XLVI, 1858 (1), pp.&nbsp;1150–1152.\n* Blair Spearman and Kenneth S. Williams, \"Characterization of solvable quintics {{math|''x''<sup>5</sup> + ''ax'' + ''b''}}, ''American Mathematical Monthly'', Vol. 101 (1994), pp.&nbsp;986–992.\n* Ian Stewart, ''Galois Theory'' 2nd Edition, Chapman and Hall, 1989. {{isbn|0-412-34550-1}}. Discusses Galois Theory in general including a proof of insolvability of the general quintic.\n* [[Jörg Bewersdorff]], ''Galois theory for beginners: A historical perspective'', American Mathematical Society, 2006. {{isbn|0-8218-3817-2}}. Chapter 8 ({{webarchive|url=https://web.archive.org/web/20100331181637/http://www.ams.org/bookstore/pspdf/stml-35-prev.pdf|title=The solution of equations of the fifth degree|date=31 March 2010}}) gives a description of the solution of solvable quintics {{math|''x''<sup>5</sup> + ''cx'' + ''d''}}.\n* Victor S. Adamchik and David J. Jeffrey, \"Polynomial transformations of Tschirnhaus, Bring and Jerrard,\" ''ACM SIGSAM Bulletin'', Vol. 37, No. 3, September 2003, pp.&nbsp;90–94.\n* Ehrenfried Walter von Tschirnhaus, \"A method for removing all intermediate terms from a given equation,\" ''ACM SIGSAM Bulletin'', Vol. 37, No. 1, March 2003, pp.&nbsp;1–3.\n* Daniel Lazard, \"Solving quintics in radicals\", in [[Olav Arnfinn Laudal]], [[Ragni Piene]], ''The Legacy of Niels Henrik Abel'', pp.&nbsp;207–225, Berlin, 2004, {{isbn|3-540-43826-2}}, available at {{webarchive |url=https://web.archive.org/web/20050106213419/http://www.loria.fr/publications/2002/A02-R-449/A02-R-449.ps |date=January 6, 2005 }}\n* {{citation | title = Finite Möbius groups, minimal immersions of spheres, and moduli| first = Gábor | last = Tóth | year = 2002 }}\n{{refend}}\n\n==External links==\n* [http://mathworld.wolfram.com/QuinticEquation.html Mathworld - Quintic Equation] – more details on methods for solving Quintics.\n* [http://www.emba.uvm.edu/~dummit/quintics/solvable.pdf Solving Solvable Quintics] – a method for solving solvable quintics due to David S. Dummit.\n* [https://web.archive.org/web/20090226035640/http://www.sigsam.org/bulletin/articles/143/tschirnhaus.pdf A method for removing all intermediate terms from a given equation] - a recent English translation of Tschirnhaus' 1683 paper.\n\n{{Polynomials}}\n{{Interwiki extra|qid=Q768390}}\n{{DEFAULTSORT:Quintic Equation}}\n[[Category:Equations]]\n[[Category:Galois theory]]\n[[Category:Polynomial functions]]"
    },
    {
      "title": "Rainville polynomials",
      "url": "https://en.wikipedia.org/wiki/Rainville_polynomials",
      "text": "In mathematics, the '''Rainville polynomials'''  ''p''<sub>''n''</sub>(''z'') are  polynomials introduced by {{harvtxt|Rainville|1945}} given by the [[generating function]]\n\n:<math>\\displaystyle e^wI_0(zw) = \\sum_np_n(z)w^n</math>\n\n{{harvtxt|Boas|Buck|1958|loc=p.46}}.\n\n==References==\n\n*{{Citation | last1=Boas | first1=Ralph P. | last2=Buck | first2=R. Creighton | title=Polynomial expansions of analytic functions | url=https://books.google.com/books?id=eihMuwkh4DsC | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Ergebnisse der Mathematik und ihrer Grenzgebiete. Neue Folge.  | mr=0094466 | year=1958 | volume=19}}\n*{{Citation | last1=Rainville | first1=Earl D. | title=Notes on Legendre polynomials | doi=10.1090/S0002-9904-1945-08330-X  |mr=0011750 | year=1945 | journal=[[Bulletin of the American Mathematical Society]] | issn=0002-9904 | volume=51 | pages=268–271}}\n[[Category:Polynomials]]"
    },
    {
      "title": "Real-root isolation",
      "url": "https://en.wikipedia.org/wiki/Real-root_isolation",
      "text": "{{short description|Methods for locating real roots of a polynomial}}\nIn [[mathematics]], and, more specifically in [[numerical analysis]] and [[computer algebra]], '''real-root isolation''' of a polynomial consist of producing disjoint [[interval (mathematics)|interval]]s of the [[real line]], which contain each one (and only one) real [[root of a function|root]] of the polynomial, and, together, contain all the real roots of the polynomial. \n\nReal-root isolation is useful because usual [[root-finding algorithm]]s for computing the real roots of a polynomial may produce some real roots, but, cannot generally certify having found all real roots. In particular, if such an algorithm does not find any root, one does not know whether it is because there is no real root. Some algorithms compute all complex roots, but, as there are generally much fewer real roots than complex roots, most of their computation time is generally spent for computing non-real roots (in the average, a polynomial of degree {{mvar|n}} has {{mvar|n}} complex roots, and only {{math|log ''n''}} roots{{cn|date=December 2018}}). Moreover, it may be difficult to distinguish the real roots from the non-real roots with small imaginary part (see the example of [[Wilkinson's polynomial]] in next section).\n\nThe first complete real-root isolation algorithm results from [[Sturm's theorem]] (1829). However, when real-root-isolation algorithms began to be implemented on [[computer]]s it appeared that algorithms derived from Sturm's theorem are less efficient than those derived from [[Descartes' rule of signs]] (1637). \n\nSince the beginning of 20th century there is an active research activity for improving the algorithms derived from Descartes' rule of signs, getting very efficient implementations, and computing their [[computational complexity]]. The best implementations can routinely isolate real roots of polynomials of degree more than 1,000.<ref name=TE>{{harvnb|Tsigaridas|Emiris|2006}}</ref><ref name=\"KRS\" />\n\n==Specifications and general strategy==\nFor finding real roots of a polynomial, the common strategy is to divide the [[real line]] (or an interval of it where root are searched) into disjoint intervals until having at most one root in each interval. Such a procedure is called '''root isolation''', and a resulting interval that contains exactly one root is an '''isolating interval''' for this root.\n\n[[Wilkinson's polynomial]] shows that a very small modification of one coefficient of a polynomial may change dramatically not only the value of the roots, but also their nature (real or complex). Also, even with a good approximation, when one evaluates a polynomial at an approximate root, one may get a result that is far to be close to zero. For example, if a polynomial of degree 20 (the degree of Wilkinson's polynomial) has a root close to 10, the derivative of the polynomial at the root may be of the order of <math>10^{20};</math> this implies that an error of <math>10^{-10}</math> on the value of the root may produce a value of the polynomial at the approximate root that is of the order of <math>10^{10}.</math> It follows that, except maybe for very low degrees, a root-isolation procedure cannot give reliable results without using exact arithmetic. Therefore if one wants to isolate roots of a polynomial with [[floating-point]] coefficients, it is often better to convert them to [[rational number]]s, and then take the [[primitive part]] of the resulting polynomial, for having a polynomial with integer coefficients.\n\nFor this reason, although the methods that are described below work theoretically with real numbers, they are generally used in practice with polynomials with integer coefficients, and intervals ending with rational numbers.  Also, the polynomials are always supposed to be [[square-free polynomial|square free]]. There are two reasons for that. Firstly [[Yun's algorithm]] for computing the [[square-free factorization]] is less costly than twice the cost of the computation of the [[polynomial greatest common divisor|greatest common divisor]] of the polynomial and its derivative. As this may produce factors of lower degrees, it is generally advantageous to apply root-isolation algorithms only on polynomials without multiple roots, even when this is not required by the algorithm. The second reason for considering only square-free polynomials is that the fastest root-isolation algorithms do not work in the case of multiple roots.\n\nFor root isolation, one requires a procedure for counting the real roots of a polynomial in an interval without having to compute them, or, at least a procedure for deciding whether an interval contains zero, one or more roots. With such a decision procedure, one may work with a working list of intervals that may contain real roots. At the beginning, the list contains a single interval containing all roots of interest, generally the whole real line or its positive part. Then each interval of the list is divided into two smaller intervals. If one of the new intervals does not contains any root, it is removed from the list. If it contains one root, it is put in an output list of isolating intervals. Otherwise, it is kept in the working list for further divisions, and the process may continue until all roots are eventually isolated\n\n==Sturm's theorem==\nThe first complete root-isolation procedure results of [[Sturm's theorem]] (1829), which expresses the number of real roots in an interval in terms of the number of [[sign variation]]s of the values of a sequence of polynomials, called ''Sturm's sequence'', at the ends of the interval. Sturm's sequence is the sequence of remainders that occur in a variant of [[Euclidean algorithm for polynomials|Euclidean algorithm]] applied to the polynomial and its derivatives. When implemented on computers, it appeared that root isolation with Sturm's theorem is less efficient than the other methods that are described below.<ref name=CA>{{harvnb|Collins|Akritas|1976}}</ref> Consequently, Sturm's theorem is rarely used for effective computations, although it remains useful for theoretical purpose.\n\n==Descartes' rule of signs and its generalizations==\n\n[[Descartes' rule of signs]] asserts that the difference between the number of [[sign variation]]s in the sequence of the coefficients of a polynomial and the number of its positive real roots is a nonnegative even integer. It results that if this number of sign variations is zero, then the polynomial does not have any positive real roots, and, if this number is one, then the polynomial has a unique positive real root, which is a single root. Unfortunately the converse is not true, that is, a polynomial which has either no positive real root or as a single positive simple root may have a number of sign variations greater than 1. \n\nThis has been generalized by [[Budan's theorem]] (1807), into a similar result for the real roots in a [[half-open interval]] {{math|(''a'', ''b'']}}: If {{math|''f''(''x'')}} is a polynomial, and {{mvar|v}} is the difference between of the numbers of sign variations of the sequences of the coefficients of {{math|''f''(''x'' + ''a'')}} and {{math|''f''(''x'' + ''b'')}}, then {{mvar|v}} minus the number of real roots in the interval, counted with their multiplicities, is a nonnegative even integer. This is a generalization of Descartes' rule of signs, because, for {{mvar|b}} sufficiently large, there is no sign variation in the coefficients of {{math|''f''(''x'' + ''b'')}}, and all real roots are smaller than {{mvar|b}}.\n\nBudan's may provide a real-root-isolation algorithm for a [[square-free polynomial]] (a polynomial without multiple root): from the coefficients of polynomial, one may compute an upper bound {{mvar|M}} of the absolute values of the roots and a lower bound {{mvar|m}} on the absolute values of the differences of two roots (see [[Properties of polynomial roots]]). Then, if one divides the interval {{math|[–''M'', ''M'']}} into intervals of length less than {{mvar|m}}, then every real root is contained in some interval, and no interval contains two roots. The isolating intervals are thus the intervals for which Budan's theorem asserts an odd number of roots. \n\nHowever, this algorithm is very inefficient, as one cannot use a coarser partition of the interval {{math|[–''M'', ''M'']}}, because, if Budan's theorem gives a result larger than 1 for an interval of larger size, there is no way for insuring that it does not contain several roots.\n\n==Vincent's and related theorems==\n'''{{vanchor|Vincent's theorem}}''' (1834)<ref name=Vincent-memoire/> provides a method for real-root isolation, which is at the basis of the most efficient real-root-isolation algorithms. It concerns the positive real roots of a [[square-free polynomial]] (that is a polynomial without multiple roots). If <math>a_1, a_2,\\ldots,</math> is a sequence of positive real numbers, let \n:<math>c_k=a_1 + \\cfrac{1}{a_2 + \\cfrac{1}{a_3 + \\cfrac{1}{ \\ddots + \\cfrac{1}{a_k} }}}</math>\nbe the {{mvar|k}}th [[convergent (continued fraction)|convergent]] of the [[continued fraction]]\n:<math>a_1 + \\cfrac{1}{a_2 + \\cfrac{1}{a_3 + \\cfrac{1}{ \\ddots}}}.</math>\n\n{{math theorem|Let <math>p_0(x)</math> be a square-free polynomial of degree {{mvar|n}}, and <math>a_1, a_2,\\ldots,</math> be a sequence of real numbers. For {{math|1=''i'' = 1, 2,...,}} consider the polynomial\n:<math>p_i(x)=x^np_{i-1}\\left(a_i+1/x\\right).</math>\nThen, there is an integer {{mvar|k}} such that either <math>p_k(0)=0,</math> or the sequence of the coefficients of <math>p_k</math> has at most one sign variation.\n\nIn the first case, the convergent {{math|''c<sub>k</sub>''}} is a positive root of <math>p_0.</math> Otherwise, this number of sign variations (either 0 or 1) is the number of real roots of <math>p_0</math> in the interval defined by <math>c_{k-1}</math> and <math>c_k.</math>|name=Vincent's theorem}}\n\nFor proving his theorem, Vincent proved a result that is useful on its own:<ref name=Vincent-memoire/>\n{{math theorem|If {{math|''p''(''x'')}} is a square-free polynomial of degree {{mvar|n}}, and {{math|''a'', ''b'', ''c'', ''d''}} are nonnegative real numbers such that <math>\\left|\\frac ac-\\frac bd\\right|</math> is small enough (but not 0), then there is at most one sign variation in the coefficients of the polynomial\n:<math>q(x)=(cx+d)^n p\\left(\\frac{ax+b}{cx+d}\\right),</math>\nand this number of sign variations is the number of real roots of {{math|''p''(''x'')}} in the open interval defined by <math>\\frac ac</math> and <math>\\frac bd.</math>\n|name=Vincent's auxiliary theorem}}\n\nFor working with real numbers, one may always choose {{math|1=''c'' = ''d'' = 1}}, but, as effective computations are done with [[rational number]]s, it is generally convenient to suppose that {{math|''a'', ''b'', ''c'', ''d''}} are integers.\n\nThe \"small enough\" condition has been quantified independently by [[Nikola Obreshkov]],<ref name=Obr_1920>{{harvnb|Obreschkoff|1963}}</ref> and [[Alexander Ostrowski]]:<ref name=O_1950>{{harvnb|Ostrowski|1950}}</ref>\n[[File:Sketch of proof.jpg|thumb|x220px|right|Obreschkoff–Ostrowski theorem: in blue and yellow, the regions of the complex plane where there should be no root for having 0  or 1 sign variation; on the left the regions excluded for the roots of {{mvar|p}}, on the right, the regions excluded for the roots of the transformed polynomial {{mvar|q}}; in blue, the regions that are excluded for having one sign variation, but allowed for zero sign variations.]]\n\n{{math theorem|The conclusion of Vincent's auxiliary result holds if the polynomial {{math|''p''(''x'')}} has at most one root {{math|''α'' + ''iβ''}} such that\n:<math>\\left(\\alpha-\\frac ac\\right)\\left(\\alpha -\\frac bd\\right)  +\\beta^2\\le \\frac 1\\sqrt 3\\left|\\beta\\left(\\frac ac-\\frac bd\\right)\\right|.</math> \nIn particular the conclusion holds if\n:<math>\\left|\\frac ac -\\frac bd\\right| < \\frac {\\operatorname{sep}(p)}{2\\sqrt 3},</math>\nwhere {{math|sep(''p'')}} is the minimal distance between two roots of {{mvar|p}}.\n|note=Obreschkoff–Ostrowski}}\n\nFor polynomials with integer coefficients, the minimum distance {{math|sep(''p'')}} may be lower bounded in terms of the degree of the polynomial and the maximal absolute value of its coefficients; see {{slink|Properties of polynomial roots|Root separation}}. This allows the analysis of [[worst-case complexity]] of algorithms based on Vincent's theorems. However, Obreschkoff–Ostrowski theorem shows that the number of iterations of these algorithms depend on the distances between roots in the neighborhood of the working interval; therefore, the number of iterations may vary dramatically for different roots of the same polynomial.\n\n[[James V. Uspensky]] gave a bound on the length of the continued fraction (the integer {{mvar|k}} needed, in Vincent's theorem, for getting zero or one sign variations:<ref name=TE /><ref name=Uspensky>{{harvnb|Uspensky|1948}}</ref> \n{{math theorem|\nLet {{math|''p''(''x'')}} be a polynomial of degree {{mvar|n}}, and {{math|sep(''p'')}} be the minimal distance between two roots of {{math|p}}. Let\n:<math>\\varepsilon = \\left(n+\\frac 1n\\right)^\\frac 1{n-1}-1.</math>\nThen the integer {{mvar|k}}, whose existence is asserted in Vincent's theorem, is not greater than the smallest integer {{mvar|h}} such that \n:<math>F_{h-1}\\operatorname{sep}(p)>2\\quad\\text{and}\\quad F_{h-1}F_k\\operatorname{sep}(p)> \\frac 1\\varepsilon,</math>\nwhere <math>F_h</math> is the {{mvar|h}}th [[Fibonacci number]].\n|note=Uspensky}}\n\n==Continued fraction method==\n\nThe use of [[continued fraction]]s for real-root isolation has been introduced by Vincent, although he credited [[Joseph-Louis Lagrange]] for this idea, without providing a reference.<ref name=Vincent-memoire>{{harvnb|Vincent|1834}}</ref> For making an [[algorithm]] of Vincent's theorem, one must provide a criterion for choosing the <math>a_i</math> that occur in his theorem. Vincent himself provided some choice (see below). Some other choices are possible, and the efficiency of the algorithm may depends dramatically of these choices. Therefore, an algorithm is presented here, in which these choices result from an auxiliary function that will be discussed later.\n\nFor running this algorithm one must work with a list of intervals represented by a specific data structure. The algorithm works by choosing an interval, removing it from the list, adding zero, one or two smaller intervals to the list, and possibly outputs an isolation interval. \n\nFor isolating the real roots of a polynomial {{math|''p''(''x'')}} of degree {{mvar|n}}, each interval is represented  by a pair <math>(A(x), M(x)), </math> where {{math|''A''(''x'')}} is a polynomial of degree {{mvar|n}} and <math>M(x)=\\frac{px+r}{qx+s}</math> is a [[Möbius transformation]] with integer coefficients. One has \n:<math>A(x)=p(M(x)),</math>\nand the interval represented by this data structure is the interval that has <math>M(\\infty)=\\frac pq</math> and <math>M(0)=\\frac rs</math> as end points. The Möbius transformation maps the roots of {{mvar|p}} in this interval to the roots of {{mvar|A}} in {{math|(0, +∞)}}.\n\nThe algorithm works with a list of intervals that, at the beginning, contains the two intervals <math>(A(x)=p(x), M(x)=x)</math> and <math>(A(x)=p(-x), M(x)=-x),</math> corresponding to the partition of the reals into the positive and the negative ones (one may suppose that zero is not a root, as, if it were, it suffices to apply the algorithm to {{math|''p''(''x'')/''x''}}). Then for each interval {{math|(''A''(''x''), ''M''(''x''))}} in the list, the algorithm remove it from the list; if the number of sign variations of the coefficients of {{mvar|A}} is zero, there is no root in the interval, and one passes to the next interval. If the number of sign variations is one, the interval defined by <math>M(0)</math> and <math>M(\\infty)</math> is an isolating interval. Otherwise, one chooses a positive real number {{mvar|b}} for dividing the interval {{math|(0, +∞)}} into {{math|(0, b)}} and {{math|(b, +∞)}}, and, for each subinterval, one composes {{mvar|M}} with a Möbius transformation that maps the interval onto {{math|(0, +∞)}}, for getting two new intervals to be added to the list. In pseudocode, this gives the following, where {{math|var(''A'')}} denotes the number of sign variations of the coefficients of the polynomial {{mvar|A}}.\n\n '''function''' continued fraction '''is'''\n    '''input''': P(x), a [[square-free polynomial]],\n    '''output''': a list of pairs of rational numbers defining isolating intervals\n    /* ''Initialization'' */\n    L := [(P(x), x), (P(–x), –x)]                /* ''two starting intervals'' */\n    Isol := [ ]\n     /* Computation */\n    '''while''' L {{math|≠}} [ ] '''do'''\n       '''Choose''' (A(x), M(x)) '''in''' L, '''and remove it from''' L\n       v := var(''A'')\n       '''if''' v = 0 '''then exit'''                        /* no root in the interval */\n       '''if''' v = 1 '''then'''                             /* isolating interval found */\n          '''add''' (M(0), M(∞)) '''to''' Isol\n          '''exit'''\n       b := some positive integer\n       B(x) = A(x + b)\n       w := v – var(B)\n       '''if''' B(0) = 0 then                          /* rational root found */\n          '''add''' (M(b), M(b)) '''to''' Isol\n          B(x) := B(x)/x\n       '''add''' (B(x),  M(b + x) '''to''' L                 /* roots in (b, +∞) */\n       '''if''' w = 0 '''then exit'''                        /* [[Budan's theorem]] */ \n       '''if''' w = 1 '''then'''                             /* Budan's theorem again */ \n          '''add''' (M(0), M(b)) '''to''' Isol\n       '''if''' w > 1 '''then'''\n          '''add''' A(b/(1 + x)),  M(b/(1 + x)) '''to''' L      /* roots in (0, b) */\n\nThe different variants of the algorithm depend essentially on the choice of {{mvar|b}}. In Vincent's papers, and in Uspensky's book, one has always {{math|1=''b'' = 1}}, with the difference that Uspensky did not use Budan's theorem for avoiding further bisections of the interval associated to {{math|(0, b)}}\n\nThe drawback of always choosing {{math|1=''b'' = 1}} is that one has to do many successive changes of variable of the form {{math|''x'' → 1 + ''x''}}. These may be replaced by a single change of variable {{math|''x'' → ''n'' + ''x''}}, but, nevertheless, one has to do the intermediate changes of variables for applying Budan's theorem.\n\nA way for improving the efficiency of the algorithm is to take for {{mvar|b}} a lower bound of the positive real roots, computed from the coefficients of the polynomial (see [[Properties of polynomial roots]] for such bounds).<ref>{{harvnb|Akritas|Strzeboński|2005}}</ref><ref name=\"TE\" />\n\n==Bisection method==\nThe bisection method consists roughly of starting from an interval containing all real roots of a polynomial, and divides it recursively into two parts until getting eventually intervals that contain either zero of one root. The starting interval may be of the form {{math|(-''B'', ''B'')}}, where {{mvar|B}} is an upper bound on the absolute values of the roots, such as those that are given in {{slink|Properties of polynomial roots|Bounds on (complex) polynomial roots}}. For technical reasons (simpler changes of variable, simpler [[complexity analysis]], possibility of taking advantage of the binary analysis of computers), the algorithms are generally presented as starting with the interval {{math|[0, 1]}}. There is no loss of generality, as the changes of variables {{math|1=''x'' = ''By''}} and {{math|1=''x'' = –''By''}} move respectively the positive and the negative roots in the interval {{math|[0, 1]}}. (The single changes variable {{math|1=''x'' =  (2''By'' – ''B'')}} may also be used.)\n\nThe method requires an algorithm for testing whether an interval has zero, one, or possibly several roots, and for warranting termination, this testing algorithm must excludes the possibility of getting infinitely many times the output \"possibility of several roots\". [[Sturm's theorem]] and Vincent's auxiliary theorem provide such convenient tests. As the use [[Descartes' rule of signs]] and Vincent's auxiliary theorem is much more computationally efficient than the use of Sturm's theorem, only the former is described in this section.\n\nThe bisection method based on Descartes' rules of signs and Vincent's auxiliary theorem has been introduced in 1976 by Akritas and [[George E. Collins|Collins]] under the name of ''Modified Uspensky algorithm'',<ref name=CA/> and has been referred to as ''Uspensky algorithm'', ''Vincent–Akritas–Collins algorithm'', or ''Descartes method'', although Descartes, Vincent and Uspensky have never described it.\n\nThe method works as follows. For searching the roots in some interval, one changes first the variable for mapping the interval onto {{math|[0, 1]}} giving a new polynomial {{math|''q''(''x'')}}. For searching the roots of {{mvar|q}} in {{math|[0, 1]}}, one maps the interval {{math|[0, 1]}} onto {{math|[0, +∞])}} by the change of variable <math>x\\to\\frac{1}{x+1},</math> giving a polynomial {{math|''r''(''x'')}}. Descartes' rule of signs applied to the polynomial {{mvar|r}} gives indications on the number of real roots of {{mvar|q}} in the interval {{math|[0, 1]}}, and thus on the number of roots of the initial polynomial in the interval that has been mapped on {{math|[0, 1]}}. If there is no sign variation in the sequence of the coefficients of {{mvar|r}}, then there is no real root in the considered intervals. If there is one sign variation, then one has an isolation interval. Otherwise, one splits the interval {{math|[0, 1]}} into {{math|[0, 1/2]}} and {{math|[1/2, 1]}}, one maps them onto {{math|[0, 1]}} by the changes of variable {{math|1=''x'' = ''y''/2}} and {{math|1=''x'' = (''y'' + 1)/2}}. Vincent's auxiliary theorem insures the termination of this procedure.\n\nExcept for the initialization, all these changes of variable consists of the composition of at most two very simple changes of variable which are the scalings by two {{math|''x'' → ''x''/2 }}, the [[translation (mathematics)|translation]] {{math|''x'' → ''x'' + 1}}, and the inversion {{math|''x'' → 1/''x'' }}, the latter consisting simply of reverting the order of the coefficients of the polynomial. As most of the computing time is devoted to changes of variable, the method consisting of mapping eery interval to {{math|[0, 1]}} is fundamental for insuring a good efficiency.\n\n===Pseudocode===\n\nThe following notation is used in the pseudocode that follows.\n*{{math|''p''(''x'')}} is the polynomial for which the real roots in the interval {{math|[0, 1]}} have to be isolated\n*{{math|var(''q''(''x''))}} denotes the number of [[sign variation]]s in the sequence of the coefficients of the polynomial {{mvar|q}}\n*The elements of working list have the form {{math|(''c'', ''k'', ''q''(''x''))}}, where \n**{{mvar|c}} and {{mvar|k}} are two nonnegative integers such that {{math|''c'' < 2<sup>''k''</sup>}}, which represent the interval <math>\\left[\\frac c{2^k}, \\frac{c+1}{2^k}\\right],</math>\n**<math>q(x)=2^{kn} p\\left(\\frac{x+c}{2^k}\\right),</math> where {{mvar|n}} is the degree of {{mvar|p}} (the polynomial {{mvar|q}} may be computed directly from {{mvar|p}}, {{mvar|c}} and {{mvar|k}}, but it is less costly to compute it incrementally, as it will be done in the algorithm; if {{mvar|p}} has integer coefficients, the same is true for {{mvar|q}})\n\n '''function''' bisection '''is'''\n    '''input''': {{math|''p''(''x'')}}, a [[square-free polynomial]], such that {{math|''p''(0) ''p''(1) ≠ 0}}, \n                      for which the roots in the interval {{math|[0, 1]}} are searched\n    '''output''': a list of triples {{math|(''c'', ''k'', ''h'')}}, \n                      representing isolating intervals of the form <math>\\left[\\frac c{2^k}, \\frac{c+h}{2^k}\\right]</math>\n    /* ''Initialization'' */\n    L := [(0, 0, ''p''(''x''))] /* ''a single element in the working list'' L */\n    Isol := [ ]\n    n := degree(''p''}}\n    /* Computation */\n    '''while''' L {{math|≠}} [ ] '''do'''\n       '''Choose''' (c, k, {{math|''q''(''x''))}} '''in''' L, '''and remove it from''' L\n       '''if''' {{math|1=''q''(0) = 0}} '''then'''\n          {{math|1=''q''(''x'') := ''q''(''x'')/''x''}}\n          n := n – 1                /* A rational root found */\n          '''add''' (c, k, 0) '''to''' Isol\n       v := <math>\\operatorname{var}((x+1)^n q(1/(x+1)))</math>\n       '''if''' v = 1 '''then'''                /* An isolating interval found */\n          '''add''' (c, k, 1) '''to''' Isol\n       '''if''' v > 1 '''then'''                /* Bisecting */\n          '''add''' (2c, k + 1, <math>2^n q(x/2)</math>  '''to''' L\n          '''add''' (2c + 1, k + 1, <math>2^n q((x + 1)/2)</math>  '''to''' L\n    '''end'''.\n\nThis procedure is essentially the one that has been described by Collins and Akritas.<ref name=CA/> The running time depends mainly on the number of intervals that have to be considered, and on the changes of variables. There are ways for improving the efficiency, which have been an active subject of research since the publication of the algorithm, and mainly since the beginning of 20th century. \n\n===Recent improvements===\nVarious ways for improving Akritas–Collins bisection algorithm have been proposed. They include a method for avoiding storing a long list of polynomials without losing the simplicity of the changes of variables,<ref name=\"RZ\">{{harvnb|Rouillier|Zimmerman|2004}}</ref> the use of approximate arithmetic ([[floating point]] and [[interval arithmetic]]) when it allows getting the right value for the number of sign variations,<ref name=\"RZ\" /> the use of [[Newton's method]] when possible,<ref name=\"RZ\" /> the use of fast polynomial arithmetic,<ref name= SM>{{harvnb|Sagraloff|Mehlhorn|2016}}</ref> shortcuts for long chains of bisections in case of clusters of close roots,<ref name=SM /> bisections in unequal parts for limiting instability problems in polynomial evolution.<ref name=SM />\n\nAll these improvement lead to an algorithm for isolating all real roots of a polynomial with integer coefficients, which has the [[computational complexity|complexity]] (using [[soft O notation]] <math>\\tilde O</math> for omitting logarithmic factors)\n:<math>\\tilde O (n^2(k+t)),</math>\nwhere {{mvar|n}} is the degree of the polynomial, {{mvar|k}} is the number of nonzero terms, {{mvar|t}} is the maximum of [[numerical digit|digits]] of the coefficients.<ref name=\"SM\" />\n\nThe implementation of this algorithm appears to be more efficient than any other implemented method for computing the real roots of a polynomial, even in the case of polynomials having very close roots (the case which was previously the most difficult for the bisection method).<ref name=KRS>{{harvnb|Kobel|Rouillier|Sagraloff|2016}}</ref>\n\n==References==\n{{reflist|34em}}\n\n==Sources==\n{{refbegin|34em}}\n*{{cite journal|last=Alesina|first=Alberto|author2=Massimo Galuzzi|title=A new proof of Vincent's theorem|url=http://retro.seals.ch/cntmng?type=pdf&rid=ensmat-001:1998:44::149&subp=hires|journal=L'Enseignement Mathématique|year=1998|volume=44|number=3–4|pages=219–256|access-date=2018-12-16|archive-url=https://web.archive.org/web/20140714222659/http://retro.seals.ch/cntmng?type=pdf&rid=ensmat-001:1998:44::149&subp=hires|archive-date=2014-07-14|dead-url=yes|df=}}\n*{{cite conference|last=Akritas |first=Alkiviadis G. |title=There's no \"Uspensky's Method\" |url=http://dl.acm.org/citation.cfm?id=32457 |year=1986 |conference=Proceedings of the fifth ACM Symposium on Symbolic and Algebraic Computation (SYMSAC '86) |location=Waterloo, Ontario, Canada |pages=88–90}}\n*{{cite journal |last=Akritas |first=Alkiviadis G. |first2=A. W. |last2=Strzeboński |first3=P. S. |last3=Vigklas |title=Improving the performance of the continued fractions method using new bounds of positive roots |journal=Nonlinear Analysis: Modelling and Control |year=2008 |volume=13 |pages=265–279 |url=http://www.lana.lt/journal/30/Akritas.pdf}}\n*{{cite journal |last=Akritas |first=Alkiviadis G. |first2=Adam W. |last2=Strzeboński |title=A Comparative Study of Two Real Root Isolation Methods |journal=Nonlinear Analysis: Modelling and Control |year=2005 |volume=10 |number=4 |pages=297–304 |url=http://www.lana.lt/journal/19/Akritas.pdf |ref=harv}}\n*{{cite conference |last=Collins |first=George E. |authorlink=George E. Collins |first2=Alkiviadis G. |last2=Akritas |title=Polynomial Real Root Isolation Using Descartes' Rule of Signs |year=1976 |pages=272–275 |conference=SYMSAC '76, Proceedings of the third ACM symposium on Symbolic and algebraic computation |publisher=ACM |location=Yorktown Heights, NY, USA |ref=harv|doi=10.1145/800205.806346 }}\n*{{cite proceedings |first1=Alexander |last1=Kobel |first2=Fabrice |last2=Rouillier |first3=Michael |last3=Sagraloff |title=Computing real roots of real polynomials ... and now for real! |book-title=ISSAC '16, Proceedings of the ACM on International Symposium on Symbolic and Algebraic Computation |date=2016 |location=Waterloo, Canada |doi=10.1145/2930889.2930937 |arxiv=1605.00410 |ref=harv}}\n*{{cite book |last=Obreschkoff |first=Nikola |authorlink=Nikola Obreshkov |title=Verteilung und Berechnung der Nullstellen reeller Polynome |language=de |publisher=[[VEB Deutscher Verlag der Wissenschaften]] |date=1963 |location=Berlin |page=81 |ref=harv}}\n*{{cite journal |last=Ostrowski |first=A. M. |authorlink=Alexander Ostrowski |title=Note on Vincent's theorem |jstor=1969443 |journal=Annals of Mathematics |series=Second Series |date=1950 |volume=52 |number=3 |pages=702–707 |doi=10.2307/1969443 |ref=harv}}\n*{{cite journal |first1=F. |last1=Rouillier |first2=P. |last2=Zimmerman |title=Efficient isolation of polynomial's real roots |journal=Journal of Computational and Applied Mathematics |volume=162 |issue= |pages=33–50 |date=2004 |doi=10.1016/j.cam.2003.08.015 |ref=harv}}\n*{{cite journal |first1= M. |last1= Sagraloff |first2= K. |last2=Mehlhorn |title=Computing real roots of real polynomials |journal=Journal of Symbolic Computation |volume=73 |pages=46–86 |date=2016 |arxiv=1308.4088|doi=10.1016/j.jsc.2015.03.004 |ref=harv}}\n*{{cite journal |last=Tsigaridas |first=P. E. |first2=I. Z. |last2=Emiris |title=Univariate polynomial real root isolation: Continued fractions revisited |journal=LNCS |year=2006 |volume=4168 |pages=817–828 |doi=10.1007/11841036_72 |arxiv=cs/0604066 |ref=harv|series=Lecture Notes in Computer Science |isbn=978-3-540-38875-3 |bibcode=2006cs........4066T }}\n*{{cite book |last=Uspensky |first=James Victor |authorlink=J. V. Uspensky |title=Theory of Equations |date=1948 |publisher=McGraw–Hill Book Company |location=New York |url=https://www.google.com/search?q=uspensky+theory+of+equations&btnG=Search+Books&tbo=1 |ref=harv}}\n*{{cite journal |last=Vincent |first=Alexandre Joseph Hidulphe |year=1834 |title=Mémoire sur la résolution des équations numériques |language=fr |url=http://gallica.bnf.fr/ark:/12148/bpt6k57787134/f4.image.r=Agence%20Rol.langEN |journal=Mémoires de la Société Royale des Sciences, de L' Agriculture et des Arts, de Lille |pages=1–34 |ref=harv}}\n*{{cite journal|last=Vincent|first=Alexandre Joseph Hidulphe|year=1836|title=Note sur la résolution des équations numériques|url=http://sites.mathdoc.fr/JMPA/PDF/JMPA_1836_1_1_A28_0.pdf|journal=Journal de Mathématiques Pures et Appliquées|volume=1|pages=341–372|via=}}\n*{{cite journal|last=Vincent|first=Alexandre Joseph Hidulphe|year=1838|title=Addition à une précédente note relative à la résolution des équations numériques|url=http://math-doc.ujf-grenoble.fr/JMPA/PDF/JMPA_1838_1_3_A19_0.pdf|journal=Journal de Mathématiques Pures et Appliquées|volume=3|pages=235–243|access-date=2018-12-16|archive-url=https://web.archive.org/web/20131029193332/http://math-doc.ujf-grenoble.fr/JMPA/PDF/JMPA_1838_1_3_A19_0.pdf|archive-date=2013-10-29|dead-url=yes|df=}}\n{{refend}}\n\n[[Category:Polynomials]]\n[[Category:Root-finding algorithms]]\n[[Category:Real algebraic geometry]]\n[[Category:Computer algebra]]"
    },
    {
      "title": "Reciprocal polynomial",
      "url": "https://en.wikipedia.org/wiki/Reciprocal_polynomial",
      "text": "In [[algebra]], the '''reciprocal polynomial''', or '''reflected polynomial'''<ref name=\"concrete\">*{{cite book | last = Graham | first = Ronald |last2=Knuth|first2=Donald E.|last3=Patashnik|first3=Oren | title = [[Concrete mathematics]] : a foundation for computer science | publisher = Addison-Wesley | location = Reading, Mass | year = 1994 | isbn = 978-0201558029 | page= 339}}</ref><ref name=\"Aigner\">{{cite book | last = Aigner | first = Martin | title = A course in enumeration | publisher = Springer | location = Berlin New York | year = 2007 | isbn = 978-3540390329 | page = 94 }}</ref> {{math|''p''<sup>∗</sup>}} or {{math|''p''<sup>R</sup>}},<ref name=\"Aigner\"/><ref name=\"concrete\"/> of a [[polynomial]] {{math|''p''}} of degree {{math|''n''}} with coefficients from an arbitrary [[Field (mathematics)|field]], such as\n:<math>p(x) = a_0 + a_1x + a_2x^2 + \\cdots + a_nx^n, \\,\\!</math>\nis the polynomial<ref>{{harvnb|Roman|1995|loc=pg.37}}</ref>\n\n: <math>p^*(x) = a_n + a_{n-1}x + \\cdots + a_0x^n = x^n p(x^{-1}).</math>\n\nEssentially, the coefficients are written in reverse order. They arise naturally in [[linear algebra]] as the [[characteristic polynomial]] of the [[inverse of a matrix]].\n\nIn the special case that the [[polynomial]] {{math|''p''}} has [[complex number|complex]] coefficients, that is,\n\n:<math>p(z) = a_0 + a_1z + a_2z^2 + \\cdots + a_nz^n, \\,\\!</math>\n\nthe '''conjugate reciprocal polynomial''', {{math|''p''<sup>†</sup>}} given by,\n\n:<math>p^{\\dagger}(z) = \\overline{a_n} + \\overline{a_{n-1}}z + \\cdots + \\overline{a_0}z^n = z^n\\overline{p(\\bar{z}^{-1})},</math>\n\nwhere <math>\\overline{a_i}</math> denotes the [[complex conjugate]] of <math>a_i \\,\\!</math>, is also called the reciprocal polynomial when no confusion can arise.\n\nA polynomial {{math|''p''}} is called '''self-reciprocal''' or '''palindromic''' if {{math|1=''p''(''x'') = ''p''<sup>∗</sup>(''x'')}}.\nThe coefficients of a self-reciprocal polynomial satisfy {{math|1=''a''<sub>''i''</sub> = ''a''<sub>''n''&minus;''i''</sub>}}. In the conjugate reciprocal case, the coefficients must be [[Real number|real]] to satisfy the condition.\n\n== Properties ==\nReciprocal polynomials have several connections with their original polynomials, including:\n# {{math|1=''p''(''x'') = ''x''<sup>''n''</sup>''p''<sup>∗</sup>(''x''<sup>−1</sup>)}}<ref name=\"Aigner\"/>\n# {{math|''α''}} is a root of polynomial {{math|''p''}} if and only if {{math|α<sup>−1</sup>}} is a root of {{math|''p''<sup>∗</sup>}}.<ref name=\"Pless 1990 loc=pg. 57\">{{harvnb|Pless|1990|loc=pg. 57}}</ref>\n# If {{math|''p''(''x'') ≠ ''x''}} then {{math|''p''}} is [[Irreducible polynomial|irreducible]] if and only if {{math|''p''<sup>∗</sup>}} is irreducible.<ref name=\"Roman 1995 loc= pg. 37\">{{harvnb|Roman|1995|loc= pg. 37}}</ref>\n# {{math|''p''}} is [[Primitive polynomial (field theory)|primitive]] if and only if {{math|''p''<sup>∗</sup>}} is primitive.<ref name=\"Pless 1990 loc=pg. 57\"/>\n\nOther properties of reciprocal polynomials may be obtained, for instance:\n* If a polynomial is self-reciprocal and irreducible then it must have even degree.<ref name=\"Roman 1995 loc= pg. 37\"/>\n\n=={{anchor|Palindromic polynomial}} Palindromic and antipalindromic polynomials==\nA self-reciprocal polynomial is also called palindromic because its coefficients, when the polynomial is written in the order of ascending or descending powers, form a [[palindrome]]. That is, if\n:<math> P(x) = \\sum_{i=0}^n a_ix^i</math> \nis a polynomial of [[Degree of a polynomial|degree]] {{math|''n''}}, then {{math|''P''}} is ''palindromic'' if {{math|1=''a<sub>i</sub>'' = ''a''<sub>''n'' − ''i''</sub>}} for {{math|1=''i'' = 0, 1, ..., ''n''}}. Some authors use the terms ''palindromic'' and ''reciprocal'' interchangeably.\n\nSimilarly, {{math|''P''}}, a polynomial of degree {{math|''n''}}, is called '''antipalindromic''' if {{math|1=''a<sub>i</sub>'' = −''a''<sub>''n'' − ''i''</sub>}} for {{math|1=''i'' = 0, 1, ... ''n''}}. That is, a polynomial {{math|''P''}} is ''antipalindromic'' if {{math|1=''P''(''x'') = – ''P''<sup>∗</sup>(''x'')}}.\n\n===Examples===\nFrom the properties of the [[binomial coefficient]]s, it follows that the polynomials {{math|1=''P''(''x'') = (''x'' + 1 )<sup>''n''</sup>}} are palindromic for all positive integers {{math|''n''}}, while the polynomials {{math|1=''Q''(''x'') = (''x'' – 1 )<sup>''n''</sup>}} are palindromic when {{math|''n''}} is even and antipalindromic when {{math|''n''}} is odd.\n\nOther examples of palindromic polynomials include [[cyclotomic polynomial]]s and [[Eulerian polynomial]]s.\n\n===Properties===\n* If {{math|''a''}} is a root of a polynomial that is either palindromic or antipalindromic, then {{sfrac|{{math|''a''}}}} is also a root and has the same [[multiplicity (mathematics)|multiplicity]].<ref>{{harvnb|Pless|1990|loc=pg. 57}} for the palindromic case only</ref>\n* The converse is true: If a polynomial is such that if {{math|''a''}} is a root then {{sfrac|{{math|''a''}}}} is also a root of the same multiplicity, then the polynomial is either palindromic or antipalindromic.\n* For any polynomial {{math|''q''}}, the polynomial {{math|''q'' + ''q''<sup>∗</sup>}} is palindromic and the polynomial {{math|''q'' − ''q''<sup>∗</sup>}} is antipalindromic.\n* Any polynomial {{math|''q''}} can be written as the sum of a palindromic and an antipalindromic polynomial.<ref>{{citation|first=Jonathan Y.|last=Stein|title=Digital Signal Processing: A Computer Science Perspective|publisher=Wiley Interscience|year=2000|page=384|isbn=9780471295464}}</ref>\n* The product of two palindromic or antipalindromic polynomials is palindromic.\n* The product of a palindromic polynomial and an antipalindromic polynomial is antipalindromic.\n* A palindromic polynomial of odd degree is a multiple of {{math|''x'' + 1}} (it has –1 as a root) and its quotient by {{math|''x'' + 1}} is also palindromic.\n* An antipalindromic polynomial is a multiple of {{math|''x'' – 1}} (it has 1 as a root) and its quotient by {{math|''x'' – 1}} is palindromic.\n* An antipalindromic polynomial of even degree is a multiple of {{math|''x''<sup>2</sup> – 1}} (it has -1 and 1 as a roots) and its quotient by {{math|''x''<sup>2</sup> – 1}} is palindromic. \n* If {{math|''p''(''x'')}} is a palindromic polynomial of even degree {{mvar|2d}}, then there is a polynomial {{math|''q''}} of degree {{math|''d''}} such that {{math|1=''p''(''x'') = ''x''<sup>''d''</sup>''q''(''x'' + {{sfrac|1|''x''}})}} (Durand 1961).\n* If {{math|''p''(''x'')}} is a monic antipalindromic polynomial of even degree {{mvar|2d}} over a field {{mvar|k}} with odd [[Characteristic (field)|characteristic]], then it can be written uniquely as {{math|1=''p''(''x'') = ''x''<sup>''d''</sup> (''Q''(''x'') − ''Q''({{sfrac|''x''}}))}}, where {{mvar|Q}} is a monic polynomial of degree {{mvar|d}} with no constant term.<ref>{{citation|first=Nicholas M.|last=Katz|title=Convolution and Equidistribution : Sato-Tate Theorems for Finite Field Mellin Transformations|publisher=Princeton University Press|year=2012|isbn=9780691153315|page=146}}</ref>\n* If an antipalindromic polynomial {{math|''P''}} has even degree {{math|2''n''}}, then its \"middle\" coefficient (of power {{math|''n''}}) is 0 since {{math|1=''a<sub>n</sub>'' = −''a<sub>2n – n</sub>''}}.\n\n===Real coefficients===\nA polynomial with [[Real number|real]] coefficients all of whose [[Complex number|complex]] roots lie on the unit circle in the [[complex plane]] (all the roots are unimodular) is either palindromic or antipalindromic.<ref>{{citation|first1=Ivan|last1=Markovsky|first2=Shodhan|last2=Rao|title=Palindromic polynomials, time-reversible systems and conserved quantities|journal=Control and Automation|year=2008|doi=10.1109/MED.2008.4602018}}</ref>\n\n==Conjugate reciprocal polynomials==\n\nA polynomial is '''conjugate reciprocal''' if <math>p(x) \\equiv p^{\\dagger}(x)</math> and '''self-inversive''' if <math>p(x) = \\omega p^{\\dagger}(x)</math> for a scale factor {{math|''ω''}} on the [[unit circle]].<ref name=SV08>{{cite book | last1=Sinclair | first1=Christopher D. | last2=Vaaler | first2=Jeffrey D. | chapter=Self-inversive polynomials with all zeros on the unit circle | zbl=1334.11017 | editor1-last=McKee | editor1-first=James | editor2-last=Smyth | editor2-first=C. J. | title=Number theory and polynomials. Proceedings of the workshop, Bristol, UK, April 3–7, 2006 | location=Cambridge | publisher=[[Cambridge University Press]] | isbn=978-0-521-71467-9 | series=London Mathematical Society Lecture Note Series | volume=352 | pages=312–321 | year=2008 }}</ref>\n\nIf {{math|''p''(''z'')}} is the [[Minimal polynomial (field theory)|minimal polynomial]] of {{math|''z''<sub>0</sub>}} with {{math|1={{abs|''z''<sub>0</sub>}} = 1, ''z''<sub>0</sub> ≠ 1}}, and {{math|''p''(''z'')}} has [[real number|real]] coefficients, then {{math|''p''(''z'')}} is self-reciprocal.  This follows because\n\n:<math>z_0^n\\overline{p(1/\\bar{z_0})} = z_0^n\\overline{p(z_0)} = z_0^n\\bar{0} = 0.</math>\n\nSo {{math|''z''<sub>0</sub>}} is a root of the polynomial <math>z^n\\overline{p(\\bar{z}^{-1})}</math> which has degree {{math|''n''}}.  But, the minimal polynomial is unique, hence \n:<math>cp(z) = z^n\\overline{p(\\bar{z}^{-1})}</math>\nfor some constant {{math|''c''}}, i.e. <math>ca_i=\\overline{a_{n-i}}=a_{n-i}</math>. Sum from {{math|1=''i'' = 0}} to {{math|''n''}} and note that 1 is not a root of {{math|''p''}}. We conclude that {{math|1=''c'' = 1}}.\n\nA consequence is that the [[cyclotomic polynomial]]s {{math|Φ<sub>''n''</sub>}} are self-reciprocal for {{math|''n'' > 1}}. This is used in the [[special number field sieve]] to allow numbers of the form {{math|''x''<sup>11</sup> ± 1, ''x''<sup>13</sup> ± 1, ''x''<sup>15</sup> ± 1}} and {{math|''x''<sup>21</sup> ± 1}} to be factored taking advantage of the algebraic factors by using polynomials of degree 5, 6, 4 and 6 respectively – note that {{math|''φ''}} ([[Euler's totient function]]) of the exponents are 10, 12, 8 and 12.\n\n==Application in coding theory==\n\nThe reciprocal polynomial finds a use in the theory of [[Cyclic code|cyclic error correcting codes]]. Suppose {{math|''x''<sup>''n''</sup> &minus; 1}} can be factored into the product of two polynomials, say {{math|1=''x''<sup>''n''</sup> &minus; 1 = ''g''(''x'')''p''(''x'')}}. When {{math|''g''(''x'')}} generates a cyclic code {{math|''C''}}, then the reciprocal polynomial {{math|''p''<sup>∗</sup>}} generates {{math|''C''<sup>⊥</sup>}}, the [[orthogonal complement]] of {{math|''C''}}.<ref>{{harvnb|Pless|1990|loc = pg. 75, Theorem 48}}</ref>\nAlso, {{math|''C''}} is ''self-orthogonal'' (that is, {{math|''C'' ⊆ ''C''<sup>⊥</sup>)}}, if and only if  {{math|''p''<sup>∗</sup>}} divides {{math|''g''(''x'')}}.<ref>{{harvnb|Pless|1990|loc = pg. 77, Theorem 51}}</ref>\n\n== Notes ==\n{{reflist}}\n\n==References==\n{{More citations needed|date=June 2008}}\n* {{citation|first=Vera|last=Pless|title=Introduction to the Theory of Error Correcting Codes|edition=2nd|publisher=Wiley-Interscience|place=New York|year=1990|isbn=0-471-61884-5}}\n* {{citation|first=Steven|last=Roman|title=Field Theory|publisher=Springer-Verlag|place=New York|year=1995|isbn=0-387-94408-7}}\n* Émile Durand (1961) Solutions numériques des équations algrébriques I, Masson et Cie: XV - polynômes dont les coefficients sont symétriques ou antisymétriques, p.&nbsp;140-141.\n\n== External links ==\n* {{MathPages|id=home/kmath294/kmath294|title=The Fundamental Theorem for Palindromic Polynomials}}\n* [http://mathworld.wolfram.com/ReciprocalPolynomial.html Reciprocal Polynomial] (on [[MathWorld]])\n\n[[Category:Polynomials]]"
    },
    {
      "title": "Resultant",
      "url": "https://en.wikipedia.org/wiki/Resultant",
      "text": "{{about|the resultant of polynomials|the result of adding two or more vectors|Euclidean vector#Addition and subtraction|the musical phenomenon|Resultant tone}}\n\nIn [[mathematics]], the '''resultant''' of two [[polynomial]]s is a [[polynomial expression]] of their coefficients, which is equal to zero if and only if the polynomials have a common [[root of a function|root]] (possibly in a [[field extension]]), or, equivalently, a common factor (over their field of coefficients). In some older texts, the resultant is also called the '''eliminant'''.{{sfn|Salmon|1885|loc=lesson VIII, p. 66}}\n\nThe resultant is widely used in [[number theory]], either directly or through the [[discriminant]], which is essentially the resultant of a polynomial and its derivative. The resultant of two polynomials with [[rational number|rational]] or polynomial coefficients may be computed efficiently on a computer. It is a basic tool of [[computer algebra]], and is a built-in function of most [[computer algebra system]]s. It is used, among others, for [[cylindrical algebraic decomposition]], [[Symbolic integration|integration]] of [[rational function]]s and drawing of [[curve]]s defined by a [[Polynomial#Number of variables|bivariate]] [[polynomial equation]].\n\nThe resultant of ''n'' [[homogeneous polynomial]]s in ''n'' variables (also called '''multivariate resultant''', or '''Macaulay's resultant''' for distinguishing it from the usual resultant) is a generalization, introduced by [[Francis Sowerby Macaulay|Macaulay]], of the usual resultant.{{sfn|Macaulay|1902}}  It is, with [[Gröbner basis|Gröbner bases]], one of the main tools of effective [[elimination theory]] (elimination theory on computers).\n\n==Notation==\nThe resultant of two univariate polynomials {{math|''A''}} and {{math|''B''}} is commonly denoted <math>\\operatorname{res}(A,B)</math> or <math>\\operatorname{Res}(A,B).</math>\n\nIn many applications of the resultant, the polynomials depend on several indeterminates and may be considered as univariate polynomials in one of their indeterminates, with polynomials in the other indeterminates as coefficients. In this case, the indeterminate that is selected for defining and computing the resultant is indicated as a subscript: <math>\\operatorname{res}_x(A,B)</math> or <math>\\operatorname{Res}_x(A,B).</math>\n\nThe degrees of the polynomials are used in the definition of the resultant. However, a polynomial of degree {{math|''d''}} may also be considered as a polynomial of higher degree where the leading coefficients  are zero. If such a higher degree is used for the resultant, it is usually indicated as a subscript or a superscript, such as <math>\\operatorname{res}_{d,e}(A,B)</math> or <math>\\operatorname{res}_x^{d,e}(A,B).</math>\n\n==Definition==\n\nThe '''resultant''' of two [[univariate polynomial]]s over a [[field (mathematics)|field]] or over a [[commutative ring]] is commonly defined as the [[determinant]] of their [[Sylvester matrix]]. More precisely, let \n:<math>A=a_0x^d +a_1x^{d-1} + \\cdots + a_d</math>\nand\n:<math>B=b_0x^e +b_1x^{e-1} + \\cdots + b_e</math>\nbe nonzero polynomials of degrees {{math|''d''}} and {{math|''e''}} respectively. Let us denote by <math>\\mathcal{P}_i</math> the [[vector space]] (or [[free module]] if the coefficients belong to a commutative ring) of dimension ''i'' whose elements are the polynomials of degree strictly less than ''i''. The map\n:<math>\\varphi:\\mathcal{P}_{e}\\times \\mathcal{P}_{d} \\rightarrow \\mathcal{P}_{d+e}</math> \nsuch that \n:<math>\\varphi(P,Q)=AP+BQ</math>\nis a [[linear map]] between two spaces of the same dimension. Over the basis of the powers of {{math|''x''}}, this map is represented by a square matrix of dimension {{math|''d'' + ''e''}}, which is called the ''Sylvester matrix'' of {{math|''A''}} and {{math|''B''}} (for many authors and in the article [[Sylvester matrix]], the Sylvester matrix is defined as the transpose of this matrix; this convention is not used here, as it breaks the usual convention for writing the matrix of a linear map).\n\nThe resultant of {{math|''A''}} and {{math|''B''}} is thus the determinant\n\n:<math>\\begin{vmatrix} \na_0      & 0           & \\cdots & 0          & b_0        & 0              & \\cdots & 0       \\\\\na_1    & a_0       & \\cdots & 0           & b_1     & b_0           & \\cdots & 0  \\\\\na_2    & a_1     & \\ddots & 0           & b_2     & b_1         & \\ddots & 0 \\\\\n\\vdots  &\\vdots   & \\ddots & a_0        & \\vdots   &\\vdots       & \\ddots & b_0  \\\\\na_d       & a_{d-1} & \\cdots & \\vdots   & b_e       & b_{e-1}     & \\cdots & \\vdots\\\\\n0          & a_d       & \\ddots &  \\vdots  & 0          & b_e          & \\ddots &  \\vdots  \\\\\n\\vdots  & \\vdots   & \\ddots & a_{d-1}  & \\vdots  & \\vdots      & \\ddots & b_{e-1}   \\\\\n0          & 0          & \\cdots  & a_d       & 0           & 0              & \\cdots & b_e   \n\\end{vmatrix},</math>\nwhich has {{math|''e''}} columns of {{math|''a''<sub>''i''</sub>}} and {{math|''d''}} columns of {{math|''b''<sub>''j''</sub>}} (the fact that the first column of {{mvar|a}}'s and the first column of {{mvar|b}}'s have the same length, that is {{math|1=''d'' = ''e''}}, is here only for simplifying the display of the determinant).\n\nIf the coefficients of the polynomials belong to an [[integral domain]], then \n:<math>\\operatorname{res}(A, B) = a_0^e b_0^d \\prod_{\\begin{array}{c}1 \\leq i \\leq d\\\\ 1 \\leq j \\leq e\\end{array}} (\\lambda_i-\\mu_j) = a_0^e \\prod_{i=1}^d B(\\lambda_i) = (-1)^{de} b_0^d \\prod_{j=1}^e A(\\mu_j),</math>\nwhere <math>\\lambda_1, \\dots, \\lambda_d</math> and <math>\\mu_1,\\dots,\\mu_e</math> are respectively the roots, counted with their multiplicities, of {{mvar|A}} and {{mvar|B}} in any [[algebraically closed field]] containing the integral domain.\nThis is a straightforward consequence of the characterizing properties of the resultant that appear below. In the common case of integer coefficients, the algebraically closed field is generally chosen as the field of [[complex number]]s.\n\n==Properties==\nIn this section and its subsections, {{math|''A''}} and {{math|''B''}} are two polynomials in {{math|''x''}} of respective degrees {{math|''d''}} and {{math|''e''}}, and their resultant is denoted \n<math>\\operatorname{res}(A,B).</math>\n\n===Characterizing properties===\nThe resultant of two polynomials {{mvar|A}} and {{mvar|B}} of respective degrees {{mvar|d}} and {{mvar|e}}, with coefficients in \na [[commutative ring]] {{math|''R''}}, has the following properties that characterize the resultant, if {{mvar|R}} is a [[field (mathematics)|field]] or, more generally, an [[integral domain]]\n\n* If {{mvar|R}} is a [[subring]] of another ring {{mvar|S}}, then <math>\\operatorname{res}_R(A,B) = \\operatorname{res}_S(A,B).</math> That is {{mvar|A}} and {{mvar|B}} have the same resultant when considered as polynomials over {{mvar|R}} or {{mvar|S}}.\n*If {{math|1=''d'' = 0}} (that is if <math>A=a_0</math> is a nonzero constant) then <math>\\operatorname{res}(A,B) = a_0^e.</math> Similarly, if {{math|1=''e'' = 0}}, then <math>\\operatorname{res}(A,B) = b_0^d.</math>\n* <math>\\operatorname{res}(x-a_1, x-b_1) = b_1-a_1</math>\n* <math>\\operatorname{res}(B,A)=(-1)^{de} \\operatorname{res}(A,B)</math> \n* <math>\\operatorname{res}(AB,C) = \\operatorname{res}(A,C)\\operatorname{res}(B,C)</math>\n\nIn other words, the resultant is the unique function of the coefficients of two polynomials that has these properties.\n\nSome mathematics software, including [[Wolfram Mathematica|Mathematica]] and [[Maple (software)|Maple]], use the alternate convention <math>\\operatorname{res}(x-a_1, x-b_1) = a_1-b_1</math>.\n\n===Zeros===\n* The resultant of two polynomials with coefficients in an [[integral domain]] is zero if and only if they have a [[Greatest common divisor of two polynomials|common divisor]] of positive degree.\n* The resultant of two polynomials with coefficients in an integral domain is zero if and only if they have a common root in an [[algebraically closed field]] containing the coefficients.\n* There exist a polynomial {{math|''P''}} of degree less than {{math|''e''}} and a polynomial {{math|''Q''}} of degree less than {{math|''d''}} such that <math> \\operatorname{res}(A,B)=AP+BQ.</math> This is a generalization of [[Bézout's identity]] to polynomials over an arbitrary commutative ring.  In other words, the resultant of two polynomials belongs to the [[ideal (ring theory)|ideal]] generated by these polynomials.\n\n===Invariance by ring homomorphisms===\nLet {{math|''A''}} and {{math|''B''}} be two polynomials of respective degrees {{math|''d''}} and {{math|''e''}} with coefficients in a [[commutative ring]] {{math|''R''}}, and <math>\\varphi\\colon R\\to S</math> a [[ring homomorphism]] of {{math|''R''}} into another commutative ring {{math|''S''}}. Applying <math>\\varphi</math> to the coefficients of a polynomial extends <math>\\varphi</math> to a homomorphism of polynomial rings <math>R[x]\\to S[x]</math>, which is also denoted <math>\\varphi.</math> With this notation, we have:\n* If <math>\\varphi</math> preserves the degrees of  {{math|''A''}} and {{math|''B''}} (that is if <math>\\deg(\\varphi(A)) = d</math> and <math>\\deg(\\varphi(B))= e</math>), then \n::<math>\\varphi(\\operatorname{res}(A,B))=\\operatorname{res}(\\varphi(A), \\varphi(B)).</math>\n* If <math>\\deg(\\varphi(A)) < d</math> and <math>\\deg(\\varphi(B))< e,</math> then \n::<math>\\varphi(\\operatorname{res}(A,B)) = 0.</math>\n* If <math>\\deg(\\varphi(A)) = d</math> and <math>\\deg(\\varphi(B)) =f < e,</math>  and the leading coefficient of {{math|''A''}} is <math>a_0</math> then \n::<math>\\varphi(\\operatorname{res}(A,B))=\\varphi(a_0)^{e-f}\\operatorname{res}(\\varphi(A), \\varphi(B)).</math>\n* If <math>\\deg(\\varphi(A)) = f<d</math> and <math>\\deg(\\varphi(B)) = e,</math>  and the leading coefficient of {{math|''B''}} is <math>b_0</math> then \n::<math>\\varphi(\\operatorname{res}(A,B)) = (-1)^{e(d-f)}\\varphi(b_0)^{d-f}\\operatorname{res}(\\varphi(A), \\varphi(B)).</math>\n\nThese properties are easily deduced from the definition of the resultant as a determinant. They are mainly used in two situations. For computing a resultant of polynomials with integer coefficients, it is generally faster to compute it [[modular arithmetic|modulo]] several primes and to retrieve the desired resultant with [[Chinese remainder theorem]]. When {{math|''R''}} is a polynomial ring in other indeterminates, and {{math|''S''}} is the ring obtained by specializing to numerical values some or all indeterminates of {{math|''R''}}, these properties may be restated as ''if the degrees are preserved by the specialization, the resultant of the specialization of two polynomials is the specialization of the resultant''. This property is fundamental, for example, for [[cylindrical algebraic decomposition]].\n\n===Invariance under change of variable===\n*<math>\\operatorname{res}(A(x+a), B(x+a)) = \\operatorname{res}(A(x), B(x))</math>\n*<math>\\operatorname{res}(A(ax), B(ax)) = a^{de}\\operatorname{res}(A(x), B(x))</math>\n* If <math>A_r(x)=x^dA(1/x)</math> and <math>B_r(x)=x^eB(1/x)</math> are the [[reciprocal polynomial]]s of {{math|''A''}} and {{math|''B''}}, respectively, then\n::<math>\\operatorname{res}(A_r, B_r)= (-1)^{de}\\operatorname{res}(A,B)</math>\n\nThis means that the property of the resultant being zero is invariant under linear and projective changes of the variable.\n\n===Invariance under change of polynomials===\n*If {{math|''a''}} and {{mvar|''b''}} are nonzero constants (that is they are independent of the indeterminate {{math|''x''}}), and {{math|''A''}} and {{mvar|''B''}} are as above, then\n::<math>\\operatorname{res}(aA,bB) =a^eb^d\\operatorname{res}(A,B). </math>\n*If {{math|''A''}} and {{mvar|''B''}} are as above, and {{mvar|C}} is another polynomial such that the degree of {{math|''A'' – ''CB''}} is {{math|''{{delta}}''}}, then \n::<math>\\operatorname{res}(A-CB, B)=b_0^{\\delta-d}\\operatorname{res}(A,B). </math> \n*In particular, if either {{mvar|B}} is [[monic polynomial|monic]], or {{math|deg ''C'' < deg ''A'' – deg ''B''}}, then\n::<math>\\operatorname{res}(A-CB,B) = \\operatorname{res}(A,B), </math>\n:and, if {{math|1=''f'' = deg ''C'' > deg ''A'' – deg ''B'' = ''d'' – ''e''}}, then\n::<math>\\operatorname{res}(A-CB, B)=b_0^{e+f-d}\\operatorname{res}(A,B). </math>\n\nThese properties imply that in [[Euclidean algorithm for polynomials]], and all its variants ([[pseudo-remainder sequence]]s), the resultant of two successive remainders (or pseudo-remainders) differs from the resultant of the initial polynomials by a factor, which is easy to compute. Conversely, this allows deducing the resultant of the initial polynomials from the value of the last remainder or pseudo-remainder. This is the starting idea of the [[Polynomial greatest common divisor#Subresultant sequence with pseudo-remainders|subresultant-pseudo-remainder-sequence algorithm]], which uses above formulas for getting [[subresultant|subresultant polynomials]] as pseudo-remainders, and the resultant as the last nonzero pseudo-remainder (if the resultant is not zero). This algorithm works for polynomials over the integers or, more generally, over an integral domain, without any other division than exact divisions (that is without involving fractions). It involves <math>O(de)</math> arithmetic operations, while the computation of the determinant of the Sylvester matrix with standard algorithms require <math>O((d+e)^3)</math> arithmetic operations.\n\n===Generic properties===\nIn this section, we consider two polynomials\n:<math>A=a_0x^d +a_1x^{d-1} + \\cdots + a_d</math>\nand\n:<math>B=b_0x^e +b_1x^{e-1} + \\cdots + b_e</math>\nwhose {{math|''d'' + ''e'' + 2}} coefficients are distinct [[indeterminate (variable)|indeterminate]]s. Let\n:<math>R=\\mathbb{Z}[a_0, \\ldots, a_d, b_0, \\ldots, b_e]</math>\nbe the polynomial ring over the integers defined by these indeterminates.\nThe resultant <math>\\operatorname{res}(A,B)</math> is often called the '''generic resultant''' for the degrees {{math|''d''}} and {{math|''e''}}. It has the following properties.\n\n*<math>\\operatorname{res}(A,B)</math> is an [[absolutely irreducible]] polynomial.\n*If <math>I</math> is the [[ideal (ring theory)|ideal]] of <math>R[x]</math> generated by {{math|''A''}} and {{math|''B''}}, then <math>I\\cap R</math> is the [[principal ideal]] generated by <math>\\operatorname{res}(A,B)</math>.\n\n===Homogeneity===\nThe generic resultant for the degrees {{math|''d''}} and {{math|''e''}} is [[homogeneous polynomial|homogeneous]] in various ways. More precisely:\n* It is homogeneous of degree {{math|''e''}} in <math>a_0, \\ldots, a_d.</math>\n* It is homogeneous of degree {{math|''d''}} in <math>b_0, \\ldots, b_e.</math>\n* It is homogeneous of degree {{math|''d'' + ''e''}} in all the variables <math>a_i</math> and <math>b_j.</math>\n* If <math>a_i</math> and <math>b_i</math> are given the weight {{math|''i''}} (that is, the weight of each coefficient is its degree as [[elementary symmetric polynomial]]), then it is [[quasi-homogeneous polynomial|quasi-homogeneous]] of total weight {{math|''de''}}.\n*If {{math|''P''}} and {{math|''Q''}} are homogeneous multivariate polynomials of respective degrees {{math|''d''}} and {{math|''e''}}, then their resultant in degrees {{math|''d''}} and {{math|''e''}} with respect to an indeterminate {{math|''x''}}, denoted <math>\\operatorname{res}_x^{d,e}(P,Q)</math> in {{slink||Notation}}, is homogeneous of degree {{math|''de''}} in the other indeterminates.\n\n===Elimination properties===\nLet <math>I=\\langle A, B\\rangle </math> be the [[ideal (ring theory)|ideal]] generated by two polynomials {{math|''A''}} and {{math|''B''}} in a polynomial ring <math>R[x],</math> where {{math|''R''}} is itself a polynomial ring over a field. Then:\n* <math>I\\cap R = R\\cdot r</math> is a [[principal ideal]] generated by {{math|''r''}} for some <math>r\\in R</math>\n* <math>\\operatorname{res}_x(A,B)\\in R\\cdot r</math>\n* There exists a positive integer {{math|''k'' }}such that <math>r^k\\in R \\cdot \\operatorname{res}_x(A,B).</math>\nAn example where {{math|''k'' > 1}} in the latter property is <math>R=\\mathbb{R}[y],</math> <math>A=x^2+y^2-1</math> (the [[unit circle]]), and {{math|1=''B'' = ''y'' − 2}}. In this case, <math>\\operatorname{res}_x(A,B) = (y-2)^2</math> and <math>\\langle A, B\\rangle \\cap R=R\\cdot(y-2).</math> This example has been chosen for having a [[prime ideal]] <math>\\langle A, B\\rangle.</math> For another field of coefficients and another constant term in {{math|''B''}}, one has also {{math|''k'' > 1}}, but the ideal may be non-prime.\n\n==Computation==\n\nTheoretically, the resultant could be computed by using the formula expressing it as a product of roots differences. However, as the roots may generally not be computed exactly, such an algorithm would be inefficient and [[numerically unstable]]. As the resultant is a [[symmetric polynomial|symmetric function]] of the roots of each polynomial, it could also be computed by using the [[Symmetric polynomial#The fundamental theorem of symmetric polynomials|fundamental theorem of symmetric polynomials]], but this would be highly inefficient.\n\nAs the resultant is the [[determinant]] of the [[Sylvester matrix]] (and of the [[Bézout matrix]]), it may be computed by using any algorithm for computing determinants. This needs <math>O(n^3)</math> arithmetic operations. As algorithms are known with a better complexity (see below), this method is not used in practice.\n\nIt follows from {{slink||Invariance under change of polynomials}} that the computation of a resultant is strongly related to the [[Polynomial greatest common divisor#Euclid's algorithm|Euclidean algorithm for polynomials]]. This shows that the computation of the resultant of two polynomials of degrees {{math|''d''}} and {{math|''e''}} may be done in <math>O(de)</math> arithmetic operations in the field of coefficients.\n\nHowever, when the coefficients are integers, rational numbers or polynomials, these arithmetic operations imply a number of GCD computations of coefficients which is of the same order and make the algorithm inefficient.\nThe [[Polynomial greatest common divisor#Subresultant pseudo-remainder sequence|subresultant pseudo-remainder sequence]]s were introduced to solve this problem and avoid any fraction and any GCD computation of coefficients. A more efficient algorithm is obtained by using the good behavior of the resultant under a ring homomorphism on the coefficients: to compute a resultant of two polynomials with integer coefficients, one computes their resultants modulo sufficiently many [[prime number]]s and then reconstructs the result with the [[Chinese remainder theorem]].\n\nThe use of [[fast multiplication]] of integers and polynomials allows algorithms for resultants and greatest common divisors that have a better [[time complexity]], which is of the order of the complexity of the multiplication, multiplied by the logarithm of the size of the input (<math>\\log(s(d+e)),</math> where {{math|''s''}} is an upper bound of the number of digits of the input polynomials).\n\n==Application to polynomial systems==\n\nResultants were introduced for solving [[systems of polynomial equations]] and provide the oldest proof that there exist [[algorithm]]s for solving such systems. These are primarily intended for systems of two equations in two unknowns, but also  allow solving general systems.\n\n===Case of two equations in two unknowns===\n\nConsider the system of two polynomial equations\n:<math>\\begin{align}\nP(x,y)&=0\\\\\nQ(x,y)&=0,\n\\end{align}</math>\nwhere {{math|''P''}} and {{math|''Q''}} are polynomials of respective [[total degree]]s {{math|''d''}} and {{math|''e''}}. Then <math>R=\\operatorname{res}_y^{d,e}(P,Q)</math> is a polynomial in {{math|''x''}}, which is [[generic property|generically]] of degree {{math|''de''}} (by properties of {{slink||Homogeneity}}). A value <math>\\alpha</math> of {{math|''x''}} is a root of {{math|''R''}} if and only if either there exist <math>\\beta</math> in an [[algebraically closed field]] containing the coefficients, such that <math>P(\\alpha,\\beta)=Q(\\alpha,\\beta)=0</math>, or <math>\\deg(P(\\alpha,y)) <d </math> and <math>\\deg(Q(\\alpha,y)) <d </math> (in this case, one says that {{math|''P''}} and {{math|''Q''}} have a common root at infinity for <math>x=\\alpha</math>).\n\nTherefore, solutions to the system are obtained by computing the roots of {{math|''R''}}, and for each root <math>\\alpha,</math> computing the common root(s) of <math>P(\\alpha,y),</math> <math>Q(\\alpha,y),</math> and <math>\\operatorname{res}_x(P,Q).</math>\n\n[[Bézout's theorem]] results from the value of <math>\\deg\\left(\\operatorname{res}_y(P,Q)\\right)\\le de</math>, the product of the degrees of {{math|''P''}} and {{math|''Q''}}. In fact, after a linear change of variables, one may suppose that, for each root {{math|''x''}} of the resultant, there is exactly one value of {{math|''y''}} such that {{math|(''x'', ''y'')}} is a common zero of {{math|''P''}} and {{math|''Q''}}. This shows that the number of common zeros is at most the degree of the resultant, that is at most the product of the degrees of {{math|''P''}} and {{math|''Q''}}. With some technicalities, this proof may be extended to show that, counting multiplicities and zeros at infinity, the number of zeros is exactly the product of the degrees.\n\n===General case===\nAt first glance, it seems that resultants may be applied to a general [[polynomial system of equations]]\n:<math>P_1(x_1, \\ldots, x_n)=0</math>\n:<math>\\vdots</math>\n:<math>P_k(x_1, \\ldots, x_n)=0</math>\nby computing the resultants of every pair <math>(P_i,P_j)</math> with respect to <math>x_n</math> for eliminating one unknown, and repeating the process until getting univariate polynomials. Unfortunately, this introduces many spurious solutions, which are difficult to remove.\n\nA method, introduced at the end of the 19th century, works as follows: introduce {{math|''k'' − 1}} new indeterminates <math>U_2, \\ldots, U_k</math> and compute\n:<math>\\operatorname{res}_{x_n}(P_1, U_2P_2 +\\cdots +U_kP_k).</math> \nThis is a polynomial in <math>U_2, \\ldots, U_k</math> whose coefficients are polynomials in <math>x_1, \\ldots, x_{n-1},</math> which have the property that <math>\\alpha_1, \\ldots, \\alpha_{n-1}</math> is a common zero of these polynomial coefficients, if and only if the univariate polynomials <math>P_i(\\alpha_1, \\ldots, \\alpha_{n-1}, x_n)</math> have a common zero, possibly [[point at infinity|at infinity]]. This process may be iterated until finding univariate polynomials.\n\nTo get a correct algorithm two complements have to be added to the method. Firstly, at each step, a linear change of variable may be needed in order that the degrees of the polynomials in the last variable are the same as their total degree. Secondly, if, at any step, the resultant is zero, this means that the polynomials have a common factor and that the solutions split in two components: one where the common factor is zero, and the other which is obtained by factoring out this common factor before continuing.\n\nThis algorithm is very complicated and has a huge [[time complexity]]. Therefore, its interest is mainly historical.\n\n==Other applications==\n\n===Number theory===\nThe [[discriminant]] of a polynomial, which is a fundamental tool in [[number theory]] is the quotient by its leading coefficient of the resultant of the polynomial and its derivative.\n\nIf ''x'' and ''y'' are [[algebraic numbers]] such that <math>P(x)=Q(y)=0</math>, then <math>z=x+y</math> is a root of the resultant <math>\\operatorname{res}_x(P(x),Q(z-x)),</math> and <math>t=xy</math> is a root of <math>\\operatorname{res}_x(P(x),x^nQ(t/x))</math>, where ''n'' is the [[Degree of a polynomial|degree]] of {{math|''Q''(''y'')}}. Combined with the fact that <math>1/y</math> is a root of <math>y^nQ(1/y) = 0</math>, this shows that the set of algebraic numbers is a [[Field (mathematics)|field]].\n\nLet <math>K(\\alpha)</math> be an algebraic field extension generated by an element <math>\\alpha,</math> which has <math>P(x)</math> as [[minimal polynomial (field theory)|minimal polynomial]]. Every element of <math>\\beta \\in K(\\alpha)</math> may be written as <math>\\beta=Q(\\alpha),</math> where {{math|''Q''}} is a polynomial. Then <math>\\beta</math> is a root of <math>\\operatorname{res}_x(P(x),z-Q(x)),</math> and this resultant is a power of the minimal polynomial of <math>\\beta.</math>\n\n===Algebraic geometry===\nGiven two [[plane algebraic curve]]s defined as the zeros of the polynomials {{math|''P''(''x'', ''y'')}} and {{math|''Q''(''x'', ''y'')}}, the resultant allows the computation of their intersection. More precisely, the roots of <math>\\operatorname{res}_y(P,Q)</math> are the ''x''-coordinates of the intersection points and of the common vertical asymptotes, and the roots of <math>\\operatorname{res}_x(P,Q)</math> are the ''y''-coordinates of the intersection points and of the common horizontal asymptotes.\n\nA [[rational curve|rational plane curve]] may be defined by a [[parametric equation]]\n:<math>\nx=\\frac{P(t)}{R(t)},\\qquad\ny=\\frac{Q(t)}{R(t)},\n</math>\nwhere {{math|''P''}}, {{math|''Q''}} and {{math|''R''}} are polynomials. An [[implicit equation]] of the curve is given by\n:<math>\\operatorname{res}_t(xR-P,yR-Q).</math>\nThe ''degree'' of this curve is the highest degree of {{math|''P''}}, {{math|''Q''}} and {{math|''R''}}, which is equal to the total degree of the resultant.\n\n===Symbolic integration===\nIn [[symbolic integration]], for computing the [[antiderivative]] of a [[rational fraction]], one uses [[partial fraction decomposition]] for decomposing the integral into a \"rational part\", which is a sum of rational fractions whose antiprimitives are rational fractions, and a \"logarithmic part\" which is a sum of rational fractions of the form\n:<math>\\frac{P(x)}{Q(x)},</math>\nwhere {{math|''Q''}} is a [[square-free polynomial]] and {{math|''P''}} is a polynomial of lower degree than {{math|''Q''}}. The antiderivative of such a function involves necessarily [[logarithms]], and generally algebraic numbers (the roots of {{math|''Q''}}). In fact, the antiderivative is \n:<math>\\int \\frac{P(x)}{Q(x)}dx=\\sum_{Q(\\alpha)=0} \\frac{P(\\alpha)}{Q'(\\alpha)}\\log(x-\\alpha),</math>\nwhere the sum runs over all complex roots of {{math|''Q''}}.\n\nThe number of [[algebraic number]]s involved by this expression is generally equal to the degree of {{math|''Q''}}, but it occurs frequently that an expression with less algebraic numbers may be computed. The [[Daniel Lazard|Lazard]]–Rioboo–[[Barry Trager|Trager]] method produced an expression, where the number of algebraic numbers is minimal, without any computation with algebraic numbers.\n\nLet \n:<math> S_1(r) S_2(r)^2 \\cdots S_k(r)^k = \\operatorname{res}_r (rQ'(x)-P(x), Q(x))</math> \nbe the [[square-free factorization]] of the resultant which appears on the right. Trager proved that the antiderivative is \n:<math>\\int \\frac{P(x)}{Q(x)}dx=\\sum_{i=1}^k\\sum_{S_i(\\alpha)=0} \\alpha \\log(T_i(\\alpha,x)),</math>\nwhere the internal sums run over the roots of the <math>S_i</math> (if <math>S_i=1</math> the sum is zero, as being the [[empty sum]]), and <math>T_i(r,x)</math> is a polynomial of degree {{math|''i''}} in {{math|''x''}}. The Lazard-Rioboo contribution is the proof that <math>T_i(r,x)</math> is the [[polynomial greatest common divisor#Subresultants|subresultant]] of degree {{math|''i''}} of <math>rQ'(x)-P(x)</math> and <math>Q(x).</math> It is thus obtained for free if the resultant is computed by the [[polynomial greatest common divisor#Subresultant pseudo-remainder sequence|subresultant pseudo-remainder sequence]].\n\n===Computer algebra===\nAll preceding applications, and many others, show that the resultant is a fundamental tool in [[computer algebra]]. In fact most [[computer algebra systems]] include an efficient implementation of the computation of resultants.\n\n==Homogeneous resultant==\n\nThe resultant is also defined for two [[homogeneous polynomial]] in two indeterminates. Given two homogeneous polynomials {{math|''P''(''x'', ''y'')}} and {{math|''Q''(''x'', ''y'')}} of respective [[total degree]]s {{math|''p''}} and {{math|''q''}}, their '''homogeneous resultant''' is the [[determinant]] of the matrix over the [[monomial basis]] of the [[linear map]]\n:<math>(A,B) \\mapsto AP+BQ,</math>\nwhere {{math|''A''}} runs over the bivariate homogeneous polynomials of degree {{math|''q'' − 1}}, and {{math|''B''}} runs over the homogeneous polynomials of degree {{math|''p'' − 1}}. In other words, the homogeneous resultant of {{math|''P''}} and {{math|''Q''}} is the resultant of \n{{math|''P''(''x'', 1)}} and {{math|''Q''(''x'', 1)}} when they are considered as polynomials of degree {{math|''p''}} and {{math|''q''}} (their degree in {{math|''x''}} may be lower than their total degree):\n:<math>\\operatorname{Res}(P(x,y),Q(x,y)) = \\operatorname{res}_{p,q}(P(x,1),Q(x,1)). </math>\n(The capitalization of \"Res\" is used here for distinguishing the two resultants, although there is no standard rule for the capitalization of the abbreviation).\n\nThe homogeneous resultant has essentially the same properties as the usual resultant, with essentially two differences: instead of polynomial roots, one considers zeros in the [[projective line]], and the degree of a polynomial may not change under a [[ring homomorphism]].\nThat is:\n* The resultant of two homogeneous polynomials over an [[integral domain]] is zero if and only if they have a non-zero common zero over an [[algebraically closed field]] containing the coefficients.\n* If {{math|''P''}} and {{math|''Q''}} are two bivariate homogeneous polynomials with coefficients in a [[commutative ring]] {{math|''R''}}, and <math>\\varphi\\colon R\\to S</math> a [[ring homomorphism]] of {{math|''R''}} into another commutative ring {{math|''S''}}, then extending <math>\\varphi</math> to polynomials over {{math|''R''}}, ones has\n::<math>\\operatorname{Res}(\\varphi(P), \\varphi(Q)) = \\varphi(\\operatorname{Res}(P,Q)).</math>\n* The property of an homogeneous resultant to be zero is invariant under any projective change of variables.\n\nAny property of the usual resultant may similarly extended to the homogeneous resultant, and the resulting property is either very similar or simpler than the corresponding property of the usual resultant.\n\n==Macaulay's resultant==\n\n'''Macaulay's resultant''', named after [[Francis Sowerby Macaulay]], also called the '''multivariate resultant''', or the '''multipolynomial resultant''',<ref>{{Citation | last1=Cox | first1=David | last2=Little | first2=John | last3=O'Shea | first3=Donal | title=Using Algebraic Geometry | publisher=[[Springer Science+Business Media]] | isbn=978-0387207339 | year=2005}}, Chapter 3. Resultants</ref> is a generalization of the homogeneous resultant to {{math|''n''}} [[homogeneous polynomial]]s in {{math|''n''}} [[indeterminate (variable)|indeterminates]]. Macaulay's resultant is a polynomial in the coefficients of these {{math|''n''}} homogeneous polynomials that vanishes if and only if the polynomials have a common non-zero solution in an [[algebraically closed field]] containing the coefficients, or, equivalently, if the {{math|''n''}} hyper surfaces defined by the polynomials have a common zero in the {{math|''n''  –1}} dimensional projective space. The multivariate resultant is, with [[Gröbner basis|Gröbner bases]], one of the main tools of effective [[elimination theory]] (elimination theory on computers).\n\nLike the homogeneous resultant, Macaulay's may be defined with [[determinants]], and thus behaves well under [[ring homomorphism]]s. However, it cannot be defined by a single determinant. It follows that it is easier to define it first on [[generic polynomial]]s.\n\n===Resultant of generic homogeneous polynomials===\nA homogeneous polynomial of degree {{math|''d''}} in {{math|''n''}} variables may have up to \n:<math>\\binom{n+d-1}{n-1}=\\frac{(n+d-1)!}{(n-1)!\\,d!}</math>\ncoefficients; it is said to be ''generic'', if these coefficients are distinct indeterminates.\n\nLet <math>P_1, \\ldots, P_n</math> be {{math|''n''}} generic homogeneous polynomials in {{math|''n''}} indeterminates, of respective [[total degree|degrees]] <math>d_1, \\dots, d_n.</math> Together, they involve\n:<math>\\sum_{i=1}^n\\binom{n+d_i-1}{n-1}</math>\nindeterminate coefficients.\nLet {{math|''C''}} be the polynomial ring over the integers, in all these \nindeterminate coefficients. The polynomials <math>P_1, \\ldots, P_n</math> belong thus to <math>C[x_1,\\ldots, x_n],</math> and their resultant (still to be defined) belongs to {{math|''C''}}.\n\nThe '''Macaulay degree''' is the integer <math>D=d_1+\\cdots+d_n-n+1,</math> which is fundamental in Macaulay's theory. For defining the resultant, one considers the '''Macaulay matrix''', which is the matrix over the [[monomial basis]] of the {{math|''C''}}-linear map\n:<math>(Q_1, \\ldots, Q_n)\\mapsto Q_1P_1+\\cdots+Q_nP_n,</math>\nin which each <math>Q_i</math> runs over the homogeneous polynomials of degree <math>D-d_i,</math> and the [[codomain]] is the {{math|''C''}}-module of the homogeneous polynomials of degree {{math|''D''}}.\n\nIf {{math|1=''n'' = 2}}, the Macaulay matrix is the Sylvester matrix, and is a [[square matrix]], but this is no longer true for {{math|''n'' > 2}}. Thus, instead of considering the determinant, one considers all the maximal [[minor (linear algebra)|minors]], that is the determinants of the square submatrices that have as many rows as the Macaulay matrix. Macaulay proved that the {{math|''C''}}-ideal generated by these principal minors is a [[principal ideal]], which is generated by the [[greatest common divisor]] of these minors. As one is working with polynomials with integer coefficients, this greatest common divisor is defined up its sign. The '''generic Macaulay resultant''' is the greatest common divisor which becomes {{math|1}}, when, for each {{math|''i''}}, zero is substituted for all coefficients of <math>P_i,</math> except the coefficient of <math>x_i^{d_i},</math> for which one is substituted.\n\n====Properties of the generic Macaulay resultant====\n*The generic Macaulay resultant is an [[irreducible polynomial]].\n*It is homogeneous of degree <math>B/d_i</math> in the coefficients of <math>P_i,</math> where <math>B=d_1\\cdots d_n</math> is the [[Bézout's theorem|Bézout bound]].\n*The product with the resultant of every monomial of degree {{math|''D}} in <math>x_1, \\cdots, x_n</math> belongs to the ideal of <math>C[x_1,\\cdots,x_n]</math> generated by <math>P_1,\\ldots,P_n.</math>\n\n===Resultant of polynomials over a field===\nFrom now on, we consider that the homogeneous polynomials <math>P_1,\\ldots,P_n,</math> of degrees <math>d_1,\\ldots,d_n</math> have their coefficients in a [[field (mathematics)|field]] {{math|''k''}}, that is that they belong to <math>k[x_1,\\cdots,x_n].</math> Their '''resultant''' is defined as the element of {{math|''k''}} obtained by replacing in the generic resultant the indeterminate coefficients by the actual coefficients of the <math>P_i.</math>\n\nThe main property of the resultant is that it is zero if only if <math>P_1,\\ldots,P_n,</math> have a nonzero common zero in an [[algebraically closed extension]] of {{math|''k''}}.\n\nThe \"only if\" part of this theorem results on the last property of the preceding paragraph, and is an effective version of [[Hilbert's Nullstellensatz#Projective Nullstellensatz|Projective Nullstellensatz]]: If the resultant is nonzero, then\n:<math>\\langle x_1,\\ldots x_n\\rangle^D \\subseteq \\langle P_1,\\ldots,P_n\\rangle,</math>\nwhere <math>D=d_1+\\cdots +d_n-n+1</math> is the Macaulay degree, and <math>\\langle x_1,\\ldots x_n\\rangle</math> is the maximal homogeneous ideal. This implies that <math>P_1,\\ldots,P_n</math> have no other common zero that the unique common zero, {{math|(0, ..., 0)}}, of <math>x_1,\\ldots,x_n.</math>\n\n===Computability===\nAs the computation of a resultant may be reduced to computing determinants and [[polynomial greatest common divisor]]s, there are [[algorithm]]s for computing resultants in a finite number of steps.\n\nHowever, the generic resultant is a polynomial of very high degree (exponential in {{math|''n''}}) depending on a huge number of indeterminates. It follows that, except for very small {{math|''n''}} and very small degrees of input polynomials, the generic resultant is, in practice, impossible to compute, even with modern computers. Moreover, the number of [[monomial]]s of the generic resultant is so high, that, if it would be computable, the result could not be stored on available memory devices, even for rather small values of {{math|''n''}} and of the degrees of the input polynomials.\n\nTherefore, computing the resultant makes sense only for polynomials whose coefficients belong to a field or are polynomials in few indeterminates over a field.\n\nIn the case of input polynomials with coefficients in a field, the exact value of the resultant is rarely important, only its equality (or not) to zero matters. As the resultant is zero if and only if the rank of the Macaulay matrix is lower than its number of its rows, this equality to zero may by tested by applying [[Gaussian elimination]] to the Macaulay matrix. This provides a [[time complexity|computational complexity]] <math>d^{O(n)},</math> where {{math|''d''}} is the maximum degree of input polynomials.\n\nAnother case where the computation of the resultant may provide useful information is when the coefficients of the input polynomials are polynomials in a small number of indeterminates, often called parameters. In this case, the resultant, if not zero, defines a [[hypersurface]] in the parameter space. A point belongs to this hyper surface, if and only if there are values of <math>x_1, \\ldots,x_n</math> which, together with the coordinates of the point are a zero of the input polynomials. In other words, the resultant is the result of the \"[[elimination theory|elimination]]\" of <math>x_1, \\ldots,x_n</math> from the input polynomials.\n\n===''U''-resultant===\nMacaulay's resultant provides a method, called \"''U''-resultant\" by Macaulay, for solving [[systems of polynomial equations]].\n\nGiven {{math|''n'' − 1}} homogeneous polynomials <math>P_1, \\ldots, P_{n-1},</math> of degrees <math>d_1, \\ldots, d_{n-1},</math> in  {{math|''n''}} indeterminates <math>x_1, \\ldots, x_n,</math> over a field {{math|''k''}},  their '''''U''-resultant''' is the resultant of the {{math|''n''}} polynomials <math>P_1, \\ldots, P_{n-1}, P_n,</math> where\n:<math>P_n=u_1x_1 +\\cdots +u_nx_n</math>\nis the generic [[linear form]] whose coefficients are new indeterminates <math>u_1, \\ldots, u_n.</math> Notation <math>u_i</math> or <math>U_i</math> for these generic coefficients is traditional, and is the origin of the term ''U''-resultant.\n\nThe ''U''-resultant is a homogeneous polynomial in <math>k[u_1, \\ldots, u_n].</math> It is zero if and only if the common zeros of <math>P_1, \\ldots, P_{n-1}</math> form a [[projective algebraic set]] of positive [[dimension of an algebraic variety|dimension]] (that is, there are infinitely many projective zeros over an  [[algebraically closed extension]] of {{math|''k''}}). If the ''U''-resultant is not zero, its degree is the [[Bézout's theorem|Bézout bound]] <math>d_1\\cdots d_{n-1}.</math>\nThe ''U''-resultant factorizes over an algebraically closed extension of {{math|''k''}} into a product of linear forms. If <math>\\alpha_1u_1+\\ldots+\\alpha_nu_n</math> is such a linear factor, then <math>\\alpha_1, \\ldots, \\alpha_n</math> are the [[homogeneous coordinates]] of a common zero of <math>P_1, \\ldots, P_{n-1}.</math> Moreover, every common zero may be obtained from one of these linear factors, and the multiplicity as a factor is equal to the [[intersection multiplicity]] of the <math>P_i</math> at this zero. In other words, the ''U''-resultant provides a completely explicit version of [[Bézout's theorem]].\n\n====Extension to more polynomials and computation====\nThe ''U''-resultant as defined by Macaulay requires the number of homogeneous polynomials in the system of equations to be  <math>n-1</math>, where  <math>n</math> is the number of indeterminates. In 1981, [[Daniel Lazard]] extended the notion to the case where the number of polynomials may differ from <math>n-1</math>, and the resulting computation can be performed via a specialized [[Gaussian elimination]] procedure followed by symbolic [[determinant]] computation.\n\nLet <math>P_1, \\ldots, P_k</math> be homogeneous polynomials in <math>x_1, \\ldots, x_n,</math> of degrees <math>d_1, \\ldots, d_k,</math> over a field {{math|''k''}}. Without loss of generality, one may suppose that <math>d_1\\ge d_2\\ge \\cdots \\ge d_k.</math> Setting <math>d_i=1</math> for {{math|''i'' > ''k''}}, the Macaulay bound is <math>D=d_1+\\cdots + d_n-n+1.</math>\n\nLet <math>u_1, \\ldots, u_n</math> be new indeterninates and define <math>P_{k+1}=u_1x_1+\\cdots +u_nx_n.</math> In this case, the Macaulay matrix is defined to be the matrix, over the basis of the monomials in <math>x_1, \\ldots, x_n,</math> of the linear map\n:<math>(Q_1, \\ldots, Q_{k+1}) \\mapsto P_1Q_1+\\cdots+P_{k+1}Q_{k+1},</math>\nwhere, for each {{math|''i''}}, <math>Q_i</math> runs over the linear space consisting of zero and the homogeneous polynomials of degree <math>D-d_i</math>.\n\nReducing the Macaulay matrix by a variant of [[Gaussian elimination]], one obtains a square matrix of [[linear form]]s in <math>u_1, \\ldots, u_n.</math> The [[determinant]] of this matrix is the ''U''-resultant. As with the original ''U''-resultant, it is zero if and only if <math>P_1, \\ldots, P_k</math> have infinitely many common projective zeros (that is if the [[projective algebraic set]] defined by <math>P_1, \\ldots, P_k</math> has infinitely many points over an [[algebraic closure]] of {{math|''k''}}). Again as with the original ''U''-resultant, when this ''U''-resultant is not zero, it factorizes into linear factors over any algebraically closed extension of {{math|''k''}}. The coefficients of these linear factors are the [[homogeneous coordinates]] of the common zeros of <math>P_1, \\ldots, P_k,</math> and the multiplicity of a common zero equals the multiplicity of the corresponding linear factor.\n\nThe number of rows of the Macaulay matrix is less than <math>(ed)^n,</math> where {{math|1=''e'' ~ 2.7182}} is the usual [[e (mathematical constant)|mathematical constant]], and {{math|''d''}} is the [[arithmetic mean]] of the degrees of the <math>P_i.</math> It follows that all solutions of a [[system of polynomial equations]] with a finite number of projective zeros can be determined in [[time complexity|time]] <math>d^{O(n)}.</math> Although this bound is large, it is nearly optimal in the following sense: if all input degrees are equal, then the time complexity of the procedure is polynomial in the expected number of solutions ([[Bézout's theorem]]). This computation may be practically viable when  {{math|''n''}}, {{math|''k''}} and {{math|''d''}} are not large.\n\n==See also==\n*[[Elimination theory]]\n*[[Subresultant]]\n\n==Notes==\n\n{{Reflist}}\n\n==References==\n\n*{{Citation | last1=Gelfand | first1=I. M. | last2=Kapranov | first2=M.M. | last3=Zelevinsky | first3=A.V. | title=Discriminants, resultants, and multidimensional determinants | publisher=Boston: Birkhäuser | isbn=978-0-8176-3660-9 | year=1994}}\n*{{citation|first=F. S.|last= Macaulay|authorlink=Francis Sowerby Macaulay|title=Some Formulæ in Elimination|journal=Proc. London Math. Soc. |year=1902|volume=35|pages= 3–27|doi=10.1112/plms/s1-35.1.3 }}\n*{{citation|first=F. S. |last=Macaulay|authorlink=Francis Sowerby Macaulay|title=The Algebraic Theory of Modular Systems|url= https://archive.org/details/algebraictheoryo00macauoft| year=1916|isbn= 978-1275570412| series=The Cornell Library of Historical Mathematical Monographs|publisher=Cambridge University Press}}\n*{{Citation | last1=Salmon | first1=George | title=Lessons introductory to the modern higher algebra | origyear=1859 | url=https://archive.org/details/salmonalgebra00salmrich | publisher=Dublin, Hodges, Figgis, and Co. | edition=4th | isbn=978-0-8284-0150-0 | year=1885}}\n\n==External links==\n* {{MathWorld |urlname=Resultant |title=Resultant}}\n\n{{Polynomials}}\n\n[[Category:Polynomials]]\n[[Category:Determinants]]\n[[Category:Computer algebra]]"
    },
    {
      "title": "Root of unity",
      "url": "https://en.wikipedia.org/wiki/Root_of_unity",
      "text": "{{Use American English|date=January 2019}}{{Short description|Numbers that, raised to a natural power, can equal 1\n}}\n[[Image:One5Root.svg|thumb|right|The 5th roots of unity (blue points) in the [[complex plane]]]]\nIn [[mathematics]], a '''root of unity''', occasionally called a [[Abraham de Moivre|de Moivre]] number, is any [[complex number]] that gives 1 when [[exponentiation|raised]] to some positive integer power {{mvar|n}}. Roots of unity are used in many branches of mathematics, and are especially important in [[number theory]], the theory of [[group character]]s, and the [[discrete Fourier transform]].\n\nRoots of unity can be defined in any [[field (mathematics)|field]]. If the [[characteristic of a field|characteristic]] of the field is zero, they are complex numbers that are also [[algebraic integer]]s. In positive characteristic, they belong to a [[finite field]], and, conversely, every nonzero element of a finite field is a root of unity. Any [[algebraically closed field]] contains exactly {{mvar|n}} {{mvar|n}}th roots of unity, except if {{mvar|n}} is a multiple of the (positive) characteristic of the field.\n\n==General definition==\nAn ''{{mvar|n}}th root of unity'', where {{mvar|n}} is a positive integer (i.e. {{math|1=''n'' = 1, 2, 3, …}}), is a number {{mvar|z}} satisfying the [[equation]]<ref>{{Cite book|author=Hadlock, Charles R.|title=Field Theory and Its Classical Problems, Volume 14|publisher=Cambridge University Press|year=2000|isbn=978-0-88385-032-9|pages=84–86|url=https://books.google.com/books?id=5s1p0CyafnEC&pg=PA84}}</ref><ref>{{cite book|author=Lang, Serge|chapter=Roots of unity|title=Algebra|publisher=Springer|year=2002|isbn=978-0-387-95385-4|pages=276–277|url=https://books.google.com/books?id=Fge-BwqhqIYC&pg=PA276}}</ref>\n\n:<math>z^n = 1. </math>\nUnless otherwise specified, the roots of unity may be taken to be [[complex number]]s (including the number 1, and the number –1 if ''n'' is even, which are complex with a zero imaginary part), and in this case, the {{mvar|n}}th roots of unity are\n:<math>\\exp\\left(\\frac{2k\\pi i}{n}\\right)=\\cos\\frac{2k\\pi}{n}+i\\sin\\frac{2k\\pi}{n},\\quad\\quad k=0,1,\\dots , n-1.</math>\n\nHowever the defining equation of roots of unity is meaningful over any [[field (mathematics)|field]] (and even over any [[ring (mathematics)|ring]])  {{math|''F''}}, and this allows considering roots of unity in {{math|''F''}}. Whichever is the field {{math|''F''}}, the roots of unity in {{math|''F''}} are either complex numbers, if the [[characteristic of a ring|characteristic]] of {{math|''F''}} is 0, or, otherwise, belong to a [[finite field]]. Conversely, every nonzero element in a finite field is a root of unity in that field. See [[Root of unity modulo n|Root of unity modulo ''n'']] and [[Finite field]] for further details.\n\nAn {{mvar|n}}th root of unity is said to be '''{{visible anchor|primitive}}''' if it is not a {{mvar|k}}th root of unity for some smaller {{mvar|k}}, that is if\n\n:<math>z^n=1\\quad \\text{and} \\quad z^k \\ne 1 \\text{ for } k = 1, 2, 3, \\ldots, n-1. </math>\nIf ''n'' is a [[prime number]], all {{math|''n''}}th roots of unity, except 1, are primitive.\n\nIn the above formula in terms of exponential and trigonometric functions, the primitive {{mvar|n}}th roots of unity are those for which {{mvar|k}} and {{mvar|n}} are [[coprime integers]].\n\nSubsequent sections of this article will comply with complex roots of unity. For the case of roots of unity in fields of nonzero characteristic, see {{slink|Finite field|Roots of unity}}. For the case of roots of unity in rings of [[modular arithmetic|modular integers]], see [[Root of unity modulo n]].\n\n==Elementary properties==\nEvery {{math|''n''}}th root of unity {{math|''z''}} is a primitive {{math|''a''}}th root of unity for some {{math|''a'' ≤ ''n''}}, which is the smallest positive integer such that {{math|1=''z''<sup>''a''</sup> = 1}}. \n\nAny integer power of an {{math|''n''}}th root of unity is also an {{math|''n''}}th root of unity, as\n:<math>(z^k)^n = z^{kn} = (z^n)^k = 1^k = 1.</math>\nThis is also true for negative exponents. In particular, the reciprocal of an {{math|''n''}}th root of unity is its [[complex conjugate]], and is also an {{math|''n''}}th root of unity:\n:<math>\\frac{1}{z} = z^{-1} = z^{n-1} = \\bar z.</math>\n\nIf {{math|''z''}} is an {{math|''n''}}th root of unity and {{math|''a'' &equiv; ''b'' (mod ''n'')}} then {{math|1=''z''<sup>''a''</sup> = ''z''<sup>''b''</sup>}}. In fact, by the definition of [[congruence relation|congruence]], {{math|1=''a'' = ''b'' + ''kn''}} for some integer {{math|''k''}}, and\n:<math> z^a = z^{b+kn} = z^b z^{kn} = z^b (z^n)^k = z^b 1^k = z^b.</math>\n\nTherefore, given a power {{math|''z''<sup>''a''</sup>}} of {{math|''z''}}, one has {{math|1=''z''<sup>''a''</sup> = ''z''<sup>''r''</sup>}}, where {{math|0 ≤ ''r'' < ''n''}} is the remainder of the [[Euclidean division]] of {{mvar|a}} by {{mvar|n}}.\n\nLet {{math|''z''}} be a primitive {{math|''n''}}th root of unity. Then the powers {{math|''z''}}, {{math|''z''<sup>2</sup>}}, ..., {{math|''z''<sup>''n''−1</sup>}}, {{math|1=''z''<sup>''n''</sup> = ''z''<sup>0</sup> = 1}} are {{math|''n''}}th root of unity and are all distinct. (If {{math|1=''z''<sup>''a''</sup> = ''z''<sup>''b''</sup>}} where {{math|1 ≤ ''a'' < ''b'' ≤ ''n''}}, then {{math|1=''z''<sup>''b''−''a''</sup> = 1}}, which would imply that {{math|''z''}} would not be primitive.) This implies that {{math|''z''}}, {{math|''z''<sup>2</sup>}}, ..., {{math|''z''<sup>''n''−1</sup>}}, {{math|1=''z''<sup>''n''</sup> = ''z''<sup>0</sup> = 1}} are all of the {{math|''n''}}th roots of unity, since an {{math|''n''}}th-degree polynomial equation has at most {{math|''n''}} distinct solutions.\n\nFrom the preceding, it follows that, if {{math|''z''}} is a primitive {{math|''n''}}th root of unity, then <math>z^a = z^b</math> if and only if  <math>a\\equiv b \\pmod{ n}.</math>\nIf {{math|''z''}} is not primitive then  <math>a\\equiv b \\pmod{ n}</math> implies <math>z^a = z^b,</math> but the converse may be false, as shown by the following example. If {{math|1=''n'' = 4}}, a non-primitive {{math|''n''}}th root of unity is {{math|1= ''z'' = –1}}, and one has <math>z^2 = z^4 = 1</math>, although <math> 2 \\not\\equiv 4 \\pmod{4}.</math>\n\nLet {{math|''z''}} be a primitive {{math|''n''}}th root of unity. A power {{math|1=''w'' = ''z''<sup>''k''</sup>}} of {{mvar|z}} is a primitive {{math|''a''}}th root of unity for \n:<math> a = \\frac{n}{\\gcd(k,n)},</math>\nwhere <math>\\gcd(k,n)</math> is the [[greatest common divisor]] of {{mvar|n}} and {{mvar|k}}. This results from the fact that {{math|''ka''}} is the smallest multiple of {{mvar|k}} that is also a multiple of {{mvar|n}}. In other words, {{math|''ka''}} is the [[least common multiple]] of {{mvar|k}} and {{mvar|n}}. Thus \n:<math>a =\\frac{\\operatorname{lcm}(k,n)}{k}=\\frac{kn}{k\\gcd(k,n)}=\\frac{n}{\\gcd(k,n)}.</math>\n\nThus, if {{math|''k''}} and {{math|''n''}} are [[coprime]], {{math|''z<sup>k</sup>''}} is also a primitive {{math|''n''}}th root of unity, and therefore there are {{math|''φ''(''n'')}} (where {{math|''φ''}} is [[Euler's totient function]]) distinct primitive {{math|''n''}}th roots of unity. (This implies that if {{math|''n''}} is a prime number, all the roots except {{math|+1}} are primitive.)\n\nIn other words, if {{math|R(''n'')}} is the set of all {{math|''n''}}th roots of unity and {{math|P(''n'')}} is the set of primitive ones, {{math|R(''n'')}} is a [[disjoint union]] of the {{math|P(''n'')}}:\n\n:<math>\\operatorname{R}(n) = \\bigcup_{d\\,|\\,n}\\operatorname{P}(d),</math>\n\nwhere the notation means that {{math|''d''}} goes through all the divisors of {{math|''n''}}, including {{math|1}} and {{math|''n''}}.\n\nSince the cardinality of {{math|R(''n'')}} is {{math|''n''}}, and that of {{math|P(''n'')}} is {{math|''φ''(''n'')}}, this  demonstrates the classical formula\n\n:<math>\\sum_{d\\,|\\,n}\\varphi(d) = n.</math>\n\n==Group properties==\n===Group of all roots of unity===\nThe product and the [[multiplicative inverse]] of two roots of unity are also roots of unity. In fact, if {{math|''x<sup>m</sup>'' {{=}} 1}} and {{math|''y<sup>n</sup>'' {{=}} 1}}, then {{math|(''x''<sup>−1</sup>){{sup|''m''}} {{=}} 1}}, and {{math|(''xy''){{sup|''k''}} {{=}} 1}}, where {{math|''k''}} is the [[least common multiple]] of {{math|''m''}} and {{math|''n''}}.\n\nTherefore, the roots of unity form an [[abelian group]] under multiplication. This group is the [[torsion subgroup]] of the [[circle group]].\n\n===Group of {{math|''n''}}th roots of unity===\nThe product and the [[multiplicative inverse]] of two {{math|''n''}}th roots of unity are also {{math|''n''}}th roots of unity. Therefore, the {{math|''n''}}th roots of unity form a [[group (mathematics)|group]] under multiplication.\n\nGiven a primitive {{math|''n''}}th root of unity {{math|''ω''}}, the other {{math|''n''}}th roots are powers of {{math|''ω''}}. This means that the group of the {{math|''n''}}th roots of unity is a [[cyclic group]]. It is worth remarking that the term of ''cyclic group'' originated from the fact that this group is a subgroup of the [[circle group]].\n\n===Galois group of the primitive {{math|''n''}}th roots of unity===\nLet <math>\\mathbb Q(\\omega)</math> be the [[field extension]] of the rational numbers generated over <math>\\mathbb Q</math> by a primitive {{math|''n''}}th root of unity {{math|''ω''}}. As every {{math|''n''}}th root of unity is a power of {{math|''ω''}}, the [[field (mathematics)|field]] <math>\\mathbb Q(\\omega)</math> contains all {{math|''n''}}th roots of unity, and <math>\\mathbb Q(\\omega)</math> is a [[Galois extension]] of <math>\\mathbb Q.</math>\n\nIf {{math|''k''}} is an integer, {{math|''ω<sup>k</sup>''}} is a primitive {{math|''n''}}th root of unity if and only if {{math|''k''}} and {{math|''n''}} are [[coprime]]. In this case, the map\n:<math>\\omega \\mapsto \\omega^k</math> \ninduces an [[field automorphism|automorphism]] of <math>\\mathbb Q(\\omega)</math>, which maps every {{math|''n''}}th root of unity to its {{math|''k''}}th power. Every automorphism of <math>\\mathbb Q(\\omega)</math> is obtained in this way, and these automorphisms form the [[Galois group]] of <math>\\mathbb Q(\\omega)</math> over the field of the rationals.\n\nThe rules of [[exponentiation]] imply that the composition of two such automorphisms is obtained by multiplying the exponents. It follows that the map\n:<math>k\\mapsto \\left(\\omega \\mapsto \\omega^k\\right)</math>\ndefines a [[group isomorphism]] between the [[unit (ring theory)|units]] of the ring of [[integers modulo n|integers modulo {{math|''n''}}]] and the Galois group of <math>\\mathbb Q(\\omega).</math>\n\nThis shows that this Galois group is [[abelian group|abelian]], and implies thus that the primitive roots of unity may be expressed in terms of radicals.\n\n==Trigonometric expression==\n[[Image:3rd roots of unity.svg|thumb|right|The 3rd roots of unity]]\n[[Image:complex x hoch 3.jpg|thumb|right|Plot of {{math|''z''<sup>3</sup> − 1}}, in which a zero is represented by the color black. See [[Domain coloring]] for interpretation.]]\n[[Image:Complex x hoch 5.jpg|thumb|right|Plot of {{math|''z''<sup>5</sup> − 1}}, in which a zero is represented by the color black.]]\n\n[[De Moivre's formula]], which is valid for all real {{mvar|x}} and integers {{mvar|n}}, is\n\n:<math>\\left(\\cos x + i \\sin x\\right)^n = \\cos nx + i \\sin nx.</math>\n\nSetting {{math|1=''x'' = {{sfrac|2π|''n''}}}} gives a primitive {{mvar|n}}th root of unity, one gets\n\n:<math>\\left(\\cos\\frac{2\\pi}{n} + i \\sin\\frac{2\\pi}{n}\\right)^n = \\cos 2\\pi + i \\sin 2\\pi = 1,</math>\n\nbut\n:<math>\\left(\\cos\\frac{2\\pi}{n} + i \\sin\\frac{2\\pi}{n}\\right)^k= \\cos\\frac{2k\\pi}{n} + i \\sin\\frac{2k\\pi}{n} \\neq 1</math>\nfor {{math|1=''k'' = 1, 2, …, ''n'' − 1}}. In other words, \n:<math>\\cos\\frac{2\\pi}{n} + i \\sin\\frac{2\\pi}{n}</math>\nis a primitive {{mvar|n}}th root of unity.\n\nThis formula shows that on the [[complex plane]] the {{mvar|n}}th roots of unity are at the vertices of a  [[regular polygon|regular {{mvar|n}}-sided polygon]] inscribed in the [[unit circle]], with one vertex at 1. (See the plots for {{math|1=''n'' = 3}} and {{math|1=''n'' = 5}} on the right.) This geometric fact accounts for the term \"cyclotomic\" in such phrases as [[cyclotomic field]] and [[cyclotomic polynomial]]; it is from the Greek roots \"[[wikt:κύκλος|cyclo]]\" (circle) plus \"[[wikt:τόμος|tomos]]\" (cut, divide).\n\n[[Euler's formula]]\n\n:<math>e^{i x} = \\cos x + i \\sin x,</math>\n\nwhich is valid for all real {{mvar|x}}, can be used to put the formula for the {{mvar|n}}th roots of unity into the form\n\n:<math>e^{2\\pi i \\frac{k}{n}} \\qquad 0 \\le k < n.</math>\n\nIt follows from the discussion in the previous section that this is a primitive {{mvar|n}}th-root if and only if the fraction {{math|{{sfrac|''k''|''n''}}}} is in lowest terms, i.e. that {{mvar|k}} and {{mvar|n}} are coprime.\n\n==Algebraic expression==\n\nThe {{math|''n''}}th roots of unity are, by definition, the roots of the polynomial {{math|''x<sup>n</sup>'' − 1}}, and are thus [[algebraic number]]s. As this polynomial is not [[irreducible polynomial|irreducible]] (except for {{math|1=''n'' = 1}}), the primitive {{math|''n''}}th roots of unity are roots of an irreducible polynomial of lower degree, called the [[cyclotomic polynomial]], and often denoted {{math|Φ<sub>''n''</sub>}}. The degree of {{math|Φ<sub>''n''</sub>}} is given by [[Euler's totient function]], which counts (among other things) the number of primitive {{math|''n''}}th roots of unity. The roots of {{math|Φ<sub>''n''</sub>}} are exactly the primitive {{math|''n''}}th roots of unity.\n\n[[Galois theory]] can be used to show that cyclotomic polynomials may be conveniently solved in terms of radicals. (The trivial form <math>\\sqrt[n]{1}</math> is not convenient, because it contains non-primitive roots, such as 1, which are not roots of the cyclotomic polynomial, and because it does not give the real and imaginary parts separately.) This means that, for each positive integer {{mvar|n}}, there exists an expression built from integers by root extractions, additions, subtractions, multiplications, and divisions (nothing else), such that the primitive {{mvar|n}}th roots of unity are exactly the set of values that can be obtained by choosing values for the root extractions ({{mvar|k}} possible values for a {{mvar|k}}th root). (For more details see {{slink||Cyclotomic fields}}, below.)\n\nGauss proved that a primitive {{mvar|n}}th root of unity can be expressed using only [[square root]]s, addition, subtraction, multiplication and division if and only if it is possible to [[Compass-and-straightedge construction|construct with compass and straightedge]] the [[regular polygon|regular {{mvar|n}}-gon]]. This is the case [[if and only if]] {{math|''n''}} is either a power of two or the product of a power of two and [[Fermat prime]]s that are all different. \n\nIf {{mvar|z}} is a primitive {{mvar|n}}th root of unity, the same is true for {{math|1/''z''}}, and <math>r=z+\\frac 1z</math> is twice the real part of {{mvar|z}}. In other words, {{math|Φ<sub>''n''</sub>}} is a [[reciprocal polynomial]], the polynomial <math>R_n</math> that has {{mvar|r}} as a root may be deduced from {{math|Φ<sub>''n''</sub>}} by the standard manipulation on reciprocal polynomials, and the primitive {{mvar|n}}th roots of unity may be deduced from the roots of <math>R_n</math> by solving the [[quadratic equation]] <math>z^2-rz+1=0.</math> That is, the real part of the primitive root is <math>\\frac r2,</math> and its imaginary part is <math>\\pm i\\sqrt{1-\\left(\\frac r2\\right)^2}.</math>\n\nThe polynomial <math>R_n</math> is an irreducible polynomial whose all roots are real. Its degree is a power of two, if and only if {{mvar|n}} is a product of a power of two by a product (possibly empty) of distinct Fermat primes, and the regular {{mvar|n}}-gon is constructible with compass and straightedge. Otherwise, it is solvable in radicals, but one are in the [[casus irreducibilis]], that is, every expression of the roots in terms of radicals involves ''nonreal radicals''. \n\n===Explicit expressions in low degrees===\n* For {{math|1=''n'' = 1}}, the cyclotomic polynomial is {{math|Φ<sub>1</sub>(''x'') {{=}} ''x'' − 1}} Therefore, the only primitive first root of unity is 1, which is a non-primitive {{math|''n''}}th root of unity for every ''n'' greater than 1.\n*As {{math|Φ<sub>2</sub>(''x'') {{=}} ''x'' + 1}}, the only primitive second (square) root of unity is –1, which is also a non-primitive {{math|''n''}}th root of unity for every even {{math|''n'' > 2}}. With the preceding case, this completes the list of real roots of unity.\n*As {{math|Φ<sub>3</sub>(''x'') {{=}} ''x''<sup>2</sup> + ''x'' + 1}}, the primitive third (cube) roots of unity, which are the roots of this [[quadratic polynomial]], are \n::<math>\\frac{-1 + i \\sqrt{3}}{2}, \\frac{-1 - i\\sqrt{3}}{2} .</math>\n*As {{math|Φ<sub>4</sub>(''x'') {{=}} ''x''<sup>2</sup> + 1}}, the two primitive fourth roots of unity are {{math|''i''}} and {{math|−''i''}}.\n*As {{math|Φ<sub>5</sub>(''x'') {{=}} ''x''<sup>4</sup> + ''x''<sup>3</sup> + ''x''<sup>2</sup> + ''x'' + 1}}, the four primitive fifth roots of unity are the roots of this [[quartic polynomial]], which may be explicitly solved in terms of radicals, giving the roots\n::<math>\\frac{\\varepsilon\\sqrt 5 - 1}4 \\pm i \\frac{\\sqrt{10 + 2\\varepsilon\\sqrt 5}}{4},</math>\n:where <math>\\varepsilon</math> may take the two values 1 and –1 (the same value in the two occurrences).\n*As {{math|Φ<sub>6</sub>(''x'') {{=}} ''x''<sup>2</sup> − ''x'' + 1}}, there are two primitive sixth roots of unity, which are the negatives (and also the square roots) of the two primitive cube roots:\n::<math>\\frac{1 + i \\sqrt{3}}{2}, \\frac{1 - i \\sqrt{3}}{2}.</math>\n*As 7 is not a Fermat prime, the seventh roots of unity are the first that require [[cube root]]s. There are 6 primitive seventh roots of unity, which are pairwise [[complex conjugate]]. The sum of a root and its conjugate is twice its real part. These three sums are the three real roots of the cubic polynomial <math>r^3+r^2-2r-1,</math> and the primitive seventh roots of unity are\n::<math>\\frac{r}{2}\\pm i\\sqrt{1-\\frac{r^2}{4}},</math>\n:where {{mvar|r}} runs over the roots of the above polynomial. As for every cubic polynomial, these roots may be expressed in terms of square and cube roots. However, as these three roots are all real, this is [[casus irreducibilis]], and any such expression involves  nonreal cube roots.\n*As {{math|Φ<sub>8</sub>(''x'') {{=}} ''x''<sup>4</sup> + 1}}, the four primitive eighth roots of unity are the square roots of the primitive fourth roots, {{math|±''i''}}. They are thus \n::<math> \\pm\\frac{\\sqrt{2}}{2}  \\pm i\\frac{\\sqrt{2}}{2}.</math>\n*See [[heptadecagon]] for the real part of a 17th root of unity.\n\n==Periodicity==\nIf {{mvar|z}} is a primitive {{mvar|n}}th root of unity, then the sequence of powers\n: {{math|… , ''z''<sup>−1</sup>, ''z''<sup>0</sup>, ''z''<sup>1</sup>, …}}\nis {{mvar|n}}-periodic (because {{math|1=''z''<sup>'' j'' + ''n''</sup> = ''z''<sup>'' j''</sup>⋅''z''<sup>'' n''</sup> = ''z''<sup>'' j''</sup>⋅1 = ''z''<sup>'' j''</sup>}} for all values of {{mvar|j}}), and the {{mvar|n}} sequences of powers\n:{{math|''s<sub>k</sub>'': … , ''z''<sup>'' k''⋅(−1)</sup>, ''z''<sup>'' k''⋅0</sup>, ''z''<sup>'' k''⋅1</sup>, …}}\nfor {{math|1=''k'' = 1, … , ''n''}} are all {{mvar|n}}-periodic (because {{math|1=''z''<sup>'' k''⋅(''j'' + ''n'')</sup> = ''z''<sup>'' k''⋅''j''</sup>}}). Furthermore, the set {{math|{''s''<sub>1</sub>, … , ''s<sub>n</sub>''}}} of these sequences is a [[basis (linear algebra)|basis]] of the linear space of all {{mvar|n}}-periodic sequences. This means that ''any'' {{mvar|n}}-periodic sequence of complex numbers\n: {{math|… , ''x''<sub>−1</sub> , ''x''<sub>0</sub> , ''x''<sub>1</sub>, …}}\ncan be expressed as a [[linear combination]] of powers of a primitive {{mvar|n}}th root of unity:\n:<math> x_j = \\sum_k X_k \\cdot z^{k \\cdot j} = X_1 z^{1\\cdot j} + \\cdots + X_n \\cdot z^{n \\cdot j}</math>\nfor some complex numbers {{math|1=''X''<sub>1</sub>, … , ''X''<sub>''n''</sub>}} and every integer {{mvar|j}}.\n\nThis is a form of [[Fourier analysis]]. If {{mvar|j}} is a (discrete) time variable, then {{mvar|k}} is a [[frequency]] and {{math|''X''<sub>''k''</sub>}} is a complex [[amplitude]].\n\nChoosing for the primitive {{mvar|n}}th root of unity\n:<math>z = e^\\frac{2\\pi i}{n} = \\cos\\frac{2\\pi}{n} + i \\sin\\frac{2\\pi}{n}</math>\nallows {{math|''x''<sub>''j''</sub>}} to be expressed as a linear combination of {{math|cos}} and {{math|sin}}:\n:<math>x_j = \\sum_k A_k \\cos \\frac{2\\pi jk}{n} + \\sum_k B_k \\sin \\frac{2\\pi jk}{n}.</math>\nThis is a [[discrete Fourier transform]].\n\n==Summation==\nLet {{math|SR(''n'')}} be the sum of all the {{mvar|n}}th roots of unity, primitive or not. Then\n\n:<math>\\operatorname{SR}(n) =\n\\begin{cases}\n1, & n=1\\\\\n0, & n>1.\n\\end{cases}</math>\n\nThis is an immediate consequence of [[Vieta's formulas]]. In fact, the {{mvar|n}}th roots of unity being the roots of the polynomial {{math|''X''<sup>''n''</sup>-1}}, their sum is the coefficient of degree {{math|''n'' – 1}}, which is either 1 or 0 according whether {{math|1=''n'' = 1}} or {{math|''n'' > 1}}.\n\nAlternatively, for {{math|1=''n'' = 1}} there is nothing to prove. For {{math|1=''n'' > 1}} there exists a root {{math|''z'' ≠ 1}}. Since the set {{math|''S''}} of all the {{mvar|n}}th roots of unity is a group, {{math|1=''z{{space|hair}}S'' =  ''S''}}, so the sum satisfies {{math|1= ''z'' SR(''n'') =  SR(''n'')}}, whence {{math|1=SR(''n'') = 0}}.\n\nLet {{math|SP(''n'')}} be the sum of all the primitive {{mvar|n}}th roots of unity. Then\n\n:<math>\\operatorname{SP}(n) = \\mu(n),</math>\n\nwhere {{math|''μ''(''n'')}} is the [[Möbius function]].\n\nIn the section [[Root of unity#Elementary properties|Elementary properties]], it was shown that if {{math|R(''n'')}} is the set of all {{mvar|n}}th roots of unity and {{math|P(''n'')}} is the set of primitive ones, {{math|R(''n'')}} is a disjoint union of the {{math|P(''n'')}}:\n\n:<math>\\operatorname{R}(n) = \\bigcup_{d\\,|\\,n}\\operatorname{P}(d),</math>\n\nThis implies\n\n:<math>\\operatorname{SR}(n) = \\sum_{d\\,|\\,n}\\operatorname{SP}(d).</math>\n\nApplying the [[Möbius inversion formula]] gives\n\n:<math>\\operatorname{SP}(n) = \\sum_{d\\,|\\,n}\\mu(d)\\operatorname{SR}\\left(\\frac{n}{d}\\right).</math>\n\nIn this formula, if {{math|''d'' < ''n''}}, then {{math|1=SR({{sfrac|''n''|''d''}}) = 0}}, and for {{math|1=''d'' = ''n''}}: {{math|1=SR({{sfrac|''n''|''d''}}) = 1}}.  Therefore, {{math|1=SP(''n'') = ''μ''(''n'')}}.\n\nThis is the special case {{math|''c''<sub>''n''</sub>(1)}} of [[Ramanujan's sum]] {{math|''c''<sub>''n''</sub>(''s'')}}, defined as the sum of the {{mvar|s}}th powers of the primitive {{mvar|n}}th roots of unity:\n\n:<math>c_n(s) = \\sum_{a = 1 \\atop \\gcd(a, n) = 1}^n e^{2 \\pi i \\frac{a}{n} s}.</math>\n\n==Orthogonality==\nFrom the summation formula follows an [[orthogonality]] relationship: for {{math|1=''j'' = 1, … , ''n''}} and {{math|1=''j′'' = 1, … , ''n''}}\n\n:<math>\\sum_{k=1}^{n} \\overline{z^{j\\cdot k}} \\cdot z^{j'\\cdot k} = n \\cdot\\delta_{j,j'}</math>\n\nwhere {{mvar|&delta;}} is the [[Kronecker delta]] and {{mvar|z}} is any primitive {{mvar|n}}th root of unity.\n\nThe [[square matrix|{{math|''n'' × ''n''}} matrix]] {{mvar|U}} whose {{math|(''j'', ''k'')}}th entry is\n\n:<math>U_{j,k}=n^{-\\frac{1}{2}}\\cdot z^{j\\cdot k}</math>\n\ndefines a [[discrete Fourier transform]].  Computing the inverse transformation using [[gaussian elimination]] requires {{math|''[[big-O notation|O]]''(''n''<sup>3</sup>)}} operations.  However, it follows from the orthogonality that {{mvar|U}} is [[unitary matrix|unitary]].  That is,\n\n:<math>\\sum_{k=1}^{n} \\overline{U_{j,k}} \\cdot U_{k,j'} = \\delta_{j,j'},</math>\n\nand thus the inverse of {{mvar|U}} is simply the complex conjugate. (This fact was first noted by [[Carl Friedrich Gauss|Gauss]] when solving the problem of [[trigonometric interpolation]]).  The straightforward application of {{mvar|U}} or its inverse to a given vector requires {{math|''O''(''n''<sup>2</sup>)}} operations. The [[fast Fourier transform]] algorithms reduces the number of operations further to {{math|''O''(''n'' log ''n'')}}.\n\n==Cyclotomic polynomials==<!-- This section is linked from [[Cyclotomic polynomial]] -->\n{{Main|Cyclotomic polynomial}}\n\nThe zeroes of the [[polynomial]]\n:<math>p(z) = z^n - 1</math>\nare precisely the {{mvar|n}}th roots of unity, each with multiplicity 1. The {{mvar|n}}th '''[[cyclotomic polynomial]]''' is defined by the fact that its zeros are precisely the ''primitive'' {{mvar|n}}th roots of unity, each with multiplicity 1.\n: <math>\\Phi_n(z) = \\prod_{k=1}^{\\varphi(n)}(z-z_k) </math>\nwhere {{math|''z''<sub>1</sub>, ''z''<sub>2</sub>, ''z''<sub>3</sub>, … ,''z''<sub>φ(''n'')</sub>}} are the primitive {{mvar|n}}th roots of unity, and {{math|φ(''n'')}} is [[Euler's totient function]]. The polynomial {{math|Φ<sub>''n''</sub>(''z'')}} has integer coefficients and is an [[irreducible polynomial]] over the [[rational number]]s (i.e., it cannot be written as the product of two positive-degree polynomials with rational coefficients). The case of prime {{mvar|n}}, which is easier than the general assertion, follows by applying [[Eisenstein's criterion]] to the polynomial\n\n:<math>\\frac{(z + 1)^n - 1}{(z + 1) - 1},</math>\n\nand expanding via the binomial theorem.\n\nEvery {{mvar|n}}th root of unity is a primitive {{mvar|d}}th root of unity for exactly one positive [[divisor]] {{mvar|d}} of {{mvar|n}}. This implies that\n\n:<math>z^n - 1 = \\prod_{d\\,\\mid\\,n} \\Phi_d(z).</math>\n\nThis formula represents the factorization of the polynomial {{math|''z''<sup>''n''</sup> − 1}} into irreducible factors.\n:{{math|1=''z''<sup>1</sup> − 1 = ''z'' − 1}}\n:{{math|1=''z''<sup>2</sup> − 1 = (''z'' − 1)⋅(''z'' + 1)}}\n:{{math|1=''z''<sup>3</sup> − 1 = (''z'' − 1)⋅(''z''<sup>2</sup> + ''z'' + 1)}}\n:{{math|1=''z''<sup>4</sup> − 1 = (''z'' − 1)⋅(''z'' + 1)⋅(''z''<sup>2</sup> + 1)}}\n:{{math|1=''z''<sup>5</sup> − 1 = (''z'' − 1)⋅(''z''<sup>4</sup> + ''z''<sup>3</sup> + ''z''<sup>2</sup>  + ''z'' + 1)}}\n:{{math|1=''z''<sup>6</sup> − 1 = (''z'' − 1)⋅(''z'' + 1)⋅(''z''<sup>2</sup> + ''z'' + 1)⋅(''z''<sup>2</sup> − ''z'' + 1)}}\n:{{math|1=''z''<sup>7</sup> − 1 = (''z'' − 1)⋅(''z''<sup>6</sup> + ''z''<sup>5</sup> + ''z''<sup>4</sup> + ''z''<sup>3</sup> + ''z''<sup>2</sup>  +''z'' + 1)}}\n:{{math|1=''z''<sup>8</sup> − 1 = (''z'' − 1)⋅(''z'' + 1)⋅(''z''<sup>2</sup> + 1)⋅(''z''<sup>4</sup> + 1)}}\n\nApplying [[Möbius inversion]] to the formula gives\n\n:<math>\\Phi_n(z) = \\prod_{d\\,\\mid n}\\left(z^\\frac{n}{d} - 1\\right)^{\\mu(d)} = \\prod_{d\\,\\mid n}\\left(z^d - 1\\right)^{\\mu\\left(\\frac{n}{d}\\right)},</math>\n\nwhere {{math|''μ''}} is the [[Möbius function]].\n\nSo the first few cyclotomic polynomials are\n\n:{{math|1=Φ<sub>1</sub>(''z'') = ''z'' − 1}}\n:{{math|1=Φ<sub>2</sub>(''z'') = (''z''<sup>2</sup> − 1)⋅(''z'' − 1)<sup>−1</sup> = ''z'' + 1}}\n:{{math|1=Φ<sub>3</sub>(''z'') = (''z''<sup>3</sup> − 1)⋅(''z'' − 1)<sup>−1</sup> = ''z''<sup>2</sup> + ''z'' + 1}}\n:{{math|1=Φ<sub>4</sub>(''z'') = (''z''<sup>4</sup> − 1)⋅(''z''<sup>2</sup> − 1)<sup>−1</sup> = ''z''<sup>2</sup> + 1}}\n:{{math|1=Φ<sub>5</sub>(''z'') = (''z''<sup>5</sup> − 1)⋅(''z'' − 1)<sup>−1</sup> = ''z''<sup>4</sup> + ''z''<sup>3</sup> + ''z''<sup>2</sup> + ''z'' + 1}}\n:{{math|1=Φ<sub>6</sub>(''z'') = (''z''<sup>6</sup> − 1)⋅(''z''<sup>3</sup> − 1)<sup>−1</sup>⋅(''z''<sup>2</sup> − 1)<sup>−1</sup>⋅(''z'' − 1)  = ''z''<sup>2</sup> − ''z'' + 1}}\n:{{math|1=Φ<sub>7</sub>(''z'') = (''z''<sup>7</sup> − 1)⋅(''z'' − 1)<sup>−1</sup>  = ''z''<sup>6</sup> + ''z''<sup>5</sup> + ''z''<sup>4</sup> + ''z''<sup>3</sup> + ''z''<sup>2</sup>  +''z'' + 1}}\n:{{math|1=Φ<sub>8</sub>(''z'') = (''z''<sup>8</sup> − 1)⋅(''z''<sup>4</sup> − 1)<sup>−1</sup> = ''z''<sup>4</sup> + 1}}\n\nIf {{mvar|p}} is a [[prime number]], then all the {{mvar|p}}th roots of unity except 1 are primitive {{mvar|p}}th roots, and we have\n: <math>\\Phi_p(z) = \\frac{z^p - 1}{z - 1} = \\sum_{k = 0}^{p - 1} z^k. </math>\nSubstituting any positive integer &ge; 2 for {{mvar|z}}, this sum becomes a [[radix|base {{mvar|z}}]] [[repunit]]. Thus a necessary (but not sufficient) condition for a repunit to be prime is that its length be prime.\n\nNote that, contrary to first appearances, ''not'' all coefficients of all cyclotomic polynomials are 0, 1, or −1.  The first exception is {{math|Φ<sub>{{num|105}}</sub>}}. It is not a surprise it takes this long to get an example, because the behavior of the coefficients depends not so much on {{mvar|n}} as on how many odd prime factors appear in {{mvar|n}}.  More precisely, it can be shown that if {{mvar|n}} has 1 or 2 odd prime factors (e.g., {{math|1=''n'' = 150}}) then the {{mvar|n}}th cyclotomic  polynomial only has coefficients 0, 1 or −1.  Thus the first conceivable {{mvar|n}} for which there could be a coefficient besides 0, 1, or −1 is a product of the three smallest odd primes, and that is {{math|1=3⋅5⋅7 = 105}}.  This by itself doesn't prove the 105th polynomial has another coefficient, but does show it is the first one which even has a chance of working (and then a computation of the coefficients shows it does).  A theorem of Schur says that there are cyclotomic polynomials with coefficients arbitrarily large in absolute value. In particular, if {{math|1=''n'' = ''p''<sub>1</sub>⋅''p''<sub>2</sub>⋅ ⋯ ⋅''p''<sub>''t''</sub>}}, where {{math|1=''p''<sub>1</sub> < ''p''<sub>2</sub> < ⋯ < ''p''<sub>''t''</sub>}} are odd primes, {{math|1=''p''<sub>1</sub> + ''p''<sub>2</sub> > ''p''<sub>''t''</sub>}}, and ''t'' is odd, then {{math|1 − ''t''}} occurs as a coefficient in the {{mvar|n}}th cyclotomic polynomial.<ref>Emma Lehmer, ''On the magnitude of the coefficients of the cyclotomic polynomial'',  Bulletin of the American Mathematical Society 42 (1936), no.&nbsp;6, pp.&nbsp;389–392.</ref>\n\nMany restrictions are known about the values that cyclotomic polynomials can assume at integer values. For example, if {{mvar|p}} is prime, then {{math|''d''&thinsp;∣&thinsp;Φ<sub>''p''</sub>(''d'')}} if and only {{math|''d'' ≡ 1 (mod ''p'')}}.\n\nCyclotomic polynomials are solvable in [[Nth root|radical]]s, as roots of unity are themselves radicals. Moreover, there exist more informative radical expressions for {{mvar|n}}th roots of unity with the additional property<ref>{{Cite journal|last=Landau|first=Susan|last2=Miller|first2=Gary L.|title=Solvability by radicals is in polynomial time|journal=Journal of Computer and System Sciences|volume=30|issue=2|pages=179–208|year=1985|doi=10.1016/0022-0000(85)90013-3|postscript=<!--None-->}}</ref> that every value of the expression obtained by choosing values of the radicals (for example, signs of square roots) is a primitive {{mvar|n}}th root of unity. This was already shown by [[Carl Friedrich Gauss|Gauss]] in 1797.<ref>{{Cite book|last=Gauss|first=Carl F.|authorlink=Carl Friedrich Gauss|title=Disquisitiones Arithmeticae|pages=§§359–360|publisher=Yale University Press|year=1965|isbn=0-300-09473-6}}</ref> Efficient [[algorithm]]s exist for calculating such expressions.<ref>{{cite web|last=Weber|first=Andreas|last2=Keckeisen|first2=Michael|title=Solving Cyclotomic Polynomials by Radical Expressions|url=http://cg.cs.uni-bonn.de/personal-pages/weber/publications/pdf/WeberA/WeberKeckeisen99a.pdf|accessdate=2007-06-22|postscript=<!--None-->}}</ref>\n\n==Cyclic groups==\nThe {{mvar|n}}th roots of unity form under multiplication a [[cyclic group]] of [[order (group theory)|order]] {{mvar|n}}, and in fact these groups comprise all of the finite subgroups of the [[multiplicative group]] of the complex number field.  A [[Generating set of a group|generator]] for this cyclic group is a primitive {{mvar|n}}th root of unity.\n\nThe {{mvar|n}}th roots of unity form an irreducible [[group representation|representation]] of any cyclic group of order {{mvar|n}}. The orthogonality relationship also follows from group-theoretic principles as described in [[character group]].\n\nThe roots of unity appear as entries of the [[eigenvector]]s of any [[circulant matrix]], i.e. matrices that are invariant under cyclic shifts, a fact that also follows from group representation theory as a variant of [[Bloch wave|Bloch's theorem]].<ref>T. Inui, Y. Tanabe, and Y. Onodera, ''Group Theory and Its Applications in Physics'' (Springer, 1996).</ref>  In particular, if a circulant  [[Hermitian matrix]] is considered (for example, a discretized one-dimensional [[Laplacian]] with periodic boundaries<ref>[[Gilbert Strang]], \"[http://epubs.siam.org/sam-bin/dbq/article/33674 The discrete cosine transform],\" ''SIAM Review'' '''41''' (1), 135–147 (1999).</ref>), the orthogonality property immediately follows from the usual orthogonality of eigenvectors of Hermitian matrices.\n\n==Cyclotomic fields==\n{{Main|Cyclotomic field}}\nBy adjoining a primitive {{mvar|n}}th root of unity to '''Q''', one obtains the {{mvar|n}}th [[cyclotomic field]] {{math|'''Q'''(exp(2π''i''/''n''))}}. This [[field (mathematics)|field]] contains all {{mvar|n}}th roots of unity and is the [[splitting field]] of the {{mvar|n}}th cyclotomic polynomial over '''Q'''. The [[field extension]] {{math|'''Q'''(exp(2π''i''/''n''))/'''Q'''}} has degree φ(''n'') and its [[Galois group]] is [[Natural transformation|naturally]] [[group isomorphism|isomorphic]] to the multiplicative group of units of the ring {{math|'''Z'''/''n'''''Z'''}}.\n\nAs the Galois group of {{math|'''Q'''(exp(2π''i''/''n''))/'''Q'''}} is abelian, this is an [[abelian extension]]. Every subfield of a cyclotomic field is an abelian extension of the rationals. It follows that every ''n''th root of unity may be expressed in term of ''k''-roots, with various ''k'' not exceeding ''φ(n)''. In these cases [[Galois theory]] can be written out explicitly in terms of [[Gaussian period]]s: this theory from the ''[[Disquisitiones Arithmeticae]]'' of [[Carl Friedrich Gauss|Gauss]] was published many years before Galois.<ref>The ''Disquisitiones'' was published in 1801, [[Évariste Galois|Galois]] was born in 1811, died in 1832, but wasn't published until 1846.</ref>\n\nConversely, ''every'' abelian extension of the rationals is such a subfield of a cyclotomic field – this is the content of a theorem of [[Leopold Kronecker|Kronecker]], usually called the ''[[Kronecker–Weber theorem]]'' on the grounds that Weber completed the proof.\n\n==Relation to quadratic integers==\n[[Image:Roots of unity, golden ratio.svg|thumb|right|In the [[complex plane]], the red points are the fifth roots of unity, and the blue points are the sums of a fifth root of unity and its complex conjugate.]]\n[[Image:Star polygon 8-2.svg|thumb|right|160px|In the [[complex plane]], the corners of the two squares are the eighth roots of unity]]<!-- a better image needed -->\nFor {{math|1=''n'' = 1, 2}}, both roots of unity {{num|1}} and {{num|−1}} belong to [[integer|{{math|'''Z'''}}]]. \n\nFor three values of {{mvar|n}}, the roots of unity are [[quadratic integer]]s:\n* For {{math|1=''n'' = 3, 6}} they are [[Eisenstein integer]]s ({{math|1=[[discriminant|''D'']] = −3}}).\n* For {{math|1=''n'' = 4}} they are [[Gaussian integer]]s ({{math|1=''D'' = −1}}): see [[imaginary unit]].\n\nFor four other values of {{mvar|n}}, the primitive roots of unity are not quadratic integers, but the sum of any root of unity with its [[complex conjugate]] (also an {{mvar|n}}th root of unity) is a quadratic integer.\n\nFor {{math|1=''n'' = 5, 10}}, none of the non-real roots of unity (which satisfy a [[quartic equation]]) is a quadratic integer, but the sum {{math|1=''z'' + ''{{overline|z}}'' = 2 [[real part|Re]]''z''}} of each root with its complex conjugate (also a 5th root of unity) is an element of the [[ring (algebra)|ring]] [[quadratic integer|'''Z'''[{{sfrac|1 + {{sqrt|5}}|2}}]]] ({{math|1=''D'' = 5}}). For two pairs of non-real 5th roots of unity these sums are [[multiplicative inverse|inverse]] [[golden ratio]] and [[additive inverse|minus]] golden ratio.\n\nFor {{math|1=''n'' = 8}}, for any root of unity  {{math|''z'' + ''{{overline|z}}''}} equals to either 0, ±2, or ±[[square root of 2|{{sqrt|2}}]] ({{math|1=''D'' = 2}}).\n\nFor {{math|1=''n'' = 12}}, for any root of unity,  {{math|''z'' + ''{{overline|z}}''}} equals to either 0, ±1, ±2 or ±[[square root of 3|{{sqrt|3}}]] ({{math|1=''D'' = 3}}).\n\n==See also==\n* [[Argand system]]\n* [[Circle group]], the unit complex numbers\n* [[Group scheme of roots of unity]]\n* [[Dirichlet character]]\n* [[Ramanujan's sum]]\n* [[Kummer ring]]\n* [[Witt vector]]\n* [[Teichmüller character]]\n\n==Notes==\n{{refimprove|date=April 2012}}\n{{Reflist}}\n\n==References==\n* {{Lang Algebra|edition=3r}}\n* {{cite web|first=James S.|last=Milne|title=Algebraic Number Theory|work=Course Notes|url=http://www.jmilne.org/math|year=1998}}\n* {{cite web|first=James S.|last=Milne|title=Class Field Theory|work=Course Notes|url=http://www.jmilne.org/math|year=1997}}\n* {{Neukirch ANT}}\n* {{Cite book|first=Jürgen|last=Neukirch|title=Class Field Theory|publisher=Springer-Verlag|location=Berlin|year=1986|isbn=3-540-15251-2}}\n* {{Cite book|first=Lawrence C.|last=Washington|title=Cyclotomic fields|publisher=Springer-Verlag|location=New York|year=1997|edition=2nd|isbn=0-387-94762-0}}\n* {{Cite book|author=[[John Derbyshire|Derbyshire, John]]|title=Unknown Quantity|chapter=Roots of Unity|year=2006|publisher=[[Joseph Henry Press]]|location=Washington, D.C.|isbn=0-309-09657-X|url=http://www.JohnDerbyshire.com/Books/Unknown/page.html}}\n\n{{Algebraic numbers}}\n{{Use dmy dates|date=September 2010}}\n\n{{DEFAULTSORT:Root Of Unity}}\n[[Category:Algebraic numbers]]\n[[Category:Cyclotomic fields]]\n[[Category:Polynomials]]\n[[Category:1 (number)]]\n[[Category:Complex numbers]]"
    },
    {
      "title": "Routh–Hurwitz stability criterion",
      "url": "https://en.wikipedia.org/wiki/Routh%E2%80%93Hurwitz_stability_criterion",
      "text": "{{more citations needed|date=April 2009}}\n\nIn [[control theory|control system theory]], the '''Routh–Hurwitz stability criterion''' is a mathematical test that is a necessary and sufficient condition  for the [[stable polynomial|stability]] of a [[linear function|linear]] [[time invariant]] (LTI) [[control system]]. The Routh test is an efficient recursive algorithm that English mathematician [[Edward John Routh]] proposed in 1876 to determine whether all the [[root of a function|roots]] of the [[characteristic polynomial]] of a [[linear system]] have negative real parts.<ref>{{cite book |last=Routh |first=E. J. |year=1877 |title=A Treatise on the Stability of a Given State of Motion: Particularly Steady Motion |publisher=Macmillan |isbn= }}</ref> German mathematician [[Adolf Hurwitz]] independently proposed in 1895 to arrange the coefficients of the polynomial into a square matrix, called the Hurwitz matrix, and showed that the polynomial is stable if and only if the sequence of determinants of its principal submatrices are all positive.<ref>{{cite journal |last=Hurwitz |first=A. |year=1895 |title=Ueber die Bedingungen, unter welchen eine Gleichung nur Wurzeln mit negativen reellen Theilen besitzt |journal=[[Mathematische Annalen|Math. Ann.]] |volume=46 |issue=2 |pages=273–284 |doi=10.1007/BF01446812 }} (English translation “On the conditions under which an equation has only roots with negative real parts” by H. G. Bergmann in ''Selected Papers on Mathematical Trends in Control Theory'' R. Bellman and R. Kalaba Eds. New York: Dover, 1964 pp. 70–82.)</ref> The two procedures are equivalent, with the Routh test providing a more efficient way to compute the Hurwitz determinants than computing them directly.  A polynomial satisfying the Routh–Hurwitz criterion is called a [[Hurwitz polynomial]].\n\nThe importance of the criterion is that the roots '''''p''''' of the characteristic equation of a [[linear system]] with negative real parts represent solutions '''''e<sup>pt</sup>''''' of the system that are stable ([[BIBO stability|bounded]]).  Thus the criterion provides a way to determine if the [[equations of motion]] of a [[linear system]] have only stable solutions, without solving the system directly.  For discrete systems, the corresponding stability test can be handled by the Schur–Cohn criterion, the [[Jury stability criterion|Jury test]] and the [[Bistritz stability criterion|Bistritz test]].  With the advent of computers, the criterion has become less widely used, as an alternative is to solve the polynomial numerically, obtaining approximations to the roots directly.\n\nThe Routh test can [[Derivation of the Routh array|be derived]] through the use of the [[euclid's algorithm|Euclidean algorithm]] and [[Sturm's theorem]] in evaluating [[cauchy index|Cauchy indices]]. Hurwitz derived his conditions differently.<ref name=\"Gopal\">{{cite book\n  | last = Gopal\n  | first = M.\n  | title = Control Systems: Principles and Design, 2nd Ed.\n  | publisher = Tata McGraw-Hill Education\n  | date = 2002\n  | location = \n  | pages = 14\n  | url = https://books.google.com/books?id=FZak6CkrVLQC&pg=PA14&dq=%22flyball+governor\n  | doi = \n  | id = \n  | isbn = 0070482896}}</ref>\n\n==Using Euclid's algorithm==\n\nThe criterion is related to [[Routh–Hurwitz theorem]].  Indeed, from the statement of that theorem, we have <math>p-q=w(+\\infty)-w(-\\infty)</math> where:\n* ''p'' is the number of roots of the polynomial ''ƒ''(''z'') with negative real Part;\n* ''q'' is the number of roots of the polynomial ''ƒ''(''z'') with positive real Part (let us remind ourselves that ''ƒ'' is supposed to have no roots lying on the imaginary line);\n* ''w''(''x'') is the number of variations of the [[Sturm's theorem#Generalized Sturm chains|generalized Sturm chain]] obtained from <math>P_0(y)</math> and <math>P_1(y)</math> (by successive [[Euclid's algorithm|Euclidean divisions]]) where <math>f(iy)=P_0(y)+iP_1(y)</math> for a real ''y''.\nBy the [[fundamental theorem of algebra]], each polynomial of degree ''n'' must have ''n'' roots in the complex plane (i.e., for an ''ƒ'' with no roots on the imaginary line, ''p''&nbsp;+&nbsp;''q''&nbsp;=&nbsp;''n'').  Thus, we have the condition that ''ƒ'' is a (Hurwitz) [[stable polynomial]] if and only if ''p''&nbsp;−&nbsp;''q''&nbsp;=&nbsp;''n'' (the [[Routh-Hurwitz stability criterion#Appendix A|proof]] is given below).  Using the Routh–Hurwitz theorem, we can replace the condition on ''p'' and ''q'' by a condition on the generalized Sturm chain, which will give in turn a condition on the coefficients of&nbsp;''ƒ''.\n\n==Using matrices==\n\nLet ''f''(''z'') be a complex polynomial.  The process is as follows:\n# Compute the polynomials <math>P_0(y)</math> and <math>P_1(y)</math> such that <math>f(iy)=P_0(y)+iP_1(y)</math> where ''y'' is a real number.\n# Compute the [[Sylvester matrix]] associated to  <math>P_0(y)</math> and  <math>P_1(y)</math>.\n# Rearrange each row in such a way that an odd row and the following one have the same number of leading zeros.\n# Compute each [[minor (linear algebra)|principal minor]] of that matrix.\n# If at least one of the minors is negative (or zero), then the polynomial ''f'' is not stable.\n\n===Example===\n\n* Let <math>f(z)=az^2+bz+c</math> (for the sake of simplicity we take real coefficients) where <math> c\\neq 0</math> (to avoid a root in zero so that we can use the Routh–Hurwitz theorem).  First, we have to calculate the real polynomials <math>P_0(y)</math> and <math>P_1(y)</math>:\n:: <math>f(iy)=-ay^2+iby+c=P_0(y)+iP_1(y)=-ay^2+c+i(by).</math>\n: Next, we divide those polynomials to obtain the generalized Sturm chain:\n:* <math>P_0(y)=((-a/b)y)P_1(y)+c,</math> yields <math> P_2(y)=-c,</math>\n:* <math>P_1(y)=((-b/c)y)P_2(y),</math> yields <math>P_3(y)=0</math> and the [[Euclid's algorithm|Euclidean division]] stops.\nNotice that we had to suppose ''b'' different from zero in the first division.  The generalized Sturm chain is in this case <math>(P_0(y),P_1(y),P_2(y))=(c-ay^2,by,-c)</math>.  Putting <math>y=+\\infty</math>, the sign of <math>c-ay^2</math> is the opposite sign of ''a'' and the sign of ''by'' is the sign of ''b''.  When we put <math>y=-\\infty</math>, the sign of the first element of the chain is again the opposite sign of ''a'' and the sign of ''by'' is the opposite sign of ''b''.  Finally, -''c'' has always the opposite sign of ''c''.\n\nSuppose now that ''f'' is Hurwitz-stable.  This means that <math>w(+\\infty)-w(-\\infty)=2</math> (the degree of ''f'').  By the properties of the function ''w'', this is the same as <math>w(+\\infty)=2</math> and <math>w(-\\infty)=0</math>.  Thus, ''a'', ''b'' and ''c'' must have the same sign.   We have thus found the [[stable polynomial#Properties|necessary condition of stability]] for polynomials of degree 2.\n\n===Routh–Hurwitz criterion for second and third order polynomials===\n\n* The second-degree polynomial, <math> P(s) = s^2 + a_1s + a_0  </math>has both roots in the open left half plane (and the system with characteristic equation <math> P(s) =0</math> is stable) if and only if both coefficients satisfy <math> a_i > 0 </math>.\n* The third-order polynomial <math> P(s) = s^3 + a_2s^2 + a_1s + a_0 </math>has all roots in the open left half plane if and only if <math>a_2</math>, <math>a_0</math> are positive and <math> a_2a_1 > a_0 .</math>\n* In general the Routh stability criterion states a polynomial has all roots in the open left half plane if and only if all first-column elements of the Routh array have the same sign.\n\n===Higher-order example===\n\nA tabular method can be used to determine the stability when the roots of a higher order characteristic polynomial are difficult to obtain. For an ''n''th-degree polynomial\n* <math>D(s)=a_ns^n+a_{n-1}s^{n-1}+\\cdots+a_1s+a_0</math>\nthe table has ''n''&nbsp;+&nbsp;1 rows and the following structure:\n{| class=\"wikitable\" width=\"200px\"\n | <math>a_n</math> || <math>a_{n-2}</math> || <math>a_{n-4}</math> || <math>\\dots</math>\n |-\n | <math>a_{n-1}</math> || <math>a_{n-3}</math> || <math>a_{n-5}</math> || <math>\\dots</math>\n |-\n | <math>b_1</math> || <math>b_2</math> || <math>b_3</math> || <math>\\dots</math>\n |-\n | <math>c_1</math> || <math>c_2</math> || <math>c_3</math> || <math>\\dots</math>\n |-\n | <math>\\vdots</math> || <math>\\vdots</math> || <math>\\vdots</math> || <math>\\ddots</math>\n |}\nwhere the elements <math>b_i</math> and <math>c_i</math> can be computed as follows:\n* <math>b_i=\\frac{a_{n-1}\\times{a_{n-2i}}-a_n\\times{a_{n-(2i+1)}}}{a_{n-1}}.</math>\n* <math>c_i=\\frac{b_1\\times{a_{n-(2i+1)}}-a_{n-1}\\times{b_{i+1}}}{b_1}.</math>\nWhen completed, the number of sign changes in the first column will be the number of non-negative roots.\n{| class=\"wikitable\"\n |-\n | width=\"40px\" |0.75\n | width=\"40px\" |1.5\n | width=\"40px\" |0\n | width=\"40px\" |0\n |-\n |<nowiki>-3</nowiki>||6||0||0\n |-\n |3||0||0||0\n |-\n |6||0||0||0\n|}\nIn the first column, there are two sign changes (0.75&nbsp;→&nbsp;−3, and −3&nbsp;→&nbsp;3), thus there are two non-negative roots where the system is unstable.\n\nThe characteristic equation of a servo system is given by<ref name=\":0\">{{Cite book|title=CONTROL SYSTEMS|last=KUMAR|first=Anand|publisher=PHI Learning|year=2007|isbn=9788120331976|location=|pages=}}</ref> :\n\n* <math>b_0s^4+b_1s^3+b_2s^2+b_3s+b_4=0</math>\n\n{| class=\"wikitable\"\n|+\n!<math>b_0</math>\n!<math>b_2</math>\n!<math>b_4</math>\n!0\n|-\n|<math>b_1</math>\n|<math>b_3</math>\n|0\n|0\n|-\n|<math>b_1b_2-b_0b_3\\over b_1</math>\n|<math>b_1b_4-b_0\\times0\\over b_1\n</math>=<math>b_4</math>\n|0\n|0\n|-\n|<math>(b_1b_2-b_0b_3)b_3 -b_1^2b_4\\over b_1b_2-b_0b_3</math>\n|0\n|0\n|0\n|-\n|<math>b_4</math>\n|0\n|0\n|0\n|}\nfor stability, all the elements in the first column of the Routh array must be positive. So the conditions that must be satisfied for stability of the given system as follows<ref name=\":0\" /> :\n\n<math>b_1>0 , b_1b_2-b_0b_3>0 , (b_1b_2-b_0b_3)b_3-b_1^2b_4 >0 ,b_4>0\n</math><ref name=\":0\" />\n\nWe see that if\n\n<math>(b_1b_2-b_0b_3)b_3-b_1^2b_4\\geq0</math>\n\nthen \n\n<math>b_1b_2-b_0b_3>0 \n</math>\n\nIs satisfied.\n\n* <math>s^4+6s^3+11s^2+6s+200=0\n</math><ref name=\":1\">{{Cite book|title=Control Systems Engineering|last=Nise|first=Norman|publisher=Wiley|year=2015|isbn=9781118800829|location=|pages=}}</ref>\n\nWe have the following table :\n{| class=\"wikitable\"\n|1             \n|11         \n|200   \n|0\n|-\n|<s>6</s> 1\n|<s>6</s> 1\n|0\n|0    \n|-\n|<s>10</s>    1\n|<s>200</s>    20\n|0\n|0    \n|-\n| -19\n|0\n|0\n|0        \n|-\n|20\n|0\n|0\n|0\n|}\nthere are two sign changes. The system is unstable, since it has two right-half-plane poles and two left-half-plane poles. The system cannot have jω poles since a row of zeros did not appear in the Routh table.<ref name=\":1\" />\n\nSometimes the presence of poles on the imaginary axis creates a situation of marginal stability. In that case the coefficients of the \"Routh array\" in a whole row become zero and thus further solution of the polynomial for finding changes in sign is not possible. Then another approach comes into play. The row of polynomial which is just above the row containing the zeroes is called the \"auxiliary polynomial\".\n\n* <math>s^6+2s^5+8s^4+12s^3+20s^2+16s+16=0.\\,</math>\nWe have the following table:\n{| class=\"wikitable\"\n |-\n | width=\"40px\"|1|| width=\"40px\"|8|| width=\"40px\"|20|| width=\"40px\"|16\n |-\n | 2    || 12  || 16 || 0\n |-\n | 2    || 12  || 16 || 0\n |-\n | 0    || 0   ||  0 || 0\n |}\nIn such a case the auxiliary polynomial is <math>A(s)=2s^4+12s^2+16.\\,</math> which is again equal to zero. The next step is to differentiate the above equation which yields the following polynomial. <math>B(s)=8s^3+24s^1.\\,</math>. The coefficients of the row containing zero now become\n\"8\" and \"24\". The process of Routh array is proceeded using these values which yield two points on the imaginary axis. These two points on the imaginary axis are the prime cause of marginal stability.<ref>{{cite book|last=Saeed|first=Syed Hasan|title=Automatic Control Systems|year=2008|publisher=Katson Publishers|location=Delhi|isbn=978-81-906919-2-5|pages=206, 207}}</ref>\n\n==See also==\n{{Div col|colwidth=25em}}\n* [[Control engineering]]\n* [[Derivation of the Routh array]]\n* [[Nyquist stability criterion]]\n* [[Routh–Hurwitz theorem]]\n* [[Root locus]]\n* [[Transfer function]]\n* [[Jury stability criterion]]\n* [[Bistritz stability criterion]]\n* [[Kharitonov's theorem]]\n* [[Liénard–Chipart criterion]]\n{{Div col end}}\n\n==References==\n\n{{reflist}}\n*[[Felix Gantmacher]] (J.L. Brenner translator) (1959) ''Applications of the Theory of Matrices'', pp 177–80, New York: Interscience.\n* {{cite journal\n |author      = Pippard, A. B.\n |author2     = Dicke, R. H.\n |year        = 1986\n |title       = Response and Stability, An Introduction to the Physical Theory\n |journal     = [[American Journal of Physics]]\n |volume      = 54\n |issue       = 11\n |pages       = 1052\n |url         = http://link.aip.org/link/?AJPIAS/54/1052/1\n |accessdate  = 2008-05-07\n |doi         = 10.1119/1.14826\n |bibcode     = 1986AmJPh..54.1052P\n |deadurl     = yes\n |archiveurl  = http://arquivo.pt/wayback/20160514133902/http://link.aip.org/link/?AJPIAS/54/1052/1\n |archivedate = 2016-05-14\n |df          = \n}}\n* {{cite book\n | author = [[Richard C. Dorf]], Robert H. Bishop\n | year = 2001\n | title =Modern Control Systems |edition=9th\n | publisher = Prentice Hall\n | isbn = 0-13-030660-6\n}}\n* {{cite book | last1=Rahman | first1=Q. I. | last2=Schmeisser | first2=G. | title=Analytic theory of polynomials | series=London Mathematical Society Monographs. New Series | volume=26 | location=Oxford | publisher=[[Oxford University Press]] | year=2002 | isbn=0-19-853493-0 | zbl=1072.30006 }}\n* {{cite journal\n  |author = Weisstein, Eric W\n  |title = Routh-Hurwitz Theorem.\n  |url = http://mathworld.wolfram.com/Routh-HurwitzTheorem.html\n  |journal= MathWorld--A Wolfram Web Resource\n}}\n\n==External links==\n* [http://www.mathworks.com/matlabcentral/fileexchange/17483-routh-hurwitz-stability-criterion A MATLAB script implementing the Routh-Hurwitz test]\n* [http://crclayton.com/projects/routhhurwitz/index.html Online implementation of the Routh-Hurwitz-Criterion]\n\n{{DEFAULTSORT:Routh-Hurwitz stability criterion}}\n[[Category:Stability theory]]\n[[Category:Electronic feedback]]\n[[Category:Electronic amplifiers]]\n[[Category:Signal processing]]\n[[Category:Polynomials]]"
    },
    {
      "title": "Routh–Hurwitz theorem",
      "url": "https://en.wikipedia.org/wiki/Routh%E2%80%93Hurwitz_theorem",
      "text": "{{no footnotes|date=March 2012}}\nIn [[mathematics]], the '''Routh–Hurwitz theorem''' gives a test to determine whether all [[root of a function|roots]] of a given [[polynomial]] lie in the left half-plane. [[Polynomial]]s with this property are called Hurwitz-[[stable polynomial|stable]]. [[Derivation of the Routh array|The Routh–Hurwitz theorem was proved]] in 1895, and it was named after [[Edward John Routh]] and [[Adolf Hurwitz]].\n\n==Notations==\nLet ''f''(''z'') be a polynomial (with [[complex number|complex]] coefficients) of degree ''n'' with no roots on the [[complex plane|imaginary line]] (i.e. the line ''Z''&nbsp;=&nbsp;''ic'' where ''i'' is the [[imaginary unit]] and ''c'' is a [[real number]]).  Let us define <math>P_0(y)</math> (a polynomial of degree ''n'') and <math>P_1(y)</math> (a nonzero polynomial of degree strictly less than ''n'') by <math>f(iy)=P_0(y)+iP_1(y)</math>, respectively the [[real part|real]] and [[imaginary part]]s of ''f'' on the imaginary line.\n\nFurthermore, let us denote by:\n* ''p'' the number of roots of ''f'' in the left [[half-plane]] (taking into account multiplicities);\n* ''q'' the number of roots of ''f'' in the right half-plane (taking into account multiplicities);\n* <math>\\Delta\\arg f(iy)</math> the variation of the argument of ''f''(''iy'') when ''y'' runs from −&infin; to +&infin;;\n* ''w''(''x'') is the number of variations of the [[Sturm's theorem#Generalized Sturm chains|generalized Sturm chain]] obtained from <math>P_0(y)</math> and <math>P_1(y)</math> by applying the [[Euclid's algorithm|Euclidean algorithm]];\n* <math>I_{-\\infty}^{+\\infty}r</math> is the [[Cauchy index]] of the [[rational function]] ''r'' over the real line.\n\n==Statement==\nWith the notations introduced above, the '''Routh–Hurwitz theorem''' states that:\n\n:<math>p-q=\\frac{1}{\\pi}\\Delta\\arg f(iy)= \\left.\\begin{cases} +I_{-\\infty}^{+\\infty}\\frac{P_0(y)}{P_1(y)} & \\text{for odd degree} \\\\[10pt] -I_{-\\infty}^{+\\infty}\\frac{P_1(y)}{P_0(y)} & \\text{for even degree} \\end{cases}\\right\\} = w(+\\infty)-w(-\\infty).</math>\n\nFrom the first equality we can for instance conclude that when the variation of the argument of ''f''(''iy'') is positive, then ''f''(''z'') will have more roots to the left of the imaginary axis than to its right.\nThe equality ''p''&nbsp;&minus;&nbsp;''q'' =&nbsp;''w''(+&infin;)&nbsp;&minus;&nbsp;''w''(&minus;&infin;) can be viewed as the complex counterpart of [[Sturm's theorem]].  Note the differences:  in Sturm's theorem, the left member is ''p''&nbsp;+&nbsp;''q'' and the ''w'' from the right member is the number of variations of a Sturm chain (while ''w'' refers to a generalized Sturm chain in the present theorem).\n\n==Routh–Hurwitz stability criterion==\n{{main|Routh–Hurwitz stability criterion}}\nWe can easily determine a stability criterion using this theorem as it is trivial that ''f''(''z'') is [[Stable polynomial|Hurwitz-stable]] [[iff]] ''p''&nbsp;&minus;&nbsp;''q'' = ''n''.  We thus obtain conditions on the coefficients of ''f''(''z'') by imposing ''w''(+&infin;) = ''n'' and ''w''(&minus;&infin;) = 0.\n\n==References==\n* {{cite book\n | author = Routh, E.J.\n | year = 1877\n | title = A Treatise on the Stability of a Given State of Motion, Particularly Steady Motion.\n | publisher = Macmillan and co.\n | isbn = \n}}\n* {{cite book\n | last = Hurwitz |first=A.\n | year = 1964\n | chapter = On The Conditions Under Which An Equation Has Only Roots With Negative Real Parts\n | title = Selected Papers on Mathematical Trends in Control Theory\n | location = New York |publisher=Dover\n | editor-first = Richard |editor-last= Bellman |editorlink=Richard E. Bellman\n | editor2-first = Robert E. |editor2-last=Kalaba\n}}\n*{{cite book\n | last = Gantmacher |first=F. R. |authorlink=Felix Gantmacher\n | year = 2005 |origyear = 1959\n | title = Applications of the Theory of Matrices\n | publisher = Dover |location= New York\n | pages = 226–233\n | isbn = 0-486-44554-2\n}}\n* {{cite book | last1=Rahman | first1=Q. I. | last2=Schmeisser | first2=G. | title=Analytic theory of polynomials | series=London Mathematical Society Monographs. New Series | volume=26 | location=Oxford | publisher=[[Oxford University Press]] | year=2002 | isbn=0-19-853493-0 | zbl=1072.30006 }}\n\n==External links==\n* [http://mathworld.wolfram.com/Routh-HurwitzTheorem.html Mathworld entry]\n\n{{DEFAULTSORT:Routh-Hurwitz theorem}}\n[[Category:Polynomials]]\n[[Category:Theorems in complex analysis]]\n[[Category:Theorems in real analysis]]"
    },
    {
      "title": "Ruffini's rule",
      "url": "https://en.wikipedia.org/wiki/Ruffini%27s_rule",
      "text": "In [[mathematics]], '''Ruffini's rule''' is a practical way for paper-and-pencil computation of the [[Euclidean division]] of a [[polynomial]] by a [[binomial (polynomial)|binomial]] of the form {{mvar|''x'' – ''r''}}. It was described by [[Paolo Ruffini]] in 1804.<ref>{{cite journal | last1 = Cajori | first1 = Florian | authorlink1 = Florian Cajori | year = 1911 | journal = [[Bulletin of the American Mathematical Society]] | volume = 17 | issue = 8 | pages = 389–444 | url = http://projecteuclid.org/download/pdf_1/euclid.bams/1183421253 | title = Horner's method of approximation anticipated by Ruffini | format = PDF | doi = 10.1090/s0002-9904-1911-02072-9 }}</ref> Ruffini's rule is a special case of [[synthetic division]] when the divisor is a linear factor.\n\n==Algorithm==\n\nThe rule establishes a method for dividing the polynomial\n:<math>P(x)=a_nx^n+a_{n-1}x^{n-1}+\\cdots+a_1x+a_0</math>\nby the binomial\n:<math>Q(x)=x-r</math>\nto obtain the quotient polynomial\n:<math>R(x)=b_{n-1}x^{n-1}+b_{n-2}x^{n-2}+\\cdots+b_1x+b_0</math>;\n\nThe algorithm is in fact the [[polynomial long division|long division]] of ''P''(''x'') by ''Q''(''x'').\n\nTo divide ''P''(''x'') by ''Q''(''x''):\n\n# Take the coefficients of ''P''(''x'') and write them down in order. Then write ''r'' at the bottom left edge, just over the line:\n#:<math>\n\\begin{array}{c|c c c c|c}\n & a_n & a_{n-1} & \\dots & a_1 & a_0\\\\\nr & & & & & \\\\\n\\hline\n & & & & & \\\\\n\\end{array}\n</math>\n# Pass the leftmost coefficient (''a''<sub>''n''</sub>) to the bottom, just under the line:\n#:<math>\n\\begin{array}{c|c c c c|c}\n & a_n & a_{n-1} & \\dots & a_1 & a_0\\\\\nr & & & & & \\\\\n\\hline\n & a_n & & & & \\\\\n & =b_{n-1} & & & &\n\\end{array}\n</math>\n# Multiply the rightmost number under the line by ''r'' and write it over the line and one position to the right:\n#:<math>\n\\begin{array}{c|c c c c|c}\n & a_n & a_{n-1} & \\dots & a_1 & a_0\\\\\nr & & b_{n-1} \\cdot r & & & \\\\\n\\hline\n & a_n & & & & \\\\\n & =b_{n-1} & & & &\n\\end{array}\n</math>\n# Add the two values just placed in the same column\n#:<math>\n\\begin{array}{c|c c c c|c}\n & a_n & a_{n-1} & \\dots & a_1 & a_0\\\\\nr & & b_{n-1}\\cdot r & & & \\\\\n\\hline\n & a_n & b_{n-1}\\cdot r+a_{n-1} & & & \\\\\n & =b_{n-1} & =b_{n-2} & & &\n\\end{array}\n</math>\n# Repeat steps 3 and 4 until no numbers remain\n#:<math>\n\\begin{array}{c|c c c c|c}\n & a_n & a_{n-1} & \\dots & a_1 & a_0 \\\\\nr & & b_{n-1}\\cdot r & \\dots & b_1\\cdot r & b_0 \\cdot r \\\\\n\\hline\n & a_n & b_{n-1} \\cdot r+a_{n-1} & \\dots & b_1 \\cdot r+a_1 & a_0+b_0 \\cdot r \\\\\n & =b_{n-1} & =b_{n-2} & \\dots & =b_0 & =R \\\\\n\\end{array}\n</math>\n\nThe ''b'' values are the coefficients of the result (''R''(''x'')) polynomial, the degree of which is one less than that of ''P''(''x''). The final value obtained, ''s'', is the remainder.  As shown in the [[polynomial remainder theorem]], this remainder is equal to ''P''(''r''), the value of the polynomial at ''r''.\n\n==Uses of the rule==\n\nRuffini's rule has many practical applications; most of them rely on simple division (as demonstrated below) or the common extensions given still further below.\n\n===Polynomial division by ''x'' &minus; ''r''===\nA worked example of polynomial division, as described above.\n\nLet:\n<!-- The \\,\\! is to keep the formulae rendered as PNG instead of HTML to ensure consistency of representation. Please don't remove it.--> \n:<math>P(x)=2x^3+3x^2-4\\,\\!</math>\n:<math>Q(x)=x+1.\\,\\!</math>\n\nWe want to divide ''P''(''x'') by ''Q''(''x'') using Ruffini's rule. The main problem is that ''Q''(''x'') is not a binomial of the form ''x'' &minus; ''r'', but rather ''x'' + ''r''. We must rewrite ''Q''(''x'') in this way:\n:<math>Q(x)=x+1=x-(-1).\\,\\!</math>\nNow we apply the algorithm:\n\n1. Write down the coefficients and ''r''. Note that, as ''P''(''x'') didn't contain a coefficient for ''x'', we've written 0:\n     |     2     3     0     -4\n     |                                    \n  -1 |                                    \n ----|----------------------------\n     |                                    \n     |\n\n2. Pass the first coefficient down:\n     |     2     3     0     -4\n     |                                    \n  -1 |                                    \n ----|----------------------------\n     |     2                              \n     |\n\n3. Multiply the last obtained value by ''r'':\n     |     2     3     0     -4\n     |                                    \n  -1 |          -2                         \n ----|----------------------------\n     |     2                              \n     |\n\n4. Add the values:\n     |     2     3     0     -4\n     |\n  -1 |          -2\n ----|----------------------------\n     |     2     1\n     |\n\n5. Repeat steps 3 and 4 until we've finished:\n     |     2     3     0     -4\n     |\n  -1 |          -2    -1      1\n ----|----------------------------\n     |     2     1    -1     -3\n     |{result coefficients}{remainder}\n\n<!-- The \\,\\! is to keep the formulae rendered as PNG instead of HTML to ensure consistency of representation. Please don't remove it.-->\n\nSo, if ''original number'' = ''divisor'' &times; ''quotient'' + ''remainder'', then\n:<math>P(x)=Q(x)R(x)+s\\,\\!</math>, where\n\n:<math>R(x) = 2x^2+x-1\\,\\!</math> and <math>s=-3; \\quad \\Rightarrow 2x^3+3x^2-4 = (2x^2+x-1)(x+1) - 3\\!</math>\n\n===Polynomial root-finding===\n\nThe [[rational root theorem]] tells us that for a polynomial {{math|1=''f''(''x'') = ''a''<sub>''n''</sub>''x''<sup>''n''</sup>&nbsp;+&nbsp;''a''<sub>''n''&minus;1</sub>''x''<sup>''n''&minus;1</sup>&nbsp;+&nbsp;...&nbsp;+&nbsp;''a''<sub>1</sub>''x''&nbsp;+&nbsp;''a''<sub>0</sub>}} all of whose coefficients (''a''<sub>''n''</sub> through ''a''<sub>0</sub>) are [[integer]]s, the real [[rational number|rational]] roots are always of the form ''p''/''q'', where ''p'' is an integer divisor of ''a''<sub>0</sub> and ''q'' is an integer divisor of ''a''<sub>''n''</sub>. Thus if our polynomial is\n\n<!-- The \\,\\! is to keep the formula rendered as PNG instead of HTML to ensure consistency of representation. Please don't remove it.--> \n:<math>P(x)=x^3+2x^2-x-2=0\\,\\!,</math>\n\nthen the possible rational roots are all the integer divisors of ''a''<sub>0</sub> (&minus;2):\n:<math>\\text{Possible roots:}\\left\\{+1, -1, +2, -2\\right\\}.</math>\n\n(This example is simple because the polynomial is [[monic polynomial|monic]] (i.e. ''a''<sub>''n''</sub> = 1); for non-monic polynomials the set of possible roots will include some fractions, but only a finite number of them since ''a''<sub>''n''</sub> and ''a''<sub>0</sub> only have a finite number of integer divisors each.) In any case, for monic polynomials, every rational root is an integer, and so every integer root is just a divisor of the [[constant term]] (i.e. ''a''<sub>0</sub>). It can be shown that this remains true for non-monic polynomials, i.e. ''to find the integer roots of any polynomials with integer coefficients, it suffices to check the divisors of the constant term''.\n\nSo, setting ''r'' equal to each of these possible roots in turn, we will test-divide the polynomial by ({{math|''x''&nbsp;&minus;&nbsp;''r''}}). If the resulting quotient has no remainder, we have found a root.\n\nYou can choose one of the following three methods: they will all yield the same results, with the exception that only through the second method and the third method (when applying Ruffini's rule to obtain a factorization) can you discover that a given root is repeated. (Neither method will discover irrational or complex roots.)\n\n====Method 1====\n\nWe try to divide ''P''(''x'') by the binomial (''x'' &minus; each possible root). If the remainder is 0, the selected number is a root (and vice versa):\n     |    +1    +2    -1     -2                      |    +1    +2    -1    -2\n     |                                               |\n  +1 |          +1    +3     +2                   -1 |          -1    -1    +2\n ----|----------------------------               ----|---------------------------\n     |    +1    +3    +2      0                      |    +1    +1    -2     0\n\n     |    +1    +2    -1     -2                      |    +1    +2    -1    -2\n     |                                               |\n  +2 |          +2    +8    +14                   -2 |          -2     0    +2\n ----|----------------------------               ----|---------------------------\n     |    +1    +4    +7    +12                      |    +1     0    -1     0\n:<math>\\begin{cases}\nx_1=+1\\\\\nx_2=-1\\\\\nx_3=-2\n\\end{cases}</math>\n\n====Method 2====\n\nWe start just as in Method 1 until we find a valid root. Then, instead of restarting the process with the other possible roots, we continue testing the possible roots against the result of the Ruffini on the valid root we've just found until we only have a coefficient remaining (remember that roots can be repeated: if you get stuck, try each valid root twice):\n     |    +1    +2    -1    -2                      |    +1    +2    -1    -2\n     |                                              |\n  -1 |          -1    -1    +2                   -1 |          -1    -1    +2\n ----|---------------------------               ----|---------------------------\n     |    +1    +1    -2   | 0                      |    +1    +1    -2   | 0\n     |                                              |\n  +2 |          +2    +6                         +1 |          +1    +2\n -------------------------                      -------------------------\n     |    +1    +3   |+4                            |    +1    +2   | 0\n                                                    |\n                                                 -2 |          -2\n                                                -------------------\n                                                    |    +1   | 0\n:<math>\\begin{cases}\nx_1=-1\\\\\nx_2=+1\\\\\nx_3=-2\n\\end{cases}</math>\n\n====Method 3====\n*Determine the set of the possible integer or rational roots of the polynomial according to the [[rational root theorem]].\n*For each possible root ''r'', instead of performing the division ''P''(''x'')/(''x''–''r''), apply the [[polynomial remainder theorem]], which states that the remainder of this division is ''P''(''r''), i.e. the polynomial evaluated for ''x'' = ''r''.\n\nThus, for each ''r'' in our set, ''r'' is actually a root of the polynomial if and only if ''P''(''r'')=0\n\nThis shows that finding ''integer and rational'' roots of a polynomial neither requires any division nor the application of Ruffini's rule.\n\nHowever, once a valid root has been found, call it ''r''<sub>1</sub>:\nyou can apply Ruffini's rule to determine\n:''Q''(''x'')=''P''(''x'')/(''x''–''r''<sub>1</sub>).\nThis allows you to partially factorize the polynomial as\n:''P''(''x'')=(''x''–''r''<sub>1</sub>)·''Q''(''x'')\n\nAny additional (rational) root of the polynomial is also a root of ''Q''(''x'') and, of course, is still to be found among the possible roots determined earlier which have not yet been checked (any value already determined ''not'' to be a root of ''P''(''x'') is not a root of ''Q''(''x'') either; more formally, ''P''(''r'')≠0 → ''Q''(''r'')≠0 ).\n\nThus, you can proceed evaluating ''Q''(''r'') instead of ''P''(''r''), and (as long as you can find another root, ''r''<sub>2</sub>) dividing ''Q''(''r'') by (''x''–''r''<sub>2</sub>).\n\nEven if you're only searching for roots, this allows you to evaluate polynomials of successively smaller degree, as the factorization proceeds.\n\nIf, as is often the case, you are also factorizing a polynomial of degree ''n'', then:\n\n*if you have found ''p''=''n'' rational solutions you end up with a complete factorization (see below) into ''p''=''n'' linear factors;\n*if you have found ''p''<''n'' rational solutions you end up with a partial factorization (see below) into ''p'' linear factors and another non-linear factor of degree ''n''–''p'', which, in turn, may have irrational or complex roots.\n\n===== Examples =====\n\n======Finding roots without applying Ruffini's Rule======\n\n:{{math|1=''P''(''x'')=''x''³+2''x''²–''x''–2}}\n\nPossible roots = {1, –1, 2, -2}\n\n*''P''(1) = 0 → ''x''<sub>1</sub> = 1\n*''P''(-1) = 0 → ''x''<sub>2</sub> = -1\n*''P''(2) = 12 → 2 is not a root of the polynomial\nand the remainder of {{math|1=(''x''³+2''x''²-''x''-2)/(''x''-2)}} is 12\n*''P''(-2) = 0 → ''x''<sub>3</sub> = -2\n\n======Finding roots applying Ruffini's Rule and obtaining a (complete) factorization======\n\n:{{math|1=''P''(''x'') = ''x''³+2''x''²-''x''-2}}\n\nPossible roots = {1, -1, 2, -2}\n\n*''P''(1) = 0 → ''x''<sub>1</sub> = 1\n\nThen, applying Ruffini's Rule:\n\n:{{math|1=(''x''³+2''x''²-''x''-2)/(''x''-1) = (''x''²+3''x''+2)}}\n:{{math|1=''x''³+2''x''²-''x''-2 = (''x''-1)(''x''²+3''x''+2)}}\n\nHere, ''r''<sub>1</sub>=-1 and {{math|1=''Q''(''x'') = ''x''²+3''x''+2}}\n\n*''Q''(-1) = 0 → ''x''<sub>2</sub> = -1\n\nAgain, applying Ruffini's Rule:\n\n:{{math|1=(''x''²+3''x''+2)/(''x''+1) = (''x''+2)}}\n:{{math|1=''x''³+2''x''²-''x''-2 = (''x''-1)(''x''²+3''x''+2) = (''x''-1)(''x''+1)(''x''+2)}}\n\nAs it was possible to completely factorize the polynomial, it's clear that the last root is -2 (the previous procedure would have given the same result, with a final quotient of 1).\n\n===Polynomial factoring===\nHaving used the \"''p''/''q''\" result above (or, to be fair, any other means) to find all the real rational roots of a particular polynomial, it is but a trivial step further to partially [[factorization|factor]] that polynomial using those roots. As is well-known, each linear factor (''x''&nbsp;&minus;&nbsp;''r'') which divides a given polynomial corresponds with a root ''r'', and ''vice versa''.\n\nSo if\n<!-- The \\,\\! is to keep the formulae rendered as PNG instead of HTML to ensure consistency of representation. Please don't remove it.--> \n:<math>P(x)=a_nx^n+a_{n-1}x^{n-1}+\\cdots+a_1x+a_0\\,\\!</math>&nbsp;is our polynomial; and\n:<math>R=\\left\\{\\mbox{roots of }P(x)\\in\\mathbb{Q}\\right\\}\\,\\!</math> are the roots we have found, then consider the product\n:<math>R(x)=a_n{\\prod (x-r)} \\mbox{ for all } r\\in R. \\,\\!</math>\n\nBy the [[fundamental theorem of algebra]], ''R''(''x'') should be equal to ''P''(''x''), if all the roots of ''P''(''x'') are rational. But since we have been using a method which finds only rational roots, it is very likely that ''R''(''x'') is not equal to ''P''(''x''); it is very likely that ''P''(''x'') has some irrational or complex roots not in ''R''. So consider\n\n:<math>S(x)=\\frac{P(x)}{R(x)}\\,\\!</math>, which can be calculated using [[polynomial long division]].\n\nIf ''S''(''x'') = 1, then we know ''R''(''x'') = ''P''(''x'') and we are done. Otherwise, ''S''(''x'') will itself be a polynomial; this is another factor of ''P''(''x'') which has no real rational roots. So write out the right-hand-side of the following equation in full:\n\n:<math>P(x)=R(x) \\cdot S(x).\\,\\!</math>\n\nWe can call this a ''complete factorization'' of ''P''(''x'') over '''Q''' (the rationals) if ''S''(''x'') = 1. Otherwise, we only have a ''partial factorization'' of ''P''(''x'') over '''Q''', which may or may not be further factorable over the rationals; but which will certainly be further factorable over the reals or at worst the complex plane. (Note: by a \"complete factorization\" of ''P''(''x'') over '''Q''', we mean a factorization as a product of polynomials with rational coefficients, such that each factor is irreducible over '''Q''', where \"irreducible over '''Q'''\" means that the factor cannot be written as the product of two non-constant polynomials with rational coefficients and smaller degree.)\n\n====Example 1: no remainder====\nLet\n<!-- The \\,\\! is to keep the formulae rendered as PNG instead of HTML to ensure consistency of representation. Please don't remove it.-->\n:<math>P(x)=x^3+2x^2-x-2.\\,\\!</math>\n\nUsing the methods described above, the rational roots of ''P''(''x'') are:\n:<math>R=\\left\\{+1, -1, -2\\right\\}.\\,\\!</math>\n\nThen, the product of (''x'' &minus; each root) is\n:<math>R(x)=1(x-1)(x+1)(x+2).\\,\\!</math>\n\nAnd ''P''(''x'')/''R''(''x''):\n:<math>S(x)=1.\\,\\!</math>\n\nHence the factored polynomial is ''P''(''x'') = ''R''(''x'') &middot; 1 = ''R''(''x''):\n:<math>P(x)=(x-1)(x+1)(x+2).\\,\\!</math>\n\n====Example 2: with remainder====\nLet\n<!-- The \\,\\! is to keep the formulae rendered as PNG instead of HTML to ensure consistency of representation. Please don't remove it.-->\n:<math>P(x)=2x^4-3x^3+x^2-2x-8.\\,\\!</math>\n\nUsing the methods described above, the rational roots of ''P''(''x'') are:\n:<math>R=\\left\\{-1, +2\\right\\}.\\,\\!</math>\n\nThen, the product of (''x'' &minus; each root) is\n:<math>R(x)=(x+1)(x-2).\\,\\!</math>\n\nAnd ''P''(''x'')/''R''(''x'')\n:<math>S(x)=2x^2-x+4.\\,\\!</math>\n\nAs <math>S(x){\\ne}1</math>, the factored polynomial is ''P''(''x'') = ''R''(''x'') &middot; ''S''(''x''):\n:<math>P(x)=(x+1)(x-2)(2x^2-x+4).\\,\\!</math>\n\n====Factoring over the complexes====\nTo completely factor a given polynomial over '''C''', the complex numbers, we must know all of its roots (and that could include irrational and/or complex numbers). For example, consider the polynomial above:\n:<math>P(x)=2x^4-3x^3+x^2-2x-8.\\,\\!</math>\n\nExtracting its rational roots and factoring it, we end with:\n:<math>P(x)=(x+1)(x-2)(2x^2-x+4).\\,\\!</math>\n\nBut that is not completely factored over '''C'''. If we need to factor our polynomial to a product of linear factors, we must deal with that quadratic factor\n\n:<math>{2x^2-x+4}=0.\\,\\!</math>\n\nThe easiest way is to use [[quadratic formula]], which gives us\n:<math>x=\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}=\\frac{1\\pm\\sqrt{(-1)^2-4\\cdot 2\\cdot 4}}{2\\cdot 2}=\\frac{1\\pm\\sqrt{-31}}{4}\\,\\!</math>\n\nand the solutions\n:<math>x_1=\\frac{1+\\sqrt{-31}}{4}\\,\\!</math>\n:<math>x_2=\\frac{1-\\sqrt{-31}}{4}.\\,\\!</math>\n\nSo the completely factored polynomial over '''C''' will be:\n:<math>P(x)=2(x+1)(x-2)\\left(x-\\frac{1+i\\sqrt{31}}{4}\\right)\\left(x-\\frac{1-i\\sqrt{31}}{4}\\right).\\,\\!</math>\n\nHowever, it should be noted that we cannot in every case expect things to be so easy; the quadratic formula's analogue for fourth-order polynomials is very messy and no such analogue exists for 5th-or-higher order polynomials. See [[Galois theory]] for a theoretical explanation of why this is so, and see [[numerical analysis]] for ways to ''approximate'' roots of polynomials numerically.\n\n====Limitations====\n\nIt is entirely possible that, when looking for a given polynomial's roots, we might obtain a messy higher-order polynomial for S(x) which is further factorable over the ''rationals'' even before considering irrational or complex factorings. Consider the polynomial ''x''<sup>5</sup>&nbsp;&minus;&nbsp;3''x''<sup>4</sup>&nbsp;+&nbsp;3''x''<sup>3</sup>&nbsp;&minus;&nbsp;9''x''<sup>2</sup>&nbsp;+&nbsp;2''x''&nbsp;&minus;&nbsp;6. Using Ruffini's method we will find only one root (''x'' = 3); factoring it out gives us ''P''(''x'') = (''x''<sup>4</sup>&nbsp;+&nbsp;3''x''<sup>2</sup>&nbsp;+&nbsp;2)(''x''&nbsp;&minus;&nbsp;3).\n\nAs explained above, if our assignment was to \"factor into irreducibles over '''C'''\" we know that would have to find some way to dissect the quartic and look for its irrational and/or complex roots. But if we were asked to \"factor into irreducibles over '''Q'''\", we might think we are done; but it is important to realize that this might not necessarily be the case.\n\nFor in this instance the quartic is actually factorable as the product of two quadratics (''x''<sup>2</sup>&nbsp;+&nbsp;1)(''x''<sup>2</sup>&nbsp;+&nbsp;2). These, at last, are irreducible over the rationals (and, indeed, the reals as well in this example); so now we are done; ''P''(''x'') = (''x''<sup>2</sup>&nbsp;+&nbsp;1)(''x''<sup>2</sup>&nbsp;+&nbsp;2)(''x''&nbsp;&minus;&nbsp;3). In this instance it is in fact easy to factor our quartic by treating it as a [[Quartic equation#Biquadratic equations|biquadratic equation]]; but finding such factorings of a higher degree polynomial can be very difficult.\n\n==History==\n\nThis method was invented by [[Paolo Ruffini]]. He took part in a competition organised by the Italian Scientific Society (of Forty). The question to be answered was a method to find the roots of any polynomial. Five submissions were received. In 1804 Ruffini's was awarded the first place and the method was published. Ruffini published refinements of the method in 1807 and 1813.\n\nHorner's method was published in 1819 and in a refined version in 1845.\n\n==See also==\n*[[Horner scheme|Horner's method]]\n*[[Polynomial long division]]\n\n==References==\n<references/>\n\n==External links==\n* {{MathWorld|SyntheticDivision|Ruffini's rule}}\n* {{commons category inline}}\n\n[[Category:Polynomials]]\n[[Category:Root-finding algorithms]]\n[[Category:Division (mathematics)]]"
    },
    {
      "title": "Schwartz–Zippel lemma",
      "url": "https://en.wikipedia.org/wiki/Schwartz%E2%80%93Zippel_lemma",
      "text": "In mathematics, the '''Schwartz–Zippel lemma''' (also called the '''DeMillo-Lipton-Schwartz–Zippel lemma''') is a tool commonly used in probabilistic [[polynomial identity testing]], i.e. in the problem of determining whether a given multivariate [[polynomial]] is the\n0-polynomial (or identically equal to 0). It was discovered independently by [[Jack Schwartz]],<ref>{{harv|Schwartz|1980}}</ref> [[Richard Zippel]],<ref>{{harv|Zippel|1979}}</ref> and [[Richard DeMillo]] and [[Richard J. Lipton]], although DeMillo and Lipton's version was shown a year prior to Schwartz and Zippel's result.<ref>{{harv|DeMillo & Lipton|1978}}</ref> The finite field version of this bound was proved by [[Øystein Ore]] in 1922.<ref>Ö. Ore, Über höhere Kongruenzen. Norsk Mat. Forenings Skrifter Ser. I (1922), no. 7, 15 pages.</ref>\n\n== Statement of the lemma ==\n\nThe input to the problem is an ''n''-variable polynomial over a [[Field (mathematics)|field]] '''F'''. It can occur in the following forms:\n\n=== Algebraic form ===\n\nFor example, is\n\n: <math>(x_1 + 3x_2 - x_3)(3x_1 + x_4 - 1) \\cdots (x_7 - x_2) \\equiv 0\\  ?</math>\n\nTo solve this, we can multiply it out and check that all the coefficients are&nbsp;0.  However, this takes [[exponential time]]. In general, a polynomial can be algebraically represented by an [[arithmetic circuit|arithmetic formula or circuit]].\n\n=== Determinant of a matrix with polynomial entries ===\nLet\n\n: <math>p(x_1,x_2, \\ldots, x_n) </math>\n\nbe the [[determinant]] of the [[polynomial matrix]].\n\nCurrently, there is no known sub-exponential time [[algorithm]] that can solve this problem deterministically. However, there are randomized polynomial algorithms for testing polynomial identities. Their analysis usually requires a bound on the probability that a non-zero polynomial will have roots at randomly selected test points. The Schwartz–Zippel lemma provides this as follows:\n\n'''Theorem 1''' (Schwartz, Zippel). ''Let''\n\n: <math>P\\in F[x_1,x_2,\\ldots,x_n]</math>\n\n''be a non-zero polynomial of total [[degree of a polynomial|degree]]'' {{math|1=''d''&nbsp;≥&nbsp;0}} ''over a [[Field (mathematics)|field]]&nbsp;F. Let S be a finite subset of F and let'' {{math|''r''<sub>1</sub>,&nbsp;''r''<sub>2</sub>,&nbsp;...,&nbsp;''r<sub>n</sub>''}} ''be selected at random independently and uniformly from&nbsp;S. Then''\n\n: <math>\\Pr[P(r_1,r_2,\\ldots,r_n)=0]\\leq\\frac{d}{|S|}. </math>\n\nIn the single variable case, this follows directly from the fact that a polynomial of [[degree of a polynomial|degree]] ''d'' can have no more than ''d'' roots. It seems logical, then, to think that a similar statement would hold for multivariable polynomials. This is, in fact, the case.\n\n''Proof.''  The proof is by [[mathematical induction]] on ''n''. For {{math|1=''n''&nbsp;=&nbsp;1}}, as was mentioned before, ''P'' can have at most ''d'' roots.  This gives us the base case.\nNow, assume that the theorem holds for all polynomials in {{math|''n''&nbsp;&minus;&nbsp;1}} variables. We can then consider ''P'' to be a polynomial in ''x''<sub>1</sub> by writing it as\n\n: <math>P(x_1,\\dots,x_n)=\\sum_{i=0}^d x_1^i P_i(x_2,\\dots,x_n).</math>\n\nSince {{mvar|P}} is not identically 0, there is some {{mvar|i}} such that <math>P_i</math> is not identically 0. Take the largest such {{mvar|i}}. Then <math>\\deg P_i\\leq d-i</math>, since the degree of <math>x_1^iP_i</math> is at most d.\n\nNow we randomly pick <math>r_2,\\dots,r_n</math> from {{mvar|S}}. By the induction hypothesis, <math>\\Pr[P_i(r_2,\\ldots,r_n)=0]\\leq\\frac{d-i}{|S|}. </math>\n\nIf <math>P_i(r_2,\\ldots,r_n)\\neq 0</math>, then <math>P(x_1,r_2,\\ldots,r_n)</math> is of degree {{mvar|i}} (and thus not identically zero) so\n\n::: <math>\\Pr[P(r_1,r_2,\\ldots,r_n)=0|P_i(r_2,\\ldots,r_n)\\neq 0]\\leq\\frac{i}{|S|}.</math>\n\nIf we denote the event <math>P(r_1,r_2,\\ldots,r_n)=0</math> by {{mvar|A}}, the event <math>P_i(r_2,\\ldots,r_n)=0</math> by {{mvar|B}}, and the complement of {{mvar|B}} by <math>B^c</math>, we have\n\n:<math>\\begin{align}\n\\Pr[A] & =\\Pr[A\\cap B]+\\Pr[A\\cap B^c]\n\\\\\n&=\\Pr[B]\\Pr[A|B]+\\Pr[B^c]\\Pr[A|B^c]\n\\\\\n&\\leq \\Pr[B]+\\Pr[A|B^c]\n\\\\\n&\\leq \\frac{d-i}{|S|}+\\frac{i}{|S|}=\\frac{d}{|S|}\n\\end{align}\n</math>\n\n== Applications ==\nThe importance of the Schwartz–Zippel Theorem and Testing Polynomial Identities follows\nfrom algorithms which are obtained to problems that can be reduced to the problem\nof [[polynomial identity testing]].\n\n=== Comparison of two polynomials ===\n''Given a pair of polynomials <math>p_1(x)</math> and <math>p_2(x)</math>, is''\n    \n::: <math>p_1(x) \\equiv p_2(x)</math>?\n\nThis problem can be solved by reducing it to the problem of polynomial identity testing. It is equivalent to checking if\n\n::: <math>[p_1(x) - p_2(x)] \\equiv 0.</math>\n\nHence if we can determine that\n   \n::: <math>p(x) \\equiv 0,</math>\n   \nwhere\n\n::: <math>p(x) = p_1(x)\\;-\\;p_2(x),</math>\n   \nthen we can determine whether the two polynomials are equivalent.\n\nComparison of polynomials has applications for branching programs (also called [[binary decision diagram]]s). A read-once branching program can be represented by a [[multilinear polynomial]] which computes (over any field) on {0,1}-inputs the same [[Boolean function]] as the branching program, and two branching programs compute the same function if and only if the corresponding polynomials are equal. Thus, identity of Boolean functions computed by read-once branching programs can be reduced to polynomial identity testing.\n\nComparison of two polynomials (and therefore testing polynomial identities) also has\napplications in 2D-compression, where the problem of finding the equality of two\n2D-texts ''A'' and ''B'' is reduced to the problem\nof comparing equality of two polynomials <math>p_A(x,y)</math> and <math>p_B(x,y)</math>.\n\n=== Primality testing ===\n''Given <math>n \\in \\mathbb{Z^+}</math>, is <math>n</math> a [[prime number]]?''\n\nA simple randomized algorithm developed by [[Manindra Agrawal]] and Somenath Biswas can determine probabilistically\nwhether <math>n</math> is prime and uses polynomial identity testing to do so.\n\nThey propose that all prime numbers ''n'' (and only prime numbers) satisfy the following\npolynomial identity:\n\n:::  <math>(1+z)^n = 1+z^n (\\mbox{mod}\\;n).</math>\n\nThis is a consequence of the [[Frobenius endomorphism]].\n\nLet\n\n::: <math>\\mathcal{P}_n(z) = (1+z)^n - 1 -z^n.</math>\n\nThen <math>\\mathcal{P}_n(z) = 0\\;(\\mbox{mod}\\;n)</math> ''[[iff]] n is prime''.  The proof can be found in [4].  However, \nsince this polynomial has degree <math>n</math>, and since <math>n</math> may or may not be a prime, \nthe Schwartz–Zippel method would not work.   Agrawal and Biswas use a more sophisticated technique, which divides \n<math>\\mathcal{P}_n</math> by a random monic polynomial of small degree.\n\nPrime numbers are used in a number of applications such as hash table sizing, [[pseudorandom]] number\ngenerators and in key generation for [[cryptography]].  Therefore, finding very large prime numbers\n(on the order of (at least) <math>10^{350} \\approx 2^{1024}</math>) becomes very important and efficient primality testing algorithms\nare required.\n\n=== Perfect matching ===\n''Let <math>G = (V, E)</math> be a [[Graph (discrete mathematics)|graph]] of {{math|n}} vertices where {{math|n}} is even. Does {{math|G}} contain a [[perfect matching]]?''\n\n'''Theorem 2''' {{harv|Tutte|1947}}: ''A [[Tutte matrix]] determinant is not a {{math|0}}-polynomial [[if and only if]] there exists a perfect matching.''\n\nA subset {{math|D}} of {{math|E}} is called a matching if each vertex in {{math|V}} is incident with at most one edge in {{math|D}}.  A matching is perfect if each vertex in {{math|V}} has exactly one edge that is incident to it in {{math|D}}. Create a ''Tutte matrix'' {{math|A}} in the following way:\n\n::: <math>A = \\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1\\mathit{n}} \\\\ a_{21} & a_{22} & \\cdots & a_{2\\mathit{n}} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{\\mathit{n}1} & a_{\\mathit{n}2} & \\ldots & a_{\\mathit{nn}} \\end{bmatrix}</math>\n\nwhere\n\n::: <math>a_{ij} = \\begin{cases} x_{ij}\\;\\;\\mbox{if}\\;(i,j) \\in E \\mbox{ and } i<j\\\\\n-x_{ji}\\;\\;\\mbox{if}\\;(i,j) \\in E \\mbox{ and } i>j\\\\\n0\\;\\;\\;\\;\\mbox{otherwise}. \\end{cases}</math>\n\nThe Tutte matrix determinant (in the variables ''x<sub>ij</sub>'', {{tmath|i<j}} ) is then defined as the [[determinant]] of this [[skew-symmetric matrix]] which coincides with the square of the [[pfaffian]] of the matrix ''A'' and is non-zero (as polynomial) if and only if a perfect matching exists.\nOne can then use polynomial identity testing to find whether {{math|G}} contains a perfect matching. There exists a deterministic black-box algorithm for graphs with polynomially bounded permanents (Grigoriev & Karpinski 1987).<ref>{{harv|Grigoriev & Karpinski|1987}}</ref>\n\nIn the special case of a balanced [[bipartite graph]] on <math> n =m + m </math> vertices this matrix takes the form of a [[block matrix]] \n::: <math>A = \\begin{pmatrix} 0 & X \\\\ -X^t & 0 \\end{pmatrix}</math>\nif the first ''m'' rows (resp. columns) are indexed with the first subset of the bipartition and the last ''m'' rows with the complementary subset. In this case the pfaffian coincides with the usual determinant of the ''m'' &times; ''m'' matrix ''X'' (up to sign).  Here ''X'' is the [[Edmonds matrix]].\n\n==Notes==\n{{reflist|colwidth=25em}}\n\n==References==\n{{refbegin|colwidth=25em}}\n* {{cite journal\n|url= http://ieeexplore.ieee.org/iel5/6604/17631/00814592.pdf\n|title= Primality and Identity Testing via Chinese Remaindering\n|accessdate= 2008-06-15\n|last1=Agrawal|first1=Manindra\n|last2=Biswas | first2=Somenath\n|date= 2003-02-21\n|journal= Journal of the ACM\n|volume= 50\n|issue= 4\n|ref=harv\n|pages=429–443\n |doi=10.1145/792538.792540\n}}\n* {{cite journal\n|url= http://www.egr.unlv.edu/~larmore/Research/pattern.ps.gz\n|title= On the Complexity of Pattern Matching for Highly Compressed Two-Dimensional Texts\n|accessdate= 2008-06-15\n|last1=Berman | first1=Piotr\n|last2=Karpinski | first2=Marek | authorlink2=Marek Karpinski\n|last3=Larmore | first3=Lawrence L.\n|last4=Plandowski | first4=Wojciech\n|last5=Rytter | first5=Wojciech | author5-link = Wojciech Rytter\n|format= ps\n|journal= Journal of Computer and System Sciences \n|volume=65\n|issue= 2\n|pages=332–350\n|doi= 10.1006/jcss.2002.1852\n|ref=harv\n|year= 2002\n}}\n* {{cite book|last2=Karpinski|first2=Marek|first1=Dima|last1=Grigoriev|title=The matching problem for bipartite graphs with polynomially bounded permanents is in NC|journal=Proceedings of the Annual Symposium on Foundations of Computer Science|year=1987|pages=166–172|doi=10.1109/SFCS.1987.56|ref=harv|isbn=978-0-8186-0807-0}}\n* Moshkovitz, Dana (2010). An Alternative Proof of The Schwartz-Zippel Lemma. {{ECCC|2010|10|096}}\n* {{cite journal\n|url= http://www.sciencedirect.com/science/article/pii/0020019078900674\n|title= A probabilistic remark on algebraic program testing\n|accessdate= 2014-05-13\n|last1= DeMillo | first1=Richard A. | authorlink1=Richard DeMillo\n|last2= Lipton | first2=Richard J. | authorlink2 = Richard Lipton\n|journal = Information Processing Letters\n|volume = 7\n|number = 4\n|year = 1978\n|pages = 193–195\n|doi = 10.1016/0020-0190(78)90067-4\n|ref = DeMilloLipton1978\n}}\n* {{cite book\n|last=Rudich\n|first=Steven\n|editor=AMS\n|title=Computational Complexity Theory\n|series= IAS/Park City Mathematics Series\n|volume=10\n|isbn= 978-0-8218-2872-4\n|ref=harv\n|year=2004}}\n* {{cite journal\n|url= http://delivery.acm.org/10.1145/330000/322225/p701-schwartz.pdf\n|title= Fast probabilistic algorithms for verification of polynomial identities\n|accessdate= 2008-06-15\n|last=Schwartz |first=Jack\n|authorlink=Jack Schwartz\n|date=October 1980\n|journal= Journal of the ACM\n|pages=701–717\n|ref=harv\n|doi= 10.1145/322217.322225\n|volume= 27\n|issue= 4\n|citeseerx= 10.1.1.391.1254\n}}\n* {{cite journal\n|url= http://jlms.oxfordjournals.org/cgi/reprint/s1-22/2/107.pdf\n|title= The factorization of linear graphs\n|accessdate= 2008-06-15\n|last=Tutte |first=W.T.\n|authorlink=W. T. Tutte\n|date=April 1947\n|volume=22\n|issue= 2\n|journal=J. London Math. Soc.\n|ref=harv\n|pages=107–111\n|doi=10.1112/jlms/s1-22.2.107\n}}\n* {{cite book\n|title= Probabilistic algorithms for sparse polynomials\n|first= Richard\n|last=Zippel\n|year=1979\n|ref=harv\n|doi=10.1007/3-540-09519-5_73\n|journal=Lecture Notes in Computer Science\n|volume= 72\n|pages=216–226\n|isbn= 978-3-540-09519-4\n}}\n* {{cite web\n|url=http://historical.ncstrl.org/tr/ps/cornellcs/TR89-965.ps\n|title= An Explicit Separation of Relativised Random Polynomial Time and Relativised Deterministic Polynomial Time\n|accessdate= 2008-06-15\n|first= Richard\n|last=Zippel\n|date=February 1989\n|ref=harv\n\n|format= ps\n}}\n* {{cite book\n|last=Zippel\n|first=Richard\n|editor=Springer\n|title=Effective Polynomial Computation\n|url=https://www.springer.com/computer/mathematics/book/978-0-7923-9375-7\n|edition=\n|series=The Springer International Series in Engineering and Computer Science\n|volume=241\n|ref=harv\n|isbn= 978-0-7923-9375-7\n|year=1993}}\n\n{{refend}}\n\n==External links==\n* [http://rjlipton.wordpress.com/2009/11/30/the-curious-history-of-the-schwartz-zippel-lemma The Curious History of the Schwartz–Zippel Lemma], by [[Richard J. Lipton]]\n\n{{DEFAULTSORT:Schwartz-Zippel lemma}}\n[[Category:Polynomials]]\n[[Category:Computer algebra]]\n[[Category:Lemmas]]\n[[Category:Mathematical theorems in theoretical computer science]]"
    },
    {
      "title": "Secondary polynomials",
      "url": "https://en.wikipedia.org/wiki/Secondary_polynomials",
      "text": "{{unreferenced|date=August 2012}}\nIn [[mathematics]], the '''secondary polynomials''' <math>\\{q_n(x)\\}</math> associated with a [[sequence]] <math>\\{p_n(x)\\}</math> of [[polynomials]] [[orthogonal polynomials|orthogonal]] with respect to a density <math>\\rho(x)</math> are defined by\n\n: <math> q_n(x) = \\int_\\mathbb{R}\\! \\frac{p_n(t) - p_n(x)}{t - x} \\rho(t)\\,dt. </math>\n\nTo see that the functions <math>q_n(x)</math> are indeed polynomials, consider the simple example of <math>p_0(x)=x^3.</math> Then,\n\n:<math>\\begin{align} q_0(x) &{}\n= \\int_\\mathbb{R} \\! \\frac{t^3 - x^3}{t - x} \\rho(t)\\,dt \\\\\n&{}\n= \\int_\\mathbb{R} \\! \\frac{(t - x)(t^2+tx+x^2)}{t - x} \\rho(t)\\,dt \\\\\n&{}\n= \\int_\\mathbb{R} \\! (t^2+tx+x^2)\\rho(t)\\,dt \\\\\n&{}\n= \\int_\\mathbb{R} \\! t^2\\rho(t)\\,dt\n+ x\\int_\\mathbb{R} \\! t\\rho(t)\\,dt\n+ x^2\\int_\\mathbb{R} \\! \\rho(t)\\,dt\n\\end{align}</math>\n\nwhich is a polynomial <math>x</math> provided that the three integrals in <math>t</math> (the [[Moment (mathematics)|moments]] of the density <math>\\rho</math>) are convergent.\n\n==See also==\n\n* [[Secondary measure]]\n\n[[Category:Polynomials]]\n{{math-stub}}"
    },
    {
      "title": "Separable polynomial",
      "url": "https://en.wikipedia.org/wiki/Separable_polynomial",
      "text": "In [[mathematics]], a [[polynomial]] ''P''(''X'') over a given [[field (mathematics)|field]] ''K'' is '''separable''' if its roots are [[Distinct (mathematics)|distinct]] in an [[algebraic closure]] of ''K'', that is, the number of distinct roots is equal to the degree of the polynomial.<ref>S. Lang, Algebra, p. 178</ref>\n\nThis concept is closely related to [[square-free polynomial]]. If ''K'' is a [[perfect field]] then the two concepts coincide. In general, ''P''(''X'') is separable if and only if it is [[square-free polynomial|square-free]] over any field that contains ''K'',\nwhich holds if and only if ''P''(''X'') is [[coprime]] to its [[formal derivative]] ''D P''(''X'').\n\n==Older definition==\nIn an older definition, ''P''(''X'') was considered separable if each of its irreducible factors in K[X] is separable in the modern definition.<ref>N. Jacobson, Basic Algebra I, p. 233</ref> In this definition, separability depended on the field ''K'', for example, any polynomial over a [[perfect field]] would have been considered separable. This definition, although it can be convenient for Galois theory, is no longer in use.\n\n==Separable field extensions==\nSeparable polynomials are used to define [[separable extension]]s: A field extension {{math|''K'' ⊂ ''L''}} is a separable extension if and only if for every {{math|''α'' ∈ ''L''}}, which is algebraic over {{mvar|K}}, the [[minimal polynomial (field theory)|minimal polynomial]] of  {{math|''α''}} over {{mvar|K}} is a separable polynomial.\n\n[[Inseparable extension]]s (that is extensions which are not separable) may occur only in [[characteristic p|characteristic {{mvar|p}}]]. \n\nThe criterion above leads to the quick conclusion that if ''P'' is irreducible and not separable, then ''D P''(''X'')=0.\nThus we must have\n:''P''(''X'') = ''Q''(X<sup>''p''</sup>) \nfor some polynomial ''Q'' over ''K'', where the prime number ''p'' is the characteristic.\n\nWith this clue we can construct an example:\n:''P''(''X'') = ''X''<sup>''p''</sup> &minus; ''T'' \nwith ''K'' the field of [[rational function]]s in the indeterminate ''T'' over the finite field with ''p'' elements. Here one can prove directly that ''P''(''X'') is irreducible, and not separable. This is actually a typical example of why ''inseparability'' matters; in geometric terms ''P'' represents the mapping on the [[projective line]] over the finite field, taking co-ordinates to their ''p''th power. Such mappings are fundamental to the [[algebraic geometry]] of finite fields. Put another way, there are coverings in that setting that cannot be 'seen' by Galois theory. (See [[radical morphism]] for a higher-level discussion.)\n\nIf ''L'' is the field extension \n\n:''K''(''T''<sup>1/''p''</sup>), \n\nin other words the [[splitting field]] of ''P'', then ''L''/''K'' is an example of a [[purely inseparable field extension]]. It is of degree ''p'', but has no [[automorphism]] fixing ''K'', other than the identity, because ''T''<sup>1/''p''</sup> is the unique root of ''P''. This shows directly that Galois theory must here break down. A field such that there are no such extensions is called ''perfect''. That finite fields are perfect follows ''a posteriori'' from their known structure.\n\nOne can show that the [[tensor product of fields]] of ''L'' with itself over ''K'' for this example has [[nilpotent]] elements that are non-zero. This is another manifestation of inseparability: that is, the tensor product operation on fields need not produce a ring that is a product of fields (so, not a commutative [[semisimple ring]]).\n\nIf ''P''(''x'') is separable, and its roots form a [[group (mathematics)|group]] (a subgroup of the field ''K''), then ''P''(''x'') is an [[additive polynomial]].\n\n==Applications in Galois theory==\nSeparable polynomials occur frequently in [[Galois theory]].\n\nFor example, let ''P'' be an irreducible polynomial with integer coefficients and ''p'' be a prime number which does not divide the leading coefficient of ''P''. Let ''Q'' be the polynomial over the [[finite field]] with ''p'' elements, which is obtained by reducing [[modulo (jargon)|modulo]] ''p'' the coefficients of ''P''. Then, if ''Q'' is separable (which is the case for every ''p'' but a finite number) then the degrees of the irreducible factors of ''Q'' are the lengths of the [[cyclic permutation|cycles]] of some [[permutation]] of the [[Galois group]] of ''P''. \n\nAnother example: ''P'' being as above, a '''resolvent''' ''R'' for a [[Group (mathematics)|group]] ''G'' is a  polynomial whose coefficients are polynomials in the coefficients of ''P'', which provides some information on the [[Galois group]] of ''P''. More precisely, if ''R'' is separable and has a rational root then the [[Galois group]] of ''P'' is contained in ''G''. For example, if ''D'' is the [[discriminant]] of ''P'' then <math>X^2-D</math> is a resolvent for the [[alternating group]]. This resolvent is always separable (assuming the characteristic is not 2) if ''P'' is irreducible, but most resolvents are not always separable.\n\n==See also==\n*[[Frobenius endomorphism]]\n\n==References==\n{{Reflist}}\n* Pages 240-241 of {{Lang Algebra|edition=3}}\n\n[[Category:Field theory]]\n[[Category:Polynomials]]"
    },
    {
      "title": "Septic equation",
      "url": "https://en.wikipedia.org/wiki/Septic_equation",
      "text": "{{Other uses|Septic (disambiguation){{!}}Septic}}\n[[Image:Septic graph.svg|thumb|right|233px|Graph of a polynomial of degree 7, with 7 [[real number|real]] [[root of a polynomial|roots]] (crossings of the {{math|''x''}} axis) and 6 [[critical point (mathematics)|critical points]]. Depending on the number and vertical location of the [[minimum|minima and maxima]], the septic could have 7, 5, 3, or 1 real root; the number of [[complex number|complex]] roots is 7 minus the number of real roots.  ]]\n\nIn [[algebra]], a '''septic equation''' is an [[equation]] of the form\n\n:<math>ax^7+bx^6+cx^5+dx^4+ex^3+fx^2+gx+h=0,\\,</math>\n\nwhere {{math|''a'' ≠ 0}}.\n\nA '''septic function''' is a [[Function (mathematics)|function]] of the form\n\n:<math>f(x)=ax^7+bx^6+cx^5+dx^4+ex^3+fx^2+gx+h\\,</math>\n\nwhere {{math|''a'' ≠ 0}}. In other words, it is a [[polynomial]] of [[Degree of a polynomial|degree]] seven. If {{math|1=''a'' = 0}}, then ''f'' is a [[sextic function]] ({{math|''b'' ≠ 0}}), [[quintic function]] ({{math|1=''b'' = 0, ''c'' ≠ 0}}), etc.\n\nThe equation may be obtained from the function by setting {{math|1=''f''(''x'') = 0}}.\n\nThe ''coefficients'' {{math|''a'', ''b'', ''c'', ''d'', ''e'', ''f'', ''g'', ''h''}} may be either [[integers]], [[rational number]]s, [[real number]]s, [[complex number]]s or, more generally, members of any [[field (mathematics)|field]].\n\nBecause they have an odd degree, '''septic functions''' appear similar to [[quintic function|quintic]] or [[cubic function]] when graphed, except they may possess additional [[Maxima and minima|local maxima]] and local minima (up to three maxima and three minima). The [[derivative]] of a septic function is a [[sextic function]].\n\n==Solvable septics==\nSome seventh degree equations can be solved by factorizing into [[radical expression|radicals]], but other septics cannot. [[Évariste Galois]] developed techniques for determining whether a given equation could be solved by radicals which gave rise to the field of [[Galois theory]].  To give an example of an irreducible but solvable septic, one can generalize the solvable [[de Moivre]] [[quintic]] to get,\n:<math>x^7+7\\alpha x^5+14\\alpha^2x^3+7\\alpha^3x+\\beta = 0\\,</math>,\n\nwhere the auxiliary equation is \n:<math>y^2+\\beta y-\\alpha^7 = 0\\,</math>.\n\nThis means that the septic is obtained by eliminating {{math|''u''}} and {{math|''v''}} between {{math|1=''x'' = ''u'' + ''v''}},  {{math|1=''uv'' + ''α'' = 0}} and {{math|1=''u''<sup>7</sup> + ''v''<sup>7</sup> + ''β'' = 0}}.\n\nIt follows that the septic's seven roots are given by\n\n:<math>x_k = \\omega_k\\sqrt[7]{y_1} + \\omega_k^6\\sqrt[7]{y_2}</math>\n\nwhere {{math|''ω<sub>k</sub>''}} is any of the 7 seventh [[root of unity|roots of unity]]. The [[Galois group]] of this septic is the maximal solvable group of order 42. This is easily generalized to any other degrees {{math|''k''}}, not necessarily prime.\n\nAnother solvable family is,\n\n:<math>x^7-2x^6+(\\alpha+1)x^5+(\\alpha-1)x^4-\\alpha x^3-(\\alpha+5)x^2-6x-4 = 0\\,</math>\n\nwhose members appear in Kluner's ''Database of Number Fields''.  Its [[discriminant]] is\n\n:<math>\\Delta = -4^4\\left(4\\alpha^3+99\\alpha^2-34\\alpha+467\\right)^3\\,</math>\n\nThe [[Galois group]] of these septics is the [[dihedral group]] of order 14.\n\nThe general septic equation can be solved with the [[alternating group|alternating]] or [[symmetric group|symmetric]] [[Galois group]]s {{math|''A''<sub>7</sub>}} or {{math|''S''<sub>7</sub>}}.<ref name=\"BeyondQuartic\"/> Such equations require [[hyperelliptic function]]s and associated [[theta function]]s of [[genus (mathematics)|genus]] 3 for their solution.<ref name=\"BeyondQuartic\"/> However, these equations were not studied specifically by the nineteenth-century mathematicians studying the solutions of algebraic equations, because the [[sextic equation]]s' solutions were already at the limits of their computational abilities without computers.<ref name=\"BeyondQuartic\">{{citation|url=https://books.google.com/books?id=9cKX_9zkeg4C&pg=PA143&lpg=PA143&dq=septic+equation&source=bl&ots=nld9eMx3DE&sig=wZ9V5zL0vNqsJvCguye-NCzqhq0&hl=en&ei=aF4oS570JdGHkQWd-936DA&sa=X&oi=book_result&ct=result&resnum=7&ved=0CDMQ6AEwBg#v=onepage&q=septic%20equation&f=false |author=R. Bruce King |title=Beyond the Quartic Equation |publisher= Birkhaüser|page=  143 and 144}}</ref>\n\nSeptics are the lowest order equations for which it is not obvious that their solutions may be obtained by superimposing ''continuous functions'' of two variables.  [[Hilbert's thirteenth problem|Hilbert's 13th problem]] was the conjecture this was not possible in the general case for seventh-degree equations. [[Vladimir Arnold]] solved this in 1957, demonstrating that this was always possible.<ref>{{citation |url=https://books.google.com/books?id=SpTv44Ia-J0C&pg=PA254 |title=Kolmogorov's heritage in mathematics |author=Vasco Brattka |chapter=Kolmogorov's Superposition Theorem|publisher=Springer}}</ref>  However, Arnold himself considered the ''genuine'' Hilbert problem to be whether for septics their solutions may be obtained by superimposing ''algebraic functions'' of two variables (the problem still being open).<ref>{{citation |url=http://www.pdmi.ras.ru/~arnsem/Arnold/arnlect1.ps.gz |title=From Hilbert's Superposition Problem to Dynamical Systems |author=V.I. Arnold |page=4}}</ref>\n\n==Galois groups==\n[[Image:Fano plane.svg|thumb|[[Fano plane]]]]\n*Septic equations solvable by radicals have a [[Galois group]] which is either the [[cyclic group]] of order 7, or the [[dihedral group]] of order 14 or a [[metacyclic group]] of order 21 or 42.<ref name=\"BeyondQuartic\"/>\n*The {{math|''L''(3, 2)}} [[Galois group]] (of order 168) is formed by the [[permutations]] of the 7 vertex labels which preserve the 7 \"lines\" in the [[Fano plane]].<ref name=\"BeyondQuartic\"/> Septic equations with this [[Galois group]] {{math|''L''(3, 2)}} require [[elliptic function]]s but not [[hyperelliptic function]]s for their solution.<ref name=\"BeyondQuartic\"/>\n*Otherwise the Galois group of a septic is either the [[alternating group]] of order 2520 or the [[symmetric group]] of order 5040.\n\n==Septic equation for the squared area of a cyclic pentagon or hexagon==\n\nThe square of the area of a [[Pentagon#Cyclic pentagons|cyclic pentagon]] is a root of a septic equation whose coefficients are [[symmetric function]]s of the sides of the pentagon.<ref>Weisstein, Eric W. \"Cyclic Pentagon.\" From MathWorld--A Wolfram Web Resource. [http://mathworld.wolfram.com/CyclicPentagon.html]</ref> The same is true of the square of the area of a [[Hexagon#Cyclic hexagon|cyclic hexagon]].<ref>Weisstein, Eric W. \"Cyclic Hexagon.\" From MathWorld--A Wolfram Web Resource. [http://mathworld.wolfram.com/CyclicHexagon.html]</ref>\n\n==See also==\n*[[Cubic function]]\n*[[Quartic function]]\n*[[Quintic function]]\n*[[Sextic equation]]\n*[[Labs septic]]\n\n==References==\n<references/>\n\n{{Polynomials}}\n\n{{DEFAULTSORT:Septic Equation}}\n[[Category:Equations]]\n[[Category:Galois theory]]\n[[Category:Polynomials]]"
    },
    {
      "title": "Sextic equation",
      "url": "https://en.wikipedia.org/wiki/Sextic_equation",
      "text": "[[Image:Sextic Graph.svg|thumb|right|233px|Graph of a sextic function, with 6 [[real number|real]] [[root of a function|roots]] (crossings of the {{math|''x''}} axis) and 5 [[critical point (mathematics)|critical points]]. Depending on the number and vertical locations of [[minimum|minima and maxima]], the sextic could have 6, 4, 2, or no real roots. The number of [[complex number|complex]] roots equals 6 minus the number of real roots.]]\nIn [[algebra]], a '''sextic''' (or '''hexic''') '''polynomial''' is a [[polynomial]] of [[Degree of a polynomial|degree]] six. \nA '''sextic equation''' is a [[polynomial equation]] of degree six—that is, an [[equation]] whose left hand side is a sextic polynomial and whose right hand side is zero. More precisely, it has the form:\n\n:<math>ax^6+bx^5+cx^4+dx^3+ex^2+fx+g=0,\\,</math>\n\nwhere {{math|''a'' ≠ 0}} and the ''coefficients'' {{math| ''a'', ''b'', ''c'', ''d'', ''e'', ''f'', ''g''}} may be [[integers]], [[rational number]]s, [[real number]]s, [[complex number]]s or, more generally, members of any [[field (mathematics)|field]].\n\nA '''sextic function''' is a [[function (mathematics)|function]] defined by a sextic polynomial. Because they have an even degree, sextic functions appear similar to [[quartic function]]s when graphed, except they may possess an additional [[Maxima and minima|local maximum]] and local minimum each. The [[derivative]] of a sextic function is a [[quintic function]].\n\nSince a sextic function is defined by a polynomial with even degree, it has the same infinite limit when the argument goes to positive or negative [[infinity]]. If the [[leading coefficient]] {{math|''a''}} is positive, then the function increases to positive infinity at both sides and thus the function has a global minimum. Likewise, if {{math|''a''}} is negative, the sextic function decreases to negative infinity and has a global maximum.\n\n==Solvable sextics==\nSome sixth degree equations, such as {{math|1=''ax''<sup>6</sup> + ''dx''<sup>3</sup> + ''g'' = 0}}, can be solved by factorizing into radicals, but other sextics cannot. [[Évariste Galois]] developed techniques for determining whether a given equation could be solved by radicals which gave rise to the field of [[Galois theory]].<ref name=\"Mathworld - Sextic Equation\"/>\n\nIt follows from Galois theory that a sextic equation is solvable in term of radicals if and only if its [[Galois group]] is contained either in the group of order 48 which [[stabilizer (group theory)|stabilizes]] a partition of the set of the roots into three subsets of two roots or in the group of order 72 which stabilizes a partition of the set of the roots into two subsets of three roots.\n\nThere are formulas to test either case, and, if the equation is solvable, compute the roots in term of radicals.<ref>T. R. Hagedorn, ''General formulas for solving solvable sextic equations'', J. Algebra '''233''' (2000), 704-757</ref>\n\nThe general sextic equation can be solved in terms of [[Kampé de Fériet function]]s.<ref name=\"Mathworld - Sextic Equation\">[http://mathworld.wolfram.com/SexticEquation.html Mathworld - Sextic Equation]</ref> A more restricted class of sextics can be solved in terms of [[hypergeometric functions|generalised hypergeometric functions]] in one variable using [[Felix Klein]]'s approach to solving the [[quintic equation]].<ref name=\"Mathworld - Sextic Equation\"/>\n\n==Examples==\n\n[[Watt's curve]], which arose in the context of early work on the [[steam engine]], is a sextic in two variables.\n\nOne method of solving the [[cubic function#Vieta's substitution|cubic equation]] involves transforming variables to obtain a sextic equation having terms only of degrees 6, 3, and 0, which can be solved as a quadratic equation in the cube of the variable.\n\n== Etymology ==\nThe describer \"sextic\" comes from the [[Latin]] [[prefix]] for 6 or 6th (\"sexa-\"), and the [[Greek language|Greek]] [[suffix]] meaning \"pertaining to\" (\"-tic\"). The much less common \"hexic\" uses Greek for both its prefix (\"hex\") and its suffix (\"-tic\"). In both cases, the suffix refers to the degree of the function. Often times, these type of functions will simply be referred to as \"6th degree functions\".\n\n==See also==\n*[[Cayley's sextic]]\n*[[Cubic function]]\n*[[Septic equation]]\n\n==References==\n\n<references/>\n\n{{Polynomials}}\n\n{{DEFAULTSORT:Sextic Equation}}\n[[Category:Equations]]\n[[Category:Galois theory]]\n[[Category:Polynomials]]"
    },
    {
      "title": "Shapiro polynomials",
      "url": "https://en.wikipedia.org/wiki/Shapiro_polynomials",
      "text": "In mathematics, the '''Shapiro polynomials''' are a [[polynomial sequence|sequence of polynomials]] which were first studied by [[Harold S. Shapiro]] in 1951 when considering the magnitude of specific [[trigonometric sum]]s.<ref>{{Cite journal|title=Note on the Shapiro polynomials|author=John Brillhart and L. Carlitz|journal= Proceedings of the American Mathematical Society|volume=25|date=May 1970|pages=114–118|doi=10.2307/2036537|issue= 1|publisher=Proceedings of the American Mathematical Society, Vol. 25, No. 1|postscript=<!--None-->|jstor=2036537}}</ref> In [[signal processing]], the Shapiro polynomials have good [[autocorrelation]] properties and their values on the [[unit circle]] are small.<ref>{{Cite journal|url=http://ieeexplore.ieee.org/Xplore/login.jsp?url=/iel5/2220/4236720/04236729.pdf?arnumber=4236729|title=Binary sequences with good correlation properties|author=Somaini, U.|journal=Electronics Letters|volume=11|issue=13|date=June 26, 1975|pages=278–279|doi=10.1049/el:19750211|postscript=<!--None-->}}</ref>  The first few members of the sequence are:\n\n:<math>\n\\begin{align}\nP_1(x) & {} =1 + x \\\\\nP_2(x) & {} =1 + x + x^2 - x^3 \\\\\nP_3(x) & {} =1 + x + x^2 - x^3 + x^4 + x^5 - x^6 + x^7 \\\\\n... \\\\\nQ_1(x) & {} =1 - x \\\\\nQ_2(x) & {} =1 + x - x^2 + x^3 \\\\\nQ_3(x) & {} =1 + x + x^2 - x^3 - x^4 - x^5 + x^6 - x^7 \\\\\n... \\\\\n\\end{align}\n</math>\n\nwhere the second sequence, indicated by ''Q'', is said to be ''complementary'' to the first sequence, indicated by ''P''.\n\n==Construction==\nThe Shapiro polynomials ''P''<sub>''n''</sub>(''z'') may be constructed from the [[Rudin–Shapiro sequence|Golay–Rudin–Shapiro sequence]] ''a''<sub>''n''</sub>, which  equals 1 if the number of pairs of consecutive ones in the binary expansion of ''n'' is even, and &minus;1 otherwise. Thus ''a''<sub>0</sub>&nbsp;=&nbsp;1,  ''a''<sub>1</sub>&nbsp;=&nbsp;1, ''a''<sub>2</sub>&nbsp;=&nbsp;1, ''a''<sub>3</sub>&nbsp;=&nbsp;&minus;1, etc.\n\nThe first Shapiro ''P''<sub>''n''</sub>(''z'') is the partial sum of order 2<sup>''n''</sup>&nbsp;&minus;&nbsp;1  (where ''n''&nbsp;=&nbsp;0,&nbsp;1,&nbsp;2,&nbsp;...) of the power series\n\n:''f''(''z'') := ''a''<sub>0</sub>  +  ''a''<sub>1</sub>&thinsp;''z''  +  a<sub>2</sub>&thinsp;''z''<sup>2</sup>  +  ...\n\nThe Golay–Rudin–Shapiro sequence {''a''<sub>''n''</sub>} has a fractal-like structure &ndash; for example, ''a''<sub>''n''</sub>&nbsp;=&nbsp;''a''<sub>2''n''</sub> &ndash;  which implies that the subsequence (''a''<sub>0</sub>,&nbsp;''a''<sub>2</sub>,&nbsp;''a''<sub>4</sub>,&nbsp;...) replicates the original sequence {''a''<sub>''n''</sub>}. This in turn leads to remarkable\nfunctional equations satisfied by ''f''(''z'').\n\nThe second or complementary Shapiro polynomials ''Q''<sub>''n''</sub>(''z'') may be defined in terms of this sequence, or by the relation ''Q''<sub>''n''</sub>(''z'') = (1-)<sup>''n''</sup>''z''<sup>2<sup>''n''</sup>-1</sup>''P''<sub>''n''</sub>(-1/''z''), or by the recursions\n\n:<math>P_0(z)=1; ~~ Q_0(z) = 1 ; </math>\n:<math>P_{n+1}(z) = P_n(z) + z^{2^n} Q_n(z) ; </math>\n:<math>Q_{n+1}(z) = P_n(z) - z^{2^n} Q_n(z) . </math>\n\n==Properties==\n[[File:Rudin shapiro 8 zeros.svg|thumbnail|right|Zeroes of the polynomial of degree 255]]\nThe sequence of complementary polynomials ''Q''<sub>''n''</sub> corresponding to the ''P''<sub>''n''</sub> is uniquely characterized by the following properties:\n* (i) ''Q''<sub>''n''</sub> is of degree 2<sup>''n''</sup> &minus; 1;\n* (ii) the coefficients of ''Q''<sub>''n''</sub> are all 1 or &minus;1, and its constant term equals 1; and\n* (iii) the identity  |''P''<sub>''n''</sub>(''z'')|<sup>2</sup>  +  |''Q''<sub>''n''</sub>(''z'')|<sup>2</sup>  =  2<sup>(''n''&thinsp;+&thinsp;1)</sup> holds on the unit circle, where the complex variable ''z'' has absolute value one.\n\nThe most interesting property of the {''P''<sub>''n''</sub>} is that the absolute value of ''P''<sub>''n''</sub>(''z'') is bounded on the unit circle by the [[square root of 2]]<sup>(''n''&thinsp;+&thinsp;1)</sup>, which is on the order\nof the [[L2 norm|L<sup>2</sup> norm]] of ''P''<sub>''n''</sub>. Polynomials with coefficients from the set {&minus;1,&nbsp;1} whose maximum modulus on the unit circle is close to their mean modulus are useful for various applications in communication theory (e.g., antenna design and [[data compression]]).  Property (iii) shows that (''P'',&nbsp;''Q'') form a [[Golay pair]].\n\nThese polynomials have further properties:<ref>{{cite journal | author=J. Brillhart |author2=J.S. Lomont |author3=P. Morton | title=Cyclotomic properties of the Rudin–Shapiro polynomials | journal=[[J. Reine Angew. Math.]] | volume=288 | year=1976 | pages=37–65 }}</ref>\n:<math> P_{n+1}(z) = P_n(z^2) + z P_n(-z^2) ; \\, </math>\n:<math> Q_{n+1}(z) = Q_n(z^2) + z Q_n(-z^2) ; \\, </math>\n:<math>P_n(z) P_n(1/z) + Q_n(z) Q_n(1/z) = 2^{n+1} ; \\, </math>\n:<math>P_{n+k+1}(z) = P_n(z)P_k(z^{2^{n+1}}) + z^{2^n}Q_n(z)P_k(-z^{2^{n+1}}) ; \\, </math>\n:<math>P_n(1) = 2^{\\lfloor (n+1)/2 \\rfloor}; {~}{~} P_n(-1) = (1+(-1)^n)2^{\\lfloor n/2 \\rfloor - 1} . \\, </math>\n\n==See also==\n* [[Littlewood polynomial]]s\n\n==Notes==\n{{reflist}}\n\n==References==\n*{{cite book|last = Borwein|first = Peter B|authorlink=Peter Borwein|title = Computational Excursions in Analysis and Number Theory|publisher = Springer|year = 2002|isbn = 978-0-387-95444-8|url = https://books.google.com/books?id=A_ITwN13J6YC|accessdate = 2007-03-30}} Chapter 4.\n* {{cite book | zbl=0724.11010 | last=Mendès France | first=Michel | chapter=The Rudin-Shapiro sequence, Ising chain, and paperfolding | pages=367–390 | editor1-last=Berndt | editor1-first=Bruce C. | editor1-link=Bruce C. Berndt | editor2-last=Diamond | editor2-first=Harold G. | editor3-last=Halberstam | editor3-first=Heini | editor3-link=Heini Halberstam |display-editors = 3 | editor4-last=Hildebrand | editor4-first=Adolf | title=Analytic number theory. Proceedings of a conference in honor of Paul T. Bateman, held on April 25-27, 1989, at the University of Illinois, Urbana, IL (USA) | series=Progress in Mathematics | volume=85 | location=Boston | publisher=Birkhäuser | year=1990 | isbn=978-0-8176-3481-0 }}\n\n[[Category:Fourier analysis]]\n[[Category:Digital signal processing]]\n[[Category:Polynomials]]"
    },
    {
      "title": "Sheffer sequence",
      "url": "https://en.wikipedia.org/wiki/Sheffer_sequence",
      "text": "{{Use American English|date = March 2019}}\n{{Short description|Type of polynomial sequence}}\nIn [[mathematics]], a '''Sheffer sequence''' or '''poweroid''' is a [[polynomial sequence]], i.e., a sequence {{math|1={''p''<sub>n</sub>(''x'') : ''n'' = 0, 1, 2, 3, ... <nowiki>}</nowiki>}} of [[polynomial]]s in which the index of each polynomial equals its [[Degree of a polynomial|degree]], satisfying conditions related to the [[umbral calculus]] in combinatorics. They are named for [[Isador M. Sheffer]].\n\n==Definition==\nFix a polynomial sequence ''p''<sub>''n''</sub>. Define a linear operator ''Q'' on polynomials in ''x'' by\n\n:<math>Qp_n(x) = np_{n-1}(x)\\, .</math>\n\nThis determines ''Q'' on all polynomials. The polynomial sequence ''p''<sub>''n''</sub> is a ''Sheffer sequence'' if the linear operator ''Q'' just defined is ''shift-equivariant''. Here, we define a linear operator ''Q'' on polynomials to be ''shift-equivariant'' if, whenever ''f''(''x'') = ''g''(''x'' + ''a'') = ''T''<sub>''a''</sub> ''g''(''x'') is a \"shift\" of ''g''(''x''), then (''Qf'')(''x'') = (''Qg'')(''x'' + ''a''); i.e., ''Q'' commutes with every [[shift operator]]: ''T''<sub>''a''</sub>''Q'' =''QT''<sub>''a''</sub>. Such a ''Q'' is a [[delta operator]].\n\n==Properties==\n\nThe set of all Sheffer sequences is a [[group (mathematics)|group]] under the operation of '''umbral composition''' of polynomial sequences, defined as follows. Suppose {&nbsp;''p''<sub>''n''</sub>(x) : ''n'' = 0, 1, 2, 3,&nbsp;...&nbsp;} and {&nbsp;''q''<sub>''n''</sub>(x) : ''n'' = 0, 1, 2, 3,&nbsp;...&nbsp;} are polynomial sequences, given by\n\n:<math>p_n(x)=\\sum_{k=0}^n a_{n,k}x^k\\ \\mbox{and}\\ q_n(x)=\\sum_{k=0}^n b_{n,k}x^k.</math>\n\nThen the umbral composition <math>p \\circ q</math> is the polynomial sequence whose ''n''th term is\n\n:<math>(p_n\\circ q)(x)=\\sum_{k=0}^n a_{n,k}q_k(x)=\\sum_{0\\le k \\le \\ell \\le n} a_{n,k}b_{k,\\ell}x^\\ell</math>\n\n(the subscript ''n'' appears in ''p''<sub>''n''</sub>, since this is the ''n'' term of that sequence, but not in ''q'', since this refers to the sequence as a whole rather than one of its terms).\n\nThe neutral element of this group is the standard monomial basis\n\n:<math>e_n(x) = x^n = \\sum_{k=0}^n \\delta_{n,k} x^k.</math>\n\nTwo important subgroups are the group of [[Appell sequence]]s, which are those sequences for which the operator ''Q'' is mere differentiation, and the group of sequences of [[binomial type]], which are those that satisfy the identity\n:<math>p_n(x+y)=\\sum_{k=0}^n{n \\choose k}p_k(x)p_{n-k}(y).</math>\nA Sheffer sequence {&nbsp;''p''<sub>''n''</sub>(''x'')&nbsp;: ''n''&nbsp;=&nbsp;0,&nbsp;1,&nbsp;2,&nbsp;.&nbsp;.&nbsp;.&nbsp;} is of binomial type if and only if both\n\n:<math>p_0(x) = 1\\,</math>\n\nand\n\n:<math>p_n(0) = 0\\mbox{ for } n \\ge 1. \\,</math>\n\nThe group of Appell sequences is [[abelian group|abelian]]; the group of sequences of binomial type is not. The group of Appell sequences is a [[normal subgroup]]; the group of sequences of binomial type is not. The group of Sheffer sequences is a [[semidirect product]] of the group of Appell sequences and the group of sequences of binomial type. It follows that each [[coset]] of the group of Appell sequences contains exactly one sequence of binomial type. Two Sheffer sequences are in the same such coset if and only if the operator ''Q'' described above &ndash; called the \"[[delta operator]]\" of that sequence &ndash; is the same linear operator in both cases. (Generally, a ''delta operator'' is a shift-equivariant linear operator on polynomials that reduces degree by one. The term is due to F. Hildebrandt.)\n\nIf ''s''<sub>''n''</sub>(''x'') is a Sheffer sequence and ''p''<sub>''n''</sub>(''x'') is the one sequence of binomial type that shares the same delta operator, then\n\n:<math>s_n(x+y)=\\sum_{k=0}^n{n \\choose k}p_k(x)s_{n-k}(y).</math>\n\nSometimes the term ''Sheffer sequence'' is ''defined'' to mean a sequence that bears this relation to some sequence of binomial type. In particular, if {&nbsp;''s''<sub>''n''</sub>(''x'') } is an Appell sequence, then\n\n:<math>s_n(x+y)=\\sum_{k=0}^n{n \\choose k}x^ks_{n-k}(y).</math>\n\nThe sequence of [[Hermite polynomials]], the sequence of [[Bernoulli polynomials]], and the [[monomial]]s { ''x<sup>n</sup>'' : ''n'' = 0, 1, 2, ... } are examples of Appell sequences.\n\nA Sheffer sequence ''p''<sub>''n''</sub> is characterised by its [[exponential generating function]]\n\n:<math> \\sum_{n=0}^\\infty \\frac{p_n(x)}{n!} t^n = A(t) \\exp(x B(t)) \\, </math>\n\nwhere ''A'' and ''B'' are (formal) power series in ''t''. Sheffer sequences are thus examples of [[generalized Appell polynomials]] and hence have an associated [[recurrence relation]].\n\n==Examples==\nExamples of polynomial sequences which are Sheffer sequences include:\n* The [[Abel polynomials]];\n* The [[Bernoulli polynomials]];\n* The central factorial polynomials;\n* The [[Hermite polynomials]];\n* The [[Laguerre polynomials]];\n* The [[Mahler polynomials]];\n* The [[monomial]]s { ''x<sup>n</sup>'' : ''n'' = 0, 1, 2, ... } ;\n* The [[Mott polynomials]];\n\n==References==\n*{{cite journal |last1=Rota |first1=G.-C. |authorlink1=Gian-Carlo Rota |last2=Kahaner |first2=D. |last3=Odlyzko |first3=A. |authorlink3=Andrew Odlyzko |title=On the Foundations of Combinatorial Theory VIII: Finite Operator Calculus |journal=Journal of Mathematical Analysis and Applications |volume=42 |issue=3 |date=June 1973 |pages=684–750 |doi=10.1016/0022-247X(73)90172-8}} Reprinted in the next reference.\n*{{cite book |last1=Rota |first1=G.-C. |authorlink1=Gian-Carlo Rota |last2=Doubilet |first2=P. |last3=Greene |first3=C. |last4=Kahaner |first4=D. |last5=Odlyzko |first5=A. |last6=Stanley |first6=R. |title=Finite Operator Calculus |publisher=Academic Press |year=1975 |isbn=0-12-596650-4}}\n*{{cite journal |last=Sheffer |first=I. M. |authorlink=Isador M. Sheffer |title=Some Properties of Polynomial Sets of Type Zero |journal=[[Duke Mathematical Journal]] |volume=5 |issue=3 |pages=590–622 |year=1939 |doi=10.1215/S0012-7094-39-00549-1}}\n*{{Cite book |last=Roman |first=Steven |title=The Umbral Calculus |publisher=Academic Press Inc. [Harcourt Brace Jovanovich Publishers] |location=London |series=Pure and Applied Mathematics |isbn=978-0-12-594380-2 |mr=741185  |year=1984 |volume=111 |url=https://books.google.com/books?id=JpHjkhFLfpgC}} Reprinted by Dover, 2005.\n\n==External links==\n*{{MathWorld|ittle=Sheffer Sequence|id=ShefferSequence}}\n\n[[Category:Polynomials]]\n[[Category:Factorial and binomial topics]]"
    },
    {
      "title": "Sister Celine's polynomials",
      "url": "https://en.wikipedia.org/wiki/Sister_Celine%27s_polynomials",
      "text": "In mathematics, [[Religious sister|'''Sister''']] '''Celine's polynomials''' are a family of hypergeometric polynomials introduced by {{harvs|txt|authorlink=Mary Celine Fasenmyer|first=Mary Celine |last=Fasenmyer|year=1947}}. They include [[Legendre polynomials]], [[Jacobi polynomials]], and [[Bateman polynomials]] as special cases.\n\n==References==\n\n{{Citation | last1=Fasenmyer | first1=Mary Celine | title=Some generalized hypergeometric polynomials | url=http://www.ams.org/journals/bull/1947-53-08/S0002-9904-1947-08893-5/home.html | doi=10.1090/S0002-9904-1947-08893-5  | mr=0022276 | year=1947 | journal=[[Bulletin of the American Mathematical Society]] | issn=0002-9904 | volume=53 | issue=8 | pages=806–812}}\n\n[[Category:Polynomials]]"
    },
    {
      "title": "Square-free polynomial",
      "url": "https://en.wikipedia.org/wiki/Square-free_polynomial",
      "text": "In [[mathematics]], a '''square-free polynomial''' is a [[polynomial]] defined over a [[field (mathematics)|field]] (or more generally, a [[unique factorization domain]]) that does not have as a factor any square of a non-[[unit (ring theory)|unit]] factor. In the important case of univariate polynomials over a field {{math|''k''}}, this means that <math>f \\in k[X]</math> is square-free if and only if <math>b^2 \\nmid f</math> for every polynomial <math>b \\in k[X]</math> of positive degree.<ref name=\"yun\">Yun, David Y.Y. (1976). [https://dx.doi.org/10.1145/800205.806320 On square-free decomposition algorithms] ''SYMSAC '76 Proceedings of the third ACM symposium on Symbolic and algebraic computation'', pp. 26–35.</ref> In applications in physics and engineering, a square-free polynomial is commonly called a '''polynomial with no repeated roots'''. Such polynomials are called [[separable polynomial|separable]], but over a perfect field being separable is the same as being square-free.\n\nA '''square-free decomposition''' or '''square-free factorization''' of a polynomial is a factorization into powers of square-free factors\n:<math>\nf = a_1 a_2^2 a_3^3 \\cdots a_n^n =\\prod_{k=1}^n a_k^k\\,\n</math>\nwhere those of the {{math|''a''<sub>''k''</sub>}} that are not equal to 1 are [[pairwise coprime]] square-free polynomials.<ref name=\"yun\"/> Every non-zero polynomial with coefficients in a [[field (mathematics)|field]] admits a square-free factorization, which is unique [[up to]] the multiplication of the factors by non-zero constants. The square-free factorization is much easier to compute than the complete [[polynomial factorization|factorization]] into [[irreducible polynomial|irreducible]] factors, and is thus often preferred when the complete factorization is not really needed, as for the [[partial fraction]] decomposition and the [[symbolic integration]] of [[rational fraction]]s. Square-free factorization is the first step of the [[polynomial factorization]] algorithms which are implemented in [[computer algebra system]]s. Therefore, the algorithm of square-free factorization is basic in [[computer algebra]].\n\nIn the case of [[univariate]] polynomials over a field, any multiple factor of a polynomial introduces a nontrivial common factor of ''f'' and its [[formal derivative]] ''f''&nbsp;′, so a sufficient condition for ''f'' to be square-free is that the [[polynomial greatest common divisor|greatest common divisor]] of ''f'' and ''f''&nbsp;′ is&nbsp;1. This condition is also necessary over a field of characteristic 0 or, more generally, over a [[perfect field]], because over such a field, every irreducible polynomial is [[separable polynomial|separable]], and thus [[coprime]] with its derivative. \n\nOver a field of characteristic 0, the quotient of <math>f</math> by its GCD with its derivative is the product of the <math>a_i</math> in the above square-free decomposition. Over a perfect field of non-zero characteristic {{mvar|p}}, this quotient is the product of the <math>a_i</math> such that {{mvar|i}} is not a multiple of {{mvar|p}}. Further GCD computations and exact divisions allow computing the square-free factorization (see [[Factorization of polynomials over a finite field and irreducibility tests#Square-free factorization|square-free factorization over a finite field]]). In characteristic zero, a better algorithm is known, Yun's algorithm, which is described below.<ref name=\"yun\"/> Its [[computational complexity]] is, at most, twice that of the GCD computation of the input polynomial and its derivative. More precisely, if <math>T_{n}</math> is the time needed to compute the GCD of two polynomials of degree <math>n</math> and the quotient of these polynomial by the GCD, then <math>2T_{n}</math> is an upper bound for the time needed to compute the square free decomposition.\n\nThere are also known algorithms for the computation of the square-free decomposition of multivariate polynomials.<ref name=\"gianni\">Gianni P., Trager B. (1996). [https://dx.doi.org/10.1007/BF01613611 Square-Free Algorithms in Positive Characteristic] ''Applicable Algebra In Engineering, Communication And Computing'', 7(1), pp. 1–14.</ref>\n\n==Yun's algorithm==\nThis section describes Yun's algorithm for the square-free decomposition of univariate polynomials over a field of [[characteristic 0]].<ref name=\"yun\"/> It proceeds by a succession of [[polynomial greatest common divisor|GCD]] computations and exact divisions.\n\nThe input is thus a non-zero polynomial ''f'', and the first step of the algorithm consists of computing the GCD ''a''<sub>0</sub> of ''f'' and its [[formal derivative]] ''f'''. \n\nIf\n:<math>\nf = a_1 a_2^2 a_3^3 \\cdots a_k^k \n</math>\nis the desired factorization, we have thus  \n:<math>\na_0 =  a_2^1 a_3^2 \\cdots a_k^{k-1}, \n</math>\n:<math>\nf/a_0 = a_1 a_2 a_3 \\cdots a_k \n</math>\nand \n:<math>\nf'/a_0 =  \\sum_{i=1}^k i a_i' a_1 \\cdots a_{i-1} a_{i+1} \\cdots a_k.\n</math>\n\nIf we set <math>b_1=f/a_0</math>, <math>c_1=f'/a_0</math> and <math>d_1=c_1-b_1'</math>, we get that\n:<math>\n\\gcd(b_1,d_1) = a_1, \n</math>\n:<math>\nb_2=b_1/a_1 = a_2 a_3 \\cdots a_n, \n</math>\nand\n:<math>\nc_2=d_1/a_1 = \\sum_{i=2}^k (i-1) a_i' a_2 \\cdots a_{i-1} a_{i+1} \\cdots a_k.\n</math>\n\nIterating this process until <math>b_{k+1}=1</math>  we find all the <math>a_i.</math>\n\nThis is formalized into an algorithm as follows:\n{{quotation|\n<math> a_0:=\\gcd(f,f'); \\quad b_1:=f/a_0; \\quad c_1:= f'/a_0;\\quad d_1:=c_1-b_1'; \\quad i:=1;</math> \n<br>\nrepeat\n<br>\n<math>a_i:=\\gcd(b_i,d_i);\\quad b_{i+1}:=b_i/a_i; \\quad c_{i+1}:=d_i/a_i; \\quad i:=i+1; \\quad d_i := c_i - b_i'; </math>\n<br>\nuntil <math>b=1;</math>\n<br>\nOutput <math>a_1, \\ldots, a_{i-1}.</math>\n}}\n\nThe degree of <math>c_i</math> and <math>d_i</math> is one less than the degree of <math>b_i.</math> As <math>f</math> is the product of the <math>b_i,</math> the sum of the degrees of the <math>b_i</math> is the degree of <math>f.</math> As the complexity of GCD computations and divisions increase more than linearly with the degree, it follows that the total running time of the \"repeat\" loop is less than the running time of the first line of the algorithm, and that the total running time of Yun's algorithm is upper bounded by twice the time needed to compute the GCD of <math>f</math> and <math>f'</math> and the quotient of <math>f</math> and <math>f'</math> by their GCD.\n\n==Square root==\nIn general, a polynomial has no [[square root]]. More precisely, most polynomials cannot be written as the square of another polynomial.\n\nA polynomial has a square root if and only if all exponents of the square-free decomposition are even. In this case, the square root is obtained by dividing by 2 these exponents.\n\nThus the problem of deciding if a polynomial has a square root, and of computing it if it exists, is a special case of square-free factorization. \n\n==Notes==\n{{reflist}}\n\n{{DEFAULTSORT:Square-Free Polynomial}}\n[[Category:Polynomials]]\n[[Category:Computer algebra]]"
    },
    {
      "title": "Stability radius",
      "url": "https://en.wikipedia.org/wiki/Stability_radius",
      "text": "The stability radius  of an object (system, function, matrix, parameter) at a given nominal point is the radius of the largest [[ball (mathematics)|ball]],  centered at the nominal  point,  all of whose elements satisfy pre-determined stability conditions. The picture of this intuitive notion is this:\n\n[[Image:Radius of stability 1.png|500px]]\n\nwhere <math>\\hat{p}</math> denotes the nominal point,  <math>P</math> denotes the space of all possible values of the object <math>p</math>, and the shaded area, <math>P(s)</math>,  represents the set of points that satisfy the stability conditions. The radius of the blue circle, shown in red, is the stability radius.\n\n== Abstract definition ==\nThe formal definition of this concept varies,  depending on the application area. The following abstract definition is quite useful<ref name=\"zlobec09\">Zlobec S. (2009). Nondifferentiable optimization: Parametric programming. Pp. 2607-2615, in ''Encyclopedia of Optimization,'' Floudas C.A and Pardalos, P.M. editors, Springer.</ref><ref name=\"MS10\">Sniedovich, M. (2010). A bird's view of info-gap decision theory. ''Journal of Risk Finance,'' 11(3), 268-283.</ref>\n\n:<math>\\hat{\\rho}(\\hat{p}):= \\max \\ \\{\\rho\\ge 0: p\\in P(s), \\forall p\\in B(\\rho,\\hat{p})\\}</math>\n\nwhere <math>B(\\rho,\\hat{p})</math> denotes a closed  [[ball (mathematics)|ball]] of radius <math>\\rho</math> in <math>P</math> centered at <math>\\hat{p}</math>.\n\n== History ==\nIt looks like the concept was invented in the early 1960s.<ref name=\"wilf\">Wilf, H.S. (1960).  Maximally stable numerical integration.  ''Journal of the Society for Industrial and Applied Mathematics,'' 8(3),537-540.</ref><ref name=\"milne\">Milne, W.E.,  and Reynolds, R.R. (1962). Fifth-order methods for the numerical solution of ordinary differential equations.  ''Journal of the ACM,'' 9(1), 64-70.</ref> In the 1980s it became popular in control theory<ref name=\"Hindrichsen86\">Hindrichsen, D. and Pritchard, A.J. (1986).  Stability radii of linear systems, ''Systems and Control  Letters,'' 7, 1-10.</ref> and optimization.<ref name=\"zlobec88\">Zlobec S. (1988). Characterizing Optimality in Mathematical  Programming Models. ''Acta Applicandae  Mathematicae,''  12, 113-180.</ref> It is widely used as a model of local robustness against small perturbations in a given nominal value of the object of interest.\n\n== Relation to Wald's maximin model ==\nIt was shown<ref name=\"MS10\" /> that the stability radius model is an instance of [[Wald's maximin model]]. That is,\n\n:<math>\\max \\ \\{\\rho\\ge 0: p\\in P(s), \\forall p\\in B(\\rho,\\hat{p})\\} \\equiv\n\n\\max_{\\rho\\ge 0}\\min_{p\\in B(\\rho,\\hat{p})} f(\\rho,p)</math>\n\nwhere\n\n:<math>f(\\rho,p) = \\left\\{\\begin{array}{cc}\\rho &, \\ p\\in P(s) \\\\ -\\infty &,\\ p\\notin P(s)\\end{array}\\right.</math>\n\nThe large penalty (<math>-\\infty</math>) is a device to force the <math>\\max</math> player not to perturb the nominal value beyond the stability radius of the system. It is an indication that the stability model is a model of local stability/robustness, rather than a global one.\n\n== Info-gap decision theory ==\n\n[[Info-gap decision theory]] is a recent non-probabilistic decision theory. It is claimed to be radically different from all current theories of decision under uncertainty. But it has been shown<ref name=\"MS10\" /> that its robustness model, namely\n\n:<math>\\hat{\\alpha}(q,\\tilde{u}):= \\max\\ \\{\\alpha\\ge 0: r_{c} \\le R(q,u),\\forall u\\in U(\\alpha,\\tilde{u})\\}</math>\n\nis actually a stability radius model characterized by a simple stability requirement of the form <math>r_{c}\\le R(q,u)</math> where <math>q</math> denotes the decision under consideration,  <math>u</math> denotes the parameter of interest,  <math>\\tilde{u}</math> denotes the estimate of the true value of <math>u</math> and <math>U(\\alpha,\\tilde{u})</math> denotes a ball of radius <math>\\alpha</math> centered at <math>\\tilde{u}</math>.\n\n[[Image:Infogap robustness.png|500px]]\n\nSince stability radius models are designed to deal with small perturbations in the nominal value of a parameter, info-gap's robustness model measures the ''local robustness'' of decisions in the neighborhood of the estimate <math>\\tilde{u}</math>.\n\nSniedovich<ref name=\"MS10\"/> argues that for this reason the theory is unsuitable for the treatment of severe uncertainty characterized by a poor estimate and a vast uncertainty space.\n\n== Variations on a theme ==\n\nThere are cases where it is more convenient to define the stability radius slightly different. For example, in many applications in control theory the radius of stability is defined as the size of the smallest destabilizing perturbation in the nominal value of the parameter of interest.<ref name=\"paice98\">Paice A.D.B.  and Wirth, F.R. (1998). Analysis of the Local Robustness of Stability for Flows. ''[[Mathematics of Control, Signals, and Systems]]'', 11, 289-302.</ref> The picture is this:\n\n[[Image:Radius of stability 3.png|500px]]\n\nMore formally,\n\n: <math>\\hat{\\rho}(q):= \\min_{p\\notin P(s)}  dist(p,\\hat{p})</math>\n\nwhere <math>dist(p,\\hat{p})</math> denotes the ''distance'' of <math>p\\in P</math> from <math>\\hat{p}</math>.\n\n== Stability radius of functions ==\nThe '''stability radius''' of a [[continuous function]] ''f'' (in a [[functional space]] ''F'') with respect to an [[open set|open]] stability domain ''D'' is the [[distance]] between ''f'' and the [[Set (mathematics)|set]] of unstable functions (with respect to ''D'').  We say that a function is ''stable'' with respect to ''D'' if its spectrum is in ''D''. Here, the notion of spectrum is defined on a case by case basis, as explained below.\n\n=== Definition ===\nFormally, if we denote the set of stable functions by ''S(D)'' and the stability radius by ''r(f,D)'', then:\n:<math>r(f,D)=\\inf_{g\\in C}\\{\\|g\\|:f+g\\notin S(D)\\},</math>\nwhere ''C'' is a subset of ''F''.\n\nNote that if ''f'' is already unstable (with respect to ''D''), then ''r(f,D)=0'' (as long as ''C'' contains zero).\n\n=== Applications ===\nThe notion of stability radius is generally applied to [[special function]]s as [[polynomial]]s (the spectrum is then the roots) and [[matrix (mathematics)|matrices]] (the spectrum is the [[eigenvalue]]s).  The case where ''C'' is a proper subset of ''F'' permits us to consider structured [[perturbation theory|perturbations]] (e.g. for a matrix, we could only need perturbations on the last row).  It is an interesting measure of robustness, for example in [[control theory]].\n\n=== Properties===\nLet ''f'' be a ([[complex number|complex]]) polynomial of degree ''n'', ''C=F'' be the set of polynomials of degree less than (or equal to) ''n'' (which we identify here with the set <math>\\mathbb{C}^{n+1}</math> of coefficients).  We take for ''D'' the open [[unit disk]], which means we are looking for the distance between a polynomial and the set of Schur [[stable polynomial]]s.  Then:\n:<math>r(f,D)=\\inf_{z\\in \\partial D}\\frac{|f(z)|}{\\|q(z)\\|},</math>\nwhere ''q'' contains each basis vector (e.g. <math>q(z)=(1,z,\\ldots,z^n)</math> when ''q'' is the usual power basis).  This result means that the stability radius is bound with the minimal value that ''f'' reaches on the unit circle.\n\n=== Examples ===\n* The polynomial <math>f(z)=z^8-9/10</math> (whose zeros are the 8th-roots of ''0.9'') has a stability radius of 1/80 if ''q'' is the power basis and the norm is the infinity norm.  So there must exist a polynomial ''g'' with (infinity) norm 1/90 such that ''f+g'' has (at least) a root on the unit circle.  Such a ''g'' is for example <math>g(z)=-1/90\\sum_{i=0}^8 z^i</math>.  Indeed, ''(f+g)(1)=0'' and ''1'' is on the unit circle, which means that ''f+g'' is unstable.\n\n==See also==\n* [[stable polynomial]]\n* [[Wald's maximin model]]\n\n== References ==\n<references />\n\n{{DEFAULTSORT:Stability Radius}}\n[[Category:Polynomials]]"
    },
    {
      "title": "Stable polynomial",
      "url": "https://en.wikipedia.org/wiki/Stable_polynomial",
      "text": "In the context of the [[characteristic equation (calculus)|characteristic polynomial]] of a [[differential equation]] or [[difference equation]],  a [[polynomial]] is said to be '''stable''' if either:\n* all its roots lie in the [[open set|open]] left [[half-plane]], or\n* all its roots lie in the [[open set|open]] [[unit disk]].\n\nThe first condition provides [[stability theory|stability]] for [[continuous-time]] linear systems, and the second case relates to stability\nof [[discrete-time]] linear systems. A polynomial with the first property is called at times a [[Hurwitz polynomial]] and with the second property a [[Schur polynomial]]. Stable polynomials arise in [[control theory]] and in mathematical theory\nof differential and difference equations. A linear, [[time-invariant system]] (see [[LTI system theory]]) is said to be [[BIBO stability|BIBO stable]] if every bounded input produces bounded output. A linear system is BIBO stable if its characteristic polynomial is stable.  The denominator is required to be Hurwitz stable if the system is in continuous-time and Schur stable if it is in discrete-time. In practice, stability is determined by applying any one of several [[stability criterion|stability criteria]].\n\n==Properties==\n* The [[Routh-Hurwitz theorem]] provides an algorithm for determining if a given polynomial is Hurwitz stable, which is implemented in the [[Routh–Hurwitz stability criterion|Routh–Hurwitz]] and [[Liénard–Chipart criterion|Liénard–Chipart]] tests. \n* To test if a given polynomial ''P'' (of degree ''d'') is Schur stable, it suffices to apply this theorem to the transformed polynomial\n\n:<math> Q(z)=(z-1)^d P\\left({{z+1}\\over{z-1}}\\right)\n</math>\n\nobtained after the [[Möbius transformation]] <math>z \\mapsto {{z+1}\\over{z-1}}</math> which maps the left half-plane to the open unit disc: ''P'' is Schur stable if and only if ''Q'' is Hurwitz stable and <math> P(1)\\neq 0</math>. For higher degree polynomials the extra computation involved in this mapping can be avoided by testing the Schur stability by the Schur-Cohn test, the [[Jury stability criterion|Jury test]] or the [[Bistritz stability criterion|Bistritz test]].\n\n* Necessary condition: a Hurwitz stable polynomial (with real coefficients) has coefficients of the same sign (either all positive or all negative).\n* Sufficient condition: a polynomial <math> f(z)=a_0+a_1 z+\\cdots+a_n z^n</math> with (real) coefficients such that:\n:<math> a_n>a_{n-1}>\\cdots>a_0>0,</math>\nis Schur stable.\n\n* Product rule: Two polynomials ''f'' and ''g'' are stable (of the same type) if and only if the product ''fg'' is stable.\n\n==Examples==\n* <math> 4z^3+3z^2+2z+1 </math> is Schur stable because it satisfies the sufficient condition;\n* <math> z^{10}</math> is Schur stable (because all its roots equal 0) but it does not satisfy the sufficient condition;\n* <math> z^2-z-2</math> is not Hurwitz stable (its roots are -1,2) because it violates the necessary condition;\n* <math> z^2+3z+2 </math> is Hurwitz stable (its roots are -1,-2).\n* The polynomial <math> z^4+z^3+z^2+z+1 </math> (with positive coefficients) is neither Hurwitz stable nor Schur stable. Its roots are the four primitive fifth [[root of unity|roots of unity]]\n\n::<math> z_k=\\cos\\left({{2\\pi k}\\over 5}\\right)+i \\sin\\left({{2\\pi k}\\over 5}\\right), \\, k=1, \\ldots, 4 \\ .</math>\n\n:Note here that\n\n::<math> \\cos({{2\\pi}/5})={{\\sqrt{5}-1}\\over 4}>0.\n</math>\n\n:It is a \"boundary case\" for Schur stability because its roots lie on the unit circle. The example also shows that the necessary (positivity) conditions stated above for Hurwitz stability are not sufficient.\n\n==See also==\n* [[Stability criterion]]\n* [[Stability radius]]\n\n==External links==\n* [http://mathworld.wolfram.com/StablePolynomial.html Mathworld page]\n\n[[Category:Stability theory]]\n[[Category:Polynomials]]\n\n[[fr:Polynôme de Hurwitz]]"
    },
    {
      "title": "Stirling polynomials",
      "url": "https://en.wikipedia.org/wiki/Stirling_polynomials",
      "text": "{{more footnotes|date=December 2014}}\n\nIn [[mathematics]], the '''Stirling polynomials''' are a family of [[polynomial]]s that generalize important sequences of numbers appearing in [[combinatorics]] and [[analysis]], which are closely related to the [[Stirling number]]s, the [[Bernoulli number]]s, and the generalized [[Bernoulli polynomial]]s. There are multiple variants of the ''Stirling polynomial'' sequence considered below most notably including the [[Sheffer sequence]] form of the sequence, <math>S_k(x)</math>, defined characteristically through the special form of its exponential generating function, and the ''Stirling (convolution) polynomials'', <math>\\sigma_n(x)</math>, which also satisfy a characteristic ''ordinary'' generating function and that are of use in generalizing the [[Stirling numbers]] (of both kinds) to arbitrary complex-valued inputs. We consider the \"''convolution polynomial''\" variant of this sequence and its properties second in the last subsection of the article. Still other variants of the Stirling polynomials are studied in the supplementary links to the articles given in the references.\n\n\n==Definition and examples==\n\nFor nonnegative integers ''k'', the Stirling polynomials, ''S''<sub>''k''</sub>(''x''), are a [[Sheffer sequence]] for <math>(g(t), \\bar{f}(t)) := \\left(e^{-t}, \\log\\left(\\frac{t}{1-e^{-t}}\\right)\\right)</math> <ref>See section 4.8.8 of ''The Umbral Calculus'' (1984) reference linked below.</ref> defined by the exponential generating function \n::<math>\\left( {t \\over {1-e^{-t}}} \\right) ^{x+1}= \\sum_{k=0}^\\infty S_k(x){t^k \\over k!}.</math>\n\nThe Stirling polynomials are a special case of the [[Nørlund polynomials]] (or [[generalized Bernoulli polynomials]]) <ref>See [http://mathworld.wolfram.com/NorlundPolynomial.html Norlund polynomials] on MathWorld.</ref> each with exponential generating function\n\n::<math>\\left( {t \\over {e^t-1}} \\right) ^a e^{z t}= \\sum_{k=0}^\\infty B^{(a)}_k(z){t^k \\over k!},</math>\n\ngiven by the relation <math>S_k(x)= B_k^{(x+1)}(x+1)</math>.\n\nThe first 10 Stirling polynomials are given in the following table:\n\n:<math>\\begin{array}{r|l}\nk\n& S_k(x)\\\\\n\\hline\n 0 & 1\\\\\n 1 & {\\scriptstyle\\frac{1}{2}}(x+1)\\\\\n 2\n& {\\scriptstyle\\frac{1}{12}} (3x^2+5x+2) \\\\\n 3\n& {\\scriptstyle\\frac{1}{8}} (x^3+2x^2+x) \\\\\n 4\n& {\\scriptstyle\\frac{1}{240}} (15x^4+30x^3+5x^2-18x-8) \\\\\n 5\n& {\\scriptstyle\\frac{1}{96}} (3x^5+5x^4-5x^3-13x^2-6x) \\\\\n 6\n& {\\scriptstyle\\frac{1}{4032}} (63x^6+63x^5-315x^4-539x^3-84x^2+236x+96) \\\\\n 7\n& {\\scriptstyle\\frac{1}{1152}} (9x^7-84x^5-98x^4+91x^3+194x^2+80x) \\\\\n 8\n& {\\scriptstyle\\frac{1}{34560}} (135x^8-180x^7-1890x^6-840x^5+6055x^4+8140x^3+884x^2-3088x-1152) \\\\\n 9\n& {\\scriptstyle\\frac{1}{7680}} (15x^9-45x^8-270x^7+182x^6+1687x^5+1395x^4-1576x^3-2684x^2-1008x)\n\\end{array}</math>\nYet another variant of the Stirling polynomials is considered in <ref>{{cite journal|last1=Gessel and Stanley|title=Stirling polynomials|journal=J. Combin. Theory Ser. A|date=1978|volume=53|pages=24–33}}</ref> (see also the subsection on [[Stirling polynomial#Stirling convolution polynomials|Stirling convolution polynomials]] below). In particular, the article by I. Gessel and R. P. Stanley defines the modified Stirling polynomial sequences, <math>f_k(n) := S(n+k, n)</math> and <math>g_k(n) := c(n, n-k)</math> where <math>c(n, k) := (-1)^{n-k} s(n, k)</math> are the ''unsigned'' [[Stirling numbers of the first kind]], in terms of the two [[Stirling number]] triangles for non-negative integers <math>n \\geq 1,\\ k \\geq 0</math>. For fixed <math>k \\geq 0</math>, both <math>f_k(n)</math> and <math>g_k(n)</math> are polynomials of the input <math>n \\in \\mathbb{Z}^{+}</math> each of degree <math>2k</math> and with leading coefficient given by the [[double factorial]] term <math>(1 \\cdot 3 \\cdot 5 \\cdots (2k-1)) / (2k)!</math>.\n\n==Properties==\n\nBelow <math>B_k(x)</math> denote the [[Bernoulli polynomials]] and <math>B_k = B_k(0)</math> the [[Bernoulli numbers]] under the convention <math>B_1 = B_1(0) = -\\tfrac{1}{2};</math> <math>s_{m,n}</math> denotes a [[Stirling number of the first kind]]; and <math>S_{m,n}</math> denotes [[Stirling numbers of the second kind]].\n\n*Special values:\n::<math>\\begin{align}\nS_k(-m) &= \\frac{(-1)^k}{{k+m-1 \\choose k}} S_{k+m-1,m-1} && 0 < m \\in \\Z\\\\[6pt]\nS_k(-1) &= \\delta_{k,0} \\\\[6pt]\nS_k(0)  &= (-1)^k B_k \\\\[6pt]\nS_k(1)  &= (-1)^{k+1} ((k-1) B_k+ k B_{k-1}) \\\\[6pt]\nS_k(2)  &= \\tfrac{(-1)^k}{2} ((k-1)(k-2) B_k+ 3 k(k-2) B_{k-1}+ 2 k(k-1) B_{k-2}) \\\\[6pt]\nS_k(k)  &= k! \\\\[6pt]\n\\end{align}</math>\n\n*If <math>m \\in \\Z</math> and <math>m \\geq n</math> then:<ref>Section 4.4.8 of ''The Umbral Calculus''.</ref>\n::<math>S_n(m) = (-1)^n B_n^{(m+1)}(0),</math> \n:and:\n::<math>S_n(m)= {(-1)^n \\over {m \\choose n}} s_{m+1, m+1-n}.</math>\n\n*The sequence <math>S_k(x-1)</math> is of [[binomial type]], since \n::<math>S_k(x+y-1)= \\sum_{i=0}^k {k \\choose i} S_i(x-1) S_{k-i}(y-1).</math> \n:Moreover, this basic recursion holds: \n::<math>S_k(x)= (x-k) {S_k(x-1) \\over x} + k S_{k-1}(x+1).</math>\n\n*Explicit representations involving Stirling numbers can be deduced with [[Lagrange interpolation|Lagrange's interpolation formula]]:\n::<math>\\begin{align}\nS_k(x)&= \\sum_{n=0}^k (-1)^{k-n} S_{k+n,n} {{x+n \\choose n} {x+k+1 \\choose k-n} \\over {k+n \\choose n}} \\\\[6pt]\n&= \\sum_{n=0}^k (-1)^n s_{k+n+1,n+1} {{x-k \\choose n} {x-k-n-1 \\choose k-n} \\over {k+n \\choose k}}\\\\[6pt]\n&= k! \\sum_{j=0}^k (-1)^{k-j}\\sum_{m=j}^k {x+m\\choose m}{m\\choose j}L_{k+m}^{(-k-j)}(-j) \\\\[6pt]\n\\end{align}</math>\n:Here, <math>L_n^{(\\alpha)}</math> are [[Laguerre polynomials]].\n\n*The following relations hold as well:\n::<math>{k+m \\choose k} S_k(x-m)= \\sum_{i=0}^k (-1)^{k-i} {k+m \\choose i} S_{k-i+m,m} \\cdot S_i(x),</math>\n::<math>{k-m \\choose k} S_k(x+m)= \\sum_{i=0}^k {k-m \\choose i} s_{m,m-k+i} \\cdot S_i(x).</math>\n\n*By differentiating the generating function it readily follows that\n::<math>S_k^\\prime(x)=-\\sum_{j=0}^{k-1} {k\\choose j} S_j(x) \\frac{B_{k-j}}{k-j}.</math>\n<!-- They may be recovered by <math>s_{n,m}= (-1)^{n-m} {n-1 \\choose n-m} S_{n-m}(n-1).</math>\nConversely, <math>S_{n,m}=(-1)^{n-m} {n \\choose m} S_{n-m}(-m-1)</math>; -->\n\n==Stirling convolution polynomials==\n\n===Definition and examples===\n\nAnother variant of the Stirling polynomial sequence corresponds to a special case of the '''convolution polynomials''' studied by Knuth's article <ref>{{cite journal|last1=Knuth|first1=D. E.|title=Convolution Polynomials|journal=Mathematica J.|date=1992|volume=2|pages=67–78|url=https://arxiv.org/pdf/math/9207221.pdf}}\n\nThe article contains definitions and properties of special ''convolution polynomial'' families defined by special generating functions of the form <math>F(z)^x</math> for <math>F(0)=1</math>. Special cases of these convolution polynomial sequences include the ''binomial power series'', <math>\\mathcal{B}_t(z) = 1 + z \\mathcal{B}_t(z)^t</math>, so-termed ''tree polynomials'', the [[Bell numbers]], <math>B(n)</math>, and the [[Laguerre polynomials]]. For <math>F_n(x) := [z^n] F(z)^x</math>, the polynomials <math>n! \\cdot F_n(x)</math> are said to be of ''[[binomial type]]'', and moreover, satisfy the generating function relation <math>\\frac{z F_n(x+tn)}{(x+tn)} = [z^n] \\mathcal{F}_t(z)^x</math> for all <math>t \\in \\mathbb{C}</math>, where <math>\\mathcal{F}_t(z)</math> is implicitly defined by a [[functional equation]] of the form <math>\\mathcal{F}_t(z) = F\\left(x \\mathcal{F}_t(z)^t\\right)</math>. The article also discusses asymptotic approximations and methods applied to polynomial sequences of this type.</ref> \nand in the ''Concrete Mathematics'' reference. We first define these polynomials through the [[Stirling numbers of the first kind]] as\n\n:<math>\\sigma_n(x) = \\left[\\begin{matrix} x \\\\ x-n \\end{matrix} \\right] \\cdot \\frac{1}{x(x-1)\\cdots(x-n)}. </math>\n\nIt follows that these polynomials satisfy the next recurrence relation given by\n\n:<math>(x+1) \\sigma_n(x+1) = (x-n) \\sigma_n(x) + x \\sigma_{n-1}(x),\\ n \\geq 1. </math>\n\nThese Stirling \"''convolution''\" polynomials may be used to define the Stirling numbers, <math>\\scriptstyle{\\left[\\begin{matrix} x \\\\ x-n \\end{matrix} \\right]}</math> and \n<math>\\scriptstyle{\\left\\{\\begin{matrix} x \\\\ x-n \\end{matrix} \\right\\}}</math>, for integers <math>n \\geq 0</math> and ''arbitrary'' complex values of <math>x</math>.\nThe next table provides several special cases of these Stirling polynomials for the first few <math>n \\geq 0</math>.\n\n:<math>\\begin{array}{r|c}\n n & \\sigma_n(x)\n\\\\\n\\hline\n 0 & \\frac{1}{x}\n\\\\\n 1 & \\frac{1}{2}\n\\\\\n 2 & \\frac{3x-1}{24}\n\\\\\n 3 & \\frac{x^2-x}{48}\n\\\\\n 4 & \\frac{15x^3-30x^2+5x+2}{5760}\n\\end{array}</math>\n\n===Generating functions===\n\nThis variant of the Stirling polynomial sequence has particularly nice ordinary [[generating functions]] of the following forms:\n\n:<math>\n\\begin{align} \n\\left(\\frac{z e^z}{e^z-1}\\right)^x & = \\sum_{n \\geq 0} x \\sigma_n(x) z^n \\\\ \n\\left(\\frac{1}{z} \\ln \\frac{1}{1-z}\\right)^x & = \\sum_{n \\geq 0} x \\sigma_n(x+n) z^n. \n\\end{align} \n</math>\n\nMore generally, if <math>\\mathcal{S}_t(z)</math> is a power series that satisfies <math>\\ln\\left(1-z \\mathcal{S}_t(z)^{t-1}\\right) = -z \\mathcal{S}_t(z)^t</math>, we have that\n\n:<math>\\mathcal{S}_t(z)^x = \\sum_{n \\geq 0} x \\sigma_n(x+tn) z^n. </math>\n\nWe also have the related series identity <ref>Section 7.4 of ''Concrete Mathematics''.</ref>\n\n:<math>\\sum_{n \\geq 0} (-1)^{n-1} \\sigma_n(n-1) z^n = \\frac{z}{\\ln(1+z)} = 1 +\\frac{z}{2} - \\frac{z^2}{12} + \\cdots, </math>\n\nand the Stirling (Sheffer) polynomial related generating functions given by\n\n:<math>\\sum_{n \\geq 0} (-1)^{n+1} m \\cdot \\sigma_n(n-m) z^n = \\left(\\frac{z}{\\ln(1+z)}\\right)^m</math>\n\n:<math>\\sum_{n \\geq 0} (-1)^{n+1} m \\cdot \\sigma_n(m) z^n = \\left(\\frac{z}{1-e^{-z}}\\right)^m.</math>\n\n===Properties and relations===\n\nFor integers <math>0 \\leq k \\leq n</math> and <math>r, s \\in \\mathbb{C}</math>, these polynomials satisfy the two Stirling convolution formulas given by\n\n:<math>(r+s) \\sigma_n(r+s+tn) = rs \\sum_{k=0}^n \\sigma_k(r+tk) \\sigma_{n-k}(s+t(n-k))</math>\n\nand\n\n:<math>n \\sigma_n(r+s+tn) = s \\sum_{k=0}^n k \\sigma_k(r+tk) \\sigma_{n-k}(s+t(n-k)).</math>\n\nWhen <math>n, m \\in \\mathbb{N}</math>, we also have that the polynomials, <math>\\sigma_n(m)</math>, are defined through their relations to the [[Stirling numbers]]\n\n:<math>\n\\begin{align} \n\\left\\{\\begin{matrix} n \\\\ m \\end{matrix} \\right\\} & = (-1)^{n-m+1} \\frac{n!}{(m-1)!} \\sigma_{n-m}(-m)\\ (\\text{when } m < 0) \\\\ \n\\left[\\begin{matrix} n \\\\ m \\end{matrix} \\right] & = \\frac{n!}{(m-1)!} \\sigma_{n-m}(n)\\ (\\text{when } m > n), \n\\end{align}\n</math>\n\nand their relations to the [[Bernoulli numbers]] given by\n\n:<math>\n\\begin{align} \n\\sigma_n(m) & = \\frac{(-1)^{m+n-1}}{m! (n-m)!} \\sum_{0 \\leq k < m} \\left[\\begin{matrix} m \\\\ m-k \\end{matrix} \\right] \\frac{B_{n-k}}{n-k},\\ n \\geq m > 0 \\\\ \n\\sigma_n(m) & = -\\frac{B_n}{n \\cdot n!},\\ m = 0. \n\\end{align} \n</math>\n\n==See also==\n* [[Bernoulli polynomials]]\n* [[Bernoulli polynomials of the second kind]]\n* [[Sheffer sequence|Sheffer]] and [[Appell sequence]]s\n* [[Difference polynomials]]\n* [[generating function#Other generating functions|Special polynomial generating functions]]\n* [[Gregory coefficients]]\n\n==References==\n{{Reflist}}\n* {{cite book | author=Erdeli, A., Magnus, W. and Oberhettinger, F and Tricomi, F. G. | title=Higher Transcendental Functions. Volume III:  | location=New York }}\n* {{cite book | author=Graham, Knuth and Patashnik | title=Concrete Mathematics: A Foundation for Computer Science  | year=1994 }}\n* {{cite book | author=S. Roman | title=The Umbral Calculus  | year=1984 }}\n\n==External links==\n* {{mathworld|urlname=StirlingPolynomial|title=Stirling Polynomial}}\n* {{mathworld|urlname=NorlundPolynomial|title=Nørlund Polynomial}}\n* {{PlanetMath attribution|id=37575|title=Stirling polynomial}}\n\n[[Category:Polynomials]]"
    },
    {
      "title": "Sturm's theorem",
      "url": "https://en.wikipedia.org/wiki/Sturm%27s_theorem",
      "text": "{{short description|Theorem allowing counting the roots of a polynomial in an interval without computing them}}\nIn [[mathematics]], the '''Sturm sequence''' of a [[univariate polynomial]] {{mvar|p}} is a sequence of polynomials associated with {{mvar|p}} and its derivative by a variant of [[Euclid's algorithm for polynomials]]. '''Sturm's theorem''' expresses the number of distinct [[real number|real]] [[Root of a function|root]]s of {{mvar|p}} located in an [[interval (mathematics)|interval]] in terms of the number of changes of signs of the values of the Sturm sequence at the bounds of the interval. Applied to the interval of all the real numbers, it gives the total number of real roots of {{mvar|p}}.<ref name=bpr>{{harv|Basu|Pollack|Roy|2006}}</ref>\n\nWhereas the [[fundamental theorem of algebra]] readily yields the overall number of [[complex number|complex]] roots, counted with [[multiplicity (mathematics)|multiplicity]], it does not provide a procedure for calculating them. Sturm's theorem counts the number of distinct real roots and locates them in intervals. By subdividing the intervals containing some roots, it can isolate the roots into arbitrary small intervals, each containing exactly one root. This yields the oldest [[real-root isolation]] algorithm, and arbitrary-precision [[root-finding algorithm]] for univariate polynomials.\n\nFor computing over the [[real number|reals]], Sturm's theorem is less efficient than other methods based on [[Descartes' rule of signs]]. However, it works on every [[real closed field]], and, therefore, remains fundamental for the theoretical study of the [[computational complexity]] of [[decidability of first-order theories of the real numbers|decidability]] and [[quantifier elimination]] in the [[first order theory]] of real numbers.\n\nThe Sturm sequence and Sturm's theorem are named after [[Jacques Charles François Sturm]], who discovered the theorem in 1829.<ref>{{MacTutor Biography|id=Sturm}}</ref>\n\n==The theorem==\nThe '''Sturm chain''' or '''Sturm sequence''' of a [[univariate polynomial]] {{math|''P''(''x'')}} with real coefficients is the sequence of polynomials <math>P_0, P_1, \\ldots,</math> such that \n:<math>\\begin{align}\nP_0&=P,\\\\\nP_1&=P',\\\\\nP_{i+1}&=-\\operatorname{rem}(P_{i-1},P_i),\n\\end{align}</math>\nfor {{math|''i'' ≥ 1}}, where {{math|''P'{{void}}''}} is the [[derivative]] of {{mvar|P}}, and <math>\\operatorname{rem}(P_{i-1},P_i)</math> is the remainder of the [[Euclidean division of polynomials|Euclidean division]] of <math>P_{i-1}</math> by <math>P_i.</math> The length of the Sturm sequence is at most the degree of {{mvar|P}}.\n\nThe number of [[sign variation]]s at {{mvar|ξ}} of the Sturm sequence of {{mvar|P}} is the number of sign changes–ignoring zeros—in the sequence of real numbers\n:<math>P_0(\\xi), P_1(\\xi),P_2(\\xi),\\ldots.</math>\nThis number of sign variations is denoted here {{math|''V''(''ξ'')}}. \n\nSturm's theorem states that, if {{mvar|P}} is a [[square-free polynomial]],  the number of distinct real roots of {{mvar|P}} in the [[half-open interval]] {{math|(''a'', ''b'']}} is {{math|''V''(''a'') − ''V''(''b'')}} (here, {{mvar|a}} and {{mvar|b}} are real numbers such that {{math|''a'' < ''b''}}).<ref name=\"bpr\" />\n\nThe theorem extends to unbounded intervals by defining the sign at {{math|+∞}} of a polynomial as the sign of its [[leading coefficient]] (that is, the coefficient of the term of highest degree). At {{math|–∞}} the sign of a polynomial is the sign of its leading coefficient for a polynomial of even degree, and the opposite sign for a polynomial of odd degree.\n\nIn the case of a non-square-free polynomial, if neither {{mvar|a}} nor {{mvar|b}} is a multiple root of {{mvar|p}}, then {{math|''V''(''a'') − ''V''(''b'')}} is the number of ''distinct'' real roots of {{mvar|P}}.\n\nThe proof of the theorem is as follows: when the value of {{mvar|x}} increases from {{mvar|a}} to {{mvar|b}}, it may pass through a zero of some <math>P_i</math> ({{math|''i'' > 0}}); when this occurs, the number of sign variations of <math>(P_{i-1}, P_i, P_{i+1})</math> does not changes. When {{mvar|x}} passes through a root of <math>P_0=P,</math> the number of sign variations of <math>(P_0, P_1)</math> decreases from 1 to 0. These are the only values of {{mvar|x}} where some sign may change.\n\n==Example==\nSuppose we wish to find the number of roots in some range for the polynomial <math>p(x)=x^4+x^3-x-1</math>. So\n\n:<math>\\begin{align} p_0(x) &=p(x)=x^4+x^3-x-1 \\\\\np_1(x)&=p'(x)=4x^3+3x^2-1\n\\end{align}</math>\n\nThe remainder of the [[Euclidean division]] of {{math|''p''<sub>0</sub>}} by {{math|''p''<sub>1</sub>}} is <math>-\\tfrac{3}{16}x^2-\\tfrac{3}{4}x-\\tfrac{15}{16};</math> multiplying it by {{math|−1}} we obtain \n:<math>p_2(x)=\\tfrac{3}{16}x^2+\\tfrac{3}{4}x+\\tfrac{15}{16}</math>.  \nNext dividing {{math|''p''<sub>1</sub>}} by {{math|''p''<sub>2</sub>}} and multiplying the remainder by {{math|−1}}, we obtain \n:<math>p_3(x)=-32x-64</math>. \nNow dividing {{math|''p''<sub>2</sub>}} by {{math|''p''<sub>3</sub>}} and multiplying the remainder by {{math|−1}}, we obtain \n:<math>p_4(x)=-\\tfrac{3}{16}</math>.\nAs this is a constant, this finishes the computation of the Sturm sequence.\n\nTo find the number of real roots of <math>p_0</math> one has to evaluate the sequences of the signs of these polynomials at {{math|−∞}} and {{math|∞}}, which are respectively {{math|(+, −, +, +, −)}} and {{math|(+, +, +, −, −)}}. Thus\n:<math>V(-\\infty)-V(+\\infty) = 3-1=2,</math>\nwhich shows that {{mvar|p}} has two real roots.\n\nThis can be verified by noting that {{math|''p''(''x'')}} can be factored as {{math|(''x''<sup>2</sup> − 1)(''x''<sup>2</sup> + ''x'' + 1)}}, where the first factor  has the roots {{math|−1}} and {{math|1}}, and second factor has no real roots. This last assertion results from the [[quadratic formula]], and also from Sturm's theorem, which gives the sign sequences {{math|(+, –, –)}} at {{math|−∞}} and {{math|(+, +, –)}} at {{math|+∞}}.\n\n==Generalization==\nSturm sequences have been generalized in two directions. To define each polynomial in the sequence, Sturm used the negative of the remainder of the [[Euclidean division]] of the two preceding ones. The theorem remains true if one replaces the negative of the remainder by its product or quotient by a positive constant or the square of a polynomial. It is also useful (see below) to consider sequences where the second polynomial is not the derivative of the first one.\n\nA ''generalized Sturm sequence'' is a finite sequence of polynomials with real coefficients\n:<math>P_0, P_1, \\dots, P_m</math>\nsuch that\n* the degrees are decreasing after the first one: <math>\\deg P_{i} <\\deg P_{i-1}</math> for {{math|1=''i'' = 2, ..., ''m''}};\n* <math>P_m</math> does not have any real root or does not changes of sign near its real roots.\n* if {{math|''P<sub>i</sub>''(''ξ'') {{=}} 0}} for {{math|0 < ''i'' < ''m''}} and {{mvar|ξ}} a real number, then {{math|''P''<sub>''i'' −1 </sub>(''ξ'') ''P''<sub>''i'' + 1</sub>(''ξ'') < 0}}.\n\nThe last condition implies that two consecutive polynomials do not have any common real root. In particular the original Sturm sequence is a generalized Sturm sequence, if (and only if) the polynomial has no multiple real root (otherwise the first two polynomials of its Sturm sequence have a common root).\n\nWhen computing the original Sturm sequence by Euclidean division, it may happen that one encounters a polynomial that has a factor that is never negative, such a <math>x^2</math> or <math>x^2+1</math>. In this case, if one continues the computation with the polynomial replaced by its quotient by the nonnegative factor, one gets a generalized Sturm sequence, which may also be used for computing the number of real roots, since the proof of Sturm's theorem still applies (because of the third condition). This may sometimes simplify the computation, although it is generally difficult to find such nonnegative factors, except for even powers of {{mvar|x}}.\n\n==Use of pseudo-remainder sequences==\nIn [[computer algebra]], the polynomials that are considered have integer coefficients or may be transformed to have integer coefficients. The Sturm sequence of a polynomial with integer coefficients generally contains polynomials whose coefficients are not integers (see above example). \n\nTo avoid computation with [[rational number]]s, a common method is to replace [[Euclidean division of polynomials|Euclidean division]] by [[pseudo-remainder|pseudo-division]] for computing [[polynomial greatest common divisor]]s. This amounts to replacing the remainder sequence of the [[Euclidean algorithm for polynomials|Euclidean algorithm]] by a [[pseudo-remainder sequence]], a pseudo remainder sequence being a sequence <math>p_0, \\ldots, p_k</math> of polynomials such that there are constants <math>a_i</math> and <math>b_i</math> such that <math>b_ip_{i+1}</math> is the remainder of the Euclidean division of <math>a_ip_{i-1}</math> by <math>p_i.</math> (The different kinds of pseudo-remainder sequences are defined by the choice of <math>a_i</math> and <math>b_i;</math> typically, <math>a_i</math> is chosen for not introducing denominators during Euclidean division, and <math>b_i</math> is a common divisor of the coefficients of the resulting remainder; see [[Pseudo-remainder sequence]] for details.)\nFor example, the remainder sequence of the Euclidean algorithm is a pseudo-remainder sequence with <math>a_i=b_i=1</math> for every {{mvar|i}}, and the Sturm sequence of a polynomial is a pseudo-remainder sequence with <math>a_i=1</math> and <math>b_i=-1</math> for every {{mvar|i}}.\n\nVarious pseudo-remainder sequences have been designed for computing greatest common divisors of polynomials with integer coefficients without introducing denominators (see [[Pseudo-remainder sequence]]). They can all be made generalized Sturm sequences by choosing the sign of the <math>b_i</math> to be the opposite of the sign of the <math>a_i.</math> This allows the use of Sturm's theorem with pseudo-remainder sequences.\n\n==Root isolation==\n\nFor a polynomial with real coefficients, ''root isolation'' consists of finding, for each real root, an interval that contains this root, and no other roots. \n\nThis is useful for [[root-finding algorithm|root finding]], allowing the selection of the root to be found and providing a good starting point for fast numerical algorithms such as [[Newton's method]]; it is also useful for certifying the result, as if Newton's method converge outside the interval one may immediately deduce that it converges to the wrong root.\n\nRoot isolation is also useful for computing with [[algebraic numbers]]. For computing with algebraic numbers, a common method is to represent them as a pair of a polynomial to which the algebraic number is a root, and an isolation interval. For example <math>\\sqrt 2</math> may be unambiguously represented by <math>(x^2-2, [0,2]).</math>\n\nSturm's theorem provides a way for isolating real roots that is less efficient (for polynomials with integer coefficients) than other methods involving [[Descartes' rule of signs]]. However, it remains useful in some circumstances, mainly for theoretical purposes, for example for algorithms of [[real algebraic geometry]] that involve [[infinitesimal]]s.\n\nFor isolating the real roots, one starts from an interval <math>(a,b]</math> containing all the real roots, or the roots of interest (often, typically in physical problems, only positive roots are interesting), and one computes <math>V(a)</math> and <math>V(b).</math> For defining this starting interval, one may use bounds on the size of the roots (see {{slink|Properties of polynomial roots|Bounds on (complex) polynomial roots}}). Then, one divides this interval in two, by choosing {{mvar|c}} in the middle of <math>(a,b].</math> The computation of <math>V(c)</math> provides the number of real roots in <math>(a,c]</math> and <math>(c,b],</math> and one may repeat the same operation on each subinterval. When one encounters, during this process an interval that does not contain any root, it may be suppressed from the list of intervals to consider. When one encounters an interval containing exactly one root, one may stop dividing it, as it is an isolation interval. The process stops eventually, when only isolating intervals remain. \n\nThis isolating process may be used with any method for computing the number of real roots in an interval. Theoretical [[complexity analysis]] and practical experiences show that methods based on [[Descartes' rule of signs]] are more efficient. It follows that, nowadays, Sturm sequences are rarely used for root isolation.\n\n==Application==\nGeneralized Sturm sequences allow counting the roots of a polynomial where another polynomial is positive (or negative), without computing these root explicitly. If one knows an isolating interval for a root of the first polynomial, this allows also finding the sign of the second polynomial at this particular root of the first polynomial, without computing a better approximation of the root.\n\nLet {{math|''P''(''x'')}} and {{math|''Q''(''x'')}} be two polynomials with real coefficients such that {{mvar|P}} and {{mvar|Q}} have no common root and {{mvar|P}} has no multiple roots. In other words, {{mvar|P}} and {{math|''P'{{space|hair}}Q''}} are [[coprime|coprime polynomials]]. This restriction does not really affect the generality of what follows as [[polynomial greatest common divisor|GCD]] computations allows reducing the general case to this case, and the cost of the computation of a Sturm sequence is the same as that of a GCD. \n\nLet {{math|''W''(''a'')}} denote the number of sign variations at {{mvar|a}} of a generalized Sturm sequence starting from {{mvar|P}} and {{math|''P'{{space|hair}}Q''}}. If {{math|''a'' < ''b''}} are two real numbers, then {{math|''W''(''a'') – ''W''(''b'')}} is the number of roots of {{mvar|P}} in the interval <math>(a,b]</math> such that {{math|''Q''(''a'') > 0}} minus the number of roots in the same interval such that {{math|''Q''(''a'') < 0}}. Combined with the total number of roots of {{mvar|P}} in the same interval given by Sturm's theorem, this gives the number of roots of {{mvar|P}} such that {{math|''Q''(''a'') > 0}} and the number of roots of {{mvar|P}} such that {{math|''Q''(''a'') < 0}}.<ref name=\"bpr\" />\n\n==See also==\n{{Div col|colwidth=25em}}\n* [[Routh–Hurwitz theorem]]\n* [[Hurwitz's theorem (complex analysis)]]\n* [[Descartes' rule of signs]]\n* [[Rouché's theorem]]\n* [[Properties of polynomial roots]]\n* [[Gauss–Lucas theorem]]\n* [[Turán's inequalities]]\n{{div col end}}\n\n==References==\n\n{{reflist}}\n\n* {{cite book\n|first1=Saugata\n|last1=Basu\n|first2=Richard\n|last2=Pollack\n|authorlink2=Richard M. Pollack\n|first3=Marie-Françoise\n|last3=Roy\n|authorlink3=Marie-Françoise Roy\n|title=Algorithms in real algebraic geometry\n|year=2006\n|publisher=[[Springer Science+Business Media|Springer]]\n|pages=52–57\n|section=Section 2.2.2\n|isbn=978-3-540-33098-1\n|edition=2nd\n|url=https://s3.amazonaws.com/academia.edu.documents/30838872/Saugata_Basu_Algorithms_in_Real_Algebraic_Geome.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1542452131&Signature=qTJoc%2FARfSeT01%2FOI%2F7W6zaHZ9w%3D&response-content-disposition=inline%3B%20filename%3DAlgorithms_in_real_algebraic_geometry.pdf\n}}\n* {{cite journal\n|first1=Jacques Charles François\n|last1=Sturm\n|year=1829\n|title=Mémoire sur la résolution des équations numériques\n|journal=Bulletin des Sciences de Férussac\n|volume=11\n|pages=419–425\n}}\n* {{cite journal\n|first1=J. J.\n|last1=Sylvester\n|year=1853\n|title=On a theory of the syzygetic relations of two rational integral functions, comprising an application to the theory of Sturm's functions, and that of the greatest algebraical common measure\n|journal=Phil. Trans. R. Soc. Lond.\n|volume=143\n|pages=407–548\n|jstor=108572\n|doi=10.1098/rstl.1853.0018\n}}\n* {{cite journal\n|first1=Joseph Miller\n|last1=Thomas\n|title=Sturm's theorem for multiple roots\n|year=1941\n|journal=National Mathematics Magazine\n|volume=15\n|number=8\n|pages=391–394\n|jstor=3028551\n|mr=0005945\n}}\n* {{citation\n|first1=Lee E.\n|last1=Heindel\n|title=Integer arithmetic algorithms for polynomial real zero determination\n|journal=Proc. SYMSAC '71\n|year=1971\n|page=415\n|mr=0300434\n|doi=10.1145/800204.806312\n}}\n* {{cite journal\n|first1=Don B.\n|last1=Panton\n|first2=William A.\n|last2=Verdini\n|title=A fortran program for applying Sturm's theorem in counting internal rates of return\n|journal=J. Financ. Quant. Anal.\n|year=1981\n|volume=16\n|number=3\n|pages=381–388\n|jstor=2330245\n}}\n* {{cite journal\n|title=Reflections on a pair of theorems by Budan and Fourier\n|journal=Math. Mag.\n|first1=Alkiviadis G.\n|last1=Akritas\n|mr=0678195\n|year=1982\n|volume=55\n|number=5\n|pages=292–298\n|jstor=2690097\n|doi=10.2307/2690097\n}}\n* {{cite journal\n|title=Multivariate Sturm theory\n|journal=Lecture Notes in Comp. Science\n|year=1991\n|volume=539\n|pages=318–332\n|mr=1229329\n|doi=10.1007/3-540-54522-0_120\n|first1=Paul\n|last1=Petersen\n}}\n* {{cite book|first1=Chee\n|last1=Yap\n|url=http://www.cs.nyu.edu/yap/book/berlin/\n|title=Fundamental Problems in Algorithmic Algebra\n|publisher=[[Oxford University Press]]\n|year=2000\n|isbn=0-19-512516-9\n}}\n* {{cite book | last1=Rahman | first1=Q. I. | last2=Schmeisser | first2=G. | title=Analytic theory of polynomials | series=London Mathematical Society Monographs. New Series | volume=26 | location=Oxford | publisher=[[Oxford University Press]] | year=2002 | isbn=0-19-853493-0 | zbl=1072.30006 }}\n*Baumol, William. ''Economic Dynamics'', chapter 12, Section 3, \"Qualitative information on real roots\"\n* D.G. Hook and P. R. McAree, \"Using Sturm Sequences To Bracket Real Roots of Polynomial Equations\" in Graphic Gems I (A. Glassner ed.), Academic Press, pp.&nbsp;416&ndash;422, 1990.\n\n\n{{DEFAULTSORT:Sturm's Theorem}}\n[[Category:Theorems in real analysis]]\n[[Category:Articles containing proofs]]\n[[Category:Polynomials]]\n[[Category:Computer algebra]]\n[[Category:Real algebraic geometry]]"
    },
    {
      "title": "Sylvester matrix",
      "url": "https://en.wikipedia.org/wiki/Sylvester_matrix",
      "text": "In [[mathematics]], a '''Sylvester matrix''' is a [[matrix (mathematics)|matrix]] associated to two [[univariate polynomial]]s with coefficients in a [[field (mathematics)|field]] or a [[commutative ring]]. The entries of the Sylvester matrix of two polynomials are coefficients of the polynomials. The [[determinant]] of the Sylvester matrix of two polynomials is their [[resultant]], which is zero when the two polynomials have a common root (in case of coefficients in a field) or a non-constant common divisor (in case of coefficients in an [[integral domain]]).\n\nSylvester matrices are named after [[James Joseph Sylvester]].\n\n==Definition==\nFormally, let ''p'' and ''q'' be two nonzero polynomials, respectively of degree ''m'' and&nbsp;''n''.  Thus:\n:<math>p(z)=p_0+p_1 z+p_2 z^2+\\cdots+p_m z^m,\\;q(z)=q_0+q_1 z+q_2 z^2+\\cdots+q_n z^n.</math>\nThe '''Sylvester matrix''' associated to ''p'' and ''q'' is then the <math>(n+m)\\times(n+m)</math> matrix constructed as follows:\n* if ''n'' > 0, the first row is:\n:<math>\\begin{pmatrix} p_m & p_{m-1} & \\cdots & p_1 & p_0 & 0 & \\cdots & 0 \\end{pmatrix}.</math>\n* the second row is the first row, shifted one column to the right; the first element of the row is zero.\n* the following ''n''&nbsp;&minus;&nbsp;2 rows are obtained the same way, shifting the coefficients one column to the right each time and setting the other entries in the row to be 0.\n* if ''m'' > 0 the (''n''&nbsp;+&nbsp;1)th row is:\n:<math>\\begin{pmatrix} q_n & q_{n-1} & \\cdots & q_1 & q_0 & 0 & \\cdots & 0 \\end{pmatrix}.</math>\n* the following rows are obtained the same way as before.\n\nThus, if ''m''&nbsp;=&nbsp;4 and ''n''&nbsp;=&nbsp;3, the matrix is:\n:<math>S_{p,q}=\\begin{pmatrix} \np_4 & p_3 & p_2 & p_1 & p_0 & 0 & 0 \\\\\n0 & p_4 & p_3 & p_2 & p_1 & p_0 & 0 \\\\\n0 & 0 & p_4 & p_3 & p_2 & p_1 & p_0 \\\\\nq_3 & q_2 & q_1 & q_0 & 0 & 0 & 0 \\\\\n0 & q_3 & q_2 & q_1 & q_0 & 0 & 0 \\\\\n0 & 0 & q_3 & q_2 & q_1 & q_0 & 0 \\\\\n0 & 0 & 0 & q_3 & q_2 & q_1 & q_0\n\\end{pmatrix}.</math>\n\nIf one of the degrees is zero (that is the corresponding polynomial is a nonzero constant), then there are zero rows consisting of coefficients of the other polynomial, and the Sylvester matrix is a [[diagonal matrix]] of dimension the degree of the non-constant polynomial, with the all diagonal coefficients equal to the constant polynomial. If ''m'' = ''n'' = 0, then the Sylvester matrix is the [[empty matrix]] with zero rows and zero columns.\n\n==A variant==\n\nThe above defined Sylvester matrix appears in a Sylvester's paper of 1840. In a paper of 1853, Sylvester has introduced the following matrix, which is, up to a permutation of the rows, the Sylvester matrix of ''p'' and ''q'', considered as having both the degree max(''m'', ''n'').<ref name=\"amv2014\">Akritas, A.G., Malaschonok, G.I., Vigklas, P.S.:''Sturm Sequences and Modified Subresultant Polynomial Remainder Sequences''. Serdica Journal of Computing, Vol. 8, No 1, 29--46, 2014</ref>\nThis is thus a <math>2\\,\\max(n, m)\\times 2\\,\\max(n, m)</math>-matrix containing <math>\\max(n, m)</math> pairs of rows. Assuming <math> m > n,</math> it is obtained as follows:\n* the first pair is:\n:<math>\n\\begin{pmatrix} \n        p_m & p_{m-1} &\\cdots & p_n  & \\cdots   & p_1 & p_0 & 0 & \\cdots & 0 \\\\\n        0      & \\cdots    & 0        & q_n  &  \\cdots & q_1 & q_0 & 0 & \\cdots & 0 \n\\end{pmatrix}.</math>\n* the second pair is the first pair, shifted one column to the right; the first elements in the two rows are zero.\n* the remaining <math>max(n, m)-2</math> pairs of rows are obtained the same way as above.\n\nThus, if ''m''&nbsp;=&nbsp;4 and ''n''&nbsp;=&nbsp;3, the matrix is:\n:<math>\\begin{pmatrix} \np_4 & p_3 & p_2 & p_1 & p_0 & 0 & 0 & 0\\\\\n0    & q_3 & q_2 & q_1 & q_0 & 0  & 0 & 0\\\\\n0    & p_4 & p_3 & p_2 & p_1 & p_0 & 0 & 0\\\\\n0    & 0    & q_3 & q_2 & q_1 & q_0 & 0  & 0\\\\\n0    & 0    & p_4 & p_3 & p_2 & p_1 & p_0 & 0\\\\\n0    & 0    & 0    & q_3 & q_2 & q_1 & q_0 & 0\\\\\n0    & 0    & 0    & p_4 & p_3 & p_2 & p_1 & p_0\\\\\n0    & 0    & 0    & 0     & q_3 & q_2 & q_1 & q_0\\\\\n\\end{pmatrix}.</math>\n\nThe determinant of the 1853 matrix is, up to the sign, the product of the determinant of the Sylvester matrix (which is called the [[resultant]] of ''p'' and ''q'') by  <math>p_m^{m-n}</math> (still supposing <math>m\\ge n</math>).\n\n==Applications==\nThese matrices are used in [[commutative algebra]], e.g. to test if two polynomials have a (non constant) common factor.  In such a case, the [[determinant]] of the associated '''Sylvester matrix''' (which is named the [[resultant]] of the two polynomials) equals zero. The converse is also true.\n\nThe solutions of the simultaneous linear equations\n:<math>{S_{p,q}}^\\mathrm{T}\\cdot\\begin{pmatrix}x\\\\y\\end{pmatrix} = \\begin{pmatrix}0\\\\0\\end{pmatrix}</math>\nwhere <math>x</math> is a vector of size <math>n</math> and <math>y</math> has size <math>m</math>, comprise the coefficient vectors of those and only those pairs <math>x, y</math> of polynomials (of degrees <math>n-1</math> and <math>m-1</math>, respectively) which fulfill\n:<math>x(z) \\cdot p(z) + y(z) \\cdot q(z) = 0,</math>\nwhere polynomial multiplication and addition is used.\nThis means the [[Null space|kernel]] of the transposed Sylvester matrix gives all solutions of the [[Bézout's identity|Bézout equation]] where <math>\\deg x < \\deg q</math> and <math>\\deg y < \\deg p</math>.\n\nConsequently the [[Rank_(linear_algebra)|rank]] of the Sylvester matrix determines the degree of the [[Polynomial greatest common divisor|greatest common divisor]] of ''p'' and ''q'':\n:<math>\\deg(\\gcd(p,q)) = m+n-\\operatorname{rank} S_{p,q}</math> \nMoreover, the coefficients of this greatest common divisor may be expressed as [[determinant]]s of submatrices of the Sylvester matrix (see [[Subresultant]]).\n\n==See also==\n* [[Transfer matrix]]\n* [[Bézout matrix]]\n\n==References==\n{{reflist}}\n* {{mathworld|urlname=SylvesterMatrix|title = Sylvester Matrix}}\n\n[[Category:Matrices]]\n[[Category:Polynomials]]"
    },
    {
      "title": "Synthetic division",
      "url": "https://en.wikipedia.org/wiki/Synthetic_division",
      "text": "In [[algebra]], '''synthetic division''' is a method for manually performing [[Euclidean division of polynomials]], with less writing and fewer calculations than occur with [[polynomial long division]].  It is mostly taught for division by binomials of the form \n \n:<math>x - a,\\ </math>\n\nbut the method generalizes to division by any [[monic polynomial]], and to any [[polynomial]].\n\nThe advantages of synthetic division are that it allows one to calculate without writing variables, it uses few calculations, and it takes significantly less space on paper than long division. Also, the subtractions in long division are converted to additions by switching the signs at the very beginning, preventing sign errors.\n\nSynthetic division for linear denominators is also called division through [[Ruffini's rule]].\n\n== Regular synthetic division ==\nThe first example is synthetic division with only a [[monic polynomial|monic]] linear denominator <math>x-a</math> .\n\n:<math>\\frac{x^3 - 12x^2 - 42}{x - 3}</math>\n\nWrite the coefficients of the polynomial that is to be divided at the top (the zero is for the unseen 0''x'').\n:<math>\\begin{array}{cc}\n    \\begin{array}{r} \\\\  \\\\ \\end{array}\n    &\n    \\begin{array}{|rrrr} \\ \n        1 & -12 & 0 & -42 \\\\\n          &     &   &     \\\\\n        \\hline \n    \\end{array}\n\\end{array}</math>\n\nNegate the coefficients of the divisor.\n:<math>\t\\begin{array}{rr} \n     -1x & + 3\n\\end{array}</math>\n\nWrite in every coefficient of the divisor but the first one on the left.\n:<math>\\begin{array}{cc}\n    \\begin{array}{r} \\\\ 3 \\\\ \\end{array}\n    &\n    \\begin{array}{|rrrr} \\ \n        1 & -12 & 0 & -42 \\\\\n          &     &   &     \\\\\n        \\hline \n    \\end{array}\n\\end{array}</math>\n\n\"Drop\" the first coefficient after the bar to the last row.\n:<math>\\begin{array}{cc}\n    \\begin{array}{r} \\\\ 3 \\\\ \\\\ \\end{array}\n    &\n    \\begin{array}{|rrrr}  \n        1 & -12 & 0 & -42 \\\\\n          &     &   &     \\\\\n        \\hline \n        1 &     &   &     \\\\\n    \\end{array}\n\\end{array}</math>\n\nMultiply the dropped number by the number before the bar, and place it in the next column.\n:<math>\\begin{array}{cc}\n    \\begin{array}{r} \\\\ 3 \\\\ \\\\ \\end{array}\n    &\n    \\begin{array}{|rrrr}  \n        1 & -12 & 0 & -42 \\\\\n          &   3 &   &     \\\\\n        \\hline \n        1 &     &   &     \\\\\n    \\end{array}\n\\end{array}</math>\n\nPerform an addition in the next column.\n:<math>\\begin{array}{cc}\n    \\begin{array}{c} \\\\ 3 \\\\ \\\\ \\end{array}\n    &\n    \\begin{array}{|rrrr}  \n        1 & -12 & 0 & -42 \\\\\n          &   3 &   &     \\\\\n        \\hline \n        1 &  -9 &   &     \\\\\n    \\end{array}\n\\end{array}</math>\n\nRepeat the previous two steps and the following is obtained:\n:<math>\\begin{array}{cc}\n    \\begin{array}{c} \\\\ 3 \\\\ \\\\ \\end{array}\n    &\n    \\begin{array}{|rrrr}  \n        1 & -12 &   0 & -42 \\\\\n          &   3 & -27 & -81 \\\\\n        \\hline \n        1 & -9 & -27 & -123 \n    \\end{array}\n\\end{array}</math>\n\nCount the terms to the left of the bar.  Since there is only one, the remainder has degree zero and this is the one right-most term under the bar. Mark the separation with a vertical bar.\n:<math>\t\\begin{array}{rrr|r} \n    1 &  -9 & -27 & -123 \n\\end{array}</math>\n \nThe terms are written with increasing degree from right to left beginning with degree zero for both the remainder and the result.\n:<math>\t\\begin{array}{rrr|r} \n    1x^2 &  -9x & -27 & -123 \n\\end{array}</math>\n\nThe result of our division is:\n:<math>\\frac{x^3 - 12x^2 - 42}{x - 3} = x^2 - 9x - 27 - \\frac{123}{x - 3}</math>\n\n===Evaluating polynomials by the remainder theorem===\n\nThe above form of synthetic division is useful in the context of the [[polynomial remainder theorem]] for evaluating [[univariate]] polynomials. To summarize, the value of <math>p(x)</math> at <math>a</math> is equal to the [[remainder]] of <math>\\frac{p(x)}{(x-a)}</math>. The advantage of calculating the value this way is that it requires just over half as many multiplication steps as naive evaluation. An alternative evaluation strategy is [[Horner's method]].\n\n== Expanded synthetic division ==\nThis method generalizes to division by any [[monic polynomial]] with only a slight modification with '''changes in bold'''.  Using the same steps as before,  perform the following division:\n:<math>\\frac{x^3 - 12x^2 - 42}{x^2 + x - 3}</math>\n\nWe concern ourselves only with the coefficients.\nWrite the coefficients of the polynomial to be divided at the top.\n:<math>\t\\begin{array}{|rrrr} \n\\     1 & -12 & 0 & -42 \n\\end{array}</math>\n\nNegate the coefficients of the divisor. \n:<math>\t\\begin{array}{rrr} \n     -1x^2 &-1x &+3\n\\end{array}</math>\n\nWrite in every coefficient but the first one on the left '''in an upward right diagonal''' (see next diagram).\n:<math>\\begin{array}{cc}\n    \\begin{array}{rr} \\\\ &3 \\\\ -1& \\\\ \\end{array}\n    &\n    \\begin{array}{|rrrr} \\ \n        1 & -12 & 0 & -42 \\\\\n          &     &   &     \\\\\n          &     &   &     \\\\\n        \\hline \n    \\end{array}\n\\end{array}</math>\n\nNote the change of sign from  '''1 to &minus;1 and from &minus;3 to 3 '''. \"Drop\" the first coefficient after the bar to the last row.\n\n:<math>\\begin{array}{cc}\n    \\begin{array}{rr} \\\\ &3 \\\\ -1& \\\\ \\\\ \\end{array}\n    &\n    \\begin{array}{|rrrr} \n        1 & -12 & 0 & -42 \\\\\n          &     &   &     \\\\\n          &     &   &     \\\\\n        \\hline \n        1 &     &   &     \\\\    \n    \\end{array}\n\\end{array}</math>\n\nMultiply the dropped number by the '''diagonal''' before the bar, and place the resulting entries '''diagonally to the right''' from the dropped entry.\n:<math>\\begin{array}{cc}\n    \\begin{array}{rr} \\\\ &3 \\\\ -1& \\\\ \\\\ \\end{array}\n    &\n    \\begin{array}{|rrrr} \n        1 & -12 & 0 & -42 \\\\\n          &     & 3 &     \\\\\n          &  -1 &   &     \\\\\n        \\hline \n        1 &     &   &     \\\\    \n    \\end{array}\n\\end{array}</math>\n\nPerform an addition in the next column.\n:<math>\\begin{array}{cc}\n    \\begin{array}{rr} \\\\ &3 \\\\ -1& \\\\ \\\\ \\end{array}\n    &\n    \\begin{array}{|rrrr} \n        1 & -12 & 0 & -42 \\\\\n          &     & 3 &     \\\\\n          &  -1 &   &     \\\\\n        \\hline \n        1 & -13 &   &     \\\\    \n    \\end{array}\n\\end{array}</math>\n\nRepeat the previous two steps '''until you would go past the entries at the top with the next diagonal'''.\n:<math>\\begin{array}{cc}\n    \\begin{array}{rr} \\\\ &3 \\\\ -1& \\\\ \\\\ \\end{array}\n    &\n    \\begin{array}{|rrrr} \n        1 & -12 &  0 & -42 \\\\\n          &     &  3 & -39 \\\\\n          &  -1 & 13 &     \\\\\n        \\hline \n        1 & -13 & 16 &     \\\\    \n    \\end{array}\n\\end{array}</math>\n\nThen simply add up any remaining columns.\n:<math>\\begin{array}{cc}\n    \\begin{array}{rr} \\\\ &3 \\\\ -1& \\\\ \\\\ \\end{array}\n    &\n    \\begin{array}{|rrrr} \n        1 & -12 &  0 & -42 \\\\\n          &     &  3 & -39 \\\\\n          &  -1 & 13 &     \\\\\n        \\hline \n        1 & -13 & 16 & -81 \\\\    \n    \\end{array}\n\\end{array}</math>\n\nCount the terms to the left of the bar.  Since there are two, the remainder has degree one and this is the two right-most terms under the bar. Mark the separation with a vertical bar.\n:<math>\t\\begin{array}{rr|rr} \n    1 &  -13 & 16 & -81 \n\\end{array}</math>\n \nThe terms are written with increasing degree from right to left beginning with degree zero for both the remainder and the result.\n:<math>\t\\begin{array}{rr|rr} \n    1x &  -13 & 16x & -81 \n\\end{array}</math>\n\nThe result of our division is:\n:<math>\\frac{x^3 - 12x^2 - 42}{x^2 + x - 3} = x - 13 + \\frac{16x - 81}{x^2 + x - 3}</math>\n\n=== For non-monic divisors ===\n\nWith a little prodding, the expanded technique may be generalised even further to work for any polynomial, not just monics. The usual way of doing this would be to divide the divisor <math>g(x)</math> with its leading coefficient (call it ''a''):\n:<math>h(x) = \\frac{g(x)}{a}</math>\n\nthen using synthetic division with <math>h(x)</math> as the divisor, and then dividing the quotient by ''a'' to get the quotient of the original division (the remainder stays the same). But this often produces unsightly fractions which get removed later, and is thus more prone to error. It is possible to do it without first reducing the coefficients of <math>g(x)</math>.\n\nAs can be observed by first performing long division with such a non-monic divisor, the coefficients of <math>f(x)</math> are divided by the leading coefficient of <math>g(x)</math> after \"dropping\", and before multiplying.\n\nLet's illustrate by performing the following division:\n\n:<math>\\frac{6x^3+5x^2-7}{3x^2-2x-1}</math>\n\nA slightly modified table is used:\n\n:<math>\\begin{array}{cc}\n    \\begin{array}{rrr} \\\\ &1& \\\\ 2&& \\\\ \\\\&&/3 \\\\ \\end{array}\n    \\begin{array}{|rrrr} \n        6 & 5 & 0 & -7 \\\\\n          &     &  &     \\\\\n          &    &   &     \\\\\n        \\hline\n          &     &   &     \\\\ \n          &     &   &     \\\\   \n    \\end{array}\n\\end{array}</math>\n\nNote the extra row at the bottom. This is used to write values found by dividing the \"dropped\" values by the leading coefficient of <math>g(x)</math> (in this case, indicated by the ''/3''; note that, unlike the rest of the coefficients of <math>g(x)</math>, the sign of this number is not changed).\n\nNext, the first coefficient of <math>f(x)</math> is dropped as usual:\n\n:<math>\\begin{array}{cc}\n    \\begin{array}{rrr} \\\\ &1& \\\\ 2&& \\\\ \\\\&&/3 \\\\ \\end{array}\n    \\begin{array}{|rrrr} \n        6 & 5 & 0 & -7 \\\\\n          &     &  &     \\\\\n          &    &   &     \\\\\n        \\hline\n        6 &     &   &     \\\\ \n          &     &   &     \\\\   \n    \\end{array}\n\\end{array}</math>\n\nand then the dropped value is divided by 3 and placed in the row below:\n\n:<math>\\begin{array}{cc}\n    \\begin{array}{rrr} \\\\ &1& \\\\ 2&& \\\\ \\\\&&/3 \\\\ \\end{array}\n    \\begin{array}{|rrrr} \n        6 & 5 & 0 & -7 \\\\\n          &     &  &     \\\\\n          &    &   &     \\\\\n        \\hline\n        6 &     &   &     \\\\ \n        2 &     &   &     \\\\   \n    \\end{array}\n\\end{array}</math>\n\nNext, the '''new''' (divided) value is used to fill the top rows with multiples of 2 and 1, as in the expanded technique:\n\n:<math>\\begin{array}{cc}\n    \\begin{array}{rrr} \\\\ &1& \\\\ 2&& \\\\ \\\\&&/3 \\\\ \\end{array}\n    \\begin{array}{|rrrr} \n        6 & 5 & 0 & -7 \\\\\n          &   & 2 &     \\\\\n          & 4 &   &     \\\\\n        \\hline\n        6 &     &   &     \\\\ \n        2 &     &   &     \\\\   \n    \\end{array}\n\\end{array}</math>\n\nThe 5 is dropped next, with the obligatory adding of the 4 below it, and the answer is divided again:\n\n:<math>\\begin{array}{cc}\n    \\begin{array}{rrr} \\\\ &1& \\\\ 2&& \\\\ \\\\&&/3 \\\\ \\end{array}\n    \\begin{array}{|rrrr} \n        6 & 5 & 0 & -7 \\\\\n          &   & 2 &     \\\\\n          & 4 &   &     \\\\\n        \\hline\n        6 & 9   &   &     \\\\ \n        2 & 3   &   &     \\\\   \n    \\end{array}\n\\end{array}</math>\n\nThen the 3 is used to fill the top rows:\n\n:<math>\\begin{array}{cc}\n    \\begin{array}{rrr} \\\\ &1& \\\\ 2&& \\\\ \\\\&&/3 \\\\ \\end{array}\n    \\begin{array}{|rrrr} \n        6 & 5 & 0 & -7 \\\\\n          &   & 2 &  3  \\\\\n          & 4 & 6 &     \\\\\n        \\hline\n        6 & 9 &   &     \\\\ \n        2 & 3 &   &     \\\\   \n    \\end{array}\n\\end{array}</math>\n\nAt this point, if, after getting the third sum, we were to try and use it to fill the top rows, we would \"fall off\" the right side, thus the third sum is the first coefficient of the remainder, as in regular synthetic division. But the values of the remainder are '''not''' divided by the leading coefficient of the divisor:\n\n:<math>\\begin{array}{cc}\n    \\begin{array}{rrr} \\\\ &1& \\\\ 2&& \\\\ \\\\&&/3 \\\\ \\end{array}\n    \\begin{array}{|rrrr} \n        6 & 5 & 0 & -7 \\\\\n          &   & 2 &  3  \\\\\n          & 4 & 6 &     \\\\\n        \\hline\n        6 & 9 & 8 & -4  \\\\ \n        2 & 3 &   &     \\\\   \n    \\end{array}\n\\end{array}</math>\n\nNow we can read off the coefficients of the answer. As in expanded synthetic division, the last two values (2 is the degree of the divisor) are the coefficients of the remainder, and the remaining values are the coefficients of the quotient:\n\n:<math>\t\\begin{array}{rr|rr} \n    2x &  +3 & 8x & -4 \n\\end{array}</math>\n\nand the result is\n\n:<math>\\frac{6x^3+5x^2-7}{3x^2-2x-1} = 2x + 3 + \\frac{8x - 4}{3x^2-2x-1}</math>\n\n=== Compact Expanded Synthetic Division ===\n\nHowever, the '''diagonal''' format above becomes less space-efficient when the degree of the divisor exceeds half of the degree of the dividend. It is easy to see that we have complete freedom to write each product in any row, as long as it is in the correct column. So the algorithm can be '''compactified''' by a '''greedy strategy''', as illustrated in the division below.\n\n:<math>\\dfrac{ax^7+bx^6+cx^5+dx^4+ex^3+fx^2+gx+h}{ix^4-jx^3-kx^2-lx-m}=nx^3+ox^2+px+q+\\dfrac{rx^3+sx^2+tx+u}{ix^4-jx^3-kx^2-lx-m}</math>\n\n:<math>\\begin{array}{cc} \\begin{array}{rrrr} \\\\ \\\\ \\\\ \\\\ j &k & l & m \\\\ \\end{array} & \\begin{array}{|rrrr|rrrr} & & & & qj & & & \\\\ & & & pj & pk & qk & & \\\\ & & oj & ok & ol & pl & ql & \\\\ & nj & nk & nl & nm & om & pm & qm \\\\ a & b & c & d & e & f & g & h \\\\ \\hline a & o_0 & p_0 & q_0 & r & s & t & u \\\\ n & o & p & q & & & & \\\\ \\end{array} \\end{array}</math>\n\nThe following describes how to perform the algorithm; this algorithm includes steps for dividing non-monic divisors:\n\n{{ordered list\n|1=\nWrite the coefficients of the dividend on a bar\n\n<math>\\begin{array}{cc} \\begin{array}{|rrrrrrrr} \\ a & b & c & d & e & f & g & h \\\\ \\hline \\end{array} \\end{array}</math>\n\n|2=\nIgnoring the first (leading) coefficient of the divisor, negate each coefficients and place them on the left-hand side of the bar.\n\n<math>\\begin{array}{cc} \\begin{array}{rrrr} j &k & l & m \\\\ \\end{array} & \\begin{array}{|rrrrrrrr}\\ a & b & c & d & e & f & g & h \\\\ \\hline \\end{array} \\end{array}</math>\n\n|3=\nFrom the number of coefficients placed on the left side of the bar, count the number of dividend coefficients above the bar, starting from the rightmost column. Then place a vertical bar to the left, and as well as the row below, of that column. This vertical bar marks the separation between the quotient and the remainder.\n\n<math>\\begin{array}{cc} \\begin{array}{rrrr} j &k & l & m \\\\ \\\\ \\end{array} & \\begin{array}{|rrrr|rrrr} a & b & c & d & e & f & g & h \\\\ \\hline & & & & & & & \\\\ \\end{array} \\end{array}</math>\n\n|4=\nDrop the first coefficient of the dividend below the bar.\n\n<math>\\begin{array}{cc} \\begin{array}{rrrr} j &k & l & m \\\\ \\\\ \\end{array} & \\begin{array}{|rrrr|rrrr} a & b & c & d & e & f & g & h \\\\ \\hline a &  & & & & & & \\\\ \\end{array} \\end{array}</math>\n\n|5=\n{{unordered list\n|Divide the previously dropped/summed number by the leading coefficient of the divisor and place it on the row below (this doesn't need to be done if the leading coefficient is 1).\n\nIn this case <math>n = \\dfrac{a}{i}</math>.\n\n|Multiply the previously dropped/summed number (or the divided dropped/summed number) to each negated divisor coefficients on the left (starting with the left most); skip if the dropped/summed number is zero. Place each product on top of the subsequent columns.\n\n}}\n\n<math>\\begin{array}{cc} \\begin{array}{rrrr} \\\\ j &k & l & m \\\\ \\end{array} & \\begin{array}{|rrrr|rrrr} & nj & nk & nl & nm & & & \\\\ a & b & c & d & e & f & g & h \\\\ \\hline a & & & & & & & \\\\ n & & & & & & & \\\\ \\end{array} \\end{array}</math>\n\n|6=\nPerform an column-wise addition on the next column.<br />\n<math>\\begin{array}{cc} \\begin{array}{rrrr} \\\\ j &k & l & m \\\\ \\end{array} & \\begin{array}{|rrrr|rrrr} & nj & nk & nl & nm & & & \\\\ a & b & c & d & e & f & g & h \\\\ \\hline a & o_0 & & & & & & \\\\ n & & & & & & & \\\\ \\end{array} \\end{array}</math>\n\n|7=\nRepeat the previous two steps. Stop when you performed the previous two steps on the number just before the vertical bar.\n\nLet <math>o = \\dfrac{o_0}{i}</math>.\n\n<math>\\begin{array}{cc} \\begin{array}{rrrr} \\\\ \\\\ j &k & l & m \\\\ \\end{array} & \\begin{array}{|rrrr|rrrr} & & oj & ok & ol & & & \\\\ & nj & nk & nl & nm & om & & \\\\ a & b & c & d & e & f & g & h \\\\ \\hline a & o_0 & p_0 & & & & & \\\\ n & o & & & & & & \\\\ \\end{array} \\end{array}</math>\n\nLet <math>p = \\dfrac{p_0}{i}</math>.\n\n<math>\\begin{array}{cc} \\begin{array}{rrrr} \\\\ \\\\ \\\\ j &k & l & m \\\\ \\end{array} & \\begin{array}{|rrrr|rrrr} & & & pj & pk & & & \\\\ & & oj & ok & ol & pl & & \\\\ & nj & nk & nl & nm & om & pm & \\\\ a & b & c & d & e & f & g & h \\\\ \\hline a & o_0 & p_0 & q_0 & & & & \\\\ n & o & p & & & & & \\\\ \\end{array} \\end{array}</math>\n\nLet <math>q = \\dfrac{q_0}{i}</math>.\n\n<math>\\begin{array}{cc} \\begin{array}{rrrr} \\\\ \\\\ \\\\ \\\\ j &k & l & m \\\\ \\end{array} & \\begin{array}{|rrrr|rrrr} & & & & qj & & & \\\\ & & & pj & pk & qk & & \\\\ & & oj & ok & ol & pl & ql & \\\\ & nj & nk & nl & nm & om & pm & qm \\\\ a & b & c & d & e & f & g & h \\\\ \\hline a & o_0 & p_0 & q_0 & r & & & \\\\ n & o & p & q & & & & \\\\ \\end{array} \\end{array}</math>\n\n|8=\nPerform the remaining column-wise additions on the subsequent columns (calculating the remainder).<br />\n<math>\\begin{array}{cc} \\begin{array}{rrrr} \\\\ \\\\ \\\\ \\\\ j &k & l & m \\\\ \\end{array} & \\begin{array}{|rrrr|rrrr} & & & & qj & & & \\\\ & & & pj & pk & qk & & \\\\ & & oj & ok & ol & pl & ql & \\\\ & nj & nk & nl & nm & om & pm & qm \\\\ a & b & c & d & e & f & g & h \\\\ \\hline a & o_0 & p_0 & q_0 & r & s & t & u \\\\ n & o & p & q & & & & \\\\ \\end{array} \\end{array}</math>\n\n|9=\nThe bottommost results below the horizontal bar are coefficients of the polynomials, the remainder and the quotient. Where the coefficients of the quotient is to the left of the vertical bar separation, and the coefficients of the remainder to the right. These coefficients would be interpreted with increasing degree from right to left beginning with degree zero for both the remainder and the quotient. We interpret the results to get:\n\n<math>\\dfrac{ax^7+bx^6+cx^5+dx^4+ex^3+fx^2+gx+h}{ix^4-jx^3-kx^2-lx-m}=nx^3+ox^2+px+q+\\dfrac{rx^3+sx^2+tx+u}{ix^4-jx^3-kx^2-lx-m}</math>\n\n}}\n\n=== Python implementation ===\n\nThe following snippet implements the Extended Synthetic Division for non-monic polynomials (which also supports monic polynomials, since it is a generalization):\n\n<source lang=\"python\" line>\ndef extended_synthetic_division(dividend, divisor):\n    '''\n    Fast polynomial division by using Extended Synthetic Division. \n    Also works with non-monic polynomials.\n\n    dividend and divisor are both polynomials, which are here simply lists of coefficients. \n    Eg: x**2 + 3*x + 5 will be represented as [1, 3, 5]\n    '''\n    out = list(dividend) # Copy the dividend\n    normalizer = divisor[0]\n    for i in range(len(dividend) - len(divisor) + 1):\n        out[i] /= normalizer # for general polynomial division (when polynomials are non-monic),\n                             # we need to normalize by dividing the coefficient with the divisor's first coefficient\n        coef = out[i]\n        if coef != 0: # useless to multiply if coef is 0\n            for j in range(1, len(divisor)): # in synthetic division, we always skip the first coefficient of the divisor,\n                                             # because it is only used to normalize the dividend coefficients\n                out[i + j] += -divisor[j] * coef\n    '''\n    The resulting out contains both the quotient and the remainder, the remainder being the size of the divisor (the remainder\n    has necessarily the same degree as the divisor since it is what we couldn't divide from the dividend), so we compute the index\n    where this separation is, and return the quotient and remainder.\n    '''\n    separator = 1 - len(divisor)\n    return out[:separator], out[separator:] # return quotient, remainder.\n</source>\n\n==See also==\n*[[Euclidean domain]]\n*[[Greatest common divisor of two polynomials]]\n*[[Gröbner basis]]\n*[[Horner scheme]]\n*[[Polynomial remainder theorem]]\n*[[Ruffini's rule]]\n\n==References==\n*{{cite journal |author=Lianghuo Fan |title=A Generalization of Synthetic Division and A General Theorem of Division of Polynomials |journal=Mathematical Medley |year=2003 |volume=30 |issue=1 |pages=30–37 |url=http://eprints.soton.ac.uk/168861/1/FLH_article_on_polynomial_division.pdf}}\n*{{cite journal |author=Li Zhou |title=Short Division of Polynomials |journal=College Mathematics Journal |year=2009 |volume=40 |issue=1 |pages=44–46 |doi=10.4169/193113409x469721}}\n\n==External links==\n*{{MathWorld |title=Synthetic Division |id=SyntheticDivision |author=[[Len Goodman|Goodman, Len]]; [[Christopher Stover|Stover, Christopher]]; and [[Eric W. Weisstein|Weisstein, Eric W.]] }}\n*{{MathWorld |title=Ruffini's Rule |id=RuffinisRule |author=[[Christopher Stover|Stover, Christopher]] }}\n* {{Britannica|578673|Synthetic division (mathematics)}}\n[[Category:Computer algebra]]\n[[Category:Division (mathematics)]]\n[[Category:Polynomials]]"
    },
    {
      "title": "Theory of equations",
      "url": "https://en.wikipedia.org/wiki/Theory_of_equations",
      "text": "{{distinguish|equational theory}}\n{{unreferenced|date=May 2014}}\n\nIn [[algebra]], the '''theory of equations''' is the study of [[algebraic equation]]s (also called “polynomial equations”), which are [[equation (mathematics)|equation]]s defined by a [[polynomial]]. The main problem of the theory of equations was to know when an algebraic equation has an [[algebraic solution]]. This problem was completely solved in 1830 by [[Évariste Galois]], by introducing what is now called [[Galois theory]].\n\nBefore Galois, there was no clear distinction between the “theory of equations” and “algebra”. Since then algebra has been dramatically enlarged to include many new subareas, and the theory of algebraic equations receives much less attention. Thus, the term \"theory of equations\" is mainly used in the context of the [[history of mathematics]], to avoid confusion between old and new meanings of “algebra”.\n\n==History==\nUntil the end of the 19th century, \"theory of equations\" was almost synonymous with \"algebra\". For a long time, the main problem was to find the solutions of a single non-linear polynomial equation in a single [[Equation|unknown]]. The fact that a [[complex number|complex]] solution always exists is the [[fundamental theorem of algebra]], which was proved only at the beginning of the 19th century and does not have a purely algebraic proof. Nevertheless, the main concern of the algebraists was to solve in terms of radicals, that is to express the solutions by a formula which is built with the four operations of [[arithmetics]] and with [[nth root]]s. This was done up to degree four during the 16th century. [[Scipione del Ferro]] and [[Niccolò Fontana Tartaglia]] discovered solutions for [[cubic equation]]s. [[Gerolamo Cardano]] published them in his 1545 book ''[[Ars Magna (Gerolamo Cardano)|Ars Magna]]'', together with a solution for the [[quartic equation]]s, discovered by his student [[Lodovico Ferrari]]. In 1572 [[Rafael Bombelli]] published his ''L'Algebra'' in which he showed how to deal with the [[imaginary number|imaginary quantities]] that could appear in Cardano's formula for solving cubic equations.\n\nThe case of higher degrees remained open until the 19th century, when [[Niels Henrik Abel]] proved that some fifth degree equations cannot be solved in radicals (the [[Abel–Ruffini theorem]]) and [[Évariste Galois]] introduced a theory (presently called [[Galois theory]]) to decide which equations are solvable by radicals.\n\n==Further problems==\nOther classical problems of the theory of equations are the following:\n* [[Linear equation]]s: this problem was solved during antiquity.\n* [[System of linear equations|Simultaneous linear equations]]: The general theoretical solution was provided by [[Gabriel Cramer]] in 1750. However devising efficient methods ([[algorithms]]) to solve these systems remains an active subject of research now called [[linear algebra]].\n* Finding the integer solutions of an equation or of a system of equations. These problems are now called [[Diophantine equation]]s, which are considered a part of [[number theory]] (see also [[integer programming]]).\n* [[System of polynomial equations|Systems of polynomial equations]]: Because of their difficulty, these systems, with few exceptions, have been studied only since the second part of the 19th century. They have led to the development of [[algebraic geometry]].\n\n== See also ==\n\n* [[Root-finding algorithm]]\n* [[Properties of polynomial roots]]\n* [[Quintic function]]\n\n==Further reading==\n\n*Uspensky, James Victor, ''Theory of Equations'' (McGraw-Hill),1963 [https://www.amazon.com/Theory-Equations-James-Victor-Uspensky/dp/0070667365/ref=pd_sim_sbs_14_1?ie=UTF8&refRID=0WCDZANX7RY8V3T7T8Z3]\n*Dickson, Leonard E., ''Elementary Theory of Equations'' (Classic Reprint, Forgotten Books), 2012  [https://www.amazon.com/Elementary-Theory-Equations-Classic-Reprint/dp/1440075077/ref=sr_1_22?s=books&ie=UTF8&qid=1440946671&sr=1-22&keywords=The+theory+of+equations]\n\n[[Category:History of algebra]]\n[[Category:Polynomials]]\n[[Category:Equations]]"
    },
    {
      "title": "Thomae's formula",
      "url": "https://en.wikipedia.org/wiki/Thomae%27s_formula",
      "text": "In [[mathematics]], '''Thomae's formula''' is a formula introduced by {{harvs|txt|last=Thomae|authorlink=Carl Johannes Thomae|year=1870}} relating [[theta constant]]s to the [[branch point]]s of a [[hyperelliptic curve]] {{harv|Mumford|1984|loc=section 8}}.\n\n==History==\n\nIn 1824 the [[Abel–Ruffini theorem]] established that [[polynomial equation]]s of a degree of five or higher could have no solutions in [[Nth root|radicals]].  It became clear to mathematicians since then that one needed to go beyond radicals in order to express the solutions to equations of the fifth and higher degrees. In 1858, [[Charles Hermite]], [[Leopold Kronecker]], and [[Francesco Brioschi]] independently discovered that the [[quintic equation]] could be solved with [[Bring radical#The Hermite.E2.80.93Kronecker.E2.80.93Brioschi characterization|elliptic transcendents]]. This proved to be a generalization of the radical, which can be written as:\n:<math>\\sqrt[n]{x}=\\exp \\left({{\\frac {1}{n}}\\ln x}\\right) = \\exp \\left(\\frac{1}{n}\\int^x_1\\frac{dt}{t}\\right).</math>\nWith the restriction to only this exponential, as shown by [[Galois theory]], only compositions of [[Abelian extension]]s may be constructed, which suffices only for equations of the fourth degree and below. Something more general is required for equations of higher degree, so to solve the quintic, Hermite, et. al. replaced the exponential by an [[elliptic modular function]] and the integral (logarithm) by an [[elliptic integral]]. Kronecker believed that this was a special case of a still more general method.<ref name=\"kronecker\">\n{{cite journal\n | last = Kronecker\n | first = Leopold\n | year = 1858\n | title = Sur la résolution de l'equation du cinquème degré\n | journal = C. R. Acad. Sci.\n | volume = 46\n | pages = 1150–1152}}</ref> [[Camille Jordan]] showed<ref name=\"jordan\">\n{{cite book\n | last = Jordan\n | first = Camille\n | year = 1870\n | title = Traité des substitutions et des équations algébriques\n | publisher = Gauthier-Villars\n | location = Paris}}</ref> that any algebraic equation may be solved by use of modular functions. This was accomplished by [[Carl Johannes Thomae]] in 1870.<ref name=\"thomae\">{{cite journal\n | last = Thomae\n | first = Carl Johannes\n | title=Beitrag zur Bestimmung von θ(0,0,...0) durch die Klassenmoduln algebraischer Funktionen\n | journal=Journal für die reine und angewandte Mathematik\n | volume=71\n | year= 1870\n |pages= 201–222\n |url=http://www.digizeitschriften.de/main/dms/img/?PPN=PPN243919689_0071&DMDID=dmdlog17}}</ref> The process involved replacing the exponential in the nth root and the elliptic modular function in the approach by Hermite, et. al. by still more general [[Siegel modular form]]s and the integral by a [[hyperelliptic integral]]. Hiroshi Umemura<ref>\n{{cite conference\n | last = Umemura\n | first = Hiroshi\n | year = 1984\n | title = Resolution of algebraic equations by theta constants\n | booktitle = Tata Lectures on Theta II\n | pages = 3.261–3.272\n | editor = David Mumford\n | publisher = Birkhäuser\n | isbn = 3-7643-3109-7}}</ref> expressed these modular functions in terms of higher genus [[theta function]]s.\n\n==Formula==\n\nIf we have a [[polynomial function]]:\n\n:<math>f(x) = a_0x^n + a_1x^{n-1} + \\cdots + a_n</math>\n\nwith <math>a_0 \\ne 0</math> [[irreducible polynomial|irreducible]] over a certain subfield of the complex numbers, then its roots <math>x_k</math> may be expressed by the following equation involving [[theta functions]] of zero argument ([[theta constant]]s):\n\n:<math>\n\\begin{align}\nx_k = {} & \\left[\\theta\\left(\n\\begin{matrix}\n1 & 0 & \\cdots & 0 \\\\\n0 & \\cdots & 0 & 0\n\\end{matrix}\n\\right)(\\Omega)\\right]^4 \\left[\\theta\\left(\n\\begin{matrix}\n1 & 1 & 0 & \\cdots & 0 \\\\\n0 & \\cdots & 0 & 0 & 0\n\\end{matrix}\n\\right)(\\Omega)\\right]^4 \\\\[6pt]\n& {} + \\left[\\theta\\left(\n\\begin{matrix}\n0 & \\cdots & 0 \\\\\n0 & \\cdots & 0\n\\end{matrix}\n\\right)(\\Omega)\\right]^4 \\left[\\theta\\left(\n\\begin{matrix}\n0 & 1 & 0 & \\cdots & 0 \\\\\n0 & 0 & 0 & \\cdots & 0\n\\end{matrix}\n\\right)(\\Omega)\\right]^4 \\\\[6pt]\n& {} - \\frac{\\left[\\theta\\left(\n\\begin{matrix}\n0 & 0 & \\cdots & 0 \\\\\n1 & 0 & \\cdots & 0\n\\end{matrix}\n\\right)(\\Omega)\\right]^4 \\left[\\theta\\left(\n\\begin{matrix}\n0 & 1 & 0 & \\cdots & 0 \\\\\n1 & 0 & \\cdots & 0 & 0\n\\end{matrix}\n\\right)(\\Omega)\\right]^4}{\n2 \\left[\\theta\\left(\n\\begin{matrix}\n1 & 0 & \\cdots & 0 \\\\\n0 & \\cdots & 0 & 0\n\\end{matrix}\n\\right)(\\Omega)\\right]^4 \\left[\\theta\\left(\n\\begin{matrix}\n1 & 1 & 0 & \\cdots & 0 \\\\\n0 & 0 & \\cdots & 0 & 0\n\\end{matrix}\n\\right)(\\Omega)\\right]^4\n}\n\\end{align}\n</math>\n\nwhere <math>\\Omega</math> is the [[period matrix]] derived from one of the following hyperelliptic integrals:\n\n:<math>\nu(a) = \\int^a_1 \\frac{dx}{\\sqrt{x(x-1)f(x)}}\n</math>\n\nif <math>f(x)</math> is of odd degree, or\n\n:<math>\nu(a) = \\int^a_1 \\frac{dx}{\\sqrt{x(x-1)(x-2)f(x)}}\n</math>\n\nif <math>f(x)</math> is of even degree.\n\nThis formula applies to any algebraic equation of any degree without need for a [[Tschirnhaus transformation]] or any other manipulation to bring the equation into a specific normal form, such as the [[Bring–Jerrard form]] for the quintic. However, application of this formula in practice is difficult because the relevant hyperelliptic integrals and higher genus theta functions are very complex.\n\n==Notes==\n{{reflist}}\n{{refbegin}}\n\n==References==\n\n*{{Citation | last1=Mumford | first1=David | author1-link=David Mumford | title=Tata lectures on theta. II | publisher=Birkhäuser Boston | location=Boston, MA | series=Progress in Mathematics | isbn=978-0-8176-3110-9 |mr=742776 | year=1984 | volume=43}}\n*{{citation|first=Carl Johannes |last=Thomae |title=Beitrag zur Bestimmung von θ(0,0,...0) durch die Klassenmoduln algebraischer Funktionen|journal=Journal für die reine und angewandte Mathematik|volume=71|year= 1870|pages= 201–222|url=http://www.digizeitschriften.de/main/dms/img/?PPN=PPN243919689_0071&DMDID=dmdlog17}}\n\n[[Category:Riemann surfaces]]\n[[Category:Theorems in number theory]]\n[[Category:Polynomials]]\n[[Category:Equations]]\n[[Category:Modular forms]]"
    },
    {
      "title": "Tian yuan shu",
      "url": "https://en.wikipedia.org/wiki/Tian_yuan_shu",
      "text": "{{short description|Chinese system of algebra}}\n[[File:算学启蒙.jpg|thumb|right|300px|''Tian yuan shu'' in Zhu Shijie's text ''[[Zhu Shijie#Suanxue qimeng|Suanxue qimeng]]'']]\n[[File:Wylie on Tian Yuen.jpg|thumb|right|300px|The technique described in [[Alexander Wylie (missionary)|Alexander Wylie]]'s ''Jottings on the Science of the Chinese'']]\n\n'''''Tian yuan shu''''' ({{zh|s=天元术|t=天元術|p=tiān yuán shù}}) is a Chinese system of [[algebra]] for [[polynomial]] equations created in the 13th century. It is first known from the writing of [[Li Zhi (mathematician)|Li Zhi]] (Li Ye), though it was created earlier.\n\nThe mathematical culture in which it was created was lost due to war and general suspiciousness during the [[Ming dynasty]] of knowledge from the (Mongolian) [[Yuan dynasty]]. The writings of [[Li Zhi (mathematician)|Li Zhi]] (''[[Ceyuan haijing]]''), [[Zhu Shijie]] (''[[Zhu Shijie#Jade Mirror of the Four Unknowns|Jade Mirror of the Four Unknowns]]'') and others could no longer be fully understood, until the arrival of western mathematics in China.\n\nMeanwhile, ''tian yuan shu'' arrived in Japan, where it is called '''''tengen-jutsu'''''. Zhu's text ''[[Zhu Shijie#Suanxue qimeng|Suanxue qimeng]]'' was deciphered and was important in the development of [[Japanese mathematics]] (''wasan'') in the 17th and 18th centuries.\n\n== Description ==\n''Tian yuan shu'' means \"method of the heavenly element\" or \"technique of the celestial unknown\". The \"heavenly element\" is the unknown [[variable (mathematics)|variable]], usually written {{math|''x''}} in modern notation.\n\nIt is a positional system of [[rod numeral]]s to represent [[polynomial equation]]s. For example, {{math|2''x''<sup>2</sup> + 18''x'' − 316 {{=}} 0}} is represented as\n\n[[File:Polynomial equation with rod numerals.png]], which in Arabic numerals is [[File:Polynomial equation in tian yuan shu with arabic numerals.png]]\n\nThe {{lang|zh|元}} (''yuan'') denotes the unknown {{math|''x''}}, so the numerals on that line mean {{math|18''x''}}. The line below is the constant term ({{math|-316}}) and the line above is the [[coefficient]] of the [[quadratic function|quadratic]] ({{math|''x''<sup>2</sup>}}) term. The system accommodates arbitrarily high [[exponent]]s of the unknown by adding more lines on top and negative exponents by adding lines below the constant term. Decimals can also be represented.\n\nIn later writings of Li Zhi and Zhu Shijie, the line order was reversed so that the first line is the lowest exponent.\n\n==See also==\n*''[[Yigu yanduan]]''\n*''[[Ceyuan haijing]]''\n\n== References ==\n* {{cite book|last=Martzloff|first=Jean-Claude|others=trans. Stephen S. Wilson|title=A History of Chinese Mathematics|publisher=Springer|year=2006|pages=258–272|isbn=3-540-33782-2|url=https://books.google.com/books?id=ACK1jreKgCoC&pg=PA258|accessdate=2009-12-28}}\n* {{cite book|last=Murata|first=Tamotsu|title=Companion Encyclopedia of the History and Philosophy of the Mathematical Sciences|editor=Ivor Grattan-Guinness|publisher=JHU Press|year=2003|volume=1|pages=105–106|chapter=Indigenous Japanese mathematics, Wasan|isbn=0-8018-7396-7|url=https://books.google.com/books?id=2hDvzITtfdAC&pg=PA105|accessdate=2009-12-28}}\n\n[[Category:Chinese mathematics]]\n[[Category:Japanese mathematics]]\n[[Category:Polynomials]]\n[[Category:13th-century books]]"
    },
    {
      "title": "Touchard polynomials",
      "url": "https://en.wikipedia.org/wiki/Touchard_polynomials",
      "text": "{{Use American English|date = March 2019}}\n{{Short description|Sequence of polynomials}}\n{{for|a different family of polynomials Q<sub>n</sub> occasionally called Touchard polynomials|Bateman polynomials}}\n{{distinguish|Bell polynomials}}\n\nThe '''Touchard polynomials''', studied by {{harvs|txt|authorlink=Jacques Touchard|first=Jacques|last= Touchard|year=1939}}, also called the '''exponential polynomials''' or '''Bell polynomials''', comprise a [[polynomial sequence]] of [[binomial type]] defined by\n\n:<math>T_n(x)=\\sum_{k=0}^n S(n,k)x^k=\\sum_{k=0}^n\n\\left\\{ {n \\atop k} \\right\\}x^k,</math>\n\nwhere <math>S(n,k)=\\left\\{ {n \\atop k} \\right\\}</math>is a [[Stirling number of the second kind]], i.e., the number of [[partition of a set|partitions of a set]] of size ''n'' into ''k'' disjoint non-empty subsets.<ref name=Roman>{{cite book|last=Roman|first=Steven|title=The Umbral Calculus|year=1984|publisher=Dover|isbn=0-486-44139-3}}</ref><ref>{{cite journal|last=Boyadzhiev|first=Khristo N.|title=Exponential polynomials, Stirling numbers, and evaluation of some gamma integrals |arxiv=0909.0979|doi=10.1155/2009/168672|volume=2009|journal=Abstract and Applied Analysis|pages=1–18|bibcode=2009AbApA2009....1B}}</ref><ref>{{cite web|last=Brendt|first=Bruce C|title=RAMANUJAN REACHES HIS HAND FROM HIS GRAVE TO SNATCH YOUR THEOREMS FROM YOU|url=http://www.math.uiuc.edu/~berndt/articles/gravesnatching.pdf|accessdate=23 November 2013}}</ref><ref>{{MathWorld|urlname=BellPolynomial|title=Bell Polynomial}}</ref>\n\n== Properties ==\n\n=== Basic properties ===\nThe value at 1 of the ''n''th Touchard polynomial is the ''n''th [[Bell numbers|Bell number]], i.e., the number of [[partition of a set|partitions of a set]] of size ''n'':\n:<math>T_n(1)=B_n.</math>\n\nIf ''X'' is a [[random variable]] with a [[Poisson distribution]] with expected value λ, then its ''n''th moment is E(''X''<sup>''n''</sup>) = ''T''<sub>''n''</sub>(λ), leading to the definition:\n:<math>T_{n}(x)=e^{-x}\\sum_{k=0}^\\infty \\frac {x^k k^n} {k!}.</math>\n\nUsing this fact one can quickly prove that this [[polynomial sequence]] is of [[binomial type]], i.e., it satisfies the sequence of identities:\n:<math>T_n(\\lambda+\\mu)=\\sum_{k=0}^n {n \\choose k} T_k(\\lambda) T_{n-k}(\\mu).</math>\n\nThe Touchard polynomials constitute the only polynomial sequence of binomial type with the coefficient of ''x'' equal 1 in every polynomial.\n\nThe Touchard polynomials satisfy the Rodrigues-like formula:\n:<math>T_n \\left(e^x \\right) = e^{-e^x} \\frac{d^n}{dx^n}\\;e^{e^x}.</math>\n\nThe Touchard polynomials satisfy the [[recurrence relation]]\n:<math>T_{n+1}(x)=x \\left(1+\\frac{d}{dx} \\right)T_{n}(x)</math>\nand\n:<math>T_{n+1}(x)=x\\sum_{k=0}^n{n \\choose k}T_k(x).</math>\nIn the case ''x'' = 1, this reduces to the recurrence formula for the [[Bell numbers]].\n\nUsing the [[Umbral calculus|umbral notation]] ''T''<sup>''n''</sup>(''x'')=''T''<sub>''n''</sub>(''x''), these formulas become:\n:<math>T_n(\\lambda+\\mu)=\\left(T(\\lambda)+T(\\mu) \\right)^n,</math>\n:<math>T_{n+1}(x)=x \\left(1+T(x) \\right)^n.</math>\n\nThe [[generating function]] of the Touchard polynomials is\n:<math>\\sum_{n=0}^\\infty {T_n(x) \\over n!} t^n=e^{x\\left(e^t-1\\right)},</math>\nwhich corresponds to the [[Stirling numbers of the second kind#Generating functions|generating function of Stirling numbers of the second kind]].\n\nTouchard polynomials have [[contour integral]] representation:\n:<math>T_n(x)=\\frac{n!}{2\\pi i}\\oint\\frac{e^{x({e^t}-1)}}{t^{n+1}}\\,dt.</math>\n\n=== Zeroes ===\n\nAll zeroes of the Touchard polynomials are real and negative. This fact was observed by L. H. Harper in 1967.<ref name = 'Harper'>{{Cite journal \n| last = Harper | first = L. H.\n| title = Stirling behavior is asymptotically normal\n| year = 1967\n| volume = 38\n| issue = 2\n| pages = 410–414\n| journal = The Annals of Mathematical Statistics\n | doi=10.1214/aoms/1177698956\n}}</ref> \n\nThe smallest zero is bounded from below (in absolute value) by<ref name = 'MC'>{{Cite journal \n| last1 = Mező | first1 = István\n| last2 = Corcino | first2 = Roberto B.\n| title = The estimation of the zeros of the Bell and r-Bell polynomials\n| year = 2015\n| volume = 250\n| pages = 727–732\n| journal = Applied Mathematics and Computation\n | doi=10.1016/j.amc.2014.10.058\n}}</ref>\n:<math>\\frac1n\\binom{n}{2}+\\frac{n-1}{n}\\sqrt{\\binom{n}{2}^2-\\frac{2n}{n-1}\\left(\\binom{n}{3}+3\\binom{n}{4}\\right)},</math>\nalthough it is conjectured that the smallest zero grows linearly with the index ''n''.\n\nThe [[Mahler measure]] <math>M(T_n)</math>of the Touchard polynomials can be estimated as follows:<ref>{{cite web|last1=István|first1=Mező|title=On the Mahler measure of the Bell polynomials|url=https://sites.google.com/site/istvanmezo81/others|accessdate=7 November 2017}}</ref>\n:<math>\n\\frac{\\lbrace\\textstyle{n\\atop \\Omega_n}\\rbrace}{\\binom{n}{\\Omega_n}}\\le M(T_n)\\le\\sqrt{n+1}\\left\\{{n\\atop K_n}\\right\\},\n</math>\nwhere <math>\\Omega_n</math> and <math>K_n</math> are the smallest of the maximum two ''k'' indices such that\n<math>\\lbrace\\textstyle{n\\atop k}\\rbrace/\\binom{n}{k}</math> and <math>\\lbrace\\textstyle{n\\atop k}\\rbrace</math>\nare maximal, respectively.\n\n== Generalizations ==\n* Complete [[Bell polynomial]] <math>B_n(x_1,x_2,\\dots,x_n)</math> may be viewed as a multivariate generalization of Touchard polynomial <math>T_n(x)</math>, since <math>T_n(x) = B_n(x,x,\\dots,x).</math>\n* The Touchard polynomials (and thereby the [[Bell numbers]]) can be generalized, using the real part of the above integral, to non-integer order:\n*:<math>T_n(x)=\\frac{n!}{\\pi} \\int^{\\pi}_0 e^{x \\bigl(e^{\\cos(\\theta)} \\cos(\\sin(\\theta))-1 \\bigr)} \\cos \\bigl(x e^{\\cos(\\theta)} \\sin(\\sin(\\theta)) -n\\theta\\bigr) \\, d\\theta\\, .</math>\n\n== See also ==\n* [[Bell polynomials]]\n\n==References==\n{{Reflist}}\n\n*{{Citation | last1=Touchard | first1=Jacques | title=Sur les cycles des substitutions | doi=10.1007/BF02547349 | mr=1555449 | year=1939 | journal=[[Acta Mathematica]] | issn=0001-5962 | volume=70 | issue=1 | pages=243–297}}\n\n{{DEFAULTSORT:Touchard Polynomials}}\n[[Category:Polynomials]]"
    }
  ]
}