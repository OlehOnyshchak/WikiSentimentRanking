{
  "pages": [
    {
      "title": "Distributive property",
      "url": "https://en.wikipedia.org/wiki/Distributive_property",
      "text": "{{pp-vandalism|small=yes}}\n{{redirect distinguish|Distributivity|Distributivism}}\n[[File:Illustration of distributive property with rectangles.svg|thumb|Visualization of distributive law for positive numbers]]\nIn [[abstract algebra]] and [[formal logic]], the '''distributive property''' of [[binary operation]]s generalizes the '''distributive law''' from [[Boolean algebra (structure)|Boolean algebra]] and [[elementary algebra]]. In [[propositional calculus|propositional logic]], '''distribution''' refers to two [[Validity (logic)|valid]] [[rule of replacement|rules of replacement]]. The rules allow one to reformulate [[logical conjunction|conjunctions]] and [[logical disjunction|disjunctions]] within [[formal proof|logical proofs]].\n\nFor example, in [[arithmetic]]:\n: 2 ⋅ (1 + 3) = (2 ⋅ 1) + (2 ⋅ 3), but 2 / (1 + 3) ≠ (2 / 1) + (2 / 3).\n\nIn the left-hand  side of the first equation, the 2 multiplies the sum of 1 and 3; on the right-hand side, it multiplies the 1 and the 3 individually, with the products added afterwards.\nBecause these give the same final answer (8),  multiplication by 2 is said to ''distribute'' over addition of 1 and 3.\nSince one could have put any [[real number]]s in place of 2, 1, and 3 above, and still have obtained a true equation, [[multiplication]] of real numbers ''distributes'' over [[addition]] of real numbers.\n\n==Definition==\n\nGiven a [[Set (mathematics)|set]] {{math|''S''}} and two [[binary operator]]s ∗ and + on {{math|''S''}}, the operation:\n\n∗ is ''left-distributive'' over + if, [[given any]] elements {{math|''x'', ''y''}} and {{math|''z''}} of {{math|''S''}},\n:<math>x * (y + z) = (x * y) + (x * z),</math>\n∗ is ''right-distributive'' over + if, given any elements {{math|''x'', ''y''}}, and {{math|''z''}} of {{math|''S''}},\n:<math>(y + z) * x = (y * x) + (z * x),</math> and\n∗ is ''distributive'' over + if it is left- and right-distributive.<ref>[http://mathonline.wikidot.com/distributivity-of-binary-operations Distributivity of Binary Operations] from Mathonline</ref>\n\nNotice that when ∗ is [[commutative]], the three conditions above are [[logical equivalence|logically equivalent]].\n\n==Meaning==\nThe operators used for examples in this section are the [[binary operation]]s of [[addition]] (<math>+</math>) and [[multiplication]] (<math>\\cdot</math>) of [[number]]s.\n\nThere is a distinction between left-distributivity and right-distributivity:\n\n:<math>a \\cdot \\left( b \\pm c \\right) = a \\cdot b \\pm a \\cdot c</math>&nbsp;&nbsp;(left-distributive)\n:<math>(a \\pm b) \\cdot c = a \\cdot c \\pm b \\cdot c</math>&nbsp;&nbsp;(right-distributive)\n\nIn either case, the distributive property can be described in words as:\n\nTo multiply a [[summation|sum]] (or [[difference (mathematics)|difference]]) by a factor, each summand (or [[minuend]] and [[subtrahend]]) is multiplied by this factor and the resulting products are added (or subtracted).\n\nIf the operation outside the parentheses (in this case, the multiplication) is commutative, then left-distributivity implies right-distributivity  and vice versa.\n\nOne example of an operation that is \"only\" right-distributive is division, which is not commutative:\n\n:<math>(a \\pm b) \\div c = a \\div c \\pm b \\div c</math> \nIn this case, left-distributivity does not apply:\n\n:<math>a \\div(b \\pm c) \\neq a \\div b \\pm a \\div c</math>\n\nThe distributive laws are among the axioms for [[ring (mathematics)|rings]] (like the ring of [[integer]]s) and [[field (mathematics)|fields]] (like the field of [[rational number]]s). Here multiplication is distributive over addition, but addition is not distributive over multiplication. Examples of structures in which two operations are mutually related to each other by the distributive law (e.&nbsp;g., they distribute over each other) are [[Boolean algebras]] such as the [[algebra of sets]] or the [[switching algebra]].\n\nMultiplying sums can be put into words as follows: When a sum is multiplied by a sum, multiply each summand of a sum with each summand of the other sums (keeping track of signs), and then adding up all of the resulting products.\n\n==Examples==\n\n===Real numbers===\nIn the following examples, the use of the distributive law on the set of real numbers <math>\\R</math> is illustrated. When multiplication is mentioned in elementary mathematics, it usually refers to this kind of multiplication. From the point of view of algebra, the real numbers form a [[field (mathematics)|field]], which ensures the validity of the distributive law.\n\n;First example (mental and written multiplication)\nDuring mental arithmetic, distributivity is often used unconsciously:\n\n::<math>6 \\cdot 16 = 6 \\cdot (10+6) = 6\\cdot 10 + 6 \\cdot 6 = 60+36 = 96</math>\nThus, to calculate {{nowrap|6 ⋅ 16}} in one's head, one first multiplies {{nowrap|6 ⋅ 10}} and {{nowrap|6 ⋅ 6}} and add the intermediate results. Written multiplication is also based on the distributive law.\n\n;Second example (with variables)\n::<math>3a^2b \\cdot (4a - 5b) = 3a^2b \\cdot 4a - 3a^2b \\cdot 5b = 12a^3b - 15a^2b^2</math>\n\n;Third example (with two sums)\n::<math>\n\\begin{align}\n(a + b) \\cdot (a - b) & = a \\cdot (a - b) + b \\cdot (a - b) = a^2 - ab + ba - b^2 = a^2 - b^2 \\\\\n                      & = (a + b) \\cdot a - (a + b) \\cdot b = a^2 + ba - ab - b^2 = a^2 - b^2\n\\end{align}\n</math>\n:Here the distributive law was applied twice, and it does not matter which bracket is first multiplied out.\n\n;Fourth Example\n:Here the distributive law is applied the other way around compared to the previous examples. Consider\n::<math>12 a^3 b^2 - 30 a^4 b c + 18 a^2 b^3 c^2 \\,.</math>\n:Since the factor <math>6 a^2 b</math> occurs in all summands, it can be factored out. That is, due to the distributive law one obtains\n::<math>12 a^3 b^2 - 30 a^4 b c + 18 a^2 b^3 c^2 = 6 a^2 b (2 a b - 5 a^2 c + 3 b^2 c^2)\\,.</math>\n\n===Matrices===\nThe distributive law is valid for [[matrix multiplication]]. More precisely,\n\n:<math>(A + B) \\cdot C = A \\cdot C + B \\cdot C</math>\nfor all  <math>l \\times m</math>-matrices <math>A,B</math> and <math>m \\times n</math>-matrices <math> C</math>, as well as\n\n:<math>A \\cdot (B + C) = A \\cdot B + A \\cdot C</math>\nfor all  <math>l \\times m</math>-matrices <math>A</math> and <math>m \\times n</math>-matrices <math>B, C</math>.  Because the commutative property does not hold for matrix multiplication, the second law does not follow from the first law. In this case, they are two different laws.\n\n===Other examples===\n# [[Ordinal arithmetic#Multiplication|Multiplication]] of [[ordinal number]]s, in contrast, is only left-distributive, not right-distributive.\n# The [[cross product]] is left- and right-distributive over [[vector addition]], though not commutative.\n# The [[union (set theory)|union]] of sets is distributive over [[intersection (set theory)|intersection]], and intersection is distributive over union.\n# Logical [[logical disjunction|disjunction]] (\"or\") is distributive over logical [[logical conjunction|conjunction]] (\"and\"), and vice versa.\n# For [[real number]]s (and for any [[totally ordered set]]), the maximum operation is distributive over the minimum operation, and vice versa: {{math|1=max(''a'', min(''b'', ''c'')) = min(max(''a'', ''b''), max(''a'', ''c''))}} and {{math|1=min(''a'', max(''b'', ''c'')) = max(min(''a'', ''b''), min(''a'', ''c''))}}.\n# For [[integer]]s, the [[greatest common divisor]] is distributive over the [[least common multiple]], and vice versa: {{math|1=gcd(''a'', lcm(''b'', ''c'')) = lcm(gcd(''a'', ''b''), gcd(''a'', ''c''))}} and {{math|1=lcm(''a'', gcd(''b'', ''c'')) = gcd(lcm(''a'', ''b''), lcm(''a'', ''c''))}}.\n# For real numbers, addition distributes over the maximum operation, and also over the minimum operation: {{math|1=''a'' + max(''b'', ''c'') = max(''a'' + ''b'', ''a'' + ''c'')}} and {{math|1=''a'' + min(''b'', ''c'') = min(''a'' + ''b'', ''a'' + ''c'')}}.\n# For [[Binomial_(polynomial)|binomial]] multiplication, distribution is sometimes referred to as the FOIL Method<ref>Kim Steward (2011) [http://www.wtamu.edu/academic/anns/mps/math/mathlab/beg_algebra/beg_alg_tut28_multpoly.htm Multiplying Polynomials] from Virtual Math Lab at [[West Texas A&M University]]</ref> (First terms ''ac'', Outer ''ad'', Inner ''bc'', and Last ''bd'') such as: {{math|1=(''a'' + ''b'') &middot; (''c'' + ''d'') =  ''ac'' + ''ad'' + ''bc'' + ''bd''}}.\n# [[Polynomial]] multiplication is similar to that for binomials: {{math|1=(''a'' + ''b'') &middot; (''c'' + ''d'' + ''e'') =  ''ac'' + ''ad'' + ''ae'' + ''bc'' + ''bd'' + ''be''}}.\n# [[Complex number]] multiplication is distributive: <math>u(v+w)=uv+uw, (u+v)w=uw+vw</math>\n\n== Propositional logic ==\n{{Transformation rules}}\n\n=== Rule of replacement ===\nIn standard truth-functional propositional logic, ''distribution''<ref>[[Elliott Mendelson]] (1964) ''Introduction to Mathematical Logic'', page 21, D. Van Nostrand Company</ref><ref>[[Alfred Tarski]] (1941) ''Introduction to Logic'', page 52, [[Oxford University Press]]</ref> in logical proofs uses two valid [[rule of replacement|rules of replacement]] to expand individual occurrences of certain [[logical connective]]s, within some [[logical formula|formula]], into separate applications of those connectives across subformulas of the given formula. The rules are\n\n:<math>(P \\land (Q \\lor R)) \\Leftrightarrow ((P \\land Q) \\lor (P \\land R))</math>\nand \n:<math>(P \\lor (Q \\land R)) \\Leftrightarrow ((P \\lor Q) \\land (P \\lor R))</math>\n\nwhere \"<math>\\Leftrightarrow</math>\", also written '''≡''', is a [[metalogic]]al [[Symbol (formal)|symbol]] representing \"can be replaced in a proof with\" or \"is [[logical equivalence|logically equivalent]] to\".\n\n=== Truth functional connectives ===\n''Distributivity'' is a property of some logical connectives of truth-functional [[propositional logic]]. The following logical equivalences demonstrate that distributivity is a property of particular connectives. The following are truth-functional [[tautology (logic)|tautologies]].\n\n;Distribution of conjunction over conjunction:<math>(P \\land (Q \\land R)) \\leftrightarrow ((P \\land Q) \\land (P \\land R))</math>\n\n;Distribution of conjunction over disjunction:<math>(P \\land (Q \\lor R)) \\leftrightarrow ((P \\land Q) \\lor (P \\land R))</math>\n\n;Distribution of disjunction over conjunction:<math>(P \\lor (Q \\land R)) \\leftrightarrow ((P \\lor Q) \\land (P \\lor R))</math>\n\n;Distribution of disjunction over disjunction:<math>(P \\lor (Q \\lor R)) \\leftrightarrow ((P \\lor Q) \\lor (P \\lor R))</math>\n\n;Distribution of implication:<math>(P \\to (Q \\to R)) \\leftrightarrow ((P \\to Q) \\to (P \\to R))</math>\n\n;Distribution of implication over equivalence:<math>(P \\to (Q \\leftrightarrow R)) \\leftrightarrow ((P \\to Q) \\leftrightarrow (P \\to R))</math>\n\n;Distribution of disjunction over equivalence:<math>(P \\lor (Q \\leftrightarrow R)) \\leftrightarrow ((P \\lor Q) \\leftrightarrow (P \\lor R))</math>\n\n;Double distribution:<math>\\begin{align}\n  ((P \\land Q) \\lor (R \\land S)) &\\leftrightarrow (((P \\lor R) \\land (P \\lor S)) \\land ((Q \\lor R) \\land (Q \\lor S))) \\\\\n   ((P \\lor Q) \\land (R \\lor S)) &\\leftrightarrow (((P \\land R) \\lor (P \\land S)) \\lor ((Q \\land R) \\lor (Q \\land S)))\n\\end{align}</math>\n\n==Distributivity and rounding==\nIn practice, the distributive property of multiplication (and division) over addition may appear to be compromised or lost because of the limitations of [[arithmetic precision]].  For example, the identity {{nowrap|1=⅓ + ⅓ + ⅓ = (1 + 1 + 1) / 3}} appears to fail if the addition is conducted in [[decimal arithmetic]]; however, if many [[significant digit]]s are used, the calculation will result in a closer approximation to the correct results.  For example, if the arithmetical calculation takes the form: {{nowrap|1=0.33333 + 0.33333 + 0.33333 = 0.99999 ≠ 1}}, this result is a closer approximation than if fewer significant digits had been used.  Even when fractional numbers can be represented exactly in arithmetical form, errors will be introduced if those arithmetical values are rounded or truncated.  For example, buying two books, each priced at £14.99 before a [[VAT|tax]] of 17.5%, in two separate transactions will actually save £0.01, over buying them together: {{nowrap|1=£14.99 × 1.175 = £17.61}} to the nearest £0.01, giving a total expenditure of £35.22, but {{nowrap|1=£29.98 × 1.175 = £35.23}}.  Methods such as [[Rounding#Round half to even|banker's rounding]] may help in some cases, as may increasing the precision used, but ultimately some calculation errors are inevitable.\n\n==Distributivity in rings==\nDistributivity is most commonly found in [[ring (algebra)|ring]]s and [[distributive lattice]]s.\n\nA ring has two binary operations, + and * (commonly), and one of the requirements of a ring is that ∗ must distribute over +.\nMost kinds of numbers (example 1) and matrices (example 4) form rings.\nA [[lattice (order)|lattice]] is another kind of [[algebraic structure]] with two binary operations, ∧ and ∨.\nIf either of these operations (say ∧) distributes over the other (∨), then ∨ must also distribute over ∧, and the lattice is called distributive. See also the article on [[distributivity (order theory)]].\n\nExamples 4 and 5 are [[Boolean algebra (structure)|Boolean algebra]]s, which can be interpreted either as a special kind of ring (a [[Boolean ring]]) or a special kind of distributive lattice (a [[Boolean lattice]]). Each interpretation is responsible for different distributive laws in the Boolean algebra. Examples 6 and 7 are distributive lattices which are not Boolean algebras.\n\nFailure of one of the two distributive laws brings about [[near-ring]]s and [[near-field (mathematics)|near-field]]s instead of rings and [[division ring]]s respectively. The operations are usually configured to have the near-ring or near-field distributive on the right but not on the left.\n\nRings and distributive lattices are both special kinds of [[rig (algebra)|rig]]s, certain generalizations of rings.\nThose numbers in example 1 that don't form rings at least form rigs.\n[[near-semiring|Near-rig]]s are a further generalization of rigs that are left-distributive but not right-distributive; example 2 is a near-rig.\n\n==Generalizations of distributivity==\n{{anchor|generalizations}}\nIn several mathematical areas, generalized distributivity laws are considered. This may involve the weakening of the above conditions or the extension to infinitary operations. Especially in [[order theory]] one finds numerous important variants of distributivity, some of which include infinitary operations, such as the [[infinite distributive law]]; others being defined in the presence of only ''one'' binary operation, such as the according definitions and their relations are given in the article [[distributivity (order theory)]]. This also includes the notion of a '''[[completely distributive lattice]]'''.\n\nIn the presence of an ordering relation, one can also weaken the above equalities by replacing = by either ≤ or ≥. Naturally, this will lead to meaningful concepts only in some situations. An application of this principle is the notion of '''sub-distributivity''' as explained in the article on [[interval arithmetic]].\n\nIn [[category theory]], if {{nowrap|(''S'', ''μ'', ''η'')}} and {{nowrap|1=(''S''′, ''μ''′, ''η''′)}} are [[monad (category theory)|monad]]s on a [[category (mathematics)|category]] ''C'', a '''distributive law''' {{nowrap|''S''.''S''′ → ''S''′.''S''}} is a [[natural transformation]] {{nowrap|''λ'' : ''S''.''S''′ → ''S''′.''S''}} such that {{nowrap|(''S''′, ''λ'')}} is a [[lax map of monads]] {{nowrap|''S'' → ''S''}} and {{nowrap|(''S'', ''λ'')}} is a [[colax map of monads]] {{nowrap|''S''′ → ''S''′}}. This is exactly the data needed to define a monad structure on {{nowrap|''S''′.''S''}}: the multiplication map is {{nowrap|''S''′''μ''.''μ''′''S''<sup>2</sup>.''S''′''λS''}} and the unit map is ''η''′''S''.''η''. See: [[distributive law between monads]].\n\nA [[generalized distributive law]] has also been proposed in the area of [[information theory]].\n\n=== Notions of antidistributivity ===\n\nThe ubiquitous [[Identity (mathematics)|identity]] that relates inverses to the binary operation in any [[group (mathematics)|group]], namely {{nowrap|1=(''xy'')<sup>−1</sup> = ''y''<sup>−1</sup>''x''<sup>−1</sup>}}, which is taken as an axiom in the more general context of a [[semigroup with involution]], has sometimes been called an '''antidistributive property''' (of inversion as a [[unary operation]]).<ref name=\"BrinkKahl1997\">{{cite book|author1=Chris Brink|author2=Wolfram Kahl|author3=Gunther Schmidt|title=Relational Methods in Computer Science|date=1997|publisher=Springer|isbn=978-3-211-82971-4|page=4}}</ref>\n\nIn the context of a [[near-ring]], which removes the commutativity of the additively written group and assumes only one-sided distributivity, one can speak of (two-sided) '''distributive elements''' but also of '''antidistributive elements'''. The latter reverse the order of (the non-commutative) addition; assuming a left-nearring (i.e. one which all elements distribute when multiplied on the left), then an antidistributive element ''a'' reverses the order of addition when multiplied to the right: {{nowrap|1=(''x'' + ''y'')''a'' = ''ya'' + ''xa''}}.<ref>{{cite book|author1=Celestina Cotti Ferrero|author2=Giovanni Ferrero|title=Nearrings: Some Developments Linked to Semigroups and Groups|year=2002|publisher=Kluwer Academic Publishers|isbn=978-1-4613-0267-4|pages=62 and 67}}</ref>\n\nIn the study of [[propositional logic]] and [[Boolean algebra]], the term '''antidistributive law''' is sometimes used to denote the interchange between conjunction and disjunction when implication factors over them:<ref name=\"Hehner1993\">{{cite book|author=[[Eric Hehner|Eric C.R. Hehner]]|title=A Practical Theory of Programming|year=1993|publisher=Springer Science & Business Media|isbn=978-1-4419-8596-5|page=230}}</ref>\n\n* (''a'' ∨ ''b'') ⇒ ''c''  ≡  (''a'' ⇒ ''c'') ∧ (''b'' ⇒ ''c'')\n* (''a'' ∧ ''b'') ⇒ ''c''  ≡  (''a'' ⇒ ''c'') ∨ (''b'' ⇒ ''c'')\n\nThese two [[Tautology (logic)|tautologies]] are a direct consequence of the duality in [[De Morgan's laws]].\n\n==Notes==\n{{reflist}}\n\n== External links ==\n{{Wiktionary|distributivity}}\n*[http://www.cut-the-knot.org/Curriculum/Arithmetic/DistributiveLaw.shtml A demonstration of the Distributive Law] for integer arithmetic (from [[cut-the-knot]])\n\n[[Category:Abstract algebra]]\n[[Category:Binary operations|*Distributivity]]\n[[Category:Elementary algebra]]\n[[Category:Rules of inference]]\n[[Category:Theorems in propositional logic]]"
    },
    {
      "title": "Dixmier conjecture",
      "url": "https://en.wikipedia.org/wiki/Dixmier_conjecture",
      "text": "{{distinguish|text=the [[Uniformly bounded representation#The Dixmier Problem|Dixmier Problem]] in representation theory}} \n\nIn [[algebra]] the '''Dixmier conjecture''',  asked by [[Jacques Dixmier]] in 1968,<ref>{{Citation | last1=Dixmier | first1=Jacques | authorlink=Jacques Dixmier |title=Sur les algèbres de Weyl | url=http://www.numdam.org/item?id=BSMF_1968__96__209_0 | mr=0242897 | year=1968 | journal=Bulletin de la Société Mathématique de France   | volume=96 | pages=209–242 }} (problem 1)</ref> is the conjecture that any [[endomorphism]] of a [[Weyl algebra]] is an [[automorphism]].\n\n[[Yoshifumi Tsuchimoto|Tsuchimoto]] in 2005,<ref>{{Citation | last1=Tsuchimoto | first1=Yoshifumi | title=Endomorphisms of Weyl algebra and p-curvatures | year=2005 | journal=Osaka J. Math.| volume=42 | pages=435–452}}</ref> and independently [[Belov-Kanel]] and [[Maxim Kontsevich|Kontsevich]] in 2007,<ref>{{Citation | last1=Belov-Kanel | first1=Alexei | last2=Kontsevich | first2=Maxim | title=The Jacobian conjecture is stably equivalent to the Dixmier conjecture | arxiv=math/0512171 | mr=2337879 | year=2007 | journal=Moscow Mathematical Journal   | volume=7 | issue=2 | pages=209–218| bibcode=2005math.....12171B }}</ref> showed that the Dixmier conjecture is stably equivalent to the [[Jacobian conjecture]].\n\n==References==\n{{Reflist}}\n\n[[Category:Abstract algebra]]\n[[Category:Conjectures]]\n\n\n{{algebraic-geometry-stub}}"
    },
    {
      "title": "E∞-operad",
      "url": "https://en.wikipedia.org/wiki/E%E2%88%9E-operad",
      "text": "{{DISPLAYTITLE:E<sub>∞</sub>-operad}}\nIn the theory of [[operads]] in [[algebra]] and [[algebraic topology]], an '''E<sub>∞</sub>-operad''' is a parameter space for a multiplication map that is [[associative]] and [[commutative]] \"up to all higher [[homotopy|homotopies]]\". (An operad that describes a multiplication that is  associative but not necessarily commutative \"up to homotopy\" is called an [[A-infinity operad|A<sub>∞</sub>-operad]].)\n\n== Definition ==\nFor the definition, it is necessary to work in the category of operads with an action of the [[symmetric group]]. An operad ''A'' is said to be an E<sub>∞</sub>-operad if all of its spaces ''E''(''n'') are contractible; some authors also require the action of the symmetric group ''S<sub>n</sub>'' on ''E''(''n'') to be free. In other [[Category (mathematics)|categories]] than topological spaces, the notion of ''contractibility'' has to be replaced by suitable analogs, such as [[Acyclic complex|acyclic]]ity in the category of [[chain complexes]].\n\n== ''E''<sub>''n''</sub>-operads and ''n''-fold loop spaces==\nThe letter ''E'' in the terminology stands for \"everything\" (meaning associative and commutative), and the infinity symbols says that commutativity is required up to \"all\" higher homotopies. More generally, there is a weaker notion of '''''E''<sub>''n''</sub>-operad''' (''n'' ∈ '''N'''), parametrizing multiplications that are commutative only up to a certain level of homotopies. In particular,\n\n* ''E''<sub>1</sub>-spaces are [[A-infinity operad|''A''<sub>∞</sub>-spaces]];\n* ''E''<sub>2</sub>-spaces are homotopy commutative ''A''<sub>∞</sub>-spaces.\n\nThe importance of ''E''<sub>''n''</sub>- and ''E''<sub>∞</sub>-operads in topology stems from the fact that iterated [[loop space]]s, that is, spaces of continuous maps from an ''n''-dimensional sphere to another space ''X'' starting and ending at a fixed base point, constitute algebras over an ''E''<sub>''n''</sub>-operad. (One says they are '''''E''<sub>''n''</sub>-spaces'''.)  Conversely, any connected ''E''<sub>''n''</sub>-space ''X'' is an ''n''-fold loop space on some other space (called ''B<sup>n</sup>X'', the ''n''-fold [[classifying space]] of X).\n\n== Examples ==\nThe most obvious, if not particularly useful, example of an ''E''<sub>∞</sub>-operad is the ''commutative operad'' ''c'' given by ''c''(''n'')&nbsp;=&nbsp;*, a point, for all ''n''. Note that according to some authors, this is not really an ''E''<sub>∞</sub>-operad because the ''S<sub>n</sub>''-action is not free. This operad describes strictly associative and commutative multiplications. By definition, any other ''E''<sub>∞</sub>-operad has a map to ''c'' which is a homotopy equivalence.\n\nThe [[Operad theory#\"Little something\" operads|operad of '''little ''n''-cubes''' or '''little ''n''-disks''']] is an example of an ''E''<sub>''n''</sub>-operad that acts naturally on ''n''-fold loop spaces.\n\n== See also ==\n* [[operad]]\n* [[A-infinity operad]]\n* [[loop space]]\n\n== References ==\n* {{ cite journal\n   | last = Stasheff\n   | first = Jim | authorlink = Jim Stasheff\n   | title = What Is...an Operad?\n   | journal = [[Notices of the American Mathematical Society]]\n   |date=June–July 2004\n   | volume = 51\n   | issue = 6\n   | pages = 630&ndash;631\n   | url = http://www.ams.org/notices/200406/what-is.pdf\n   | format = [[PDF]]\n   | accessdate = 2008-01-17 }}\n\n*{{cite book\n | author = J. P. May\n | year = 1972\n | publisher = Springer-Verlag\n | title = The Geometry of Iterated Loop Spaces\n | url = http://www.math.uchicago.edu/~may/BOOKSMaster.html\n}}\n\n*{{cite book\n | author = Martin Markl, [[Steve Shnider]], [[Jim Stasheff]]\n | year = 2002\n | title = Operads in Algebra, Topology and Physics\n | publisher = American Mathematical Society\n | url = http://www.ams.org/bookstore?fn=20&arg1=survseries&item=SURV-96\n}}\n\n{{DEFAULTSORT:E-Operad}}\n[[Category:Abstract algebra]]\n[[Category:Algebraic topology]]"
    },
    {
      "title": "Eigenvalues and eigenvectors",
      "url": "https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors",
      "text": "{{redirect|Characteristic root}}\n{{Use American English|date=January 2019}}\n{{Short description|Vectors that map to their scalar multiples, and the associated scalars}}\n\nIn [[linear algebra]], an '''eigenvector''' or '''characteristic vector''' of a [[Linear map|linear transformation]] is a non-zero [[Vector space|vector]] that changes by only a scalar factor when that linear transformation is applied to it. More formally, if {{mvar|T}} is a linear transformation from a [[vector space]] {{mvar|V}} over a [[Field (mathematics)|field]] {{mvar|F}} into itself and {{math|'''v'''}} is a vector in {{mvar|V}} that is not the [[zero vector]], then {{math|'''v'''}} is an eigenvector of {{mvar|T}} if {{math|''T''('''v''')}} is a scalar multiple of {{math|'''v'''}}.  This condition can be written as the equation\n:<math>T(\\mathbf{v}) = \\lambda \\mathbf{v},</math>\n\nwhere {{mvar|λ}} is a [[scalar (mathematics)|scalar]] in the field {{mvar|F}}, known as the '''eigenvalue''',  '''characteristic value''', or '''characteristic root''' associated with the eigenvector {{math|'''v'''}}.\n\nIf the vector space {{mvar|V}} is finite-dimensional, then the linear transformation {{mvar|T}} can be represented as a [[square matrix]] ''A'', and the vector {{math|'''v'''}} by a [[row and column vectors|column vector]], rendering the above mapping as a [[Matrix-vector multiplication|matrix multiplication]] on the left-hand side and a scaling of the column vector on the right-hand side in the equation\n:<math>A\\mathbf{v} = \\lambda \\mathbf{v}.</math>\n\nThere is a direct correspondence between ''n''-by-''n'' square matrices and linear transformations from an ''n''-dimensional vector space to itself, given any basis of the vector space. For this reason, it is equivalent to define eigenvalues and eigenvectors using either the language of matrices or the language of linear transformations.<ref name=\"Herstein 1964 228–299\">{{harvtxt|Herstein|1964|pp=228,229}}</ref><ref name=\"Nering 1970 38\">{{harvtxt|Nering|1970|p=38}}</ref>\n\nGeometrically, an eigenvector, corresponding to a real nonzero eigenvalue, points in a direction that is stretched by the transformation and the eigenvalue is the factor by which it is stretched.  If the eigenvalue is negative, the direction is reversed.<ref>{{harvtxt|Burden|Faires|1993|p=401}}</ref>\n\n==Overview==\nEigenvalues and eigenvectors feature prominently in the analysis of linear transformations. The prefix ''[[wikt:eigen-|eigen-]]'' is adopted from the [[German language|German]] word ''[[wikt:eigen#German|eigen]]'' for \"proper\", \"characteristic\".<ref>{{harvtxt|Betteridge|1965}}</ref> Originally utilized to study [[principal axis (mechanics)|principal axes]] of the rotational motion of [[rigid body|rigid bodies]], eigenvalues and eigenvectors have a wide range of applications, for example in [[stability theory|stability analysis]], [[vibration analysis#eigenvalue problem|vibration analysis]], [[atomic orbital]]s, [[eigenface|facial recognition]], and [[Eigendecomposition of a matrix|matrix diagonalization]].\n\nIn essence, an eigenvector '''v''' of a linear transformation ''T'' is a non-zero vector that, when ''T'' is applied to it, does not change direction. Applying ''T'' to the eigenvector only scales the eigenvector by the scalar value ''λ'', called an eigenvalue. This condition can be written as the equation\n\n:<math>T(\\mathbf{v}) = \\lambda \\mathbf{v},</math>\n\nreferred to as the '''eigenvalue equation''' or '''eigenequation'''. In general, ''λ'' may be any [[scalar (mathematics)|scalar]]. For example, ''λ'' may be negative, in which case the eigenvector reverses direction as part of the scaling, or it may be zero or [[complex number|complex]].\n\n[[File:Mona Lisa eigenvector grid.png|thumb|320px|In this [[shear mapping]] the red arrow changes direction but the blue arrow does not. The blue arrow is an eigenvector of this shear mapping because it does not change direction, and since its length is unchanged, its eigenvalue is 1.]]\n\nThe [[Mona Lisa]] example pictured at right provides a simple illustration. Each point on the painting can be represented as a vector pointing from the center of the painting to that point. The linear transformation in this example is called a [[shear mapping]]. Points in the top half are moved to the right and points in the bottom half are moved to the left proportional to how far they are from the horizontal axis that goes through the middle of the painting. The vectors pointing to each point in the original image are therefore tilted right or left and made longer or shorter by the transformation. Notice that points ''along'' the horizontal axis do not move at all when this transformation is applied. Therefore, any vector that points directly to the right or left with no vertical component is an eigenvector of this transformation because the mapping does not change its direction. Moreover, these eigenvectors all have an eigenvalue equal to one because the mapping does not change their length, either.\n\nLinear transformations can take many different forms, mapping vectors in a variety of [[vector space]]s, so the eigenvectors can also take many forms. For example, the linear transformation could be a [[differential operator]] like <math>\\tfrac{d}{dx}</math>, in which case the eigenvectors are functions called [[eigenfunction]]s that are scaled by that differential operator, such as\n\n:<math>\\frac{d}{dx}e^{\\lambda x} = \\lambda e^{\\lambda x}.</math>\n\nAlternatively, the linear transformation could take the form of an ''n'' by ''n'' matrix, in which case the eigenvectors are ''n'' by 1 matrices that are also referred to as eigenvectors. If the linear transformation is expressed in the form of an ''n'' by ''n'' matrix ''A'', then the eigenvalue equation above for a linear transformation can be rewritten as the matrix multiplication\n\n:<math>Av = \\lambda v,</math>\n\nwhere the eigenvector ''v'' is an ''n'' by 1 matrix. For a matrix, eigenvalues and eigenvectors can be used to [[matrix decomposition|decompose the matrix]], for example by [[diagonalizable matrix|diagonalizing]] it.\n\nEigenvalues and eigenvectors give rise to many closely related mathematical concepts, and the prefix ''eigen-'' is applied liberally when naming them:\n* The set of all eigenvectors of a linear transformation, each paired with its corresponding eigenvalue, is called the '''eigensystem''' of that transformation.<ref>{{harvtxt|Press|2007|p=536}}</ref><ref name=WolframEigenvector>Wolfram Research, Inc. (2010) [http://mathworld.wolfram.com/Eigenvector.html ''Eigenvector'']. Accessed on 2016-04-01.</ref>\n* The set of all eigenvectors of ''T'' corresponding to the same eigenvalue, together with the zero vector, is called an '''eigenspace''' or '''characteristic space''' of ''T''.<ref name=\"Anton 1987 305,307\">{{harvtxt|Anton|1987|pp=305,307}}</ref><ref name=\"Nering 1970 107\"/>\n* If a set of eigenvectors of ''T'' forms a [[basis (linear algebra)|basis]] of the domain of ''T'', then this basis is called an '''eigenbasis'''.\n\n==History==\nEigenvalues are often introduced in the context of [[linear algebra]] or [[matrix (mathematics)|matrix theory]]. Historically, however, they arose in the study of [[quadratic form]]s and [[differential equation]]s.\n\nIn the 18th century [[Leonhard Euler|Euler]] studied the rotational motion of a [[rigid body]] and discovered the importance of the [[Principal axis (mechanics)|principal axes]].<ref>Note:\n*  In 1751, Leonhard Euler proved that any body has a principal axis of rotation:  Leonhard Euler (presented:  October 1751 ; published: 1760) [https://archive.org/stream/histoiredelacad07unkngoog#page/n196/mode/2up \"Du mouvement d'un corps solide quelconque lorsqu'il tourne autour d'un axe mobile\"] (On the movement of any solid body while it rotates around a moving axis), ''Histoire de l'Académie royale des sciences et des belles lettres de Berlin'', pp. 176–227.  [https://archive.org/stream/histoiredelacad07unkngoog#page/n232/mode/2up On p. 212], Euler proves that any body contains a principal axis of rotation:   ''\"Théorem.  44.  De quelque figure que soit le corps, on y peut toujours assigner un tel axe, qui passe par son centre de gravité, autour duquel le corps peut tourner librement & d'un mouvement uniforme.\"''  (Theorem.  44.  Whatever be the shape of the body, one can always assign to it such an axis, which passes through its center of gravity, around which it can rotate freely and with a uniform motion.)\n*  In 1755, [[Johann Andreas Segner]] proved that any body has three principal axes of rotation:  Johann Andreas Segner, ''Specimen theoriae turbinum'' [Essay on the theory of tops (i.e., rotating bodies)] ( Halle (\"Halae\"), (Germany) : Gebauer, 1755).  [https://books.google.com/books?id=89NMAAAAcAAJ&pg=PR29#v=onepage&q&f=false On p. XXVIIII (i.e., 29)], Segner derives a third-degree equation in ''t'', which proves that a body has three principal axes of rotation.  He then states (on the same page):  ''\"Non autem repugnat tres esse eiusmodi positiones plani HM, quia in aequatione cubica radices tres esse possunt, et tres tangentis t valores.\"''  (However, it is not inconsistent [that there] be three such positions of the plane HM, because in cubic equations, [there] can be three roots, and three values of the tangent t.)\n*  The relevant passage of Segner's work was discussed briefly by [[Arthur Cayley]].  See:  A. Cayley (1862) \"Report on the progress of the solution of certain special problems of dynamics,\" ''Report of the Thirty-second meeting of the British Association for the Advancement of Science; held at Cambridge in October 1862'', '''32''' :  184–252 ; see especially [https://books.google.com/books?id=S_RJAAAAcAAJ&pg=PA225#v=onepage&q&f=false pages 225–226.]</ref> [[Lagrange]] realized that the principal axes are the eigenvectors of the inertia matrix.<ref>See {{Harvnb|Hawkins|1975|loc=§2}}</ref> In the early 19th century, [[Augustin Louis Cauchy|Cauchy]] saw how their work could be used to classify the [[quadric surface]]s, and generalized it to arbitrary dimensions.<ref name=\"hawkins3\">See {{Harvnb|Hawkins|1975|loc=§3}}</ref> Cauchy also coined the term ''racine caractéristique'' (characteristic root) for what is now called ''eigenvalue''; his term survives in ''[[Secular equation|characteristic equation]]''.<ref name=\"kline807\">See {{Harvnb|Kline|1972|loc=pp. 807–808}}</ref><ref>Augustin Cauchy (1839) \"Mémoire sur l'intégration des équations linéaires\" (Memoir on the integration of linear equations), ''Comptes rendus'', '''8''' :  827–830, 845–865, 889–907, 931–937.  [http://gallica.bnf.fr/ark:/12148/bpt6k2967c/f833.item.r=.zoom From p. 827:]  ''\"On sait d'ailleurs qu'en suivant la méthode de Lagrange, on obtient pour valeur générale de la variable prinicipale une fonction dans laquelle entrent avec la variable principale les racines d'une certaine équation que j'appellerai l'''équation caractéristique'', le degré de cette équation étant précisément l'order de l'équation différentielle qu'il s'agit d'intégrer.\"''  (One knows, moreover, that by following Lagrange's method, one obtains for the general value of the principal variable a function in which there appear, together with the principal variable, the roots of a certain equation that I will call the \"characteristic equation\", the degree of this equation being precisely the order of the differential equation that must be integrated.)</ref>\n\n[[Joseph Fourier|Fourier]] used the work of Laplace and Lagrange to solve the [[heat equation]] by [[separation of variables]] in his famous 1822 book ''[[Théorie analytique de la chaleur]]''.<ref>See {{Harvnb|Kline|1972|loc=p. 673}}</ref> [[Jacques Charles François Sturm|Sturm]] developed Fourier's ideas further and brought them to the attention of Cauchy, who combined them with his own ideas and arrived at the fact that real symmetric matrices have real eigenvalues.<ref name=\"hawkins3\" /> This was extended by [[Charles Hermite|Hermite]] in 1855 to what are now called [[Hermitian matrix|Hermitian matrices]].<ref name=\"kline807\" /> Around the same time, [[Francesco Brioschi|Brioschi]] proved that the eigenvalues of [[orthogonal matrix|orthogonal matrices]] lie on the [[unit circle]],<ref name=\"hawkins3\" /> and [[Alfred Clebsch|Clebsch]] found the corresponding result for [[skew-symmetric matrix|skew-symmetric matrices]].<ref name=\"kline807\" /> Finally, [[Karl Weierstrass|Weierstrass]] clarified an important aspect in the [[stability theory]] started by Laplace by realizing that [[defective matrix|defective matrices]] can cause instability.<ref name=\"hawkins3\" />\n\nIn the meantime, [[Joseph Liouville|Liouville]] studied eigenvalue problems similar to those of Sturm; the discipline that grew out of their work is now called ''[[Sturm–Liouville theory]]''.<ref>See {{Harvnb|Kline|1972|loc=pp. 715–716}}</ref> [[Hermann Schwarz|Schwarz]] studied the first eigenvalue of [[Laplace's equation]] on general domains towards the end of the 19th century, while [[Henri Poincaré|Poincaré]] studied [[Poisson's equation]] a few years later.<ref>See {{Harvnb|Kline|1972|loc=pp. 706–707}}</ref>\n\nAt the start of the 20th century, [[David Hilbert|Hilbert]] studied the eigenvalues of [[integral operator]]s by viewing the operators as infinite matrices.<ref>See {{Harvnb|Kline|1972|loc=p. 1063}}</ref> He was the first to use the [[German language|German]] word ''eigen'', which means \"own\", to denote eigenvalues and eigenvectors in 1904,<ref>See:\n*  David Hilbert (1904) [http://www.digizeitschriften.de/dms/img/?PPN=PPN252457811_1904&DMDID=dmdlog11&LOGID=log11&PHYSID=phys57#navi \"Grundzüge einer allgemeinen Theorie der linearen Integralgleichungen.  (Erste Mitteilung)\"] (Fundamentals of a general theory of linear integral equations. (First report)), ''Nachrichten von der Gesellschaft der Wissenschaften zu Göttingen, Mathematisch-Physikalische Klasse'' (News of the Philosophical Society at Göttingen, mathematical-physical section), pp. 49–91.  [http://www.digizeitschriften.de/dms/img/?PPN=PPN252457811_1904&DMDID=dmdlog11&LOGID=log11&PHYSID=phys57#navi From page 51:]  ''\"Insbesondere in dieser ersten Mitteilung gelange ich zu Formeln, die die Entwickelung einer willkürlichen Funktion nach gewissen ausgezeichneten Funktionen, die ich ''Eigenfunktionen'' nenne, liefern: …'' (In particular, in this first report I arrive at formulas that provide the [series] development of an arbitrary function in terms of some distinctive functions, which I call ''eigenfunctions'': … )  Later on the same page:  ''\"Dieser Erfolg ist wesentlich durch den Umstand bedingt, daß ich nicht, wie es bisher geschah, in erster Linie auf den Beweis für die Existenz der Eigenwerte ausgehe, … \"'' (This success is mainly attributable to the fact that I do not, as it has happened until now, first of all aim at a proof of the existence of eigenvalues, … )\n*  For the origin and evolution of the terms eigenvalue, characteristic value, etc., see:  [http://jeff560.tripod.com/e.html Earliest Known Uses of Some of the Words of Mathematics (E)]</ref> though he may have been following a related usage by [[Helmholtz]]. For some time, the standard term in English was \"proper value\", but the more distinctive term \"eigenvalue\" is standard today.<ref>See {{Harvnb|Aldrich|2006}}</ref>\n\nThe first numerical algorithm for computing eigenvalues and eigenvectors appeared in 1929, when [[Richard Edler von Mises|Von Mises]] published the [[power method]]. One of the most popular methods today, the [[QR algorithm]], was proposed independently by [[John G.F. Francis]]<ref>{{Citation|first=J. G. F. |last=Francis|title=The QR Transformation, I (part 1)|journal=The Computer Journal|volume= 4|issue= 3|pages =265–271 |year=1961|doi=10.1093/comjnl/4.3.265}} and {{Citation|doi=10.1093/comjnl/4.4.332|first=J. G. F. |last=Francis|title=The QR Transformation, II (part 2)|journal=The Computer Journal|volume=4|issue= 4| pages= 332–345|year=1962}}</ref> and [[Vera Kublanovskaya]]<ref>{{Citation|first=Vera N. |last=Kublanovskaya|title=On some algorithms for the solution of the complete eigenvalue problem|journal=USSR Computational Mathematics and Mathematical Physics|volume= 3| pages= 637–657 |year=1961}}. Also published in: {{Citation|title=О некоторых алгорифмах для решения полной проблемы собственных значений|trans-title=On certain algorithms for the solution of the complete eigenvalue problem|journal=Журнал вычислительной математики и математической физики (Journal of Computational Mathematics and Mathematical Physics)|volume=1|issue=4| pages=555–570 |year=1961}}</ref> in 1961.<ref>See {{Harvnb|Golub|van Loan|1996|loc=§7.3}}; {{Harvnb|Meyer|2000|loc=§7.3}}</ref>\n\n==Eigenvalues and eigenvectors of matrices==\n{{see also|Euclidean vector|Matrix (mathematics)}}\n\nEigenvalues and eigenvectors are often introduced to students in the context of linear algebra courses focused on matrices.<ref name=CornellMathCourses>Cornell University Department of Mathematics (2016) [http://www.math.cornell.edu/m/Courses/Catalog/lowerlevel ''Lower-Level Courses for Freshmen and Sophomores'']. Accessed on 2016-03-27.</ref><ref name=UMichMathCourses>University of Michigan Mathematics (2016) [https://www.lsa.umich.edu/UMICH/math/Home/Undergrad/Ugrad_Courses.pdf ''Math Course Catalogue''] {{webarchive|url=https://web.archive.org/web/20151101101339/https://www.lsa.umich.edu/UMICH/math/Home/Undergrad/Ugrad_Courses.pdf |date=2015-11-01 }}. Accessed on 2016-03-27.</ref> Furthermore, linear transformations over a finite-dimensional vector space can be represented using matrices,<ref name=\"Herstein 1964 228–299\"/><ref name=\"Nering 1970 38\"/> which is especially common in numerical and computational applications.<ref>{{harvtxt|Press|2007|pp=38}}</ref>\n\n[[File:Eigenvalue equation.svg|thumb|right|250px|Matrix ''A'' acts by stretching the vector ''x'', not changing its direction, so ''x'' is an eigenvector of ''A''.]]\n\nConsider {{mvar|n}}-dimensional vectors that are formed as a list of {{mvar|n}} scalars, such as the three-dimensional vectors\n:<math>x = \\begin{bmatrix}1\\\\3\\\\4\\end{bmatrix}\\quad\\mbox{and}\\quad y = \\begin{bmatrix}-20\\\\-60\\\\-80\\end{bmatrix}.</math>\n\nThese vectors are said to be [[scalar multiplication|scalar multiples]] of each other, or [[Parallel (geometry)|parallel]] or [[collinearity|collinear]], if there is a scalar {{mvar|λ}} such that\n:<math>x = \\lambda y.</math>\n\nIn this case {{mvar|λ}} = −1/20.\n\nNow consider the linear transformation of {{mvar|n}}-dimensional vectors defined by an {{mvar|n}} by {{mvar|n}} matrix {{mvar|A}},\n:<math>Av = w,</math>\n\nor\n:<math>\\begin{bmatrix}\n    A_{11} & A_{12} & \\ldots & A_{1n} \\\\\n    A_{21} & A_{22} & \\ldots & A_{2n} \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    A_{n1} & A_{n2} & \\ldots & A_{nn} \\\\\n  \\end{bmatrix}\\begin{bmatrix}\n    v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n\n  \\end{bmatrix} = \\begin{bmatrix}\n    w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n\n  \\end{bmatrix}\n</math>\n\nwhere, for each row,\n:<math>w_i = A_{i1} v_1 + A_{i2} v_2 + \\cdots + A_{in} v_n = \\sum_{j = 1}^n A_{ij} v_j</math>.\n\nIf it occurs that {{mvar|v}} and {{mvar|w}} are scalar multiples, that is if\n{{NumBlk|:|<math>A v = w = \\lambda v,</math>|{{EquationRef|1}}}}\nthen {{mvar|v}} is an '''eigenvector''' of the linear transformation {{mvar|A}} and the scale factor {{mvar|λ}} is the '''eigenvalue''' corresponding to that eigenvector. Equation ({{EquationNote|1}}) is the '''eigenvalue equation''' for the matrix {{mvar|A}}.\n\nEquation ({{EquationNote|1}}) can be stated equivalently as\n{{NumBlk|:\n |<math>(A - \\lambda I)v = 0,</math>\n |{{EquationRef|2}}\n}}\n\nwhere {{mvar|I}} is the {{mvar|n}} by {{mvar|n}} [[identity matrix]] and 0 is the zero vector.\n\n===Eigenvalues and the characteristic polynomial===\n{{main|Characteristic polynomial}}\n\nEquation ({{EquationNote|2}}) has a non-zero solution ''v'' [[if and only if]] the [[determinant]] of the matrix {{nowrap|(''A'' − ''λI'')}} is zero. Therefore, the eigenvalues of ''A'' are values of ''λ'' that satisfy the equation\n{{NumBlk|:\n |<math>|A-\\lambda I| = 0</math>\n |{{EquationRef|3}}\n}}\n\nUsing [[Leibniz formula for determinants|Leibniz' rule]] for the determinant, the left-hand side of Equation ({{EquationNote|3}}) is a [[polynomial]] function of the variable ''λ'' and the [[degree of a polynomial|degree]] of this polynomial is ''n'', the order of the matrix ''A''. Its [[coefficient]]s depend on the entries of ''A'', except that its term of degree ''n'' is always (−1)<sup>''n''</sup>''λ''<sup>''n''</sup>. This polynomial is called the ''[[characteristic polynomial]]'' of ''A''. Equation ({{EquationNote|3}}) is called the ''characteristic equation'' or the ''secular equation'' of ''A''.\n\nThe [[fundamental theorem of algebra]] implies that the characteristic polynomial of an ''n''-by-''n'' matrix ''A'', being a polynomial of degree ''n'', can be [[factorization|factored]] into the product of ''n'' linear terms,\n{{NumBlk|:\n |<math>|A - \\lambda I| = (\\lambda_1 - \\lambda )(\\lambda_2 - \\lambda) \\cdots (\\lambda_n - \\lambda),</math>\n |{{EquationRef|4}}\n}}\n\nwhere each ''λ''<sub>''i''</sub> may be real but in general is a complex number. The numbers ''λ''<sub>1</sub>, ''λ''<sub>2</sub>, … ''λ''<sub>''n''</sub>, which may not all have distinct values, are roots of the polynomial and are the eigenvalues of ''A''.\n\nAs a brief example, which is described in more detail in the examples section later, consider the matrix\n:<math>M = \\begin{bmatrix}\n  2 & 1\\\\\n  1 & 2\n\\end{bmatrix}.</math>\n\nTaking the determinant of {{nowrap|(''M'' − ''λI'')}}, the characteristic polynomial of ''M'' is\n:<math>|M - \\lambda I| = \\begin{vmatrix}\n    2 - \\lambda & 1 \\\\\n    1           & 2 - \\lambda\n  \\end{vmatrix} =\n  3 - 4\\lambda + \\lambda^2.\n</math>\n\nSetting the characteristic polynomial equal to zero, it has roots at {{nowrap|1=λ = 1}} and {{nowrap|1=λ = 3}}, which are the two eigenvalues of ''M''. The eigenvectors corresponding to each eigenvalue can be found by solving for the components of ''v'' in the equation ''Mv'' = ''λv''. In this example, the eigenvectors are any non-zero scalar multiples of\n:<math>v_{\\lambda=1} = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}, \\quad v_{\\lambda=3} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}.</math>\n\nIf the entries of the matrix ''A'' are all real numbers, then the coefficients of the characteristic polynomial will also be real numbers, but the eigenvalues may still have non-zero imaginary parts. The entries of the corresponding eigenvectors therefore may also have non-zero imaginary parts. Similarly, the eigenvalues may be [[irrational number]]s even if all the entries of ''A'' are [[rational number]]s or even if they are all integers. However, if the entries of ''A'' are all [[algebraic number]]s, which include the rationals, the eigenvalues are complex algebraic numbers.\n\nThe non-real roots of a real polynomial with real coefficients can be grouped into pairs of [[complex conjugate]]s, namely with the two members of each pair having imaginary parts that differ only in sign and the same real part. If the degree is odd, then by the [[intermediate value theorem]] at least one of the roots is real. Therefore, any [[real matrix]] with odd order has at least one real eigenvalue, whereas a real matrix with even order may not have any real eigenvalues. The eigenvectors associated with these complex eigenvalues are also complex and also appear in complex conjugate pairs.\n\n=== Algebraic multiplicity ===\n<!-- Algebraic multiplicity, Simple eigenvalue and Semisimple eigenvalue link here.  Please do not change. -->\nLet ''λ''<sub>''i''</sub> be an eigenvalue of an ''n'' by ''n'' matrix ''A''. The '''algebraic multiplicity''' ''μ''<sub>''A''</sub>(''λ''<sub>''i''</sub>) of the eigenvalue is its [[Multiple roots of a polynomial|multiplicity as a root]] of the characteristic polynomial, that is, the largest integer ''k'' such that (''λ'' − ''λ''<sub>''i''</sub>)<sup>''k''</sup> [[polynomial division|divides evenly]] that polynomial.<ref name=\"Nering 1970 107\"/><ref>{{harvtxt|Fraleigh|1976|p=358}}</ref><ref name=\"Golub 1996 316\"/>\n\nSuppose a matrix ''A'' has dimension ''n'' and ''d'' ≤ ''n'' distinct eigenvalues. Whereas Equation ({{EquationNote|4}}) factors the characteristic polynomial of ''A'' into the product of ''n'' linear terms with some terms potentially repeating, the characteristic polynomial can instead be written as the product of ''d'' terms each corresponding to a distinct eigenvalue and raised to the power of the algebraic multiplicity,\n:<math>|A - \\lambda I| = (\\lambda_1 - \\lambda)^{\\mu_A(\\lambda_1)}(\\lambda_2 - \\lambda)^{\\mu_A(\\lambda_2)} \\cdots (\\lambda_d - \\lambda)^{\\mu_A(\\lambda_d)}.</math>\n\nIf ''d'' = ''n'' then the right-hand side is the product of ''n'' linear terms and this is the same as Equation ({{EquationNote|4}}). The size of each eigenvalue's algebraic multiplicity is related to the dimension ''n'' as\n:<math>\\begin{align}\n      1 &\\leq \\mu_A(\\lambda_i) \\leq n, \\\\\n  \\mu_A &= \\sum_{i=1}^d \\mu_A\\left(\\lambda_i\\right) = n.\n\\end{align} </math>\n\nIf ''μ''<sub>''A''</sub>(''λ''<sub>''i''</sub>) = 1, then ''λ''<sub>''i''</sub> is said to be a ''simple eigenvalue''.<ref name=\"Golub 1996 316\"/> If ''μ''<sub>''A''</sub>(''λ''<sub>''i''</sub>) equals the geometric multiplicity of ''λ''<sub>''i''</sub>, ''γ''<sub>''A''</sub>(''λ''<sub>''i''</sub>), defined in the next section, then ''λ''<sub>''i''</sub> is said to be a ''semisimple eigenvalue''.\n\n===Eigenspaces, geometric multiplicity, and the eigenbasis for matrices===\n<!-- Geometric multiplicity redirects here -->\nGiven a particular eigenvalue ''λ'' of the ''n'' by ''n'' matrix ''A'', define the [[Set (mathematics)|set]] ''E'' to be all vectors '''''v''''' that satisfy Equation ({{EquationNote|2}}),\n:<math>E = \\left\\{\\mathbf{v} : (A - \\lambda I)\\mathbf{v} = 0\\right\\}.</math>\n\nOn one hand, this set is precisely the [[kernel (linear algebra)|kernel]] or nullspace of the matrix (''A'' − ''λI''). On the other hand, by definition, any non-zero vector that satisfies this condition is an eigenvector of ''A'' associated with ''λ''. So, the set ''E'' is the [[Union (set theory)|union]] of the zero vector with the set of all eigenvectors of ''A'' associated with ''λ'', and ''E'' equals the nullspace of (''A'' − ''λI''). ''E'' is called the '''eigenspace''' or '''characteristic space''' of ''A'' associated with ''λ''.<ref name=\"Anton 1987 305,307\"/><ref name=\"Nering 1970 107\"/> In general ''λ'' is a complex number and the eigenvectors are complex ''n'' by 1 matrices. A property of the nullspace is that it is a [[linear subspace]], so ''E'' is a linear subspace of ℂ<sup>''n''</sup>.\n\nBecause the eigenspace ''E'' is a linear subspace, it is [[closure (mathematics)|closed]] under addition. That is, if two vectors ''u'' and ''v'' belong to the set ''E'', written {{nowrap|(''u'',''v'') ∈ ''E''}}, then {{nowrap|(''u'' + ''v'') ∈ ''E''}} or equivalently ''A''(''u'' + ''v'') = ''λ''(''u'' + ''v''). This can be checked using the [[distributive property]] of matrix multiplication. Similarly, because ''E'' is a linear subspace, it is closed under scalar multiplication. That is, if {{nowrap|''v'' ∈ ''E''}} and α is a complex number,  {{nowrap|(''αv'') ∈ ''E''}} or equivalently ''A''(α''v'') = ''λ''(''αv''). This can be checked by noting that multiplication of complex matrices by complex numbers is [[commutative property|commutative]]. As long as ''u'' + ''v'' and ''αv'' are not zero, they are also eigenvectors of ''A'' associated with ''λ''.\n\nThe dimension of the eigenspace ''E'' associated with ''λ'', or equivalently the maximum number of linearly independent eigenvectors associated with ''λ'', is referred to as the eigenvalue's '''geometric multiplicity''' ''γ''<sub>''A''</sub>(''λ''). Because ''E'' is also the nullspace of (''A'' − ''λI''), the geometric multiplicity of ''λ'' is the dimension of the nullspace of (''A'' − ''λI''), also called the ''nullity'' of (''A'' − ''λI''), which relates to the dimension and rank of (''A'' − ''λI'') as\n:<math>\\gamma_A(\\lambda) = n - \\operatorname{rank}(A - \\lambda I).</math>\n\nBecause of the definition of eigenvalues and eigenvectors, an eigenvalue's geometric multiplicity must be at least one, that is, each eigenvalue has at least one associated eigenvector. Furthermore, an eigenvalue's geometric multiplicity cannot exceed its algebraic multiplicity. Additionally, recall that an eigenvalue's algebraic multiplicity cannot exceed ''n''.\n:<math> 1 \\le \\gamma_A(\\lambda) \\le \\mu_A(\\lambda) \\le n</math>\n\nTo prove the inequality <math>\\gamma_A(\\lambda)\\le\\mu_A(\\lambda)</math>, consider how the definition of geometric multiplicity implies the existence of <math>\\gamma_A(\\lambda)</math> orthonormal eigenvectors <math>\\boldsymbol{v}_1,\\, \\ldots,\\, \\boldsymbol{v}_{\\gamma_A(\\lambda)}</math>, such that <math>A \\boldsymbol{v}_k = \\lambda \\boldsymbol{v}_k</math>. We can therefore find a (unitary) matrix <math>V</math> whose first <math>\\gamma_A(\\lambda)</math> columns are these eigenvectors, and whose remaining columns can be any orthonormal set of <math>n - \\gamma_A(\\lambda)</math> vectors orthogonal to these eigenvectors of <math>A</math>. Then <math>V</math> has full rank and is therefore invertible, and <math>AV=VD</math> with <math>D</math> a matrix whose top left block is the diagonal matrix <math>\\lambda I_{\\gamma_A(\\lambda)}</math>. This implies that <math>(A - \\xi I)V = V(D - \\xi I)</math>. In other words, <math>A - \\xi I</math> is similar to <math>D - \\xi I</math>, which implies that <math>\\det(A - \\xi I) = \\det(D - \\xi I)</math>. But from the definition of <math>D</math> we know that <math>\\det(D - \\xi I)</math> contains a factor <math>(\\xi - \\lambda)^{\\gamma_A(\\lambda)}</math>, which means that the algebraic multiplicity of <math>\\lambda</math> must satisfy <math>\\mu_A(\\lambda) \\ge \\gamma_A(\\lambda)</math>.\n\nSuppose <math>A</math> has <math>d \\leq n</math> distinct eigenvalues <math>\\lambda_1, ..., \\lambda_d</math>, where the geometric multiplicity of <math>\\lambda_i</math> is <math>\\gamma_A (\\lambda_i)</math>. The total geometric multiplicity of <math>A</math>,\n:<math>\\begin{align}\n  \\gamma_A &= \\sum_{i=1}^d \\gamma_A(\\lambda_i), \\\\\n         d &\\le \\gamma_A \\le n,\n\\end{align}</math>\n\nis the dimension of the union of all the eigenspaces of <math>A</math>'s eigenvalues, or equivalently the maximum number of linearly independent eigenvectors of <math>A</math>. If <math>\\gamma_A=n</math>, then\n* The direct sum of the eigenspaces of all of <math>A</math>'s eigenvalues is the entire vector space <math>\\mathbb{C}^n</math>.\n* A basis of <math>\\mathbb{C}^n</math> can be formed from <math>n</math> linearly independent eigenvectors of <math>A</math>; such a basis is called an '''eigenbasis'''\n* Any vector in <math>\\mathbb{C}^n</math> can be written as a linear combination of eigenvectors of <math>A</math>.\n\n===Additional properties of eigenvalues===\nLet <math>A</math> be an arbitrary <math>n \\times n</math> matrix of complex numbers with eigenvalues <math>\\lambda_1, ..., \\lambda_n</math>. Each eigenvalue appears <math>\\mu_A(\\lambda_i)</math> times in this list, where <math>\\mu_A(\\lambda_i)</math> is the eigenvalue's algebraic multiplicity. The following are properties of this matrix and its eigenvalues:\n* The [[trace (linear algebra)|trace]] of <math>A</math>, defined as the sum of its diagonal elements, is also the sum of all eigenvalues,\n*: <math>\\operatorname{tr}(A) = \\sum_{i=1}^n a_{ii} = \\sum_{i=1}^n \\lambda_i = \\lambda_1 + \\lambda_2 + \\cdots + \\lambda_n.</math><ref name=\"Beauregard 1973 307\">{{harvtxt|Beauregard|Fraleigh|1973|p=307}}</ref><ref>{{harvtxt|Herstein|1964|p=272}}</ref><ref>{{harvtxt|Nering|1970|pp=115–116}}</ref>\n* The [[determinant]] of <math>A</math> is the product of all its eigenvalues,\n*: <math>\\det(A) = \\prod_{i=1}^n \\lambda_i = \\lambda_1\\lambda_2 \\cdots \\lambda_n.</math><ref name=\"Beauregard 1973 307\"/><ref>{{harvtxt|Herstein|1964|p=290}}</ref><ref>{{harvtxt|Nering|1970|p=116}}</ref>\n* The eigenvalues of the <math>k</math><sup>th</sup> power of <math>A</math>; i.e., the eigenvalues of <math>A^k</math>, for any positive integer <math>k</math>, are <math>\\lambda_1^k, ..., \\lambda_n^k</math>.\n* The matrix <math>A</math> is [[invertible matrix|invertible]] if and only if every eigenvalue is nonzero.\n* If <math>A</math> is invertible, then the eigenvalues of <math>A^{-1}</math> are <math>\\frac{1}{\\lambda_1}, ..., \\frac{1}{\\lambda_n}</math> and each eigenvalue's geometric multiplicity coincides. Moreover, since the characteristic polynomial of the inverse is the [[reciprocal polynomial]] of the original, the eigenvalues share the same algebraic multiplicity.\n* If <math>A</math> is equal to its [[conjugate transpose]] <math>A^*</math>, or equivalently if <math>A</math> is [[Hermitian matrix|Hermitian]], then every eigenvalue is real. The same is true of any [[symmetric matrix|symmetric]] real matrix.\n* If <math>A</math> is not only Hermitian but also [[positive-definite matrix|positive-definite]], positive-semidefinite, negative-definite, or negative-semidefinite, then every eigenvalue is positive, non-negative, negative, or non-positive, respectively.\n* If <math>A</math> is [[unitary matrix|unitary]], every eigenvalue has absolute value <math>|\\lambda_i|=1</math>.\n\n=== Left and right eigenvectors ===\n{{see also|left and right (algebra)}}\n\nMany disciplines traditionally represent vectors as matrices with a single column rather than as matrices with a single row. For that reason, the word \"eigenvector\" in the context of matrices almost always refers to a '''right eigenvector''', namely a ''column'' vector that ''right'' multiplies the <math>n \\times n</math> matrix <math>A</math> in the defining equation, Equation ({{EquationNote|1}}),\n:<math>Av = \\lambda v.</math>\n\nThe eigenvalue and eigenvector problem can also be defined for ''row'' vectors that ''left'' multiply matrix <math>A</math>. In this formulation, the defining equation is\n:<math>uA = \\kappa u,</math>\n\nwhere <math>\\kappa</math> is a scalar and <math>u</math> is a <math>1 \\times n</math> matrix. Any row vector <math>u</math> satisfying this equation is called a '''left eigenvector''' of <math>A</math> and <math>\\kappa</math> is its associated eigenvalue. Taking the transpose of this equation,\n:<math>A^\\textsf{T} u^\\textsf{T} = \\kappa u^\\textsf{T}.</math>\n\nComparing this equation to Equation ({{EquationNote|1}}), it follows immediately that a left eigenvector of <math>A</math> is the same as the transpose of a right eigenvector of <math>A^\\textsf{T}</math>, with the same eigenvalue. Furthermore, since the characteristic polynomial of <math>A^\\textsf{T}</math> is the same as the characteristic polynomial of <math>A</math>, the eigenvalues of the left eigenvectors of <math>A</math> are the same as the eigenvalues of the right eigenvectors of <math>A^\\textsf{T}</math>.\n\n===Diagonalization and the eigendecomposition===\n{{main|Eigendecomposition of a matrix}}\n\nSuppose the eigenvectors of ''A'' form a basis, or equivalently ''A'' has ''n'' linearly independent eigenvectors ''v''<sub>1</sub>, ''v''<sub>2</sub>, …, ''v''<sub>''n''</sub> with associated eigenvalues ''λ''<sub>1</sub>, ''λ''<sub>2</sub>, …, ''λ''<sub>''n''</sub>. The eigenvalues need not be distinct. Define a square matrix ''Q'' whose columns are the ''n'' linearly independent eigenvectors of ''A'',\n: <math>Q = \\begin{bmatrix} v_1 & v_2 & \\cdots & v_n \\end{bmatrix}.</math>\n\nSince each column of ''Q'' is an eigenvector of ''A'', right multiplying ''A'' by ''Q'' scales each column of ''Q'' by its associated eigenvalue,\n: <math>AQ = \\begin{bmatrix} \\lambda_1 v_1 & \\lambda_2 v_2 & \\cdots & \\lambda_n v_n \\end{bmatrix}.</math>\n\nWith this in mind, define a diagonal matrix Λ where each diagonal element Λ<sub>''ii''</sub> is the eigenvalue associated with the ''i''th column of ''Q''. Then\n: <math>AQ = Q\\Lambda.</math>\n\nBecause the columns of ''Q'' are linearly independent, Q is invertible. Right multiplying both sides of the equation by ''Q''<sup>−1</sup>,\n: <math>A = Q\\Lambda Q^{-1},</math>\n\nor by instead left multiplying both sides by ''Q''<sup>−1</sup>,\n: <math>Q^{-1}AQ = \\Lambda.</math>\n\n''A'' can therefore be decomposed into a matrix composed of its eigenvectors, a diagonal matrix with its eigenvalues along the diagonal, and the inverse of the matrix of eigenvectors. This is called the [[eigendecomposition of a matrix|eigendecomposition]] and it is a [[matrix similarity|similarity transformation]]. Such a matrix ''A'' is said to be ''similar'' to the diagonal matrix Λ or ''[[diagonalizable matrix|diagonalizable]]''. The matrix ''Q'' is the change of basis matrix of the similarity transformation. Essentially, the matrices ''A'' and Λ represent the same linear transformation expressed in two different bases. The eigenvectors are used as the basis when representing the linear transformation as&nbsp;Λ.\n\nConversely, suppose a matrix ''A'' is diagonalizable. Let ''P'' be a non-singular square matrix such that ''P''<sup>−1</sup>''AP'' is some diagonal matrix ''D''. Left multiplying both by ''P'', ''AP'' = ''PD''. Each column of ''P'' must therefore be an eigenvector of ''A'' whose eigenvalue is the corresponding diagonal element of ''D''. Since the columns of ''P'' must be linearly independent for ''P'' to be invertible, there exist ''n'' linearly independent eigenvectors of ''A''. It then follows that the eigenvectors of ''A'' form a basis if and only if ''A'' is diagonalizable.\n\nA matrix that is not diagonalizable is said to be [[defective matrix|defective]]. For defective matrices, the notion of eigenvectors generalizes to [[generalized eigenvector]]s and the diagonal matrix of eigenvalues generalizes to the [[Jordan normal form]]. Over an algebraically closed field, any matrix ''A'' has a [[Jordan normal form]] and therefore admits a basis of generalized eigenvectors and a decomposition into [[generalized eigenspace]]s.\n\n===Variational characterization===\n{{main|Min-max theorem}}\n\nIn the [[Hermitian matrix|Hermitian]] case, eigenvalues can be given a variational characterization. The largest eigenvalue of <math>H</math> is the maximum value of the [[quadratic form]] <math>x^\\textsf{T} H x/x^\\textsf{T} x</math>. A value of <math>x</math> that realizes that maximum, is an eigenvector.\n\n===Matrix examples===\n\n====Two-dimensional matrix example====\n\n[[File:Eigenvectors.gif|right|frame|The transformation matrix ''A'' = <math>\\bigl[\\begin{smallmatrix} 2 & 1\\\\ 1 & 2 \\end{smallmatrix}\\bigr]</math> preserves the direction of vectors parallel to ''v''<sub>''λ''=1</sub> = [1 −1]<sup>T</sup> (in purple) and ''v''<sub>''λ''=3</sub> = [1 1]<sup>T</sup> (in blue). The vectors in red are not parallel to either eigenvector, so, their directions are changed by the transformation. The blue vectors after the transformation are three times the length of the original (their eigenvalue is 3), while the lengths of the purple vectors are unchanged (reflecting an eigenvalue of 1). See also: [[:File:Eigenvectors-extended.gif|An extended version, showing all four quadrants]].]]\n\nConsider the matrix\n:<math>A = \\begin{bmatrix}\n  2 & 1\\\\\n  1 & 2\n\\end{bmatrix}.</math>\n\nThe figure on the right shows the effect of this transformation on point coordinates in the plane.\nThe eigenvectors ''v'' of this transformation satisfy Equation ({{EquationNote|1}}), and the values of ''λ'' for which the determinant of the matrix (''A''&nbsp;−&nbsp;''λI'') equals zero are the eigenvalues.\n\nTaking the determinant to find characteristic polynomial of ''A'',\n:<math>\\begin{align}\n  |A - \\lambda I| & = \\left|\\begin{bmatrix}\n    2 & 1 \\\\\n    1 & 2\n  \\end{bmatrix} - \\lambda\\begin{bmatrix}\n    1 & 0 \\\\\n    0 & 1\n  \\end{bmatrix}\\right| = \\begin{vmatrix}\n    2 - \\lambda & 1 \\\\\n    1           & 2 - \\lambda\n  \\end{vmatrix}, \\\\[6pt]\n                  & = 3 - 4\\lambda + \\lambda^2.\n\\end{align}</math>\nSetting the characteristic polynomial equal to zero, it has roots at {{nowrap|1=''λ'' = 1}} and {{nowrap|1=''λ'' = 3}}, which are the two eigenvalues of ''A''.\n\nFor {{nowrap|1=''λ'' = 1}}, Equation ({{EquationNote|2}}) becomes,\n:<math>(A - I)v_{\\lambda=1} = \\begin{bmatrix} 1 & 1\\\\ 1 & 1\\end{bmatrix}\\begin{bmatrix}v_1 \\\\ v_2\\end{bmatrix} = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix}.</math>\n\nAny non-zero vector with ''v''<sub>1</sub> = −''v''<sub>2</sub> solves this equation. Therefore,\n:<math>v_{\\lambda=1} = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}</math>\n\nis an eigenvector of ''A'' corresponding to ''λ'' = 1, as is any scalar multiple of this vector.\n\nFor {{nowrap|1=''λ'' = 3}}, Equation ({{EquationNote|2}}) becomes\n:<math>(A - 3I)v_{\\lambda=3} =\n  \\begin{bmatrix} -1 & 1\\\\ 1 & -1 \\end{bmatrix}\n    \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} =\n  \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}.\n</math>\n\nAny non-zero vector with ''v''<sub>1</sub> = ''v''<sub>2</sub> solves this equation. Therefore,\n:<math>v_{\\lambda=3} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}</math>\n\nis an eigenvector of ''A'' corresponding to ''λ'' = 3, as is any scalar multiple of this vector.\n\nThus, the vectors ''v''<sub>''λ''=1</sub> and ''v''<sub>''λ''=3</sub> are eigenvectors of ''A'' associated with the eigenvalues {{nowrap|1=''λ'' = 1}} and {{nowrap|1=''λ'' = 3}}, respectively.\n\n====Three-dimensional matrix example====\nConsider the matrix\n:<math>A = \\begin{bmatrix}\n  2 & 0 & 0 \\\\\n  0 & 3 & 4 \\\\\n  0 & 4 & 9\n\\end{bmatrix}.</math>\n\nThe characteristic polynomial of ''A'' is\n:<math>\\begin{align}\n  |A-\\lambda I| &= \\left|\\begin{bmatrix}\n    2 & 0 & 0 \\\\\n    0 & 3 & 4 \\\\\n    0 & 4 & 9\n  \\end{bmatrix} - \\lambda\\begin{bmatrix}\n    1 & 0 & 0 \\\\\n    0 & 1 & 0 \\\\\n    0 & 0 & 1\n  \\end{bmatrix}\\right| =\n  \\begin{vmatrix}\n    2 - \\lambda & 0 & 0 \\\\\n    0 & 3 - \\lambda & 4 \\\\\n    0 & 4 & 9 - \\lambda\n  \\end{vmatrix}, \\\\[6pt]\n                &= (2 - \\lambda)\\bigl[(3 - \\lambda)(9 - \\lambda) - 16\\bigr] \n                 = -\\lambda^3 + 14\\lambda^2 - 35\\lambda + 22.\n\\end{align}</math>\n\nThe roots of the characteristic polynomial are 2, 1, and 11, which are the only three eigenvalues of ''A''. These eigenvalues correspond to the eigenvectors <math>\\begin{bmatrix} 1 & 0 & 0 \\end{bmatrix}^\\textsf{T},</math> <math>\\begin{bmatrix} 0 & -2 & 1 \\end{bmatrix}^\\textsf{T},</math> and <math>\\begin{bmatrix} 0 & 1 & 2 \\end{bmatrix}^\\textsf{T}</math>, or any non-zero multiple thereof.\n\n====Three-dimensional matrix example with complex eigenvalues====\nConsider the [[permutation matrix|cyclic permutation matrix]]\n:<math>A = \\begin{bmatrix}\n  0 & 1 & 0\\\\\n  0 & 0 & 1\\\\\n  1 & 0 & 0\n\\end{bmatrix}.</math>\n\nThis matrix shifts the coordinates of the vector up by one position and moves the first coordinate to the bottom. Its characteristic polynomial is 1&nbsp;−&nbsp;''λ''<sup>3</sup>, whose roots are\n:<math>\\begin{align}\n  \\lambda_1 &= 1 \\\\\n  \\lambda_2 &= -\\frac{1}{2} + \\mathbf{i}\\frac{\\sqrt{3}}{2} \\\\\n  \\lambda_3 &= \\lambda_2^* = -\\frac{1}{2} - \\mathbf{i}\\frac{\\sqrt{3}}{2}\n\\end{align}</math>\n\nwhere <math>\\mathbf{i}</math> is an imaginary unit with <math>\\mathbf{i}^2 = -1.</math>\n\nFor the real eigenvalue ''λ''<sub>1</sub> = 1, any vector with three equal non-zero entries is an eigenvector. For example,\n:<math>\n    A \\begin{bmatrix} 5\\\\ 5\\\\ 5 \\end{bmatrix} =\n    \\begin{bmatrix} 5\\\\ 5\\\\ 5 \\end{bmatrix} =\n    1 \\cdot \\begin{bmatrix} 5\\\\ 5\\\\ 5 \\end{bmatrix}.\n</math>\n\nFor the complex conjugate pair of imaginary eigenvalues, note that\n:<math>\\lambda_2\\lambda_3 = 1, \\quad \\lambda_2^2 = \\lambda_3, \\quad \\lambda_3^2 = \\lambda_2.</math>\n\nThen\n:<math>\n  A \\begin{bmatrix} 1 \\\\ \\lambda_2 \\\\ \\lambda_3 \\end{bmatrix} =\n    \\begin{bmatrix} \\lambda_2 \\\\ \\lambda_3 \\\\ 1 \\end{bmatrix} =\n    \\lambda_2 \\cdot \\begin{bmatrix} 1 \\\\ \\lambda_2 \\\\ \\lambda_3 \\end{bmatrix},\n</math>\n\nand\n:<math>\n  A \\begin{bmatrix} 1 \\\\ \\lambda_3 \\\\ \\lambda_2 \\end{bmatrix} =\n    \\begin{bmatrix} \\lambda_3 \\\\ \\lambda_2 \\\\ 1 \\end{bmatrix} =\n    \\lambda_3 \\cdot \\begin{bmatrix} 1 \\\\ \\lambda_3 \\\\ \\lambda_2 \\end{bmatrix}.\n</math>\n\nTherefore, the other two eigenvectors of ''A'' are complex and are <math>v_{\\lambda_2} = \\begin{bmatrix} 1 & \\lambda_2 & \\lambda_3\\end{bmatrix}^\\textsf{T}</math> and <math>v_{\\lambda_3} = \\begin{bmatrix} 1 & \\lambda_3 & \\lambda_2\\end{bmatrix}^\\textsf{T}</math> with eigenvalues ''λ''<sub>2</sub> and ''λ''<sub>3</sub>, respectively. Note that the two complex eigenvectors also appear in a complex conjugate pair,\n:<math>v_{\\lambda_2} = v_{\\lambda_3}^*.</math>\n\n====Diagonal matrix example====\nMatrices with entries only along the main diagonal are called ''[[diagonal matrices]]''.  The eigenvalues of a diagonal matrix are the diagonal elements themselves. Consider the matrix\n:<math>A = \\begin{bmatrix} 1 & 0 & 0\\\\ 0 & 2 & 0\\\\ 0 & 0 & 3\\end{bmatrix}.</math>\n\nThe characteristic polynomial of ''A'' is\n:<math>|A - \\lambda I| = (1 - \\lambda)(2 - \\lambda)(3 - \\lambda),</math>\n\nwhich has the roots {{nowrap|1=''λ''<sub>1</sub> = 1}}, {{nowrap|1=''λ''<sub>2</sub> = 2}}, and {{nowrap|1=''λ''<sub>3</sub> = 3}}. These roots are the diagonal elements as well as the eigenvalues of&nbsp;''A''.\n\nEach diagonal element corresponds to an eigenvector whose only non-zero component is in the same row as that diagonal element. In the example, the eigenvalues correspond to the eigenvectors,\n:<math>\n  v_{\\lambda_1} = \\begin{bmatrix} 1\\\\ 0\\\\ 0 \\end{bmatrix},\\quad\n  v_{\\lambda_2} = \\begin{bmatrix} 0\\\\ 1\\\\ 0 \\end{bmatrix},\\quad\n  v_{\\lambda_3} = \\begin{bmatrix} 0\\\\ 0\\\\ 1 \\end{bmatrix},\n</math>\n\nrespectively, as well as scalar multiples of these vectors.\n\n====Triangular matrix example====\nA matrix whose elements above the main diagonal are all zero is called a ''lower [[triangular matrix]]'', while a matrix whose elements below the main diagonal are all zero is called an ''upper triangular matrix''.  As with diagonal matrices, the eigenvalues of triangular matrices are the elements of the main diagonal.\n\nConsider the lower triangular matrix,\n:<math>A = \\begin{bmatrix}\n  1 & 0 & 0\\\\\n  1 & 2 & 0\\\\\n  2 & 3 & 3\n\\end{bmatrix}.</math>\n\nThe characteristic polynomial of ''A'' is\n:<math>|A - \\lambda I| = (1 - \\lambda)(2 - \\lambda)(3 - \\lambda),</math>\n\nwhich has the roots {{nowrap|1=''λ''<sub>1</sub> = 1}}, {{nowrap|1=''λ''<sub>2</sub> = 2}}, and {{nowrap|1=''λ''<sub>3</sub> = 3}}. These roots are the diagonal elements as well as the eigenvalues of&nbsp;''A''.\n\nThese eigenvalues correspond to the eigenvectors,\n:<math>\n  v_{\\lambda_1} = \\begin{bmatrix} 1\\\\ -1\\\\  \\frac{1}{2}\\end{bmatrix},\\quad\n  v_{\\lambda_2} = \\begin{bmatrix} 0\\\\  1\\\\ -3\\end{bmatrix},\\quad\n  v_{\\lambda_3} = \\begin{bmatrix} 0\\\\  0\\\\  1\\end{bmatrix},\n</math>\n\nrespectively, as well as scalar multiples of these vectors.\n\n====Matrix with repeated eigenvalues example====\nAs in the previous example, the lower triangular matrix\n:<math>A = \\begin{bmatrix}\n  2 & 0 & 0 & 0 \\\\\n  1 & 2 & 0 & 0 \\\\\n  0 & 1 & 3 & 0  \\\\\n  0 & 0 & 1 & 3 \n\\end{bmatrix},</math>\n\nhas a characteristic polynomial that is the product of its diagonal elements,\n:<math>|A - \\lambda I| = \\begin{vmatrix}\n    2 - \\lambda & 0 & 0 & 0 \\\\\n    1 & 2- \\lambda & 0 & 0 \\\\\n    0 & 1 & 3- \\lambda & 0  \\\\\n    0 & 0 & 1 & 3- \\lambda \n  \\end{vmatrix} =\n  (2 - \\lambda)^2(3 - \\lambda)^2.\n</math>\n\nThe roots of this polynomial, and hence the eigenvalues, are 2 and 3. The ''algebraic multiplicity'' of each eigenvalue is 2; in other words they are both double roots. The sum of the algebraic multiplicities of each distinct eigenvalue is ''μ''<sub>''A''</sub> = 4 = ''n'', the order of the characteristic polynomial and the dimension of ''A''.\n\nOn the other hand, the ''geometric multiplicity'' of the eigenvalue 2 is only 1, because its eigenspace is spanned by just one vector <math>\\begin{bmatrix} 0 & 1 & -1 & 1 \\end{bmatrix}^\\textsf{T}</math> and is therefore 1-dimensional. Similarly, the geometric multiplicity of the eigenvalue 3 is 1 because its eigenspace is spanned by just one vector <math>\\begin{bmatrix} 0 & 0 & 0 & 1 \\end{bmatrix}^\\textsf{T}</math>. The total geometric multiplicity ''γ''<sub>''A''</sub> is 2, which is the smallest it could be for a matrix with two distinct eigenvalues. Geometric multiplicities are defined in a later section.\n\n==Eigenvalues and eigenfunctions of differential operators==\n{{main|Eigenfunction}}\n\nThe definitions of eigenvalue and eigenvectors of a linear transformation ''T'' remains valid even if the underlying vector space is an infinite-dimensional [[Hilbert space|Hilbert]] or [[Banach space]]. A widely used class of linear transformations acting on infinite-dimensional spaces are the [[differential operator]]s on [[function space]]s. Let ''D'' be a linear differential operator on the space '''C'''<sup>∞</sup> of infinitely [[derivative|differentiable]] real functions of a real argument ''t''. The eigenvalue equation for ''D'' is the [[differential equation]]\n:<math>D f(t) = \\lambda f(t)</math>\n\nThe functions that satisfy this equation are eigenvectors of ''D'' and are commonly called '''eigenfunctions'''.\n\n===Derivative operator example===\n\nConsider the derivative operator <math>\\tfrac{d}{dt}</math> with eigenvalue equation\n:<math>\\frac{d}{dt}f(t) = \\lambda f(t).</math>\n\nThis differential equation can be solved by multiplying both sides by ''dt''/''f''(''t'') and integrating. Its solution, the [[exponential function]]\n:<math>f(t) = f(0)e^{\\lambda t},</math>\n\nis the eigenfunction of the derivative operator. Note that in this case the eigenfunction is itself a function of its associated eigenvalue. In particular, note that for ''λ'' = 0 the eigenfunction ''f''(''t'') is a constant.\n\nThe main [[eigenfunction]] article gives other examples.\n\n==General definition==\n\nThe concept of eigenvalues and eigenvectors extends naturally to arbitrary [[linear map|linear transformations]] on arbitrary [[vector space]]s. Let ''V'' be any vector space over some [[field (algebra)|field]] ''K'' of [[scalar (mathematics)|scalars]], and let ''T'' be a linear transformation mapping ''V'' into ''V'',\n:<math>T:V \\to V.</math>\n\nWe say that a non-zero vector '''v''' ∈ ''V'' is an '''eigenvector''' of ''T'' if and only if there exists a scalar ''λ'' ∈ ''K'' such that\n{{NumBlk|:\n |<math>T(\\mathbf{v}) = \\lambda \\mathbf{v}.</math>\n | {{EquationRef|5}}\n}}\n\nThis equation is called the eigenvalue equation for ''T'', and the scalar ''λ'' is the '''eigenvalue''' of ''T'' corresponding to the eigenvector '''v'''. Note that ''T''('''v''') is the result of applying the transformation ''T'' to the vector '''v''', while ''λ'''''v''' is the product of the scalar ''λ'' with '''v'''.<ref>See {{Harvnb|Korn|Korn|2000|loc=Section 14.3.5a}}; {{Harvnb|Friedberg|Insel|Spence|1989|loc=p. 217}}</ref>\n\n===Eigenspaces, geometric multiplicity, and the eigenbasis===\n\nGiven an eigenvalue ''λ'', consider the set\n:<math>E = \\left\\{\\mathbf{v} : T(\\mathbf{v}) = \\lambda \\mathbf{v}\\right\\},</math>\n\nwhich is the union of the zero vector with the set of all eigenvectors associated with&nbsp;''λ''. ''E'' is called the '''eigenspace''' or '''characteristic space''' of ''T'' associated with&nbsp;''λ''.\n\nBy definition of a linear transformation,\n:<math>\\begin{align}\n  T(\\mathbf{x} + \\mathbf{y}) &= T(\\mathbf{x}) + T(\\mathbf{y}),\\\\\n        T(\\alpha \\mathbf{x}) &= \\alpha T(\\mathbf{x}),\n\\end{align}</math>\n\nfor ('''x''','''y''') ∈ ''V'' and α ∈ ''K''. Therefore, if '''u''' and '''v''' are eigenvectors of ''T'' associated with eigenvalue ''λ'', namely '''u''','''v''' ∈ ''E'', then\n:<math>\\begin{align}\n  T(\\mathbf{u} + \\mathbf{v}) &= \\lambda (\\mathbf{u} + \\mathbf{v}),\\\\\n        T(\\alpha \\mathbf{v}) &= \\lambda (\\alpha \\mathbf{v}).\n\\end{align}</math>\n\nSo, both '''u''' + '''v''' and α'''v''' are either zero or eigenvectors of ''T'' associated with ''λ'', namely '''u''' + '''v''', α'''v''' ∈ ''E'', and ''E'' is closed under addition and scalar multiplication. The eigenspace ''E'' associated with ''λ'' is therefore a linear subspace of ''V''.<ref name=\"Nering 1970 107\"/><ref>{{Harvnb|Shilov|1977|loc=p. 109}}</ref><ref>[[b:The Book of Mathematical Proofs/Algebra/Linear Transformations#Lemma for the eigenspace|Lemma for the eigenspace]]</ref> If that subspace has dimension 1, it is sometimes called an '''eigenline'''.<ref>''[https://books.google.com/books?id=pkESXAcIiCQC&pg=PA111 Schaum's Easy Outline of Linear Algebra]'', p. 111</ref>\n\nThe '''geometric multiplicity''' ''γ''<sub>''T''</sub>(''λ'') of an eigenvalue ''λ'' is the dimension of the eigenspace associated with ''λ'', i.e., the maximum number of linearly independent eigenvectors associated with that eigenvalue.<ref name=\"Nering 1970 107\">{{harvtxt|Nering|1970|p=107}}</ref><ref name=\"Golub 1996 316\">{{harvtxt|Golub|Van Loan|1996|p=316}}</ref> By the definition of eigenvalues and eigenvectors, ''γ''<sub>''T''</sub>(''λ'') ≥ 1 because every eigenvalue has at least one eigenvector.\n\nThe eigenspaces of ''T'' always form a [[direct sum]]. As a consequence, eigenvectors of ''different'' eigenvalues are always linearly independent. Therefore, the sum of the dimensions of the eigenspaces cannot exceed the dimension ''n'' of the vector space on which ''T'' operates, and there cannot be more than ''n'' distinct eigenvalues.<ref name=\"Shilov_lemma\">For a proof of this lemma, see {{Harvnb|Roman|2008|loc=Theorem 8.2 on p. 186}}; {{Harvnb|Shilov|1977|loc=p. 109}}; {{Harvnb|Hefferon|2001|loc=p. 364}}; {{Harvnb|Beezer|2006|loc=Theorem EDELI on p. 469}}; and [[b:Famous Theorems of Mathematics/Algebra/Linear Transformations#Lemma for linear independence of eigenvectors|Lemma for linear independence of eigenvectors]]</ref>\n\nAny subspace spanned by eigenvectors of ''T'' is an [[invariant subspace]] of ''T'', and the restriction of ''T'' to such a subspace is diagonalizable. Moreover, if the entire vector space ''V'' can be spanned by the eigenvectors of ''T'', or equivalently if the direct sum of the eigenspaces associated with all the eigenvalues of ''T'' is the entire vector space ''V'', then a basis of ''V'' called an '''eigenbasis''' can be formed from linearly independent eigenvectors of ''T''. When ''T'' admits an eigenbasis, ''T'' is diagonalizable.\n\n===Zero vector as an eigenvector===\n\nWhile the definition of an eigenvector used in this article excludes the [[zero vector]], it is possible to define eigenvalues and eigenvectors such that the zero vector is an eigenvector.<ref>{{Citation|last=Axler|first= Sheldon |title=Linear Algebra Done Right|edition=2nd |chapter=Ch. 5|page= 77}}</ref>\n\nConsider again the eigenvalue equation, Equation ({{EquationNote|5}}). Define an '''eigenvalue''' to be any scalar ''λ'' ∈ ''K'' such that there exists a non-zero vector '''v''' ∈ ''V'' satisfying Equation ({{EquationNote|5}}). It is important that this version of the definition of an eigenvalue specify that the vector be non-zero, otherwise by this definition the zero vector would allow any scalar in ''K'' to be an eigenvalue. Define an '''eigenvector''' '''v''' associated with the eigenvalue ''λ'' to be any vector that, given ''λ'', satisfies Equation ({{EquationNote|5}}). Given the eigenvalue, the zero vector is among the vectors that satisfy Equation ({{EquationNote|5}}), so the zero vector is included among the eigenvectors by this alternate definition.\n\n===Spectral theory===\n{{main|Spectral theory}}\n\nIf ''λ'' is an eigenvalue of ''T'', then the operator (''T'' − ''λI'') is not one-to-one, and therefore its inverse (''T'' − ''λI'')<sup>−1</sup> does not exist. The converse is true for finite-dimensional vector spaces, but not for infinite-dimensional vector spaces. In general, the operator (''T'' − ''λI'') may not have an inverse even if ''λ'' is not an eigenvalue.\n\nFor this reason, in [[functional analysis]] eigenvalues can be generalized to the [[spectrum (functional analysis)|spectrum of a linear operator]] ''T'' as the set of all scalars ''λ'' for which the operator (''T'' − ''λI'') has no [[bounded operator|bounded]] inverse. The spectrum of an operator always contains all its eigenvalues but is not limited to them.\n\n===Associative algebras and representation theory===\n{{main|Weight (representation theory)}}\n\nOne can generalize the algebraic object that is acting on the vector space, replacing a single operator acting on a vector space with an [[algebra representation]] – an [[associative algebra]] acting on a [[module (mathematics)|module]]. The study of such actions is the field of [[representation theory]].\n\nThe [[weight (representation theory)|representation-theoretical concept of weight]] is an analog of eigenvalues, while ''weight vectors'' and ''weight spaces'' are the analogs of eigenvectors and eigenspaces, respectively.\n\n==Dynamic equations==\n\nThe simplest [[difference equation]]s have the form\n: <math>x_t = a_1 x_{t-1} + a_2 x_{t-2} + \\cdots + a_k x_{t-k}.</math>\n\nThe solution of this equation for ''x'' in terms of ''t'' is found by using its characteristic equation\n: <math>\\lambda^k - a_1\\lambda^{k-1} - a_2\\lambda^{k-2} - \\cdots - a_{k-1}\\lambda-a_k = 0,</math>\n\nwhich can be found by stacking into matrix form a set of equations consisting of the above difference equation and the ''k''&nbsp;–&nbsp;1 equations <math>x_{t-1} = x_{t-1},\\  \\dots,\\  x_{t-k+1} = x_{t-k+1},</math> giving a ''k''-dimensional system of the first order in the stacked variable vector <math>\\begin{bmatrix} x_t & \\cdots & x_{t-k+1} \\end{bmatrix}</math> in terms of its once-lagged value, and taking the characteristic equation of this system's matrix. This equation gives ''k'' characteristic roots <math>\\lambda_1,\\, \\ldots,\\, \\lambda_k,</math> for use in the solution equation\n: <math>x_t = c_1\\lambda_1^t + \\cdots + c_k\\lambda_k^t.</math>\n\nA similar procedure is used for solving a [[differential equation]] of the form\n: <math>\\frac{d^k x}{dt^k} + a_{k-1}\\frac{d^{k-1}x}{dt^{k-1}} + \\cdots + a_1\\frac{dx}{dt} + a_0 x = 0.</math>\n\n==Calculation==\n{{main|Eigenvalue algorithm}}\nThe calculation of eigenvalues and eigenvectors is a topic where theory, as presented in elementary linear algebra textbooks, is often very far from practice.\n\n===Classical method===\nThe classical method is to first find the eigenvalues, and then calculate the eigenvectors for each eigenvalue. It is in several ways poorly suited for non-exact arithmetics such as [[floating-point]].\n\n====Eigenvalues====\nThe eigenvalues of a matrix <math>A</math> can be determined by finding the roots of the characteristic polynomial. This is easy for <math> 2 \\times 2 </math> matrices, but the difficulty increases rapidly with the size of the matrix.\n\nIn theory, the coefficients of the characteristic polynomial can be computed exactly, since they are sums of products of matrix elements; and there are algorithms that can find all the roots of a polynomial of arbitrary degree to any required [[accuracy]].<ref name=\"TrefethenBau\" /> However, this approach is not viable in practice because the coefficients would be contaminated by unavoidable [[round-off error]]s, and the roots of a polynomial can be an extremely sensitive function of the coefficients (as exemplified by [[Wilkinson's polynomial]]).<ref name=TrefethenBau>{{Citation |first1=Lloyd N. |last1=Trefethen |first2=David |last2=Bau |title = Numerical Linear Algebra |publisher=SIAM |year=1997 }}</ref> Even for matrices whose elements are integers the calculation becomes nontrivial, because the sums are very long; the constant term is the [[determinant]], which for an <math> n \\times n </math> is a sum of <math> n! </math> different products.{{NoteTag|By doing [[Gaussian elimination]] over [[formal power series]] truncated to <math>n</math> terms it is possible to get away with <math>O(n^4)</math> operations, but that does not take [[combinatorial explosion]] into account.}}\n\nExplicit [[algebraic solution|algebraic formulas]] for the roots of a polynomial exist only if the degree <math>n</math> is 4 or less. According to the [[Abel–Ruffini theorem]] there is no general, explicit and exact algebraic formula for the roots of a polynomial with degree 5 or more. (Generality matters because any polynomial with degree <math>n</math> is the characteristic polynomial of some [[companion matrix]] of order <math>n</math>.) Therefore, for matrices of order 5 or more, the eigenvalues and eigenvectors cannot be obtained by an explicit algebraic formula, and must therefore be computed by approximate [[numerical method]]s. Even the [[Cubic function#General solution to the cubic equation with real coefficients|exact formula]] for the roots of a degree 3 polynomial is numerically impractical.\n\n====Eigenvectors====\nOnce the (exact) value of an eigenvalue is known, the corresponding eigenvectors can be found by finding non-zero solutions of the eigenvalue equation, that becomes a [[linear system|system of linear equations]] with known coefficients. For example, once it is known that 6 is an eigenvalue of the matrix\n:<math>A = \\begin{bmatrix} 4 & 1\\\\ 6 & 3\\end{bmatrix}</math>\nwe can find its eigenvectors by solving the equation <math>A v = 6 v</math>, that is\n:<math>\\begin{bmatrix} 4 & 1\\\\ 6 & 3\\end{bmatrix}\\begin{bmatrix}x \\\\y\\end{bmatrix} = 6 \\cdot \\begin{bmatrix}x \\\\y\\end{bmatrix}</math>\nThis matrix equation is equivalent to two [[linear equation]]s\n:<math>\n  \\left\\{ \\begin{aligned} 4x + y &= 6x \\\\ 6x + 3y &= 6y\\end{aligned} \\right.\n  </math> {{spaces|4}} that is {{spaces|4}} <math>\n  \\left\\{ \\begin{aligned} -2x + y &= 0 \\\\ 6x - 3y &= 0\\end{aligned} \\right.\n</math>\n\nBoth equations reduce to the single linear equation <math>y=2x</math>. Therefore, any vector of the form <math>\\begin{bmatrix} a \\\\ 2a \\end{bmatrix}</math>, for any non-zero real number <math>a</math>, is an eigenvector of <math>A</math> with eigenvalue <math>\\lambda = 6</math>.\n\nThe matrix <math>A</math> above has another eigenvalue <math>\\lambda=1</math>. A similar calculation shows that the corresponding eigenvectors are the non-zero solutions of <math>3x+y=0</math>, that is, any vector of the form <math>\\begin{bmatrix} b \\\\ -3b \\end{bmatrix}</math>, for any non-zero real number <math>b</math>.\n\n===Simple iterative methods===\n{{main|Power iteration}}\nThe converse approach, of first seeking the eigenvectors and then determining each eigenvalue from its eigenvector, turns out to be far more tractable for computers. The easiest algorithm here consists of picking an arbitrary starting vector and then repeatedly multiplying it with the matrix (optionally normalising the vector to keep its elements of reasonable size); surprisingly this makes the vector converge towards an eigenvector. [[inverse iteration|A variation]] is to instead multiply the vector by <math>(A - \\mu I)^{-1}</math>; this causes it to converge to an eigenvector of the eigenvalue closest to <math>\\mu \\in \\mathbb{C}</math>.\n\nIf <math>\\mathbf{v}</math> is (a good approximation of) an eigenvector of <math>A</math>, then the corresponding eigenvalue can be computed as\n: <math> \\lambda = \\frac{\\mathbf{v}^* A\\mathbf{v}}{\\mathbf{v}^* \\mathbf{v}}</math>\n\nwhere <math>\\mathbf{v}^*</math> denotes the [[conjugate transpose]] of <math>\\mathbf{v}</math>.\n\n===Modern methods===\nEfficient, accurate methods to compute eigenvalues and eigenvectors of arbitrary matrices were not known until the advent of the [[QR algorithm]] in 1961.<ref name=TrefethenBau/> Combining the [[Householder transformation]] with the LU decomposition results in an algorithm with better convergence than the QR algorithm.{{citation needed|date=March 2013}} For large [[Hermitian matrix|Hermitian]] [[sparse matrix|sparse matrices]], the [[Lanczos algorithm]] is one example of an efficient [[iterative method]] to compute eigenvalues and eigenvectors, among several other possibilities.<ref name=TrefethenBau/>\n\nMost numeric methods that compute the eigenvalues of a matrix also determine a set of corresponding eigenvectors as a by-product of the computation, although sometimes the implementors choose to discard the eigenvector information as soon as it is not needed anymore.\n\n==Applications==\n\n===Eigenvalues of geometric transformations===\nThe following table presents some example transformations in the plane along with their 2×2 matrices, eigenvalues, and eigenvectors.\n{| class=\"wikitable\" style=\"text-align:center; margin:1em auto 1em auto;\"\n!\n! [[Scaling (geometry)|Scaling]]\n! Unequal scaling\n! [[Rotation (geometry)|Rotation]]\n! [[Shear mapping|Horizontal shear]]\n! [[Hyperbolic rotation]]\n|-\n! Illustration\n| [[File:Homothety in two dim.svg|100px|alt=Equal scaling ([[Homothetic transformation|homothety]])]]\n| [[File:Unequal scaling.svg|100px|alt=Vertical shrink and horizontal stretch of a unit square.]]\n| [[File:Rotation.png|100px|alt=Rotation by 50 degrees]]\n| [[File:Shear.svg|100px|center|alt=Horizontal shear mapping]]\n| [[File:Squeeze r=1.5.svg|100px]]\n|- style=\"vertical-align:top\"\n! Matrix\n| <math>\\begin{bmatrix}k & 0\\\\ 0 & k\\end{bmatrix}</math>\n| <math>\\begin{bmatrix}k_1 & 0\\\\ 0 & k_2\\end{bmatrix}</math>\n| <math>\\begin{bmatrix}c & -s\\\\ s & c\\end{bmatrix}</math><br /><math>c = \\cos\\theta</math><br /><math>s = \\sin\\theta</math>\n| <math> \\begin{bmatrix}1 & k\\\\ 0 & 1\\end{bmatrix}</math>\n| <math>\\begin{bmatrix}c & s\\\\ s & c\\end{bmatrix}</math><br /><math>c = \\cosh\\varphi</math><br /><math>s = \\sinh\\varphi</math>\n|-\n! Characteristic<br />polynomial\n| <math>\\ (\\lambda - k)^2</math>\n| <math>(\\lambda - k_1)(\\lambda - k_2)</math>\n| <math>\\lambda^2 - 2c\\lambda + 1</math>\n| <math>\\ (\\lambda - 1)^2</math>\n| <math>\\lambda^2 - 2c\\lambda + 1</math>\n|-\n! Eigenvalues, <math>\\lambda_i</math>\n| <math>\\lambda_1 = \\lambda_2 = k</math>\n| <math>\\lambda_1 = k_1</math><br /><math>\\lambda_2 = k_2</math>\n| <math>\\lambda_1 = e^{\\mathbf{i}\\theta} = c + s\\mathbf{i}</math><br /><math>\\lambda_2 = e^{-\\mathbf{i}\\theta} = c - s\\mathbf{i}</math>\n| <math>\\lambda_1 = \\lambda_2 = 1</math>\n| <math>\\lambda_1 = e^\\varphi</math><br /><math>\\lambda_2 = e^{-\\varphi}</math>,\n|-\n! Algebraic {{abbr|mult.|multiplicity}},<br /><math>\\mu_i = \\mu(\\lambda_i)</math>\n| <math>\\mu_1 = 2</math>\n| <math>\\mu_1 = 1</math><br /><math>\\mu_2 = 1</math>\n| <math>\\mu_1 = 1</math><br /><math>\\mu_2 = 1</math>\n| <math>\\mu_1 = 2</math>\n| <math>\\mu_1 = 1</math><br /><math>\\mu_2 = 1</math>\n|-\n! Geometric {{abbr|mult.|multiplicity}},<br /><math>\\gamma_i = \\gamma(\\lambda_i)</math>\n| <math>\\gamma_1 = 2</math>\n| <math>\\gamma_1 = 1</math><br /><math>\\gamma_2 = 1</math>\n| <math>\\gamma_1 = 1</math><br /><math>\\gamma_2 = 1</math>\n| <math>\\gamma_1 = 1</math>\n| <math>\\gamma_1 = 1</math><br /><math>\\gamma_2 = 1</math>\n|-\n! Eigenvectors\n| All non-zero vectors\n| <math>\\begin{align}\n  u_1 &= \\begin{bmatrix} 1\\\\ 0\\end{bmatrix} \\\\\n  u_2 &= \\begin{bmatrix} 0\\\\ 1\\end{bmatrix}\n\\end{align}</math>\n| <math>\\begin{align}\n  u_1 &= \\begin{bmatrix}{\\ } 1\\\\ -\\mathbf{i}\\end{bmatrix} \\\\\n  u_2 &= \\begin{bmatrix}{\\ } 1\\\\ +\\mathbf{i}\\end{bmatrix}\n\\end{align}</math>\n| <math>u_1 = \\begin{bmatrix} 1\\\\ 0\\end{bmatrix}</math>\n| <math>\\begin{align}\n  u_1 &= \\begin{bmatrix}{\\ } 1\\\\ {\\ }1\\end{bmatrix} \\\\\n  u_2 &= \\begin{bmatrix}{\\ } 1\\\\ -1\\end{bmatrix}.\n\\end{align}</math>\n|}\n\nNote that the characteristic equation for a rotation is a [[quadratic equation]] with [[discriminant]] <math>D = -4(\\sin\\theta)^2</math>, which is a negative number whenever {{mvar|θ}} is not an integer multiple of 180°. Therefore, except for these special cases, the two eigenvalues are complex numbers, <math>\\cos\\theta \\pm \\mathbf{i}\\sin\\theta</math>; and all eigenvectors have non-real entries. Indeed, except for those special cases, a rotation changes the direction of every nonzero vector in the plane.\n\nA linear transformation that takes a square to a rectangle of the same area (a [[squeeze mapping]]) has reciprocal eigenvalues.\n\n===Schrödinger equation===<!-- This section is linked from [[Eigenstate]] -->\n\n[[File:HAtomOrbitals.png|thumb|271px|The [[wavefunction]]s associated with the [[bound state]]s of an [[electron]] in a [[hydrogen atom]] can be seen as the eigenvectors of the [[hydrogen atom|hydrogen atom Hamiltonian]] as well as of the [[angular momentum operator]]. They are associated with eigenvalues interpreted as their energies (increasing downward: <math>n = 1,\\, 2,\\, 3,\\, \\ldots</math>) and [[angular momentum]] (increasing across: <!-- do not italicize! -->s, p, d, …). The illustration shows the square of the absolute value of the wavefunctions. Brighter areas correspond to higher [[probability density function|probability density]] for a position [[measurement in quantum mechanics|measurement]]. The center of each figure is the [[atomic nucleus]], a [[proton]].]]\n\nAn example of an eigenvalue equation where the transformation <math>T</math> is represented in terms of a differential operator is the time-independent [[Schrödinger equation]] in [[quantum mechanics]]:\n\n: <math>H\\psi_E = E\\psi_E \\,</math>\n\nwhere <math>H</math>, the [[Hamiltonian (quantum mechanics)|Hamiltonian]], is a second-order [[differential operator]] and <math>\\psi_E</math>, the [[wavefunction]], is one of its eigenfunctions corresponding to the eigenvalue <math>E</math>, interpreted as its [[energy]].\n\nHowever, in the case where one is interested only in the [[bound state]] solutions of the Schrödinger equation, one looks for <math>\\psi_E</math> within the space of [[Square-integrable function|square integrable]] functions. Since this space is a [[Hilbert space]] with a well-defined [[scalar product]], one can introduce a [[Basis (linear algebra)|basis set]] in which <math>\\psi_E</math> and <math>H</math> can be represented as a one-dimensional array (i.e., a vector) and a matrix respectively. This allows one to represent the Schrödinger equation in a matrix form.\n\nThe [[bra–ket notation]] is often used in this context. A vector, which represents a state of the system, in the Hilbert space of square integrable functions is represented by <math>|\\Psi_E\\rangle</math>. In this notation, the Schrödinger equation is:\n: <math>H|\\Psi_E\\rangle = E|\\Psi_E\\rangle</math>\n\nwhere <math>|\\Psi_E\\rangle</math> is an '''eigenstate''' of <math>H</math> and <math>E</math> represents the eigenvalue. <math>H</math> is an [[observable]] [[self adjoint operator]], the infinite-dimensional analog of Hermitian matrices. As in the matrix case, in the equation above <math>H|\\Psi_E\\rangle</math> is understood to be the vector obtained by application of the transformation <math>H</math> to <math>|\\Psi_E\\rangle</math>.\n\n===Molecular orbitals===\nIn [[quantum mechanics]], and in particular in [[atomic physics|atomic]] and [[molecular physics]], within the [[Hartree–Fock]] theory, the [[atomic orbital|atomic]] and [[molecular orbital]]s can be defined by the eigenvectors of the [[Fock operator]]. The corresponding eigenvalues are interpreted as [[ionization potential]]s via [[Koopmans' theorem]]. In this case, the term eigenvector is used in a somewhat more general meaning, since the Fock operator is explicitly dependent on the orbitals and their eigenvalues. Thus, if one wants to underline this aspect, one speaks of nonlinear eigenvalue problems. Such equations are usually solved by an [[iteration]] procedure, called in this case [[self-consistent field]] method. In [[quantum chemistry]], one often represents the Hartree–Fock equation in a non-[[orthogonal]] [[basis set (chemistry)|basis set]]. This particular representation is a [[generalized eigenvalue problem]] called [[Roothaan equations]].\n\n===Geology and glaciology===\nIn [[geology]], especially in the study of [[glacial till]], eigenvectors and eigenvalues are used as a method by which a mass of information of a clast fabric's constituents' orientation and dip can be summarized in a 3-D space by six numbers. In the field, a geologist may collect such data for hundreds or thousands of [[clasts]] in a soil sample, which can only be compared graphically such as in a Tri-Plot (Sneed and Folk) diagram,<ref>{{Citation|doi=10.1002/1096-9837(200012)25:13<1473::AID-ESP158>3.0.CO;2-C|last1=Graham|first1=D.|last2=Midgley|first2= N.|title=Graphical representation of particle shape using triangular diagrams: an Excel spreadsheet method|year= 2000|journal= [[Earth Surface Processes and Landforms]] |volume=25|pages=1473–1477|issue=13|bibcode = 2000ESPL...25.1473G }}</ref><ref>{{Citation|doi=10.1086/626490|last1=Sneed|first1= E. D.|last2=Folk|first2= R. L.|year= 1958|title=Pebbles in the lower Colorado River, Texas, a study of particle morphogenesis|journal= Journal of Geology|volume= 66|issue=2|pages=114–150|bibcode=1958JG.....66..114S}}</ref> or as a Stereonet on a Wulff Net.<ref>{{Citation |last1=Knox-Robinson |first1=C. |last2=Gardoll |first2=Stephen J. |year=1998 |page=243 |volume=24 |journal=Computers & Geosciences |title=GIS-stereoplot: an interactive stereonet plotting module for ArcView 3.0 geographic information system |issue=3 |doi=10.1016/S0098-3004(97)00122-2 |bibcode=1998CG.....24..243K}}</ref>\n\nThe output for the orientation tensor is in the three orthogonal (perpendicular) axes of space. The three eigenvectors are ordered <math>v_1, v_2, v_3</math> by their eigenvalues <math>E_1 \\geq E_2 \\geq E_3</math>;<ref>[http://www.ruhr-uni-bochum.de/hardrock/downloads.htm Stereo32 software]</ref> <math>v_1</math> then is the primary orientation/dip of clast, <math>v_2</math> is the secondary and <math>v_3</math> is the tertiary, in terms of strength. The clast orientation is defined as the direction of the eigenvector, on a [[compass rose]] of [[turn (geometry)|360°]]. Dip is measured as the eigenvalue, the modulus of the tensor: this is valued from 0° (no dip) to 90° (vertical). The relative values of <math>E_1</math>, <math>E_2</math>, and <math>E_3</math> are dictated by the nature of the sediment's fabric. If <math>E_1 = E_2 = E_3</math>, the fabric is said to be isotropic. If <math>E_1 = E_2 > E_3</math>, the fabric is said to be planar. If <math>E_1 > E_2 > E_3</math>, the fabric is said to be linear.<ref>{{Citation |last1=Benn |first1=D. |last2=Evans |first2=D. |year=2004 |title=A Practical Guide to the study of Glacial Sediments |location=London |publisher=Arnold |pages=103–107}}</ref>\n\n===Principal component analysis===\n[[File:GaussianScatterPCA.png|thumb|right|PCA of the [[multivariate Gaussian distribution]] centered at <math>(1, 3)</math> with a standard deviation of 3 in roughly the <math>(0.878, 0.478)</math> direction and of&nbsp;1 in the orthogonal direction. The vectors shown are unit eigenvectors of the (symmetric, positive-semidefinite) [[covariance matrix]] scaled by the square root of the corresponding eigenvalue. (Just as in the one-dimensional case, the square root is taken because the [[standard deviation]] is more readily visualized than the [[variance]].]]\n{{Main|Principal component analysis}}\n{{See also|Positive semidefinite matrix|Factor analysis}}\n\nThe [[Eigendecomposition of a matrix#Real symmetric matrices|eigendecomposition]] of a [[symmetric matrix|symmetric]] [[positive semidefinite matrix|positive semidefinite]] (PSD) [[positive semidefinite matrix|matrix]] yields an [[orthogonal basis]] of eigenvectors, each of which has a nonnegative eigenvalue. The orthogonal decomposition of a PSD matrix is used in [[multivariate statistics|multivariate analysis]], where the [[sample variance|sample]] [[covariance matrix|covariance matrices]] are PSD. This orthogonal decomposition is called [[principal components analysis]] (PCA) in statistics. PCA studies [[linear relation]]s among variables. PCA is performed on the [[covariance matrix]] or the [[correlation matrix]] (in which each variable is scaled to have its [[sample variance]] equal to one). For the covariance or correlation matrix, the eigenvectors correspond to [[principal components analysis|principal components]] and the eigenvalues to the [[explained variance|variance explained]] by the principal components. Principal component analysis of the correlation matrix provides an [[orthogonal basis|orthonormal eigen-basis]] for the space of the observed data: In this basis, the largest eigenvalues correspond to the principal components that are associated with most of the covariability among a number of observed data.\n\nPrincipal component analysis is used to study [[data mining|large]] [[data set]]s, such as those encountered in [[bioinformatics]], [[data mining]], [[chemometrics|chemical research]], [[psychometrics|psychology]], and in [[marketing]]. PCA is also popular in psychology, especially within the field of [[psychometrics]]. In [[Q methodology]], the eigenvalues of the correlation matrix determine the Q-methodologist's judgment of ''practical'' significance (which differs from the [[statistical significance]] of [[hypothesis testing]]; cf. [[Scree's test|criteria for determining the number of factors]]). More generally, principal component analysis can be used as a method of [[factor analysis]] in [[structural equation model]]ing.\n\n===Vibration analysis===\n[[File:Mode Shape of a Tuning Fork at Eigenfrequency 440.09 Hz.gif|thumb|Mode shape of a tuning fork at eigenfrequency 440.09{{nbsp}}Hz]]\n{{Main|Vibration}}\n\nEigenvalue problems occur naturally in the vibration analysis of mechanical structures with many [[Degrees of freedom (mechanics)|degrees of freedom]]. The eigenvalues are the natural frequencies (or '''eigenfrequencies''') of vibration, and the eigenvectors are the shapes of these vibrational modes. In particular, undamped vibration is governed by\n:<math>m\\ddot{x} + kx = 0</math>\n\nor\n:<math>m\\ddot{x} = -kx</math>\n\nthat is, acceleration is proportional to position (i.e., we expect <math>x</math> to be sinusoidal in time).\n\nIn <math>n</math> dimensions, <math>m</math> becomes a [[mass matrix]] and <math>k</math> a [[stiffness matrix]]. Admissible solutions are then a linear combination of solutions to the [[generalized eigenvalue problem]]\n:<math>-kx = \\omega^2 mx</math>\n\nwhere <math>\\omega^2</math> is the eigenvalue and <math>\\omega</math> is the (imaginary) [[angular frequency]]. Note that the principal vibration modes are different from the principal compliance modes, which are the eigenvectors of <math>k</math> alone. Furthermore, [[damped vibration]], governed by\n:<math>m\\ddot{x} + c\\dot{x} + kx = 0</math>\n\nleads to a so-called [[quadratic eigenvalue problem]],\n:<math>\\left(\\omega^2 m + \\omega c + k\\right)x = 0.</math>\n\nThis can be reduced to a generalized eigenvalue problem by [[quadratic eigenvalue problem#Methods of Solution|algebraic manipulation]] at the cost of solving a larger system.\n\nThe orthogonality properties of the eigenvectors allows decoupling of the differential equations so that the system can be represented as linear summation of the eigenvectors. The eigenvalue problem of complex structures is often solved using [[finite element analysis]], but neatly generalize the solution to scalar-valued vibration problems.\n\n===Eigenfaces===\n[[File:Eigenfaces.png|thumb|200px|[[Eigenface]]s as examples of eigenvectors]]\n{{Main|Eigenface}}\nIn [[image processing]], processed images of faces can be seen as vectors whose components are the [[brightness]]es of each [[pixel]].<ref>{{Citation\n | last1=Xirouhakis\n | first1=A.\n | last2=Votsis\n | first2=G.\n | last3=Delopoulus\n | first3=A.\n | title=Estimation of 3D motion and structure of human faces\n | publisher=National Technical University of Athens\n | url=http://www.image.ece.ntua.gr/papers/43.pdf\n | format=PDF\n | year=2004\n}}</ref> The dimension of this vector space is the number of pixels. The eigenvectors of the [[covariance matrix]] associated with a large set of normalized pictures of faces are called '''[[eigenface]]s'''; this is an example of [[principal component analysis]]. They are very useful for expressing any face image as a [[linear combination]] of some of them. In the [[Facial recognition system|facial recognition]] branch of [[biometrics]], eigenfaces provide a means of applying [[data compression]] to faces for [[Recognition of human individuals|identification]] purposes. Research related to eigen vision systems determining hand gestures has also been made.\n\nSimilar to this concept, '''eigenvoices''' represent the general direction of variability in human pronunciations of a particular utterance, such as a word in a language. Based on a linear combination of such eigenvoices, a new voice pronunciation of the word can be constructed. These concepts have been found useful in automatic speech recognition systems for speaker adaptation.\n\n===Tensor of moment of inertia===\nIn [[mechanics]], the eigenvectors of the [[inertia tensor|moment of inertia tensor]] define the [[principal axis (mechanics)|principal axes]] of a [[rigid body]]. The [[tensor]] of moment of [[inertia]] is a key quantity required to determine the rotation of a rigid body around its [[center of mass]].\n\n===Stress tensor===\nIn [[solid mechanics]], the [[stress (mechanics)|stress]] tensor is symmetric and so can be decomposed into a [[diagonal]] tensor with the eigenvalues on the diagonal and eigenvectors as a basis. Because it is diagonal, in this orientation, the stress tensor has no [[Shear (mathematics)|shear]] components; the components it does have are the principal components.\n\n===Graphs===\nIn [[spectral graph theory]], an eigenvalue of a [[graph theory|graph]] is defined as an eigenvalue of the graph's [[adjacency matrix]] <math>A</math>, or (increasingly) of the graph's [[Laplacian matrix]] due to its [[discrete Laplace operator]], which is either <math>D - A</math> (sometimes called the ''combinatorial Laplacian'') or <math>I - D^{-1/2}A D^{-1/2}</math> (sometimes called the ''normalized Laplacian''), where <math>D</math> is a diagonal matrix with <math>D_{ii}</math> equal to the degree of vertex <math>v_i</math>, and in <math>D^{-1/2}</math>, the <math>i</math>th diagonal entry is <math>1/\\sqrt{\\deg(v_i)}</math>. The <math>k</math>th principal eigenvector of a graph is defined as either the eigenvector corresponding to the <math>k</math>th largest or <math>k</math>th smallest eigenvalue of the Laplacian. The first principal eigenvector of the graph is also referred to merely as the principal eigenvector.\n\nThe principal eigenvector is used to measure the [[eigenvector centrality|centrality]] of its vertices. An example is [[Google]]'s [[PageRank]] algorithm. The principal eigenvector of a modified [[adjacency matrix]] of the World Wide Web graph gives the page ranks as its components. This vector corresponds to the [[stationary distribution]] of the [[Markov chain]] represented by the row-normalized adjacency matrix; however, the adjacency matrix must first be modified to ensure a stationary distribution exists. The second smallest eigenvector can be used to partition the graph into clusters, via [[spectral clustering]]. Other methods are also available for clustering.\n\n===Basic reproduction number===\n{{main|Basic reproduction number}}\nThe basic reproduction number (<math>R_0</math>) is a fundamental number in the study of how infectious diseases spread. If one infectious person is put into a population of completely susceptible people, then <math>R_0</math> is the average number of people that one typical infectious person will infect. The generation time of an infection is the time, <math>t_G</math>, from one person becoming infected to the next person becoming infected. In a heterogeneous population, the next generation matrix defines how many people in the population will become infected after time <math>t_G</math> has passed. <math>R_0</math> is then the largest eigenvalue of the next generation matrix.<ref>\n{{Citation\n | vauthors=Diekmann O, Heesterbeek JA, Metz JA\n | year = 1990\n | title = On the definition and the computation of the basic reproduction ratio R0 in models for infectious diseases in heterogeneous populations\n | journal = Journal of Mathematical Biology\n | volume = 28\n | issue = 4\n | pages = 365–382\n | pmid = 2117040\n | doi = 10.1007/BF00178324\n}}</ref><ref>\n{{Citation\n |author1=Odo Diekmann |author2=J. A. P. Heesterbeek\n | title = Mathematical epidemiology of infectious diseases\n | series = Wiley series in mathematical and computational biology\n | publisher = John Wiley & Sons\n | location = West Sussex, England\n | year = 2000\n | url = https://books.google.com/books?id=5VjSaAf35pMC&printsec=frontcover#v=onepage&q&f=false\n}}</ref>\n\n==See also==\n* [[Antieigenvalue theory]]\n* [[Eigenoperator]]\n* [[Eigenplane]]\n* [[Eigenvalue algorithm]]\n* [[Introduction to eigenstates]]\n* [[Jordan normal form]]\n* [[List of numerical analysis software]]\n* [[Nonlinear eigenproblem]]\n* [[Quadratic eigenvalue problem]]\n* [[Singular value]]\n\n== Notes ==\n{{NoteFoot}}\n\n== References ==\n=== Citations ===\n{{Reflist}}\n\n=== Sources ===\n{{refbegin}}\n* {{Citation\n| last1=Akivis\n| first1=Max A.\n| last2=Goldberg\n| first2=Vladislav V.\n| title=Tensor calculus\n| series=Russian\n| publisher=Science Publishers, Moscow\n| year=1969\n}}\n* {{Citation\n| last = Aldrich\n| first = John\n| title = Earliest Known Uses of Some of the Words of Mathematics\n| url = http://jeff560.tripod.com/e.html\n| editor = Jeff Miller\n| year = 2006\n| chapter = Eigenvalue, eigenfunction, eigenvector, and related terms\n| chapterurl = http://jeff560.tripod.com/e.html\n| accessdate = 2006-08-22 }}\n* {{Citation\n| last=Alexandrov\n| first=Pavel S.\n| title=Lecture notes in analytical geometry\n| series=Russian\n| publisher=Science Publishers, Moscow\n| year=1968\n| isbn=\n}}\n* {{ Citation\n| last1 = Anton\n| first1 = Howard\n| year = 1987\n| isbn = 0-471-84819-0\n| title = Elementary Linear Algebra\n| edition = 5th\n| publisher = [[John Wiley & Sons|Wiley]]\n| location = New York }}\n* {{ citation\n| last1 = Beauregard\n| first1 = Raymond A.\n| first2 = John B.\n| last2 = Fraleigh\n| year = 1973\n| isbn = 0-395-14017-X\n| title = A First Course In Linear Algebra: with Optional Introduction to Groups, Rings, and Fields\n| publisher = [[Houghton Mifflin Co.]]\n| location = Boston }}\n* {{Citation\n| last=Beezer\n| first=Robert A.\n| title=A first course in linear algebra\n| url=http://linear.ups.edu/\n| publisher=Free online book under GNU licence, University of Puget Sound\n| year=2006\n| isbn=\n}}\n* {{Citation\n| last1 = Betteridge\n| first1 = Harold T.\n| title = The New Cassell's German Dictionary\n| publisher = [[Funk & Wagnall]]\n| location = New York\n| lccn = 58-7924\n| year = 1965\n}}\n* {{Citation\n| last1=Bowen\n| first1=Ray M.\n| last2=Wang\n| first2=Chao-Cheng\n| title=Linear and multilinear algebra\n| publisher=Plenum Press, New York\n| year=1980\n| isbn=0-306-37508-7\n}}\n* {{ citation\n| last1 = Burden\n| first1 = Richard L.\n| first2 = J. Douglas\n| last2 = Faires\n| year = 1993\n| isbn = 0-534-93219-3\n| title = Numerical Analysis\n| edition = 5th\n| publisher = [[Prindle, Weber and Schmidt]]\n| location = Boston }}\n* {{Citation\n| last1=Carter\n| first1=Tamara A.\n| last2=Tapia\n| first2=Richard A.\n| last3=Papaconstantinou\n| first3=Anne\n| title=Linear Algebra: An Introduction to Linear Algebra for Pre-Calculus Students\n| publisher=Rice University, Online Edition\n| url=http://ceee.rice.edu/Books/LA/index.html\n| accessdate=2008-02-19\n}}\n* {{Citation\n| last = Cohen-Tannoudji\n| first = Claude\n| authorlink = Claude Cohen-Tannoudji\n| title = Quantum mechanics\n| publisher = John Wiley & Sons\n| year = 1977\n| chapter = Chapter II. The mathematical tools of quantum mechanics\n| isbn = 0-471-16432-1 }}\n* {{ citation\n| last =  Curtis\n| first = Charles W.\n| authorlink = Charles W. Curtis\n| title = Linear Algebra: An Introductory Approach\n| publisher = Springer\n| edition = 4th\n| year = 1999\n| ISBN = 0-387-90992-3 }}\n* {{Citation\n| last=Demmel\n| first=James W. | authorlink = James Demmel\n| title=Applied numerical linear algebra\n| publisher=SIAM\n| year=1997\n| isbn=0-89871-389-7\n}}\n* {{ citation\n| last1 = Fraleigh\n| first1 = John B.\n| year = 1976\n| isbn = 0-201-01984-1\n| title = A First Course In Abstract Algebra\n| edition = 2nd\n| publisher = [[Addison-Wesley]]\n| location = Reading }}\n* {{Citation\n| last1 = Fraleigh\n| first1 = John B.\n| first2 = Raymond A.\n| last2 = Beauregard\n| title = Linear algebra\n| edition = 3rd\n| publisher = Addison-Wesley Publishing Company\n| year = 1995\n| isbn = 0-201-83999-7}}\n* {{Citation\n| last = Friedberg\n| first = Stephen H.\n| first2 = Arnold J.\n| last2 = Insel\n| first3 = Lawrence E.\n| last3 = Spence\n| title = Linear algebra\n| edition = 2nd\n| publisher = Prentice Hall\n| location = Englewood Cliffs, New Jersey 07632\n| year = 1989\n| isbn = 0-13-537102-3 }}\n* {{Citation\n| last=Gelfand\n| first=I. M.\n| title=Lecture notes in linear algebra\n| series=Russian\n| publisher=Science Publishers, Moscow\n| year=1971\n| isbn=\n}}\n* {{Citation\n| last1=Gohberg\n| first1=Israel\n| last2=Lancaster\n| first2=Peter\n| last3=Rodman\n| first3=Leiba\n| title=Indefinite linear algebra and applications\n| publisher=Birkhäuser Verlag\n| place=Basel-Boston-Berlin\n| year=2005\n| isbn=3-7643-7349-0\n}}\n* {{Citation\n| last1 = Golub\n| first1 = Gene F.\n| first2 = Henk A.\n| last2 = van der Vorst\n| title = Eigenvalue computation in the 20th century\n| journal = Journal of Computational and Applied Mathematics\n| volume = 123\n| pages = 35–65\n| year = 2000\n| doi = 10.1016/S0377-0427(00)00413-1 |bibcode = 2000JCoAM.123...35G }}\n* {{Citation\n| last1=Golub\n| first1=Gene H.\n| authorlink1 = Gene H. Golub\n| last2=Van Loan\n| first2=Charles F.\n| authorlink2 = Charles F. Van Loan\n| title=Matrix computations\n| edition=3rd\n| publisher=Johns Hopkins University Press, Baltimore, Maryland\n| year=1996\n| isbn=978-0-8018-5414-9\n}}\n* {{Citation\n| last=Greub\n| first=Werner H.\n| title=Linear Algebra\n| edition=4th\n| publisher=Springer-Verlag, New York\n| year=1975\n| isbn=0-387-90110-8\n}}\n* {{Citation\n| last = Halmos\n| first = Paul R.\n| authorlink = Paul Halmos\n| title = Finite-dimensional vector spaces\n| edition = 8th\n| publisher = Springer-Verlag\n| location = New York\n| year = 1987\n| isbn = 0-387-90093-4 }}\n* {{Citation\n| last = Hawkins\n| first = T.\n| title = Cauchy and the spectral theory of matrices\n| journal = Historia Mathematica\n| volume = 2\n| pages = 1–29\n| year = 1975\n| doi = 10.1016/0315-0860(75)90032-4 }}\n* {{Citation\n| last=Hefferon\n| first=Jim\n| title=Linear Algebra\n| publisher=Online book, St Michael's College, Colchester, Vermont, USA\n| url=http://joshua.smcvt.edu/linearalgebra/\n| year=2001\n| isbn=\n}}\n* {{ citation\n| last1 = Herstein\n| first1 = I. N.\n| year = 1964\n| isbn = 978-1114541016\n| title = Topics In Algebra\n| publisher = [[Blaisdell Publishing Company]]\n| location = Waltham }}\n* {{Citation\n| last1=Horn\n| first1=Roger A.\n| last2=Johnson\n| first2=Charles F.\n| title=Matrix analysis\n| publisher=Cambridge University Press\n| year=1985\n| isbn=0-521-30586-1\n}}\n* {{Citation\n| last=Kline\n| first=Morris\n| title=Mathematical thought from ancient to modern times\n| publisher=Oxford University Press\n| year=1972\n| isbn=0-19-501496-0\n}}\n* {{Citation\n| last1=Korn\n| first1=Granino A.\n| last2=Korn\n| first2=Theresa M.\n| title=Mathematical Handbook for Scientists and Engineers: Definitions, Theorems, and Formulas for Reference and Review\n| publisher=Dover Publications\n| edition=2nd Revised\n| year=2000\n| isbn=0-486-41147-8\n| bibcode=1968mhse.book.....K\n| journal=New York: McGraw-Hill\n}}\n* {{Citation\n| last=Kuttler\n| first=Kenneth\n| title=An introduction to linear algebra\n| publisher=Online e-book in PDF format, Brigham Young University\n| url=http://www.math.byu.edu/~klkuttle/Linearalgebra.pdf\n| format=PDF\n| year=2007\n| isbn=\n}}\n* {{Citation\n| last = Lancaster\n| first = P.\n| title = Matrix theory\n| series = Russian\n| publisher = Science Publishers\n| location = Moscow, Russia\n| year = 1973 }}\n* {{Citation\n| last1=Larson\n| first1=Ron\n| last2=Edwards\n| first2=Bruce H.\n| title=Elementary linear algebra\n| edition=5th\n| publisher=Houghton Mifflin Company\n| year=2003\n| isbn=0-618-33567-6\n}}\n* {{Citation\n| last = Lipschutz\n| first = Seymour\n| title = Schaum's outline of theory and problems of linear algebra\n| edition = 2nd\n| publisher = McGraw-Hill Companies\n| location = New York\n| series = Schaum's outline series\n| year = 1991\n| isbn = 0-07-038007-4 }}\n* {{Citation\n| last=Meyer\n| first=Carl D.\n| title=Matrix analysis and applied linear algebra\n| publisher=Society for Industrial and Applied Mathematics (SIAM), Philadelphia\n| year=2000\n| isbn=978-0-89871-454-8\n}}\n* {{ citation\n| last1 = Nering\n| first1 = Evar D.\n| year = 1970\n| title = Linear Algebra and Matrix Theory\n| edition = 2nd\n| publisher = [[John Wiley & Sons|Wiley]]\n| location = New York\n| lccn = 76091646\n}}\n* {{ru icon}}{{ cite encyclopedia\n| last1 = Pigolkina\n| first1 = T. S.\n| last2 = Shulman\n| first2 = V. S.\n| title = Eigenvalue\n| editor-last = Vinogradov\n| editor-first = I. M.\n| encyclopedia = Mathematical Encyclopedia\n| volume = 5\n| publisher = Soviet Encyclopedia\n| place = Moscow\n| year = 1977 }}\n* {{Citation\n| last1=Press\n| first1=William H.\n| last2=Teukolsky\n| first2=Saul A.\n| authorlink2=Saul Teukolsky\n| last3=Vetterling\n| first3=William T.\n| last4=Flannery\n| first4=Brian P.\n| title=Numerical Recipes: The Art of Scientific Computing\n| year=2007\n| edition=3rd\n| isbn=9780521880688\n}}\n* {{Citation\n| last=Roman\n| first=Steven\n| title=Advanced linear algebra\n| edition=3rd\n| publisher=Springer Science + Business Media, LLC\n| place=New York\n| year=2008\n| isbn=978-0-387-72828-5\n}}\n* {{Citation\n| last=Sharipov\n| first=Ruslan A.\n| title=Course of Linear Algebra and Multidimensional Geometry: the textbook\n| year=1996\n| isbn=5-7477-0099-5\n| arxiv=math/0405323\n|bibcode = 2004math......5323S }}\n* {{Citation\n| last=Shilov\n| first=Georgi E.\n| title=Linear algebra\n| others=Translated and edited by Richard A. Silverman\n| publisher=Dover Publications\n| place=New York\n| year=1977\n| isbn=0-486-63518-X\n}}\n* {{Citation\n| last=Shores\n| first=Thomas S.\n| title=Applied linear algebra and matrix analysis\n| publisher=Springer Science+Business Media, LLC\n| year=2007\n| isbn=0-387-33194-8\n}}\n* {{Citation\n| last=Strang\n| first=Gilbert\n| title=Introduction to linear algebra\n| publisher=Wellesley-Cambridge Press, Wellesley, Massachusetts\n| year=1993\n| isbn=0-9614088-5-5\n}}\n* {{Citation\n| last=Strang\n| first=Gilbert\n| title=Linear algebra and its applications\n| publisher=Thomson, Brooks/Cole, Belmont, California\n| year=2006\n| isbn=0-03-010567-6\n}}\n{{refend}}\n\n== External links ==\n{{Wikibooks|Linear Algebra|Eigenvalues and Eigenvectors}}\n* [http://www.physlink.com/education/AskExperts/ae520.cfm What are Eigen Values?] – non-technical introduction from PhysLink.com's \"Ask the Experts\"\n* [http://people.revoledu.com/kardi/tutorial/LinearAlgebra/EigenValueEigenVector.html Eigen Values and Eigen Vectors Numerical Examples] – Tutorial and Interactive Program from Revoledu.\n* [https://web.archive.org/web/20100325112901/http://khanexercises.appspot.com/video?v=PhfbEr2btGQ Introduction to Eigen Vectors and Eigen Values] – lecture from Khan Academy\n* {{cite web |last=Hill |first=Roger |title=λ – Eigenvalues |url=http://www.sixtysymbols.com/videos/eigenvalues.htm |work=Sixty Symbols |publisher=[[Brady Haran]] for the [[University of Nottingham]] |year=2009}}\n* {{cite web |title=A Beginner's Guide to Eigenvectors |url=http://deeplearning4j.org/eigenvector |publisher=[[Deeplearning4j]] |year=2015}}\n* [https://www.youtube.com/watch?v=PFDu9oVAE-g&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&index=14 Eigenvectors and eigenvalues | Essence of linear algebra, chapter 10] – A visual explanation with [[3Blue1Brown]]\n* [https://www.symbolab.com/solver/matrix-eigenvectors-calculator Matrix Eigenvectors Calculator] from Symbolab (Click on the bottom right button of the 2x12 grid to select a matrix size. Select an <math>n \\times n</math> size (for a square matrix), then fill out the entries numerically and click on the Go button. It can accept complex numbers as well.)\n\n===Theory===\n* {{Springer |title=Eigen value |id=p/e035150}}\n* {{Springer |title=Eigen vector |id=p/e035180}}\n* {{planetmath reference|id=4397|title=Eigenvalue (of a matrix)}}\n* [http://mathworld.wolfram.com/Eigenvector.html Eigenvector] – Wolfram [[MathWorld]]\n* [http://ocw.mit.edu/ans7870/18/18.06/javademo/Eigen/ Eigen Vector Examination working applet]\n* [http://web.mit.edu/18.06/www/Demos/eigen-applet-all/eigen_sound_all.html Same Eigen Vector Examination as above in a Flash demo with sound]\n* [http://www.sosmath.com/matrix/eigen1/eigen1.html Computation of Eigenvalues]\n* [http://www.cs.utk.edu/~dongarra/etemplates/index.html Numerical solution of eigenvalue problems] Edited by Zhaojun Bai, [[James Demmel]], Jack Dongarra, Axel Ruhe, and [[Henk van der Vorst]]\n* Eigenvalues and Eigenvectors on the Ask Dr. Math forums: [http://mathforum.org/library/drmath/view/55483.html], [http://mathforum.org/library/drmath/view/51989.html]\n\n===Demonstration applets===\n* [http://scienceapplets.blogspot.com/2012/03/eigenvalues-and-eigenvectors.html Java applet about eigenvectors in the real plane]\n* [http://reference.wolfram.com/language/guide/DifferentialEquations.html Wolfram Language functionality for Eigenvalues, Eigenvectors and Eigensystems]\n\n{{Linear algebra}}\n{{Areas of mathematics |collapsed}}\n\n{{DEFAULTSORT:Eigenvalues And Eigenvectors}}\n[[Category:Mathematical physics]]\n[[Category:Abstract algebra]]\n[[Category:Linear algebra]]\n[[Category:Matrix theory]]\n[[Category:Singular value decomposition]]\n[[Category:Articles including recorded pronunciations]]"
    },
    {
      "title": "Embedding",
      "url": "https://en.wikipedia.org/wiki/Embedding",
      "text": "{{Short description|Inclusion of one mathematical structure in another, preserving properties of interest}}{{Redirect|Isometric embedding|related concepts for [[metric space]]s|isometry}}\n{{Other uses}}\n\nIn [[mathematics]], an '''embedding''' (or '''imbedding'''<ref>{{harvnb|Spivak|1999|page=49}} suggests that \"the English\" (i.e. the British) use \"embedding\" instead of \"imbedding\".</ref>) is one instance of some [[mathematical structure]] contained within another instance, such as a [[group (mathematics)|group]] that is a [[subgroup]].\n\nWhen some object ''X'' is said to be embedded in another object ''Y'', the embedding is given by some [[Injective function|injective]] and structure-preserving map {{nowrap|''f'' : ''X'' → ''Y''}}. The precise meaning of \"structure-preserving\" depends on the kind of mathematical structure of which ''X'' and ''Y'' are instances. In the terminology of [[category theory]], a structure-preserving map is called a [[morphism]].\n\nThe fact that a map {{nowrap|''f'' : ''X'' → ''Y''}} is an embedding is often indicated by the use of a \"hooked arrow\" ({{unichar|21AA|RIGHTWARDS ARROW WITH HOOK|ulink=Unicode}});<ref name=\"Unicode Arrows\">{{cite web| title = Arrows – Unicode| url = https://www.unicode.org/charts/PDF/U2190.pdf| accessdate = 2017-02-07}}</ref> thus: <math> f : X \\hookrightarrow Y.</math> (On the other hand, this notation is sometimes reserved for [[inclusion map]]s.)\n\nGiven ''X'' and ''Y'', several different embeddings of ''X'' in ''Y'' may be possible. In many cases of interest there is a standard (or \"canonical\") embedding, like those of the [[natural number]]s in the [[integer]]s, the integers in the [[rational number]]s, the rational numbers in the [[real number]]s, and the real numbers in the [[complex number]]s. In such cases it is common to identify the [[Domain (mathematics)|domain]] ''X'' with its [[image (mathematics)|image]] ''f''(''X'') contained in ''Y'', so that {{nowrap|''X'' ⊆ ''Y''}}.\n\n==Topology and geometry==\n===General topology===\n\nIn [[general topology]], an embedding is a [[homeomorphism]] onto its image.<ref>{{harvnb|Hocking|Young|1988|page=73}}. {{harvnb|Sharpe|1997|page=16}}.</ref> More explicitly, an injective [[continuous function (topology)|continuous]] map <math>f : X \\to Y</math> between [[topological space]]s <math>X</math> and <math>Y</math> is a '''topological embedding''' if <math>f</math> yields a homeomorphism between <math>X</math> and <math>f(X)</math> (where <math>f(X)</math> carries the [[subspace topology]] inherited from <math>Y</math>). Intuitively then, the embedding <math>f : X \\to Y</math> lets us treat <math>X</math> as a [[subspace topology|subspace]] of <math>Y</math>. Every embedding is injective and [[continuous function (topology)|continuous]]. Every map that is injective, continuous and either [[open map|open]] or [[closed map|closed]] is an embedding; however there are also embeddings which are neither open nor closed. The latter happens if the image <math>f(X)</math> is neither an [[open set]] nor a [[closed set]] in <math>Y</math>.\n\nFor a given space <math>Y</math>, the existence of an embedding <math>X \\to Y</math> is a [[topological invariant]] of <math>X</math>. This allows two spaces to be distinguished if one is able to be embedded in a space while the other is not.\n\n===Differential topology===\n\nIn [[differential topology]]:\nLet <math>M</math> and <math>N</math> be smooth [[manifold]]s and <math>f:M\\to N</math> be a smooth map. Then <math>f</math> is called an [[immersion (mathematics)|immersion]] if its [[pushforward (differential)|derivative]] is everywhere injective. An '''embedding''', or a '''smooth embedding''', is defined to be an injective immersion which is an embedding in the topological sense mentioned above (i.e. [[homeomorphism]] onto its image).<ref>{{harvnb|Bishop|Crittenden|1964|page=21}}. {{harvnb|Bishop|Goldberg|1968|page=40}}. {{harvnb|Crampin|Pirani|1994|page=243}}. {{harvnb|do Carmo|1994|page=11}}. {{harvnb|Flanders|1989|page=53}}. {{harvnb|Gallot|Hulin|Lafontaine|2004|page=12}}. {{harvnb|Kobayashi|Nomizu|1963|page=9}}. {{harvnb|Kosinski|2007|page=27}}. {{harvnb|Lang|1999|page=27}}. {{harvnb|Lee|1997|page=15}}. {{harvnb|Spivak|1999|page=49}}. {{harvnb|Warner|1983|page=22}}.</ref> \n \nIn other words, the domain of an embedding is [[diffeomorphism|diffeomorphic]] to its image, and in particular the image of an embedding must be a [[submanifold]]. An immersion is a local embedding (i.e. for any point <math>x\\in M</math> there is a neighborhood <math>x\\in U\\subset M</math> such that <math>f:U\\to N</math> is an embedding.)\n\nWhen the domain manifold is compact, the notion of a smooth embedding is equivalent to that of an injective immersion.\n\nAn important case is <math>N = \\mathbb{R}^n</math>. The interest here is in how large <math>n</math> must be for an embedding, in terms of the dimension <math>m</math> of <math>M</math>. The [[Whitney embedding theorem]]<ref>Whitney H., ''Differentiable manifolds,'' Ann. of Math. (2), '''37''' (1936), pp. 645–680</ref> states that <math>n = 2m</math> is enough, and is the best possible linear bound. For example the [[real projective space]] '''RP'''<sup>''m''</sup> of dimension <math>m</math>, where <math>m</math> is a power of two, requires <math>n = 2m</math> for an embedding. However, this does not apply to immersions; for instance, '''RP'''<sup>2</sup> can be immersed in <math>\\mathbb{R}^3</math> as is explicitly shown by [[Boy's surface]]&mdash;which has self-intersections. The [[Roman surface]] fails to be an immersion as it contains [[cross-cap]]s.\n\nAn embedding is '''proper''' if it behaves well with respect to [[Topological_manifold#Manifolds_with_boundary|boundaries]]: one requires the map <math>f: X \\rightarrow Y</math> to be such that\n\n*<math>f(\\partial X) = f(X) \\cap \\partial Y</math>, and\n*<math>f(X)</math> is [[Transversality (mathematics)|transverse]] to <math>\\partial Y</math> in any point of <math>f(\\partial X)</math>.\n\nThe first condition is equivalent to having <math>f(\\partial X) \\subseteq \\partial Y</math> and <math>f(X \\setminus \\partial X) \\subseteq Y \\setminus \\partial Y</math>. The second condition, roughly speaking, says that ''f''(''X'') is not tangent to the boundary of ''Y''.\n\n===Riemannian geometry===\n\nIn [[Riemannian geometry]]:\nLet (''M,g'') and (''N,h'') be [[Riemannian manifold]]s.\nAn '''isometric embedding''' is a smooth embedding ''f'' : ''M'' → ''N'' which preserves the  [[Riemannian metric|metric]] in the sense that ''g'' is equal to the [[pullback (differential geometry)|pullback]] of ''h'' by ''f'', i.e. ''g'' = ''f''*''h''. Explicitly, for any two tangent vectors \n\n:<math>v,w\\in T_x(M)</math>\n\nwe have \n\n:<math>g(v,w)=h(df(v),df(w)).</math>\n\nAnalogously, '''isometric immersion''' is an immersion between Riemannian manifolds which preserves the Riemannian metrics.\n\nEquivalently, an isometric embedding (immersion) is a smooth embedding (immersion) which preserves length of [[curve]]s (cf. [[Nash embedding theorem]]).<ref>Nash J.,  ''The embedding problem for Riemannian manifolds,'' Ann. of Math. (2), '''63''' (1956), 20–63.</ref>\n\n==Algebra==\nIn general, for an algebraic category ''C'', an embedding between two ''C''-algebraic structures ''X'' and ''Y'' is a ''C''-morphism {{nowrap|''e'' : ''X'' → ''Y''}} that is injective.\n\n===Field theory===\n\nIn [[field theory (mathematics)|field theory]], an '''embedding''' of a [[field (mathematics)|field]] ''E'' in a field ''F'' is a [[ring homomorphism]] {{nowrap|''σ'' : ''E'' → ''F''}}.\n\nThe [[Kernel (algebra)|kernel]] of ''σ'' is an [[ideal (ring theory)|ideal]] of ''E'' which cannot be the whole field ''E'', because of the condition {{nowrap|1=''σ''(1) = 1}}. Furthermore, it is a well-known property of fields that their only ideals are the zero ideal and the whole field itself. Therefore, the kernel is 0, so any embedding of fields is a [[monomorphism]]. Hence, ''E'' is [[isomorphic]] to the subfield ''σ''(''E'') of ''F''. This justifies the name ''embedding'' for an arbitrary homomorphism of fields.\n\n===Universal algebra and model theory===\n{{further|Substructure (mathematics)|Elementary equivalence}}\nIf σ is a [[signature (logic)|signature]] and <math>A,B</math> are σ-[[structure (mathematical logic)|structures]] (also called σ-algebras in [[universal algebra]] or models in [[model theory]]), then a map <math>h:A \\to B</math> is a σ-embedding [[iff]] all of the following hold:\n* <math>h</math> is injective,\n* for every <math>n</math>-ary function symbol <math>f \\in\\sigma</math> and <math>a_1,\\ldots,a_n \\in A^n,</math> we have <math>h(f^A(a_1,\\ldots,a_n))=f^B(h(a_1),\\ldots,h(a_n))</math>,\n* for every <math>n</math>-ary relation symbol <math>R \\in\\sigma</math> and <math>a_1,\\ldots,a_n \\in A^n,</math> we have <math>A \\models R(a_1,\\ldots,a_n)</math> iff <math>B \\models R(h(a_1),\\ldots,h(a_n)).</math>\n\nHere <math>A\\models R (a_1,\\ldots,a_n)</math> is a model theoretical notation equivalent to <math>(a_1,\\ldots,a_n)\\in R^A</math>. In model theory there is also a stronger notion of [[elementary embedding]].\n\n==Order theory and domain theory==\nIn [[order theory]], an embedding of [[partial order]]s is a function F from X to Y such that:\n\n:<math>\\forall x_1,x_2\\in X: x_1\\leq x_2\\Leftrightarrow F(x_1)\\leq F(x_2)</math>.\n\nIn [[domain theory]], an additional requirement is: \n\n:<math> \\forall y\\in Y:\\{x: F(x)\\leq y\\}</math> is [[Directed set|directed]].\n\n==Metric spaces==\n\nA mapping <math>\\phi: X \\to Y</math> of [[metric spaces]] is called an ''embedding''\n(with [[stretch factor|distortion]] <math>C>0</math>) if \n:<math> L d_X(x, y) \\leq d_Y(\\phi(x), \\phi(y)) \\leq CLd_X(x,y) </math>\nfor some constant <math>L>0</math>.\n\n=== Normed spaces ===\n\nAn important special case is that of [[normed spaces]]; in this case it is natural to consider linear embeddings. \n\nOne of the basic questions that can be asked about a finite-dimensional [[normed space]] <math>(X, \\| \\cdot \\|)</math> is, ''what is the maximal dimension <math>k</math> such that the [[Hilbert space]] <math>\\ell_2^k</math> can be linearly embedded into <math>X</math> with constant distortion?''\n\nThe answer is given by [[Dvoretzky's theorem]].\n\n==Category theory==\n\nIn [[category theory]], there is no satisfactory and generally accepted definition of embeddings that is applicable in all categories. One would expect that all isomorphisms and all compositions of embeddings are embeddings, and that all embeddings are monomorphisms. Other typical requirements are: any [[monomorphism#Related concepts|extremal monomorphism]] is an embedding and embeddings are stable under [[Pullback (category theory)|pullback]]s.\n\nIdeally the class of all embedded [[subobject]]s of a given object, up to isomorphism, should also be [[small class|small]], and thus an [[ordered set]]. In this case, the category is said to be well powered with respect to the class of embeddings. This allows defining new local structures in the category (such as a [[closure operator]]). \n\nIn a [[concrete category]], an '''embedding''' is a morphism ''ƒ'':&nbsp;''A''&nbsp;→&nbsp;''B'' which is an injective function from the underlying set of ''A'' to the underlying set of ''B'' and is also an '''initial morphism''' in the following sense:\nIf ''g'' is a function from the underlying set of an object ''C'' to the underlying set of ''A'', and if its composition with ''ƒ'' is a morphism ''ƒg'':&nbsp;''C''&nbsp;→&nbsp;''B'', then ''g'' itself is a morphism.\n\nA [[factorization system]] for a category also gives rise to a notion of embedding. If (''E'',&nbsp;''M'') is a factorization system, then the morphisms in ''M'' may be regarded as the embeddings, especially when the category is well powered with respect to&nbsp;''M''. Concrete theories often have a factorization system in which ''M'' consists of the embeddings in the previous sense. This is the case of the majority of the examples given in this article.\n\nAs usual in category theory, there is a [[dual (category theory)|dual]] concept, known as quotient. All the preceding properties can be dualized.\n\nAn embedding can also refer to an [[Subcategory#Embeddings|embedding functor]].\n\n==See also==\n*[[Closed immersion]]\n*[[Cover (algebra)|Cover]]\n*[[Dimension reduction]]\n*[[Immersion (mathematics)|Immersion]]\n*[[Johnson–Lindenstrauss lemma]]\n*[[Submanifold]]\n*[[Subspace (topology)|Subspace]]\n*[[Universal spaces in the topology and topological dynamics|Universal space]]\n\n==Notes==\n{{reflist}}\n\n== References ==\n* {{cite book|ref=harv|last1=Bishop|first1=Richard Lawrence|authorlink1=Richard L. Bishop|last2=Crittenden|first2=Richard J.|title=Geometry of manifolds|publisher=Academic Press|location=New York|year=1964|isbn=978-0-8218-2923-3}}\n* {{cite book|ref=harv | last1=Bishop|first1=Richard Lawrence|author1-link=Richard L. Bishop|last2=Goldberg|first2=Samuel Irving| title = Tensor Analysis on Manifolds| publisher=The Macmillan Company | year=1968|edition=First Dover 1980|isbn=0-486-64039-6}}\n* {{cite book|ref=harv|last1=Crampin|first1=Michael|last2=Pirani|first2=Felix Arnold Edward|authorlink2=Felix Pirani|title=Applicable differential geometry|publisher=Cambridge University Press|location=Cambridge, England|year=1994|isbn=978-0-521-23190-9}}\n*{{cite book|ref=harv|title = Riemannian Geometry|first=Manfredo Perdigao | last = do Carmo |authorlink=Manfredo do Carmo | year = 1994|isbn=978-0-8176-3490-2}}\n* {{cite book|ref=harv|last=Flanders|first=Harley|authorlink=Harley Flanders|title=Differential forms with applications to the physical sciences|publisher=Dover|year=1989|isbn=978-0-486-66169-8}}\n* {{Cite book|ref=harv | last1=Gallot | first1=Sylvestre | last2=Hulin | first2=Dominique | last3=Lafontaine | first3=Jacques | title=Riemannian Geometry | publisher=[[Springer-Verlag]] | location=Berlin, New York | edition=3rd | isbn=978-3-540-20493-0 | year=2004}}\n* {{cite book|ref=harv|first1=John Gilbert|last1=Hocking|first2=Gail Sellers|last2=Young|title=Topology|year=1988|origyear=1961|publisher=Dover|isbn=0-486-65676-4}}\n*{{cite book|ref=harv|last=Kosinski|first=Antoni Albert|year=2007|origyear=1993|title=Differential manifolds|location=Mineola, New York|publisher=Dover Publications|isbn=978-0-486-46244-8}}\n*{{Cite book|ref=harv | isbn = 978-0-387-98593-0 | title = Fundamentals of Differential Geometry | last1 = Lang | first1 = Serge |authorlink1=Serge Lang| year = 1999 |publisher=Springer|location=New York| series = Graduate Texts in Mathematics}}\n*{{cite book|ref=harv|last1=Kobayashi|first1=Shoshichi|authorlink1=Shoshichi Kobayashi|last2=Nomizu|first2=Katsumi|authorlink2=Katsumi Nomizu| title = Foundations of Differential Geometry, Volume 1| publisher=Wiley-Interscience |location=New York| year=1963}}\n* {{cite book|ref=harv|first=John Marshall|last=Lee|title=Riemannian manifolds|publisher=Springer Verlag|year=1997|isbn=978-0-387-98322-6}}\n* {{cite book|ref=harv| first = R.W. | last = Sharpe | title = Differential Geometry: Cartan's Generalization of Klein's Erlangen Program | publisher = Springer-Verlag, New York | year = 1997| isbn = 0-387-94732-9}}.\n* {{cite book|ref=harv|last=Spivak|first=Michael|authorlink=Michael Spivak|title=A Comprehensive introduction to differential geometry (Volume 1)|year=1999|origyear=1970|publisher=Publish or Perish|isbn=0-914098-70-5}}\n* {{cite book|ref=harv| first=Frank Wilson| last = Warner | title = Foundations of Differentiable Manifolds and Lie Groups | publisher = Springer-Verlag, New York | year = 1983| isbn = 0-387-90894-3}}.\n\n== External links ==\n*{{cite book|last=Adámek|first=Jiří|author2=Horst Herrlich |author3=George Strecker |title=Abstract and Concrete Categories (The Joy of Cats)|url=http://katmat.math.uni-bremen.de/acc/|year=2006}}\n* [http://www.map.mpim-bonn.mpg.de/Embedding Embedding of manifolds] on the Manifold Atlas\n[[Category:Abstract algebra]]\n[[Category:Category theory]]\n[[Category:General topology]]\n[[Category:Differential topology]]\n[[Category:Functions and mappings]]\n[[Category:Maps of manifolds]]\n[[Category:Model theory]]\n[[Category:Order theory]]\n\n{{set index article}}"
    },
    {
      "title": "Euclidean vector",
      "url": "https://en.wikipedia.org/wiki/Euclidean_vector",
      "text": "{{short description|Geometric object that has magnitude (or length) and direction}}\n{{about|the vectors mainly used in physics and engineering to represent directed quantities|mathematical vectors in general|Vector (mathematics and physics)|other uses|Vector (disambiguation)}}\n\n[[Image:vector from A to B.svg|thumb|A vector pointing from ''A'' to ''B'']]\nIn [[mathematics]], [[physics]], and [[engineering]], a '''Euclidean vector''' (sometimes called a '''geometric'''<ref>{{harvnb|Ivanov|2001}}</ref> or '''spatial vector''',<ref>{{harvnb|Heinbockel|2001}}</ref> or—as here—simply a '''vector''') is a geometric object that has [[Magnitude (mathematics)|magnitude]] (or [[euclidean norm|length]]) and [[Direction (geometry)|direction]]. Vectors can be added to other vectors according to [[vector algebra]]. A Euclidean vector is frequently represented by a [[line segment]] with a definite direction, or graphically as an arrow, connecting an ''initial point'' ''A'' with a ''terminal point'' ''B'',<ref>{{harvnb|Ito|1993|p=1678}}; {{harvnb|Pedoe|1988}}</ref> and denoted by <math>\\overrightarrow{AB}.</math>\n\nA vector is what is needed to \"carry\" the point ''A'' to the point ''B''; the Latin word ''vector'' means \"carrier\".<ref>Latin: vectus, [[perfect participle]] of vehere, \"to carry\"/ ''veho'' = \"I carry\".  For historical development of the word ''vector'', see {{OED|vector ''n.''}} and {{cite web|author = Jeff Miller| url = http://jeff560.tripod.com/v.html | title = Earliest Known Uses of Some of the Words of Mathematics | accessdate = 2007-05-25}}</ref> It was first used by 18th century astronomers investigating planetary revolution around the Sun.<ref>{{cite book|title=The Oxford english dictionary.|year=2001|publisher=Claredon Press|location=London|isbn=9780195219425|edition=2nd.}}</ref>  The magnitude of the vector is the distance between the two points and the direction refers to the direction of displacement from ''A'' to ''B''. Many [[algebraic operation]]s on [[real number]]s such as [[addition]], [[subtraction]], [[multiplication]], and [[Additive inverse|negation]] have close analogues for vectors, operations which obey the familiar algebraic laws of [[commutativity]], [[associativity]], and [[distributivity]]. These operations and associated laws qualify [[Euclidean space|Euclidean]] vectors as an example of the more generalized concept of vectors defined simply as elements of a [[vector space]].\n\nVectors play an important role in [[physics]]: the [[velocity]] and [[acceleration]] of a moving object and the [[force]]s acting on it can all be described with vectors. Many other physical quantities can be usefully thought of as vectors. Although most of them do not represent distances (except, for example, [[position (vector)|position]] or [[displacement (vector)|displacement]]), their magnitude and direction can still be represented by the length and direction of an arrow. The mathematical representation of a physical vector depends on the [[coordinate system]] used to describe it. Other vector-like objects that describe physical quantities and transform in a similar way under changes of the coordinate system include [[pseudovector]]s and [[tensor]]s.\n\n==History==\nThe concept of vector, as we know it today, evolved gradually over a period of more than 200 years. About a dozen people made significant contributions.<ref name=\"Crowe\">Michael J. Crowe, [[A History of Vector Analysis]]; see also his {{cite web |url=http://www.nku.edu/~curtin/crowe_oresme.pdf |title=lecture notes |accessdate=2010-09-04 |deadurl=yes |archiveurl=https://web.archive.org/web/20040126161844/http://www.nku.edu/~curtin/crowe_oresme.pdf |archivedate=January 26, 2004 |df= }} on the subject.</ref>\n\n[[Giusto Bellavitis]] abstracted the basic idea in 1835 when he established the concept of [[equipollence (geometry)|equipollence]]. Working in a Euclidean plane, he made equipollent any pair of line segments of the same length and orientation. Essentially he realized an [[equivalence relation]] on the pairs of points (bipoints) in the plane and thus erected the first space of vectors in the plane.<ref name=\"Crowe\"/>{{rp|52–4}}\n\nThe term ''vector'' was introduced by [[William Rowan Hamilton]] as part of a [[quaternion]], which is a sum {{math|1=''q'' = ''s'' + ''v''}} of a [[Real number]] {{math|''s''}} (also called ''scalar'') and a 3-dimensional ''vector''. Like Bellavitis, Hamilton viewed vectors as representative of [[equivalence class|classes]] of equipollent directed segments. As [[complex number]]s use an [[imaginary unit]] to complement the [[real line]], Hamilton considered the vector {{math|''v''}} to be the ''imaginary part'' of a quaternion:\n:The algebraically imaginary part, being geometrically constructed by a straight line, or radius vector, which has, in general, for each determined quaternion, a determined length and determined direction in space, may be called the vector part, or simply the vector of the quaternion.<ref>W. R. Hamilton (1846) ''London, Edinburgh & Dublin Philosophical Magazine'' 3rd series 29 27</ref>\n\nSeveral other mathematicians developed vector-like systems in the middle of the nineteenth century,  including [[Augustin Cauchy]], [[Hermann Grassmann]], [[August Möbius]], [[Comte de Saint-Venant]], and [[Matthew O'Brien (mathematician)|Matthew O'Brien]]. Grassmann's 1840 work ''Theorie der Ebbe und Flut'' (Theory of the Ebb and Flow) was the first system of spatial analysis similar to today's system and had ideas corresponding to the cross product, scalar product and vector differentiation. Grassmann's work was largely neglected until the 1870s.<ref name=\"Crowe\"/>\n\n[[Peter Guthrie Tait]] carried the quaternion standard after Hamilton. His 1867 ''Elementary Treatise of Quaternions'' included extensive treatment of the nabla or [[del|del operator]] ∇.\n\nIn 1878 ''[[Elements of Dynamic]]'' was published by [[William Kingdon Clifford]]. Clifford simplified the quaternion study by isolating the [[dot product]] and [[cross product]] of two vectors from the complete quaternion product. This approach made vector calculations available to engineers and others working in three dimensions and skeptical of the fourth.\n\n[[Josiah Willard Gibbs]], who was exposed to quaternions through [[James Clerk Maxwell]]'s ''Treatise on Electricity and Magnetism'', separated off their vector part for independent treatment. The first half of Gibbs's ''Elements of Vector Analysis'', published in 1881, presents what is essentially the modern system of vector analysis.<ref name=\"Crowe\" /> In 1901 [[Edwin Bidwell Wilson]] published ''[[Vector Analysis]]'', adapted from Gibb's lectures, which banished any mention of quaternions in the development of vector calculus.\n\n==Overview==\nIn [[physics]] and [[engineering]], a vector is typically regarded as a geometric entity characterized by a [[magnitude (mathematics)|magnitude]] and a direction. It is formally defined as a directed [[line segment]], or arrow, in a [[Euclidean space]].<ref>{{harvnb|Ito|1993|p=1678}}</ref> In [[pure mathematics]], a vector is defined more generally as any element of a [[vector space]]. In this context, vectors are abstract entities which may or may not be characterized by a magnitude and a direction. This generalized definition implies that the above-mentioned geometric entities are a special kind of vectors, as they are elements of a special kind of vector space called [[Euclidean space]].\n\nThis article is about vectors strictly defined as arrows in Euclidean space. When it becomes necessary to distinguish these special vectors from vectors as defined in pure mathematics, they are sometimes referred to as '''geometric''', '''spatial''', or '''Euclidean''' vectors.\n\nBeing an arrow, a Euclidean vector possesses a definite ''initial point'' and ''terminal point''. A vector with fixed initial and terminal point is called a '''bound vector'''.<ref>Formerly known as ''located vector''. See {{harvnb|Lang|1986|page=9}}.</ref> When only the magnitude and direction of the vector matter, then the particular initial point is of no importance, and the vector is called a '''free vector'''. Thus two arrows <math>\\overrightarrow{AB}</math> and <math>\\overrightarrow{A'B'}</math> in space represent the same free vector if they have the same magnitude and direction: that is, they are [[equipollence (geometry)|equipollent]] if the quadrilateral ''ABB′A′'' is a [[parallelogram]]. If the Euclidean space is equipped with a choice of [[origin (mathematics)|origin]], then a free vector is equivalent to the bound vector of the same magnitude and direction whose initial point is the origin.\n\nThe term ''vector'' also has generalizations to higher dimensions and to more formal approaches with much wider applications.\n\n===Examples in one dimension===\nSince the physicist's concept of [[force (physics)|force]] has a direction and a magnitude, it may be seen as a vector. As an example, consider a rightward force ''F'' of 15 [[Newton (unit)|newtons]]. If the positive [[axis (mathematics)|axis]] is also directed rightward, then ''F'' is represented by the vector 15 N, and if positive points leftward, then the vector for ''F'' is −15 N. In either case, the magnitude of the vector is 15 N. Likewise, the vector representation of a displacement Δ''s'' of 4 [[meter (unit)|meters]] would be 4 m or −4 m, depending on its direction, and its magnitude would be 4 m regardless.\n\n===In physics and engineering===\nVectors are fundamental in the physical sciences. They can be used to represent any quantity that has magnitude, has direction, and which adheres to the rules of vector addition. An example is [[velocity]], the magnitude of which is [[speed]]. For example, the velocity ''5 meters per second upward'' could be represented by the vector (0, 5) (in 2 dimensions with the positive ''y''-axis as 'up'). Another quantity represented by a vector is [[force]], since it has a magnitude and direction and follows the rules of vector addition. Vectors also describe many other physical quantities, such as linear displacement, [[displacement (vector)|displacement]], linear acceleration, [[angular acceleration]], [[linear momentum]], and [[angular momentum]]. Other physical vectors, such as the [[electric field|electric]] and [[magnetic field]], are represented as a system of vectors at each point of a physical space; that is, a [[vector field]]. Examples of quantities that have magnitude and direction but fail to follow the rules of vector addition are angular displacement and electric current. Consequently, these are not vectors.\n\n===In Cartesian space===\nIn the [[Cartesian coordinate system]], a bound vector can be represented by identifying the coordinates of its initial and terminal point. For instance, the points ''A'' = (1, 0, 0) and ''B'' = (0, 1, 0) in space determine the bound vector <math>\\overrightarrow{AB}</math> pointing from the point ''x'' = 1 on the ''x''-axis to the point ''y'' = 1 on the ''y''-axis.\n\nIn Cartesian coordinates a free vector may be thought of in terms of a corresponding bound vector, in this sense, whose initial point has the coordinates of the origin ''O'' = (0, 0, 0). It is then determined by the coordinates of that bound vector's terminal point. Thus the free vector represented by (1, 0, 0) is a vector of unit length pointing along the direction of the positive ''x''-axis.\n\nThis coordinate representation of free vectors allows their algebraic features to be expressed in a convenient numerical fashion. For example, the sum of the two (free) vectors (1, 2, 3) and (−2, 0, 4) is the (free) vector\n:{{nowrap begin}}(1, 2, 3) + (−2, 0, 4) = (1 − 2, 2 + 0, 3 + 4) = (−1, 2, 7).{{nowrap end}}\n\n===Euclidean and affine vectors===\nIn the geometrical and physical settings, sometimes it is possible to associate, in a natural way, a ''length'' or magnitude and a direction to vectors. In addition, the notion of direction is strictly associated with the notion of an ''angle'' between two vectors. If the [[dot product]] of two vectors is defined—a scalar-valued product of two vectors—then it is also possible to define a length; the dot product gives a convenient algebraic characterization of both angle (a function of the dot product between any two non-zero vectors) and length (the square root of the dot product of a vector by itself). In three dimensions, it is further possible to define the [[cross product]], which supplies an algebraic characterization of the [[area]] and [[orientation (geometry)|orientation]] in space of the [[parallelogram]] defined by two vectors (used as sides of the parallelogram). In any dimension (and, in particular, higher dimensions), it's possible to define the [[exterior product]], which (among other things) supplies an algebraic characterization of the area and orientation in space of the ''n''-dimensional [[parallelepiped#Parallelotope|parallelotope]] defined by ''n'' vectors.\n\nHowever, it is not always possible or desirable to define the length of a vector in a natural way. This more general type of spatial vector is the subject of [[vector space]]s (for free vectors) and [[affine space]]s (for bound vectors, as each represented by an ordered pair of \"points\"). An important example is [[Minkowski space]] that is important to our understanding of [[special relativity]], where there is a generalization of length that permits non-zero vectors to have zero length. Other physical examples come from [[thermodynamics]], where many of the quantities of interest can be considered vectors in a space with no notion of length or angle.<ref name=\"thermo-forms\"\n>[http://www.av8n.com/physics/thermo-forms.htm Thermodynamics and Differential Forms]</ref>\n\n===Generalizations===\nIn physics, as well as mathematics, a vector is often identified with a [[tuple]] of components, or list of numbers, that act as scalar coefficients for a set of [[basis vector]]s. When the basis is transformed, for example by rotation or stretching, then the components of any vector in terms of that basis also transform in an opposite sense. The vector itself has not changed, but the basis has, so the components of the vector must change to compensate. The vector is called ''covariant'' or ''contravariant'' depending on how the transformation of the vector's components is related to the transformation of the basis. In general, contravariant vectors are \"regular vectors\" with units of distance (such as a displacement) or distance times some other unit (such as velocity or acceleration); covariant vectors, on the other hand, have units of one-over-distance such as [[gradient]]. If you change units (a special case of a change of basis) from meters to millimeters, a scale factor of 1/1000, a displacement of 1&nbsp;m becomes 1000&nbsp;mm—a contravariant change in numerical value. In contrast, a gradient of 1&nbsp;[[Kelvin|K]]/m becomes 0.001&nbsp;K/mm—a covariant change in value. See [[covariance and contravariance of vectors]]. [[Tensor]]s are another type of quantity that behave in this way; a vector is one type of [[tensor]].\n\nIn pure [[mathematics]], a vector is any element of a [[vector space]] over some [[field (mathematics)|field]] and is often represented as a [[coordinate vector]]. The vectors described in this article are a very special case of this general definition because they are contravariant with respect to the ambient space. Contravariance captures the physical intuition behind the idea that a vector has \"magnitude and direction\".\n\n==Representations==\n[[Image:vector from A to B.svg|right|200px|Vector arrow pointing from ''A'' to ''B'']]\n\nVectors are usually denoted in [[lowercase]] boldface, as '''a''' or lowercase italic boldface, as '''''a'''''. ([[Uppercase]] letters are typically used to represent [[matrix (mathematics)|matrices]].) Other conventions include <math>\\vec{a}</math> or <u>''a''</u>, especially in handwriting. Alternatively, some use a [[tilde]] (~) or a wavy underline drawn beneath the symbol, e.g. <math>\\underset{^\\sim}a</math>, which is a convention for indicating boldface type. If the vector represents a directed [[distance]] or [[displacement (vector)|displacement]] from a point ''A'' to a point ''B'' (see figure), it can also be denoted as <math>\\stackrel{\\longrightarrow}{AB}</math> or <u>''AB''</u>. Especially in literature in [[German language|German]] it was common to represent vectors with small [[fraktur]] letters as <math>\\mathfrak{a}</math>.\n\nVectors are usually shown in graphs or other diagrams as arrows (directed [[line segment]]s), as illustrated in the figure. Here the point ''A'' is called the ''origin'', ''tail'', ''base'', or ''initial point''; point ''B'' is called the ''head'', ''tip'', ''endpoint'', ''terminal point'' or ''final point''. The length of the arrow is proportional to the vector's [[magnitude (mathematics)|magnitude]], while the direction in which the arrow points indicates the vector's direction.\n\n[[Image:Notation for vectors in or out of a plane.svg|right|200px]]\nOn a two-dimensional diagram, sometimes a vector [[perpendicular]] to the [[plane (mathematics)|plane]] of the diagram is desired. These vectors are commonly shown as small circles. A circle with a dot at its centre (Unicode U+2299 ⊙) indicates a vector pointing out of the front of the diagram, toward the viewer. A circle with a cross inscribed in it (Unicode U+2297 ⊗) indicates a vector pointing into and behind the diagram. These can be thought of as viewing the tip of an [[arrow (weapon)|arrow]] head on and viewing the flights of an arrow from the back.\n\n[[Image:Position vector.svg|thumb|right|A vector in the Cartesian plane, showing the position of a point ''A'' with coordinates (2, 3).]]\n[[Image:3D Vector.svg|300px|right]]\nIn order to calculate with vectors, the graphical representation may be too cumbersome. Vectors in an ''n''-dimensional Euclidean space can be represented as [[coordinate vector]]s in a [[Cartesian coordinate system]]. The endpoint of a vector can be identified with an ordered list of ''n'' real numbers (''n''-[[tuple]]). These numbers are the [[Cartesian coordinate|coordinates]] of the endpoint of the vector, with respect to a given [[Cartesian coordinate system]], and are typically called the '''[[scalar component]]s''' (or '''scalar projections''') of the vector on the axes of the coordinate system.\n\nAs an example in two dimensions (see figure), the vector from the origin ''O'' = (0, 0) to the point ''A'' = (2, 3) is simply written as\n:<math>\\mathbf{a} = (2,3).</math>\n\nThe notion that the tail of the vector coincides with the origin is implicit and easily understood. Thus, the more explicit notation <math>\\overrightarrow{OA}</math> is usually not deemed necessary and very rarely used.\n\nIn ''three dimensional'' Euclidean space (or {{math|'''R'''<sup>3</sup>}}), vectors are identified with triples of scalar components:\n:<math>\\mathbf{a} = (a_1, a_2, a_3).</math>\n:also written\n:<math>\\mathbf{a} = (a_x, a_y, a_z).</math>\n\nThis can be generalised to ''n-dimensional'' Euclidean space (or {{math|'''R'''<sup>''n''</sup>}}).\n:<math>\\mathbf{a} = (a_1, a_2, a_3, \\cdots, a_{n-1}, a_n).</math>\n\nThese numbers are often arranged into a [[column vector]] or [[row vector]], particularly when dealing with [[matrix (mathematics)|matrices]], as follows:\n:<math>\\mathbf{a} =\n  \\begin{bmatrix}\n    a_1\\\\\n    a_2\\\\\n    a_3\\\\\n  \\end{bmatrix} =\n  [ a_1\\ a_2\\ a_3 ].\n</math>\n\nAnother way to represent a vector in ''n''-dimensions is to introduce the [[standard basis]] vectors. For instance, in three dimensions, there are three of them:\n:<math>{\\mathbf e}_1 = (1,0,0),\\ {\\mathbf e}_2 = (0,1,0),\\ {\\mathbf e}_3 = (0,0,1).</math>\nThese have the intuitive interpretation as vectors of unit length pointing up the ''x''-, ''y''-, and ''z''-axis of a [[Cartesian coordinate system]], respectively. In terms of these, any vector '''a''' in {{math|'''R'''<sup>3</sup>}} can be expressed in the form:\n:<math>\\mathbf{a} = (a_1,a_2,a_3) = a_1(1,0,0) + a_2(0,1,0) + a_3(0,0,1), \\ </math>\n\nor\n:<math>\\mathbf{a} = \\mathbf{a}_1 + \\mathbf{a}_2 + \\mathbf{a}_3 = a_1{\\mathbf e}_1 + a_2{\\mathbf e}_2 + a_3{\\mathbf e}_3,</math>\n\nwhere '''a'''<sub>1</sub>, '''a'''<sub>2</sub>, '''a'''<sub>3</sub> are called the '''[[vector component]]s''' (or '''vector projections''') of '''a''' on the basis vectors or, equivalently, on the corresponding Cartesian axes ''x'', ''y'', and ''z'' (see figure), while ''a''<sub>1</sub>, ''a''<sub>2</sub>, ''a''<sub>3</sub> are the respective [[scalar component]]s (or scalar projections).\n\nIn introductory physics textbooks, the standard basis vectors are often instead denoted <math>\\mathbf{i},\\mathbf{j},\\mathbf{k}</math> (or <math>\\mathbf{\\hat{x}}, \\mathbf{\\hat{y}}, \\mathbf{\\hat{z}}</math>, in which the [[hat symbol]] '''^''' typically denotes [[unit vector]]s). In this case, the scalar and vector components are denoted respectively ''a<sub>x</sub>'', ''a<sub>y</sub>'', ''a<sub>z</sub>'', and '''a'''<sub>''x''</sub>, '''a'''<sub>''y''</sub>, '''a'''<sub>''z''</sub> (note the difference in boldface). Thus,\n\n:<math>\\mathbf{a} = \\mathbf{a}_x + \\mathbf{a}_y + \\mathbf{a}_z = a_x{\\mathbf i} + a_y{\\mathbf j} + a_z{\\mathbf k}.</math>\n\nThe notation '''e'''<sub>''i''</sub> is compatible with the [[index notation]] and the [[summation convention]] commonly used in higher level mathematics, physics, and engineering.\n\n=== {{anchor|Vector component|Decomposition}} Decomposition or resolution===\n{{details|Basis (linear algebra)}}\nAs explained [[Euclidean vector#Representations|above]] a vector is often described by a set of vector components that [[#Addition and subtraction|add up]] to form the given vector. Typically, these components are the [[Vector projection|projections]] of the vector on a set of mutually perpendicular reference axes (basis vectors). The vector is said to be ''decomposed'' or ''resolved with respect to'' that set.\n\n[[Image:Surface normal tangent.svg|right|thumb|Illustration of tangential and normal components of a vector to a surface.]]\n\nThe decomposition or resolution<ref>[[Josiah Willard Gibbs|Gibbs, J.W.]] (1901). ''Vector Analysis: A Text-book for the Use of Students of Mathematics and Physics, Founded upon the Lectures of J. Willard Gibbs'', by E.B. Wilson, Chares Scribner's Sons, New York, p. 15: \"Any vector {{math|'''r'''}} coplanar with two non-collinear vectors {{math|'''a'''}} and {{math|'''b'''}} may be resolved into two components parallel to {{math|'''a'''}} and {{math|'''b'''}} respectively. This resolution may be accomplished by constructing the parallelogram ...\"</ref> of a vector into components is not unique, because it depends on the choice of the axes on which the vector is projected.\n\nMoreover, the use of Cartesian unit vectors such as <math>\\mathbf{\\hat{x}}, \\mathbf{\\hat{y}}, \\mathbf{\\hat{z}}</math> as a [[Basis (linear algebra)|basis]] in which to represent a vector is not mandated. Vectors can also be expressed in terms of an arbitrary basis, including the unit vectors of a [[cylindrical coordinate system]] (<math>\\boldsymbol{\\hat{\\rho}}, \\boldsymbol{\\hat{\\phi}}, \\mathbf{\\hat{z}}</math>) or [[spherical coordinate system]] (<math>\\mathbf{\\hat{r}}, \\boldsymbol{\\hat{\\theta}}, \\boldsymbol{\\hat{\\phi}}</math>). The latter two choices are more convenient for solving problems which possess cylindrical or spherical symmetry respectively.\n\nThe choice of a basis does not affect the properties of a vector or its behaviour under transformations.\n\nA vector can also be broken up with respect to \"non-fixed\" basis vectors that change their [[orientation (geometry)|orientation]] as a function of time or space. For example, a vector in three-dimensional space can be decomposed with respect to two axes, respectively ''normal'', and ''tangent'' to a surface (see figure). Moreover, the ''radial'' and ''[[tangential component]]s'' of a vector relate to the ''[[radius]] of [[rotation]]'' of an object. The former is [[Parallel (geometry)|parallel]] to the radius and the latter is [[Perpendicular|orthogonal]] to it.<ref>[http://www.physics.uoguelph.ca/tutorials/torque/Q.torque.intro.angacc.html U. Guelph Physics Dept., \"Torque and Angular Acceleration\"]</ref>\n\nIn these cases, each of the components may be in turn decomposed with respect to a fixed coordinate system or basis set (e.g., a ''global'' coordinate system, or [[inertial reference frame]]).\n\n==Basic properties==\nThe following section uses the [[Cartesian coordinate system]] with basis vectors\n:<math>{\\mathbf e}_1 = (1,0,0),\\ {\\mathbf e}_2 = (0,1,0),\\ {\\mathbf e}_3 = (0,0,1)</math>\nand assumes that all vectors have the origin as a common base point. A vector '''a''' will be written as\n:<math>{\\mathbf a} = a_1{\\mathbf e}_1 + a_2{\\mathbf e}_2 + a_3{\\mathbf e}_3.</math>\n\n===Equality===\nTwo vectors are said to be equal if they have the same magnitude and direction. Equivalently they will be equal if their coordinates are equal. So two vectors\n:<math>{\\mathbf a} = a_1{\\mathbf e}_1 + a_2{\\mathbf e}_2 + a_3{\\mathbf e}_3</math>\nand\n:<math>{\\mathbf b} = b_1{\\mathbf e}_1 + b_2{\\mathbf e}_2 + b_3{\\mathbf e}_3</math>\nare equal if\n:<math>a_1 = b_1,\\quad a_2=b_2,\\quad a_3=b_3.\\,</math>\n\n===Opposite, parallel, and antiparallel vectors===\nTwo vectors are opposite if they have the same magnitude but opposite direction.  So two vectors\n:<math>{\\mathbf a} = a_1{\\mathbf e}_1 + a_2{\\mathbf e}_2 + a_3{\\mathbf e}_3</math>\nand\n:<math>{\\mathbf b} = b_1{\\mathbf e}_1 + b_2{\\mathbf e}_2 + b_3{\\mathbf e}_3</math>\nare opposite if\n:<math>a_1 = -b_1,\\quad a_2=-b_2,\\quad a_3=-b_3.\\,</math>\nTwo vectors are parallel if they have the same direction but not necessarily the same magnitude, or antiparallel if they have opposite direction but not necessarily the same magnitude.\n\n===Addition and subtraction===\n{{details|Vector space}}\nAssume now that '''a''' and '''b''' are not necessarily equal vectors, but that they may have different magnitudes and directions. The sum of '''a''' and '''b''' is\n:<math>\\mathbf{a}+\\mathbf{b}\n=(a_1+b_1)\\mathbf{e}_1\n+(a_2+b_2)\\mathbf{e}_2\n+(a_3+b_3)\\mathbf{e}_3.</math>\n\nThe addition may be represented graphically by placing the tail of the arrow '''b''' at the head of the arrow '''a''', and then drawing an arrow from the tail of '''a''' to the head of '''b'''. The new arrow drawn represents the vector '''a''' + '''b''', as illustrated below:\n\n[[Image:Vector addition.svg|250px|center|The addition of two vectors '''a''' and '''b''']]\n\nThis addition method is sometimes called the ''parallelogram rule'' because '''a''' and '''b''' form the sides of a [[parallelogram]] and '''a''' + '''b''' is one of the diagonals. If '''a''' and '''b''' are bound vectors that have the same base point, this point will also be the base point of '''a''' + '''b'''. One can check geometrically that '''a''' + '''b''' = '''b''' + '''a''' and ('''a''' + '''b''') + '''c''' = '''a''' + ('''b''' + '''c''').\n\nThe difference of '''a''' and '''b''' is\n\n:<math>\\mathbf{a}-\\mathbf{b}\n=(a_1-b_1)\\mathbf{e}_1\n+(a_2-b_2)\\mathbf{e}_2\n+(a_3-b_3)\\mathbf{e}_3.</math>\n\nSubtraction of two vectors can be geometrically illustrated as follows: to subtract '''b''' from '''a''', place the tails of '''a''' and '''b''' at the same point, and then draw an arrow from the head of '''b''' to the head of '''a'''. This new arrow represents the vector '''(-b)''' + '''a''', with '''(-b)''' being the opposite of  '''b''',  see drawing. And '''(-b)''' + '''a''' = '''a''' − '''b'''. \n\n[[Image:Vector subtraction.svg|125px|center|The subtraction of two vectors '''a''' and '''b''']]\n\n===Scalar multiplication===\n{{main article|Scalar multiplication}}\n\n[[Image:Scalar multiplication by r=3.svg|250px|thumb|right|Scalar multiplication of a vector by a factor of 3 stretches the vector out.]]\nA vector may also be multiplied, or re-''scaled'', by a [[real number]] ''r''. In the context of [[vector analysis|conventional vector algebra]], these real numbers are often called '''scalars''' (from ''scale'') to distinguish them from vectors. The operation of multiplying a vector by a scalar is called ''scalar multiplication''. The resulting vector is\n\n:<math>r\\mathbf{a}=(ra_1)\\mathbf{e}_1\n+(ra_2)\\mathbf{e}_2\n+(ra_3)\\mathbf{e}_3.</math>\n\nIntuitively, multiplying by a scalar ''r'' stretches a vector out by a factor of ''r''. Geometrically, this can be visualized (at least in the case when ''r'' is an integer) as placing ''r'' copies of the vector in a line where the endpoint of one vector is the initial point of the next vector.\n\nIf ''r'' is negative, then the vector changes direction: it flips around by an angle of 180°. Two examples (''r'' = −1 and ''r'' = 2) are given below:\n\n[[Image:Scalar multiplication of vectors2.svg|250px|thumb|left|The scalar multiplications −'''a''' and 2'''a''' of a vector '''a''']]\n\nScalar multiplication is [[Distributivity|distributive]] over vector addition in the following sense: ''r''('''a''' + '''b''') = ''r'''''a''' + ''r'''''b''' for all vectors '''a''' and '''b''' and all scalars ''r''. One can also show that '''a''' − '''b''' = '''a''' + (−1)'''b'''.\n<!--\nThe set of all geometrical vectors, together with the operations of vector addition and scalar multiplication, satisfies all the axioms of a [[vector space]]. Similarly, the set of all bound vectors with a common base point forms a vector space. This is where the term \"vector space\" originated.\n-->\n{{clear}}\n\n===Length===<!-- This section is linked from [[Law of cosines]] -->\nThe ''[[length]]'' or ''[[Magnitude (mathematics)|magnitude]]'' or ''[[Norm (mathematics)|norm]]'' of the vector '''a''' is denoted by ‖'''a'''‖ or, less commonly, |'''a'''|, which is not to be confused with the [[absolute value]] (a scalar \"norm\").\n\nThe length of the vector '''a''' can be computed with the [[Euclidean norm]]\n\n:<math>\\left\\|\\mathbf{a}\\right\\|=\\sqrt{a_1^2+a_2^2+a_3^2}</math>\n\nwhich is a consequence of the [[Pythagorean theorem]] since the basis vectors '''e'''<sub>1</sub>, '''e'''<sub>2</sub>, '''e'''<sub>3</sub> are orthogonal unit vectors.\n\nThis happens to be equal to the square root of the [[dot product]], discussed below, of the vector with itself:\n\n:<math>\\left\\|\\mathbf{a}\\right\\|=\\sqrt{\\mathbf{a}\\cdot\\mathbf{a}}.</math>\n\n;Unit vector\n[[Image:Vector normalization.svg|thumb|right|The normalization of a vector '''a''' into a unit vector '''â''']]\n{{main article|Unit vector}}\n\nA ''unit vector'' is any vector with a length of one; normally unit vectors are used simply to indicate direction. A vector of arbitrary length can be divided by its length to create a unit vector. This is known as ''normalizing'' a vector. A unit vector is often indicated with a hat as in '''â'''.\n\nTo normalize a vector {{nowrap|1='''a''' = (''a''<sub>1</sub>, ''a''<sub>2</sub>, ''a''<sub>3</sub>)}}, scale the vector by the reciprocal of its length ‖'''a'''‖. That is:\n\n:<math>\\mathbf{\\hat{a}} = \\frac{\\mathbf{a}}{\\left\\|\\mathbf{a}\\right\\|} = \\frac{a_1}{\\left\\|\\mathbf{a}\\right\\|}\\mathbf{e}_1 + \\frac{a_2}{\\left\\|\\mathbf{a}\\right\\|}\\mathbf{e}_2 + \\frac{a_3}{\\left\\|\\mathbf{a}\\right\\|}\\mathbf{e}_3</math>\n\n;Zero vector\n{{main article|Zero vector}}\n\nThe ''zero vector'' is the vector with length zero. Written out in coordinates, the vector is {{nowrap|(0, 0, 0)}}, and it is commonly denoted <math>\\vec{0}</math>, '''0''', or simply 0. Unlike any other vector, it has an arbitrary or indeterminate direction, and cannot be normalized (that is, there is no unit vector that is a multiple of the zero vector). The sum of the zero vector with any vector '''a''' is '''a''' (that is, {{nowrap|1='''0''' + '''a''' = '''a'''}}).\n\n===Dot product===\n{{main article|Dot product}}\n\nThe ''dot product'' of two vectors '''a''' and '''b''' (sometimes called the ''[[inner product space|inner product]]'', or, since its result is a scalar, the ''scalar product'') is denoted by '''a'''&nbsp;∙&nbsp;'''b''' and is defined as:\n\n:<math>\\mathbf{a}\\cdot\\mathbf{b}\n=\\left\\|\\mathbf{a}\\right\\|\\left\\|\\mathbf{b}\\right\\|\\cos\\theta</math>\n\nwhere ''θ'' is the measure of the [[angle]] between '''a''' and '''b''' (see [[trigonometric function]] for an explanation of cosine). Geometrically, this means that '''a''' and '''b''' are drawn with a common start point and then the length of '''a''' is multiplied with the length of the component of '''b''' that points in the same direction as '''a'''.\n\nThe dot product can also be defined as the sum of the products of the components of each vector as\n\n:<math>\\mathbf{a} \\cdot \\mathbf{b} = a_1 b_1 + a_2 b_2 + a_3 b_3.</math>\n\n===Cross product===\n{{main article|Cross product}}\n\nThe ''cross product'' (also called the ''vector product'' or ''outer product'') is only meaningful in three or [[Seven-dimensional cross product|seven]] dimensions. The cross product differs from the dot product primarily in that the result of the cross product of two vectors is a vector. The cross product, denoted '''a'''&nbsp;×&nbsp;'''b''', is a vector perpendicular to both '''a''' and '''b''' and is defined as\n\n:<math>\\mathbf{a}\\times\\mathbf{b}\n=\\left\\|\\mathbf{a}\\right\\|\\left\\|\\mathbf{b}\\right\\|\\sin(\\theta)\\,\\mathbf{n}</math>\n\nwhere ''θ'' is the measure of the angle between '''a''' and '''b''', and '''n''' is a unit vector [[perpendicular]] to both '''a''' and '''b''' which completes a [[Right-hand rule|right-handed]] system. The right-handedness constraint is necessary because there exist ''two'' unit vectors that are perpendicular to both '''a''' and '''b''', namely, '''n''' and (–'''n''').\n[[Image:Cross product vector.svg|thumb|right|An illustration of the cross product]]\n\nThe cross product '''a'''&nbsp;×&nbsp;'''b''' is defined so that '''a''', '''b''', and '''a'''&nbsp;×&nbsp;'''b''' also becomes a right-handed system (but note that '''a''' and '''b''' are not necessarily [[orthogonal]]). This is the [[right-hand rule]].\n\nThe length of '''a'''&nbsp;×&nbsp;'''b''' can be interpreted as the area of the parallelogram having '''a''' and '''b''' as sides.\n\nThe cross product can be written as\n:<math>{\\mathbf a}\\times{\\mathbf b} = (a_2 b_3 - a_3 b_2) {\\mathbf e}_1 + (a_3 b_1 - a_1 b_3) {\\mathbf e}_2 + (a_1 b_2 - a_2 b_1) {\\mathbf e}_3.</math>\n\nFor arbitrary choices of spatial orientation (that is, allowing for left-handed as well as right-handed coordinate systems) the cross product of two vectors is a [[pseudovector]] instead of a vector (see below).\n\n===Scalar triple product===\n{{main article|Triple product#Scalar triple product|l1=Scalar triple product}}\nThe ''scalar triple product'' (also called the ''box product'' or ''mixed triple product'') is not really a new operator, but a way of applying the other two multiplication operators to three vectors. The scalar triple product is sometimes denoted by ('''a''' '''b''' '''c''') and defined as:\n\n:<math>(\\mathbf{a}\\ \\mathbf{b}\\ \\mathbf{c})\n=\\mathbf{a}\\cdot(\\mathbf{b}\\times\\mathbf{c}).</math>\n\nIt has three primary uses. First, the absolute value of the box product is the volume of the [[parallelepiped]] which has edges that are defined by the three vectors. Second, the scalar triple product is zero if and only if the three vectors are [[linear independence|linearly dependent]], which can be easily proved by considering that in order for the three vectors to not make a volume, they must all lie in the same plane. Third, the box product is positive if and only if the three vectors '''a''', '''b''' and '''c''' are right-handed.\n\nIn components (''with respect to a right-handed orthonormal basis''), if the three vectors are thought of as rows (or columns, but in the same order), the scalar triple product is simply the [[determinant]] of the 3-by-3 [[Matrix (mathematics)|matrix]] having the three vectors as rows\n:<math>(\\mathbf{a}\\ \\mathbf{b}\\ \\mathbf{c})=\\left|\\begin{pmatrix}\n a_1 & a_2 & a_3 \\\\\n b_1 & b_2 & b_3 \\\\\n c_1 & c_2 & c_3 \\\\\n\\end{pmatrix}\\right|</math>\n\nThe scalar triple product is linear in all three entries and anti-symmetric in the following sense:\n:<math>\n(\\mathbf{a}\\ \\mathbf{b}\\ \\mathbf{c}) = (\\mathbf{c}\\ \\mathbf{a}\\ \\mathbf{b}) = (\\mathbf{b}\\ \\mathbf{c}\\ \\mathbf{a})=\n -(\\mathbf{a}\\ \\mathbf{c}\\ \\mathbf{b}) = -(\\mathbf{b}\\ \\mathbf{a}\\ \\mathbf{c}) = -(\\mathbf{c}\\ \\mathbf{b}\\ \\mathbf{a}).</math>\n\n===Conversion between multiple Cartesian bases===\nAll examples thus far have dealt with vectors expressed in terms of the same basis, namely, the ''e'' basis {'''e'''<sub>1</sub>, '''e'''<sub>2</sub>, '''e'''<sub>3</sub>}. However, a vector can be expressed in terms of any number of different bases that are not necessarily aligned with each other, and still remain the same vector.  In the ''e'' basis, a vector '''a''' is expressed, by definition, as\n\n:<math>\\mathbf{a} = p\\mathbf{e}_1 + q\\mathbf{e}_2 + r\\mathbf{e}_3</math>.\n\nThe scalar components in the ''e'' basis are, by definition,\n\n:<math>p = \\mathbf{a}\\cdot\\mathbf{e}_1</math>, \n:<math>q = \\mathbf{a}\\cdot\\mathbf{e}_2</math>, \n:<math>r = \\mathbf{a}\\cdot\\mathbf{e}_3</math>.\n\nIn another orthonormal basis ''n'' = {'''n'''<sub>1</sub>, '''n'''<sub>2</sub>, '''n'''<sub>3</sub>} that is not necessarily aligned with ''e'', the vector '''a''' is expressed as\n\n:<math>\\mathbf{a} = u\\mathbf{n}_1 + v\\mathbf{n}_2 + w\\mathbf{n}_3</math>\n\nand the scalar components in the ''n'' basis are, by definition,\n\n:<math>u = \\mathbf{a}\\cdot\\mathbf{n}_1</math>,\n:<math>v = \\mathbf{a}\\cdot\\mathbf{n}_2</math>,\n:<math>w = \\mathbf{a}\\cdot\\mathbf{n}_3</math>.\n\nThe values of ''p'', ''q'', ''r'', and ''u'', ''v'', ''w'' relate to the unit vectors in such a way that the resulting vector sum is exactly the same physical vector '''a''' in both cases.  It is common to encounter vectors known in terms of different bases (for example, one basis fixed to the Earth and a second basis fixed to a moving vehicle).  In such a case it is necessary to develop a method to convert between bases so the basic vector operations such as addition and subtraction can be performed.  One way to express ''u'', ''v'', ''w'' in terms of ''p'', ''q'', ''r'' is to use column matrices along with a [[direction cosine matrix]] containing the information that relates the two bases.  Such an expression can be formed by substitution of the above equations to form\n\n:<math>u = (p\\mathbf{e}_1 + q\\mathbf{e}_2 + r\\mathbf{e}_3)\\cdot\\mathbf{n}_1</math>,\n:<math>v = (p\\mathbf{e}_1 + q\\mathbf{e}_2 + r\\mathbf{e}_3)\\cdot\\mathbf{n}_2</math>,\n:<math>w = (p\\mathbf{e}_1 + q\\mathbf{e}_2 + r\\mathbf{e}_3)\\cdot\\mathbf{n}_3</math>.\n\nDistributing the dot-multiplication gives\n\n:<math>u = p\\mathbf{e}_1\\cdot\\mathbf{n}_1 + q\\mathbf{e}_2\\cdot\\mathbf{n}_1 + r\\mathbf{e}_3\\cdot\\mathbf{n}_1</math>,\n:<math>v = p\\mathbf{e}_1\\cdot\\mathbf{n}_2 + q\\mathbf{e}_2\\cdot\\mathbf{n}_2 + r\\mathbf{e}_3\\cdot\\mathbf{n}_2</math>,\n:<math>w = p\\mathbf{e}_1\\cdot\\mathbf{n}_3 + q\\mathbf{e}_2\\cdot\\mathbf{n}_3 + r\\mathbf{e}_3\\cdot\\mathbf{n}_3</math>.\n\nReplacing each dot product with a unique scalar gives\n\n:<math>u = c_{11}p + c_{12}q + c_{13}r</math>,\n:<math>v = c_{21}p + c_{22}q + c_{23}r</math>,\n:<math>w = c_{31}p + c_{32}q + c_{33}r</math>,\n\nand these equations can be expressed as the single matrix equation\n\n:<math>\\begin{bmatrix} u \\\\ v \\\\ w \\\\ \\end{bmatrix} = \\begin{bmatrix} c_{11} & c_{12} & c_{13} \\\\ c_{21} & c_{22} & c_{23} \\\\ c_{31} & c_{32} & c_{33} \\end{bmatrix} \\begin{bmatrix} p \\\\ q \\\\ r \\end{bmatrix} </math>.\n\nThis matrix equation relates the scalar components of '''a''' in the ''n'' basis (''u'',''v'', and ''w'') with those in the ''e'' basis (''p'', ''q'', and ''r'').  Each matrix element ''c''<sub>''jk''</sub> is the [[Direction cosine#Cartesian coordinates|direction cosine]] relating '''n'''<sub>''j''</sub> to '''e'''<sub>''k''</sub>.<ref name=\"dynon16\">{{harvnb|Kane|Levinson|1996|pp=20–22}}</ref> The term ''direction cosine'' refers to the [[cosine]] of the angle between two unit vectors, which is also equal to their [[#Dot product|dot product]].<ref name=\"dynon16\"/> Therefore,\n\n:<math>c_{11} = \\mathbf{n}_1\\cdot\\mathbf{e}_1</math>\n:<math>c_{12} = \\mathbf{n}_1\\cdot\\mathbf{e}_2</math>\n:<math>c_{13} = \\mathbf{n}_1\\cdot\\mathbf{e}_3</math>\n:<math>c_{21} = \\mathbf{n}_2\\cdot\\mathbf{e}_1</math>\n:<math>c_{22} = \\mathbf{n}_2\\cdot\\mathbf{e}_2</math>\n:<math>c_{23} = \\mathbf{n}_2\\cdot\\mathbf{e}_3</math>\n:<math>c_{31} = \\mathbf{n}_3\\cdot\\mathbf{e}_1</math>\n:<math>c_{32} = \\mathbf{n}_3\\cdot\\mathbf{e}_2</math>\n:<math>c_{33} = \\mathbf{n}_3\\cdot\\mathbf{e}_3</math>\n\nBy referring collectively to '''e'''<sub>1</sub>, '''e'''<sub>2</sub>, '''e'''<sub>3</sub> as the ''e'' basis and to '''n'''<sub>1</sub>, '''n'''<sub>2</sub>, '''n'''<sub>3</sub> as the ''n'' basis, the matrix containing all the ''c''<sub>''jk''</sub> is known as the \"'''[[transformation matrix]]''' from ''e'' to ''n''\", or the \"'''[[rotation matrix]]''' from ''e'' to ''n''\" (because it can be imagined as the \"rotation\" of a vector from one basis to another), or the \"'''direction cosine matrix''' from ''e'' to ''n''\"<ref name=\"dynon16\"/> (because it contains direction cosines).  The properties of a [[rotation matrix]] are such that its [[matrix inverse|inverse]] is equal to its [[matrix transpose|transpose]]. This means that the \"rotation matrix from ''e'' to ''n''\" is the transpose of \"rotation matrix from ''n'' to ''e''\".\n\nThe properties of a direction cosine matrix, C are<ref>{{Cite book|url=https://www.worldcat.org/oclc/652389481|title=Applied mathematics in integrated navigation systems|last=M.|first=Rogers, Robert|date=2007|publisher=American Institute of Aeronautics and Astronautics|isbn=9781563479274|edition=3rd|location=Reston, Va.|oclc=652389481}}</ref>:\n* the determinant is unity, |C| = 1\n* the inverse is equal to the transpose,\n* the rows and columns are orthogonal unit vectors, therefore their dot products are zero.\n\nThe advantage of this method is that a direction cosine matrix can usually be obtained independently by using [[Euler angles]] or a [[quaternion]] to relate the two vector bases, so the basis conversions can be performed directly, without having to work out all the dot products described above.\n\nBy applying several matrix multiplications in succession, any vector can be expressed in any basis so long as the set of direction cosines is known relating the successive bases.<ref name=\"dynon16\"/>\n\n===Other dimensions===\nWith the exception of the cross and triple products, the above formulae generalise to two dimensions and higher dimensions. For example, addition generalises to two dimensions as\n:<math>(a_1{\\mathbf e}_1 + a_2{\\mathbf e}_2)+(b_1{\\mathbf e}_1 + b_2{\\mathbf e}_2) = (a_1+b_1){\\mathbf e}_1 + (a_2+b_2){\\mathbf e}_2</math>\nand in four dimensions as\n:<math>\\begin{align}(a_1{\\mathbf e}_1 + a_2{\\mathbf e}_2 + a_3{\\mathbf e}_3 + a_4{\\mathbf e}_4) &+ (b_1{\\mathbf e}_1 + b_2{\\mathbf e}_2 + b_3{\\mathbf e}_3 + b_4{\\mathbf e}_4) =\\\\\n(a_1+b_1){\\mathbf e}_1 + (a_2+b_2){\\mathbf e}_2 &+ (a_3+b_3){\\mathbf e}_3 + (a_4+b_4){\\mathbf e}_4.\\end{align}</math>\n\nThe cross product does not readily generalise to other dimensions, though the closely related [[Exterior algebra#Areas in the plane|exterior product]] does, whose result is a [[bivector]]. In two dimensions this is simply a [[pseudoscalar]]\n:<math>(a_1{\\mathbf e}_1 + a_2{\\mathbf e}_2)\\wedge(b_1{\\mathbf e}_1 + b_2{\\mathbf e}_2) = (a_1 b_2 - a_2 b_1)\\mathbf{e}_1 \\mathbf{e}_2.</math>\n\nA [[seven-dimensional cross product]] is similar to the cross product in that its result is a vector orthogonal to the two arguments; there is however no natural way of selecting one of the possible such products.\n\n==Physics==\nVectors have many uses in physics and other sciences.\n\n===Length and units===\nIn abstract vector spaces, the length of the arrow depends on a [[Dimensionless number|dimensionless]] [[Scale (measurement)|scale]]. If it represents, for example, a force, the \"scale\" is of [[Dimensional analysis|physical dimension]] length/force. Thus there is typically consistency in scale among quantities of the same dimension, but otherwise scale ratios may vary; for example, if \"1 newton\" and \"5 m\" are both represented with an arrow of 2 cm, the scales are 1 m:50 N and 1:250 respectively. Equal length of vectors of different dimension has no particular significance unless there is some [[proportionality constant]] inherent in the system that the diagram represents. Also length of a unit vector (of dimension length, not length/force, etc.) has no coordinate-system-invariant significance.\n\n===Vector-valued functions===\n{{main article|Vector-valued function}}\nOften in areas of physics and mathematics, a vector evolves in time, meaning that it depends on a time parameter ''t''. For instance, if '''r''' represents the position vector of a particle, then '''r'''(''t'') gives a [[parametric equation|parametric]] representation of the trajectory of the particle. Vector-valued functions can be [[derivative|differentiated]] and [[integral|integrated]] by differentiating or integrating the components of the vector, and many of the familiar rules from [[calculus]] continue to hold for the derivative and integral of vector-valued functions.\n\n===Position, velocity and acceleration===\nThe position of a point '''x''' = (''x''<sub>1</sub>, ''x''<sub>2</sub>, ''x''<sub>3</sub>) in three-dimensional space can be represented as a [[position vector]] whose base point is the origin\n:<math>{\\mathbf x} = x_1 {\\mathbf e}_1 + x_2{\\mathbf e}_2 + x_3{\\mathbf e}_3.</math>\nThe position vector has dimensions of [[length]].\n\nGiven two points '''x''' = (''x''<sub>1</sub>, ''x''<sub>2</sub>, ''x''<sub>3</sub>), '''y''' = (''y''<sub>1</sub>, ''y''<sub>2</sub>, ''y''<sub>3</sub>) their [[Displacement (vector)|displacement]] is a vector\n:<math>{\\mathbf y}-{\\mathbf x}=(y_1-x_1){\\mathbf e}_1 + (y_2-x_2){\\mathbf e}_2 + (y_3-x_3){\\mathbf e}_3.</math>\nwhich specifies the position of ''y'' relative to ''x''. The length of this vector gives the straight-line distance from ''x'' to ''y''. Displacement has the dimensions of length.\n\nThe [[velocity]] '''v''' of a point or particle is a vector, its length gives the [[speed]]. For constant velocity the position at time ''t'' will be\n:<math>{\\mathbf x}_t= t {\\mathbf v} + {\\mathbf x}_0,</math>\nwhere '''x'''<sub>0</sub> is the position at time ''t'' = 0. Velocity is the [[Euclidean vector#Ordinary derivative|time derivative]] of position. Its dimensions are length/time.\n\n[[Acceleration]] '''a''' of a point is vector which is the [[Euclidean vector#Ordinary derivative|time derivative]] of velocity. Its dimensions are length/time<sup>2</sup>.\n\n===Force, energy, work===\n[[Force]] is a vector with dimensions of mass×length/time<sup>2</sup> and [[Newton's second law]] is the scalar multiplication\n:<math>{\\mathbf F} = m{\\mathbf a}</math>\n\nWork is the dot product of [[force]] and [[displacement (vector)|displacement]]\n:<math>E = {\\mathbf F} \\cdot ({\\mathbf x}_2 - {\\mathbf x}_1).</math>\n<!--\nIn physics, scalars may also have a unit of measurement associated with them. For instance, [[Newton's second law]] is\n:<math>{\\mathbf F} = m{\\mathbf a}</math>\nwhere '''F''' has units of force, '''a''' has units of acceleration, and the scalar ''m'' has units of mass. In one possible physical interpretation of the above diagram, the scale of acceleration is, for instance, 2 m/s<sup>2</sup> : cm, and that of force 5 N : cm. Thus a scale ratio of 2.5 kg : 1 is used for mass. Similarly, if displacement has a scale of 1:1000 and velocity of 0.2 cm : 1 m/s, or equivalently, 2 ms : 1, a scale ratio of 0.5 : s is used for time.\n-->\n\n==Vectors as directional derivatives==\nA vector may also be defined as a ''[[directional derivative]]'': consider a [[function (mathematics)|function]] <math>f(x^\\alpha)</math> and a curve <math>x^\\alpha (\\tau)</math>. Then the directional derivative of <math>f</math> is a scalar defined as\n\n:<math>\\frac{df}{d\\tau} = \\sum_{\\alpha=1}^n \\frac{dx^\\alpha}{d\\tau}\\frac{\\partial f}{\\partial x^\\alpha}.</math>\n\nwhere the index <math>\\alpha</math> is [[Summation convention|summed over]] the appropriate number of dimensions (for example, from 1 to 3 in 3-dimensional Euclidean space, from 0 to 3 in 4-dimensional spacetime, etc.). Then consider a vector tangent to <math>x^\\alpha (\\tau)</math>:\n\n:<math>t^\\alpha = \\frac{dx^\\alpha}{d\\tau}.</math>\n\nThe directional derivative can be rewritten in differential form (without a given function <math>f</math>) as\n\n:<math>\\frac{d}{d\\tau} = \\sum_\\alpha t^\\alpha\\frac{\\partial}{\\partial x^\\alpha}.</math>\n\nTherefore, any directional derivative can be identified with a corresponding vector, and any vector can be identified with a corresponding directional derivative. A vector can therefore be defined precisely as\n\n:<math>\\mathbf{a} \\equiv a^\\alpha \\frac{\\partial}{\\partial x^\\alpha}.</math>\n\n==Vectors, pseudovectors, and transformations==\nAn alternative characterization of Euclidean vectors, especially in physics, describes them as lists of quantities which behave in a certain way under a [[coordinate system|coordinate transformation]]. A ''contravariant vector'' is required to have components that \"transform opposite to the basis\" under changes of [[Basis (linear algebra)|basis]]. The vector itself does not change when the basis is transformed; instead, the components of the vector make a change that cancels the change in the basis. In other words, if the reference axes (and the basis derived from it) were rotated in one direction, the component representation of the vector would rotate in the opposite way to generate the same final vector. Similarly, if the reference axes were stretched in one direction, the components of the vector would reduce in an exactly compensating way. Mathematically, if the basis undergoes a transformation described by an [[invertible matrix]] ''M'', so that a coordinate vector '''x''' is transformed to {{nowrap|1='''x'''′ = ''M'''''x'''}}, then a contravariant vector '''v''' must be similarly transformed via {{nowrap|1='''v'''′ = ''M''<math>^{-1}</math>'''v'''}}. This important requirement is what distinguishes a contravariant vector from any other triple of physically meaningful quantities. For example, if ''v'' consists of the ''x'', ''y'', and ''z''-components of [[velocity]], then ''v'' is a contravariant vector: if the coordinates of space are stretched, rotated, or twisted, then the components of the velocity transform in the same way. On the other hand, for instance, a triple consisting of the length, width, and height of a rectangular box could make up the three components of an abstract [[vector space|vector]], but this vector would not be contravariant, since rotating the box does not change the box's length, width, and height. Examples of contravariant vectors include [[displacement (vector)|displacement]], [[velocity]], [[electric field]], [[momentum]], [[force]], and [[acceleration]].\n\nIn the language of [[differential geometry]], the requirement that the components of a vector transform according to the same matrix of the coordinate transition is equivalent to defining a ''contravariant vector'' to be a [[tensor]] of [[Covariance and contravariance of vectors|contravariant]] rank one. Alternatively, a contravariant vector is defined to be a [[tangent space|tangent vector]], and the rules for transforming a contravariant vector follow from the [[chain rule]].\n\nSome vectors transform like contravariant vectors, except that when they are reflected through a mirror, they flip ''and'' gain a minus sign. A transformation that switches right-handedness to left-handedness and vice versa like a mirror does is said to change the ''[[orientation (mathematics)|orientation]]'' of space. A vector which gains a minus sign when the orientation of space changes is called a ''[[pseudovector]]'' or an ''axial vector''. Ordinary vectors are sometimes called ''true vectors'' or ''polar vectors'' to distinguish them from pseudovectors. Pseudovectors occur most frequently as the [[cross product]] of two ordinary vectors.\n\nOne example of a pseudovector is [[angular velocity]]. Driving in a [[car]], and looking forward, each of the [[wheel]]s has an angular velocity vector pointing to the left. If the world is reflected in a mirror which switches the left and right side of the car, the ''reflection'' of this angular velocity vector points to the right, but the ''actual'' angular velocity vector of the wheel still points to the left, corresponding to the minus sign. Other examples of pseudovectors include [[magnetic field]], [[torque]], or more generally any cross product of two (true) vectors.\n\nThis distinction between vectors and pseudovectors is often ignored, but it becomes important in studying [[symmetry]] properties. See [[parity (physics)]].\n\n==See also==\n{{Div col|colwidth=25em}}\n* [[Affine space]], which distinguishes between vectors and [[Point (geometry)|points]]\n* [[Array data structure]] or [[Vector (Computer Science)]]\n* [[Banach space]]\n* [[Clifford algebra]]\n* [[Complex number]]\n* [[Coordinate system]]\n* [[Covariance and contravariance of vectors]]\n* [[Four-vector]], a non-Euclidean vector in Minkowski space (i.e. four-dimensional spacetime), important in [[theory of relativity|relativity]]\n* [[Function space]]\n* [[Grassmann]]'s ''Ausdehnungslehre''\n* [[Hilbert space]]\n* [[Normal vector]]\n* [[Null vector]]\n* [[Pseudovector]]\n* [[Quaternion]]\n* [[Tangential and normal components]] (of a vector)\n* [[Tensor]]\n* [[Unit vector]]\n* [[Vector bundle]]\n* [[Vector calculus]]\n* [[Vector notation]]\n* [[Vector-valued function]]\n{{div col end}}\n\n==Notes==\n{{Reflist}}\n\n==References==\n\n===Mathematical treatments===\n*{{Cite book\n | ref = harv\n | first = Tom\n | last = Apostol\n | author-link = Tom Apostol\n | title = Calculus\n | volume = Vol. 1: One-Variable Calculus with an Introduction to Linear Algebra\n | publisher = Wiley\n | year = 1967\n | isbn = 978-0-471-00005-1\n}}\n*{{Cite book\n | ref = harv\n | first = Tom\n | last = Apostol\n | author-link = Tom Apostol\n | title = Calculus\n | volume = Vol. 2: Multi-Variable Calculus and Linear Algebra with Applications\n | publisher = Wiley\n | year = 1969\n | isbn = 978-0-471-00007-5\n}}\n*{{Citation\n | title = Introduction to Tensor Calculus and Continuum Mechanics\n | first = J. H.\n | last = Heinbockel\n | publisher = Trafford Publishing\n | year = 2001\n | isbn = 1-55369-133-4\n | url = http://www.math.odu.edu/~jhh/counter2.html\n}}.\n*{{Citation\n | last = Ito\n | first = Kiyosi\n | title = Encyclopedic Dictionary of Mathematics\n | publisher = [[MIT Press]]\n | edition = 2nd\n | isbn = 978-0-262-59020-4\n | year = 1993\n}}.\n*{{springer|id=V/v096340|title=Vector, geometric|first=A.B.|last=Ivanov}}.\n*{{Citation\n | last1 = Kane\n | first1 = Thomas R.\n | last2 = Levinson\n | first2 = David A.\n | title = Dynamics Online\n | publisher = OnLine Dynamics\n | location = Sunnyvale, California\n | year = 1996\n}}.\n*{{Cite book\n | ref = harv\n | first = Serge\n | last = Lang\n | author-link = Serge Lang\n | title = Introduction to Linear Algebra\n | edition = 2nd\n | publisher = Springer\n | year = 1986\n | isbn = 0-387-96205-0\n}}\n*{{Cite book\n | ref = harv\n | first = Daniel\n | last = Pedoe\n | author-link = Daniel Pedoe\n | title = Geometry: A comprehensive course\n | publisher = Dover\n | year = 1988\n | isbn = 0-486-65812-0\n}}\n\n===Physical treatments===\n*{{Cite book\n | ref = harv\n | last = Aris\n | first = R.\n | title = Vectors, Tensors and the Basic Equations of Fluid Mechanics\n | publisher = Dover\n | year = 1990\n | isbn =978-0-486-66110-0\n}}\n*{{Cite book\n | ref = harv\n | first1 = Richard\n | last1 = Feynman\n | author1-link = Richard Feynman\n | first2 = R.\n | last2 = Leighton\n | first3 = M.\n | last3 = Sands\n | year = 2005\n | edition = 2nd\n | title = [[The Feynman Lectures on Physics]]\n | volume = Vol. I\n | publisher = Addison Wesley\n | isbn = 978-0-8053-9046-9\n | chapter = Chapter 11\n}}\n\n==External links==\n{{Wikiquote}}\n{{commons category|Vectors}}\n{{Wikibooks|Waves|Vectors}}\n* {{springer|title=Vector|id=p/v096340}}\n* [http://wwwppd.nrl.navy.mil/nrlformulary/vector_identities.pdf Online vector identities] ([[Portable Document Format|PDF]])\n* [http://www.marco-learningsystems.com/pages/roche/introvectors.htm Introducing Vectors] A conceptual introduction ([[applied mathematics]])\n* [https://github.com/dvdvideo1234/TSP Tensor state processing (TSP) Vector implementation written in C++ by Deyan Dobromirov, Sofia, Bulgaria]\n{{linear algebra}}\n\n[[Category:Abstract algebra]]\n[[Category:Vector calculus]]\n[[Category:Linear algebra]]\n[[Category:Concepts in physics]]\n[[Category:Vectors (mathematics and physics)]]"
    },
    {
      "title": "Expression (mathematics)",
      "url": "https://en.wikipedia.org/wiki/Expression_%28mathematics%29",
      "text": "{{short description|Well-formed combination of mathematical symbols denoting a mathematical object}}\n{{Refimprove|date=January 2012}}\n{{inline|date=October 2014}}\nIn [[mathematics]], an '''expression''' or '''mathematical expression''' is a finite combination of [[List of mathematical symbols|symbols]] that is [[well-formed formula|well-formed]] according to rules that depend on the context.  Mathematical symbols can designate numbers ([[Constant (mathematics)|constants]]), [[Variable (mathematics)|variables]], [[Operation (mathematics)|operations]], [[function (mathematics)|functions]], [[Bracket (mathematics)|brackets]], punctuation, and grouping to help determine [[order of operations]], and other aspects of [[syntax (logic)|logical syntax]].\n\n==Examples==\n\nThe use of expressions ranges from the simple:\n\n::<math>0+0</math>\n\n::<math>8x-5</math> &nbsp;&nbsp;([[linear polynomial]])\n\n::<math>7{{x}^{2}}+4x-10</math> &nbsp;&nbsp;([[quadratic polynomial]])\n\n::<math>\\frac{x-1}{{{x}^{2}}+12}</math> &nbsp;&nbsp;([[rational fraction]])\n\nto the complex: \n\n::<math>f(a)+\\sum_{k=1}^n\\left.\\frac{1}{k!}\\frac{d^k}{dt^k}\\right|_{t=0}f(u(t)) + \\int_0^1 \\frac{(1-t)^n }{n!} \\frac{d^{n+1}}{dt^{n+1}} f(u(t))\\, dt.</math>\n\n==Syntax versus semantics==\n\n===Syntax===\n{{Main|Syntax}}\n\nAn expression is a syntactic construct, it must be [[Well-formed formula|well-formed]]: the allowed [[operator (mathematics)|operators]] must have the correct number of inputs in the correct places, the characters that make up these inputs must be valid, have a clear [[order of operations]], etc. Strings of symbols that violate the rules of syntax are not well-formed and are not valid mathematical expressions.\n\nFor example, in the usual notation of [[arithmetic]], the expression ''1 + 2 × 3'' is well-formed, but the following expression is not:\n\n:<math>\\times4)x+,/y</math>.\n\n===Semantics===\n{{Main|Semantics|Formal semantics (logic)}}\n\nSemantics is the study of meaning. Formal semantics is about attaching meaning to expressions.\n\nIn [[algebra]], an expression may be used to designate a value, which might depend on values assigned to [[variable (mathematics)|variable]]s occurring in the expression. The determination of this value depends on the [[semantics]] attached to the symbols of the expression.  The choice of semantics depends on the context of the expression.  The same syntactic expression ''1 + 2 × 3'' can have different values (mathematically 7, but also 9), depending on the [[order of operations]] implied by the context (See [[Order of operation#Calculators|Order of operations:Calculators]]).\n\nThe semantic rules may declare that certain expressions do not designate any value (for instance when they involve division by 0); such expressions are said to have an undefined value, but they are well-formed expressions nonetheless. In general the meaning of expressions is not limited to designating values; for instance, an expression might designate a condition, or an [[equation]] that is to be solved, or it can be viewed as an object in its own right that can be manipulated according to certain rules. Certain expressions that designate a value simultaneously express a condition that is assumed to hold, for instance those involving the operator <math>\\oplus</math> to designate an internal [[direct sum]].\n\n===Formal languages and lambda calculus===\n{{Main|Formal language|Lambda calculus}}\n\nFormal languages allow [[Formal system|formalizing]] the concept of well-formed expressions.\n\nIn the 1930s, a new type of expressions, called [[lambda expressions]], were introduced by [[Alonzo Church]] and [[Stephen Kleene]] for formalizing [[function (mathematics)|functions]] and their evaluation. They form the basis for [[lambda calculus]], a [[formal system]] used in [[mathematical logic]] and the [[Programming language theory|theory of programming languages]].\n\nThe equivalence of two lambda expressions is [[decision problem|undecidable]]. This is also the case for the expressions representing real numbers, which are built from the integers by using the arithmetical operations, the logarithm and the exponential ([[Richardson's theorem]]).\n\n==Variables==\n\nMany mathematical expressions include [[Variable (mathematics)|variables]]. Any variable can be classified as being either a [[free variable]] or a [[bound variable]].\n\nFor a given combination of values for the free variables, an expression may be evaluated, although for some combinations of values of the free variables, the value of the expression may be undefined. Thus an expression represents a [[function (mathematics)|function]] whose inputs are the values assigned to the free variables and whose output is the resulting value of the expression.{{cn|date=October 2014}}\n\nFor example, the expression\n:<math> x/y </math>\n\nevaluated for ''x'' = 10, ''y'' = 5, will give 2; but it is [[Division by zero|undefined]] for ''y'' = 0.\n\nThe evaluation of an expression is dependent on the definition of the mathematical operators and on the system of values that is its context.\n\nTwo expressions are said to be equivalent if, for each combination of values for the free variables, they have the same output, i.e., they represent the same function. Example:\n\nThe expression\n:<math>\\sum_{n=1}^{3} (2nx)</math>\nhas free variable ''x'', bound variable ''n'', constants 1, 2, and 3, two occurrences of an implicit multiplication operator, and a summation operator. The expression is equivalent to the simpler expression 12''x''. The value for ''x''&nbsp;=&nbsp;3 is 36.\n\n==See also==\n\n{{div col|colwidth=22em}}\n* [[Algebraic closure]]\n* [[Algebraic expression]]\n* [[Analytic expression]]\n* [[Closed-form expression]]\n* [[Combinator]]\n* [[Computer algebra#Expressions|Computer algebra expression]]\n* [[Defined and undefined]]\n* [[Equation]]\n* [[Expression (programming)]]\n* [[Formal grammar]]\n* [[Formula]]\n* [[Functional programming]]\n* [[Logical expression]]\n* [[Term (logic)]]\n{{div col end}}\n\n==Notes==\n\n{{Reflist}}\n\n==References==\n\n*Redden, John. [http://catalog.flatworldknowledge.com/bookhub/reader/128?e=fwk-redden-ch02 ''Elementary Algebra'']. Flat World Knowledge, 2011.\n\n[[Category:Abstract algebra]]\n[[Category:Logical expressions]]\n\n[[bg:Израз]]\n[[pl:Wyrażenie algebraiczne]]"
    },
    {
      "title": "External (mathematics)",
      "url": "https://en.wikipedia.org/wiki/External_%28mathematics%29",
      "text": "{{unreferenced|date=December 2007}}\nThe term '''external''' is useful for describing certain algebraic structures. The term comes from the concept of an [[Binary operation#External binary operations|external binary operation]] which is a binary operation that draws from some ''external set''. To be more specific, a '''left external binary operation''' on ''S'' over ''R'' is a function <math>f : R \\times S \\rightarrow S</math> and a '''right external binary operation''' on ''S'' over ''R'' is a function <math>f : S \\times R \\rightarrow S</math> where ''S'' is the set the operation is defined on, and ''R'' is the external set (the set the operation is defined ''over'').\n\n== Generalizations ==\n\nThe ''external'' concept is a generalization rather than a specialization, and as such, it is different from many terms in mathematics. A similar but opposite concept is that of an ''internal binary function'' from ''R'' to ''S'', defined as a function <math>f : R \\times R \\rightarrow S</math>. Internal binary functions are like binary functions, but are a form of specialization, so they only accept a subset of the domains of binary functions. Here we list these terms with the [[Function (mathematics)|function]] [[Method signature|signatures]] they imply, along with some examples:\n\n* <math>f : Q \\times R \\rightarrow S</math> ([[binary function]])\n** Example:  [[exponentiation]] (<math>z^q : \\mathbb{Z} \\times \\mathbb{Q} \\rightarrow \\mathbb{C}</math> as in <math>{(-1)}^{1/2} = i</math>), \n** Example: [[element (mathematics)|set membership]] (<math>(\\in) : S \\times \\mathbf{Set} \\rightarrow \\mathbb{B}</math> where <math> \\mathbf{Set} </math> is the [[category of sets]])\n** Examples: [[matrix multiplication]], the [[tensor product]], and the [[Cartesian product]]\n* <math>f : R \\times R \\rightarrow S</math> (internal binary function)\n** Example: internal [[binary relations]] (<math>(\\le) : R \\times R \\rightarrow \\mathbb{B}</math>)\n** Examples: the [[dot product]], the [[inner product space|inner product]], and [[metric (mathematics)|metrics]].\n* <math>f : R \\times S \\rightarrow S</math> ([[Binary operation#External binary operations|external binary operation]])\n** Examples: [[dynamical system]] [[flow (mathematics)|flows]], [[Group action (mathematics)|group action]]s, [[projection (set theory)|projection maps]], and [[scalar multiplication]].\n* <math>f : S \\times S \\rightarrow S</math> ([[binary operation]]).\n** Examples: [[addition]], [[multiplication]],  [[permutation]]s, and the [[cross product]].\n\n== External monoids ==\n\nSince [[monoid]]s are defined in terms of [[binary operations]], we can define an ''external monoid'' in terms of ''external binary operations''. For the sake of simplicity, unless otherwise specified, a ''left'' external binary operation is implied. Using the term ''external'', we can make the generalizations:\n\n* An '''external [[magma (mathematics)|magma]]''' <math>(S, \\times)</math> over ''R'' is a set ''S'' with an external binary operation. This satisfies <math>r \\times s \\in S</math> for all <math>s \\in S, r \\in R</math> (external [[closure (mathematics)|closure]]). \n* An '''external [[semigroup]]''' <math>(S, \\times)</math> over <math>(R, \\cdot)</math> is an external magma that satisfies <math>(r_1 \\cdot r_2) \\times s = r_1 \\times (r_2 \\times s)</math> for all <math>s \\in S, r_1, r_2 \\in R</math> (externally [[associative]]). \n* An '''external [[monoid]]''' <math>(S, \\times)</math> over <math>(R, \\cdot)</math> is an external semigroup in which there exists <math>1 \\in R</math> such that <math>1 \\times s = s</math> for all <math>s \\in S</math> (has external [[identity element]]).\n\n== Modules as external rings ==\n\nMuch of the machinery of [[module (mathematics)|modules]] and [[vector spaces]] are fairly straightforward, or discussed above. The only thing not covered yet is their distribution axioms. The external ring multiplication <math>\\otimes</math> is externally [[Distributive property|distributive]] in <math>(S, \\oplus, \\otimes)</math> over the [[ring (mathematics)|ring]] <math>(R, +, \\cdot)</math> [[iff]]:\n* <math>r \\otimes (s_1 \\oplus s_2) = (r \\otimes s_1) \\oplus (r \\otimes s_2)</math> for all <math>s_1,s_2 \\in S, r \\in R</math> and: \n* <math>(r_1 + r_2) \\otimes s = (r_1 \\otimes s) \\oplus (r_2 \\otimes s)</math> for all <math>s \\in S, r_1,r_2 \\in R</math>\n\nUsing these terminology we can make the following local generalizations:\n* An '''external semiring''' <math>(S, \\oplus, \\otimes)</math> over the [[semiring]] <math>(R, +, \\cdot)</math> is a [[commutative]] [[monoid]] <math>(S, \\oplus)</math> and an external monoid <math>(S, \\otimes)</math> where <math>\\otimes</math> is externally [[Distributive property|distributive]] in <math>(S, \\oplus, \\otimes)</math> over the [[semiring]] <math>(R, +, \\cdot)</math>. \n* An '''external ring''' <math>(S, \\oplus, \\otimes)</math> over the [[ring (mathematics)|ring]] <math>(R, +, \\cdot)</math> is an [[abelian group]] <math>(S, \\oplus)</math> and an external monoid <math>(S, \\otimes)</math> where <math>\\otimes</math> is externally [[Distributive property|distributive]] in <math>(S, \\oplus, \\otimes)</math> over the [[ring (mathematics)|ring]] <math>(R, +, \\cdot)</math>.\n\n== Other examples ==\n\nNow that we have all the terminology we need, we can make simple connections between various structures:\n* Complex exponentiation forms an external [[monoid]] <math>(\\mathbb{C}, \\uparrow)</math> over the [[abelian group]] <math>(\\mathbb{C}, \\cdot)</math>.\n* Prime factorization forests form an external [[semiring]] <math>(\\mathbb{N}, \\cdot, \\uparrow)</math> over the [[semiring]] <math>(\\mathbb{N}, +, \\cdot)</math>.\n* A [[dynamical system (definition)|dynamical system]] <math>(T, S, \\Phi)</math> is an '''external monoid''' <math>(S, \\Phi)</math> over the [[monoid]] <math>(T, {+})</math>.\n* A [[semimodule]] is an '''external semiring''' over a [[semiring]].\n* A [[module (mathematics)|module]] is an '''external ring''' over a [[ring (mathematics)|ring]].\n* A [[vector space]] is an '''external ring''' over a [[field (mathematics)|field]].\n\n== Usefulness ==\n\nIt could be argued that we already have terms for the concepts described here, like [[dynamical systems]], [[Group action (mathematics)|group actions]], [[module (mathematics)|modules]], and [[vector spaces]]. However, there is still no other terminology available for an '''external monoid''' for which this terminology gives us a concise expression. Above all else, this is a reason this term should be of use in the mathematical community.\n\n[[Category:Abstract algebra]]\n[[Category:Binary operations]]"
    },
    {
      "title": "Field (mathematics)",
      "url": "https://en.wikipedia.org/wiki/Field_%28mathematics%29",
      "text": "{{short description|Algebraic structure with two binary operations}}\n{{about|fields in algebra|fields in geometry|Vector field|other uses|Field (disambiguation)#Mathematics{{!}}Field § Mathematics}}\n{{good article}}\n\n[[File:Regular_polygon_7_annotated.svg|thumb|262px|The [[regular polygon|regular]] [[heptagon]] cannot be constructed using only a [[straightedge and compass construction]]; this can be proven using the field of [[constructible number]]s.]]\n{{Algebraic structures}}\nIn [[mathematics]], a '''field'''  is a [[set (mathematics)|set]] on which [[addition]], [[subtraction]], [[multiplication]], and [[division (mathematics)|division]] are defined, and behave as the corresponding operations on  [[rational number|rational]] and [[real number]]s do. \nA field is thus a fundamental [[algebraic structure]], which is widely used in [[algebra]], [[number theory]] and many other areas of mathematics.\n\nThe best known fields are the field of [[rational number]]s, the field of [[real number]]s and the field of [[complex number]]s. Many other fields, such as [[field of rational functions|fields of rational functions]], [[algebraic function field]]s, [[algebraic number field]]s, and [[p-adic number|''p''-adic fields]] are commonly used and studied in mathematics, particularly in number theory and [[algebraic geometry]]. Most [[cryptographic protocol]]s rely on [[finite field]]s, i.e., fields with finitely many [[element (set)|elements]].\n\nThe relation of two fields is expressed by the notion of a [[field extension]]. [[Galois theory]], initiated by [[Évariste Galois]] in the 1830s, is devoted to understanding the symmetries of field extensions. Among other results, this theory shows that [[angle trisection]] and [[squaring the circle]] can not be done with a [[compass and straightedge]]. Moreover, it shows that [[quintic equation]]s are algebraically unsolvable.\n\nFields serve as foundational notions in several mathematical domains. This includes different branches of [[analysis]], which are based on fields with additional structure. Basic theorems in analysis hinge on the structural properties of the field of real numbers. Most importantly for algebraic purposes, any field may be used as the [[scalar (mathematics)|scalars]] for a [[vector space]], which is the standard general context for [[linear algebra]]. [[Number field]]s, the siblings of the field of rational numbers, are studied in depth in [[number theory]]. [[Function field of an algebraic variety|Function fields]] can help describe properties of geometric objects.\n\n== Definition ==\nInformally, a field is a set, along with two [[binary operation|operation]]s defined on that set: an addition operation written as {{math|''a'' + ''b''}}, and a multiplication operation written as {{math|''a'' ⋅ ''b''}}, both of which behave similarly as they behave for [[rational number]]s and [[real number]]s, including the existence of an [[additive inverse]] {{math|''&minus;a''}} for all elements ''a'', and of a [[multiplicative inverse]] ''b''<sup>−1</sup> for every nonzero element ''b''. This allows us to consider also the so-called ''inverse'' operations of [[Subtraction#Real numbers|subtraction]] {{math|''a'' &minus; ''b''}}, and [[Division (mathematics)|division]] {{math|''a'' / ''b''}}, via defining:\n:{{math|1=''a'' &minus; ''b'' = ''a'' + (&minus;''b'')}},\n:{{math|1=''a'' / ''b'' = ''a'' · ''b''<sup>&minus;1</sup>}}.\n\n===Classic definition===\n\nFormally, a field is a [[set (mathematics)|set]] {{math|''F''}} together with two [[binary operation|operations]] called ''addition'' and ''multiplication''.<ref>{{harvtxt|Beachy|Blair|2006|loc=Definition 4.1.1, p.&nbsp;181}}</ref> An operation is a mapping that associates an element of the set to ''every'' pair of its elements. The result of the addition of {{math|''a''}} and {{math|''b''}} is called the ''sum'' of {{math|''a''}} and {{math|''b''}} and denoted {{math|''a'' + ''b''}}. Similarly, the result of the multiplication of {{math|''a''}} and {{math|''b''}} is called the ''product'' of {{math|''a''}} and {{math|''b''}}, and denoted {{math|''ab''}} or {{math|''a''⋅''b''}}. These operations are required to satisfy the following properties, referred to as ''[[Axiom#Non-logical axioms|field axioms]]''. In these axioms,  {{math|''a'', ''b''}} and {{math|''c''}} are arbitrary [[element (mathematics)|element]]s of the field {{math|''F''}}.\n\n* ''[[Associativity]]'' of addition and multiplication: {{math|1=''a'' + (''b'' + ''c'') = (''a'' + ''b'') + ''c''}}  and  {{math|1=''a'' · (''b'' · ''c'') = (''a'' · ''b'') · ''c''}}.\n* ''[[Commutativity]]'' of addition and multiplication: {{math|1=''a'' + ''b'' = ''b'' + ''a''}} and {{math|1=''a'' · ''b'' = ''b'' · ''a''}}.\n* ''[[Additive identity|Additive]]'' and ''[[multiplicative identity]]'': there exist two different elements {{math|0}} and {{math|1}} in {{math|''F''}} such that {{math|1=''a'' + 0 = ''a''}} and {{math|1=''a'' · 1 = ''a''}}.\n* ''[[Additive inverse]]s'': for every {{math|''a''}} in {{math|''F''}}, there exists an element in {{math|''F''}}, denoted {{math|−''a''}}, called the ''additive inverse'' of ''a'', such that {{math|1=''a'' + (−''a'') = 0}}.\n* ''[[Multiplicative inverse]]s'': for every {{math|''a'' ≠ 0}} in {{math|''F''}}, there exists an element in {{math|''F''}}, denoted by {{math|''a''<sup>−1</sup>}}, {{math|1/''a''}},  or {{math|{{sfrac|1|''a''}}}}, called the ''multiplicative inverse'' of ''a'', such that {{math|1=''a'' · ''a''<sup>−1</sup> = 1}}.\n* ''[[Distributivity]]'' of multiplication over addition: {{math|1=''a'' · (''b'' + ''c'') = (''a'' · ''b'') + (''a'' · ''c'')}}.\n\nThis may be summarized by saying: a field has two operations, called ''addition'' and ''multiplication''; it is an [[abelian group]] under the addition, with 0 as [[additive identity]]; the nonzero elements are an abelian group under the multiplication, with 1 as [[multiplicative identity]]; the multiplication is distributive over the addition.\n\n===Alternative definition===\nFields can also be defined in different, but equivalent ways. One can alternatively define a field by four binary operations (add, subtract, multiply, divide) and their required properties. [[Division by zero]] is, by definition, excluded.<ref>{{harvtxt|Clark|1984|loc=Chapter 3}}.</ref> In order to avoid [[existential quantifier]]s, fields can be defined by two binary operations (addition and multiplication), two unary operations (yielding the additive and multiplicative inverses respectively), and two [[arity#Nullary|nullary]] operations (the constants {{math|0}} and {{math|1}}). These operations are then subject to the conditions above. Avoiding existential quantifiers is important in [[constructive mathematics]] and [[computing]].<ref>{{harvtxt|Mines|Richman|Ruitenburg|1988|loc=§II.2}}. See also [[Heyting field]].</ref> One may equivalently define a field by the same two binary operations, one unary operation (the multiplicative inverse), and two constants 1 and −1, since {{math|1=0 = 1 + (−1)}} and {{math|1=−a = (−1) a}}.<ref group=nb>The a priori twofold use of the symbol \"−\" for denoting one part of a constant and for the additive inverses is justified by this latter condition.</ref>\n\n==Examples==\n\n===Rational numbers===\n{{main|Rational number}}\nRational numbers have been widely used a long time before the elaboration of the concept of field.\nThey are numbers that can be written as [[fraction (mathematics)|fractions]]\n{{math|''a''/''b''}}, where {{math|''a''}} and {{math|''b''}} are [[integer]]s, and {{math|''b'' ≠ 0}}. The additive inverse of such a fraction is {{math|−''a''/''b''}}, and the multiplicative inverse (provided that {{math|''a'' ≠ 0}}) is {{math|''b''/''a''}}, which can be seen as follows:\n\n: <math> \\frac b a \\cdot \\frac a b  = \\frac{ba}{ab} = 1.</math>\n\nThe abstractly required field axioms reduce to standard properties of rational numbers. For example, the law of distributivity can be proven as follows:<ref>{{harvtxt|Beachy|Blair|2006|loc=p. 120, Ch. 3}}</ref>\n:<math>\n\\begin{align}\n& \\frac a b \\cdot \\left(\\frac c d + \\frac e f \\right) \\\\[6pt]\n= {} & \\frac a b  \\cdot \\left(\\frac c d  \\cdot \\frac f f + \\frac e f \\cdot \\frac d d \\right) \\\\[6pt]\n= {} & \\frac{a}{b} \\cdot \\left(\\frac{cf}{df} + \\frac{ed}{fd}\\right) = \\frac{a}{b} \\cdot \\frac{cf + ed}{df} \\\\[6pt]\n= {} & \\frac{a(cf + ed)}{bdf} = \\frac{acf}{bdf} +  \\frac{aed}{bdf} = \\frac{ac}{bd} +  \\frac{ae}{bf} \\\\[6pt]\n= {} & \\frac a b \\cdot \\frac c d + \\frac a b \\cdot \\frac e f.\n\\end{align}\n</math>\n\n===Real and complex numbers===\n\n[[File:Complex_multi.svg|thumb|255px|The multiplication of complex numbers can be visualized geometrically by rotations and scalings.]]\n\n{{main|Real number| Complex number}}\nThe [[real number]]s {{math|'''R'''}}, with the usual operations of addition and multiplication, also form a field. The [[complex number]]s {{math|'''C'''}} consist of expressions\n:{{math|''a'' + ''bi'',}} with {{math|''a'', ''b''}} real,\nwhere {{math|''i''}} is the [[imaginary unit]], i.e., a (non-real) number satisfying {{math|1=''i''<sup>2</sup> = −1}}.\nAddition and multiplication of real numbers are defined in such a way that expressions of this type satisfy all field axioms and thus hold for {{math|'''C'''}}. For example, the distributive law enforces\n:{{math|1=(''a'' + ''bi'')(''c'' + ''di'') = ''ac'' + ''bci'' + ''adi'' + ''bdi''<sup>2</sup> = ''ac''−''bd'' + (''bc'' + ''ad'')''i''.}}\nIt is immediate that this is again an expression of the above type, and so the complex numbers form a field. Complex numbers can be geometrically represented as points in the [[Plane (geometry)|plane]], with [[Cartesian coordinates]] given by the real numbers of their describing expression, or as the arrows from the origin to these points, specified by their length and an angle enclosed with some distinct direction. Addition then corresponds to combining the arrows to the intuitive parallelogram (adding the Cartesian coordinates), and the multiplication is &ndash;less intuitively&ndash; combining rotating and scaling of the arrows (adding the angles and multiplying the lengths). The fields of real and complex numbers are used throughout mathematics, physics, engineering, statistics, and many other scientific disciplines.\n\n===Constructible numbers===\n[[File:Root_construction_geometric_mean5.svg|thumb|255px|The [[geometric mean theorem]] asserts that {{math|1=''h''<sup>2</sup> = ''pq''}}. Choosing {{math|1=''q''&nbsp;=&nbsp;1}} allows construction of the square root of a given constructible number {{math|''p''}}.]]\n{{main|Constructible numbers}}\n \nIn antiquity, several geometric problems concerned the (in)feasibility of constructing certain numbers with [[compass and straightedge]]. For example, it was unknown to the Greeks that it is in general impossible to trisect a given angle in this way. These problems can be settled using the field of [[constructible numbers]].<ref>{{harvtxt|Artin|1991|loc=Chapter 13.4}}</ref> Real constructible numbers are, by definition, lengths of line segments that can be constructed from the points 0 and 1 in finitely many steps using only [[compass]] and [[straightedge]]. These numbers, endowed with the field operations of real numbers, restricted to the constructible numbers, form a field, which properly includes the field {{math|'''Q'''}} of rational numbers. The illustration shows the construction of [[square root]]s of constructible numbers, not necessarily contained within {{math|'''Q'''}}. Using the labeling in the illustration, construct the segments {{math|''AB''}}, {{math|''BD''}}, and a [[semicircle]] over {{math|''AD''}} (center at the [[midpoint]] {{math|''C''}}), which intersects the [[perpendicular]] line through ''B'' in a point {{math|''F''}}, at a distance of exactly <math>h=\\sqrt p</math> from {{math|''B''}} when {{math|''BD''}} has length one.\n\nNot all real numbers are constructible. It can be shown that <math>\\sqrt[3] 2</math> is not a constructible number, which implies that it is impossible to construct with compass and straightedge the length of the side of a [[Doubling the cube|cube with volume 2]], another problem posed by the ancient Greeks.\n\n===A field with four elements===\n{|class=\"wikitable floatright\"\n|+ \n! scope=\"col\" style=\"float:text-align:center;\"| Addition\n! scope=\"col\" style=\"float:text-align:center'\"| Multiplication\n|-\n! scope=\"row\" | \n{| class=\"wikitable\" \n|-\n! style=\"width:20%;\"| + !! style=\"width:20%;\"| {{math|''O''}} !! style=\"width:20%;\"| {{math|''I''}} !! style=\"width:20%;\"| {{math|''A''}} !! style=\"width:20%;\"| {{math|''B''}}\n|-\n! {{math|''O''}}\n| style=\"background:#fdd;\"|{{color|blue| {{math|''O''}}}}\n| style=\"background:#fdd;\"| {{color|blue| {{math|''I''}}}}\n|| {{math|''A''}}\n|| {{math|''B''}}\n|-\n! {{math|''I''}}\n| style=\"background:#fdd;\"| {{color|blue| {{math|''I''}}}}\n| style=\"background:#fdd;\"| {{color|blue| {{math|''O''}}}}\n|| {{math|''B''}}\n|| {{math|''A''}}\n|-\n! {{math|''A''}}\n|| {{math|''A''}}\n|| {{math|''B''}}\n|| {{math|''O''}}\n|| {{math|''I''}}\n|-\n! {{math|''B''}}\n|| {{math|''B''}}\n|| {{math|''A''}}\n|| {{math|''I''}}\n|| {{math|''O''}}\n|}\n! scope=\"row\" | \n{| class=\"wikitable\"\n\n|-\n! style=\"width:20%;\"| · !! style=\"width:20%;\"| {{math|''O''}} !! style=\"width:20%;\"| {{math|''I''}} !! style=\"width:20%;\"| {{math|''A''}} !! style=\"width:20%;\"| {{math|''B''}}\n|-\n! {{math|''O''}}\n| style=\"background:#fdd;\"|{{color|blue| {{math|''O''}}}}\n| style=\"background:#fdd;\"|{{color|blue| {{math|''O''}}}}\n|| {{math|''O''}}\n|| {{math|''O''}}\n|-\n! {{math|''I''}}\n| style=\"background:#fdd;\"|{{color|blue| {{math|''O''}}}}\n| style=\"background:#fdd;\"|{{color|blue| {{math|''I''}}}}\n|| {{math|''A''}}\n|| {{math|''B''}}\n|-\n! {{math|''A''}}\n|| {{math|''O''}}\n|| {{math|''A''}}\n|| {{math|''B''}}\n|| {{math|''I''}}\n|-\n! {{math|''B''}}\n|| {{math|''O''}}\n|| {{math|''B''}}\n|| {{math|''I''}}\n|| {{math|''A''}}\n|}\n|}\n\nIn addition to familiar number systems such as the rationals, there are other, less immediate examples of fields. The following example is a field consisting of four elements called {{math|''O''}}, {{math|''I''}}, {{math|''A''}}, and {{math|''B''}}. The notation is chosen such that {{math|''O''}} plays the role of the additive identity element (denoted 0 in the axioms above), and I is the multiplicative identity (denoted 1 in the axioms above). The field axioms can be verified by using some more field theory, or by direct computation. For example,\n\n: {{math|1=''A'' · (''B'' + ''A'') = ''A'' · ''I'' = ''A''}}, which equals {{nowrap|1={{math|1=''A'' · ''B'' + ''A'' · ''A'' = ''I'' + ''B'' = ''A''}}}}, as required by the distributivity.\n\nThis field is called a [[finite field]] with four elements, and is denoted {{math|'''F'''<sub>4</sub>}} or {{math|GF(4)}}.<ref>{{harvtxt|Lidl|Niederreiter|2008|loc=Example 1.62}}</ref> The subset consisting of {{math|''O''}} and {{math|''I''}} (highlighted in red in the tables at the right) is also a field, known as the ''[[binary field]]'' {{math|'''F'''<sub>2</sub>}} or {{math|GF(2)}}. In the context of [[computer science]] and [[Boolean algebra]], {{math|''O''}} and {{math|''I''}} are often denoted respectively by ''false'' and ''true'', the addition is then denoted [[XOR]] (exclusive or), and the multiplication is denoted [[Bitwise AND|AND]]. In other words, the structure of the binary field is the basic structure that allows computing with [[bit]]s.\n\n==Elementary notions==\nIn this section, {{math|''F''}} denotes an arbitrary field and {{math|''a''}} and {{math|''b''}} are arbitrary [[element (set theory)|elements]] of {{math|''F''}}.\n\n===Consequences of the definition===\nOne has {{math|1=''a'' · 0 = 0}} and {{math|1=−''a'' = (−1) · ''a''}}.<ref>{{harvtxt|Beachy|Blair|2006|loc=p. 120, Ch. 3}}</ref> In particular, one may deduce the additive inverse of every element as soon as one knows {{math|–1}}.\n\nIf {{math|1=''ab'' = 0}} then {{math|1=''a''}} or {{math|''b''}} must be 0. Indeed, if ''a''&nbsp;&ne;&nbsp;0, then \n{{math|1=0 = ''a''<sup>–1</sup>⋅0 = ''a''<sup>–1</sup>(''ab'') = (''a''<sup>–1</sup>''a'')''b'' = ''b''}}. This means that every field is an [[integral domain]].\n\n===The additive and the multiplicative group of a field===\nThe axioms of a field {{math|''F''}} imply that it is an [[abelian group]] under addition. This group is called the [[additive group]] of the field, and is sometimes denoted by {{math|(''F'', +)}} when denoting it simply as {{math|''F''}} could be confusing.\n\nSimilarly, the ''nonzero'' elements of {{math|''F''}} form an abelian group under multiplication, called the [[multiplicative group]], and denoted by {{nowrap|(''F'' \\ {0}, ·)}} or just {{math|''F'' \\ {0}}} or {{math|''F''<sup>*</sup>}}. \n\nA field may thus be defined as set {{math|''F''}} equipped with two operations denoted as an addition and a multiplication such that {{math|''F''}} is an abelian group under addition, {{math|''F'' \\ {0}}} is an abelian group under multiplication (where 0 is the identity element of the addition), and multiplication is [[distributive property|distributive]] over addition.<ref group=nb>Equivalently, a field is an [[algebraic structure]] {{math|⟨''F'', +, ·, −, <sup>−1</sup>, 0, 1⟩}} of type {{math|⟨2, 2, 1, 1, 0, 0⟩}}, such that {{math|0<sup>−1</sup>}} is not defined, {{math|⟨''F'', +, –, 0⟩}} and \n{{math|⟨''F'' ∖ {0}, ·, <sup>−1</sup>⟩}} are abelian groups, and \n· is distributive over +. {{harvtxt|Wallace|1998|loc=Th. 2}}</ref> Some elementary statements about fields can therefore be obtained by applying general facts of [[group (mathematics)|groups]]. For example, the additive and multiplicative inverses {{math|−''a''}} and {{math|''a''<sup>−1</sup>}} are uniquely determined by {{math|''a''}}.\n\nThe requirement {{math|1 ≠ 0}} follows, because 1 is the identity element of a group that does not contain 0.<ref>{{harvtxt|Sharpe|1987|loc=Theorem 1.3.2}}</ref> Thus, the [[trivial ring]], consisting of a single element, is not a field.\n\nEvery finite subgroup of the multiplicative group of a field is [[cyclic group|cyclic]] (see {{slink|Root of unity|Cyclic groups}}).\n\n===Characteristic===\nIn addition to the multiplication of two elements of ''F'', it is possible to define the product {{math|''n'' ⋅ ''a''}} of an arbitrary element {{math|''a''}} of {{math|''F''}} by a positive [[integer]] {{math|''n''}} to be the {{math|''n''}}-fold sum\n:{{math|''a'' + ''a'' + ... + ''a''}} (which is an element of  {{math|''F''}}.)\n\nIf there is no positive integer such that\n:{{math|1=''n'' ⋅ 1 = 0}},\nthen {{math|''F''}} is said to have [[characteristic (algebra)|characteristic]] 0.<ref>{{harvtxt|Adamson|2007|loc=§I.2, p.&nbsp;10}}</ref> For example, the field of rational rumbers {{math|'''Q'''}} has characteristic 0 since no positive integer {{math|''n''}} is zero. Otherwise, if there ''is'' a positive integer {{math|''n''}} satisfying this equation, the smallest such positive integer can be shown to be a [[prime number]]. It is usually denoted by {{math|''p''}} and the field is said to have characteristic {{math|''p''}} then.\nFor example, the field {{math|'''F'''<sub>4</sub>}} has characteristic 2 since (in the notation of the above addition table) {{math| 1= I + I = O }}.\n\nIf {{math|''F''}} has characteristic {{math|''p''}}, then {{math|1=''p'' ⋅ ''a'' = 0}} for all {{math|''a''}} in {{math|''F''}}. This implies that \n:{{math|1=(''a'' + ''b'')<sup>''p''</sup> = ''a''<sup>''p''</sup> + ''b''<sup>''p''</sup>}},\nsince all other [[binomial coefficient]]s appearing in the [[binomial formula]] are divisible by {{math|''p''}}. Here, {{math|1=''a''<sup>''p''</sup> := ''a'' ⋅ ''a'' ⋅ ... ⋅ ''a''}} ({{math|''p''}} factors) is the {{math|''p''}}-th power, i.e., the {{math|''p''}}-fold product of the element {{math|''a''}}. Therefore, the [[Frobenius map]]\n:{{math|Fr: ''F'' &rarr; ''F'', ''x'' ⟼ ''x''<sup>''p''</sup>}}\nis compatible with the addition in {{math|''F''}} (and also with the multiplication), and is therefore a field homomorphism.<ref>{{harvtxt|Escofier|2012|loc=14.4.2}}</ref> The existence of this homomorphism makes fields in characteristic {{math|''p''}} quite different from fields of characteristic 0.\n\n===Subfields and prime fields===\nA ''[[Field extension|subfield]]'' {{math|''E''}} of a field {{math|''F''}} is a subset of {{math|''F''}} that is a field with respect to the field operations of {{math|''F''}}. Equivalently {{math|''E''}} is a subset of {{math|''F''}} that contains {{math|1}}, and is closed under addition, multiplication, additive inverse and multiplicative inverse of a nonzero element. This means that {{math|1 ∊ ''E''}}, that for all {{math|''a'', ''b'' ∊ ''E''}} both {{math|''a'' + ''b''}} and {{math|''a'' · ''b''}} are in {{math|''E''}}, and that for all {{math|''a'' ≠ 0}} in {{math|''E''}}, both {{math|–''a''}} and {{math|1/''a''}} are in {{math|''E''}}.\n\n[[Field homomorphism]]s are maps {{math|''f'': ''E'' &rarr; ''F''}} between two fields such that {{math|1=''f''(''e''<sub>1</sub> + ''e''<sub>2</sub>) = ''f''(''e''<sub>1</sub>) + ''f''(''e''<sub>2</sub>)}}, {{math|1=''f''(''e''<sub>1</sub>''e''<sub>2</sub>) = ''f''(''e''<sub>1</sub>)''f''(''e''<sub>2</sub>)}}, and {{math|1=''f''(1<sub>E</sub>) = 1<sub>F</sub>}}, where {{math|''e''<sub>1</sub>}} and {{math|''e''<sub>2</sub>}} are arbitrary elements of {{math|''E''}}. All field homomorphisms are [[injective]].<ref>{{harvtxt|Adamson|2007|loc=section I.3}}</ref> If {{math|''f''}} is also [[surjective]], it is called an isomorphism (or the fields {{math|''E''}} and {{math|''F''}} are called isomorphic).\n\nA field is called a [[prime field]] if it has no proper (i.e., strictly smaller) subfields. Any field {{math|''F''}} contains a prime field. If the characteristic of {{math|''F''}} is {{math|''p''}} (a prime number), the prime field is isomorphic to the finite field {{math|'''F'''<sub>''p''</sub>}} introduced below. Otherwise the prime field is isomorphic to {{math|'''Q'''}}.<ref>{{harvtxt|Adamson|2007|loc=p. 12}}</ref>\n\n==Finite fields==\n{{main|Finite field}}\n''Finite fields'' (also called ''Galois fields'') are fields with finitely many elements, whose number is also referred to as the order of the field. The above introductory example {{math|'''F'''<sub>4</sub>}} is a field with four elements. Its subfield {{math|'''F'''<sub>2</sub>}} is the smallest field, because by definition a field has at least two distinct elements 1 ≠ 0.\n\n[[File:Clock_group.svg|thumb|In modular arithmetic modulo&nbsp;12, 9&nbsp;+&nbsp;4&nbsp;=&nbsp;1 since 9&nbsp;+&nbsp;4&nbsp;=&nbsp;13 in {{math|'''Z'''}}, which divided by 12 leaves remainder&nbsp;1. However, {{math|'''Z'''/12'''Z'''}} is not a field because 12 is not a prime number.]]\nThe simplest finite fields, with prime order, are most directly accessible using [[modular arithmetic]]. For a fixed positive integer {{math|''n''}}, arithmetic \"modulo {{math|''n''}}\" means to work with the numbers\n:{{math|1='''Z'''/''n'''''Z''' = {0, 1, ..., ''n'' − 1}.}}\nThe addition and multiplication on this set are done by performing the operation in question in the set {{math|'''Z'''}} of integers, dividing by {{math|''n''}} and taking the remainder as result. This construction yields a field precisely if {{math|''n''}} is a [[prime number]]. For example, taking the prime {{math|1=''n''&nbsp;=&nbsp;2}} results in the above-mentioned field {{math|'''F'''<sub>2</sub>}}. For {{math|1=''n''&nbsp;=&nbsp;4}} and more generally, for any [[composite number]] (i.e., any number {{math|''n''}} which can be expressed as a product {{math|1=''n''&nbsp;=&nbsp;''r''⋅''s''}} of two strictly smaller natural numbers), {{math|1='''Z'''/''n'''''Z'''}} is not a field: the product of two non-zero elements is zero since {{math|1=''r''⋅''s''&nbsp;=&nbsp;0}} in {{math|'''Z'''/''n'''''Z'''}}, which, as was explained [[#Consequences of the definition|above]], prevents {{math|'''Z'''/''n'''''Z'''}} from being a field. The field {{math|'''Z'''/''p'''''Z'''}} with {{math|''p''}} elements ({{math|''p''}} being prime) constructed in this way is usually denoted by  {{math|'''F'''<sub>''p''</sub>}}.\n\nEvery finite field {{math|''F''}} has {{math|1=''q''&nbsp;=&nbsp;''p''<sup>''n''</sup>}} elements, where {{math|1=''p''}} is prime and {{math|''n''&nbsp;&ge;&nbsp;1}}. This statement holds since {{math|''F''}} may be viewed as a [[vector space]] over its prime field. The [[dimension of a vector space|dimension]] of this vector space is necessarily finite, say {{math|''n''}}, which implies the asserted statement.<ref>{{harvtxt|Lidl|Niederreiter|2008|loc=Lemma 2.1, Theorem 2.2}}</ref>\n\nA field with {{math|1=''q''&nbsp;=&nbsp;''p''<sup>''n''</sup>}} elements can be constructed as the [[splitting field]] of the polynomial \n:{{math|1=''f''(''x'') = ''x''<sup>''q''</sup> &minus; ''x''}}.\nSuch a splitting field is an extension of {{math|'''F'''<sub>''p''</sub>}} in which the polynomial {{math|''f''}} has {{math|''q''}} zeros. This means {{math|''f''}} has as many zeros as possible since the [[degree of a polynomial|degree]] of {{math|''f''}} is {{math|''q''}}. For {{math|1=''q''&nbsp;=&nbsp;2<sup>2</sup>&nbsp;=&nbsp;4}}, it can be checked case by case using the above multiplication table that all four elements of {{math|'''F'''<sub>4</sub>}} satisfy the equation {{math|1=''x''<sup>4</sup>&nbsp;=&nbsp;''x''}}, so they are zeros of {{math|''f''}}. By contrast, in {{math|'''F'''<sub>2</sub>}}, {{math|''f''}} has only two zeros (namely 0 and 1), so {{math|''f''}} does not split into linear factors in this smaller field. Elaborating further on basic field-theoretic notions, it can be shown that two finite fields with the same order are isomorphic.<ref>{{harvtxt|Lidl|Niederreiter|2008|loc=Theorem 1.2.5}}</ref> It is thus customary to speak of ''the'' finite field with {{math|''q''}} elements, denoted by {{math|'''F'''<sub>''q''</sub>}} or {{math|GF(''q'')}}.\n\n==History==\nHistorically, three algebraic disciplines led to the concept of a field: the question of solving polynomial equations, [[algebraic number theory]], and [[algebraic geometry]].<ref>{{harvtxt|Kleiner|2007|loc=p. 63}}</ref> A first step towards the notion of a field was made in 1770 by [[Joseph-Louis Lagrange]], who observed that permuting the zeros {{math|''x''<sub>1</sub>, ''x''<sub>2</sub>, ''x''<sub>3</sub>}} of a [[cubic polynomial]] in the expression\n:{{math|(''x''<sub>1</sub> + &omega;''x''<sub>2</sub> + &omega;<sup>2</sup>''x''<sub>3</sub>)<sup>3</sup>}}\n(with {{math|&omega;}} being a third [[root of unity]]) only yields two values. This way, Lagrange conceptually explained the classical solution method of [[Scipione del Ferro]] and [[François Viète]], which proceeds by reducing a cubic equation for an unknown {{math|''x''}} to an quadratic equation for {{math|''x''<sup>3</sup>}}.<ref>{{harvtxt|Kiernan|1971|loc=p. 50}}</ref> Together with a similar observation for [[quartic polynomial|equations of degree 4]], Lagrange thus linked what eventually became the concept of fields and the concept of groups.<ref>{{harvtxt|Bourbaki|1994|loc=pp. 75–76}}</ref> [[Alexandre-Théophile Vandermonde|Vandermonde]], also in 1770, and to a fuller extent, [[Carl Friedrich Gauss]], in his ''[[Disquisitiones Arithmeticae]]'' (1801), studied the equation\n:{{math|1=''x''<sup>''p''</sup> = 1}}\nfor a prime {{math|''p''}} and, again using modern language, the resulting cyclic [[Galois group]]. Gauss deduced that a [[regular polygon|regular {{math|''p''}}-gon]] can be constructed if {{math|1=''p''&nbsp;=&nbsp;2<sup>2<sup>''k''</sup></sup>&nbsp;+&nbsp;1}}. Building on Lagrange's work, [[Paolo Ruffini]] claimed (1799) that [[quintic equation]]s (polynomial equations of degree 5) cannot be solved algebraically, however his arguments were flawed. These gaps were filled by [[Niels Henrik Abel]] in 1824.<ref>{{harvtxt|Corry|2004|loc=p.24}}</ref> [[Évariste Galois]], in 1832, devised necessary and sufficient criteria for a polynomial equation to be algebraically solvable, thus establishing in effect what is known as [[Galois theory]] today. Both Abel and Galois worked with what is today called an [[algebraic number field]], but conceived neither an explicit notion of a field, nor of a group.\n\nIn 1871 [[Richard Dedekind]] introduced, for a set of real or complex numbers that is closed under the four arithmetic operations, the [[German (language)|German]] word ''Körper'', which means \"body\" or \"corpus\" (to suggest an organically closed entity). The English term \"field\" was introduced by {{harvtxt|Moore|1893}}.<ref>[http://jeff560.tripod.com/f.html ''Earliest Known Uses of Some of the Words of Mathematics (F)'']</ref>\n\n{{Quote|text=By a field we will mean every infinite system of real or complex numbers so closed in itself and perfect that addition, subtraction, multiplication, and division of any two of these numbers again yields a number of the system.\n|author=Richard Dedekind, 1871<ref>{{harvtxt|Dirichlet|1871|loc=p. 42}}, translation by {{harvtxt|Kleiner|2007|loc=p. 66}}</ref>}}\n\nIn 1881 [[Leopold Kronecker]] defined what he called a ''domain of rationality'', which is a field of [[rational fraction]]s in modern terms. Kronecker's notion did not cover the field of all algebraic numbers (which is a field in Dedekind's sense), but on the other hand was more abstract than Dedekind's in that it made no specific assumption on the nature of the elements of a field. Kronecker interpreted a field such as {{math|'''Q'''(&pi;)}} abstractly as the rational function field {{math|'''Q'''(''X'')}}. Prior to this, examples of transcendental numbers were known since [[Joseph Liouville]]'s work in 1844, until [[Charles Hermite]] (1873) and [[Ferdinand von Lindemann]] (1882) proved the transcendence of {{math|''e''}} and {{math|&pi;}}, respectively.<ref>{{harvtxt|Bourbaki|1994|loc=p. 81}}</ref>\n\nThe first clear definition of an abstract field is due to {{harvtxt|Weber|1893}}.<ref>{{harvtxt|Corry|2004|loc=p. 33}}. See also {{harvtxt|Fricke|Weber|1924}}.</ref> In particular, [[Heinrich Martin Weber]]'s notion included the field '''F'''<sub>''p''</sub>. [[Giuseppe Veronese]] (1891) studied the field of formal power series, which led {{harvtxt|Hensel|1904}} to introduce the field of ''p''-adic numbers. {{harvtxt|Steinitz|1910}} synthesized the knowledge of abstract field theory accumulated so far. He axiomatically studied the properties of fields and defined many important field-theoretic concepts. The majority of the theorems mentioned in the sections [[#Galois theory|Galois theory]], [[#Constructing fields|Constructing fields]] and [[#Elementary notions|Elementary notions]] can be found in Steinitz's work. {{harvtxt|Artin|Schreier|1927}} linked the notion of [[ordered field|orderings in a field]], and thus the area of analysis, to purely algebraic properties.<ref>{{harvtxt|Bourbaki|1994|loc=p. 92}}</ref> [[Emil Artin]] redeveloped Galois theory from 1928 through 1942, eliminating the dependency on the [[primitive element theorem]].\n\n==Constructing fields==\n\n===Constructing fields from rings===\nA [[commutative ring]] is a set, equipped with an addition and multiplication operation, satisfying all the axioms of a field, except for the existence of multiplicative inverses {{math|''a''<sup>&minus;1</sup>}}.<ref>{{harvtxt|Lang|2002|loc=§II.1}}</ref> For example, the integers {{math|'''Z'''}} form a commutative ring, but not a field: the [[Multiplicative inverse|reciprocal]] of an integer {{math|''n''}} is not itself an integer, unless {{math|1=''n'' = ±1}}.\n\nIn the hierarchy of algebraic structures fields can be characterized as the commutative rings {{math|''R''}} in which every nonzero element is a [[unit (ring theory)|unit]] (which means every element is invertible). Similarly, fields are the commutative rings with precisely two distinct [[Ideal (ring theory)|ideal]]s, {{math|(0)}} and {{math|''R''}}. Fields are also precisely the commutative rings in which {{math|(0)}} is the only [[prime ideal]].\n\nGiven a commutative ring {{math|''R''}}, there are two ways to construct a field related to {{math|''R''}}, i.e., two ways of modifying {{math|''R''}} such that all nonzero elements become invertible: forming the field of fractions, and forming residue fields. The field of fractions of {{math|'''Z'''}} is {{math|'''Q'''}}, the rationals, while the residue fields of {{math|'''Z'''}} are the finite fields {{math|'''F'''<sub>''p''</sub>}}.\n\n====Field of fractions====\n\nGiven an [[integral domain]] {{math|''R''}}, its [[field of fractions]] {{math|''Q''(''R'')}} is built with the fractions of two elements of {{math|''R''}} exactly as '''Q''' is constructed from the integers. More precisely, the elements of {{math|''Q''(''R'')}} are the fractions {{math|''a''/''b''}} where {{math|''a''}} and {{math|''b''}}  are in {{math|''R''}}, and  {{math|''b'' ≠ 0}}. Two fractions {{math|''a''/''b''}} and  {{math|''c''/''d''}} are equal if and only if {{math|1=''ad'' = ''bc''}}. The operation on the fractions work exactly as for rational numbers. For example, \n:<math>\\frac{a}{b}+\\frac{c}{d} = \\frac{ad+bc}{bd}.</math>\nIt is straightforward to show that, if the ring is an integral domain, the set of the fractions form a field.<ref>{{harvtxt|Artin|1991|loc=Section 10.6}}</ref>\n\nThe field {{math|''F''(''x'')}} of the [[rational fraction]]s over a field (or an integral domain) {{math|''F''}} is the field of fractions of the [[polynomial ring]] {{math|''F''[''x'']}}. The field {{math|''F''((''x''))}} of [[Laurent series]]\n:<math>\\sum_{i=k}^\\infty a_i x^i \\ (k \\in \\Z, a_i \\in F)</math>\nover a field {{math|''F''}} is the field of fractions of the ring {{math|''F''<nowiki>[[</nowiki>''x'']]}} of [[formal power series]] (in which {{math|''k'' &ge; 0}}). Since any Laurent series is a fraction of a power series divided by a power of {{math|''x''}} (as opposed to an arbitrary power series), the representation of fractions is less important in this situation, though.\n\n====Residue fields====\nIn addition to the field of fractions, which embeds {{math|''R''}} [[injective map|injectively]] into a field, a field can be obtained from a commutative ring {{math|''R''}} by means of a [[surjective map]] onto a field {{math|''F''}}. Any field obtained in this way is a [[quotient ring|quotient]] {{math|{{nowrap|''R'' / ''m''}}}}, where {{math|''m''}} is a [[maximal ideal]] of {{math|''R''}}. If {{math|''R''}} [[local ring|has only one maximal ideal]] {{math|''m''}}, this field is called the [[residue field]] of {{math|''R''}}.<ref>{{harvtxt|Eisenbud|1995|loc=p. 60}}</ref>\n\nThe [[principal ideal|ideal generated by a single polynomial]] {{math|''f''}} in the polynomial ring {{math|1=''R'' = ''E''[''X'']}} (over a field ''E'') is maximal if and only if {{math|''f''}} is [[irreducible polynomial|irreducible]] in {{math|''E''}}, i.e., if {{math|''f''}} can not be expressed as the product of two polynomials in {{math|''E''[''X'']}} of smaller [[degree of a polynomial|degree]]. This yields a field\n:{{math|1={{nowrap begin}}''F'' = ''E''[''X''] / (''f''(''X'')).{{nowrap end}}}}\nThis field {{math|''F''}} contains an element {{math|''x''}} (namely the [[residue class]] of {{math|''X''}}) which satisfies the equation\n:{{math|1=''f''(''x'') = 0}}.\nFor example, {{math|'''C'''}} is obtained from {{math|'''R'''}} by [[adjunction (field theory)|adjoining]] the [[imaginary unit]] symbol i, which satisfies {{math|1=''f''(i) = 0}}, where {{math|1=''f''(''X'') = ''X''<sup>2</sup> + 1}}. Moreover, {{math|''f''}} is irreducible over {{math|'''R'''}}, which implies that the map that sends a polynomial {{math|''f''(''X'') ∊ '''R'''[''X'']}} to {{math|''f''(''i'')}} yields an isomorphism\n:<math>\\mathbf R[X]/\\left(X^2 + 1\\right) \\ \\stackrel \\cong \\longrightarrow \\ \\mathbf C.</math>\n\n===Constructing fields within a bigger field===\nFields can be constructed inside a given bigger container field. Suppose given a field {{math|''E''}}, and a field {{math|''F''}} containing {{math|''E''}} as a subfield. For any element {{math|''x''}} of {{math|''F''}}, there is a smallest subfield of {{math|''F''}} containing {{math|''E''}} and {{math|''x''}}, called the subfield of ''F'' generated by {{math|''x''}} and denoted {{math|''E''(''x'')}}.<ref>{{harvtxt|Jacobson|2009|loc=p. 213}}</ref> The passage from {{math|''E''}} to {{math|''E''(''x'')}} is referred to by ''[[adjunction (field theory)|adjoining]] an element'' to ''E''. More generally, for a subset {{math|''S'' ⊂ ''F''}}, there is a minimal subfield of {{math|''F''}} containing {{math|''E''}} and {{math|''S''}}, denoted by {{math|''E''(''S'')}}.\n\nThe [[compositum]] of two subfields {{math|''E''}} and {{math|''E' ''}} of some field {{math|''F''}} is the smallest subfield of {{math|''F''}} containing both {{math|''E''}} and {{math|''E'.''}} The compositum can be used to construct the biggest subfield of {{math|''F''}} satisfying a certain property, for example the biggest subfield of {{math|''F''}}, which is, in the language introduced below, algebraic over {{math|''E''}}.<ref group=nb>Further examples include the maximal [[unramified extension]] or the maximal [[abelian extension]] within {{math|''F''}}.</ref>\n\n===Field extensions===\n{{See|Glossary of field theory}}\nThe notion of a subfield {{math|''E'' ⊂ ''F''}} can also be regarded from the opposite point of view, by referring to {{math|''F''}} being a ''[[field extension]]'' (or just extension) of {{math|''E''}}, denoted by\n:{{math|''F'' / ''E''}},\nand read \"{{math|''F''}} over {{math|''E''}}\".\n\nA basic datum of a field extension is its [[degree of a field extension|degree]] {{math|[''F'' : ''E'']}}, i.e., the dimension of {{math|''F''}} as an {{math|''E''}}-vector space. It satisfies the formula<ref>{{harvtxt|Artin|1991|loc=Theorem 13.3.4}}</ref>\n:{{math|1=[''G'' : ''E''] = [''G'' : ''F''] [''F'' : ''E'']}}.\nExtensions whose degree is finite are referred to as finite extensions. The extensions {{math|'''C''' / '''R'''}} and {{math|'''F'''<sub>4</sub> / '''F'''<sub>2</sub>}} are of degree 2, whereas {{math|'''R''' / '''Q'''}} is an infinite extension.\n\n====Algebraic extensions====\nA pivotal notion in the study of field extensions {{math|''F'' / ''E''}} are [[algebraic element]]s. An element {{tmath|x\\in F}} is ''algebraic'' over {{mvar|E}} if it is a [[zero of a function|root]] of a [[polynomial]] with coefficients in {{mvar|E}}, that is, if it satisfies a [[polynomial equation]]\n:{{math|1=''e''<sub>''n''</sub>''x''<sup>''n''</sup> + ''e''<sub>''n''−1</sub>''x''<sup>''n''−1</sup> + ··· + ''e''<sub>1</sub>''x'' + ''e''<sub>0</sub> = 0}},\nwith {{math|''e''<sub>''n''</sub>, ..., ''e''<sub>0</sub>}} in {{mvar|E}}, and {{math|''e''<sub>''n''</sub> &ne; 0}}. \nFor example, the [[imaginary unit]] {{math|''i''}} in  {{math|'''C'''}} is algebraic over {{math|'''R'''}}, and even over {{math|'''Q'''}}, since it satisfies the equation\n:{{math|1=''i''<sup>2</sup> + 1 = 0}}.\nA field extension in which every element of {{math|''F''}} is algebraic over {{math|''E''}} is called an [[algebraic extension]]. Any finite extension is necessarily algebraic, as can be deduced from the above multiplicativity formula.<ref>{{harvtxt|Artin|1991|loc=Corollary 13.3.6}}</ref>\n\nThe subfield {{math|''E''(''x'')}} generated by an element {{math|''x''}}, as above, is an algebraic extension of {{math|''E''}} if and only if {{math|''x''}} is an algebraic element. That is to say, if {{math|''x''}} is algebraic, all other elements of {{math|''E''(''x'')}} are necessarily algebraic as well. Moreover, the degree of the extension {{math|''E''(''x'') / ''E''}}, i.e., the dimension of {{math|''E''(''x'')}} as an {{math|''E''}}-vector space, equals the minimal degree {{math|''n''}} such that there is a polynomial equation involving {{math|''x''}}, as above. If this degree is {{math|''n''}}, then the elements of {{math|''E''(''x'')}} have the form \n:<math>\\sum_{k=0}^{n-1} a_k x^k, \\ \\ a_k \\in E.</math>\n\nFor example, the field {{math|'''Q'''(''i'')}} of [[Gaussian rational]]s is the subfield of {{math|'''C'''}} consisting of all numbers of the form {{math|''a'' + ''bi''}} where both {{math|''a''}} and {{math|''b''}} are rational numbers: summands of the form {{math|''i''<sup>2</sup>}} (and similarly for higher exponents) don't have to be considered here, since {{math|''a'' + ''bi'' + ''ci''<sup>2</sup>}} can be simplified to {{math|''a'' &minus; ''c'' + ''bi''}}.\n\n====Transcendence bases====\nThe above-mentioned field of [[rational fraction]]s {{math|''E''(''X'')}}, where {{math|''X''}} is an [[indeterminate (variable)|indeterminate]], is not an algebraic extension of {{math|''E''}} since there is no polynomial equation with coefficients in {{math|''E''}} whose zero is {{math|''X''}}. Elements, such as {{math|''X''}}, which are not algebraic are called [[Algebraic element|transcendental]]. Informally speaking, the indeterminate {{math|''X''}} and its powers do not interact with elements of {{math|''E''}}. A similar construction can be carried out with a set of indeterminates, instead of just one.\n\nOnce again, the field extension {{math|''E''(''x'') / ''E''}} discussed above is a key example: if {{math|''x''}} is not algebraic (i.e., {{math|''x''}} is not a [[root of a function|root]] of a polynomial with coefficients in {{math|''E''}}), then {{math|''E''(''x'')}} is isomorphic to {{math|''E''(''X'')}}. This isomorphism is obtained by substituting {{math|''x''}} to {{math|''X''}} in rational fractions.\n\nA subset {{math|''S''}} of a field {{math|''F''}} is a [[transcendence basis]] if it is [[algebraically independent]] (don't satisfy any polynomial relations) over {{math|''E''}} and if {{math|''F''}} is an algebraic extension of {{math|''E''(''S'')}}. Any field extension {{math|''F'' / ''E''}} has a transcendence basis.<ref>{{harvtxt|Bourbaki|1988|loc=Chapter V, §14, No. 2, Theorem 1}}</ref> Thus, field extensions can be split into ones of the form {{math|''E''(''S'') / ''E''}} ([[transcendental extension|purely transcendental extensions]]) and algebraic extensions.\n\n===Closure operations===\nA field is [[algebraically closed]] if it does not have any strictly bigger algebraic extensions or, equivalently, if any [[polynomial equation]]\n:{{math|1=''f''<sub>''n''</sub>''x''<sup>''n''</sup> + ''f''<sub>''n''−1</sub>''x''<sup>''n''−1</sup> + ··· + ''f''<sub>1</sub>''x'' + ''f''<sub>0</sub> = 0}}, with [[coefficient]]s {{math|''f''<sub>''n''</sub>, ..., ''f''<sub>0</sub> ∈ ''F'', ''n'' > 0}},\nhas a solution {{math|''x'' ∊ ''F''}}.<ref>{{harvtxt|Artin|1991|loc=Section 13.9}}</ref> By the [[fundamental theorem of algebra]], {{math|'''C'''}} is algebraically closed, i.e., ''any'' polynomial equation with complex coefficients has a complex solution. The rational and the real numbers are ''not'' algebraically closed since the equation\n:{{math|1=''x''<sup>2</sup> + 1 = 0}}\ndoes not have any rational or real solution. A field containing {{math|''F''}} is called an ''[[algebraic closure]]'' of {{math|''F''}} if it is [[algebraic extension|algebraic]] over {{math|''F''}} (roughly speaking, not too big compared to {{math|''F''}}) and is algebraically closed (big enough to contain solutions of all polynomial equations).\n\nBy the above, {{math|'''C'''}} is an algebraic closure of {{math|'''R'''}}. The situation that the algebraic closure is a finite extension of the field {{math|''F''}} is quite special: by the [[Artin-Schreier theorem]], the degree of this extension is necessarily 2, and {{math|''F''}} is [[elementarily equivalent]] to {{math|'''R'''}}. Such fields are also known as [[real closed field]]s.\n\nAny field {{math|''F''}} has an algebraic closure, which is moreover unique up to (non-unique) isomorphism. It is commonly referred to as ''the'' algebraic closure and denoted {{Overline|''F''}}. For example, the algebraic closure {{math|{{Overline|'''Q'''}}}} of {{math|'''Q'''}} is called the field of [[algebraic number]]s. The field {{Overline|''F''}} is usually rather implicit since its construction requires the [[ultrafilter lemma]], a set-theoretic axiom that is weaker than the [[axiom of choice]].<ref>{{harvtxt|Banaschewski|1992}}. [https://mathoverflow.net/questions/46566/is-the-statement-that-every-field-has-an-algebraic-closure-known-to-be-equivalent Mathoverflow post]</ref> In this regard, the algebraic closure of {{math|'''F'''<sub>''q''</sub>}}, is exceptionally simple. It is the union of the finite fields containing {{math|'''F'''<sub>''q''</sub>}} (the ones of order {{math|''q''<sup>''n''</sup>}}). For any algebraically closed field {{math|''F''}} of characteristic 0, the algebraic closure of the field {{math|''F''((''t''))}} of [[Laurent series]] is the field of [[Puiseux series]], obtained by adjoining roots of {{math|''t''}}.<ref>{{harvtxt|Ribenboim|1999|loc=p. 186, §7.1}}</ref>\n\n==Fields with additional structure==\nSince fields are ubiquitous in mathematics and beyond, several refinements of the concept have been adapted to the needs of particular mathematical areas.\n\n===Ordered fields===\n{{main|Ordered field}}\nA field ''F'' is called an ''ordered field'' if any two elements can be compared, so that {{math|''x''&nbsp;+&nbsp;''y''&nbsp;&ge;&nbsp;0}} and {{math|''xy''&nbsp;&ge;&nbsp;0}} whenever {{math|''x''&nbsp;&ge;&nbsp;0}} and {{math|''y''&nbsp;&ge;&nbsp;0}}. For example, the reals form an ordered field, with the usual ordering&nbsp;&ge;. The [[Artin-Schreier theorem]] states that a field can be ordered if and only if it is a [[formally real field]], which means that any quadratic equation\n:<math>x_1^2 + x_2^2 + \\dots + x_n^2 = 0</math>\nonly has the solution {{math|1=''x''<sub>1</sub> = ''x''<sub>2</sub> = ... = ''x''<sub>''n''</sub> = 0}}.<ref>{{harvtxt|Bourbaki|1988|loc=Chapter VI, §2.3, Corollary 1}}</ref> The set of all possible orders on a fixed field ''F'' is isomorphic to the set of [[ring homomorphism]]s from the [[Witt ring (forms)|Witt ring]] W(''F'') of [[quadratic form]]s over ''F'', to '''Z'''.<ref>{{harvtxt|Lorenz|2008|loc=§22, Theorem 1}}</ref>\n\nAn [[Archimedean field]] is an ordered field such that for each element there exists a finite expression\n:{{nowrap|1 + 1 + ··· + 1}}\nwhose value is greater than that element, that is, there are no infinite elements.  Equivalently, the field contains no [[infinitesimals]] (elements smaller than all rational numbers); or, yet equivalent, the field is isomorphic to a subfield of {{math|'''R'''}}.\n\n[[File:Illustration of supremum.svg|thumb|300px|Each bounded real set has a least upper bound.]]\nAn ordered field is [[Dedekind-complete]] if all [[upper bound]]s, [[lower bound]]s (see [[Dedekind cut]]) and limits, which should exist, do exist. More formally, each [[bounded set|bounded subset]] of {{math|''F''}} is required to have a least upper bound. Any complete field is necessarily Archimedean,<ref>{{harvtxt|Prestel|1984|loc=Proposition 1.22}}</ref> since in any non-Archimedean field there is neither a greatest infinitesimal nor a least positive rational, whence the sequence {{math|1/2, 1/3, 1/4, &hellip;}}, every element of which is greater than every infinitesimal, has no limit.\n\nSince every proper subfield of the reals also contains such gaps, {{math|'''R'''}} is the unique complete ordered field, up to isomorphism.<ref>{{harvtxt|Prestel|1984|loc=Theorem 1.23}}</ref> Several foundational results in [[calculus]] follow directly from this characterization of the reals.\n\nThe [[hyperreals]] {{math|'''R'''<sup>*</sup>}} form an ordered field that is not Archimedean. It is an extension of the reals obtained by including infinite and infinitesimal numbers. These are larger, respectively smaller than any real number. The hyperreals form the foundational basis of [[non-standard analysis]].\n\n===Topological fields===\nAnother refinement of the notion of a field is a [[topological field]], in which the set ''F'' is a [[topological space]], such that all operations of the field (addition, multiplication, the maps {{math|''a'' ↦ &minus;''a''}} and {{math|''a'' ↦ ''a''<sup>&minus;1</sup>}}) are [[continuous map]]s with respect to the topology of the space.<ref>{{harvtxt|Warner|1989|loc=Chapter 14}}</ref>\nThe topology of all the fields discussed below is induced from a [[metric (mathematics)|metric]], i.e., a function\n:{{math|''d'' : ''F'' × ''F'' → '''R''',}}\nthat measures a ''distance'' between any two elements of {{math|''F''}}.\n\nThe [[completion (metric space)|completion]] of {{math|''F''}} is another field in which, informally speaking, the \"gaps\" in the original field {{math|''F''}} are filled, if there are any. For example, any [[irrational number]] {{math|''x''}}, such as {{math|1=''x''&nbsp;=&nbsp;{{radic|2}}}}, is a \"gap\" in the rationals {{math|'''Q'''}} in the sense that it is a real number that can be approximated arbitrarily closely by rational numbers {{math|''p''/''q''}}, in the sense that distance of {{math|''x''}} and {{math|''p''/''q''}} given by the [[absolute value]] {{math|<nowiki>|</nowiki>''x''&nbsp;&minus;&nbsp;''p''/''q''<nowiki>|</nowiki>}} is as small as desired.\nThe following table lists some examples of this construction. The fourth column shows an example of a zero [[sequence]], i.e., a sequence whose limit (for {{math|''n''&nbsp;&rarr;&nbsp;∞}}) is zero.\n\n{| class=\"wikitable\"\n! Field !! Metric !! Completion !! zero sequence\n|-\n| {{math|{{math|'''Q'''}}}} || {{math|1=<nowiki>|</nowiki>''x'' &minus; ''y''<nowiki>|</nowiki>}} (usual [[absolute value]]) || '''R''' || {{math|1/''n''}}\n|-\n| {{math|'''Q'''}} \n|| obtained using the [[p-adic valuation|''p''-adic valuation]], for a prime number {{math|''p''}}\n|| {{math|{{math|'''Q'''}}<sub>''p''</sub>}} [[p-adic number|{{math|''p''}}-adic numbers]]\n|| {{math|''p''<sup>''n''</sup>}}\n|-\n| {{math|''F''(''t'')}} ({{math|''F''}} any field)\n|| obtained using the {{math|''t''}}-adic valuation\n|| {{math|''F''((''t''))}}\n|| {{math|''t''<sup>''n''</sup>}}\n|}\n\nThe field {{math|{{math|'''Q'''}}<sub>''p''</sub>}} is used in number theory and [[p-adic analysis|{{math|''p''}}-adic analysis]]. The algebraic closure {{math|{{overline|'''Q'''}}<sub>''p''</sub>}} carries a unique norm extending the one on {{math|'''Q'''<sub>''p''</sub>}}, but is not complete. The completion of this algebraic closure, however, is algebraically closed. Because of its rough analogy to the complex numbers, it is called the field of [[complex p-adic number]]s and is denoted by {{math|'''C'''<sub>''p''</sub>}}.<ref>{{harvtxt|Gouvêa|1997|loc=§5.7}}</ref>\n\n====Local fields====\nThe following topological fields are called ''[[local field]]s'':<ref>{{harvtxt|Serre|1979}}</ref><ref group=nb>Some authors also consider the fields {{math|'''R'''}} and {{math|'''C'''}} to be local fields. On the other hand, these two fields, also called Archimedean local fields, share little similarity with the local fields considered here, to a point that {{harvtxt|Cassels|1986|loc=p. vi}} calls them \"completely anomalous\".</ref>\n* finite extensions of {{math|'''Q'''<sub>''p''</sub>}} (local fields of characteristic zero)\n* finite extensions of {{math|'''F'''<sub>''p''</sub>((''t''))}}, the field of Laurent series over {{math|'''F'''<sub>''p''</sub>}} (local fields of characteristic {{math|''p''}}).\n\nThese two types of local fields share some fundamental similarities. In this relation, the elements {{math|''p'' ∈ '''Q'''<sub>''p''</sub>}} and {{math|''t'' ∈ '''F'''<sub>''p''</sub>((''t''))}} (referred to as [[uniformizer]]) correspond to each other. The first manifestation of this is at an elementary level: the elements of both fields can be expressed as power series in the uniformizer, with coefficients in {{math|'''F'''<sub>''p''</sub>}}. (However, since the addition in {{math|'''Q'''<sub>''p''</sub>}} is done using [[carry (arithmetic)|carry]]ing, which is not the case in {{math|'''F'''<sub>''p''</sub>((''t''))}}, these fields are not isomorphic.) The following facts show that this superficial similarity goes much deeper:\n* Any [[first order logic|first order]] statement that is true for almost all {{math|'''Q'''<sub>''p''</sub>}} is also true for almost all {{math|'''F'''<sub>''p''</sub>((''t''))}}. An application of this is the [[Ax-Kochen theorem]] describing zeros of homogeneous polynomials in {{math|'''Q'''<sub>''p''</sub>}}.\n* [[Splitting of prime ideals in Galois extensions|Tamely ramified extension]]s of both fields are in bijection to one another.\n* Adjoining arbitrary {{math|''p''}}-power roots of {{math|''p''}} (in {{math|'''Q'''<sub>''p''</sub>}}), respectively of {{math|''t''}} (in {{math|'''F'''<sub>''p''</sub>((''t''))}}), yields (infinite) extensions of these fields known as [[perfectoid field]]s. Strikingly, the Galois groups of these two fields are isomorphic, which is the first glimpse of a remarkable parallel between these two fields:<ref>{{harvtxt|Scholze|2014}}</ref>\n::<math>\\operatorname {Gal}\\left(\\mathbf Q_p\\left(p^{1/p^\\infty}\\right)\\right) \\cong \\operatorname {Gal}\\left(\\mathbf F_p((t))\\left(t^{1/p^\\infty}\\right)\\right).</math>\n\n===Differential fields===\n[[Differential field]]s are fields equipped with a [[derivation (abstract algebra)|derivation]], i.e., allow to take derivatives of elements in the field.<ref>{{harvtxt|van der Put|Singer|2003|loc=§1}}</ref> For example, the field '''R'''(''X''), together with the standard derivative of polynomials forms a differential field. These fields are central to [[differential Galois theory]], a variant of Galois theory dealing with [[linear differential equation]]s.\n\n==Galois theory==\n{{main|Galois theory}}\n\nGalois theory studies [[algebraic extension]]s of a field by studying the [[Symmetry group#Symmetry groups in general|symmetry]] in the arithmetic operations of addition and multiplication. An important notion in this area are [[finite extension|finite]] [[Galois extension]]s {{math|''F'' / ''E''}}, which are, by definition, those that are [[separable extension|separable]] and [[normal extension|normal]]. The [[primitive element theorem]] shows that finite separable extensions are necessarily [[simple extension|simple]], i.e., of the form\n:{{math|1=''F'' = ''E''[''X''] / ''f''(''X'')}},\nwhere {{math|''f''}} is an irreducible polynomial (as above).<ref>{{harvtxt|Lang|2002|loc=Theorem V.4.6}}</ref> For such an extension, being normal and separable means that all zeros of {{math|''f''}} are contained in {{math|''F''}} and that {{math|''f''}} has only simple zeros. The latter condition is always satisfied if {{math|''E''}} has characteristic 0.\n\nFor a finite Galois extension, the [[Galois group]] {{math|Gal(''F''/''E'')}} is the group of [[field automorphism]]s of {{math|''F''}} that are trivial on {{math|''E''}} (i.e., the [[bijection]]s {{math|σ : ''F'' → ''F''}} that preserve addition and multiplication and that send elements of {{math|''E''}} to themselves). The importance of this group stems from the [[fundamental theorem of Galois theory]], which constructs an explicit [[one-to-one correspondence]] between the set of [[subgroup]]s of {{math|Gal(''F''/''E'')}} and the set of intermediate extensions of the extension {{math|''F''/''E''}}.<ref>{{harvtxt|Lang|2002|loc=§VI.1}}</ref> By means of this correspondence, group-theoretic properties translate into facts about fields. For example, if the Galois group of a Galois extension as above is not [[solvable group|solvable]] (can not be built from [[abelian group]]s), then the zeros of {{math|''f''}} can ''not'' be expressed in terms of addition, multiplication, and radicals, i.e., expressions involving <math>\\sqrt[n]{\\ }</math>. For example, the [[symmetric group]]s {{math|S<sub>''n''</sub>}} is not solvable for {{math|''n''&ge;5}}. Consequently, as can be shown, the zeros of the following polynomials are not expressible by sums, products, and radicals. For the latter polynomial, this fact is known as the [[Abel–Ruffini theorem]]:\n:{{math|1=''f''(''X'') = ''X''<sup>5</sup> &minus; 4''X'' + 2}} (and {{math|1=''E'' = '''Q'''}}),<ref>{{harvtxt|Lang|2002|loc=Example VI.2.6}}</ref>\n:{{math|1=''f''(''X'') = ''X''<sup>''n''</sup> + ''a''<sub>''n''&minus;1</sub>''X''<sup>''n''&minus;1</sup> + ... + ''a''<sub>0</sub>}} (where {{math|''f''}} is regarded as a polynomial in {{math|''E''(''a''<sub>0</sub>, ..., ''a''<sub>''n''&minus;1</sub>)}}, for some indeterminates {{math|''a''<sub>''i''</sub>}}, {{math|''E''}} is any field, and {{math|''n'' &ge; 5}}).\n\nThe [[tensor product of fields]] is not usually a field. For example, a finite extension {{math|''F'' / ''E''}} of degree {{math|''n''}} is a Galois extension if and only if there is an isomorphism of {{math|''F''}}-algebras\n:{{math|''F''  ⊗<sub>''E''</sub> ''F'' ≅ ''F''<sup>''n''</sup>}}.\nThis fact is the beginning of [[Grothendieck's Galois theory]], a far-reaching extension of Galois theory applicable to algebro-geometric objects.<ref>{{harvtxt|Borceux|Janelidze|2001}}. See also [[Étale fundamental group]].</ref>\n\n==Invariants of fields==\n\nBasic invariants of a field {{math|''F''}} include the characteristic and the [[transcendence degree]] of {{math|''F''}} over its prime field. The latter is defined as the maximal number of elements in {{math|''F''}} that are algebraically independent over the prime field. Two algebraically closed fields {{math|''E''}} and {{math|''F''}} are isomorphic precisely if these two data agree.<ref>{{harvtxt|Gouvêa|2012|loc=Theorem 6.4.8}}</ref> This implies that any two [[uncountable]] algebraically closed fields of the same [[cardinality]] and the same characteristic are isomorphic. For example, {{math|{{overline|'''Q'''}}<sub>''p''</sub>, '''C'''<sub>''p''</sub>}} and {{math|'''C'''}} are isomorphic (but ''not'' isomorphic as topological fields).\n\n===Model theory of fields===\nIn [[model theory]], a branch of [[mathematical logic]], two fields {{math|''E''}} and {{math|''F''}} are called [[elementarily equivalent]] if every mathematical statement that is true for {{math|''E''}} is also true for {{math|''F''}} and conversely. The mathematical statements in question are required to be [[first-order logic|first-order]] sentences (involving 0, 1, the addition and multiplication). A typical example is\n:{{math|&phi;(''E'')}} = \"for any {{math|''n'' > 0}}, any polynomial of degree {{math|''n''}} in {{math|''E''}} has a zero in {{math|''E''}}\" (which amounts to saying that {{math|''E''}} is algebraically closed).\nThe [[Lefschetz principle]] states that {{math|'''C'''}} is elementarily equivalent to any algebraically closed field {{math|''F''}} of characteristic zero. Moreover, any fixed statement {{math|&phi;}} holds in {{math|'''C'''}} if and only if it holds in any algebraically closed field of sufficiently high characteristic.<ref>{{harvtxt|Marker|Messmer|Pillay|2006|loc=Corollary 1.2}}</ref>\n\nIf {{math|''U''}} is an [[ultrafilter]] on a set {{math|''I''}}, and {{math|''F''<sub>''i''</sub>}} is a field for every {{math|''i''}} in {{math|''I''}}, the [[ultraproduct]] of the {{math|''F''<sub>''i''</sub>}} with respect to {{math|''U''}} is a field.<ref>{{harvtxt|Schoutens|2002|loc=§2}}</ref> It is denoted by\n:{{math|ulim<sub>''i''&rarr;∞</sub> ''F''<sub>''i''</sub>}},\nsince it behaves in several ways as a limit of the fields {{math|''F''<sub>''i''</sub>}}: [[Łoś's theorem]] states that any first order statement that holds for all but finitely many {{math|''F''<sub>''i''</sub>}}, also holds for the ultraproduct. Applied to the above sentence {{math|&phi;}}, this shows that there is an isomorphism<ref group=nb>Both {{math|'''C'''}} and {{math|ulim<sub>''p''</sub> {{overline|'''F'''}}<sub>''p''</sub>}} are algebraically closed by Łoś's theorem. For the same reason, they both have characteristic zero. Finally, they are both uncountable, so that they are isomorphic.</ref>\n:<math>\\operatorname{ulim}_{p \\to \\infty} \\overline \\mathbf F_p \\cong \\mathbf C.</math>\nThe Ax–Kochen theorem mentioned above also follows from this and an isomorphism of the ultraproducts (in both cases over all primes {{math|''p''}})\n:{{math|ulim<sub>''p''</sub> '''Q'''<sub>''p''</sub> ≅ ulim<sub>''p''</sub> '''F'''<sub>''p''</sub>((''t''))}}.\nIn addition, model theory also studies the logical properties of various other types of fields, such as [[real closed field]]s or [[exponential field]]s (which are equipped with an exponential function  {{math|exp : ''F'' &rarr; ''F''<sup>x</sup>}}).<ref>{{harvtxt|Kuhlmann|2000}}</ref>\n\n===The absolute Galois group===\nFor fields that are not algebraically closed (or not separably closed), the [[absolute Galois group]] {{math|Gal(''F'')}} is fundamentally important: extending the case of finite Galois extensions outlined above, this group governs ''all'' finite separable extensions of {{math|''F''}}. By elementary means, the group {{math|Gal('''F'''<sub>''q''</sub>)}} can be shown to be the [[Prüfer group]], the [[profinite completion]] of {{math|'''Z'''}}. This statement subsumes the fact that the only algebraic extensions of {{math|Gal('''F'''<sub>''q''</sub>)}} are the fields {{math|Gal('''F'''<sub>''q''<sup>''n''</sup></sub>)}} for {{math|''n'' > 0}}, and that the Galois groups of these finite extensions are given by\n:{{math|1=Gal('''F'''<sub>''q''<sup>''n''</sup></sub> / '''F'''<sub>''q''</sub>) = '''Z'''/''n'''''Z'''}}.\nA description in terms of generators and relations is also known for the Galois groups of {{math|''p''}}-adic number fields (finite extensions of {{math|'''Q'''<sub>''p''</sub>}}).<ref>{{harvtxt|Jannsen|Wingberg|1982}}</ref>\n\n[[Galois representation|Representations of Galois groups]] and of related groups such as the [[Weil group]] are fundamental in many branches of arithmetic, such as the [[Langlands program]]. The cohomological study of such representations is done using [[Galois cohomology]].<ref>{{harvtxt|Serre|2002}}</ref> For example, the [[Brauer group]], which is classically defined as the group of [[central simple algebra|central simple {{math|''F''}}-algebras]], can be reinterpreted as a Galois cohomology group, namely\n:{{math|1=Br(''F'') = H<sup>2</sup>(''F'', '''G'''<sub>m</sub>)}}.\n\n===K-theory===\n[[Milnor K-theory]] is defined as\n:<math>K_n^M(F) = F^\\times \\otimes \\cdots \\otimes F^\\times / \\langle x \\otimes (1-x)\\mid x \\in F \\smallsetminus \\{0, 1\\} \\rangle.</math>\nThe [[norm residue isomorphism theorem]], proved around 2000 by [[Vladimir Voevodsky]], relates this to Galois cohomology by means of an isomorphism\n:<math>K_n^M(F) / p = H^n(F, \\mu_l^{\\otimes n}).</math>\n[[Algebraic K-theory]] is related to the group of [[invertible matrix|invertible matrices]] with coefficients the given field. For example, the process of taking the [[determinant (mathematics)|determinant]] of an invertible matrix leads to an isomorphism K<sub>1</sub>(''F'') = ''F''<sup>&times;</sup>. [[Matsumoto's theorem (K-theory)|Matsumoto's theorem]] shows that K<sub>2</sub>(''F'') agrees with K<sub>2</sub><sup>M</sup>(''F''). In higher degrees, K-theory diverges from Milnor K-theory and remains hard to compute in general.\n\n==Applications==\n\n===Linear algebra and commutative algebra===\n[[File:euler2a.gif|thumb|[[Euler angles]] express the relation of different coordinate systems, i.e., bases of {{math|'''R'''<sup>3</sup>}}. They are used in computer graphics.]]\nIf {{math|''a'' ≠ 0}}, then the [[equation]] \n:{{math|1=''ax'' = ''b''}}\nhas a unique solution {{math|''x''}} in {{math|''F''}}, namely {{math|1=''x'' = ''b''/''a''}}. This observation, which is an immediate consequence of the definition of a field, is the essential ingredient used to show that any [[vector space]] has a [[basis (linear algebra)|basis]].<ref>{{harvtxt|Artin|1991|loc=§3.3}}</ref> Roughly speaking, this allows choosing a coordinate system in any vector space, which is of central importance in [[linear algebra]] both from a theoretical point of view, and also for practical applications.\n\n[[module (mathematics)|Modules]] (the analogue of vector spaces) over most [[ring (mathematics)|ring]]s, including the ring {{math|'''Z'''}} of integers, have a more complicated structure. A particular situation arises when a ring {{math|''R''}} is a vector space over a field {{math|''F''}} in its own right. Such rings are called [[algebra over a field|{{math|''F''}}-algebras]] and are studied in depth in the area of [[commutative algebra]]. For example, [[Noether normalization]] asserts that any [[finitely generated algebra|finitely generated {{math|''F''}}-algebra]] is closely related to (more precisely, [[finitely generated module|finitely generated as a module]] over) a polynomial ring {{math|''F''[''x''<sub>1</sub>, ..., ''x''<sub>''n''</sub>]}}.<ref>{{harvtxt|Eisenbud|1995|loc=Theorem 13.3}}</ref>\n\n===Finite fields: cryptography and coding theory===\n\n[[File:ECClines.svg|thumb|The sum of three points ''P'', ''Q'', and ''R'' on an elliptic curve ''E'' (red) is zero if there is a line (blue) passing through these points.]]\nA widely applied cryptographic routine uses the fact that discrete exponentiation, i.e., computing\n:{{math|1=''a''<sup>''n''</sup> = ''a'' ⋅ ''a'' ⋅ ... ⋅ ''a''}} ({{math|''n''}} factors, for an integer {{math|''n'' ≥ 1}})\nin a (large) finite field {{math|'''F'''<sub>''q''</sub>}} can be performed much more efficiently than the [[discrete logarithm]], which is the inverse operation, i.e., determining the solution {{math|''n''}} to an equation\n:{{math|1=''a''<sup>''n''</sup> = ''b''}}.\nIn [[elliptic curve cryptography]], the multiplication in a finite field is replaced by the operation of adding points on an [[elliptic curve]], i.e., the solutions of an equation of the form\n:{{math|1=''y''<sup>2</sup> = ''x''<sup>3</sup> + ''ax'' + ''b''}}.\n\nFinite fields are also used in [[coding theory]] and [[combinatorics]].\n\n===Geometry: field of functions===\n[[File:Double torus illustration.png|thumb|A compact Riemann surface of [[genus (mathematics)|genus]] two (two handles). The genus can be read off the field of meromorphic functions on the surface.]]\n[[function (mathematics)|Functions]] on a suitable [[topological space]] {{math|''X''}} into a field {{mvar|k}} can be added and multiplied pointwise, e.g., the product of two functions is defined by the product of their values within the domain:\n:{{math|1={{nowrap|1=(''f'' ⋅ ''g'')(''x'') = ''f''(''x'') ⋅ ''g''(''x'')}}}}.\nThis makes these functions a {{mvar|k}}-[[associative algebra|commutative algebra]].\n\nFor having a ''field'' of functions, one must consider algebras of functions that are [[integral domains]]. In this case the ratios of two functions, i.e., expressions of the form\n:<math>\\frac{f(x)}{g(x)},</math>\nform a field, called field of functions.\n\nThis occurs in two main cases. When {{math|''X''}} is a [[complex manifold]] {{math|''X''}}. In this case, one considers the algebra of [[holomorphic functions]], i.e., complex differentiable functions. Their ratios form the field of  [[meromorphic function]]s on {{math|''X''}}.\n\nThe [[function field of an algebraic variety|function field]] of an [[algebraic variety]] {{math|''X''}} (a geometric object defined as the common zeros of polynomial equations) consists of ratios of [[regular function]]s, i.e., ratios of polynomial functions on the variety. The function field of the {{math|''n''}}-dimensional [[affine space|space]] over a field {{math|''k''}} is {{math|''k''(''x''<sub>1</sub>, ..., ''x''<sub>''n''</sub>)}}, i.e., the field consisting of ratios of polynomials in {{math|''n''}} indeterminates. The function field of {{math|''X''}} is the same as the one of any [[Zariski topology|open]] dense subvariety. In other words, the function field is insensitive to replacing {{math|''X''}} by a (slightly) smaller subvariety.\n\nThe function field is invariant under [[isomorphism]] and [[birational equivalence]] of varieties. It is therefore an important tool for the study of [[abstract algebraic variety|abstract algebraic varieties]] and for the classification of algebraic varieties. For example, the [[dimension of an algebraic variety|dimension]], which equals the transcendence degree of {{math|''k''(''X'')}}, is invariant under birational equivalence.<ref>{{harvtxt|Eisenbud|1995|loc=§13, Theorem A}}</ref> For [[algebraic curve|curves]] (i.e., the dimension is one), the function field {{math|''k''(''X'')}} is very close to {{math|''X''}}: if {{math|''X''}} is [[smooth variety|smooth]] and [[proper map|proper]] (the analogue of being [[compact topological space|compact]]), {{math|''X''}} can be reconstructed, up to isomorphism, from its field of functions.<ref group=nb>More precisely, there is an [[equivalence of categories]] between smooth proper algebraic curves over an algebraically closed field {{math|''F''}} and finite field extensions of {{math|''F''(''T'')}}.</ref> In higher dimension the function field remembers less, but still decisive information about {{math|''X''}}. The study of function fields and their geometric meaning in higher dimensions is referred to as [[birational geometry]]. The [[minimal model program]] attempts to identify the simplest (in a certain precise sense) algebraic varieties with a prescribed function field.\n\n===Number theory: global fields===\n\n[[Global field]]s are in the limelight in [[algebraic number theory]] and [[arithmetic geometry]].\nThey are, by definition, [[number field]]s (finite extensions of {{math|'''Q'''}}) or function fields over {{math|'''F'''<sub>''q''</sub>}} (finite extensions of {{math|'''F'''<sub>''q''</sub>(''t'')}}). As for local fields, these two types of fields share several similar features, even though they are of characteristic 0 and positive characteristic, respectively. This [[function field analogy]] can help to shape mathematical expectations, often first by understanding questions about function fields, and later treating the number field case. The latter is often more difficult. For example, the [[Riemann hypothesis]] concerning the zeros of the [[Riemann zeta function]] (open as of 2017) can be regarded as being parallel to the [[Weil conjectures]] (proven in 1974 by [[Pierre Deligne]]).\n\n[[File:One5Root.svg|thumb|The fifth roots of unity form a [[regular pentagon]].]]\n[[Cyclotomic field]]s are among the most intensely studied number fields. They are of the form {{math|'''Q'''(ζ<sub>''n''</sub>)}}, where {{math|ζ<sub>''n''</sub>}} is a primitive {{math|''n''}}-th [[root of unity]], i.e., a complex number satisfying {{math|1={{nowrap begin}}ζ<sup>''n''</sup> = 1{{nowrap end}}}} and {{math|ζ<sup>''m''</sup> ≠ 1}} for all {{math|''m'' < ''n''}}.<ref>{{harvtxt|Washington|1997}}</ref> For {{math|''n''}} being a [[regular prime]], [[Ernst Kummer|Kummer]] used cyclotomic fields to prove [[Fermat's last theorem]], which asserts the non-existence of rational nonzero solutions to the equation\n:{{math|1=''x''<sup>''n''</sup> + ''y''<sup>''n''</sup> = ''z''<sup>''n''</sup>}}.\n\nLocal fields are completions of global fields. [[Ostrowski's theorem]] asserts that the only completions of {{math|'''Q'''}}, a global field, are the local fields {{math|'''Q'''<sub>''p''</sub>}} and {{math|'''R'''}}. Studying arithmetic questions in global fields may sometimes be done by looking at the corresponding questions locally. This technique is called the [[local-global principle]]. For example, the [[Hasse–Minkowski theorem]] reduces the problem of finding rational solutions of quadratic equations to solving these equations in {{math|'''R'''}} and {{math|'''Q'''<sub>''p''</sub>}}, whose solutions can easily be described.<ref>{{harvtxt|Serre|1978|loc=Chapter IV}}</ref>\n\nUnlike for local fields, the Galois groups of global fields are not known. [[Inverse Galois theory]] studies the (unsolved) problem whether any finite group is the Galois group {{math|Gal(''F''/'''Q''')}} for some number field {{math|''F''}}.<ref>{{harvtxt|Serre|1992}}</ref> [[Class field theory]] describes the [[abelian extension]]s, i.e., ones with abelian Galois group, or equivalently the abelianized Galois groups of global fields. A classical statement, the [[Kronecker–Weber theorem]], describes the maximal abelian {{math|'''Q'''<sup>ab</sup>}} extension of {{math|'''Q'''}}: it is the field\n:{{math|'''Q'''(&zeta;<sub>''n''</sub>, ''n'' &ge; 2)}}\nobtained by adjoining all primitive {{math|''n''}}-th roots of unity. [[Kronecker Jugendtraum|Kronecker's Jugendtraum]] asks for a similarly explicit description of {{math|''F''<sup>ab</sup>}} of general number fields {{math|''F''}}. For [[imaginary quadratic field]]s, <math>F=\\mathbf Q(\\sqrt{-d})</math>, {{math|''d'' > 0}}, the theory of [[complex multiplication]] describes {{math|''F''<sup>ab</sup>}} using [[elliptic curves]]. For general number fields, no such explicit description is known.\n\n==Related notions==\n\nIn addition to the additional structure that fields may enjoy, fields admit various other related notions. Since in any field 0 &ne; 1, any field has at least two elements. Nonetheless, there is a concept of [[field with one element]], which is suggested to be a limit of the finite fields {{math|'''F'''<sub>''p''</sub>}}, as {{math|''p''}} tends to 1.<ref>{{harvtxt|Tits|1957}}</ref> In addition to division rings, there are various other weaker algebraic structures related to fields such as [[quasifield]]s, [[Near-field (mathematics)|near-field]]s and [[semifield]]s.\n\nThere are also [[proper class]]es with field structure, which are sometimes called '''Fields''', with a capital F. The [[surreal number]]s form a Field containing the reals, and would be a field except for the fact that they are a proper class, not a set. The [[nimber]]s, a concept from [[game theory]] form a Field.<ref>{{harvtxt|Conway|1976}}</ref>\n\n===Division rings===\n[[File:Hairy_ball.png|thumb|The hairy ball theorem states that a ball can not be combed. More formally, there is no [[Continuous function|continuous]] [[tangent bundle|tangent vector field]] on the [[sphere]] {{math|S<sup>2</sup>}}, which is everywhere non-zero.]]\n\nDropping one or several axioms in the definition of a field leads to other algebraic structures. As was mentioned above, commutative rings satisfy all axioms of fields, except for multiplicative inverses. Dropping instead the condition that multiplication is commutative leads to the concept of a ''[[division ring]]'' or ''skew field''.<ref group=nb>Historically, division rings were sometimes referred to as fields, while fields were called ''commutative fields''.</ref> The only division rings that are finite-dimensional {{math|'''R'''}}-vector spaces are {{math|'''R'''}} itself, {{math|'''C'''}} (which is a field), the [[quaternion]]s {{math|'''H'''}} (in which multiplication is non-commutative), and the [[octonion]]s {{math|'''O'''}} (in which multiplication is neither commutative nor associative). This fact was proved using methods of [[algebraic topology]] in 1958 by [[Michel Kervaire]], [[Raoul Bott]], and [[John Milnor]].<ref>{{harvtxt|Baez|2002}}</ref> The non-existence of an odd-dimensional division algebra is more classical. It can be deduced from the [[hairy ball theorem]] illustrated at the right.{{citation needed|date=September 2018}}\n\n==Notes==\n{{reflist|group=nb}}\n\n{{reflist|30em}}\n\n==References==\n{{Wikibooks|Abstract algebra|Fields}}\n\n{{refbegin|30em}}\n* {{Citation|title=Introduction to Field Theory| last=Adamson|first=I. T.|isbn=978-0-486-46266-0|year=2007|publisher=Dover Publications}}\n* {{Citation | last1=Allenby | first1=R. B. J. T. | title=Rings, Fields and Groups | publisher=Butterworth-Heinemann | isbn=978-0-340-54440-2 | year=1991}}\n* {{Citation | last1=Artin | first1=Michael | author1-link=Michael Artin | title=Algebra | publisher=[[Prentice Hall]] | isbn=978-0-13-004763-2 | year=1991}}, especially Chapter 13\n* {{Citation|last1=Artin|first1=Emil|last2=Schreier|first2=Otto|author1-link=Emil Artin|author2-link=Otto Schreier|title=Eine Kennzeichnung der reell abgeschlossenen Körper|journal=Abhandlungen aus dem Mathematischen Seminar der Universität Hamburg|issn=0025-5858| volume=5|pages=225–231|year=1927|language=German|doi=10.1007/BF02952522|jfm=53.0144.01}}\n* {{Citation| last=Ax |first=James|authorlink=James Ax|year=1968|title=The elementary theory of finite fields|journal=Ann. of Math. |series= 2|volume=88|pages=239–271|doi=10.2307/1970573}}\n* {{Citation|last=Baez|first=John C.|authorlink=John C. Baez|title=The octonions|journal=[[Bulletin of the American Mathematical Society]] |volume=39|year=2002|pages=145–205|doi=10.1090/S0273-0979-01-00934-X|arxiv=math/0105155}}\n* {{Citation| last=Banaschewski|first=Bernhard|title=Algebraic closure without choice.|year=1992|journal=Z. Math. Logik Grundlagen Math.|volume=38|issue=4|pages=383–385|zbl=0739.03027}}\n* {{Citation|last1=Beachy|first1=John. A|last2=Blair|first2=William D.|title=Abstract Algebra|edition=3|publisher=Waveland Press|isbn=1-57766-443-4|year=2006}}\n* {{Citation | last1=Blyth | first1=T. S. | last2=Robertson | first2=E. F. |author2-link=Edmund F. Robertson| title=Groups, rings and fields: Algebra through practice| publisher=[[Cambridge University Press]] | year=1985}}. See especially Book 3 ({{ISBN|0-521-27288-2}}) and Book 6 ({{ISBN|0-521-27291-2}}).\n* {{Citation|last1=Borceux|first=Francis|last2=Janelidze|first2=George|title=Galois theories|\nisbn=0-521-80309-8|year=2001|publisher=Cambridge University Press|zbl=0978.12004}}\n* {{Citation|last1=Bourbaki|first1=Nicolas|authorlink=Nicolas Bourbaki|title=Elements of the history of mathematics|publisher=Springer|year=1994|isbn=3-540-19376-6|mr=1290116|doi=10.1007/978-3-642-61693-8}}\n* {{Citation|title=Algebra II. Chapters 4–7|last=Bourbaki|first=Nicolas|authorlink=Nicolas Bourbaki|isbn=0-387-19375-8|year=1988|publisher=Springer}}\n* {{Citation|last=Cassels|first=J. W. S.|authorlink=J. W. S. Cassels|title=Local fields|series=London Mathematical Society Student Texts|volume=3|publisher=Cambridge University Press|year=1986|isbn=0-521-30484-9|mr=861410|doi=10.1017/CBO9781139171885}}\n* {{Citation|title=Elements of Abstract Algebra|last=Clark|first=A.|isbn=978-0-486-64725-8|series=Dover Books on Mathematics Series|year=1984|publisher=Dover Publications}}\n* {{Citation\n|first1=John Horton\n|last1=Conway\n|authorlink1=John Horton Conway\n|title=[[On Numbers and Games]]\n|publisher=[[Academic Press]] Inc. (London) Ltd.\n|year=1976\n}}\n* {{Citation|last=Corry|first=Leo|authorlink=Leo Corry|title=Modern algebra and the rise of mathematical structures|edition=2nd|isbn=3-7643-7002-5|year=2004|publisher=Birkhäuser|zbl=1044.01008}}\n* {{Citation|last1=Dirichlet|first1=Peter Gustav Lejeune|authorlink=Peter Gustav Lejeune Dirichlet|editor-last=Dedekind|editor-first=Richard|editor-link=Richard Dedekind|year=1871|edition=2nd|language=German|volume=1|location=Braunschweig, Germany|publisher=Friedrich Vieweg und Sohn|title=Vorlesungen über Zahlentheorie (Lectures on Number Theory)|url=https://books.google.com/books?id=SRJTAAAAcAAJ&pg=PA424#v=onepage&q&f=false}}\n* {{Citation| last=Eisenbud|first=David|authorlink=David Eisenbud | title=Commutative algebra with a view toward algebraic geometry | location=New York | publisher=[[Springer-Verlag]] | series=[[Graduate Texts in Mathematics]] | volume=150 | year=1995 | mr=1322960 | isbn=0-387-94268-8 | doi=10.1007/978-1-4612-5350-1}}\n* {{Citation|last=Escofier|first=J. P.|isbn=978-1-4613-0191-2|title=Galois Theory|publisher=Springer|year=2012}}\n* {{Citation | last1= Fricke | first1= Robert|author1-link=Robert Fricke | last2= Weber | first2= Heinrich Martin | author2-link= Heinrich Martin Weber | title= Lehrbuch der Algebra | language=German|url= http://resolver.sub.uni-goettingen.de/purl?PPN234788267 | publisher= Vieweg | year= 1924 | jfm= 50.0042.03}}\n* {{Citation|title=''p''-adic numbers|\nlast=Gouvêa|first=Fernando Q.|authorlink=Fernando Q. Gouvêa|edition=2nd|year=1997|publisher=Springer|series=Universitext}}\n* {{Citation|title=A Guide to Groups, Rings, and Fields|\nlast=Gouvêa|first=Fernando Q.|authorlink=Fernando Q. Gouvêa|isbn=978-0-88385-355-9|year=2012|publisher=Mathematical Association of America}}\n* {{springer|title=Field|id=p/f040090}}\n* {{Citation|last1=Hensel|first1=Kurt|author1-link=Kurt Hensel|title=Über eine neue Begründung der Theorie der algebraischen Zahlen|journal=Journal für die Reine und Angewandte Mathematik|\nissn=0075-4102|volume=128|pages=1–32|year=1904|language=German|jfm=35.0227.01|url=https://eudml.org/doc/149187}}\n* {{Citation| last=Jacobson| first=Nathan| author-link=Nathan Jacobson| year=2009| title=Basic algebra| edition=2nd| volume = 1 | series= | publisher=Dover| isbn = 978-0-486-47189-1}}\n* {{Citation|mr=0679774|first1=Uwe|last1=Jannsen|first2=Kay|last2=Wingberg|title=Die Struktur der absoluten Galoisgruppe 𝔭-adischer Zahlkörper. [The structure of the absolute Galois group of 𝔭-adic number fields]|journal=Invent. Math.|volume=70|year=1982|issue=1|pages=71–98|url=http://epub.uni-regensburg.de/26689/|doi=10.1007/bf01393199|bibcode=1982InMat..70...71J}}\n* {{Citation|last=Kleiner|first=Israel|authorlink=Israel Kleiner (mathematician)|title=A history of abstract algebra|publisher=Birkhäuser|year=2007|isbn=978-0-8176-4684-4|mr=2347309|doi=10.1007/978-0-8176-4685-1}}\n* {{Citation|last=Kiernan|first=B. Melvin|title=The development of Galois theory from Lagrange to Artin|journal=Archive for History of Exact Sciences\n|volume=8|year=1971|number=1-2|pages=40–154|mr=1554154|doi=10.1007/BF00327219}}\n* {{Citation|last1=Kuhlmann|first=Salma|title=Ordered exponential fields|series=Fields Institute Monographs|volume=12|publisher=American Mathematical Society|year=2000|isbn=0-8218-0943-1|mr=1760173}}\n* {{Citation|first=Serge|last=Lang|authorlink=Serge Lang|title=Algebra|series=Graduate Texts in Mathematics|volume=211|edition=3rd|publisher=Springer|year=2002|isbn=0-387-95385-X|doi=10.1007/978-1-4613-0041-0}}\n* {{Citation|first=Rudolf|last=Lidl|first2=Harald|last2=Niederreiter|author2-link=Harald Niederreiter|title=Finite fields| edition=2nd|year=2008|isbn=978-0-521-06567-2|publisher=Cambridge University Press|zbl=1139.11053}}\n* {{Citation|last=Lorenz|first=Falko|title=Algebra, Volume II: Fields with Structures, Algebras and Advanced Topics|year=2008|isbn=978-0-387-72487-4|publisher=Springer}}\n* {{Citation|last1=Marker|first1=David|last2=Messmer|first2=Margit|last3=Pillay|first3=Anand|\ntitle=Model theory of fields|series=Lecture Notes in Logic|volume=5|edition=2nd|publisher=Association for Symbolic Logic|year=2006|isbn=978-1-56881-282-3|mr=2215060|citeseerx=10.1.1.36.8448}}\n* {{Citation|last1=Mines|first1=Ray|last2=Richman|first2=Fred|last3=Ruitenburg|first3=Wim|title=A course in constructive algebra|series=Universitext|publisher=Springer|year=1988|isbn=0-387-96640-4|mr=919949|doi=10.1007/978-1-4419-8640-5}}\n* {{citation\n | last = Moore | first = E. Hastings | authorlink = E. H. Moore\n | doi = 10.1090/S0002-9904-1893-00178-X\n | issue = 3\n | journal = [[Bulletin of the American Mathematical Society]]\n | mr = 1557275\n | pages = 73–78\n | title = A doubly-infinite system of simple groups\n | volume = 3\n | year = 1893}}\n* {{Citation|last=Prestel|first=Alexander|title=Lectures on formally real fields\n |series=Lecture Notes in Mathematics,    |volume=1093|publisher=Springer|year=1984|isbn=3-540-13885-4|mr=769847|doi=10.1007/BFb0101548}}\n* {{Citation|last=Ribenboim|first=Paulo|authorlink=Paulo Ribenboim|title=The theory of classical valuations|series=Springer Monographs in Mathematics|publisher=Springer|year=1999|isbn=0-387-98525-5|mr=1677964| doi=10.1007/978-1-4612-0551-7}}\n* {{Citation|last=Scholze|first=Peter|author-link=Peter Scholze|chapter=Perfectoid spaces and their Applications|year=2014|chapter-url=http://www.math.uni-bonn.de/people/scholze/ICM.pdf|title=Proceedings of the International Congress of Mathematicians 2014|url=http://www.icm2014.org/en/vod/proceedings.html|isbn=978-89-6105-804-9}}\n* {{Citation|last=Schoutens|first=Hans|isbn=978-3-642-13367-1|title=The Use of Ultraproducts in Commutative Algebra|year=2002|publisher=Springer|series=Lecture Notes in Mathematics|volume=1999}}\n* {{Citation|last=Serre|first=Jean-Pierre|authorlink=Jean-Pierre Serre|title=A course in arithmetic. Translation of ''Cours d'arithmetique''|edition=2nd|year=1978|series=Graduate Text in Mathematics|volume=7|publisher=Springer|zbl=0432.10001}}\n* {{Citation|last=Serre|first=Jean-Pierre|authorlink=Jean-Pierre Serre|title=Local fields|series=Graduate Texts in Mathematics|volume=67|publisher=Springer|year=1979|isbn=0-387-90424-7|mr=554237}}\n* {{Citation|first=Jean-Pierre|last=Serre|authorlink=Jean-Pierre Serre|title=Topics in Galois theory|isbn=0-86720-210-6|year=1992|publisher=Jones and Bartlett Publishers|zbl=0746.12001}}\n* {{Citation | last1=Serre | first1=Jean-Pierre | author1-link= Jean-Pierre Serre | title=Galois cohomology | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Springer Monographs in Mathematics | isbn=978-3-540-42192-4 | mr=1867431  | year=2002 | zbl=1004.12003 | others=Translated from the French by [[Patrick Ion]]}}\n* {{Citation|last=Sharpe|first=David|title=Rings and factorization|isbn=0-521-33718-6|year=1987|publisher=Cambridge University Press|zbl=0674.13008}}\n* {{Citation\n| last1=Steinitz | first1=Ernst | author1-link=Ernst Steinitz\n| title=Algebraische Theorie der Körper |trans-title=Algebraic Theory of Fields\n| url=http://resolver.sub.uni-goettingen.de/purl?GDZPPN002167042\n| year=1910\n| journal=[[Journal für die reine und angewandte Mathematik]]\n| issn=0075-4102 | volume=137 | pages=167–309\n| jfm=41.0445.03 | doi=10.1515/crll.1910.137.167\n}}\n* {{ Citation | last1 = Tits | first1 = Jacques | authorlink=Jacques Tits | chapter = Sur les analogues algébriques des groupes semi-simples complexes | title = Colloque d'algèbre supérieure, tenu à Bruxelles du 19 au 22 décembre 1956, Centre Belge de Recherches Mathématiques Établissements Ceuterick, Louvain | publisher = Librairie Gauthier-Villars | place = Paris | year = 1957 | pages = 261–289 }}\n* {{Citation|title=Galois Theory of Linear Differential Equations|first1=M.|last1=van der Put|first2=M. F.|last2=Singer|year=2003|publisher=Springer|series=Grundlehren der mathematischen Wissenschaften|volume=328|url=http://www4.ncsu.edu/~singer/papers/dbook2.ps}}\n* {{Citation|last=von Staudt|first=Karl Georg Christian|authorlink=Karl Georg Christian von Staudt |url=https://books.google.com/books?id=XwEHAAAAcAAJ&pg=PA127#v=onepage&q&f=false|title=Beiträge zur Geometrie der Lage (Contributions to the Geometry of Position)|volume=2|location=Nürnberg (Germany)|publisher=Bauer and Raspe|year=1857}} \n* {{Citation|last=Wallace|first=D. A. R.|year=1998|title=Groups, Rings, and Fields|series=SUMS|publisher=Springer|volume=151}}\n* {{Citation|first=Seth|last=Warner|title=Topological fields|isbn=0-444-87429-1|year=1989|publisher=North-Holland|zbl=0683.12014}}\n* {{citation|first=Lawrence C.|last= Washington|authorlink=Lawrence C. Washington|title=Introduction to Cyclotomic Fields|series=Graduate Texts in Mathematics|volume= 83|publisher=Springer-Verlag|place= New York|year= 1997|edition=2|isbn=0-387-94762-0 |mr=1421575|doi=10.1007/978-1-4612-1934-7}}\n* {{Citation|last=Weber|first=Heinrich|author-link= Heinrich Martin Weber |title=Die allgemeinen Grundlagen der Galois'schen Gleichungstheorie|journal=Mathematische Annalen|issn=0025-5831\n|volume=43|pages=521–549|year=1893|language=German|doi=10.1007/BF01446451|jfm=25.0137.01|url=https://eudml.org/doc/157689}}\n\n{{refend}}\n\n{{DEFAULTSORT:Field (Mathematics)}}\n[[Category:Field theory| ]]\n[[Category:Algebraic structures]]\n[[Category:Abstract algebra]]\n\n[[eo:Kampo (algebro)]]"
    },
    {
      "title": "Formal derivative",
      "url": "https://en.wikipedia.org/wiki/Formal_derivative",
      "text": "In [[mathematics]], the '''formal derivative''' is an operation on elements of a [[polynomial ring]] or a ring of [[formal power series]] that mimics the form of the derivative from [[derivative|calculus]].  Though they appear similar, the algebraic advantage of a formal derivative is that it does not rely on the notion of a [[limit (mathematics)|limit]], which is in general impossible to define for a [[ring (mathematics)|ring]].  Many of the properties of the derivative are true of the formal derivative, but some, especially those that make numerical statements, are not.\n\nFormal differentiation is used in algebra to test for [[multiple roots of a polynomial]].\n\n==Definition==\nThe definition of formal derivative is as follows: fix a ring ''R'' (not necessarily commutative) and let ''A'' = ''R''[''x''] be the ring of polynomials over ''R''.  Then the formal derivative is an operation on elements of ''A'', where if\n\n:<math>f(x)\\,=\\,a_n x^n + \\cdots + a_1 x + a_0,</math>\n\nthen its formal derivative is\n\n:<math>f'(x)\\,=\\,Df(x) = n a_n x^{n - 1} + \\cdots + 2 a_2 x + a_1,</math>\n\njust as for polynomials over the [[real numbers|real]] or [[complex numbers|complex]] numbers. Here <math>m a_i</math> does not mean multiplication in the ring, but rather <math>\\sum_{k=1}^m a_i,</math> where <math>k</math> is never used inside the sum.\n\nThere is a problem with this definition for noncommutative rings. The formula itself is correct, but there is no standard form of a polynomial. Therefore using this definition it is difficult to prove that <math>(f(x)\\cdot b)'=f'(x)\\cdot b.</math>\n\n==Axiomatic definition well suited for noncommutative rings==\nAs opposed to the above formula one may define the formal derivative axiomatically as the map <math>(\\ast)^\\prime\\colon R[x] \\to R[x]</math> satisfying the following properties. \n\n1) <math>r'=0</math> for all <math>r\\in R\\subset R[x].</math>\n\n2) The normalization axiom, <math>x' = 1.</math> \n\n3) The map commutes with the addition operation in the polynomial ring,  <math>(a+b)' = a'+b'.</math>\n\n4) The map satisfies Leibniz's law with respect to the polynomial ring's multiplication operation, <math>(a\\cdot b)'=a'\\cdot b+a\\cdot b'.</math>\n\nOne may prove that this axiomatic definition yields a well defined map respecting all of the usual ring axioms.\n\nThe formula above (i.e. the definition of the formal derivative when the coefficient ring is commutative) is a direct consequence of the aforementioned axioms:\n<math>(\\sum_i a_ix^i)'=\\sum_i (a_ix^i)'=\\sum_i ((a_i)'x^i+a_i(x^i)')=\\sum_i(0x^i+a_i(\\sum_{j=1}^ix^{j-1}(x')x^{i-j}))=\\sum_i\\sum_{j=1}^i a_ix^{i-1}.</math>\n\n==Properties==\nIt can be verified that:\n\n* Formal differentiation is linear: for any two polynomials ''f''(''x''),''g''(''x'') in ''R''[''x''] and elements ''r'',''s'' of ''R'' we have\n\n::<math>(r \\cdot f + s \\cdot g)'(x) = r \\cdot f'(x) + s \\cdot g'(x).</math>\n\n:When ''R'' is not commutative there is another, different, linearity property in which ''r'' and ''s'' appear on the right rather than on the left.  When ''R'' does not contain an identity element, neither of these reduces to the case of simply a sum of polynomials or the sum of a polynomial with a multiple of another polynomial, which must also be included as a \"linearity\" property.\n\n* The formal derivative satisfies the [[Product rule|Leibniz rule]]:\n\n::<math>(f \\cdot g)'(x) = f'(x) \\cdot g(x) + f(x) \\cdot g'(x).</math>\n\n:Note the order of the factors; when ''R'' is not commutative this is important.\n\nThese two properties make ''D'' a [[derivation (abstract algebra)|derivation]] on ''A'' (see [[module of relative differential forms]] for a discussion of a generalization).\n\n==Application to finding repeated factors==\nAs in calculus, the derivative detects multiple roots. If ''R'' is a field then ''R''[''x''] is a [[Euclidean domain]], and in this situation we can define multiplicity of roots; for every polynomial ''f''(''x'') in ''R''[''x''] and every element ''r'' of ''R'', there exists a nonnegative integer ''m<sub>r</sub>'' and a polynomial ''g''(''x'') such that\n\n:<math>f(x) = (x - r)^{m_r} g(x)</math>\n\nwhere ''g''(''r''){{Thin space}}≠{{Thin space}}''0''.  ''m<sub>r</sub>'' is the multiplicity of ''r'' as a root of ''f''.  It follows from the Leibniz rule that in this situation, ''m<sub>r</sub>'' is also the number of differentiations that must be performed on ''f''(''x'') before ''r'' is no longer a root of the resulting polynomial.  The utility of this observation is that although in general not every polynomial of degree ''n'' in ''R''[''x''] has ''n'' roots counting multiplicity (this is the maximum, by the above theorem), we may pass to [[field extension]]s in which this is true (namely, [[algebraic closure]]s).  Once we do, we may uncover a multiple root that was not a root at all simply over ''R''.  For example, if ''R'' is the field with three elements, the polynomial\n\n:<math>f(x)\\,=\\,x^6 + 1</math>\n\nhas no roots in ''R''; however, its formal derivative is zero since 3 = 0 in ''R'' and in any extension of ''R'', so when we pass to the algebraic closure it has a multiple root that could not have been detected by factorization in ''R'' itself.  Thus, formal differentiation allows an [[computability theory (computer science)|effective]] notion of multiplicity.  This is important in [[Galois theory]], where the distinction is made between [[separable field extension]]s (defined by polynomials with no multiple roots) and inseparable ones.\n\n==Correspondence to analytic derivative==\nWhen the ring ''R'' of scalars is commutative, there is an alternative and equivalent definition of the formal derivative, which resembles the one seen in differential calculus. The element Y–X of the ring ''R''[X,Y] divides Y<sup>''n''</sup> – X<sup>''n''</sup> for any nonnegative integer ''n'', and therefore divides ''f''(Y) – ''f''(X) for any polynomial ''f'' in one indeterminate. If the quotient in ''R''[X,Y] is denoted by ''g'', then\n\n:<math>g(X,Y) = \\frac{f(Y) - f(X)}{Y - X}.</math>\n\nIt is then not hard to verify that ''g''(X,X) (in ''R''[X]) coincides with the formal derivative of ''f'' as it was defined above.\n\nThis formulation of the derivative works equally well for a formal power series, as long as the ring of coefficients is commutative.\n\nActually, if the division in this definition is carried out in the class of functions of <math>Y</math> continuous at <math>X</math>, it will recapture the classical definition of the derivative. If it is carried out in the class of functions continuous in both <math>X</math> and <math>Y</math>, we get uniform differentiability, and our function <math>f</math> will be continuously differentiable. Likewise, by choosing different classes of functions (say, the Lipschitz class), we get different flavors of differentiability. In this way, differentiation becomes a part of algebra of functions.\n\n==See also==\n\n* [[Derivative]]\n* [[Euclidean domain]]\n* [[Module of relative differential forms]]\n* [[Galois theory]]\n* [[Formal power series]]\n* [[Pincherle derivative]]\n\n==References==\n* {{Lang Algebra|edition=3r}}\n* [https://arxiv.org/abs/0905.3611 Michael Livshits, You could simplify calculus, arXiv:0905.3611v1 ]\n[[Category:Abstract algebra]]"
    },
    {
      "title": "Formal power series",
      "url": "https://en.wikipedia.org/wiki/Formal_power_series",
      "text": "{{Refimprove|date=December 2009}}\nIn [[mathematics]], a '''formal power series''' is a generalization of a [[polynomial]], where the number of terms is allowed to be infinite; this implies giving up the possibility of replacing the variable in the polynomial with an arbitrary number. Thus a formal power series differs from a polynomial in that it may have infinitely many terms, and differs from a [[power series]], whose variables can take on numerical values. One way to view a formal power series is as an infinite ordered sequence of numbers. In this case, the powers of the variable are used only to indicate the order of the coefficients, so that the coefficient of <math>x^5</math> is the fifth term in the sequence. In [[combinatorics]], formal power series provide representations of numerical [[sequence]]s and of [[multiset]]s, and for instance allow concise expressions for [[recursion|recursively]] defined sequences regardless of whether the recursion can be explicitly solved; this is known as the method of [[generating function]]s. More generally, formal power series can include series with any finite number of variables, and with coefficients in an arbitrary [[ring (mathematics)|ring]]. Formal power series can be created from [[Taylor polynomial|Taylor polynomials]] using [[formal moduli]].\n\n==Introduction==\nA formal power series can be loosely thought of as an object that is like a [[polynomial]], but with infinitely many terms. Alternatively, for those familiar with [[power series]] (or [[Taylor series]]), one may think of a formal power series as a power series in which we ignore questions of [[Convergent series|convergence]] by not assuming that the variable ''X'' denotes any numerical value (not even an unknown value). For example, consider the series\n\n:<math>A = 1 - 3X + 5X^2 - 7X^3 + 9X^4 - 11X^5 + \\cdots.</math>\n\nIf we studied this as a power series, its properties would include, for example, that its [[radius of convergence]] is 1. However, as a formal power series, we may ignore this completely; all that is relevant is the sequence of [[coefficient]]s [1, −3, 5, −7, 9, −11, ...]. In other words, a formal power series is an object that just records a sequence of coefficients. It is perfectly acceptable to consider a formal power series with the [[factorial]]s [1, 1, 2, 6, 24, 120, 720, 5040, … ] as coefficients, even though the corresponding power series diverges for any nonzero value of ''X''.\n\nArithmetic on formal power series is carried out by simply pretending that the series are polynomials. For example, if\n\n:<math>B = 2X + 4X^3 + 6X^5 + \\cdots,</math>\n\nthen we add ''A'' and ''B'' term by term:\n\n:<math>A + B = 1 - X + 5X^2 - 3X^3 + 9X^4 - 5X^5 + \\cdots.</math>\n\nWe can multiply formal power series, again just by treating them as polynomials (see in particular [[Cauchy product]]):\n\n:<math>AB = 2X - 6X^2 + 14X^3 - 26X^4 + 44X^5 + \\cdots.</math>\n\nNotice that each coefficient in the product ''AB'' only depends on a ''finite'' number of coefficients of ''A'' and ''B''. For example, the ''X''<sup>5</sup> term is given by\n\n:<math>44X^5 = (1\\times 6X^5) + (5X^2 \\times 4X^3) + (9X^4 \\times 2X).</math>\n\nFor this reason, one may multiply formal power series without worrying about the usual questions of [[absolute convergence|absolute]], [[conditional convergence|conditional]] and [[uniform convergence]] which arise in dealing with power series in the setting of [[Mathematical analysis|analysis]].\n\nOnce we have defined multiplication for formal power series, we can define multiplicative inverses as follows. The multiplicative inverse of a formal power series ''A'' is a formal power series ''C'' such that ''AC'' = 1, provided that such a formal power series exists. It turns out that if ''A'' has a multiplicative inverse, it is unique, and we denote it by ''A''<sup>−1</sup>. Now we can define division of formal power series by defining ''B''/''A'' to be the product ''BA''<sup>−1</sup>, provided that the inverse of ''A'' exists. For example, one can use the definition of multiplication above to verify the familiar formula\n\n:<math>\\frac{1}{1 + X} = 1 - X + X^2 - X^3 + X^4 - X^5 + \\cdots.</math>\n\nAn important operation on formal power series is coefficient extraction. In its most basic form, the coefficient extraction operator <math>[X^n]</math> applied to a formal power series <math>A</math> in one variable extracts the coefficient of the <math>n</math>th power of the variable, so that <math>[X^2]A=5</math> and <math>[X^5]A=-11</math>. Other examples include\n\n:<math>\\begin{align}\n\\left[X^3\\right] (B) &= 4, \\\\ \n\\left[X^2 \\right] (X + 3 X^2 Y^3 + 10 Y^6) &= 3Y^3, \\\\\n\\left[X^2Y^3 \\right] ( X + 3 X^2 Y^3 + 10 Y^6) &= 3, \\\\\n\\left[X^n \\right] \\left(\\frac{1}{1+X} \\right) &= (-1)^n, \\\\ \n\\left[X^n \\right] \\left(\\frac{X}{(1-X)^2} \\right) &= n.\n\\end{align}</math>\n\nSimilarly, many other operations that are carried out on polynomials can be extended to the formal power series setting, as explained below.\n\n==The ring of formal power series==\nThe set of all formal power series in ''X'' with coefficients in a [[commutative ring]] ''R'' form another ring that is written <math>R[[X]],</math> and called the '''ring of formal power series''' in the variable&nbsp;''X'' over ''R''.\n\n===Definition of the formal power series ring===\nOne can characterize <math>R[[X]]</math> abstractly as the [[completeness (topology)|completion]] of the [[polynomial]] ring <math>R[X]</math> equipped with a particular [[metric space|metric]]<!-- in which the powers of the ideal ''I'' of <math>R[X]</math> generated by <math>X</math> form a shrinking set of neighbourhoods of 0; a precise description would be too long to spell out here-->. This automatically gives <math>R[[X]]</math> the structure of a [[topological ring]] (and even of a complete metric space). But the general construction of a completion of a metric space is more involved than what is needed here, and would make formal power series seem more complicated than they are. \nIt is possible to describe <math>R[[X]]</math> more explicitly, and define the ring structure and topological structure separately, as follows.\n\n====Ring structure====\nAs a set, <math>R[[X]]</math> can be constructed as the set <math>R^\\N</math> of all infinite sequences of elements of <math>R</math>, indexed by the [[natural number]]s (taken to include 0). Designating a sequence whose term at index <math>n</math> is <math>a_n</math> by <math>(a_n)</math>, one defines addition of two such sequences by\n\n:<math>(a_n)_{n\\in\\N} + (b_n)_{n\\in\\N} = \\left( a_n + b_n \\right)_{n\\in\\N}</math>\n\nand multiplication by\n\n:<math>(a_n)_{n\\in\\N} \\times (b_n)_{n\\in\\N} = \\left( \\sum_{k=0}^n a_k b_{n-k} \\right)_{n\\in\\N}.</math>\n\nThis type of product is called the [[Cauchy product]] of the two sequences of coefficients, and is a sort of discrete [[convolution]]. With these operations, <math>R^\\N</math> becomes a commutative ring with zero element <math>(0,0,0,\\ldots)</math> and multiplicative identity <math>(1,0,0,\\ldots)</math>.\n\nThe product is in fact the same one used to define the product of polynomials in one indeterminate, which suggests using a similar notation. One embeds <math>R</math> into <math>R[[X]]</math> by sending any (constant) <math>a \\in R</math> to the sequence <math>(a,0,0,\\ldots)</math> and designates the sequence <math>(0,1,0,0,\\ldots)</math> by <math>X</math>; then using the above definitions every sequence with only finitely many nonzero terms can be expressed in terms of these special elements as\n\n:<math>(a_0, a_1, a_2, \\ldots, a_n, 0, 0, \\ldots) = a_0 + a_1 X + \\cdots + a_n X^n = \\sum_{i=0}^n a_i X^i;</math>\n\nthese are precisely the polynomials in <math>X</math>. Given this, it is quite natural and convenient to designate a general sequence <math>(a_n)_{n\\in\\N}</math> by the formal expression <math>\\textstyle\\sum_{i\\in\\N}a_i X^i</math>, even though the latter ''is not'' an expression formed by the operations of addition and multiplication defined above (from which only finite sums can be constructed). This notational convention allows reformulation of the above definitions as\n\n:<math>\\left(\\sum_{i\\in\\N} a_i X^i\\right)+\\left(\\sum_{i\\in\\N} b_i X^i\\right) = \\sum_{i\\in\\N}(a_i+b_i) X^i</math>\n\nand\n\n:<math>\\left(\\sum_{i\\in\\N} a_i X^i\\right) \\times \\left(\\sum_{i\\in\\N} b_i X^i\\right) = \\sum_{n\\in\\N} \\left(\\sum_{k=0}^n a_k b_{n-k}\\right) X^n.</math>\n\nwhich is quite convenient, but one must be aware of the distinction between formal summation (a mere convention) and actual addition.\n\n==== Topological structure ====\nHaving stipulated conventionally that\n\n:<math>(a_0, a_1, a_2, a_3, \\ldots) = \\sum_{i=0}^\\infty a_i X^i, \\qquad (1)</math>\n\none would like to interpret the right hand side as a well-defined infinite summation. To that end, a notion of convergence in <math>R^\\N</math> is defined and a [[topology]] on <math>R^\\N</math> is constructed. There are several equivalent ways to define the desired topology.\n\n* We may give <math>R^\\N</math> the [[product topology]], where each copy of <math>R</math> is given the [[discrete topology]].\n\n* We may give <math>R^\\N</math> the [[I-adic topology]], where <math>I=(X)</math> is the ideal generated by <math>X</math>, which consists of all sequences whose first term <math>a_0</math> is zero.\n\n* The desired topology could also be derived from the following [[metric space|metric]]. The distance between distinct sequences <math>(a_n), (b_n) \\in R^{\\N},</math> is defined to be\n::<math>d((a_n), (b_n)) = 2^{-k},</math>\n:where <math>k</math> is the smallest [[natural number]] such that <math>a_k\\neq b_k</math>; the distance between two equal sequences is of course zero.\n\nInformally, two sequences <math>\\{a_n\\}</math> and <math>\\{b_n\\}</math> become closer and closer if and only if more and more of their terms agree exactly. Formally, the sequence of [[partial sum]]s of some infinite summation converges if for every fixed power of <math>X</math> the coefficient stabilizes: there is a point beyond which all further partial sums have the same coefficient. This is clearly the case for the right hand side of (1), regardless of the values <math>a_n</math>, since inclusion of the term for <math>i=n</math> gives the last (and in fact only) change to the coefficient of <math>X^n</math>. It is also obvious that the [[limit of a sequence|limit]] of the sequence of partial sums is equal to the left hand side.\n\nThis topological structure, together with the ring operations described above, form a topological ring. This is called the '''ring of formal power series over <math>R</math>''' and is denoted by <math>R[[X]]</math>. The topology has the useful property that an infinite summation converges if and only if the sequence of its terms converges to 0, which just means that any fixed power of <math>X</math> occurs in only finitely many terms.\n\nThe topological structure allows much more flexible usage of infinite summations. For instance the rule for multiplication can be restated simply as\n\n:<math>\\left(\\sum_{i\\in\\N} a_i X^i\\right) \\times \\left(\\sum_{i\\in\\N} b_i X^i\\right) = \\sum_{i,j\\in\\N} a_i b_j X^{i+j},</math>\n\nsince only finitely many terms on the right affect any fixed <math>X^n</math>. Infinite products are also defined by the topological structure; it can be seen that an infinite product converges if and only if the sequence of its factors converges to 1.\n\n==== Alternative topologies ====\nThe above topology is the [[Comparison of topologies|finest topology]] for which \n\n:<math>\\sum_{i=0}^\\infty a_i X^i</math> \n\nalways converges as a summation to the formal power series designated by the same expression, and it often suffices to give a meaning to infinite sums and products, or other kinds of limits that one wishes to use to designate particular formal power series. It can however happen occasionally that one wishes to use a coarser topology, so that certain expressions become convergent that would otherwise diverge. This applies in particular when the base ring <math>R</math> already comes with a topology other than the discrete one, for instance if it is also a ring of formal power series.\n\nConsider the ring of formal power series: <math>\\Z[[X]][[Y]]</math>; then the topology of above construction only relates to the indeterminate <math>Y</math>, since the topology that was put on <math>\\Z[[X]]</math> has been replaced by the discrete topology when defining the topology of the whole ring. So\n\n:<math>\\sum_{i\\in\\N}XY^i</math>\n\nconverges to the power series suggested, which can be written as <math>\\tfrac{X}{1-Y}</math>; however the summation\n\n:<math>\\sum_{i\\in\\N}X^iY</math>\n\nwould be considered to be divergent, since every term affects the coefficient of <math>Y</math> (which coefficient is itself a power series in <math>X</math>). This asymmetry disappears if the power series ring in <math>Y</math> is given the product topology where each copy of <math>\\Z[[X]]</math> is given its topology as a ring of formal power series rather than the discrete topology. As a consequence, for convergence of a sequence of elements of <math>\\Z[[X]][[Y]]</math> it then suffices that the coefficient of each power of <math>Y</math> converges to a formal power series in <math>X</math>, a weaker condition than stabilizing entirely; for instance in the second example given here the coefficient of <math>Y</math>converges to <math>\\tfrac{1}{1-X}</math>, so the whole summation converges to <math>\\tfrac{Y}{1-X}</math>.\n\nThis way of defining the topology is in fact the standard one for repeated constructions of rings of formal power series, and gives the same topology as one would get by taking formal power series in all indeterminates at once. In the above example that would mean constructing <math>\\Z[[X,Y]],</math> and here a sequence converges if and only if the coefficient of every monomial <math>X^iY^j</math> stabilizes. This topology, which is also the <math>I</math>-adic topology, where <math>I=(X,Y)</math> is the ideal generated by <math>X</math> and <math>Y</math>, still enjoys the property that a summation converges if and only if its terms tend to 0.\n\nThe same principle could be used to make other divergent limits converge. For instance in <math>\\R[[X]]</math> the limit\n\n:<math>\\lim_{n\\to\\infty}\\left(1+\\frac{X}{n}\\right)^n</math>\n\ndoes not exist, so in particular it does not converge to \n\n:<math>\\exp(X)=\\sum_{n\\in\\N}\\frac{X^n}{n!}.</math>\n\nThis is because for <math>i\\geq 2</math> the coefficient <math>\\tbinom{n}{i}/n^i</math> of <math>X^i</math> does not stabilize as <math>n\\to \\infty</math>. It does however converge in the usual topology of <math>\\R</math>, and in fact to the coefficient <math>\\tfrac{1}{i!}</math> of <math>\\exp(X)</math>. Therefore, if one would give <math>\\R[[X]]</math> the product topology of <math>\\R^\\N</math> where the topology of <math>\\R</math> is the usual topology rather than the discrete one, then the above limit would converge to <math>\\exp(X)</math>. This more permissive approach is not however the standard when considering formal power series, as it would lead to convergence considerations that are as subtle as they are in [[analysis (mathematics)|analysis]], while the philosophy of formal power series is on the contrary to make convergence questions as trivial as they can possibly be. With this topology it would ''not'' be the case that a summation converges if and only if its terms tend to 0.\n\n===Universal property===\nThe ring <math>R[[X]]</math> may be characterized by the following [[universal property]]. If <math>S</math> is a commutative associative algebra over <math>R</math>, if <math>I</math> is an ideal of <math>S</math> such that the <math>I</math>-adic topology on <math>S</math> is complete, and if <math>x</math> is an element of <math>I</math>, then there is a ''unique'' <math>\\Phi: R[[X]]\\to S</math> with the following properties:\n\n* <math>\\Phi</math> is an <math>R</math>-algebra homomorphism\n\n* <math>\\Phi</math> is continuous\n\n* <math>\\Phi(X)=x</math>.\n\n== Operations on formal power series ==\nOne can perform algebraic operations on power series to generate new power series.<ref name=\"Zwillinger_2014\">{{cite book |author-first1=Izrail Solomonovich |author-last1=Gradshteyn |author-link1=Izrail Solomonovich Gradshteyn |author-first2=Iosif Moiseevich |author-last2=Ryzhik |author-link2=Iosif Moiseevich Ryzhik |author-first3=Yuri Veniaminovich |author-last3=Geronimus |author-link3 =Yuri Veniaminovich Geronimus |author-first4=Michail Yulyevich |author-last4=Tseytlin |author-link4=Michail Yulyevich Tseytlin |author-first5=Alan |author-last5=Jeffrey |editor1-first=Daniel |editor1-last=Zwillinger |editor2-first=Victor Hugo |editor2-last=Moll |translator=Scripta Technica, Inc. |title=Table of Integrals, Series, and Products |publisher=[[Academic Press, Inc.]] |date=2015 |orig-year=October 2014 |edition=8 |language=English |isbn=978-0-12-384933-5 |lccn=2014010276 <!-- |url=https://books.google.com/books?id=NjnLAwAAQBAJ |access-date=2016-02-21-->|titlelink=Gradshteyn and Ryzhik |chapter=0.313 |page=18}} (Several previous editions as well.)</ref><ref name=formalpowerseries>{{cite journal | first=Ivan | last=Niven | authorlink=Ivan Niven | title=Formal Power Series | journal=[[American Mathematical Monthly]] | volume=76 | issue=8 | date=October 1969 | pages=871–889 | doi=10.1080/00029890.1969.12000359}}</ref> Besides the ring structure operations defined above, we have the following.\n\n===Power series raised to powers===\nFor any [[natural number]] ''n'' we have\n\n:<math> \\left( \\sum_{k=0}^\\infty a_k X^k \\right)^n = \\sum_{m=0}^\\infty c_m X^m,</math>\n\nwhere\n\n:<math>\\begin{align} \nc_0 &= a_0^n,\\\\\nc_m &= \\frac{1}{m a_0} \\sum_{k=1}^m (kn - m+k) a_{k} c_{m-k}, \\ \\ \\ m \\geq 1.\n\\end{align}</math>\n\n(This formula can only be used if ''m'' and ''a''<sub>0</sub> are invertible in the ring of coefficients.)\n\nIn the case of formal power series with complex coefficients, the complex powers are well defined at least for series ''f'' with constant term equal to 1. In this case, <math>f^{\\alpha}</math> can be defined either by composition with the [[binomial series]] (1+''x'')<sup>α</sup>, or by composition with the exponential and the logarithmic series, <math>f^{\\alpha}=\\exp(\\alpha\\log(f)),</math> or as the solution of the differential equation <math>f( f^{\\alpha})' = \\alpha f^{\\alpha} f'</math> with constant term 1, the three definitions being equivalent. The rules of calculus <math>(f^\\alpha)^\\beta = f^{\\alpha\\beta}</math> and <math>f^\\alpha g^\\alpha = (fg)^\\alpha</math> easily follow.\n\n=== Inverting series ===\nThe series\n\n:<math>A = \\sum_{n=0}^\\infty a_n X^n \\in R[[X]]</math>\n\nis invertible in <math>R[[X]]</math> if and only if its constant coefficient <math>a_0</math> is invertible in <math>R</math>. This condition is necessary, for the following reason: if we suppose that <math>A</math> has an inverse <math>B = b_0 + b_1 x + \\cdots</math> then the [[constant term]] <math>a_0b_0</math> of <math>A \\cdot B</math> is the constant term of the identity series, i.e. it is 1. This condition is also sufficient; we may compute the coefficients of the inverse series <math>B</math> via the explicit recursive formula\n\n:<math>\\begin{align}\nb_0 &= \\frac{1}{a_0},\\\\\nb_n &= -\\frac{1}{a_0} \\sum_{i=1}^n a_i b_{n-i}, \\ \\ \\ n \\geq 1.\n\\end{align}</math>\n\nAn important special case is that the [[geometric series]] formula is valid in <math>K[[X]]</math>:\n\n:<math>(1 - X)^{-1} = \\sum_{n=0}^\\infty X^n.</math>\n\nIf <math>R=K</math> is a field, then a series is invertible if and only if the constant term is non-zero, i.e. if and only if the series is not divisible by <math>X</math>. This means that <math>K[[X]]</math> is a [[discrete valuation ring]] with uniformizing parameter <math>X</math>.\n\n===Dividing series===\nThe computation of a quotient <math>f/g=h</math>\n\n:<math> \\frac{\\sum_{n=0}^\\infty b_n X^n }{\\sum_{n=0}^\\infty a_n X^n } =\\sum_{n=0}^\\infty c_n X^n, </math>\n\nassuming the denominator is invertible (that is, <math>a_0</math> is invertible in the ring of scalars), can be performed as a product <math>f</math> and the inverse of <math>g</math>, or directly equating the coefficients in <math>f=gh</math>:\n\n:<math>c_n = \\frac{1}{a_0}\\left(b_n - \\sum_{k=1}^n a_k c_{n-k}\\right).</math>\n\n=== Extracting coefficients ===\nThe coefficient extraction operator applied to a formal power series \n\n:<math>f(X) = \\sum_{n=0}^\\infty a_n X^n </math>\n\nin ''X'' is written\n\n:<math> \\left[ X^m \\right] f(X) </math>\n\nand extracts the coefficient of ''X<sup>m</sup>'', so that\n\n:<math> \\left[ X^m \\right] f(X) = \\left[ X^m \\right] \\sum_{n=0}^\\infty a_n X^n = a_m.</math>\n\n=== Composition of series ===\nGiven formal power series\n\n:<math>f(X) = \\sum_{n=1}^\\infty a_n X^n = a_1 X + a_2 X^2 + \\cdots</math>\n:<math>g(X) = \\sum_{n=0}^\\infty b_n X^n = b_0 + b_1 X + b_2 X^2 + \\cdots,</math>\n\none may form the ''composition''\n\n:<math>g(f(X)) = \\sum_{n=0}^\\infty b_n (f(X))^n = \\sum_{n=0}^\\infty c_n X^n,</math>\n\nwhere the coefficients ''c''<sub>''n''</sub> are determined by \"expanding out\" the powers of ''f''(''X''):\n\n:<math>c_n:=\\sum_{k\\in\\N, |j|=n} b_k a_{j_1} a_{j_2} \\cdots a_{j_k}.</math>\n\nHere the sum is extended over all (''k'', ''j'') with <math>k\\in\\N</math> and <math>j\\in\\N_+^k</math> with <math>|j|:=j_1+\\cdots+j_k=n.</math>\n\nA more explicit description of these coefficients is provided by [[Faà di Bruno's formula#Formal power series version|Faà di Bruno's formula]], at least in the case where the coefficient ring is a field of [[Characteristic (algebra)|characteristic 0]].\n\nA point here is that this operation is only valid when <math>f(X)</math> has ''no constant term'', so that each <math>c_n</math> depends on only a finite number of coefficients of <math>f(X)</math> and <math>g(X)</math>. In other words, the series for <math>g(f(X))</math> converges in the [[Completion (ring theory)#Krull topology|topology]] of <math>R[[X]]</math>.\n\n==== Example ====\nAssume that the ring <math>R</math> has characteristic 0 and the nonzero integers are invertible in <math>R</math>. If we denote by <math>\\exp(X)</math> the formal power series\n\n:<math>\\exp(X) = 1 + X + \\frac{X^2}{2!} + \\frac{X^3}{3!} + \\frac{X^4}{4!} + \\cdots,</math>\n\nthen the expression\n\n:<math>\\exp(\\exp(X) - 1) = 1 + X + X^2 + \\frac{5X^3}6 + \\frac{5X^4}8 + \\cdots</math>\n\nmakes perfect sense as a formal power series. However, the statement\n\n:<math>\\exp(\\exp(X)) = e \\exp(\\exp(X) - 1) = e + eX + eX^2 + \\frac{5eX^3}{6} + \\cdots</math>\n\nis not a valid application of the composition operation for formal power series. Rather, it is confusing the notions of convergence in <math>R[[X]]</math> and convergence in <math>R</math>; indeed, the ring <math>R</math> may not even contain any number <math>e</math> with the appropriate properties.\n\n=== Composition inverse ===\nWhenever a formal series \n\n:<math>f(X)=\\sum_k f_k X^k \\in R[[X]]</math> \n\nhas ''f''<sub>0</sub> = 0 and ''f''<sub>1</sub> being an invertible element of ''R'', there exists a series \n\n:<math>g(X)=\\sum_k g_k X^k</math> \n\nthat is the [[composition inverse]] of <math>f</math>, meaning that composing <math>f</math> with <math>g</math> gives the series representing the [[identity function]] (whose first coefficient is 1 and all other coefficients are zero). The coefficients of <math>g</math> may be found recursively by using the above formula for the coefficients of a composition, equating them with those of the composition identity ''X'' (that is 1 at degree 1 and 0 at every degree greater than 1). In the case when the coefficient ring is a field of characteristic 0, the [[#The Lagrange inversion formula|Lagrange inversion formula]] provides a powerful tool to compute the coefficients of ''g'', as well as the coefficients of the (multiplicative) powers of ''g''.\n\n=== Formal differentiation of series ===\nGiven a formal power series\n\n:<math>f = \\sum_{n\\geq 0} a_n X^n \\in R[[X]],</math>\n\nwe define its '''[[formal derivative]]''', denoted ''Df'' or ''f''′, by\n\n:<math> Df = \\sum_{n \\geq 1} a_n n X^{n-1}.</math>\n\nThe symbol ''D'' is called the '''formal differentiation operator'''. The motivation behind this definition is that it simply mimics term-by-term differentiation of a polynomial.\n\nThis operation is ''R''-[[linear operator|linear]]:\n\n:<math>D(af + bg) = a \\cdot Df + b \\cdot Dg</math>\n\nfor any ''a'', ''b'' in ''R'' and any ''f'', ''g'' in <math>R[[X]].</math> Additionally, the formal derivative has many of the properties of the usual [[derivative]] of calculus. For example, the [[product rule]] is valid:\n\n:<math>D(fg) = f \\cdot (Dg) + (Df) \\cdot g,</math>\n\nand the [[chain rule]] works as well:\n\n:<math>D(f\\circ g ) = ( Df\\circ g ) \\cdot Dg,</math>\n\nwhenever the appropriate compositions of series are defined (see above under [[#Composition of series|composition of series]]).\n\nThus, in these respects formal power series behave like [[Taylor series]]. Indeed, for the ''f'' defined above, we find that\n\n:<math>(D^k f)(0) = k! a_k, </math>\n\nwhere ''D''<sup>''k''</sup> denotes the ''k''th formal derivative (that is, the result of formally differentiating ''k'' times).\n\n== Properties ==\n\n=== Algebraic properties of the formal power series ring ===\n<math>R[[X]]</math> is an [[associative algebra]] over <math>R</math> which contains the ring <math>R[X]</math> of polynomials over <math>R</math>; the polynomials correspond to the sequences which end in zeros.\n\nThe [[Jacobson radical]] of <math>R[[X]]</math> is the [[ideal (ring theory)|ideal]] generated by <math>X</math> and the Jacobson radical of <math>R</math>; this is implied by the element invertibility criterion discussed above.\n\nThe [[maximal ideal]]s of <math>R[[X]]</math> all arise from those in <math>R</math> in the following manner: an ideal <math>M</math> of <math>R[[X]]</math> is maximal if and only if <math>M\\cap R</math> is a maximal ideal of <math>R</math> and <math>M</math> is generated as an ideal by <math>X</math> and <math>M\\cap R</math>.\n\nSeveral algebraic properties of <math>R</math> are inherited by <math>R[[X]]</math>:\n\n* if <math>R</math> is a [[local ring]], then so is <math>R[[X]]</math>,\n\n* if <math>R</math> is [[noetherian ring|Noetherian]], then so is <math>R[[X]]</math>. This is a version of the [[Hilbert basis theorem]],\n\n* if <math>R</math> is an [[integral domain]], then so is <math>R[[X]]</math>,\n\n* if <math>K</math> is a [[field (mathematics)|field]], then <math>K[[X]]</math> is a [[discrete valuation ring]].\n\n=== Topological properties of the formal power series ring ===\nThe metric space <math>(R[[X]], d)</math> is [[completeness (topology)|complete]].\n\nThe ring <math>R[[X]]</math> is [[compact space|compact]] if and only if ''R'' is [[finite set|finite]]. This follows from [[Tychonoff's theorem]] and the characterisation of the topology on <math>R[[X]]</math> as a product topology.\n\n=== Weierstrass preparation ===\n{{main article|Weierstrass preparation theorem#Formal power series in complete local rings}}\nThe ring of formal power series with coefficients in a [[complete local ring]] satisfies the [[Weierstrass preparation theorem]].\n\n==Applications==\nFormal power series can be used to solve recurrences occurring in number theory and combinatorics. For an example involving finding a closed form expression for the [[Fibonacci number]]s, see the article on [[Examples of generating functions]].\n\nOne can use formal power series to prove several relations familiar from analysis in a purely algebraic setting. Consider for instance the following elements of <math>\\Q[[X]]</math>:\n\n:<math> \\sin(X) := \\sum_{n \\ge 0} \\frac{(-1)^n} {(2n+1)!} X^{2n+1} </math>\n:<math> \\cos(X) := \\sum_{n \\ge 0} \\frac{(-1)^n} {(2n)!} X^{2n} </math>\n\nThen one can show that\n\n:<math>\\sin^2(X) + \\cos^2(X) = 1,</math>\n:<math>\\frac{\\partial}{\\partial X} \\sin(X) = \\cos(X),</math>\n:<math>\\sin (X+Y) = \\sin(X) \\cos(Y) + \\cos(X) \\sin(Y).</math>\n\nThe last one being valid in the ring <math>\\Q[[X, Y]].</math>\n\nFor ''K'' a field, the ring <math>K[[X_1, \\ldots, X_r]]</math> is often used as the \"standard, most general\" complete local ring over ''K'' in algebra.\n\n==Interpreting formal power series as functions==\nIn [[mathematical analysis]], every convergent [[power series]] defines a [[function (mathematics)|function]] with values in the [[real number|real]] or [[complex number|complex]] numbers. Formal power series can also be interpreted as functions, but one has to be careful with the [[function domain|domain]] and [[codomain]]. Let \n\n:<math>f = \\sum a_n X^n \\in R[[X]],</math> \n\nand suppose ''S'' is a commutative associative algebra over ''R'', ''I'' is an ideal in ''S'' such that the [[I-adic topology]] on ''S'' is complete, and ''x'' is an element of ''I''. Define:\n\n:<math>f(x) = \\sum_{n\\ge 0} a_n x^n.</math>\n\nThis series is guaranteed to converge in ''S'' given the above assumptions on ''x''. Furthermore, we have\n\n:<math> (f+g)(x) = f(x) + g(x)</math>\n\nand\n\n:<math> (fg)(x) = f(x) g(x).</math>\n\nUnlike in the case of bona fide functions, these formulas are not definitions but have to be proved.\n\nSince the topology on <math>R[[X]]</math> is the (''X'')-adic topology and <math>R[[X]]</math> is complete, we can in particular apply power series to other power series, provided that the arguments don't have [[constant coefficients]] (so that they belong to the ideal (''X'')): ''f''(0), ''f''(''X''<sup>2</sup>−''X'') and ''f''((1−''X'')<sup>−1</sup>&nbsp;−&nbsp;1) are all well defined for any formal power series <math>f \\in R[[X]].</math>\n\nWith this formalism, we can give an explicit formula for the multiplicative inverse of a power series ''f'' whose constant coefficient ''a'' = ''f''(0) is invertible in ''R'':\n\n:<math>f^{-1} = \\sum_{n \\ge 0} a^{-n-1} (a-f)^n.</math>\n\nIf the formal power series ''g'' with ''g''(0) = 0 is given implicitly by the equation\n\n:<math>f(g) =X</math>\n\nwhere ''f'' is a known power series with ''f''(0) = 0, then the coefficients of ''g'' can be explicitly computed using the [[#The Lagrange inversion formula|Lagrange inversion formula]].\n\n==Generalizations==\n\n=== Formal Laurent series ===\nA '''formal Laurent series''' over a ring <math>R</math> is defined in a similar way to a formal power series, except that we also allow finitely many terms of negative degree (this is different from the classical [[Laurent series]]), that is series of the form\n\n:<math>f = \\sum_{n\\in\\Z} a_n X^n</math>\n\nwhere '''<math>a_n=0</math> for all but finitely many negative indices <math>n</math>'''. Multiplication of such series can be defined. Indeed, similarly to the definition for formal power series, the coefficient of ''X<sup>k</sup>'' of two series with respective sequences of coefficients <math>\\{a_n\\}</math> and <math>\\{b_n\\}</math> is\n\n:<math>\\sum_{i\\in\\Z}a_ib_{k-i},</math>\n\nwhich sum is effectively finite because of the assumed vanishing of coefficients at sufficiently negative indices, and which sum zero for sufficiently negative <math>k</math> for the same reason.\n\nFor a non-zero formal Laurent series, the minimal integer <math>n</math> such that <math> a_n\\neq 0</math> is called the order of <math>f</math>, denoted <math>\\operatorname{ord}(f).</math> (The order of the zero series is <math>+\\infty</math>.) The formal Laurent series form the '''ring of formal Laurent series''' over <math>R</math>, denoted by <math>R((X))</math>. It is equal to the [[localization of a ring|localization]] of <math>R[[X]]</math> with respect to the set of positive powers of <math>X</math>. It is a topological ring with the metric:\n\n:<math>d(f,g)=2^{-\\operatorname{ord}(f-g)}.</math>\n\nIf <math>R=K</math> is a [[field (mathematics)|field]], then <math>K((X))</math> is in fact a field, which may alternatively be obtained as the [[field of fractions]] of the [[integral domain]] <math>K[[X]]</math>.\n\nOne may define formal differentiation for formal Laurent series in a natural way (term-by-term). Precisely, the formal derivative of the formal Laurent series <math>f</math> above is\n\n:<math>f' = Df = \\sum_{n\\in\\Z} na_n X^{n-1}</math>\n\nwhich is again an element of <math>K((X))</math>. Notice that if <math>f</math> is a non-constant formal Laurent series, and K is a field of characteristic 0, then one has\n\n:<math>\\operatorname{ord}(f')= \\operatorname{ord}(f)-1.</math>\n\nHowever, in general this is not the case since the factor ''n'' for the lowest order term could be equal to 0 in ''R''.\n\n====Formal residue====\nAssume that <math>K</math> is a field of [[characteristic (algebra)|characteristic]] 0. Then the map\n\n:<math>D\\colon K((X))\\to K((X))</math>\n\nis a <math>K</math>-[[derivation (abstract algebra)|derivation]] that satisfies\n\n:<math>\\ker D=K</math>\n:<math>\\operatorname{im} D= \\left \\{f\\in K((X)) : [X^{-1}]f=0 \\right \\}.</math>\n\nThe latter shows that the coefficient of <math>X^{-1}</math> in <math>f</math> is of particular interest; it is called ''formal residue of <math>f</math>'' and denoted <math>\\operatorname{Res}(f)</math>. The map\n\n:<math>\\operatorname{Res} : K((X))\\to K</math>\n\nis <math>K</math>-linear, and by the above observation one has an [[exact sequence]]\n\n:<math>0 \\to K \\to K((X)) \\xrightarrow{D} K((X)) \\;\\xrightarrow{\\operatorname{Res}}\\; K \\to 0.</math>\n\n'''Some rules of calculus'''. As a quite direct consequence of the above definition, and of the rules of formal derivation, one has, for any <math>f, g\\in K((X))</math>\n:i. <math>\\operatorname{Res}(f')=0;</math>\n:ii. <math>\\operatorname{Res}(fg')=-\\operatorname{Res}(f'g);</math>\n:iii. <math>\\operatorname{Res}(f'/f)=\\operatorname{ord}(f),\\qquad \\forall f\\neq0;</math>\n:iv. <math>\\operatorname{Res}\\left(( f\\circ g) g'\\right) = \\operatorname{ord}(g)\\operatorname{Res}(f),</math> if <math>\\operatorname{ord}(g)>0;</math>\n:v. <math>[X^n]f(X)=\\operatorname{Res}\\left(X^{-n-1}f(X)\\right).</math>\n\nProperty (i) is part of the exact sequence above. Property (ii) follows from (i) as applied to <math>(fg)'=f'g+fg'</math>. Property (iii): any <math>f</math> can be written in the form <math>f=X^mg</math>, with <math>m=\\operatorname{ord}(f)</math> and <math>\\operatorname{ord}(g)=0</math>: then <math>f'/f = mX^{-1}+g'/g.</math> <math>\\operatorname{ord}(g)=0</math> implies <math>g</math> is invertible in <math>K[[X]]\\subset \\operatorname{im}(D) = \\ker(\\operatorname{Res}),</math> whence <math>\\operatorname{Res}(f'/f)=m.</math> Property (iv): Since <math>\\operatorname{im}(D) = \\ker(\\operatorname{Res}),</math> we can write <math>f=f_{-1}X^{-1}+F',</math> with <math>F \\in K[[X]]</math>. Consequently, <math>(f\\circ g)g'= f_{-1}g^{-1}g'+(F'\\circ g)g' = f_{-1}g'/g + (F \\circ g)'</math> and (iv) follows from (i) and (iii). Property (v) is clear from the definition.\n\n===The Lagrange inversion formula===\n{{main|Lagrange inversion theorem}}\nAs mentioned above, any formal series <math>f \\in K[[X]]</math> with ''f''<sub>0</sub> = 0 and ''f''<sub>1</sub> ≠ 0 has a composition inverse <math>g \\in K[[X]].</math> The following relation between the coefficients ''g<sup>n</sup>'' and ''f''<sup>−''k''</sup> holds (\"{{Visible anchor|Lagrange inversion formula}}\"):\n\n:<math>k[X^k] g^n=n[X^{-n}]f^{-k}.</math>\n\nIn particular, for ''n''&nbsp;=&nbsp;1 and all ''k''&nbsp;≥&nbsp;1,\n\n:<math>[X^k] g=\\frac{1}{k} \\operatorname{Res}\\left( f^{-k}\\right).</math>\n\nSince the proof of the Lagrange inversion formula is a very short computation, it is worth reporting it here. Since <math>\\operatorname{ord}(f) =1 </math>, by the above rules of calculus,\n\n:<math>\n\\begin{align}\nk[X^k] g^n & =k\\operatorname{Res}\\left( g^n X^{-k-1}  \\right)\n=k\\operatorname{Res}\\left(X^n f^{-k-1}f\\,'\\right)\n=-\\operatorname{Res}\\left(X^n (f^{-k})^'\\right) \\\\\n& =\\operatorname{Res}\\left(\\left(X^n\\right)' f^{-k}\\right)\n=n\\operatorname{Res}\\left(X^{n-1}f^{-k}\\right)\n=n[X^{-n}]f^{-k}.\n\\end{align}\n</math>\n\n'''Generalizations.''' One may observe that the above computation can be repeated plainly in more general settings than ''K''((''X'')): a generalization of the Lagrange inversion formula is already available working in the <math>\\Complex((X))</math>-modules <math>X^{\\alpha}\\Complex((X)),</math> where α is a complex exponent. As a consequence, if ''f'' and ''g'' are as above, with <math>f_1=g_1=1</math>, we can relate the complex powers of ''f''/''X'' and ''g''/''X'': precisely, if α and β are non-zero complex numbers with negative integer sum, <math>m=-\\alpha-\\beta\\in\\N,</math> then\n\n:<math>\\frac{1}{\\alpha}[X^m]\\left( \\frac{f}{X} \\right)^\\alpha=-\\frac{1}{\\beta}[X^m]\\left( \\frac{g}{X} \\right)^\\beta.</math>\n\nFor instance, this way one finds the power series for [[Lambert W function#Integer and complex powers|complex powers of the Lambert function]].\n\n=== Power series in several variables ===\nFormal power series in any number of indeterminates (even infinitely many) can be defined. If ''I'' is an index set and ''X<sub>I</sub>'' is the set of indeterminates ''X<sub>i</sub>'' for ''i''∈''I'', then a [[monomial]] ''X''<sup>α</sup> is any finite product of elements of ''X<sub>I</sub>'' (repetitions allowed); a formal power series in ''X<sub>I</sub>'' with coefficients in a ring ''R'' is determined by any mapping from the set of monomials ''X''<sup>α</sup> to a corresponding coefficient ''c''<sub>α</sub>, and is denoted <math>\\textstyle\\sum_\\alpha c_\\alpha X^\\alpha</math>. The set of all such formal power series is denoted <math>R[[X_I]],</math> and it is given a ring structure by defining\n\n:<math>\\left(\\sum_\\alpha c_\\alpha X^\\alpha\\right)+\\left(\\sum_\\alpha d_\\alpha X^\\alpha \\right)= \\sum_\\alpha (c_\\alpha+d_\\alpha) X^\\alpha</math>\n\nand\n\n:<math>\\left(\\sum_\\alpha c_\\alpha X^\\alpha\\right)\\times\\left(\\sum_\\beta d_\\beta X^\\beta\\right)=\\sum_{\\alpha,\\beta} c_\\alpha d_\\beta X^{\\alpha+\\beta}</math>\n\n==== Topology ====\nThe topology on <math>R[[X_I]]</math> is such that a sequence of its elements converges only if for each monomial ''X''<sup>α</sup> the corresponding coefficient stabilizes. If ''I'' is finite, then this the ''J''-adic topology, where ''J'' is the ideal of <math>R[[X_I]]</math> generated by all the indeterminates in ''X<sub>I</sub>''. This does not hold if ''I'' is infinite. For example, if <math>I=\\N,</math> then the sequence <math>(f_n)_{n\\in \\N}</math> with <math>f_n = X_n + X_{n+1} + X_{n+2} + \\cdots </math> does not converge with respect to any ''J''-adic topology on ''R'', but clearly for each monomial the corresponding coefficient stabilizes.\n\nAs remarked above, the topology on a repeated formal power series ring like <math>R[[X]][[Y]]</math> is usually chosen in such a way that it becomes isomorphic as a [[topological ring]] to <math>R[[X,Y]].</math>\n\n====Operations====\nAll of the operations defined for series in one variable may be extended to the several variables case.\n\n* A series is invertible if and only if its constant term is invertible in ''R''.\n\n* The composition ''f''(''g''(''X'')) of two series ''f'' and ''g'' is defined if ''f'' is a series in a single indeterminate, and the constant term of ''g'' is zero. For a series ''f'' in several indeterminates a form of \"composition\" can similarly be defined, with as many separate series in the place of ''g'' as there are indeterminates.\n\nIn the case of the formal derivative, there are now separate [[partial derivative]] operators, which differentiate with respect to each of the indeterminates. They all commute with each other.\n\n==== Universal property ====\nIn the several variables case, the universal property characterizing <math>R[[X_1, \\ldots, X_r]]</math> becomes the following. If ''S'' is a commutative associative algebra over ''R'', if ''I'' is an ideal of ''S'' such that the ''I''-adic topology on ''S'' is complete, and if ''x''<sub>1</sub>, ..., ''x<sub>r</sub>'' are elements of ''I'', then there is a ''unique'' map <math>\\Phi: R[[X_1, \\ldots, X_r]] \\to S</math> with the following properties:\n\n* Φ is an ''R''-algebra homomorphism\n\n* Φ is continuous\n\n* Φ(''X''<sub>''i''</sub>) = ''x''<sub>''i''</sub> for ''i'' = 1, ..., ''r''.\n\n===Non-commuting variables===\nThe several variable case can be further generalised by taking ''non-commuting variables'' ''X<sub>i</sub>'' for ''i'' ∈ ''I'', where ''I'' is an index set and then a [[monomial]] ''X''<sup>α</sup> is any [[word (mathematics)|word]] in the ''X<sub>I</sub>''; a formal power series in ''X<sub>I</sub>'' with coefficients in a ring ''R'' is determined by any mapping from the set of monomials ''X''<sup>α</sup> to a corresponding coefficient ''c''<sub>α</sub>, and is denoted <math>\\textstyle\\sum_\\alpha c_\\alpha X^\\alpha </math>. The set of all such formal power series is denoted ''R''«''X<sub>I</sub>''», and it is given a ring structure by defining addition pointwise\n\n:<math>\\left(\\sum_\\alpha c_\\alpha X^\\alpha\\right)+\\left(\\sum_\\alpha d_\\alpha X^\\alpha\\right)=\\sum_\\alpha(c_\\alpha+d_\\alpha)X^\\alpha</math>\n\nand multiplication by\n\n:<math>\\left(\\sum_\\alpha c_\\alpha X^\\alpha\\right)\\times\\left(\\sum_\\alpha d_\\alpha X^\\alpha\\right)=\\sum_{\\alpha,\\beta} c_\\alpha d_\\beta X^{\\alpha} \\cdot X^{\\beta}</math>\n\nwhere · denotes concatenation of words. These formal power series over ''R'' form the '''Magnus ring''' over ''R''.<ref>{{cite book | first=Helmut | last=Koch | title=Algebraic Number Theory | publisher=[[Springer-Verlag]] | year=1997 | isbn=978-3-540-63003-6 | zbl=0819.11044 | series=Encycl. Math. Sci. | volume=62 | edition=2nd printing of 1st | page=167 }}</ref><ref>{{cite book | title=The Mathematical Theory of Knots and Braids: An Introduction | volume=82 | series=North-Holland Mathematics Studies | first=Siegfried | last=Moran | publisher=Elsevier | year=1983 | isbn=978-0-444-86714-8 | page=211 | zbl=0528.57001 }}</ref>\n\n=== On a semiring ===\n{{expand section|sum, product, examples|date=August 2014}}\n\nGiven an [[Alphabet (formal_languages)|alphabet]] <math>\\Sigma</math> and a [[semiring]] <math>S</math>. The formal power series over <math>S</math> supported on the language <math>\\Sigma^*</math> is denoted by <math>S\\langle\\langle \\Sigma^*\\rangle\\rangle</math>. It consists of all mappings <math>r:\\Sigma^*\\to S</math>, where <math>\\Sigma^*</math> is the [[free monoid]] generated by the non-empty set <math>\\Sigma</math>.\n\nThe elements of <math>S\\langle\\langle \\Sigma^*\\rangle\\rangle</math> can be written as formal sums\n\n:<math>r = \\sum_{w \\in \\Sigma^*} (r,w)w.</math>\n\nwhere <math>(r,w)</math> denotes the value of <math>r</math> at the word <math>w\\in\\Sigma^*</math>. The elements <math>(r,w)\\in S</math> are called the coefficients of <math>r</math>.\n\nFor <math>r\\in S\\langle\\langle \\Sigma^*\\rangle\\rangle</math> the support of <math>r</math> is the set\n\n:<math>\\operatorname{supp}(r)=\\{w\\in\\Sigma^*|\\ (r,w)\\neq 0\\}</math>\n\nA series where every coefficient is either <math>0</math> or <math>1</math> is called the characteristic series of its support.\n\nThe subset of <math>S\\langle\\langle \\Sigma^*\\rangle\\rangle</math> consisting of all series with a finite support is denoted by <math>S\\langle \\Sigma^*\\rangle</math> and called polynomials.\n\nFor <math>r_1, r_2\\in S\\langle\\langle \\Sigma^*\\rangle\\rangle</math> and <math>s\\in S</math>, the sum <math>r_1+r_2</math> is defined by \n:<math>(r_1+r_2,w)=(r_1,w)+(r_2,w)</math>\nThe (Cauchy) product <math>r_1\\cdot r_2</math> is defined by\n:<math>(r_1\\cdot r_2,w) = \\sum_{w_1w_2=w}(r_1,w_1)(r_2,w_2)</math>\nThe Hadamard product <math>r_1\\odot r_2</math> is defined by\n:<math>(r_1\\odot r_2,w)=(r_1,w)(r_2,w)</math>\nAnd the products by a scalar <math>sr_1</math> and <math>r_1s</math> by\n:<math>(sr_1,w)=s(r_1,w)</math> and <math>(r_1s,w)=(r_1,w)s</math>, respectively.\n\nWith these operations <math>(S\\langle\\langle \\Sigma^*\\rangle\\rangle,+,\\cdot,0,\\varepsilon)</math> and <math>(S\\langle \\Sigma^*\\rangle, +,\\cdot,0,\\varepsilon)</math> are semirings, where <math>\\varepsilon</math> is the empty word in <math>\\Sigma^*</math>.\n\nThese formal power series are used to model the behavior of [[weighted automata]], in [[theoretical computer science]], when the coefficients <math>(r,w)</math> of the series are taken to be the weight of a path with label <math>w</math> in the automata.<!-- the correct symbols for the double angled braces are &#10218; and &#10219; but they work poorly in many browsers. Wikipedia's TeX doesn't support \\llangle and \\rrangle. Also no support for Greek italics in wiki TeX it seems --><ref> Droste, M., & Kuich, W. (2009). Semirings and Formal Power Series. ''Handbook of Weighted Automata'', 3–28. {{doi|10.1007/978-3-642-01492-5_1}}, p. 12</ref>\n\n===Replacing the index set by an ordered abelian group===\n{{Main|Hahn series}}\n\nSuppose <math>G</math> is an ordered abelian group, meaning an abelian group with a total ordering <math><</math> respecting the group's addition, so that <math>a<b</math> if and only if <math>a+c<b+c</math> for all <math>c</math>. Let '''I''' be a [[well-order]]ed subset of <math>G</math>, meaning '''I''' contains no infinite descending chain. Consider the set consisting of\n\n:<math>\\sum_{i \\in I} a_i X^i </math>\n\nfor all such '''I''', with <math>a_i</math> in a commutative ring <math>R</math>, where we assume that for any index set, if all of the <math>a_i</math> are zero then the sum is zero. Then <math>R((G))</math> is the ring of formal power series on <math>G</math>; because of the condition that the indexing set be well-ordered the product is well-defined, and we of course assume that two elements which differ by zero are the same. Sometimes the notation <math>[[R^G]]</math> is used to denote <math>R((G))</math>.<ref>{{cite journal | first1=Khodr | last1=Shamseddine | first2=Martin | last2=Berz | url= http://www.physics.umanitoba.ca/~khodr/Publications/RS-Overview-offprints.pdf | title=Analysis on the Levi-Civita Field: A Brief Overview | journal= Contemporary Mathematics | volume=508 | pages=215–237 | date=2010}}</ref>\n\nVarious properties of <math>R</math> transfer to <math>R((G))</math>. If <math>R</math> is a field, then so is <math>R((G))</math>. If <math>R</math> is an ordered field, we can order <math>R((G))</math> by setting any element to have the same sign as its leading coefficient, defined as the least element of the index set '''I''' associated to a non-zero coefficient. Finally if <math>G</math> is a [[divisible group]] and <math>R</math> is a [[real closed field]], then <math>R((G))</math> is a real closed field, and if <math>R</math> is [[algebraically closed]], then so is <math>R((G))</math>.\n\nThis theory is due to [[Hans Hahn (mathematician)|Hans Hahn]], who also showed that one obtains subfields when the number of (non-zero) terms is bounded by some fixed infinite cardinality.\n\n==Examples and related topics==\n* [[Bell series]] are used to study the properties of [[Multiplicative function|multiplicative arithmetic functions]]\n* [[Formal group]]s are used to define an abstract group law using formal power series\n* [[Puiseux series]] are an extension of formal Laurent series, allowing fractional exponents\n* [[Rational series]]\n\n==Notes==\n{{Reflist}}\n\n== References ==\n* {{cite book | last1=Berstel | first1=Jean | last2=Reutenauer | first2=Christophe | title=Noncommutative rational series with applications | series =Encyclopedia of Mathematics and Its Applications | volume=137 | location=Cambridge | publisher=[[Cambridge University Press]] | year=2011 | isbn=978-0-521-19022-0 | zbl=1250.68007 }}\n* [[Nicolas Bourbaki]]: ''Algebra'', IV, §4. Springer-Verlag 1988.\n\n==Further reading==\n* W. Kuich. Semirings and formal power series: Their relevance to formal languages and automata theory. In G. Rozenberg and A. Salomaa, editors, Handbook of Formal Languages, volume 1, Chapter 9, pages 609–677. Springer, Berlin, 1997, {{ISBN|3-540-60420-0}}\n* Droste, M., & Kuich, W. (2009). Semirings and Formal Power Series. ''Handbook of Weighted Automata'', 3–28. {{doi|10.1007/978-3-642-01492-5_1}}\n\n{{DEFAULTSORT:Formal Power Series}}\n[[Category:Abstract algebra]]\n[[Category:Ring theory]]\n[[Category:Enumerative combinatorics]]\n[[Category:Mathematical series]]"
    },
    {
      "title": "Free object",
      "url": "https://en.wikipedia.org/wiki/Free_object",
      "text": "In [[mathematics]], the idea of a '''free object''' is one of the basic concepts of [[abstract algebra]]. It is a part of [[universal algebra]], in the sense that it relates to all types of algebraic structure (with [[finitary]] operations). It also has a formulation in terms of [[category theory]], although this is in yet more abstract terms. Examples include [[free group]]s, [[tensor algebra]]s, or [[free lattice]]s.   Informally, a free object over a set ''A'' can be thought of as being a \"generic\" algebraic structure over ''A'': the only equations that hold between elements of the free object are those that follow from the defining axioms of the algebraic structure.\n\n==Definition==\n\nFree objects are the direct generalization to [[Category (mathematics)|categories]] of the notion of [[Basis (linear algebra)|basis]] in a vector space. A linear function {{nowrap|''u'' : ''E''<sub>1</sub> → ''E''<sub>2</sub>}} between vector spaces is entirely determined by its values on a basis of the vector space ''E''<sub>1</sub>. The following definition translates this to any category.\n\nLet (''C'',''F'') be a [[concrete category]] (i.e. {{nowrap|''F'' : ''C'' → '''Set'''}} is a [[faithful functor]]), let ''X'' be a set (called ''basis''), {{nowrap|''A'' ∈ ''C''}} an object, and {{nowrap|''i'' : ''X'' → ''F''(''A'')}} a injective map between sets (called ''canonical insertion''). We say that ''A'' is the '''free object on ''X''''' (with respect to ''i'') if and only if they satisfy this [[universal property]]:\n:for any object ''B'' and any map between sets {{nowrap|''f'' : ''X'' → ''F''(''B'')}}, there exists a unique morphism {{nowrap|''g'' : ''A'' → ''B''}} such that {{nowrap|1=''f'' = ''F''(''g'') ∘ ''i''}}. That is, the following diagram commutes:\n\n:<math>\n\\begin{array}{c}\nX \\xrightarrow{\\quad i \\quad} F(A) \\\\\n{}_f \\searrow \\quad \\swarrow {}_{F(g)} \\\\\nF(B) \\quad \\\\\n\\end{array}\n</math>\n\nIn this way the free functor that builds the free object ''A'' from the set ''X'' becomes [[left adjoint]] to the [[forgetful functor]].\n\n==Examples==\nThe creation of free objects proceeds in two steps. For algebras that conform to the [[associative law]], the first step is to consider the collection of all possible [[string (computer science)|word]]s formed from an [[alphabet (computer science)|alphabet]]. Then one imposes a set of [[equivalence relation]]s upon the words, where the relations are the defining relations of the algebraic object at hand. The free object then consists of the set of [[equivalence class]]es.\n\nConsider, for example, the construction of the free group in two generators. One starts with an alphabet consisting of the five letters <math>\\{e,a,b,a^{-1},b^{-1}\\}</math>. In the first step, there is not yet any assigned meaning to the \"letters\" <math>a^{-1}</math> or <math>b^{-1}</math>; these will be given later, in the second step. Thus, one could equally well start with the alphabet in five letters that is <math>S=\\{a,b,c,d,e\\}</math>. In this example, the set of all words or strings <math>W(S)</math> will include strings such as ''aebecede'' and ''abdc'', and so on, of arbitrary finite length, with the letters arranged in every possible order.\n\nIn the next step, one imposes a set of equivalence relations. The equivalence relations for a [[group (mathematics)|group]] are that of multiplication by the identity, <math>ge=eg=g</math>, and the multiplication of inverses: <math>gg^{-1}=g^{-1}g=e</math>. Applying these relations to the strings above, one obtains\n\n:<math>aebecede=aba^{-1}b^{-1}</math>\n\nwhere it was understood that ''c'' is a stand-in for <math>a^{-1}</math>, and ''d'' is a stand-in for <math>b^{-1}</math>, while ''e'' is the identity element. Similarly, one has\n\n:<math>abdc=abb^{-1}a^{-1}=e</math>\n\nDenoting the equivalence relation or [[congruence relation|congruence]] by <math>\\sim</math>, the free object is then the collection of [[equivalence class]]es of words. Thus, in this example, the free group in two generators is the [[quotient set|quotient]]\n\n:<math>F_2=W(S)/\\sim</math>\n\nThis is often written as\n\n:<math>F_2=W(S)/E</math>\n\nwhere\n:<math>W(S)=\\{a_1a_2\\ldots a_n\\,\\vert\\; a_k\\in S\\,; \\,n\\mbox{ finite } \\}</math>\n\nis the set of all words, and\n\n:<math>E=\\{a_1a_2\\ldots a_n\\,\\vert\\; e=a_1a_2\\ldots a_n\\,;\\, a_k\\in S\\,;\\,n\\mbox{ finite }\\}</math>\nis the equivalence class of the identity, after the relations defining a group are imposed.\n\nA simpler example are the [[free monoid]]s. The free monoid on a set ''X'', is the monoid of all finite [[string (computer science)|strings]] using ''X'' as alphabet, with operation [[concatenation]] of strings. The identity is the empty string. In essence, the free monoid is simply the set of all words, with no equivalence relations imposed. This example is developed further in the article on the [[Kleene star]].\n\n===General case===\nIn the general case, the algebraic relations need not be associative, in which case the starting point is not the set of all words, but rather, strings punctuated with parentheses, which are used to indicate the non-associative groupings of letters. Such a string may equivalently be represented by a [[binary tree]] or a [[free magma]]; the leaves of the tree are the letters from the alphabet.\n\nThe algebraic relations may then be general [[arity|arities]] or [[finitary relation]]s on the leaves of the tree. Rather than starting with the collection of all possible parenthesized strings, it can be more convenient to start with the [[Herbrand universe]]. Properly describing or enumerating the contents of a free object can be easy or difficult, depending on the particular algebraic object in question. For example, the free group in two generators is easily described. By contrast, little or nothing is known about the structure of [[free Heyting algebra]]s in more than one generator.<ref>Peter T. Johnstone, ''Stone Spaces'', (1982) Cambridge University Press, {{ISBN|0-521-23893-5}}. ''(A treatment of the one-generator free Heyting algebra is given in chapter 1, section 4.11)''</ref> The problem of determining if two different strings belong to the same equivalence class is known as the [[word problem (mathematics)|word problem]].\n\nAs the examples suggest, free objects look like constructions from [[syntax]]; one may reverse that to some extent by saying that major uses of syntax can be explained and characterised as free objects, in a way that makes apparently heavy 'punctuation' explicable (and more memorable).{{Clarify|date=May 2017}}\n\n==Free universal algebras==\n{{main|term algebra}}\n{{Expand section|date=June 2008}}\n\nLet <math>S</math> be any set, let <math>\\mathbf{A}</math> be an [[algebraic structure]] of type <math>\\rho</math> generated by <math>S</math>. Let the underlying set of this algebraic structure <math>\\mathbf{A}</math>, sometimes called its universe, be <math>A</math>, and let <math>\\psi :S \\longrightarrow A</math> be a function.  We say that <math>(</math><math>A</math>,<math> \\psi)</math> (or informally just <math>\\mathbf{A}</math>) is a ''free algebra'' (of type <math>\\rho</math>) on the set <math>S</math> of ''free generators'' if, for every algebra <math>\\mathbf{B}</math> of type <math>\\rho</math> and function <math>\\tau : S \\longrightarrow B</math>, where <math>B</math> is a universe of <math>\\mathbf{B}</math>, there exists a unique homomorphism <math>\\sigma :A \\longrightarrow B</math> such that <math>\\sigma \\psi = \\tau</math>.\n\n==Free functor<!--'Free functor' and 'Cofree functor' redirect here-->==\nThe most general setting for a free object is in [[category theory]], where one defines a [[functor]], the '''free functor'''<!--boldface per WP:R#PLA-->, that is the [[left adjoint]] to the [[forgetful functor]].\n\nConsider the category '''C''' of [[algebraic structure]]s; these can be thought of as sets plus operations, obeying some laws. This category has a functor, <math>U:\\mathbf{C}\\to\\mathbf{Set}</math>, the [[forgetful functor]], which maps objects and functions in '''C''' to '''Set''', the [[category of sets]]. The forgetful functor is very simple: it just ignores all of the operations.\n\nThe free functor ''F'', when it exists, is the left adjoint to ''U''. That is, <math>F:\\mathbf{Set}\\to\\mathbf{C}</math> takes sets ''X'' in '''Set''' to their corresponding free objects ''F''(''X'') in the category '''C'''.  The set ''X'' can be thought of as the set of \"generators\" of the free object ''F''(''X'').\n\nFor the free functor to be a left adjoint, one must also have a '''Set'''-morphism  <math>\\eta:X\\to U(F(X))\\,\\!</math>.   More explicitly, ''F'' is, up to isomorphisms in '''C''', characterized by the following [[universal property]]:\n{{block indent|Whenever ''A'' is an algebra in '''C''', and {{nowrap|''g'' : ''X'' → ''U''(''A'')}} is a function (a morphism in the category of sets), then there is a unique '''C'''-morphism {{nowrap|''h'' : ''F''(''X'') → ''A''}} such that {{nowrap|1=''U''(''h'') ∘ ''η'' = ''g''}}.}}\n\nConcretely, this sends a set into the free object on that set; it is the \"inclusion of a basis\". Abusing notation, <math>X \\to F(X)</math> (this abuses notation because ''X'' is a set, while ''F''(''X'') is an algebra; correctly, it is <math>X \\to U(F(X))</math>).\n\nThe [[natural transformation]] <math>\\eta:\\operatorname{id}_\\mathbf{Set}\\to UF</math> is called the [[unit (category theory)|unit]]; together with the [[counit]] <math>\\varepsilon:FU\\to \\operatorname {id}_\\mathbf{C}</math>, one may construct a [[T-algebra]], and so a [[monad (category theory)|monad]].\n\nThe '''cofree functor'''<!--boldface per WP:R#PLA--> is the [[right adjoint]] to the forgetful functor.\n\n===Existence===\nThere are general existence theorems that apply; the most basic of them guarantees that \n{{block indent|Whenever '''C''' is a [[variety (universal algebra)|variety]], then for every set ''X'' there is a free object ''F''(''X'') in '''C'''.}}\n\nHere, a variety is a synonym for a [[finitary algebraic category]], thus implying that the set of relations are [[finitary relation|finitary]], and ''algebraic'' because it is [[monad (category theory)|monadic]] over '''Set'''.\n\n===General case===\nOther types of forgetfulness also give rise to objects quite like free objects, in that they are left adjoint to a forgetful functor, not necessarily to sets.\n\nFor example, the [[tensor algebra]] construction on a [[vector space]] as left adjoint to the functor on [[associative algebra]]s that ignores the algebra structure. It is therefore often also called a [[free algebra]].\n\nLikewise the [[symmetric algebra]] and [[exterior algebra]] are free symmetric and anti-symmetric algebras on a vector space.\n\n==List of free objects==\n{{See also|Category:Free algebraic structures}}\nSpecific kinds of free objects include:\n*[[free algebra]]\n**[[free associative algebra]]\n**[[free commutative algebra]]\n*[[free category]]\n**[[free strict monoidal category]]\n*[[free group]]\n**[[free abelian group]]\n**[[free partially commutative group]]\n*[[Kleene algebra#Examples|free Kleene algebra]]\n*[[free lattice]]\n**[[free Boolean algebra]]\n**[[distributive lattice#Free distributive lattices|free distributive lattice]]\n**[[free Heyting algebra]]\n*[[free Lie algebra]]\n*[[free magma]]\n*[[free module]], and in particular, [[vector space]]\n*[[free monoid]]\n**[[free monoid#The free commutative monoid|free commutative monoid]]\n**[[free partially commutative monoid]]\n*[[free ring]]\n*[[free semigroup]]\n*[[free semiring]]\n**[[semiring#Examples|free commutative semiring]]\n*[[free theory]]\n*[[term algebra]]\n*[[discrete space]]\n\n==See also==\n*[[Generating set]]\n\n==Notes==\n<references/>\n\n{{DEFAULTSORT:Free Object}}\n[[Category:Mathematics articles needing expert attention]]\n[[Category:Abstract algebra]]\n[[Category:Free algebraic structures| ]]\n[[Category:Combinatorics on words]]\n[[Category:Adjoint functors]]"
    },
    {
      "title": "Garside element",
      "url": "https://en.wikipedia.org/wiki/Garside_element",
      "text": "{{Orphan|date=May 2014}}\n\nIn [[mathematics]], a '''Garside element''' is an element of an [[algebraic structure]] such as a [[monoid]] that has several desirable properties.\n\nFormally, if ''M'' is a monoid, then an element &Delta; of ''M'' is said to be a '''Garside element''' if the set of all right divisors of &Delta;,\n:<math>\\{ r \\in M \\mid \\mbox{for some } x \\in M, \\Delta = x r \\},</math>\nis the same set as the set of all left divisors of &Delta;,\n:<math>\\{ \\ell \\in M \\mid \\mbox{for some } x \\in M, \\Delta = \\ell x \\},</math>\nand this set [[generating set|generates]] ''M''.\n\nA Garside element is in general not unique: any power of a Garside element is again a Garside element.\n\n==Garside monoid and Garside group==\nA '''Garside monoid''' is a monoid with the following properties:\n* Finitely generated and atomic;\n* [[Cancellative]];\n* The [[Partially ordered set|partial order]] relations of divisibility are [[Lattice (order)|lattices]];\n* There exists a Garside element.\n\nA Garside monoid satisfies the [[Ore condition for multiplicative sets]] and hence embeds in its group of fractions: such a group is a '''Garside group'''.  A Garside group is [[biautomatic group|biautomatic]] and hence has soluble [[Word problem for groups|word problem]] and [[conjugacy problem]].  Examples of such groups include [[braid group]]s and, more generally, [[Artin group]]s of [[Artin group of finite type|finite Coxeter type]].<ref name=dehorneyparis>{{citation | last1=Dehornoy | first1=Patrick | last2=Paris | first2=Luis | title=Gaussian groups and Garside groups, two generalisations of Artin groups | journal=Proceedings of the London Mathematical Society | volume=79 | number=3 | pages=569–604 | year=1999 | doi=10.1112/s0024611599012071| citeseerx=10.1.1.595.739 }}</ref>\n\nThe name was coined by Dehornoy and Paris<ref name=dehorneyparis/> to mark the work of Frank Arnold Garside (1915-1988), a teacher at Magdalen College School, Oxford who served as Lord Mayor of Oxford in 1984-5, on the conjugacy problem for braid groups.<ref>{{citation | last=Garside | first=F.A. | title=The braid group and other groups | journal=Q. J. Math., Oxf. II. Ser. | volume=20 | pages=235–254 | year=1969 | doi=10.1093/qmath/20.1.235}}</ref>\n\n==References==\n{{reflist}}\n* Benson Farb, ''Problems on mapping class groups and related topics'' (Volume 74 of Proceedings of symposia in pure mathematics) AMS Bookstore, 2006, {{ISBN|0-8218-3838-5}}, p.&nbsp;357\n* Patrick Dehornoy, \"Groupes de Garside\", ''Ann .Sci. Ecole Norm. Sup. (4)'' '''35''' (2002) 267-306.  [[Mathematics Reviews|MR]] 2003f:20067.\n* Matthieu Picantin, \"Garside monoids vs divisibility monoids\", ''Math. Structures Comput. Sci.'' '''15''' (2005) 231-242.  [[Mathematics Reviews|MR]] 2006d:20102.\n\n[[Category:Abstract algebra]]\n[[Category:Semigroup theory]]\n\n\n{{Abstract-algebra-stub}}"
    },
    {
      "title": "General linear group",
      "url": "https://en.wikipedia.org/wiki/General_linear_group",
      "text": "{{Group theory sidebar |top/lie}}{{Use American English|date = January 2019}}\n{{Short description|n x n invertible matrices over a ring}}\n{{Lie groups |classical}}\n\nIn [[mathematics]], the '''general linear group''' of degree ''n'' is the set of {{nowrap|''n''×''n''}} [[invertible matrix|invertible matrices]], together with the operation of ordinary [[matrix multiplication]]. This forms a [[group (mathematics)|group]], because the product of two invertible matrices is again invertible, and the inverse of an invertible matrix is invertible. The group is so named because the columns of an invertible matrix are [[linearly independent]], hence the vectors/points they define are in [[general linear position]], and matrices in the general linear group take points in general linear position to points in general linear position.\n\nTo be more precise, it is necessary to specify what kind of objects may appear in the entries of the matrix. For example, the general linear group over '''R''' (the set of [[real numbers]]) is the group of {{nowrap|''n''×''n''}} invertible matrices of real numbers, and is denoted by GL<sub>''n''</sub>('''R''') or {{nowrap|GL(''n'', '''R''')}}.\n\nMore generally, the general linear group of degree ''n'' over any [[field (mathematics)|field]] ''F'' (such as the [[complex number]]s), or a [[ring (mathematics)|ring]] ''R'' (such as the ring of [[integer]]s), is the set of {{nowrap|''n''×''n''}} invertible matrices with entries from ''F'' (or ''R''), again with matrix multiplication as the group operation.<ref name=\"ring\">Here rings are assumed to be [[Ring (mathematics)#Notes on the definition|associative and unital]].</ref> Typical notation is GL<sub>''n''</sub>(''F'') or {{nowrap|GL(''n'', ''F'')}}, or simply GL(''n'') if the field is understood.\n\nMore generally still, the [[#General linear group of a vector space|general linear group of a vector space]] GL(''V'') is the abstract [[automorphism group]], not necessarily written as matrices.\n\nThe '''[[#Special linear group|special linear group]]''', written {{nowrap|SL(''n'', ''F'')}} or SL<sub>''n''</sub>(''F''), is the [[subgroup]] of {{nowrap|GL(''n'', ''F'')}} consisting of matrices with a [[determinant]] of 1.\n\nThe group {{nowrap|GL(''n'', ''F'')}} and its [[subgroup]]s are often called '''linear groups''' or '''matrix groups''' (the abstract group GL(''V'') is a linear group but not a matrix group). These groups are important in the theory of [[group representation]]s, and also arise in the study of spatial [[symmetry|symmetries]] and symmetries of [[vector space]]s in general, as well as the study of [[polynomials]]. The [[modular group]] may be realised as a quotient of the special linear group {{nowrap|SL(2, '''Z''')}}.\n\nIf {{nowrap|''n'' ≥ 2}}, then the group {{nowrap|GL(''n'', ''F'')}} is not [[abelian group|abelian]].\n\n== General linear group of a vector space ==\n\nIf ''V'' is a [[vector space]] over the field ''F'', the general linear group of ''V'', written GL(''V'') or Aut(''V''), is the group of all [[automorphism]]s of ''V'', i.e. the set of all [[bijective]] [[linear transformation]]s {{nowrap|''V'' → ''V''}}, together with functional composition as group operation. If ''V'' has finite [[Hamel dimension|dimension]] ''n'', then GL(''V'') and {{nowrap|GL(''n'', ''F'')}} are [[group isomorphism|isomorphic]]. The isomorphism is not canonical; it depends on a choice of [[basis (linear algebra)|basis]] in ''V''. Given a basis {{nowrap|(''e''<sub>1</sub>, ..., ''e''<sub>''n''</sub>)}} of ''V'' and an automorphism ''T'' in GL(''V''), we have then for every basis vector ''e''<sub>''i''</sub> that\n: <math>Te_i = \\sum_{j=1}^n a_{ij} e_j</math>\nfor some constants ''a''<sub>''ij''</sub> in ''F''; the matrix corresponding to ''T'' is then just the matrix with entries given by the ''a''<sub>''ij''</sub>.\n\nIn a similar way, for a commutative ring ''R'' the group {{nowrap|GL(''n'', ''R'')}} may be interpreted as the group of automorphisms of a ''[[free module|free]]'' ''R''-module ''M'' of rank ''n''. One can also define GL(''M'') for any ''R''-module, but in general this is not isomorphic to {{nowrap|GL(''n'', ''R'')}} (for any ''n'').\n\n== In terms of determinants ==\n\nOver a field ''F'', a matrix is [[invertible]] if and only if its [[determinant]] is nonzero. Therefore, an alternative definition of {{nowrap|GL(''n'', ''F'')}} is as the group of matrices with nonzero determinant.\n\nOver a [[commutative ring]] ''R'', more care is needed: a matrix over ''R'' is invertible if and only if its determinant is a [[unit (ring theory)|unit]] in ''R'', that is, if its determinant is invertible in ''R''. Therefore, {{nowrap|GL(''n'', ''R'')}} may be defined as the group of matrices whose determinants are units.\n\nOver a non-commutative ring ''R'', determinants are not at all well behaved. In this case, {{nowrap|GL(''n'', ''R'')}} may be defined as the [[unit group]] of the [[matrix ring]] {{nowrap|M(''n'', ''R'')}}.\n\n== As a Lie group ==\n\n=== Real case ===\n\nThe general linear group {{nowrap|GL(''n'', '''R''')}} over the field of [[real number]]s is a real [[Lie group]] of dimension ''n''<sup>2</sup>. To see this, note that the set of all {{nowrap|''n''×''n''}} real matrices, M<sub>''n''</sub>('''R'''), forms a [[real vector space]] of dimension ''n''<sup>2</sup>. The subset {{nowrap|GL(''n'', '''R''')}} consists of those matrices whose [[determinant]] is non-zero. The determinant is a [[polynomial]] map, and hence {{nowrap|GL(''n'', '''R''')}} is an [[algebraic variety|open affine subvariety]] of M<sub>''n''</sub>('''R''') (a [[non-empty]] [[open subset]] of M<sub>''n''</sub>('''R''') in the [[Zariski topology]]), and therefore<ref>\nSince the Zariski topology is [[coarsest topology|coarser]] than the metric topology; equivalently, polynomial maps are [[continuous function (topology)|continuous]].</ref>\na [[smooth manifold]] of the same dimension.\n\nThe [[Lie algebra]] of {{nowrap|GL(''n'', '''R''')}}, denoted <math>\\mathfrak{gl}_n,</math> consists of all {{nowrap|''n''×''n''}} real matrices with the [[commutator]] serving as the Lie bracket.\n\nAs a manifold, {{nowrap|GL(''n'', '''R''')}} is not [[connected space|connected]] but rather has two [[connected space|connected components]]: the matrices with positive determinant and the ones with negative determinant. The [[identity component]], denoted by {{nowrap|GL<sup>+</sup>(''n'', '''R''')}}, consists of the real {{nowrap|''n''×''n''}} matrices with positive determinant. This is also a Lie group of dimension ''n''<sup>2</sup>; it has the same Lie algebra as {{nowrap|GL(''n'', '''R''')}}.\n\nThe group {{nowrap|GL(''n'', '''R''')}} is also [[compact space|noncompact]]. \"The\"<ref>A maximal compact subgroup is not unique, but is [[essentially unique]], hence one often refers to \"the\" maximal compact subgroup.</ref> [[maximal compact subgroup]] of {{nowrap|GL(''n'', '''R''')}} is the [[orthogonal group]] O(''n''), while \"the\" maximal compact subgroup of {{nowrap|GL<sup>+</sup>(''n'', '''R''')}} is the [[special orthogonal group]] SO(''n''). As for SO(''n''), the group {{nowrap|GL<sup>+</sup>(''n'', '''R''')}} is not [[simply connected]] (except when {{nowrap|1=''n'' = 1)}}, but rather has a [[fundamental group]] isomorphic to '''Z''' for {{nowrap|1=''n'' = 2}} or '''Z'''<sub>2</sub> for {{nowrap|''n'' > 2}}.\n\n=== Complex case ===\n\nThe general linear group over the field of [[complex number]]s, {{nowrap|GL(''n'', '''C''')}}, is a ''complex'' [[Lie group]] of complex dimension ''n''<sup>2</sup>. As a real Lie group (through realification) it has dimension 2''n''<sup>2</sup>. The set of all real matrices forms a real Lie subgroup. These correspond to the inclusions\n:GL(''n'', '''R''') < GL(''n'', '''C''') <  GL(''2n'', '''R'''),\nwhich have real dimensions ''n''<sup>2</sup>, 2''n''<sup>2</sup>, and {{nowrap|1=4''n''<sup>2</sup> = (2''n'')<sup>2</sup>}}. Complex ''n''-dimensional matrices can be characterized as real 2''n''-dimensional matrices that preserve a [[linear complex structure]] &mdash; concretely, that commute with a matrix ''J'' such that {{nowrap|1=''J''<sup>2</sup> = −''I''}}, where ''J'' corresponds to multiplying by the imaginary unit ''i''.\n\nThe [[Lie algebra]] corresponding to {{nowrap|GL(''n'', '''C''')}} consists of all {{nowrap|''n''×''n''}} complex matrices with the [[commutator]] serving as the Lie bracket.\n\nUnlike the real case, {{nowrap|GL(''n'', '''C''')}} is [[connected space|connected]]. This follows, in part, since the multiplicative group of complex numbers '''C'''<sup>∗</sup> is connected. The group manifold {{nowrap|GL(''n'', '''C''')}} is not compact; rather its [[maximal compact subgroup]] is the [[unitary group]] U(''n''). As for U(''n''), the group manifold {{nowrap|GL(''n'', '''C''')}} is not [[simply connected]] but has a [[fundamental group]] isomorphic to '''Z'''.\n\n== Over finite fields ==\n[[File:Symmetric group 3; Cayley table; GL(2,2).svg|thumb|[[Cayley table]] of {{nowrap|GL(2, 2)}}, which is isomorphic to [[Dihedral group of order 6|S<sub>3</sub>]].]]\nIf ''F'' is a [[finite field]] with ''q'' elements, then we sometimes write {{nowrap|GL(''n'', ''q'')}} instead of {{nowrap|GL(''n'', ''F'')}}. When ''p'' is prime, {{nowrap|GL(''n'', ''p'')}} is the [[outer automorphism group]]  of the group '''Z'''{{sub|''p''}}{{sup|''n''}}, and also the [[automorphism]] group, because '''Z'''{{sub|''p''}}{{sup|''n''}} is abelian, so the [[inner automorphism group]] is trivial.\n\nThe order of {{nowrap|GL(''n'', ''q'')}} is: \n: <math>\\prod_{k=0}^{n-1}(q^n-q^k)=(q^n - 1)(q^n - q)(q^n - q^2)\\ \\cdots\\ (q^n - q^{n-1}).</math>\n\nThis can be shown by counting the possible columns of the matrix: the first column can be anything but the zero vector; the second column can be anything but the multiples of the first column; and in general, the ''k''th column can be any vector not in the [[linear span]] of the first {{nowrap|''k'' − 1}} columns. In [[q-analog|''q''-analog]] notation, this is <math>[n]_q!(q-1)^n q^{n \\choose 2}</math>.\n\nFor example, {{nowrap|GL(3, 2)}} has order {{nowrap|1=(8 − 1)(8 − 2)(8 − 4) = 168}}. It is the automorphism group of the [[Fano plane]] and of the group '''Z'''{{sub|2}}{{sup|3}}, and is also known as {{nowrap|[[PSL(2,7)|PSL(2, 7)]]}}.\n\nMore generally, one can count points of [[Grassmannian]] over ''F'': in other words the number of subspaces of a given dimension ''k''. This requires only finding the order of the [[stabilizer (group theory)|stabilizer]] subgroup of one such subspace and dividing into the formula just given, by the [[orbit-stabilizer theorem]].\n\nThese formulas are connected to the [[Schubert decomposition]] of the Grassmannian, and are [[q-analog|''q''-analogs]] of the [[Betti number]]s of complex Grassmannians. This was one of the clues leading to the [[Weil conjectures]].\n\nNote that in the limit {{nowrap|''q'' ↦ 1}} the order of {{nowrap|GL(''n'', ''q'')}} goes to 0! &ndash; but under the correct procedure (dividing by {{nowrap|(''q'' &minus; 1)<sup>''n''</sup>}}) we see that it is the order of the symmetric group (See Lorscheid's article) &ndash; in the philosophy of the [[field with one element]], one thus interprets the [[symmetric group]] as the general linear group over the field with one element: {{nowrap|''S''<sub>n</sub> ≅ GL(''n'', 1)}}.\n\n=== History ===\nThe general linear group over a prime field, {{nowrap|GL(''ν'', ''p'')}}, was constructed and its order computed by [[Évariste Galois]] in 1832, in his last letter (to Chevalier) and second (of three) attached manuscripts, which he used in the context of studying the [[Galois group]] of the general equation of order ''p''<sup>''ν''</sup>.<ref name=\"chevalier-letter\">{{cite journal\n | last = Galois\n | first = Évariste\n | year = 1846\n | title = Lettre de Galois à M. Auguste Chevalier\n | journal = [[Journal de Mathématiques Pures et Appliquées]]\n | volume = XI\n | pages = 408&ndash;415\n | url = http://visualiseur.bnf.fr/ark:/12148/cb343487840/date1846\n | accessdate = 2009-02-04\n | postscript =, GL(''ν'',''p'') discussed on p. 410.}}</ref>\n\n== Special linear group ==\n{{main article|Special linear group}}\n\nThe special linear group, {{nowrap|SL(''n'', ''F'')}}, is the group of all matrices with [[determinant]] 1. They are special in that they lie on a [[Algebraic variety|subvariety]] &ndash; they satisfy a polynomial equation (as the determinant is a polynomial in the entries). Matrices of this type form a group as the determinant of the product of two matrices is the product of the determinants of each matrix. {{nowrap|SL(''n'', ''F'')}} is a [[normal subgroup]] of {{nowrap|GL(''n'', ''F'')}}.\n\nIf we write ''F''<sup>×</sup> for the [[multiplicative group]] of ''F'' (excluding 0), then the determinant is a [[group homomorphism]]\n:det: GL(''n'', ''F'') → ''F''<sup>×</sup>.\nthat is surjective and its [[kernel (algebra)|kernel]] is the special linear group. Therefore, by the [[first isomorphism theorem]], {{nowrap|GL(''n'', ''F'')/SL(''n'', ''F'')}} is [[isomorphic]] to ''F''<sup>×</sup>. In fact, {{nowrap|GL(''n'', ''F'')}} can be written as a [[semidirect product]]:\n:GL(''n'', ''F'') = SL(''n'', ''F'') ⋊ ''F''<sup>×</sup>\n\nThe special linear group is also the [[derived group]] (also known as commutator subgroup) of the GL(''n'', ''F'') (for a field or a [[division ring]] ''F'') provided that <math>n \\ne 2</math> or ''k'' is not the [[finite field|field with two elements]].<ref>{{citation|author=Suprunenko|first=D.A.|title=Matrix groups|publisher=American Mathematical Society|year=1976|series=Translations of Mathematical Monographs}}, Theorem II.9.4</ref>\n\nWhen ''F'' is '''R''' or '''C''', {{nowrap|SL(''n'', ''F'')}} is a [[Lie subgroup]] of {{nowrap|GL(''n'', ''F'')}} of dimension {{nowrap|''n''<sup>2</sup> − 1}}. The [[Lie algebra]] of {{nowrap|SL(''n'', ''F'')}} consists of all {{nowrap|''n''×''n''}} matrices over ''F'' with vanishing [[trace (matrix)|trace]]. The Lie bracket is given by the [[commutator]].\n\nThe special linear group {{nowrap|SL(''n'', '''R''')}} can be characterized as the group of ''[[volume]] and [[orientation (mathematics)|orientation]] preserving'' linear transformations of '''R'''<sup>''n''</sup>.\n\nThe group {{nowrap|SL(''n'', '''C''')}} is simply connected, while {{nowrap|SL(''n'', '''R''')}} is not. {{nowrap|SL(''n'', '''R''')}} has the same fundamental group as {{nowrap|GL<sup>+</sup>(''n'', '''R''')}}, that is, '''Z''' for {{nowrap|1=''n'' = 2}} and '''Z'''<sub>2</sub> for {{nowrap|''n'' > 2}}.\n\n== Other subgroups ==\n\n=== Diagonal subgroups ===\n\nThe set of all invertible [[diagonal matrix|diagonal matrices]] forms a subgroup of {{nowrap|GL(''n'', ''F'')}} isomorphic to (''F''<sup>×</sup>)<sup>''n''</sup>. In fields like '''R''' and '''C''', these correspond to rescaling the space; the so-called dilations and contractions.\n\nA '''scalar matrix''' is a diagonal matrix which is a constant times the [[identity matrix]]. The set of all nonzero scalar matrices forms a subgroup of {{nowrap|GL(''n'', ''F'')}} isomorphic to ''F''<sup>×</sup> . This group is the [[center of a group|center]] of {{nowrap|GL(''n'', ''F'')}}. In particular, it is a normal, abelian subgroup.\n\nThe center of {{nowrap|SL(''n'', ''F'')}} is simply the set of all scalar matrices with unit determinant, and is isomorphic to the group of ''n''th [[roots of unity]] in the field ''F''.\n\n=== Classical groups ===\n\nThe so-called [[classical group]]s are subgroups of GL(''V'') which preserve some sort of [[bilinear form]] on a vector space ''V''. These include the\n* '''[[orthogonal group]]''', O(''V''), which preserves a [[non-degenerate]] [[quadratic form]] on ''V'',\n* '''[[symplectic group]]''', Sp(''V''), which preserves a [[Symplectic vector space|symplectic form]] on ''V'' (a non-degenerate [[alternating form]]),\n* '''[[unitary group]]''', U(''V''), which, when {{nowrap|1=''F'' = '''C'''}}, preserves a non-degenerate [[hermitian form]] on ''V''.\nThese groups provide important examples of Lie groups.\n\n== Related groups and monoids ==\n\n=== Projective linear group ===\n{{main article|Projective linear group}}\nThe [[projective linear group]] {{nowrap|PGL(''n'', ''F'')}} and the [[projective special linear group]] {{nowrap|PSL(''n'', ''F'')}} are the [[quotient group|quotients]] of {{nowrap|GL(''n'', ''F'')}} and {{nowrap|SL(''n'', ''F'')}} by their [[Group center|centers]] (which consist of the multiples of the identity matrix therein); they are the induced [[Group action (mathematics)|action]] on the associated [[projective space]].\n\n=== Affine group ===\n{{main article|Affine group}}\n\nThe [[affine group]] {{nowrap|Aff(''n'', ''F'')}} is an [[group extension|extension]] of {{nowrap|GL(''n'', ''F'')}} by the group of translations in ''F''<sup>''n''</sup>. It can be written as a [[semidirect product]]:\n:Aff(''n'', ''F'') = GL(''n'', ''F'') ⋉ ''F''<sup>''n''</sup>\nwhere {{nowrap|GL(''n'', ''F'')}} acts on ''F''<sup>''n''</sup> in the natural manner. The affine group can be viewed as the group of all [[affine transformation]]s of the [[affine space]] underlying the vector space ''F''<sup>''n''</sup>.\n\nOne has analogous constructions for other subgroups of the general linear group: for instance, the [[special affine group]] is the subgroup defined by the semidirect product, {{nowrap|SL(''n'', ''F'') ⋉ ''F''<sup>''n''</sup>}}, and the [[Poincaré group]] is the affine group associated to the [[Lorentz group]], {{nowrap|O(1, 3, ''F'') ⋉ ''F''<sup>''n''</sup>}}.\n\n=== General semilinear group ===\n{{main article|General semilinear group}}\nThe [[general semilinear group]] {{nowrap|ΓL(''n'', ''F'')}} is the group of all invertible [[semilinear transformation]]s, and contains GL. A semilinear transformation is a transformation which is linear \"up to a twist\", meaning \"up to a [[field automorphism]] under scalar multiplication\". It can be written as a semidirect product:\n:ΓL(''n'', ''F'') = Gal(''F'') ⋉ GL(''n'', ''F'')\nwhere Gal(''F'') is the [[Galois group]] of ''F'' (over its [[prime field]]), which acts on {{nowrap|GL(''n'', ''F'')}} by the Galois action on the entries.\n\nThe main interest of {{nowrap|ΓL(''n'', ''F'')}} is that the associated [[projective semilinear group]] {{nowrap|PΓL(''n'', ''F'')}} (which contains {{nowrap|PGL(''n'', ''F''))}} is the [[collineation group]] of [[projective space]], for {{nowrap|''n'' > 2}}, and thus semilinear maps are of interest in [[projective geometry]].\n\n=== Full linear monoid ===\n{{expand section|basic properties|date=April 2015}}\nIf one removes the restriction of the determinant being non-zero, the resulting algebraic structure is a [[monoid]], usually called the '''full linear monoid''',<ref name=\"Okniński1998\">{{cite book|author=Jan Okniński|title=Semigroups of Matrices|year=1998|publisher=World Scientific|isbn=978-981-02-3445-4|at=Chapter 2: Full linear monoid}}</ref><ref name=\"Meakin\">{{cite book|editor=C. M. Campbell|title=Groups St Andrews 2005|year=2007|publisher=Cambridge University Press|isbn=978-0-521-69470-4|page=471|chapter=Groups and Semigroups: Connections and contrast|author=Meakin}}</ref><ref name=\"RhodesSteinberg2009\">{{cite book|author1=John Rhodes|author2=Benjamin Steinberg|title=The q-theory of Finite Semigroups|year=2009|publisher=Springer Science & Business Media|isbn=978-0-387-09781-7|page=306}}</ref> but occasionally also ''full linear semigroup'',<ref name=\"JespersOkniski2007\">{{cite book|author1=Eric Jespers|author2=Jan Okniski|title=Noetherian Semigroup Algebras|year=2007|publisher=Springer Science & Business Media|isbn=978-1-4020-5810-3|at=2.3: Full linear semigroup}}</ref> ''general linear monoid''<ref name=\"Geck2013\">{{cite book|author=Meinolf Geck|title=An Introduction to Algebraic Geometry and Algebraic Groups|year=2013|publisher=Oxford University Press|isbn=978-0-19-967616-3|page=132}}</ref><ref name=\"CanLi2014\">{{cite book|author1=Mahir Bilen Can|author2=Zhenheng Li|author3=Benjamin Steinberg|author4=Qiang Wang|title=Algebraic Monoids, Group Embeddings, and Algebraic Combinatorics|year=2014|publisher=Springer|isbn=978-1-4939-0938-4|page=142}}</ref> etc. It is actually a [[regular semigroup]].<ref name=\"Meakin\"/>\n\n== Infinite general linear group ==\nThe '''infinite general linear group''' or '''[[direct limit of groups|stable]] general linear group''' is the [[direct limit]] of the inclusions {{nowrap|GL(''n'', ''F'') → GL(''n'' + 1, ''F'')}} as the upper left [[block matrix]]. It is denoted by either GL(''F'') or {{nowrap|GL(∞, ''F'')}}, and can also be interpreted as invertible infinite matrices which differ from the identity matrix in only finitely many places.<ref name=Mil25>{{cite book | last1=Milnor | first1=John Willard | author1-link= John Milnor | title=Introduction to algebraic K-theory | publisher=[[Princeton University Press]] | location=Princeton, NJ | mr=0349811 | year=1971 | zbl=0237.18005 | series=Annals of Mathematics Studies | volume=72 | page=25 }}</ref>\n\nIt is used in [[algebraic K-theory]] to define [[Algebraic K-theory#K1|K<sub>1</sub>]], and over the reals has a well-understood topology, thanks to [[Bott periodicity]].\n\nIt should not be confused with the space of (bounded) invertible operators on a [[Hilbert space]], which is a larger group, and topologically much simpler, namely contractible &ndash; see [[Kuiper's theorem]].\n\n== See also ==\n* [[List of finite simple groups]]\n* [[SL2(R)|SL<sub>2</sub>('''R''')]]\n* [[Representation theory of SL2(R)|Representation theory of SL<sub>2</sub>('''R''')]]\n\n== Notes ==\n{{reflist|2}}\n\n== External links ==\n*{{springer|title=General linear group|id=p/g043680}}\n* [http://demonstrations.wolfram.com/GL2PAndGL33ActingOnPoints/ \"GL(2, ''p'') and GL(3, 3) Acting on Points\"] by [[Ed Pegg, Jr.]], [[Wolfram Demonstrations Project]], 2007.\n\n[[Category:Abstract algebra]]\n[[Category:Linear algebra]]\n[[Category:Lie groups]]\n[[Category:Linear algebraic groups]]"
    },
    {
      "title": "Generating set of a module",
      "url": "https://en.wikipedia.org/wiki/Generating_set_of_a_module",
      "text": "In algebra, a '''generating set''' ''G'' of a [[module (mathematics)|module]] ''M'' over a [[ring (mathematics)|ring]] ''R'' is a subset of ''M'' such that the smallest submodule of ''M'' containing ''G'' is ''M'' itself (the smallest submodule containing a subset is the intersection of all submodules containing the set). The set ''G'' is then said to generate ''M''. For example, the ring ''R'' is generated by the identity element 1 as a left ''R''-module over itself. If there is a finite generating set, then a module is said to be [[finitely generated module|finitely generated]].\n\nExplicitly, if ''G'' is a generating set of a module ''M'', then every element of ''M'' is a (finite) ''R''-linear combination of some elements of ''G''; i.e., for each ''x'' in ''M'', there are ''r''<sub>1</sub>, ..., ''r''<sub>''m''</sub> in ''R'' and ''g''<sub>1</sub>, ..., ''g''<sub>''m''</sub> in ''G'' such that\n\n: <math> x = r_1 g_1 + \\cdots + r_m g_m. </math>\n\nPut in another way, there is a surjection\n\n: <math> \\bigoplus_{g \\in G} R \\to M, \\, r_g \\mapsto r_g g.</math>\n\nwhere we wrote ''r''<sub>''g''</sub> for an element in the ''g''-th component of the direct sum. (Coincidentally, since a generating set always exists; for example, ''M'' itself, this shows that a module is a quotient of a free module, a useful fact.)\n\nA generating set of a module is said to be '''minimal''' if no proper subset of the set generates the module. If ''R'' is a [[field (mathematics)|field]], then it is the same thing as a [[basis (linear algebra)|basis]]. Unless the module is [[finitely-generated module|finitely-generated]], there may exist no minimal generating set.<ref>{{cite web|url=http://mathoverflow.net/questions/33540/existence-of-a-minimal-generating-set-of-a-module|title=ac.commutative algebra – Existence of a minimal generating set of a module – MathOverflow|work=mathoverflow.net}}</ref>\n\nThe cardinality of a minimal generating set need not be an invariant of the module; '''Z''' is generated as a principal ideal by 1, but it is also generated by, say, a minimal generating set {{nowrap|{ 2, 3 }}}. What is uniquely determined by a module is the [[infimum]] of the numbers of the generators of the module.\n\nLet ''R'' be a local ring with maximal ideal ''m'' and residue field ''k'' and ''M'' finitely generated module. Then [[Nakayama's lemma]] says that ''M'' has a minimal generating set whose cardinality is <math>\\dim_k M / mM = \\dim_k M \\otimes_R k</math>. If ''M'' is flat, then this minimal generating set is [[linearly independent]] (so ''M'' is free). See also: [[minimal resolution (algebra)|minimal resolution]].\n\nA more refined information is obtained if one considers the relations between the generators; cf. [[free presentation of a module]].\n\n== See also ==\n*[[Countably generated module]]\n*[[Flat module]]<!-- explain how to use \"flat\" to show a minimal generating set is linearly indep. -->\n*[[Invariant basis number]]\n\n== References ==\n{{reflist}}\n*Dummit, David; Foote, Richard. ''Abstract Algebra''.\n\n[[Category:Abstract algebra]]\n\n{{algebra-stub}}"
    },
    {
      "title": "Generator (mathematics)",
      "url": "https://en.wikipedia.org/wiki/Generator_%28mathematics%29",
      "text": "{{hatnote|\"Generating set\" redirects here. For other uses of \"generator\" see [[generator (disambiguation)]].}}\n[[File:One5Root.svg|thumb|The 5th [[roots of unity]] in the complex plane under multiplication form a [[group (mathematics)|group]] of order 5. Each non-identity element by itself is a generator for the whole group.]]\nIn [[mathematics]] and [[physics]], the term '''generator''' or '''generating set''' may refer to any of a number of related concepts.  The underlying concept in each case is that of a smaller [[set (mathematics)|set]] of objects, together with a set of [[Operation (mathematics)|operation]]s that can be applied to it, that result in the creation of a larger collection of objects, called the '''generated set'''.  The larger set is then said to be '''generated by''' the smaller set.  It is commonly the case that the generating set has a simpler set of properties than the generated set, thus making it easier to discuss and examine.  It is usually the case that properties of the generating set are in some way preserved by the act of generation; likewise, the properties of the generated set are often reflected in the generating set.\n\n==List of generators==\n\nA list of examples of generating sets follow.\n\n* Generating set or [[spanning set]] of a [[vector space]]: a set that spans the vector space.\n* [[Generating set of a group]]: A subset of a [[group (mathematics)|group]] that is not contained in any [[subgroup]] of the group other than the entire group.\n* [[Subring#Subring generated by a set|Generating set of a ring]]: A subset ''S'' of a ring ''A'' '''generates''' ''A'' if the only [[subring]] of ''A'' containing ''S'' is ''A''.\n* [[Ideal (ring theory)#Ideal generated by a set|Generating set of an ideal]] in a ring.\n* [[Generating set of a module]]\n* A [[generator (category theory)|generator]], in [[category theory]], is an [[object (category theory)|object]] that can be used to distinguish [[morphism]]s.\n* In [[topology]], a collection of sets that generate the topology is called a [[subbase]].\n* Generating set of a [[topological algebra]]: ''S'' is a generating set of a [[topological algebra]] ''A'' if the smallest closed [[subalgebra]] of ''A'' containing ''S'' is ''A''.\n\n==Differential equations==\nIn the study of [[differential equation]]s, and commonly those occurring in [[physics]], one has the idea of a set of infinitesimal displacements that can be extended to obtain a [[manifold]], or at least, a local part of it, by means of integration.  The general concept is of using the [[exponential map (Riemannian geometry)|exponential map]] to take the vectors in the [[tangent space]] and extend them, as [[geodesic]]s, to an open set surrounding the tangent point. In this case, it is not unusual to call the elements of the tangent space the ''generators'' of the manifold. When the manifold possesses some sort of symmetry, there is also the related notion of a ''charge'' or ''current'', which is sometimes also called the generator, although, strictly speaking, charges are not elements of the tangent space.\n\n* Elements of the [[Lie algebra]] to a [[Lie group]] are sometimes referred to as \"generators of the group,\" especially by physicists.<ref>{{cite book |title=Quantum Field Theory|first1=D. |last1= McMahon|publisher=Mc Graw Hill|year=2008|isbn=978-0-07-154382-8}}</ref> The [[Lie algebra]] can be thought of as the infinitesimal vectors generating the group, at least locally, by means of the [[exponential map (Lie theory)|exponential map]], but the Lie algebra does not form a generating set in the strict sense.<ref>{{cite book |title=McGraw Hill Encyclopaedia of Physics |first1=C.B. |last1= Parker|edition=2nd|publisher=Mc Graw Hill|year=1994|isbn=0-07-051400-3}}</ref>\n* In [[stochastic processes|stochastic analysis]], an [[Itō diffusion]] or more general [[Itō process]] has an [[Infinitesimal generator (stochastic processes)|infinitesimal generator]].\n* The '''generator''' of any [[continuous symmetry]] implied by [[Noether's theorem]], the generators of a [[Lie group]] being a special case. In this case, a generator is sometimes called a [[charge (physics)|charge]] or [[Noether charge]], examples include:\n**[[angular momentum#Total angular momentum as generator of rotations|angular momentum]] as the generator of [[rotation]]s,<ref>{{cite book |title=Quantum Mechanics|first1=E.|last1= Abers|publisher=Addison Wesley, Prentice Hall Inc|year=2004|isbn=978-0-131-461000}}</ref>\n**[[momentum#Symmetry and conservation|linear momentum]] as the generator of [[Translation (geometry)|translation]]s,<ref>{{cite book |title=Quantum Mechanics|first1=E.|last1= Abers|publisher=Addison Wesley, Prentice Hall Inc|year=2004|isbn=978-0-131-461000}}</ref>\n**[[electric charge]] being the generator of the [[U(1)]] symmetry group of [[electromagnetism]],\n**the [[color charge]]s of [[quark]]s are the generators of the [[SU(3)]] [[color symmetry]] in [[quantum chromodynamics]],\n*More precisely, \"charge\" should apply only to the [[root system]] of a Lie group.\n\n==See also==\n\n*[[Generating function]]\n*[[Lie theory]]\n*[[Symmetry (physics)]]\n*[[Particle physics]]\n*[[Supersymmetry]]\n*[[Gauge theory]]\n*[[Field (physics)]]\n\n==References==\n\n{{reflist}}\n\n==External links==\n*[http://www.math.uconn.edu/~kconrad/blurbs/grouptheory/genset.pdf Generating Sets, K. Conrad]\n\n[[Category:Abstract algebra]]\n[[Category:Universal algebra]]"
    },
    {
      "title": "Goodman–Nguyen–van Fraassen algebra",
      "url": "https://en.wikipedia.org/wiki/Goodman%E2%80%93Nguyen%E2%80%93van_Fraassen_algebra",
      "text": "A '''Goodman–Nguyen–van Fraassen algebra''' is a type of [[conditional event algebra]] (CEA) that embeds the standard [[Boolean algebra (structure)|Boolean algebra]] of unconditional events in a larger algebra which is itself Boolean. The goal (as with all CEAs) is to equate the [[conditional probability]] ''P''(''A'' ∩ ''B'') / ''P''(''A'') with the probability of a conditional event, ''P''(''A'' → ''B'') for more than just trivial choices of ''A'', ''B'', and ''P''.\n\n==Construction of the algebra==\n\nGiven set Ω, which is the set of possible outcomes, and set ''F'' of subsets of Ω&mdash;so that ''F'' is the set of possible events&mdash;consider an infinite [[Cartesian product]] of the form ''E''<sub>1</sub> × ''E''<sub>2</sub> ×  … × ''E''<sub>''n''</sub> × Ω × Ω × Ω ×  …, where ''E''<sub>1</sub>, ''E''<sub>2</sub>, … ''E''<sub>''n''</sub> are members of ''F''. Such a product specifies the set of all infinite sequences whose first element is in ''E''<sub>1</sub>, whose second element is in ''E''<sub>2</sub>, …, and whose ''n''th element is in ''E''<sub>''n''</sub>, and all of whose elements are in Ω. Note that one such product is the one where ''E''<sub>1</sub> = ''E''<sub>2</sub> = … = ''E''<sub>''n''</sub> = Ω, i.e., the set Ω × Ω ×  Ω ×  Ω × …. Designate this set as <math>\\hat{\\Omega}</math>; it is the set of all infinite sequences whose elements are in Ω.\n\nA new Boolean algebra is now formed, whose elements are subsets of <math>\\hat{\\Omega}</math>. To begin with, any event which was formerly represented by subset ''A'' of Ω is now represented by <math>\\hat{A}</math> = ''A'' × Ω × Ω × Ω × ….\n\nAdditionally, however, for events ''A'' and ''B'', let the conditional event ''A'' → ''B'' be represented as the following infinite union of disjoint sets: \n\n:[(''A'' ∩ ''B'') ×  Ω ×  Ω ×  Ω × …] ∪  \n:[''A''′ × (''A'' ∩ ''B'') × Ω × Ω ×  Ω × …] ∪ \n:[''A''′ × ''A'' ′ × (''A'' ∩ ''B'') ×  Ω × Ω ×  Ω × …] ∪ …. \n\nThe motivation for this representation of conditional events will be explained shortly. Note that the construction can be iterated; ''A'' and ''B'' can themselves be conditional events.\n\nIntuitively, unconditional event ''A'' ought to be representable as conditional event Ω → ''A''. And indeed: because Ω ∩ ''A'' = ''A'' and Ω′ = ∅, the infinite union representing Ω → ''A'' reduces to ''A'' ×  Ω ×  Ω ×  Ω × ….\n\nLet <math>\\hat{F}</math> now be a set of subsets of <math>\\hat{\\Omega}</math>, which contains representations of all events in ''F'' and is otherwise just large enough to be closed under construction of conditional events and under the familiar [[Algebra of sets|Boolean operations]]. <math>\\hat{F}</math> is a Boolean algebra of conditional events which contains a Boolean algebra corresponding to the algebra of ordinary events.\n\n==Definition of the extended probability function==\n\nCorresponding to the newly constructed logical objects, called conditional events, is a new definition of a probability function, <math>\\hat{P}</math>, based on a standard [[probability function]] ''P'': \n\n:<math>\\hat{P}</math>(''E''<sub>1</sub> × ''E''<sub>2</sub> × … ''E''<sub>''n''</sub> × Ω × Ω × Ω × …) = ''P''(''E''<sub>1</sub>)⋅''P''(''E''<sub>2</sub>)⋅ … ⋅''P''(''E''<sub>''n''</sub>)⋅''P''(Ω)⋅''P''(Ω)⋅''P''(Ω)⋅ … = ''P''(''E''<sub>1</sub>)⋅''P''(''E''<sub>2</sub>)⋅ … ⋅''P''(''E''<sub>''n''</sub>), since ''P''(Ω) = 1.\n\nIt follows from the definition of <math>\\hat{P}</math> that <math>\\hat{P}</math> (<math>\\hat{A}</math>) = ''P''(''A''). Thus <math>\\hat{P}</math> = ''P'' over the domain of ''P''.\n\n==''P''(''A'' → ''B'') = ''P''(''B''|''A'')==\n\nNow comes the insight that motivates all of the preceding work. For ''P'', the original probability function, ''P''(''A''′) = 1&nbsp;–&nbsp;''P''(''A''), and therefore ''P''(''B''|''A'') = ''P''(''A'' ∩ ''B'') / ''P''(''A'') can be rewritten as ''P''(''A'' ∩ ''B'') / [1 – ''P''(''A''′)]. The factor 1 / [1 – ''P''(''A''′)], however, can in turn be represented by its [[Taylor series|Maclaurin series expansion]], 1 + ''P''(''A''′) + ''P''(''A''′)<sup>2</sup> …. Therefore, ''P''(''B''|''A'') = ''P''(''A'' ∩ ''B'') + ''P''(''A''′)''P''(''A'' ∩ ''B'') + ''P''(''A''′)<sup>2</sup>''P''(''A'' ∩ ''B'') + ….\n\nThe right side of the equation is exactly the expression for the probability <math>\\hat{P}</math> of ''A'' → ''B'', just defined as a union of carefully chosen disjoint sets. Thus that union can be taken to represent the conditional event ''A''→ ''B'', such that <math>\\hat{P}</math>(''A'' → ''B'') = ''P''(''B''|''A'') for any choice of ''A'', ''B'', and ''P''. But since <math>\\hat{P}</math> = ''P'' over the domain of ''P'', the hat notation is optional. So long as the context is understood (i.e., conditional event algebra), one can write ''P''(''A'' → ''B'') = ''P''(''B''|''A''), with ''P'' now being the extended probability function.\n\n==References==\n\nBamber, Donald, I. R. Goodman, and H. T. Nguyen. 2004. \"Deduction from Conditional Knowledge\". ''Soft Computing'' 8: 247–255.\n\nGoodman, I. R., R. P. S. Mahler, and H. T. Nguyen. 1999. \"What is conditional event algebra and why should you care?\" ''SPIE Proceedings'', Vol 3720.\n\n{{DEFAULTSORT:Goodman-Nguyen-Van Fraassen Algebra}}\n[[Category:Abstract algebra]]\n[[Category:Conditional probability]]"
    },
    {
      "title": "Graded-commutative ring",
      "url": "https://en.wikipedia.org/wiki/Graded-commutative_ring",
      "text": "In algebra, a '''graded-commutative ring''' (also called a '''skew-commutative ring''') is a [[graded ring]] that is commutative in the graded sense; that is, homogeneous elements ''x'', ''y'' satisfy\n:<math>xy = (-1)^{|x||y|} yx ,</math>\nwhere |''x''|, |''y''| denote the degrees of ''x'', ''y''.\n\nA [[commutative ring|commutative (non-graded) ring]], with trivial grading, is a basic example. An [[exterior algebra]] is an example of a graded-commutative ring that is not commutative in the non-graded sense.\n\nA [[cup product]] on cohomology satisfies the skew-commutative relation; hence, a [[cohomology ring]] is graded-commutative. In fact, many<!-- majority? --> examples of graded-commutative rings come from [[algebraic topology]] and [[homological algebra]].\n\n== References ==\n* [[David Eisenbud]], ''Commutative Algebra. With a view toward algebraic geometry'', [[Graduate Texts in Mathematics]], vol 150, [[Springer-Verlag]], New York, 1995.  {{ISBN|0-387-94268-8}}\n*{{Cite arxiv|last=Beck|first=Kristen A.|last2=Sather-Wagstaff|first2=Sean|date=2013-07-01|title=A somewhat gentle introduction to differential graded commutative algebra|eprint=1307.0369|class=math.AC}}\n\n== See also ==\n*[[DG algebra]]\n*[[graded-symmetric algebra]]\n*[[supercommutative algebra]]\n\n[[Category:Abstract algebra]]\n\n\n{{algebra-stub}}"
    },
    {
      "title": "Harmonic polynomial",
      "url": "https://en.wikipedia.org/wiki/Harmonic_polynomial",
      "text": "In [[mathematics]], in [[abstract algebra]], a multivariate [[polynomial]] {{math|p}} over a field such that the [[Laplacian]] of {{math|p}} is zero is termed a '''harmonic polynomial'''.<ref>{{cite journal|author=Walsh, J. L.|authorlink=Joseph L. Walsh|title=On the Expansion of Harmonic Functions in Terms of Harmonic Polynomials|journal=Proceedings of the National Academy of Sciences|volume=13|issue=4|year=1927|pages=175–180|pmc=1084921|pmid=16577046|doi=10.1073/pnas.13.4.175}}</ref><ref>{{cite book|author=Helgason, Sigurdur|authorlink=Sigurdur Helgason (mathematician)|chapter=Chapter III. Invariants and Harmonic Polynomials|title=Groups and Geometric Analysis: Integral Geometry, Invariant Differential Operators, and Spherical Functions|year=2003|publisher=American Mathematical Society|series=Mathematical Surveys and Monographs, vol. 83|pages=345–384|chapter-url=https://books.google.com/books?id=WuDyBwAAQBAJ&pg=345}}</ref>\n\nThe harmonic polynomials form a [[vector space|vector subspace]] of the vector space of polynomials over the field. In fact, they form a [[graded algebra|graded subspace]].<ref>{{cite arxiv|author=Felder, Giovanni|author2=Veselov, Alexander P.|title=Action of Coxeter groups on m-harmonic polynomials and KZ equations|year=2001|eprint=math/0108012}}</ref> For the [[Real number|real field]], the harmonic polynomials are important in mathematical physics.<ref>{{cite book|author=Sobolev, Sergeĭ Lʹvovich|authorlink=Sergei Sobolev|title=Partial Differential Equations of Mathematical Physics|series=International Series of Monographs in Pure and Applied Mathematics|publisher=Elsevier|year=2016|pages=401–408|url=https://books.google.com/books?id=P-xPDAAAQBAJ&pg=PA401|isbn=9781483181363}}</ref><ref>{{cite journal|url=https://babel.hathitrust.org/cgi/pt?id=njp.32101080167032;view=1up;seq=351|author=Whittaker, Edmund T.|authorlink=E. T. Whittaker|title=On the partial differential equations of mathematical physics|journal=Mathematische Annalen|volume=57|issue=3|year=1903|pages=333–355|doi=10.1007/bf01444290}}</ref><ref>{{cite book|author=Byerly, William Elwood|authorlink=William Elwood Byerly|chapter=Chapter VI. Spherical Harmonics|title=An Elementary Treatise on Fourier's Series, and Spherical, Cylindrical, and Ellipsoidal Harmonics, with Applications to Problems in Mathematical Physics|year=1893|pages=195–218|publisher=Dover|chapter-url=https://books.google.com/books?id=BMQ0AQAAMAAJ&pg=PA195}}</ref>\n\nThe Laplacian is the sum of second partials with respect to all the variables, and is an [[invariant (mathematics)|invariant]] [[differential operator]] under the action of the [[orthogonal group]] viz the [[Group (mathematics)|group]] of rotations.\n\nThe standard [[separation of variables theorem]] states that every multivariate polynomial over a field can be decomposed as a finite sum of products of a [[radical polynomial]] and a harmonic polynomial. This is equivalent to the statement that the polynomial ring is a [[free module]] over the ring of radical polynomials.\n\n==See also==\n*[[Harmonic function]]\n*[[Spherical harmonics]]\n*[[Zonal spherical harmonics]]\n\n==References==\n<references/>\n* ''Lie Group Representations of Polynomial Rings'' by [[Bertram Kostant]] published in the ''American Journal of Mathematics'' Vol 85 No 3 (July 1963) {{doi|10.2307/2373130}}\n\n[[Category:Abstract algebra]]\n[[Category:Polynomials]]\n\n\n{{algebra-stub}}"
    },
    {
      "title": "Hasse–Schmidt derivation",
      "url": "https://en.wikipedia.org/wiki/Hasse%E2%80%93Schmidt_derivation",
      "text": "\nIn mathematics, a '''Hasse–Schmidt derivation''' is an extension of the notion of a [[derivation (differential algebra)|derivation]]. The concept was introduced by {{harvtxt|Schmidt|Hasse|1937}}.\n\n==Definition==\n\nFor a (not necessarily commutative nor associative) [[ring (mathematics)|ring]] ''B'' and a ''B''-[[algebra]] ''A'', a Hasse–Schmidt derivation is a map of ''B''-algebras\n\n:<math>D: A \\to A[[t]]</math>\n\ntaking values in the ring of [[formal power series]] with coefficients in ''A''. This definition is found in several places, such as {{harvtxt|Gatto|Salehyan|2016|loc=§3.4}}, which also contains the following example: for ''A'' being the ring of infinitely [[differentiable function]]s (defined on, say, '''R'''<sup>''n''</sup>) and ''B''='''R''', the map\n\n:<math>f \\mapsto \\exp\\left(t \\frac d {dx}\\right) f(x) = f + t \\frac {df}{dx} + \\frac {t^2}2 \\frac {d^2 f}{dx^2} + \\cdots</math>\n\nis a Hasse–Schmidt derivation, as follows from applying the [[Product rule|Leibniz rule]] iteratedly.\n==Equivalent characterizations==\n\n{{harvtxt|Hazewinkel|2012}} shows that a Hasse–Schmidt derivation is equivalent to an action of the [[bialgebra]]\n\n:<math>\\operatorname{NSymm} = \\mathbf Z \\langle Z_1, Z_2, \\ldots \\rangle</math>\n\nof [[noncommutative symmetric function]]s in countably many variables ''Z''<sub>1</sub>, ''Z''<sub>2</sub>, ...: the part <math>D_i : A \\to A</math> of ''D'' which picks the coefficient of <math>t^i</math>, is the action of the indeterminate ''Z''<sub>''i''</sub>.\n\n==Applications==\n\nHasse–Schmidt derivations on the [[exterior algebra]] <math display=\"inline\">A = \\bigwedge M</math> of some ''B''-module ''M'' have been studied by {{harvtxt|Gatto|Salehyan|2016|loc=§4}}. Basic properties of derivations in this context lead to a conceptual proof of the [[Cayley–Hamilton theorem]].\n\n==References==\n* {{Citation|author1=Gatto|first1=Letterio|author2=Salehyan|first2=Parham|title=Hasse–Schmidt derivations on Grassmann algebras|publisher=Springer|year=2016|isbn=978-3-319-31842-4|mr=3524604|doi=10.1007/978-3-319-31842-4}}\n* {{Citation|title=Hasse–Schmidt Derivations and the Hopf Algebra of Non-Commutative Symmetric Functions|author=Hazewinkel|first=Michiel|journal=Axioms|year=2012|volume=1|issue=2|pages=149–154|doi=10.3390/axioms1020149}}\n* {{Citation|author1=Schmidt|first1=F.K.|author2=Hasse|first2=H.|author2-link=Helmut Hasse|title=Noch eine Begründung der Theorie der höheren Differentialquotienten in einem algebraischen Funktionenkörper einer Unbestimmten. (Nach einer brieflichen Mitteilung von F.K. Schmidt in Jena)|journal=J. Reine Angew. Math.|volume=177|year=1937|pages=215&mdash;237|issn=0075-4102|mr=1581557|doi=10.1515/crll.1937.177.215}}\n[[Category:Abstract algebra]]"
    },
    {
      "title": "Hidden algebra",
      "url": "https://en.wikipedia.org/wiki/Hidden_algebra",
      "text": "{{context|date=July 2013}}\n\n'''Hidden algebra''' provides a [[formal semantics of programming languages|formal semantics]] for use in the field of [[software engineering]], especially for concurrent distributed [[object system]]s.<ref name=\"homepage\">{{cite web |url=http://cseweb.ucsd.edu/~goguen/projs/halg.html | title=Hidden Algebra Homepage | publisher=[[University of California, San Diego]], USA |accessdate=September 26, 2011 | authorlink=Joseph Goguen | first=Joseph | last=Goguen }}</ref> It supports [[Bayes error rate|correctness proofs]].<ref>{{cite web | url=http://www.csc.liv.ac.uk/~grant/Research/Alg/HA/index.html | title=Hidden Algebra: Behavioural specification and proof for systems with state | publisher=[[University of London]], UK | date=July 31, 2006 | accessdate=September 26, 2011 | author=Malcolm, Grant}}</ref>\n\nHidden algebra was studied by [[Joseph Goguen]].<ref name=\"homepage\" /><ref>{{cite paper | title=Hidden algebra and concurrent distributed software | publisher=[[Association for Computing Machinery|ACM]], USA | journal=[[ACM SIGSOFT Software Engineering Notes]] | volume=25 | pages=51–52 | number=1 | date=January 2000 | doi=10.1145/340855.340889 | authorlink=Joseph Goguen | first=Joseph | last=Goguen }}</ref> It handles features of large software-based systems, including [[Concurrency (computer science)|concurrency]], [[Distributed system|distribution]], [[nondeterministic algorithm|nondeterminism]], and [[local state]]s. It also handled [[object-oriented]] features like [[class (computer programming)|classes]], [[Subclass (computer science)|subclasses]] ([[Inheritance (object-oriented programming)|inheritance]]), [[Attribute (computing)|attributes]], and [[Method (computer programming)|methods]]. Hidden algebra generalizes [[process algebra]] and [[transition system]] approaches.\n\n==References==\n{{reflist}}\n\n==External links==\n* [http://cseweb.ucsd.edu/groups/tatami/handdemos/doc/haidx.htm Hidden Algebra Tutorial]\n\n{{software-eng-stub}}\n\n[[Category:Abstract algebra]]\n[[Category:Universal algebra]]\n[[Category:Logical calculi]]\n[[Category:Concurrent computing]]\n[[Category:Distributed computing]]"
    },
    {
      "title": "Higher-order operad",
      "url": "https://en.wikipedia.org/wiki/Higher-order_operad",
      "text": "In [[abstract algebra|algebra]], a '''higher-order operad''' is a higher-dimensional generalization of an [[operad]].\n\n== See also ==\n*[[opetope]]\n\n== References ==\n*{{cite journal|first1=Gijs |last1=Heuts |first2=Vladimir |last2=Hinich |first3=Ieke |last3=Moerdijk |title=On the equivalence between Lurie's model and the dendroidal model for infinity-operads|arxiv=1305.3658|journal=[[Advances in Mathematics]] |volume=302 |year=2016 |pages=869-1043 |doi=10.1016/j.aim.2016.07.021}}\n\n== External links ==\n*https://ncatlab.org/nlab/show/%28infinity%2C1%29-operad\n\n[[Category:Abstract algebra]]\n\n\n{{algebra-stub}}"
    },
    {
      "title": "Homomorphic secret sharing",
      "url": "https://en.wikipedia.org/wiki/Homomorphic_secret_sharing",
      "text": "{{Multiple issues|\n{{refimprove|date=November 2007}}\n{{confusing|date=April 2009}}\n}}\n\nIn [[cryptography]], '''homomorphic secret sharing''' is a type of [[secret sharing]] [[algorithm]] in which the secret is encrypted via [[homomorphic encryption]]. A [[homomorphism]] is a transformation from one [[algebraic structure]] into another of the same type so that the structure is preserved. Importantly, this means that for every kind of manipulation of the original data, there is a corresponding manipulation of the transformed data.<ref>{{cite journal|last=Schoenmakers|first=Berry|title=A simple publicly verifiable secret sharing scheme and its application to electronic voting|journal=Advances in Cryptology|year=1999|volume=1666|pages=148–164|citeseerx = 10.1.1.102.9375 }}</ref>\n\n== Technique ==\n\nHomomorphic secret sharing is used to transmit a secret to several recipients as follows:\n\n# Transform the \"secret\" using a homomorphism. This often puts the secret into a form which is easy to manipulate or store. In particular, there may be a natural way to 'split' the new form as required by step (2).\n# Split the transformed secret into several parts, one for each recipient. The secret must be split in such a way that it can only be recovered when all or most of the parts are combined. (See [[secret sharing]])\n# Distribute the parts of the secret to each of the recipients.\n# Combine each of the recipients' parts to recover the transformed secret, perhaps at a specified time.\n# Reverse the homomorphism to recover the original secret.\n\n== Example: decentralized voting protocol ==\n\nSuppose a community wants to perform an election, but they want to ensure that the vote-counters won't lie about the results. Using a type of homomorphic secret sharing known as [[Shamir's secret sharing]], each member of the community can add their vote to a form that is split into pieces, each piece is then submitted to a different vote-counter. The pieces are designed so that the vote-counters can't predict how any alterations to each piece will affect the whole, thus, discouraging vote-counters from tampering with their pieces. When all votes have been received, the vote-counters combine them, allowing them to recover the aggregate election results.\n\n/In detail, suppose we have an election with:\n* Two possible outcomes, either ''yes'' or ''no''. We'll represent those outcomes numerically by +1 and -1, respectively.\n* A number of authorities, ''k'', who will count the votes.\n* A number of voters, ''n'', who will submit votes.\n\nAssume the election has two outcomes, so each member of the community can vote either ''yes'' or ''no''. We'll represent those votes numerically by +1 and -1, respectively.\n\n# In advance, each authority generates a publicly available numerical key, ''x<sub>k</sub>''.\n# Each voter encodes his vote in a polynomial ''p<sub>n</sub>'' according to the following rules: The polynomial should have degree ''k-1'', its constant term should be either ''+1'' or ''-1'' (corresponding to voting \"yes\" or voting \"no\"), and its other coefficients should be randomly generated.\n#  Each voter computes the value of his polynomial ''p<sub>n</sub>'' at each authority's public key ''x<sub>k</sub>''.\n#*  This produces ''k'' points, one for each authority.\n#* These ''k'' points are the \"pieces\" of the vote: If you know all of the points, you can figure out the polynomial ''p<sub>n</sub>'' (and hence you can figure out how the voter voted). However, if you know only some of the points, you can't figure out the polynomial. (This is because you need ''k'' points to determine a degree-''k-1'' polynomial. Two points determine a line, three points determine a parabola, etc.)\n# The voter sends each authority the value that was produced using the authority's key.\n# Each authority collects the values that he receives. Since each authority only gets one value from each voter, he can't discover any given voter's polynomial. Moreover, he can't predict how altering the submissions will affect the vote.\n# Once the voters have submitted their votes, each authority ''k'' computes and announces the sum ''A<sub>k</sub>'' of all the values he's received.\n# There are ''k'' sums, ''A<sub>k</sub>''; when they are combined together, they determine a unique polynomial ''P(x)''---specifically, the sum of all the voter polynomials: P(x) = p<sub>1</sub>(x) + p<sub>2</sub>(x) + … + p<sub>n</sub>(x).\n#* The constant term of ''P(x)'' is in fact the sum of all the votes, because the constant term of P(x) is the sum of the constant terms of the individual ''p<sub>n</sub>''.\n#* Thus the constant term of ''P(x)'' provides the aggregate election result: if it's positive, more people voted for +1 than for -1; if it's negative, more people voted for -1 than for +1.\n\n[[File:Homomorphic secret sharing, voting example.svg|frame|center|alt=A table illustrating the voting protocol| An illustration of the voting protocol. Each column represents the pieces of a particular voter's vote. Each row represents the pieces received by a particular authority.]]\n\n=== Features ===\n\nThis protocol works as long as not all of the <math>k</math> authorities are corrupt — if they were, then they could collaborate to reconstruct <math>P(x)</math> for each voter and also subsequently alter the votes.\n\nThe [[Cryptographic protocol|protocol]] requires t+1 authorities to be completed, therefore in case there are N>t+1 authorities, N-t-1 authorities can be corrupted, which gives the protocol a certain degree of robustness.\n\nThe protocol manages the IDs of the voters (the IDs were submitted with the ballots) and therefore can verify that only legitimate voters have voted.\n\nUnder the assumptions on t:\n#A ballot cannot be backtracked to the ID so the privacy of the voters is preserved.\n#A voter cannot prove how they voted.\n#It is impossible to verify a vote.\n\nThe [[Cryptographic protocol|protocol]] implicitly prevents corruption of ballots.\nThis is because the authorities have no incentive to change the ballot since each authority has only a share of the ballot and has no knowledge how changing this share will affect the outcome.\n\n=== Vulnerabilities ===\n\n*The voter cannot be certain that their vote has been recorded correctly.\n*The authorities cannot be sure the votes were legal and equal, for example the voter can choose a value which is not a valid option (i.e. not in {-1, 1}) such as -20, 50 which will tilt the results in their favor.\n\n== See also ==\n* [[End-to-end auditable voting systems]]\n* [[Electronic voting]]\n* [[Certification of voting machines]]\n* [[Electoral fraud#Tampering_with_electronic_voting_machines|Techniques of potential election fraud through physical tampering with voting machines]]\n* [[Election fraud#Testing and certification of electronic voting|Preventing Election fraud: Testing and certification of electronic voting]]\n* [[Vote counting system]]\n* [[E-democracy]]\n* [[Secure multi-party computation]]\n* [[Mental poker]]\n\n==References==\n{{reflist}}\n\n{{DEFAULTSORT:Homomorphic Secret Sharing}}\n[[Category:Functions and mappings]]\n[[Category:Abstract algebra]]"
    },
    {
      "title": "Hyperstructure",
      "url": "https://en.wikipedia.org/wiki/Hyperstructure",
      "text": "{{about|a mathematical concept|the architectural concept|arcology}}\n'''Hyperstructures''' are [[algebraic structure]]s equipped with at least one [[multi-valued]] operation, called a ''hyperoperation''. The largest classes of the hyperstructures are the ones called <math>Hv</math> – structures.\n\nA hyperoperation <math>(\\star)</math> on a non-empty set <math>H</math> is a mapping from <math>H \\times H</math> to [[power set]] <math>P^*(H)</math> (the set of all non-empty subsets of <math>H</math>), i.e.\n\n<math>(\\star): H \\times H \\to P^*(H): (x,y) \\mapsto x \\star y \\subseteq H.</math>\n\nIf <math> A,B \\subseteq H</math> then we define\n\n: <math> A \\star B = \\bigcup_{a \\in A, b\\in B} (a \\star b)</math> and <math> A \\star x = A \\star \\{ x \\}, x \\star B = \\{x\\} \\star B.</math>\n\n<math> (H, \\star ) </math> is a ''semihypergroup'' if <math>(\\star)</math> is an [[associative]] hyperoperation, i.e. <math> x \\star (y \\star z) = (x \\star y)\\star z, \\ \\forall x, y, z \\in H.</math>  \n\nFurthermore, a '''hypergroup''' is a semihypergroup <math> (H, \\star ) </math>, where the [[reproduction axiom]] is valid, i.e. \n<math> a \\star H = H \\star a = H, \\  \\forall a \\in H.</math>\n\n==References==\n{{reflist}}\n*AHA (Algebraic Hyperstructures & Applications). A scientific group at Democritus  University of Thrace, School of Education, Greece. [http://aha.eled.duth.gr aha.eled.duth.gr]\n*[https://books.google.com/books?id=uvCrZ3iGur4C Applications of Hyperstructure Theory], Piergiulio Corsini, Violeta Leoreanu, Springer, 2003, {{ISBN|1-4020-1222-5}}, {{ISBN|978-1-4020-1222-8}}\n*[http://www.worldscientific.com/worldscibooks/10.1142/8481 Functional Equations on Hypergroups], László, Székelyhidi, World Scientific Publishing, 2012, {{ISBN|978-981-4407-00-7}}\n\n[[Category:Abstract algebra]]"
    },
    {
      "title": "Icosian calculus",
      "url": "https://en.wikipedia.org/wiki/Icosian_calculus",
      "text": "{{Use dmy dates|date=August 2012}}\nThe '''icosian calculus''' is a non-commutative [[algebraic structure]] discovered by the Irish mathematician [[William Rowan Hamilton]] in 1856.<ref>{{Cite journal\n|title=Memorandum respecting a new System of Roots of Unity\n|author= William Rowan Hamilton\n|author-link=William Rowan Hamilton\n|url=http://www.maths.tcd.ie/pub/HistMath/People/Hamilton/Icosian/NewSys.pdf\n|journal=[[Philosophical Magazine]]\n|volume=12\n|year=1856\n|page=446\n}}</ref><ref>{{cite book |author=Thomas L. Hankins |title=Sir William Rowan Hamilton |publisher=The Johns Hopkins University Press |location=Baltimore |year=1980 |page=474 |isbn=0-8018-6973-0 |oclc= |doi=}}</ref>\nIn modern terms, he gave a [[group presentation]] of the [[icosahedral group|icosahedral rotation group]] by [[Generating set of a group|generators]] and relations.\n\nHamilton’s discovery derived from his attempts to find an algebra of [[tuple|\"triplets\" or 3-tuples]] that he believed would reflect the three [[Cartesian coordinate system#Cartesian coordinates in three dimensions|Cartesian axes]]. The symbols of the icosian calculus can be equated to moves between vertices on a [[dodecahedron]]. Hamilton’s work in this area resulted indirectly in the terms [[Hamiltonian circuit]] and [[Hamiltonian path]] in graph theory.<ref name=\"biggs\">{{cite book |author1=Norman L. Biggs |author2=E. Keith Lloyd |author3=Robin J. Wilson |title=Graph theory 1736–1936 |publisher=Clarendon Press |location=Oxford |year=1976 |page=239 |isbn=0-19-853901-0 |oclc= |doi=}}</ref> He also invented the [[icosian game]] as a means of illustrating and popularising his discovery.\n\n== Informal definition ==\n[[File:Icosian_grid_small_with_labels2.svg|thumb|right|250px|[[Stereographic projection]] of dodecahedron used for Hamilton's [[icosian game]]]]\n\nThe algebra is based on three symbols that are each [[roots of unity]], in that repeated application of any of them yields the value 1 after a particular number of steps. They are:\n\n:<math>\n\\begin{align}\n\\iota^2 & = 1, \\\\\n\\kappa^3 & = 1, \\\\\n\\lambda^5 & = 1.\n\\end{align}\n</math>\n\nHamilton also gives one other relation between the symbols:\n\n:<math>\\lambda = \\iota\\kappa.</math>\n(In modern terms this is the (2,3,5) [[triangle group]].)\n\nThe operation is [[associative]] but not [[commutative]]. They generate a group of order 60, isomorphic to the [[group (mathematics)|group]] of rotations of a regular [[icosahedron]] or [[dodecahedron]], and therefore to the [[alternating group]] of degree five.\n\nAlthough the algebra exists as a purely abstract construction, it can be most easily visualised in terms of operations on the edges and vertices of a dodecahedron. Hamilton himself used a flattened dodecahedron as the basis for his instructional game.\n\nImagine an insect crawling along a particular edge of Hamilton's labelled dodecahedron in a certain direction, say from <math>B</math> to <math>C</math>. We can represent this [[Directed graph|directed edge]] by <math>BC</math>.\n\n[[File:Icosian_calculus_iota2.svg|thumb|right|400px|Geometrical illustration of operation iota in icosian calculus]]\n\n*The icosian symbol <math>\\iota</math> equates to changing direction on any edge, so the insect crawls from <math>C</math> to <math>B</math> (following the directed edge <math>CB</math>).\n*The icosian symbol <math>\\kappa</math> equates to rotating the insect's current travel anti-clockwise around the end point. In our example this would mean changing the initial direction <math>BC</math> to become <math>DC</math>.\n*The icosian symbol <math>\\lambda</math> equates to making a right-turn at the end point, moving from <math>BC</math> to <math>CD</math>.\n\n== Legacy ==\nThe icosian calculus is one of the earliest examples of many mathematical ideas, including:\n* presenting and studying a group by [[Presentation of a group|generators and relations]];\n* a [[triangle group]], later generalized to [[Coxeter group]]s;\n* visualization of a group by a graph, which led to [[combinatorial group theory]] and later [[geometric group theory]];\n* [[Hamiltonian circuit]]s and [[Hamiltonian path]]s in graph theory;<ref name=\"biggs\" />\n* [[dessin d'enfant]]<ref>{{Cite journal | title = Dessins d'enfants: bipartite maps and Galois groups | first = Gareth | last = Jones | journal = [[Séminaire Lotharingien de Combinatoire]] | volume = B35d | year = 1995 | pages = 4 | url = http://radon.mat.univie.ac.at/~slc/s/s35jones.html | postscript =, [http://www.emis.de/journals/SLC/wpapers/s35jones.pdf PDF] }}</ref><ref>W. R. Hamilton, Letter to John T. Graves \"On the Icosian\" (17 October 1856), ''Mathematical papers, Vol. III, Algebra,'' eds. H. Halberstam and R. E. Ingram, Cambridge University Press, Cambridge, 1967, pp. 612–625.</ref> – see [[dessin d'enfant#History|dessin d'enfant: history]] for details.\n\n== References ==\n{{reflist}}\n\n[[Category:Graph theory]]\n[[Category:Abstract algebra]]\n[[Category:Binary operations]]\n[[Category:Rotational symmetry]]\n[[Category:William Rowan Hamilton]]"
    },
    {
      "title": "Idealizer",
      "url": "https://en.wikipedia.org/wiki/Idealizer",
      "text": "In [[abstract algebra]], the '''idealizer''' of a subsemigroup ''T'' of a [[semigroup]] ''S'' is the largest subsemigroup of ''S'' in which ''T'' is an [[Semigroup#Subsemigroups and ideals|ideal]].{{sfn|Mikhalev|2002|loc=p.30}} Such an idealizer is given by\n\n:<math>\\mathbb{I}_S(T)=\\{s\\in S \\mid sT\\subseteq T \\text{ and } Ts\\subseteq T\\}</math>\n\nIn [[ring theory]], if ''A'' is an additive subgroup of a [[ring (mathematics)|ring]] ''R'', then <math>\\mathbb{I}_R(A)</math> (defined in the multiplicative semigroup of ''R'') is the largest subring of ''R'' in which ''A'' is a two-sided ideal.{{sfn|Goodearl|1976|loc=p.121}}{{sfn|Levy|Robson|2011|loc=p.7}}\n\nIn [[Lie algebra]], if ''L'' is a [[Lie ring]] (or [[Lie algebra]]) with Lie product [''x'',''y''], and ''S'' is an additive subgroup of ''L'', then the set\n\n:<math>\\{r\\in L\\mid [r,S]\\subseteq S\\}</math>\n\nis classically called the '''[[normalizer]]''' of ''S'', however it is apparent that this set is actually the Lie ring equivalent of the idealizer. It is not necessary to mention that [''S'',''r'']⊆''S'', because [[anticommutativity]] of the Lie product causes [''s'',''r'']&nbsp;=&nbsp;−[''r'',''s'']∈''S''. The Lie \"normalizer\" of ''S'' is the largest subring of ''S'' in which ''S'' is a Lie ideal.\n\n==Comments==\n\nOften, when right or left ideals are the additive subgroups of ''R'' of interest, the idealizer is defined more simply by taking advantage of the fact that multiplication by ring elements is already absorbed on one side. Explicitly,\n:<math>\\mathbb{I}_R(T)=\\{r\\in R \\mid rT\\subseteq T \\}</math>\nif ''T'' is a right ideal, or\n:<math>\\mathbb{I}_R(L)=\\{r\\in R \\mid Lr\\subseteq L \\}</math>\nif ''L'' is a left ideal.\n\nIn [[commutative algebra]], the idealizer is related to a more general construction. Given a commutative ring ''R'', and given two subsets ''A'' and ''B'' of an ''R'' module ''M'', the '''conductor''' or '''transporter''' is given by \n:<math>(A:B):=\\{r\\in R \\mid Br\\subseteq A\\}</math>.\nIn terms of this conductor notation, an additive subgroup ''B'' of ''R'' has idealizer \n:<math>\\mathbb{I}_R(B)=(B:B)</math>.\n\nWhen ''A'' and ''B'' are ideals of ''R'', the conductor is part of the structure of the [[residuated lattice]] of ideals of ''R''.\n\n;Examples\nThe [[multiplier algebra]] ''M''(''A'') of a ''C''<sup>*</sup>-algebra ''A'' is isomorphic to the idealizer of ''&pi;''(''A'') where ''&pi;'' is any faithful nondegenerate representation of ''A'' on a [[Hilbert space]]&nbsp;''H''.\n\n==Notes==\n{{Reflist}}\n\n==References==\n*{{citation   |last=Goodearl|first=K. R.   |title=Ring theory: Nonsingular rings and modules   |series=Pure and Applied Mathematics, No. 33   |publisher=Marcel Dekker Inc.   |place=New York   |year=1976   |pages=viii+206   |mr=0429962}}\n*{{citation |last1=Levy|first1=Lawrence S. |last2=Robson |first2=J. Chris|title=Hereditary Noetherian prime rings and idealizers |series=Mathematical Surveys and Monographs |volume=174 |publisher=American Mathematical Society |place=Providence, RI |year=2011 |pages=iv+228 |isbn=978-0-8218-5350-4|mr=2790801}}\n*{{citation |title=The concise handbook of algebra |editor1=Mikhalev, Alexander V.  |editor2=Pilz, Günter F. |publisher=Kluwer Academic Publishers |place=Dordrecht |year=2002 |pages=xvi+618 |isbn=0-7923-7072-4 |mr=1966155}}\n\n[[Category:Abstract algebra]]\n[[Category:Group theory]]\n[[Category:Ring theory]]\n\n\n{{Abstract-algebra-stub}}"
    },
    {
      "title": "Idempotence",
      "url": "https://en.wikipedia.org/wiki/Idempotence",
      "text": "{{For|the concept in matrix algebra|Idempotent matrix}}\n[[File:On Off - Zał Wył (3086204137).jpg|thumb|''On''/''off'' buttons of a Polish [[desk calculator]]. Pressing the ''On'' button (green) is an idempotent operation, since it has the same effect whether done once or multiple times.]]\n'''Idempotence''' ({{IPAc-en|UK|,|ɪ|d|ɛ|m|ˈ|p|əʊ|t|ən|s}},<ref>{{cite dictionary |title=idempotence |dictionary=[[Oxford English Dictionary]] |url=http://www.oed.com/view/Entry/273873 |date=2010 |edition= 3rd|publisher=Oxford University Press }}</ref> {{IPAc-en|US|ˌ|aɪ|d|ə|m|-}})<ref>{{cite dictionary |title=idempotent |url=http://www.merriam-webster.com/dictionary/idempotent |dictionary=[[Merriam-Webster]] |deadurl=no |archiveurl=https://web.archive.org/web/20161019143953/http://www.merriam-webster.com/dictionary/idempotent |archivedate=2016-10-19 }}</ref> is the property of certain [[operation (mathematics)|operations]] in [[mathematics]] and [[computer science]] whereby they can be applied multiple times without changing the result beyond the initial application. The concept of idempotence arises in a number of places in [[abstract algebra]] (in particular, in the theory of [[projector (linear algebra)|projector]]s and [[closure operator]]s) and [[functional programming]] (in which it is connected to the property of [[Referential transparency (computer science)|referential transparency]]).\n\nThe term was introduced by [[Benjamin Peirce]]<ref>Polcino & Sehgal (2002), p. 127.</ref> in the context of elements of algebras that remain invariant when raised to a positive integer power, and literally means \"(the quality of having) the same power\", from {{wikt-lang|la|idem}} + ''[[wikt:potence|potence]]'' (same + power).\n\n== Definition ==\nAn element ''x'' of a [[Magma (algebra)|magma]] (''M'', •) is said to be ''idempotent'' if:<ref>{{cite book |last=Valenza |first=Robert |date=2012 |title=Linear Algebra: An Introduction to Abstract Mathematics |url=https://books.google.com/books?id=7x8MCAAAQBAJ |location=Berlin |publisher=Springer Science & Business Media |page=22 |isbn=9781461209010 |quote=An element ''s'' of a magma such that ''ss'' = ''s'' is called ''idempotent''.}}</ref><ref>{{cite book |last=Doneddu |first=Alfred |date=1976 |title=Polynômes et algèbre linéaire |url=https://books.google.fr/books?id=5Ry7AAAAIAAJ |language=fr |location=Paris |publisher=Vuibert |page=180 |isbn=|quote=Soit ''M'' un magma, noté multiplicativement. On nomme idempotent de ''M'' tout élément ''a'' de ''M'' tel que ''a''<sup>2</sup> = ''a''.}}</ref>\n: {{nowrap|1=''x'' • ''x'' = ''x''}}.\nIf all elements are idempotent with respect to •, then • is called idempotent.\nThe formula ∀''x'', {{nowrap|1=''x'' • ''x'' = ''x''}} is called the idempotency law for •.<ref>{{cite book | author=George Grätzer | title=General Lattice Theory | location=Basel | publisher=Birkhäuser | year=2003 }} Here: Sect.1.2, p.5.</ref><ref>{{cite book | author=Garrett Birkhoff | title=Lattice Theory | location=Providence | publisher=Am. Math. Soc. | series=Colloquium Publications | volume=25 | year=1967 }}. Here: Sect.I.5, p.8.</ref>\n\n== Examples ==\n* The natural number 1 is an idempotent element with respect to [[multiplication]] (since 1[[multiplication|×]]1 = 1), and so is 0 (since 0×0 = 0), but no other natural number is (e.g. 2×2 = 2 does not hold). For the latter reason, multiplication of natural numbers is ''not'' an idempotent operation. More formally, in the [[monoid]] ([[Natural number|ℕ]], ×), idempotent elements are just 0 and 1.\n* In a magma (''M'', •), an [[identity element]] ''e'' or an [[absorbing element]] ''a'', if it exists, is idempotent. Indeed, {{nowrap|1=''e'' • ''e'' = ''e''}} and {{nowrap|1=''a'' • ''a'' = ''a''}}.\n* In a [[Group (mathematics)|group]] (''G'', •), the identity element ''e'' is the only idempotent element. Indeed, if ''x'' is an element of ''G'' such that {{nowrap|1=''x'' • ''x'' = ''x''}}, then {{nowrap|1=''x'' • ''x'' = ''x'' • ''e''}} and finally ''x'' = ''e'' by multiplying on the left by the [[inverse element]] of ''x''.\n\n* Taking the [[intersection (set theory)|intersection]] ''x''[[intersection (set theory)|∩]]''y'' of two sets ''x'' and ''y'' is an idempotent operation, since ''x''∩''x'' always equals ''x''. This means that the idempotency law ∀''x'', ''x''∩''x'' = ''x'' is true. Similarly, taking the union of two sets is an idempotent operation. Formally, in the monoids (𝒫(''E''), ∪) and (𝒫(''E''), ∩) of the [[power set]] of the set ''E'' with the [[Union (set theory)|set union]] ∪ and [[Intersection (set theory)|set intersection]] ∩ respectively, all elements are idempotent; hence ∪ and ∩ are idempotent operations on 𝒫(''E'').\n* In the monoids ({0, 1}, ∨) and ({0, 1}, ∧) of the [[Boolean domain]] with the [[logical disjunction]] ∨ and the  [[logical conjunction]] ∧ respectively, all elements are idempotent.\n* In a [[Boolean ring]], multiplication is idempotent.\n* In a [[Tropical semiring]], addition is idempotent.\n\n===Idempotent functions===\nIn the monoid (''F<sup>E</sup>'', ∘) of the [[Function (mathematics)|functions]] from a set ''E'' to a [[subset]] ''F'' of ''E'' with the [[function composition]] ∘, idempotent elements are the functions {{nowrap|''f'': ''E'' → ''F''}} such that {{nowrap|1=''f'' ∘ ''f'' = ''f''}}, in other words such that for all ''x'' in ''E'', {{nowrap|1=''f''(''f''(''x'')) = ''f''(''x'')}} (the image of each element in ''E'' is a [[Fixed point (mathematics)|fixed point]] of ''f''). For example:\n* Taking the [[absolute value]] ''abs''(''x'')<ref>A more common notation for this is <math>|x|</math>, however, it is harder to read when expressions are nested.</ref> of an [[integer number]] ''x'' is an idempotent function for the following reason: ''abs''(''abs''(''x'')) = ''abs''(''x'') is true for each integer number ''x''.<ref>In fact, this equation holds for all [[rational number|rational]], [[real number|real]] and even [[complex number]]s, too.</ref> This means that ''abs'' [[function composition|∘]] ''abs'' = ''abs''<ref>This is an equation between functions. Two functions are equal if their domains and ranges agree, and their output values agree on their whole domain.</ref> holds, that is, ''abs'' is an idempotent element in the set of all functions (from integers to integers)<ref>This set of functions is formally denoted as [[integer number|ℤ]]<sup>ℤ</sup>.</ref> with respect to function composition. Therefore, ''abs'' satisfies the above definition of an idempotent function.\nOther examples include:\n* the [[Identity function|identity]] function is idempotent;\n* [[Constant function|constant]] functions are idempotent;\n* the [[Floor and ceiling functions|floor]], [[Floor and ceiling functions|ceiling]] and [[fractional part]] functions are idempotent;\n* the [[Generating set of a group|subgroup generated]] function from the power set of a group to itself is idempotent;\n* the [[convex hull]] function from the power set of an [[affine space]] over the [[Real number|reals]] to itself is idempotent;\n* the [[Closure (topology)|closure]] and [[Interior (topology)|interior]] functions of the power set of a [[topological space]] to itself are idempotent;\n* the [[Kleene star]] and [[Kleene plus]] functions of the power set of a monoid to itself are idempotent;\n* the idempotent [[endomorphism]]s of a [[vector space]] are its [[Projection (linear algebra)|projections]].\n\nIf the set ''E'' has ''n'' elements, we can partition it into ''k'' chosen fixed points and {{nowrap|''n'' − ''k''}} non-fixed points under ''f'', and then ''k''<sup>''n''−''k''</sup> is the number of different idempotent functions. Hence, taking into account all possible partitions,\n: <math>\\sum_{k=0}^n {n \\choose k} k^{n-k}</math>\nis the total number of possible idempotent functions on the set. The [[integer sequence]] of the number of idempotent functions as given by the sum above for ''n'' = 0, 1, 2, 3, 4, 5, 6, 7, 8, … starts  with 1, 1, 3, 10, 41, 196, 1057, 6322, 41393, … {{OEIS|A000248}}.\n\nNeither the property of being idempotent nor that of being not is preserved under function composition.<ref>If ''f'' and ''g'' commute, i.e. if {{nowrap|1=''f'' ∘ ''g'' = ''g'' ∘ ''f''}}, then idempotency of both ''f'' and ''g'' implies that of {{nowrap|''f'' ∘ ''g''}}, since {{nowrap|1=(''f'' ∘ ''g'') ∘ (''f'' ∘ ''g'') = (''f'' ∘ ''f'') ∘ (''g'' ∘ ''g'') = ''f'' ∘ ''g''}}, using the associativity of composition.</ref> As an example for the former, {{nowrap|1=''f''(''x'') = ''x''}} [[modulo arithmetic|mod]] 3 and ''g''(''x'') = max(''x'', 5) are both idempotent, but {{nowrap|''f'' ∘ ''g''}} is not,<ref>e.g. ''f''(''g''(7)) = ''f''(7) = 1, but ''f''(''g''(1)) = ''f''(5) = 2 ≠ 1</ref> although {{nowrap|''g'' ∘ ''f''}} happens to be.<ref>also showing that commutation of ''f'' and ''g'' is not a [[necessary condition]] for idempotency preservation</ref> As an example for the latter, the negation function ¬ on the Boolean domain is not idempotent, but {{nowrap|¬ ∘ ¬}} is. Similarly, unary negation {{nowrap|−( )}} of real numbers is not idempotent, but  {{nowrap|−( ) ∘ −( )}} is.\n\n== Computer science meaning ==\n{{See also|Referential transparency (computer science)|Reentrant (subroutine)|Stable sort}}\nIn [[computer science]], the term ''idempotence'' may have a different meaning depending on the context in which it is applied:\n* in [[imperative programming]], a [[subroutine]] with [[Side effect (computer science)|side effects]] is idempotent if the system state remains the same after one or several calls, in other words if the function from the system state space to itself associated to the subroutine is idempotent in the mathematical sense given in the [[#Definition|definition]];\n* in [[functional programming]], a [[pure function]] is idempotent if it is idempotent in the mathematical sense given in the [[#Definition|definition]].\n\nThis is a very useful property in many situations, as it means that an operation can be repeated or retried as often as necessary without causing unintended effects. With non-idempotent operations, the algorithm may have to keep track of whether the operation was already performed or not. \n\n=== Computer science examples ===\nA function looking up a customer's name and address in a [[database]] is typically idempotent, since this will not cause the database to change.  Similarly, changing a customer's address to XYZ is typically idempotent, because the final address will be the same no matter how many times XYZ is submitted. However, placing an order for a cart for the customer is typically not idempotent, since running the call several times will lead to several orders being placed. Canceling an order is idempotent, because the order remains canceled no matter how many requests are made.\n\nA composition of idempotent methods or subroutines, however, is not necessarily idempotent if a later method in the sequence changes a value that an earlier method depends on – ''idempotence is not closed under composition''.  For example, suppose the initial value of a variable is 3 and there is a sequence that reads the variable, then changes it to 5, and then reads it again.  Each step in the sequence is idempotent: both steps reading the variable have no side effects and changing a variable to 5 will always have the same effect no matter how many times it is executed.  Nonetheless, executing the entire sequence once produces the output (3, 5), but executing it a second time produces the output (5, 5), so the sequence is not idempotent.<!-- {{Citation needed|date=December 2017}} please discuss this on talk page before reinstating -->\n\nIn the [[Hypertext Transfer Protocol]] (HTTP), idempotence and [[Hypertext Transfer Protocol#Safe methods|safety]] are the major attributes that separate [[Hypertext Transfer Protocol#Request methods|HTTP verbs]].  Of the major HTTP verbs, GET, PUT, and DELETE should be implemented in an idempotent manner according to the standard, but POST need not be.<ref name=\"httpStd-methods\">IETF, [http://tools.ietf.org/html/rfc7231#section-4.2.2 Hypertext Transfer Protocol (HTTP/1.1): Semantics and Content] {{webarchive|url=https://web.archive.org/web/20140608213403/http://tools.ietf.org/html/rfc7231 |date=2014-06-08 }}.  See also [[Hypertext Transfer Protocol|HyperText Transfer Protocol]].</ref>  GET retrieves a resource; PUT stores content at a resource; and DELETE eliminates a resource.  As in the example above, reading data usually has no side effects, so it is idempotent (in fact [[wiktionary:nullipotent|''nullipotent'']]).  Storing and deleting a given set of content are each usually idempotent as long as the request specifies a location or identifier that uniquely identifies that resource and only that resource again in the future.  The PUT and DELETE operations with unique identifiers reduce to the simple case of assignment to an immutable variable of either a value or the null-value, respectively, and are idempotent for the same reason; the end result is always the same as the result of the initial execution, even if the response differs.<ref>{{cite IETF|title=Hypertext Transfer Protocol (HTTP/1.1): Semantics and Content |rfc=7231 |section=4.2.2 |sectionname=Idempotent Methods |quote=It knows that repeating the request will have the same intended effect, even if the original request succeeded, though the response might differ.}}</ref>\n\nViolation of the unique identification requirement in storage or deletion typically causes violation of idempotence.  For example, storing or deleting a given set of content without specifying a unique identifier: POST requests, which do not need to be idempotent, often do not contain unique identifiers, so the creation of the identifier is delegated to the receiving system which then creates a corresponding new record.  Similarly, PUT and DELETE requests with nonspecific criteria may result in different outcomes depending on the state of the system - for example, a request to delete the most recent record. In each case, subsequent executions will further modify the state of the system, so they are not idempotent. \n\nIn [[Event stream processing]], idempotence refers to the ability of a system to produce the same outcome, even if the same file, event or message is received more than once.\n\nIn a [[load-store architecture]], instructions that might possibly cause a [[page fault]] are idempotent. So if a page fault occurs, the OS can load the page from disk and then simply re-execute the faulted instruction.  In a processor where such instructions are not idempotent, dealing with page faults is much more complex.{{fact|date=August 2016}}\n\nWhen reformatting output, [[Prettyprint|pretty-printing]] is expected to be idempotent. In other words, if the output is already \"pretty\", there should be nothing to do for the pretty-printer.{{Citation needed|date=March 2017}}\n\nIn [[Service-oriented_architecture|Service oriented architecture]] (SOA), a multiple-step orchestration process composed entirely of idempotent steps can be replayed without side-effects if any part of that process fails.\n\n== Applied examples ==\n[[File:Colección de hombres cruzando.JPG|thumb|A typical crosswalk button is an example of an idempotent system]]\nApplied examples that many people could encounter in their day-to-day lives include [[elevator]] call buttons and [[crosswalk button]]s.<ref>https://web.archive.org/web/20110523081716/http://www.nclabor.com/elevator/geartrac.pdf  For example, this design specification includes detailed algorithm for when elevator cars will respond to subsequent calls for service</ref> The initial activation of the button moves the system into a requesting state, until the request is satisfied. Subsequent activations of the button between the initial activation and the request being satisfied have no effect, unless the system is designed to adjust the time for satisfying the request based on the number of activations.\n\n== See also ==\n* [[Closure operator]]\n* [[Fixed point (mathematics)]]\n* [[Idempotent of a code]]\n* [[Nilpotent]]\n* [[Idempotent matrix]]\n* [[Idempotent relation]] &mdash; a generalization of idempotence to binary relations\n* [[List of matrices]]\n* [[Pure function]]\n* [[Referential transparency]]\n* [[Iterated function]]\n* [[Biordered set]]\n* [[Involution (mathematics)]]\n\n== References ==\n{{Reflist|30em}}\n\n== Further reading ==\n{{wiktionary}}\n{{wikibooks}}\n{{wikiversity | Portal:Computer Science}}\n* “[http://foldoc.org/idempotent idempotent]” at [[FOLDOC]]\n*{{citation\n |author=Goodearl, K. R.\n |title=von Neumann regular rings\n |edition=2\n |publisher=Robert E. Krieger Publishing Co. Inc.\n |place=Malabar, FL\n |year=1991\n |pages=xviii+412\n |isbn=978-0-89464-632-4\n |mr=1150975}}\n* {{citation | last=Gunawardena | first=Jeremy | chapter=An introduction to idempotency | zbl=0898.16032 | editor1-last=Gunawardena | editor1-first=Jeremy | title=Idempotency. Based on a workshop, Bristol, UK, October 3–7, 1994 | location=Cambridge | publisher=[[Cambridge University Press]] | pages=1–49 | year=1998 | chapter-url=http://www.hpl.hp.com/techreports/96/HPL-BRIMS-96-24.pdf }}\n* {{springer|title=Idempotent|id=p/i050080}}\n*{{citation\n |author1=[[Hazewinkel, Michiel]]\n |author2=Gubareni, Nadiya\n |author3=Kirichenko, V. V.\n |title=Algebras, rings and modules. vol. 1\n |series=Mathematics and its Applications\n |volume=575\n |publisher=Kluwer Academic Publishers\n |place=Dordrecht\n |year=2004\n |pages=xii+380\n |isbn=978-1-4020-2690-4\n |mr=2106764}}\n*{{citation\n |author=Lam, T. Y.\n |title=A first course in noncommutative rings\n |series=Graduate Texts in Mathematics\n |volume=131\n |edition=2\n |publisher=Springer-Verlag\n |place=New York\n |year=2001\n |pages=xx+385\n |isbn=978-0-387-95183-6\n |mr=1838439\n |doi=10.1007/978-1-4419-8616-0}}\n* {{Lang Algebra|edition=3}} p.&nbsp;443\n* Peirce, Benjamin. [http://www.math.harvard.edu/history/peirce_algebra/index.html ''Linear Associative Algebra''] 1870.\n* {{citation\n |author1=Polcino Milies, César\n |author2=Sehgal, Sudarshan K.\n |title=An introduction to group rings\n |series=Algebras and Applications\n |volume=1\n |publisher=Kluwer Academic Publishers\n |place=Dordrecht\n |year=2002\n |pages=xii+371\n |isbn=978-1-4020-0238-0\n |mr=1896125\n |doi=10.1007/978-94-010-0405-3}}\n\n[[Category:Abstract algebra]]\n[[Category:Closure operators]]\n[[Category:Mathematical relations]]\n[[Category:Theoretical computer science]]\n[[Category:Binary operations]]"
    },
    {
      "title": "Indeterminate (variable)",
      "url": "https://en.wikipedia.org/wiki/Indeterminate_%28variable%29",
      "text": "{{multiple issues|\n{{more footnotes|date=April 2017}}\n{{one source|date=April 2017}}\n{{more citations needed|date=April 2017}}\n}}\nIn [[mathematics]], and particularly in formal [[algebra]], an '''indeterminate''' is a symbol that is treated as a variable, but does not stand for anything else but itself and is used as a placeholder in objects such as [[polynomial]]s and [[formal power series]]. In particular, it does not designate a constant or a [[parameter]] of the problem, it is not an unknown that could be solved for, and it is not a [[variable (mathematics)|variable]] designating a function argument or being summed or integrated over; it is not any type of [[bound variable]].\n\n==Polynomials==\nA polynomial in an indeterminate ''X'' is an expression of the form <math>a_0 + a_1X + a_2X^2 + \\ldots + a_nX^n</math>, where the ''a''<sub>''i''</sub> are called the [[coefficient]]s of the polynomial. Two such polynomials are equal only if the corresponding coefficients are equal.<ref>{{harvnb|Herstein|1975|loc=Section 3.9}}.</ref> In contrast, two polynomial functions in a variable ''x'' may be equal or not depending on the value of ''x''.\n\nFor example, the functions\n:<math>f(x) = 2 + 3x, \\quad g(x) = 5 + 2x</math>\nare equal when ''x'' = 3 and not equal otherwise. But the two polynomials\n:<math>2 + 3X, \\quad 5 + 2X</math>\nare unequal, since 2 does not equal 5, and 3 does not equal 2. In fact,\n:<math>2 + 3X = a + bX</math>\ndoes not hold ''unless'' ''a''&nbsp;=&nbsp;2 and ''b''&nbsp;=&nbsp;3. This is because ''X'' is not, and does not designate, a number.\n\nThe distinction is subtle, since a polynomial in ''X'' can be changed to a function in ''x'' by substitution. But the distinction is important because information may be lost when this substitution is made. Working in [[Modular arithmetic|modulo 2]]:\n:<math>0 - 0^2 = 0, \\quad 1 - 1^2 = 0,</math>\nso the polynomial function ''x'' − ''x''<sup>2</sup> is identically equal to 0 for ''x'' having any value in the modulo-2 system. But the polynomial ''X'' − ''X''<sup>2</sup> is not the zero polynomial, since the coefficients, 0, 1 and −1, are not all zero.\n\n==Formal power series==\nA formal power series in an indeterminate ''X'' is an expression of the form ''a''<sub>0</sub> + ''a''<sub>1</sub>''X'' + ''a''<sub>2</sub>''X''<sup>2</sup> + …. This is similar to the definition of a polynomial, except that an infinite number of the coefficients may be nonzero. Unlike the [[power series]] encountered in calculus, questions of convergence are irrelevant. So power series that would diverge for values of ''x'', such as 1 + ''x'' + 2''x''<sup>2</sup> + 6''x''<sup>3</sup> + … + ''n''!''x''<sup>''n''</sup> + …, are allowed.\n\n==As generators==\nIndeterminates are useful in [[abstract algebra]] for generating [[mathematical structure]]s. For example, given a [[field (mathematics)|field]] ''K'', the set of polynomials with coefficients in ''K'' is the [[polynomial ring]] with [[polynomial arithmetic|polynomial addition and multiplication]] as operations. If two indeterminates ''X'' and ''Y'' are used, the polynomial ring ''K''[''X,Y''] also uses these operations, and convention holds that ''XY'' = ''YX''.\n\nIndeterminates may also be used to generate a [[free algebra]] over a [[commutative ring]] ''A''. For instance, with two indeterminates ''X'' and ''Y'', the free algebra ''A''⟨''X,Y''⟩ includes sums of strings in ''X'' and ''Y'', with coefficients in ''A'', and with the stipulation that ''XY'' and ''YX'' are distinct.\n\n==See also==\n*[[Indeterminate system]]\n*[[Polynomial]]\n*[[Formal power series]]\n\n==Notes==\n{{reflist}}\n\n==References==\n*{{Cite book\n | first = I. N.\n | last1 = Herstein\n | title = Topics in Algebra\n | publisher = Wiley\n | year = 1975\n}}\n\n{{PlanetMath attribution|title=indeterminate|id=6444}}\n\n[[Category:Abstract algebra]]\n[[Category:Polynomials]]\n[[Category:Mathematical series]]"
    },
    {
      "title": "Infinite expression",
      "url": "https://en.wikipedia.org/wiki/Infinite_expression",
      "text": "In [[mathematics]], an '''infinite expression''' is an [[Expression (mathematics)|expression]] in which some operators take an infinite number of arguments, or in which the nesting of the operators continues to an infinite depth.<ref>{{cite journal |last1=Helmer |first1=Olaf |authorlink1=Olaf Helmer |date=January 1938 |title=The syntax of a language with infinite expressions |journal=Bulletin of the American Mathematical Society |volume=44 |issue=1 |pages=33–34 |type=Abstract |issn=0002-9904 |oclc=5797393 |doi=10.1090/S0002-9904-1938-06672-4}}.</ref>  A generic concept for infinite expression can lead to ill-defined or self-inconsistent constructions (much like a [[universal set|set of all sets]]), but there are several instances of infinite expressions that are well defined.\n\nExamples of well-defined infinite expressions include<ref>\n{{cite book |last1=Euler |first1=Leonhard |authorlink1=Leonhard Euler |others=J.D. Blanton (translator) |title=Introduction to Analysis of the Infinite, Book I |date=November 1, 1988 |publisher=Springer Verlag |isbn=978-0-387-96824-7 |page=[https://books.google.com/books?id=H58dmcLEnk4C&pg=PA303&dq=%22continued+fraction%22+%22infinite+expression%22&hl=en&ei=U6H-TI-5DJC8sQPBoOWvCw&sa=X&oi=book_result&ct=result&resnum=1&ved=0CCkQ6AEwAA#v=onepage&q=%22continued%20fraction%22%20%22infinite%20expression%22&f=false 303] |type=Hardcover}}\n</ref><ref>\n{{cite book |last1=Wall |first1=Hubert Stanley |authorlink1=Hubert Stanley Wall |title=Analytic Theory of Continued Fractions |date=March 28, 2000 |publisher=American Mathematical Society |isbn=978-0-8218-2106-0 |page=[https://books.google.com/books?id=eE8PpgM3ucMC&pg=PA14&dq=%22continued+fraction%22+%22infinite+expression%22&hl=en&ei=U6H-TI-5DJC8sQPBoOWvCw&sa=X&oi=book_result&ct=result&resnum=5&ved=0CEAQ6AEwBA#v=onepage&q=%22continued%20fraction%22%20%22infinite%20expression%22&f=false 14] |type=Hardcover}}\n</ref> [[infinite sum]]s, whether expressed using [[summation]] notation or as an [[Series (mathematics)|infinite series]], such as\n\n:<math>\\sum_{n=0}^\\infty a_n = a_0 + a_1 + a_2 + \\cdots \\,;</math>\n\n[[infinite product]]s, whether expressed using product notation or expanded, such as\n\n:<math>\\prod_{n=0}^\\infty b_n = b_0 \\times b_1 \\times b_2 \\times \\cdots </math>\n\n[[Nested radical#Infinitely nested radicals|infinite nested radical]]s, such as\n\n: <math>\\sqrt{1+2\\sqrt{1+3 \\sqrt{1+\\cdots}}} </math>\n\n[[Tetration#Extension_to_infinite_heights|infinite power towers]], such as \n\n: <math>\\sqrt{2}^{\\sqrt{2}^{\\sqrt{2}^{\\cdot^{\\cdot^{\\cdot}}}}} </math>\n\nand [[continued fraction|infinite continued fraction]]s, whether expressed using [[Carl Friedrich Gauss|Gauss]]'s [[generalized continued fraction|Kettenbruch]] notation or expanded, such as\n\n:<math>c_0 + \\operatorname*{K}_{n=1}^\\infty \\frac{1}{c_n} = c_0 + \\cfrac{1}{c_1 + \\cfrac{1}{c_2 + \\cfrac{1}{c_3 + \\cfrac{1}{c_4 + \\ddots}}}}</math>\n\nIn [[infinitary logic]], one can use infinite conjunctions and infinite disjunctions.\n\nEven for well-defined infinite expressions, the ''value'' of the infinite expression may be ambiguous or not well defined; for instance, there are multiple summation rules available for assigning values to series, and the same series may have different values according to different summation rules if the series is not [[absolute convergence|absolutely convergent]].\n\n==From the hyperreal viewpoint==\nFrom the point of view of the hyperreals, such an infinite expression <math>E_\\infty</math> is obtained in every case from the sequence <math>\\langle E_n : n \\in \\mathbb{N}\\rangle</math> of finite expressions, by evaluating the sequence at a [[hypernatural]] value <math>n=H</math> of the index ''n'', and applying the [[standard part]], so that <math>E_\\infty=\\operatorname{st}(E_H)</math>.\n\n==See also==\n*[[Iterated binary operation]]\n*[[Infinite word]]\n*[[Sequence]]\n*[[Decimal expansion]]\n*[[Power series]]\n*[[Infinite compositions of analytic functions]]\n*[[Omega language]]\n\n==References==\n{{Reflist}}\n\n[[Category:Abstract algebra]]\n[[Category:Mathematical analysis]]"
    },
    {
      "title": "Information algebra",
      "url": "https://en.wikipedia.org/wiki/Information_algebra",
      "text": "The term \"'''information algebra'''\" refers to mathematical techniques of [[information processing]]. Classical [[information theory]] goes back to [[Claude Shannon]]. It is a theory of information transmission, looking at communication and storage. However, it has not been considered so far that information comes from different sources and that it is therefore usually combined. It has furthermore been neglected in classical information theory that one wants to extract those parts out of a piece of information that are relevant to specific questions.\n\nA mathematical phrasing of these operations leads to an '''algebra of information''', describing basic modes of information processing. Such an algebra involves several formalisms of [[computer science]], which seem to be different on the surface: relational databases, multiple systems of formal logic or numerical problems of linear algebra. It allows the development of generic procedures of information processing and thus a unification of basic methods of computer science, in particular of [[distributed information processing]].\n\nInformation relates to precise questions, comes from different sources, must be aggregated, and can be focused on questions of interest. Starting from these considerations, information algebras {{Harv|Kohlas|2003}} are [[Structure (mathematical logic)#Many-sorted structures|two-sorted]] algebras <math>(\\Phi,D)\\,</math>, where <math>\\Phi\\,</math> is a [[semigroup]], representing combination or aggregation of information, <math>D\\,</math> is a [[lattice (order)|lattice]] of [[Domain (mathematics)|domain]]s (related to questions) whose [[partial order]] reflects the granularity of the domain or the question, and a [[mixed operation]] representing focusing or extraction of information.\n\n== Information and its operations ==\nMore precisely, in the two-sorted algebra <math>(\\Phi,D)\\,</math>, the following operations are defined\n\n{| border=1 style=\"border:0px\"\n| style=\"border:1px solid #448800; padding:0.4em; background-color:#EEFFEE\" |\n; Combination : <math>\\otimes: \\Phi \\otimes \\Phi \\rightarrow \\Phi,~ (\\phi,\\psi) \\mapsto \\phi \\otimes \\psi\\,</math>\n; Focusing :   <math>\\Rightarrow: \\Phi \\otimes D \\rightarrow \\Phi,~ (\\phi,x) \\mapsto \\phi^{\\Rightarrow x}\\,</math>\n| style=\"border:1px\" | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n|}\n\nAdditionally, in <math>D\\,</math> the usual lattice operations (meet and join) are defined.\n\n== Axioms and definition ==\nThe axioms of the two-sorted algebra <math>(\\Phi,D)\\,</math>, in addition to the axioms of the lattice <math>D\\,</math>:\n\n{| border=1 style=\"border:0px\"\n| style=\"border:1px solid #448800; padding:0.4em; background-color:#EEFFEE\" |\n; Semigroup : <math>\\Phi\\,</math> is a commutative semigroup under combination with a neutral element (representing vacuous information).\n; Distributivity of Focusing over Combination : <math>(\\phi^{\\Rightarrow x} \\otimes \\psi)^{\\Rightarrow x} = \\phi^{\\Rightarrow x} \\otimes \\psi^{\\Rightarrow x}\\,</math>\nTo focus an information on <math>x\\,</math> combined with another information to domain <math>x\\,</math>, one may as well first focus the second information to <math>x\\,</math> and combine then.\n; Transitivity of Focusing : <math>(\\phi^{\\Rightarrow x})^{\\Rightarrow y} = \\phi^{\\Rightarrow x \\wedge y}\\,</math>\nTo focus an information on <math>x\\,</math> and <math>y\\,</math>, one may focus it to <math>x \\wedge y\\,</math>.\n; Idempotency : <math>\\phi \\otimes \\phi^{\\Rightarrow x} = \\phi\\,</math>\nAn information combined with a part of itself gives nothing new.\n; Support : <math>\\forall \\phi \\in \\Phi,~ \\exists x \\in D\\,</math> such that <math>\\phi = \\phi^{\\Rightarrow x}\\,</math>\nEach information refers to at least one domain (question).\n| style=\"border:1px\" | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n|}\n\nA two-sorted algebra <math>(\\Phi,D)\\,</math> satisfying these axioms is called an '''Information Algebra'''.\n\n== Order of information ==\nA partial order of information can be introduced by defining <math>\\phi \\leq \\psi\\,</math> if <math>\\phi \\otimes \\psi = \\psi\\,</math>. This means that <math>\\phi\\,</math> is less informative than <math>\\psi\\,</math> if it adds no new information to <math>\\psi\\,</math>. The semigroup <math>\\Phi\\,</math> is a semilattice relative to this order, i.e. <math>\\phi \\otimes \\psi = \\phi \\vee \\psi\\,</math>. Relative to any domain (question) <math>x \\in D\\,</math> a partial order can be introduced by defining <math>\\phi \\leq_{x} \\psi\\,</math>  if <math>\\phi^{\\Rightarrow x} \\leq \\psi^{\\Rightarrow x}\\,</math>. It represents the order of information content of <math>\\phi\\,</math> and <math>\\psi\\,</math> relative to the domain (question) <math>x\\,</math>.\n\n== Labeled information algebra ==\nThe pairs <math>(\\phi,x) \\ \\,</math>, where <math>\\phi \\in \\Phi\\,</math> and <math>x \\in D\\,</math> such that <math>\\phi^{\\Rightarrow x} = \\phi\\,</math> form a '''labeled Information Algebra'''. More precisely, in the two-sorted algebra <math>(\\Phi,D) \\ \\,</math>, the following operations are defined\n{| border=1 style=\"border:0px\"\n| style=\"border:1px solid #448800; padding:0.4em; background-color:#EEFFEE\" |\n; Labeling : <math>d(\\phi,x) = x \\ \\,</math>\n; Combination : <math>(\\phi,x) \\otimes (\\psi,y) = (\\phi \\otimes \\psi,x \\vee y)~~~~\\,</math>\n; Projection : <math>(\\phi,x)^{\\downarrow y} = (\\phi^{\\Rightarrow y},y)\\text{ for }y \\leq x\\,</math>\n| style=\"border:1px\" | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n|}\n\n== Models of information algebras ==\nHere follows an incomplete list of instances of information algebras:\n*[[Relational algebra]]: The reduct of a relational algebra with natural join as combination and the usual projection is a labeled information algebra, see [[#Worked-out example: relational algebra|Example]].\n*[[Constraint system]]s: Constraints form an information algebra {{Harv|Jaffar|Maher|1994}}.\n*[[Semiring valued algebra]]s: C-Semirings induce information algebras {{Harv|Bistarelli|Montanari|Rossi1997}};{{Harv|Bistarelli|Fargier|Montanari|Rossi|Schiex|Verfaillie|1999}};{{Harv|Kohlas|Wilson|2006}}. \n*[[Logic]]: Many logic systems induce information algebras {{Harv|Wilson|Mengin|1999}}. Reducts of [[cylindric algebra]]s {{Harv|Henkin|Monk|Tarski|1971}} or [[polyadic algebra]]s are information algebras related to [[predicate logic]] {{Harv|Halmos|2000}}.\n*[[Module (mathematics)|Module algebra]]s: {{Harv|Bergstra|Heering|Klint|1990}};{{Harv|de Lavalette|1992}}.\n*[[Linear system]]s: Systems of linear equations or linear inequalities induce information algebras {{Harv|Kohlas|2003}}.\n\n=== Worked-out example: relational algebra ===\n{{cleanup section|reason=\\texttt|date=August 2014}}\nLet <math>{\\mathcal A}\\,</math> be a set of symbols, called ''attributes'' (or ''column\nnames''). For each <math>\\alpha\\in{\\mathcal A}\\,</math> let <math>U_\\alpha\\,</math> be a non-empty set, the\nset of all possible values of the attribute <math>\\alpha\\,</math>. For example, if \n<math>{\\mathcal A}= \\{\\texttt{name},\\texttt{age},\\texttt{income}\\}\\,</math>, then <math>U_{\\texttt{name}}\\,</math> could\nbe the set of strings, whereas <math>U_{\\texttt{age}}\\,</math> and <math>U_{\\texttt{income}}\\,</math> are both\nthe set of non-negative integers.\n\nLet <math>x\\subseteq{\\mathcal A}\\,</math>. An ''<math>x\\,</math>-tuple'' is a function <math>f\\,</math> so that\n<math>\\hbox{dom}(f)=x\\,</math> and <math>f(\\alpha)\\in U_\\alpha\\,</math> for each <math>\\alpha\\in x\\,</math> The set\nof all <math>x\\,</math>-tuples is denoted by <math>E_x\\,</math>. For an <math>x\\,</math>-tuple <math>f\\,</math> and a subset\n<math>y\\subseteq x\\,</math> the restriction <math>f[y]\\,</math> is defined to be the\n<math>y\\,</math>-tuple <math>g\\,</math> so that <math>g(\\alpha)=f(\\alpha)\\,</math> for all <math>\\alpha\\in y\\,</math>.\n\nA ''relation <math>R\\,</math> over <math>x\\,</math>'' is a set of <math>x\\,</math>-tuples, i.e. a subset of <math>E_x\\,</math>.\nThe set of attributes <math>x\\,</math> is called the ''domain'' of <math>R\\,</math> and denoted by\n<math>d(R)\\,</math>. For <math>y\\subseteq d(R)\\,</math> the ''projection'' of <math>R\\,</math> onto <math>y\\,</math> is defined\nas follows:\n:<math>\\pi_y(R):=\\{f[y]\\mid f\\in R\\}.\\,</math>\nThe ''join'' of a relation <math>R\\,</math> over <math>x\\,</math> and a relation <math>S\\,</math> over <math>y\\,</math> is\ndefined as follows:\n:<math>R\\bowtie S:=\\{f\\mid f \\quad (x\\cup y)\\hbox{-tuple},\\quad f[x]\\in R,\n  \\;f[y]\\in S\\}.\\,</math>\nAs an example, let <math>R\\,</math> and <math>S\\,</math> be the following relations:\n:<math>R=\n   \\begin{matrix}\n    \\texttt{name} & \\texttt{age} \\\\\n    \\texttt{A} & \\texttt{34} \\\\\n    \\texttt{B} & \\texttt{47} \\\\\n    \\end{matrix}\\qquad\n   S=\n   \\begin{matrix}\n    \\texttt{name} & \\texttt{income} \\\\\n    \\texttt{A} & \\texttt{20'000} \\\\\n    \\texttt{B} & \\texttt{32'000} \\\\\n   \\end{matrix}\\,</math>\nThen the join of <math>R\\,</math> and <math>S\\,</math> is:\n:<math>R\\bowtie S=\n   \\begin{matrix}\n    \\texttt{name} & \\texttt{age} & \\texttt{income} \\\\\n    \\texttt{A} & \\texttt{34} & \\texttt{20'000} \\\\\n    \\texttt{B} & \\texttt{47} & \\texttt{32'000} \\\\\n   \\end{matrix}\\,</math>\nA relational database with natural join <math>\\bowtie\\,</math> as combination and the usual projection <math>\\pi\\,</math> is an information algebra.\nThe operations are well defined since\n* <math>d(R\\bowtie S)=d(R)\\cup d(S)\\,</math>\n* If <math>x\\subseteq d(R)\\,</math>, then <math>d(\\pi_x(R))=x\\,</math>.\nIt is easy to see that relational databases satisfy the axioms of a labeled\ninformation algebra:\n; semigroup : <math>(R_1\\bowtie R_2)\\bowtie R_3=R_1\\bowtie(R_2\\bowtie R_3)\\,</math> and <math>R\\bowtie S=S\\bowtie R\\,</math>\n; transitivity : If <math>x\\subseteq y\\subseteq d(R)\\,</math>, then <math>\\pi_x(\\pi_y(R))=\\pi_x(R)\\,</math>.\n; combination : If <math>d(R)=x\\,</math> and <math>d(S)=y\\,</math>, then <math>\\pi_x(R\\bowtie S)=R\\bowtie\\pi_{x\\cap y}(S)\\,</math>.\n; idempotency : If <math>x\\subseteq d(R)\\,</math>, then <math>R\\bowtie\\pi_x(R)=R\\,</math>.\n; support : If <math> x = d(R)\\,</math>, then <math>\\pi_x(R)=R\\,</math>.\n\n== Connections ==\n{{expand section|date=March 2014}}\n; Valuation algebras : Dropping the idempotency axiom leads to [[valuation algebra]]s. These axioms have been introduced by {{Harv|Shenoy|Shafer|1990}} to generalize ''local computation schemes'' {{Harv|Lauritzen|Spiegelhalter|1988}} from Bayesian networks to more general formalisms, including belief function, possibility potentials, etc. {{Harv|Kohlas |Shenoy|2000}}. For a book-length exposition on the topic see {{Harvtxt|Pouly|Kohlas|2011}}.\n; Domains and information systems: ''Compact Information Algebras'' {{Harv|Kohlas|2003}} are related to [[Scott domain]]s and [[Scott information system]]s  {{Harv|Scott|1970}};{{Harv|Scott|1982}};{{Harv|Larsen|Winskel|1984}}.\n; Uncertain information : Random variables with values in information algebras represent ''[[probabilistic argumentation]] systems'' {{Harv|Haenni|Kohlas|Lehmann|2000}}.\n; Semantic information : Information algebras introduce semantics by relating information to questions through focusing and combination {{Harv|Groenendijk|Stokhof|1984}};{{Harv|Floridi|2004}}.\n; Information flow : Information algebras are related to information flow, in particular classifications  {{Harv|Barwise|Seligman|1997}}.\n; Tree decomposition : ...\n; Semigroup theory : ...\n; Compositional models: Such models may be defined within the framework of information algebras: https://arxiv.org/abs/1612.02587\n; Extended axiomatic foundations of information and valuation algebras: The concept of conditional independence is basic for information algebras and a new axiomatic foundation of information algebras, based on conditional independence, extending the old one (see above) is available: https://arxiv.org/abs/1701.02658\n\n== Historical Roots ==\nThe axioms for information algebras are derived from \nthe axiom system proposed in (Shenoy and Shafer, 1990), see also (Shafer, 1991).\n\n== References ==\n* {{Citation | first1=J. | last1= Barwise | author1link=Jon Barwise | first2=J. | last2=Seligman | title=Information Flow: The Logic of Distributed Systems | year=1997 | publisher=Number 44 in Cambridge Tracts in Theoretical Computer Science, Cambridge University Press | place=Cambridge U.K. }}\n* {{Citation | first1=J.A. | last1=Bergstra | first2=J.| last2=Heering | first3=P. | last3=Klint | title=Module algebra | journal=J. Of the Assoc. For Computing Machinery|volume=73|issue=2|pages=335–372 | year=1990| doi=10.1145/77600.77621 }}\n* {{Citation | first1=S. | last1=Bistarelli | first2=H. | last2=Fargier | first3=U. | last3=Montanari | first4=F. | last4=Rossi | first5=T. |last5=Schiex | first6=G.|last6=Verfaillie| title=Semiring-based CSPs and valued CSPs: Frameworks, properties, and comparison | journal=Constraints |volume=4 |issue=3 | pages=199&ndash;240 | year=1999 | url=ftp://ftp.irit.fr/pub/IRIT/RPDMP/PapersFargier/valuatedItaliens.ps.gz| doi=10.1023/A:1026441215081 }}\n* {{Citation | first1=Stefano | last1=Bistarelli | first2=Ugo |last2=Montanari |first3=Francesca | last3=Rossi |title=Semiring-based constraint satisfaction and optimization | journal=Journal of the ACM |volume=44 |issue=2 | pages=201–236 | year=1997 |  url=ftp://ftp.di.unipi.it/pub/Papers/rossi/jacm.ps.gz | doi=10.1145/256303.256306| citeseerx=10.1.1.45.5110 }}\n* {{Citation | first=Gerard R. Renardel | last=de Lavalette | chapter= Logical semantics of modularisation | editor=Egon Börger |editor2=Gerhard Jäger |editor3=Hans Kleine Büning |editor4=Michael M. Richter | title=CSL: 5th Workshop on Computer Science Logic  | pages=306–315 | publisher=Volume 626 of Lecture Notes in Computer Science, Springer | year=1992 | isbn=978-3-540-55789-0 |\nchapter-url=http://citeseer.ist.psu.edu/484529.html}}\n* {{Citation | first=Luciano | last= Floridi | title=Outline of a theory of strongly semantic information | journal=Minds and Machines |volume=14 |issue=2 | pages=197–221 | year=2004 | doi=10.1023/b:mind.0000021684.50925.c9}} \n* {{Citation | first1=J. | last1=Groenendijk | first2=M. | last2=Stokhof | title=Studies on the Semantics of Questions and the Pragmatics of Answers | publisher=PhD thesis, Universiteit van Amsterdam | year=1984}}\n* {{Citation|first1=R. |last1=Haenni |first2=J. |last2=Kohlas |first3=N. |last3=Lehmann |chapter=Probabilistic argumentation systems |editor=J. Kohlas |editor2=S. Moral |title=Handbook of Defeasible Reasoning and Uncertainty Management Systems |pages=221–287 |publisher=Volume 5: Algorithms for Uncertainty and Defeasible Reasoning, Kluwer |place=Dordrecht |year=2000 |chapter-url=http://diuf.unifr.ch/tcs/publications/ps/hkl2000.pdf |deadurl=yes |archiveurl=https://web.archive.org/web/20050125040324/http://diuf.unifr.ch/tcs/publications/ps/hkl2000.pdf |archivedate=January 25, 2005 }}\n* {{Citation | first=Paul R. | last=Halmos | authorlink=Paul R. Halmos  | title=An autobiography of polyadic algebras | journal=Logic Journal of the IGPL |volume=8 |issue=4 | pages=383–392 | year=2000 | doi=10.1093/jigpal/8.4.383 }} \n* {{Citation | first1=L. | last1=Henkin | author1link=Leon Henkin | first2=J. D. | last2=Monk | first3=A. | last3=Tarski | author3link=Alfred Tarski | title=Cylindric Algebras | publisher=North-Holland | place=Amsterdam | year= 1971 | isbn =978-0-7204-2043-2}}\n* {{Citation | first1=J. | last1=Jaffar | first2= M. J. | last2=Maher | title= Constraint logic programming: A survey | journal= J. Of Logic Programming |volume= 19/20 | pages=503–581 | year= 1994 | doi=10.1016/0743-1066(94)90033-7}} \n* {{Citation | first=J. | last=Kohlas | title= Information Algebras: Generic Structures for Inference | publisher=Springer-Verlag | year= 2003 | isbn = 978-1-85233-689-9}}\n* {{Citation | first1=J. | last1=Kohlas | first2= P.P. | last2=Shenoy | chapter=Computation in valuation algebras | editor=J. Kohlas |editor2=S. Moral | title=Handbook of Defeasible Reasoning and Uncertainty Management Systems, Volume 5: Algorithms for Uncertainty and Defeasible Reasoning |pages=5–39|publisher=Kluwer | place=Dordrecht | year= 2000}} \n* {{Citation|first1=J. |last1=Kohlas |first2=N. |last2=Wilson |title=Exact and approximate local computation in semiring-induced valuation algebras |publisher=Technical Report 06-06, Department of Informatics, University of Fribourg |year=2006 |url=http://diuf.unifr.ch/tcs/publications/ps/kohlaswilson06.pdf |deadurl=yes |archiveurl=https://web.archive.org/web/20060924230816/http://diuf.unifr.ch/tcs/publications/ps/kohlaswilson06.pdf |archivedate=September 24, 2006 }} \n* {{Citation | first1=K. G. | last1=Larsen | first2=G. |last2=Winskel | chapter=Using information systems to solve recursive domain equations effectively | editor=Gilles Kahn |editor2=David B. MacQueen |editor3=Gordon D. Plotkin | title=Semantics of Data Types, International Symposium, Sophia-Antipolis, France, June 27&ndash;29, 1984, Proceedings |volume=173 of Lecture Notes in Computer Science | pages=109–129 | location=Berlin | year= 1984 | publisher=Springer}}\n* {{Citation | first1=S. L. | last1= Lauritzen |first2=D. J.|last2=Spiegelhalter | title=Local computations with probabilities on graphical structures and their application to expert systems | journal= Journal of the Royal Statistical Society, Series B |volume= 50 | pages=157–224 | year= 1988}} \n* {{citation|first1=Marc |last1=Pouly | first2=Jürg |last2=Kohlas|title=Generic Inference: A Unifying Theory for Automated Reasoning|year=2011|publisher=John Wiley & Sons|isbn=978-1-118-01086-0}}\n* {{Citation | first=Dana S. | last= Scott | authorlink=Dana Scott | title= Outline of a mathematical theory of computation | publisher=Technical Monograph PRG–2, Oxford University Computing Laboratory, Programming Research Group | year=1970}} \n* {{Citation | first=D.S. | last=Scott | chapter=Domains for denotational semantics | editor= M. Nielsen |editor2=E.M. Schmitt | title= Automata, Languages and Programming | pages= 577–613 | publisher= Springer | year= 1982}} \n* {{Citation | first=G. | last= Shafer | title=An axiomatic study of computation in hypertrees | publisher=Working Paper 232, School of Business, University of Kansas | year= 1991}} \n* {{Citation | first1=P. P. | last1=Shenoy | first2=G. | last2=Shafer | chapter=Axioms for probability and belief-function proagation | editor=Ross D. Shachter |editor2=Tod S. Levitt |editor3=Laveen N. Kanal |editor4=John F. Lemmer | title= Uncertainty in Artificial Intelligence 4 |volume= 9 | journal= Machine Intelligence and Pattern Recognition |pages = 169–198 | place=Amsterdam | year= 1990 | publisher=Elsevier | isbn= 978-0-444-88650-7| doi=10.1016/B978-0-444-88650-7.50019-6 }}\n* {{Citation | first1=Nic | last1=Wilson |first2= Jérôme | last2= Mengin | chapter=Logical deduction using the local computation framework | editor=Anthony Hunter |editor2=Simon Parsons | title=Symbolic and Quantitative Approaches to Reasoning and Uncertainty, European Conference, ECSQARU'99, London, UK, July 5&ndash;9, 1999, Proceedings, volume 1638 of Lecture Notes in Computer Science | pages= 386–396 | publisher= Springer | year= 1999 | isbn = 978-3-540-66131-3 | chapter-url = http://springerlink.metapress.com/openurl.asp?genre=article&amp;issn=0302-9743&amp;volume=1638&amp;spage=0386}}\n\n[[Category:Information theory]]\n[[Category:Abstract algebra]]"
    },
    {
      "title": "Interior algebra",
      "url": "https://en.wikipedia.org/wiki/Interior_algebra",
      "text": "In [[abstract algebra]], an '''interior algebra''' is a certain type of [[algebraic structure]] that encodes the idea of the topological [[Interior (topology)|interior]] of a set.  Interior algebras are to [[topology]] and the [[modal logic]] '''S4''' what [[Boolean algebra (structure)|Boolean algebra]]s are to [[set theory]] and ordinary [[propositional logic]]. Interior algebras form a [[variety (universal algebra)|variety]] of [[modal algebra]]s.\n\n==Definition==\nAn '''interior algebra''' is an [[algebraic structure]] with the [[signature (logic)|signature]]\n\n:⟨''S'', ·, +, ′, 0, 1, <sup>I</sup>⟩\n\nwhere\n\n:⟨''S'', ·, +, ′, 0, 1⟩\n\nis a [[Boolean algebra (structure)|Boolean algebra]] and postfix <sup>I</sup> designates a [[unary operator]], the '''interior operator''', satisfying the identities:\n\n# ''x''<sup>I</sup> ≤ ''x''\n# ''x''<sup>II</sup> = ''x''<sup>I</sup>\n# (''xy'')<sup>I</sup> = ''x''<sup>I</sup>''y''<sup>I</sup>\n# 1<sup>I</sup> = 1\n\n''x''<sup>I</sup> is called the '''interior''' of ''x''.\n\nThe [[duality (order theory)|dual]] of the interior operator is the '''[[closure operator]]''' <sup>C</sup> defined by ''x''<sup>C</sup> = ((''x''′)<sup>I</sup>)′. ''x''<sup>C</sup> is called the '''closure''' of ''x''. By the [[duality (order theory)|principle of duality]], the closure operator satisfies the identities:\n\n# ''x''<sup>C</sup> ≥ ''x''\n# ''x''<sup>CC</sup> = ''x''<sup>C</sup>\n# (''x'' + ''y'')<sup>C</sup> = ''x''<sup>C</sup> + ''y''<sup>C</sup>\n# 0<sup>C</sup> = 0\n\nIf the closure operator is taken as primitive, the interior operator can be defined as ''x''<sup>I</sup> = ((''x''′)<sup>C</sup>)′. Thus the theory of interior algebras may be formulated using the closure operator instead of the interior operator, in which case one considers '''closure algebras''' of the form ⟨''S'', ·, +, ′, 0, 1, <sup>C</sup>⟩, where ⟨''S'', ·, +, ′, 0, 1⟩ is again a Boolean algebra and <sup>C</sup> satisfies the above identities for the closure operator. Closure and interior algebras form [[duality (order theory)|dual]] pairs, and are paradigmatic instances of \"Boolean algebras with operators.\" The early literature on this subject (mainly Polish topology) invoked closure operators, but the interior operator formulation eventually became the norm following the work of [[Wim Blok]].\n\n== Open and closed elements ==\nElements of an interior algebra satisfying the condition ''x''<sup>I</sup> = ''x'' are called '''[[open set|open]]'''. The [[complement (order theory)|complements]] of open elements are called '''[[closed set|closed]]''' and are characterized by the condition ''x''<sup>C</sup> = ''x''. An interior of an element is always open and the closure of an element is always closed. Interiors of closed elements are called '''[[regular open set|regular open]]''' and closures of open elements are called '''regular closed'''. Elements which are both open and closed are called '''[[clopen set|clopen]]'''. 0 and 1 are clopen.\n\nAn interior algebra is called '''Boolean''' if all its elements are open (and hence clopen). Boolean interior algebras can be identified with ordinary Boolean algebras as their interior and closure operators provide no meaningful additional structure. A special case is the class of '''trivial''' interior algebras which are the single element interior algebras characterized by the identity 0 = 1.\n\n== Morphisms of interior algebras ==\n\n=== Homomorphisms ===\nInterior algebras, by virtue of being [[algebraic structure]]s, have [[homomorphism]]s. Given two interior algebras ''A'' and ''B'', a map ''f'' : ''A'' → ''B'' is an '''interior algebra homomorphism''' [[if and only if]] ''f'' is a homomorphism between the underlying Boolean algebras of ''A'' and ''B'', that also preserves interiors and closures. Hence:\n*''f''(''x''<sup>I</sup>) = ''f''(''x'')<sup>I</sup>;\n*''f''(''x''<sup>C</sup>) = ''f''(''x'')<sup>C</sup>.\n\n=== Topomorphisms ===\nTopomorphisms are another important, and more general, class of [[morphism]]s between interior algebras. A map ''f'' : ''A'' → ''B'' is a [[topomorphism]] if and only if ''f'' is a homomorphism between the Boolean algebras underlying ''A'' and ''B'', that also preserves the open and closed elements of ''A''. Hence:\n* If ''x'' is open in ''A'', then ''f''(''x'') is open in ''B'';\n* If ''x'' is closed in ''A'', then ''f''(''x'') is closed in ''B''.\n(Such morphisms have also been called ''stable homomorphisms'' and ''closure algebra semi-homomorphisms''.) Every interior algebra homomorphism is a topomorphism, but not every topomorphism is an interior algebra homomorphism.\n\n=== Boolean homomorphisms ===\nEarly research often considered mappings between interior algebras which were homomorphisms of the underlying Boolean algebras but which did not necessarily preserve the interior or closure operator. Such mappings were called '''Boolean homomorphisms'''. (The terms ''closure homomorphism'' or ''topological homomorphism'' were used in the case where these were preserved, but this terminology is now redundant as the standard definition of a homomorphism in [[universal algebra]] requires that it preserves all operations.) Applications involving countably complete interior algebras (in which countable meets and joins always exist, also called ''σ-complete'') typically made use of countably complete Boolean homomorphisms also called '''Boolean σ-homomorphisms''' - these preserve countable meets and joins.\n\n=== Continuous morphisms ===\nThe earliest generalization of continuity to interior algebras was [[Roman Sikorski|Sikorksi]]'s based on the inverse image map of a continuous map. This is a Boolean homomorphism, preserves unions of sequences and includes the closure of an inverse image in the inverse image of the closure. Sikorski thus defined a ''continuous homomorphism'' as a Boolean σ-homomorphism ''f'' between two σ-complete interior algebras such that ''f''(''x'')<sup>C</sup> ≤ ''f''(''x''<sup>C</sup>). This definition had several difficulties: The construction acts [[Functor#Covariance and contravariance|contravariantly]] producing a dual of a continuous map rather than a generalization. On the one hand σ-completeness is too weak to characterize inverse image maps (completeness is required), on the other hand it is too restrictive for a generalization. (Sikorski remarked on using non-σ-complete homomorphisms but included σ-completeness in his axioms for ''closure algebras''.) Later J. Schmid defined a '''continuous homomorphism''' or '''continuous morphism''' for interior algebras as a Boolean homomorphism ''f'' between two interior algebras satisfying ''f''(''x''<sup>C</sup>) ≤ ''f''(''x'')<sup>C</sup>. This generalizes the forward image map of a continuous map - the image of a closure is contained in the closure of the image. This construction is [[Functor#Covariance and contravariance|covariant]] but not suitable for category theoretic applications as it only allows construction of continuous morphisms from continuous maps in the case of bijections. (C. Naturman returned to Sikorski's approach while dropping σ-completeness to produce topomorphisms as defined above. In this terminology, Sikorski's original \"continuous homomorphisms\" are σ-complete topomorphisms between σ-complete interior algebras.)\n\n== Relationships to other areas of mathematics ==\n\n=== Topology ===\nGiven a [[topological space]] '''''X''''' = ⟨''X'', ''T''⟩ one can form the [[power set]] Boolean algebra of ''X'':\n\n:⟨''P''(''X''), ∩, ∪, ′, ø, ''X''⟩\n\nand extend it to an interior algebra\n\n:'''''A'''''('''''X''''') = ⟨''P''(''X''), ∩, ∪, ′, ø, ''X'', <sup>I</sup>⟩,\n\nwhere <sup>I</sup> is the usual topological interior operator. For all ''S'' ⊆ ''X'' it is defined by\n\n:''S''<sup>I</sup> = ∪ {''O'' : ''O'' ⊆ ''S'' and ''O'' is open in '''''X'''''}\n\nFor all ''S'' ⊆ ''X'' the corresponding closure operator is given by\n\n:''S''<sup>C</sup> = ∩ {''C'' : ''S'' ⊆ ''C'' and ''C'' is closed in '''''X'''''}\n\n''S''<sup>I</sup> is the largest open subset of ''S'' and ''S''<sup>C</sup> is the smallest closed superset of ''S'' in '''''X'''''. The open, closed, regular open, regular closed and clopen elements of the interior algebra '''''A'''''('''''X''''') are just the open, closed, regular open, regular closed and clopen subsets of '''''X''''' respectively in the usual topological sense.\n\nEvery [[Completeness (order theory)|complete]] [[Atomic (order theory)|atomic]] interior algebra is [[isomorphism|isomorphic]] to an interior algebra of the form '''''A'''''('''''X''''') for some [[topological space]] '''''X'''''.  Moreover, every interior algebra can be [[embedding|embedded]] in such an interior algebra giving a representation of an interior algebra as a '''[[field of sets|topological field of sets]]'''. The properties of the structure '''''A'''''('''''X''''') are the very motivation for the definition of interior algebras. Because of this intimate connection with topology, interior algebras have also been called '''topo-Boolean algebras''' or '''topological Boolean algebras'''.\n\nGiven a [[continuous map]] between two topological spaces\n\n:''f''&nbsp;:&nbsp;'''''X'''''&nbsp;→&nbsp;'''''Y'''''\n\nwe can define a [[completeness (order theory)|complete]] topomorphism\n\n:'''''A'''''(''f'')&nbsp;:&nbsp;'''''A'''''('''''Y''''')&nbsp;→&nbsp;'''''A'''''('''''X''''')\n\nby\n\n:'''''A'''''(''f'')(''S'') = ''f''<sup>−1</sup>[''S'']\n\nfor all subsets ''S'' of '''''Y'''''. Every complete topomorphism between two complete atomic interior algebras can be derived in this way. If '''Top''' is the [[category of topological spaces]] and continuous maps and '''Cit''' is the [[category theory|category]] of complete atomic interior algebras and complete topomorphisms then '''Top''' and '''Cit''' are dually isomorphic and '''''A'''''&nbsp;:&nbsp;'''Top'''&nbsp;→&nbsp;'''Cit''' is a [[functor|contravariant functor]] that is a dual isomorphism of categories. '''''A'''''(''f'') is a homomorphism if and only if ''f'' is a continuous [[open map]].\n\nUnder this dual isomorphism of categories many natural topological properties correspond to algebraic properties, in particular connectedness properties correspond to irreducibility properties:\n\n*'''''X''''' is [[empty set|empty]] if and only if '''''A'''''('''''X''''') is trivial\n*'''''X''''' is [[indiscrete space|indiscrete]] if and only if '''''A'''''('''''X''''') is [[simple algebra|simple]]\n*'''''X''''' is [[discrete space|discrete]] if and only if '''''A'''''('''''X''''') is Boolean\n*'''''X''''' is [[almost discrete space|almost discrete]] if and only if '''''A'''''('''''X''''') is [[semisimple algebraic group|semisimple]]\n*'''''X''''' is [[Alexandrov topology|finitely generated]] (Alexandrov) if and only if '''''A'''''('''''X''''') is '''operator complete''' i.e. its interior and closure operators distribute over arbitrary meets and joins respectively\n*'''''X''''' is [[connected space|connected]] if and only if '''''A'''''('''''X''''') is [[directly indecomposable]]\n*'''''X''''' is [[ultraconnected space|ultraconnected]] if and only if '''''A'''''('''''X''''') is [[finitely subdirectly irreducible]]\n*'''''X''''' is [[compact space|compact]] ultra-connected if and only if '''''A'''''('''''X''''') is [[subdirectly irreducible]]\n\n==== Generalized topology ====\n\nThe modern formulation of topological spaces in terms of [[topological space|topologies]] of open subsets, motivates an alternative formulation of interior algebras: A '''generalized topological space''' is an [[algebraic structure]] of the form\n\n:⟨''B'', ·, +, ′, 0, 1, ''T''⟩\n\nwhere ⟨''B'', ·, +, ′, 0, 1⟩ is a Boolean algebra as usual, and ''T'' is a unary relation on ''B'' (subset of ''B'') such that:\n\n#0,1&nbsp;∈&nbsp;''T''\n#''T'' is closed under arbitrary joins (i.e. if a join of an arbitrary subset of ''T'' exists then it will be in ''T'')\n#''T'' is closed under finite meets\n#For every element ''b'' of ''B'', the join ∑{''a''&nbsp;∈''T'' : ''a''&nbsp;≤&nbsp;''b''} exists\n\n''T'' is said to be a '''generalized topology''' in the Boolean algebra.\n\nGiven an interior algebra its open elements form a generalized topology. Conversely given a generalized topological space\n\n:⟨''B'', ·, +, ′, 0, 1, ''T''⟩\n\nwe can define an interior operator on ''B'' by ''b''<sup>I</sup> = ∑{''a''&nbsp;∈''T'' : ''a''&nbsp;≤&nbsp;''b''} thereby producing an interior algebra whose open elements are precisely ''T''. Thus generalized topological spaces are equivalent to interior algebras.\n\nConsidering interior algebras to be generalized topological spaces, topomorphisms are then the standard homomorphisms of Boolean algebras with added relations, so that standard results from [[universal algebra]] apply.\n\n==== Neighbourhood functions and neighbourhood lattices ====\nThe topological concept of [[Neighbourhood (mathematics)|neighbourhood]]s can be generalized to interior algebras: An element ''y'' of an interior algebra is said to be a '''neighbourhood''' of an element ''x'' if ''x''&nbsp;≤&nbsp;''y''<sup>I</sup>. The set of neighbourhoods of ''x'' is denoted by ''N''(''x'') and forms a [[Filter (mathematics)|filter]]. This leads to another formulation of interior algebras:\n\nA '''neighbourhood function''' on a Boolean algebra is a mapping ''N'' from its underlying set ''B'' to its set of filters, such that:\n\n#For all ''x''&nbsp;∈&nbsp;''B'', max{''y''&nbsp;∈&nbsp;''B'' : ''x''&nbsp;∈&nbsp;''N''(''y'')} exists\n#For all ''x'',''y''&nbsp;∈&nbsp;''B'', ''x''&nbsp;∈&nbsp;''N(y)'' if and only if there is a ''z''&nbsp;∈&nbsp;''B'' such that ''y''&nbsp;≤&nbsp;''z''&nbsp;≤&nbsp;''x'' and ''z''&nbsp;∈&nbsp;''N(z)''.\n\nThe mapping ''N'' of elements of an interior algebra to their filters of neighbourhoods is a neighbourhood function on the underlying Boolean algebra of the interior algebra. Moreover, given a neighbourhood function ''N'' on a Boolean algebra with underlying set ''B'', we can define an interior operator by ''x''<sup>I</sup> = max {y&nbsp;∈&nbsp;''B'' : ''x''&nbsp;∈&nbsp;''N(y)''} thereby obtaining an interior algebra. ''N(x)'' will then be precisely the filter of neighbourhoods of ''x'' in this interior algebra. Thus interior algebras are equivalent to Boolean algebras with specified neighbourhood functions.\n\nIn terms of neighbourhood functions, the open elements are precisely those elements ''x'' such that ''x''&nbsp;∈&nbsp;''N(x)''. In terms of open elements ''x''&nbsp;∈&nbsp;''N(y)'' if and only if there is an open element ''z'' such that ''y''&nbsp;≤&nbsp;''z''&nbsp;≤&nbsp;''x''.\n\nNeighbourhood functions may be defined more generally on [[semilattice|(meet)-semilattice]]s producing the structures known as [[neighbourhood lattice|neighbourhood (semi)lattice]]s. Interior algebras may thus be viewed as precisely the '''Boolean neighbourhood lattices''' i.e. those neighbourhood lattices whose underlying semilattice forms a Boolean algebra.\n\n=== Modal logic ===\nGiven a theory (set of formal sentences) ''M'' in the modal logic '''S4''', we can form its [[Lindenbaum–Tarski algebra]]:\n\n:'''''L'''''(''M'') = ⟨''M'' / ~, ∧, ∨, ¬, ''F'', ''T'', □⟩\n\nwhere ~ is the equivalence relation on sentences in ''M'' given by ''p'' ~ ''q'' if and only if ''p'' and ''q'' are [[Logical equivalence|logically equivalent]] in ''M'', and ''M'' / ~ is the set of equivalence classes under this relation. Then '''''L'''''(''M'') is an interior algebra. The interior operator in this case corresponds to the [[modal logic|modal operator]] □ ('''necessarily'''), while the closure operator corresponds to ◊ ('''possibly'''). This construction is a special case of a more general result for [[modal algebra]]s and modal logic.\n\nThe open elements of '''''L'''''(''M'') correspond to sentences that are only true if they are '''necessarily''' true, while the closed elements correspond to those that are only false if they are '''necessarily''' false.\n\nBecause of their relation to '''S4''', interior algebras are sometimes called '''S4 algebras''' or '''Lewis algebras''', after the [[philosophical logic|logician]] [[Clarence Irving Lewis|C. I. Lewis]], who first proposed the modal logics '''S4''' and '''S5'''.\n\n=== Preorders ===\nSince interior algebras are (normal) [[Boolean algebra (structure)|Boolean algebra]]s with [[unary operation|operators]], they can be represented by [[field of sets|fields of sets]] on appropriate relational structures. In particular, since they are [[modal algebra]]s, they can be represented as [[field of sets|fields of sets]] on a set with a single [[binary relation]], called a [[Kripke semantics|modal frame]]. The modal frames corresponding to interior algebras are precisely the [[Preorder|preordered sets]]. [[Preorder|Preordered sets]] (also called ''S4-frames'') provide the [[Kripke semantics]] of the modal logic '''S4''', and the connection between interior algebras and preorders is deeply related to their connection with modal logic.\n\nGiven a [[Preorder|preordered set]] '''''X''''' = ⟨''X'', «⟩ we can construct an interior algebra\n\n: '''''B'''''('''''X''''') = ⟨''P''(''X''), ∩, ∪, ′, ø, ''X'', <sup>I</sup>⟩\n\nfrom the [[power set]] [[Boolean algebra (structure)|Boolean algebra]] of ''X'' where the interior operator <sup>I</sup> is given by\n\n:''S''<sup>I</sup> = {''x'' ∈ ''X'' : for all ''y'' ∈ ''X'', ''x'' « ''y'' implies ''y'' ∈ ''S''} for all ''S'' ⊆ ''X''.\n\nThe corresponding closure operator is given by\n\n:''S''<sup>C</sup> = {''x'' ∈ ''X'' : there exists a ''y'' ∈ ''S'' with ''x'' « ''y''} for all ''S'' ⊆ ''X''.\n\n''S''<sup>I</sup> is the set of all ''worlds'' inaccessible from ''worlds'' outside ''S'', and ''S''<sup>C</sup> is the set of all ''worlds'' accessible from some ''world'' in ''S''. Every interior algebra can be [[embedding|embedded]] in an interior algebra of the form '''''B'''''('''''X''''') for some [[Preorder|preordered set]] '''''X''''' giving the above-mentioned representation as a [[field of sets]] (a '''preorder field''').\n\nThis construction and representation theorem is a special case of the more general result for [[modal algebra]]s and modal frames. In this regard, interior algebras are particularly interesting because of their connection to [[topology]]. The construction provides the [[Preorder|preordered set]] '''''X''''' with a [[topological space|topology]], the [[Alexandrov topology]], producing a [[topological space]] '''''T'''''('''''X''''') whose open sets are:\n\n:{''O'' ⊆ ''X'' : for all ''x'' ∈ ''O'' and all ''y'' ∈ ''X'', ''x'' « ''y'' implies ''y'' ∈ ''O''}.\n\nThe corresponding closed sets are:\n\n:{''C'' ⊆ ''X'' : for all ''x'' ∈ ''C'' and all ''y'' ∈ ''X'', ''y'' « ''x'' implies ''y'' ∈ ''C''}.\n\nIn other words, the open sets are the ones whose ''worlds'' are inaccessible from outside (the '''up-sets'''), and the closed sets are the ones for which every outside ''world'' is inaccessible from inside (the '''down-sets'''). Moreover, '''''B'''''('''''X''''') = '''''A'''''('''''T'''''('''''X''''')).\n\n=== Monadic Boolean algebras ===\nAny [[monadic Boolean algebra]] can be considered to be an interior algebra where the interior operator is the universal quantifier and the closure operator is the existential quantifier. The monadic Boolean algebras are then precisely the [[Variety (universal algebra)|variety]] of interior algebras satisfying the identity ''x''<sup>IC</sup> = ''x''<sup>I</sup>. In other words, they are precisely the interior algebras in which every open element is closed or equivalently, in which every closed element is open. Moreover, such interior algebras are precisely the [[Semisimple algebra|semisimple]] interior algebras. They are also the interior algebras corresponding to the modal logic '''S5''', and so have also been called '''S5 algebras'''.\n\nIn the relationship between preordered sets and interior algebras they correspond to the case where the preorder is an [[equivalence relation]], reflecting the fact that such preordered sets provide the Kripke semantics for '''S5'''. This also reflects the relationship between the [[monadic logic]] of quantification (for which monadic Boolean algebras provide an [[Lindenbaum–Tarski algebra|algebraic description]]) and '''S5''' where the modal operators □ ('''necessarily''') and ◊ ('''possibly''') can be interpreted in the Kripke semantics using monadic universal and existential quantification, respectively, without reference to an accessibility relation.\n\n=== Heyting algebras ===\nThe open elements of an interior algebra form a [[Heyting algebra]] and the closed elements form a [[duality (order theory)|dual]] Heyting algebra. The regular open elements and regular closed elements correspond to the pseudo-complemented elements and [[duality (order theory)|dual]] pseudo-complemented elements of these algebras respectively and thus form Boolean algebras. The clopen elements correspond to the complemented elements and form a common subalgebra of these Boolean algebras as well as of the interior algebra itself. Every [[Heyting algebra]] can be represented as the open elements of an interior algebra and the latter may be chosen to an interior algebra generated by its open elements - such interior algebras correspond one to one with Heyting algebras (up to isomorphism) being the free Boolean extensions of the latter.\n\nHeyting algebras [[Lindenbaum–Tarski algebra|play the same role]] for [[intuitionistic logic]] that interior algebras play for the modal logic '''S4''' and [[Boolean algebra (structure)|Boolean algebra]]s play for [[propositional logic]]. The relation between Heyting algebras and interior algebras reflects the relationship between intuitionistic logic and '''S4''', in which one can interpret theories of intuitionistic logic as '''S4''' theories [[deductive closure|closed]] under [[logical truth|necessity]]. The one to one correspondence between Heyting algebras and interior algebras generated by their open elements reflects the correspondence between extensions of intuitionistic logic and normal extensions of the modal logic '''S4.Grz'''.\n\n=== Derivative algebras ===\nGiven an interior algebra '''''A''''', the closure operator obeys the axioms of the [[Derivative algebra (abstract algebra)|derivative operator]], <sup>D</sup>. Hence we can form a [[Abstract algebra|derivative algebra]] '''''D'''''('''''A''''') with the same underlying Boolean algebra as '''''A''''' by using the closure operator as a derivative operator.\n\nThus interior algebras are [[Derivative algebra (abstract algebra)|derivative algebras]]. From this perspective, they are precisely the [[variety (universal algebra)|variety]] of derivative algebras satisfying the identity ''x''<sup>D</sup> ≥ ''x''. Derivative algebras provide the appropriate [[Lindenbaum–Tarski algebra|algebraic semantics]] for the modal logic '''WK4'''. Hence derivative algebras stand to topological [[derived set (mathematics)|derived set]]s and '''WK4''' as interior/closure algebras stand to topological interiors/closures and '''S4'''.\n \nGiven a derivative algebra '''''V''''' with derivative operator <sup>D</sup>, we can form an interior algebra '''''I'''''('''''V''''') with the same underlying Boolean algebra as '''''V''''', with interior and closure operators defined by ''x''<sup>I</sup> = ''x''·''x''&nbsp;′&nbsp;<sup>D</sup>&nbsp;′ and ''x''<sup>C</sup> = ''x'' + ''x''<sup>D</sup>, respectively. Thus every derivative algebra can be regarded as an interior algebra. Moreover, given an interior algebra '''''A''''', we have '''''I'''''('''''D'''''('''''A''''')) = '''''A'''''. However, '''''D'''''('''''I'''''('''''V''''')) = '''''V''''' does ''not'' necessarily hold for every derivative algebra '''''V'''''.\n\n==Stone duality and representation for interior algebras==\n[[Stone duality]] provides a category theoretic duality between Boolean algebras and a class of topological spaces known as [[Boolean space]]s. Building on nascent ideas of relational semantics (later formalized by [[Saul Kripke|Kripke]]) and a result of R. S. Pierce, [[Bjarni Jónsson|Jónsson]], [[Alfred Tarski|Tarski]] and G. Hansoul extended Stone duality to [[Boolean algebras with operators]] by equipping Boolean spaces with relations that correspond to the operators via a [[Field of sets#Complex algebras and fields of sets on relational structures|power set construction]]. In the case of interior algebras the interior (or closure) operator corresponds to a pre-order on the Boolean space. Homomorphisms between interior algebras correspond to a class of continuous maps between the Boolean spaces known as '''pseudo-epimorphisms''' or '''p-morphisms''' for short. This generalization of Stone duality to interior algebras based on the Jónsson–Tarski representation was investigated by Leo Esakia and is also known as the ''Esakia duality for S4-algebras (interior algebras)'' and is closely related to the [[Esakia duality]] for Heyting algebras.\n\nWhereas the Jónsson–Tarski generalization of Stone duality applies to Boolean algebras with operators in general, the connection between interior algebras and topology allows for another method of generalizing Stone duality that is unique to interior algebras. An intermediate step in the development of Stone duality is [[Stone's representation theorem for Boolean algebras|Stone's representation theorem]] which represents a Boolean algebra as a [[field of sets]]. The Stone topology of the corresponding Boolean space is then generated using the field of sets as a [[topological basis]]. Building on the [[topological semantics]] introduced by Tang Tsao-Chen for Lewis's modal logic, [[J.C.C. McKinsey|McKinsey]] and Tarski showed that by generating a topology equivalent to using only the complexes that correspond to open elements as a basis, a representation of an interior algebra is obtained as a [[Field of sets#topological field of sets|topological field of sets]] - a field of sets on a topological space that is closed with respect to taking interiors or closures. By equipping topological fields of sets with appropriate morphisms known as '''field maps''' C. Naturman showed that this approach can be formalized as a category theoretic Stone duality in which the usual Stone duality for Boolean algebras corresponds to the case of interior algebras having redundant interior operator (Boolean interior algebras).\n\nThe pre-order obtained in the Jónsson–Tarski approach corresponds to the accessibility relation in the Kripke semantics for an S4 theory, while the intermediate field of sets corresponds to a representation of the Lindenbaum–Tarski algebra for the theory using the sets of possible worlds in the Kripke semantics in which sentences of the theory hold. Moving from the field of sets to a Boolean space somewhat obfuscates this connection. By treating fields of sets on pre-orders as a category in its own right this deep connection can be formulated as a category theoretic duality that generalizes Stone representation without topology. R. Goldblatt had shown that with restrictions to appropriate homomorphisms such a duality can be formulated for arbitrary modal algebras and modal frames. Naturman showed that in the case of interior algebras this duality applies to more general topomorphisms and can be factored via a category theoretic functor through the duality with topological fields of sets. The latter represent the Lindenbaum–Tarski algebra using sets of points satisfying sentences of the S4 theory in the topological semantics. The pre-order can be obtained as the specialization pre-order of the McKinsey-Tarski topology. The Esakia duality can be recovered via a functor that replaces the field of sets with the Boolean space it generates. Via a functor that instead replaces the pre-order with its corresponding Alexandrov topology, an alternative representation of the interior algebra as a field of sets is obtained where the topology is the Alexandrov bico-reflection of the McKinsey-Tarski topology. The approach of formulating a topological duality for interior algebras using both the Stone topology of the Jónsson–Tarski approach and the Alexandrov topology of the pre-order to form a bi-topological space has been investigated by G. Bezhanishvili, R.Mines, and P.J. Morandi. The McKinsey-Tarski topology of an interior algebra is the intersection of the former two topologies.\n\n==Metamathematics==\nGrzegorczyk proved the elementary theory of closure algebras [[decision problem|undecidable]].<ref>[[Andrzej Grzegorczyk]] (1951) \"Undecidability of some topological theories,\" ''Fundamenta Mathematicae 38'': 137-52.</ref> Naturman demonstrated that the theory is [[hereditarily undecidable]] (all its subtheories are undecidable) and demonstrated an infinite chain of elementary classes of interior algebras with hereditarily undecidable theories.\n\n==Notes==\n<references />\n\n==References==\n* Blok, W.A., 1976, ''Varieties of interior algebras,'' Ph.D. thesis, University of Amsterdam. \n* Esakia, L., 2004, \"[http://www.sciencedirect.com/science/article/pii/S0168007203001192 Intuitionistic logic and modality via topology],\" ''Annals of Pure and Applied Logic 127'': 155-70.\n* McKinsey, J.C.C.  and [[Alfred Tarski]], 1944, \"The Algebra of Topology,\" ''Annals of Mathematics 45'': 141-91.\n* Naturman, C.A., 1991, ''Interior Algebras and Topology'', Ph.D. thesis, University of Cape Town Department of Mathematics.\n* Bezhanishvili, G., Mines, R. and Morandi, P.J., 2008, ''Topo-canonical completions of closure algebras and Heyting algebras'', ''Algebra Universalis 58'': 1-34.\n* Schmid, J., 1973, ''On the compactification of closure algebras'', ''Fundamenta Mathematicae 79'': 33-48\n* Sikorski R., 1955, ''Closure homomorphisms and interior mappings'', ''Fundamenta Mathematicae 41'': 12-20\n\n[[Category:Abstract algebra]]\n[[Category:Mathematical logic]]\n[[Category:Boolean algebra]]\n[[Category:Closure operators]]\n[[Category:Modal logic]]"
    },
    {
      "title": "Inverse limit",
      "url": "https://en.wikipedia.org/wiki/Inverse_limit",
      "text": "In [[mathematics]], the '''inverse limit''' (also called the '''projective limit''') is a construction that allows one to \"glue together\" several related [[mathematical object|objects]], the precise manner of the gluing process being specified by morphisms between the objects. Inverse limits can be defined in any [[category (mathematics)|category]], and they are a special case of the concept of a [[Limit (category theory)|limit in category theory]].\n\n== Formal definition ==\n\n=== Algebraic objects ===\n\nWe start with the definition of an '''[[inverse system]]''' (or projective system) of [[group (mathematics)|groups]] and [[group homomorphism|homomorphisms]]. Let (''I'', ≤) be a [[directed set|directed]] [[poset]] (not all authors require ''I'' to be directed). Let (''A''<sub>''i''</sub>)<sub>''i''∈''I''</sub> be a [[indexed family|family]] of groups and suppose we have a family of homomorphisms ''f''<sub>''ij''</sub>: ''A''<sub>''j''</sub> → ''A''<sub>''i''</sub> for all ''i'' ≤ ''j'' (note the order), called bonding maps, with the following properties:\n# ''f''<sub>''ii''</sub> is the identity on ''A''<sub>''i''</sub>,\n# ''f''<sub>''ik''</sub> = ''f''<sub>''ij''</sub> ∘ ''f''<sub>''jk''</sub> for all ''i'' ≤ ''j'' ≤ ''k''.\nThen the pair ((''A''<sub>''i''</sub>)<sub>''i''∈''I''</sub>, (''f''<sub>''ij''</sub>)<sub>''i''≤ ''j''∈''I''</sub>) is called an inverse system of groups and morphisms over ''I'', and the morphisms ''f''<sub>''ij''</sub> are called the transition morphisms of the system.\n\nWe define the '''inverse limit''' of the inverse system ((''A''<sub>''i''</sub>)<sub>''i''∈''I''</sub>, (''f''<sub>''ij''</sub>)<sub>''i''≤ ''j''∈''I''</sub>) as a particular [[subgroup]] of the [[direct product]] of the ''A''<sub>''i''</sub>'s:\n\n:<math>A = \\varprojlim_{i\\in I}{A_i} = \\left\\{\\left.\\vec a \\in \\prod_{i\\in I}A_i \\;\\right|\\; a_i = f_{ij}(a_j) \\text{ for all } i \\leq j \\text{ in } I\\right\\}.</math>\n\nThe inverse limit ''A'' comes equipped with ''natural projections'' π<sub>''i''</sub>: ''A'' → ''A''<sub>''i''</sub> which pick out the ''i''th component of the direct product for each ''i'' in ''I''. The inverse limit and the natural projections satisfy a [[universal property]] described in the next section.\n\nThis same construction may be carried out if the ''A''<sub>''i''</sub>'s are [[Set (mathematics)|sets]],<ref name=\"same-construction\">John Rhodes & Benjamin Steinberg. The q-theory of Finite Semigroups. p. 133. {{ISBN|978-0-387-09780-0}}.</ref> semigroups,<ref name=\"same-construction\"/> topological spaces,<ref name=\"same-construction\"/> [[ring (mathematics)|rings]], [[module (mathematics)|modules]] (over a fixed ring), [[algebra over a field|algebras]] (over a fixed ring), etc., and the [[homomorphism]]s are morphisms in the corresponding [[category theory|category]]. The inverse limit will also belong to that category.\n\n=== General definition ===\n\nThe inverse limit can be defined abstractly in an arbitrary [[category (mathematics)|category]] by means of a [[universal property]]. Let (''X''<sub>''i''</sub>, ''f''<sub>''ij''</sub>) be an inverse system of objects and [[morphism]]s  in a category ''C'' (same definition as above). The '''inverse limit''' of this system is an object ''X'' in ''C'' together with morphisms π<sub>''i''</sub>: ''X'' → ''X''<sub>''i''</sub> (called ''projections'') satisfying π<sub>''i''</sub> = ''f''<sub>''ij''</sub> ∘ π<sub>''j''</sub> for all ''i'' ≤ ''j''. The pair (''X'', π<sub>''i''</sub>)  must be universal in the sense that for any other such pair (''Y'', ψ<sub>''i''</sub>)  (i.e. ψ<sub>''i''</sub>: ''Y'' → ''X''<sub>''i''</sub> with ψ<sub>''i''</sub> = ''f''<sub>''ij''</sub> ∘ ψ<sub>''j''</sub> for all ''i'' ≤ ''j'') there exists a unique morphism ''u'': ''Y'' → ''X'' such that the diagram\n\n<div style=\"text-align: center;\">[[Image:InverseLimit-01.png]]</div>\n\n[[commutative diagram|commutes]] for all ''i'' ≤ ''j'', for which it suffices to show that ψ<sub>''i''</sub> = π<sub>''i''</sub> ∘ ''u'' for all ''i''. The inverse limit is often denoted\n:<math>X = \\varprojlim X_i</math>\nwith the inverse system (''X''<sub>''i''</sub>, ''f''<sub>''ij''</sub>) being understood.\n\nIn some categories, the inverse limit of certain inverse systems does not exist. If it does, however, it is unique in a strong sense: given any two inverse limits ''X'' and ''X''' of an inverse system, there exists a ''unique'' [[isomorphism]] ''X''&prime; → ''X'' commuting with the projection maps.\n\nWe note that an inverse system in a category ''C'' admits an alternative description in terms of [[functor]]s. Any partially ordered set ''I'' can be considered as a [[small category]] where the morphisms consist of arrows ''i'' → ''j'' [[if and only if]] ''i'' ≤ ''j''. An inverse system is then just a [[contravariant functor]] ''I'' → ''C'', and the inverse limit functor\n<math>\\varprojlim:C^{I^{op}}\\rightarrow C</math> is a [[covariant functor]].\n\n== Examples ==\n\n* The ring of [[p-adic number|''p''-adic integers]] is the inverse limit of the rings '''Z'''/''p''<sup>''n''</sup>'''Z''' (see [[modular arithmetic]]) with the index set being the [[natural number]]s with the usual order, and the morphisms being \"take remainder\". That is, one considers sequences of integers <math>(n_0, n_1, \\cdots)</math> such that each element of the sequence \"projects\" down to the previous ones, namely, that <math>n_i\\equiv n_j \\mbox{ mod } p^{i+1}</math> whenever <math>i<j.</math> The natural topology on the ''p''-adic integers is the one implied here, namely the [[product topology]] with [[cylinder set]]s as the open sets.\n* The ring <math>\\textstyle R[[t]]</math> of [[formal power series]] over a commutative ring ''R'' can be thought of as the inverse limit of the rings <math>\\textstyle R[t]/t^nR[t]</math>, indexed by the natural numbers as usually ordered, with the morphisms from <math>\\textstyle R[t]/t^{n+j}R[t]</math> to <math>\\textstyle R[t]/t^nR[t]</math> given by the natural projection.\n* [[Pro-finite group]]s are defined as inverse limits of (discrete) finite groups.\n* Let the index set ''I'' of an inverse system (''X''<sub>''i''</sub>, ''f''<sub>''ij''</sub>) have a [[greatest element]] ''m''. Then the natural projection π<sub>''m''</sub>: ''X'' → ''X''<sub>''m''</sub> is an isomorphism.\n*In the [[category of sets]], every inverse system has an inverse limit, which can be constructed in an elementary manner as a subset of the product of the sets forming the inverse system. The inverse limit of any inverse system of non-empty finite sets is non-empty. This is a generalization of [[Kőnig's lemma]] in graph theory and may be proved with [[Tychonoff's theorem]], viewing the finite sets as compact discrete spaces, and then applying the [[finite intersection property]] characterization of compactness.\n* In the [[category of topological spaces]], every inverse system has an inverse limit. It is constructed by placing the [[initial topology]] on the underlying set-theoretic inverse limit.  This is known as the '''limit topology'''.\n** The set of infinite [[String (computer science)|strings]] is the inverse limit of the set of finite strings, and is thus endowed with the limit topology. As the original spaces are [[discrete topology|discrete]], the limit space is [[totally disconnected]]. This is one way of realizing the [[p-adic|''p''-adic numbers]] and the [[Cantor set]] (as infinite strings).\n\n==Derived functors of the inverse limit==\n\nFor an [[abelian category]] ''C'', the inverse limit functor\n:<math>\\varprojlim:C^I\\rightarrow C</math>\nis [[Exact functor|left exact]]. If ''I'' is ordered (not simply partially ordered) and [[countable]], and ''C'' is the category '''Ab''' of abelian groups, the Mittag-Leffler condition is a condition on the transition morphisms ''f''<sub>''ij''</sub> that ensures the exactness of <math>\\varprojlim</math>. Specifically, [[Samuel Eilenberg|Eilenberg]] constructed a functor\n:<math>\\varprojlim{}^1:\\operatorname{Ab}^I\\rightarrow\\operatorname{Ab}</math>\n(pronounced \"lim one\") such that if (''A''<sub>''i''</sub>, ''f''<sub>''ij''</sub>), (''B''<sub>''i''</sub>, ''g''<sub>''ij''</sub>), and (''C''<sub>''i''</sub>, ''h''<sub>''ij''</sub>) are three inverse systems of abelian groups, and\n:<math>0\\rightarrow A_i\\rightarrow B_i\\rightarrow C_i\\rightarrow0</math>\nis a [[short exact sequence]] of inverse systems, then\n:<math>0\\rightarrow\\varprojlim A_i\\rightarrow\\varprojlim B_i\\rightarrow\\varprojlim C_i\\rightarrow\\varprojlim{}^1A_i</math>\nis an exact sequence in '''Ab'''.\n\n===Mittag-Leffler condition===\n\nIf the ranges of the morphisms of an inverse system of abelian groups (''A''<sub>''i''</sub>, ''f''<sub>''ij''</sub>) are ''stationary'', that is, for every ''k'' there exists ''j'' ≥ ''k'' such that for all ''i'' ≥ ''j'' :<math> f_{kj}(A_j)=f_{ki}(A_i)</math> one says that the system satisfies the '''Mittag-Leffler condition'''.\n\nThe name \"Mittag-Leffler\" for this condition was given by Bourbaki in their chapter on uniform structures for a similar result about inverse limits of complete Hausdorff uniform spaces. Mittag-Leffler used a similar argument in the proof of [[Mittag-Leffler's theorem]].\n\nThe following situations are examples where the Mittag-Leffler condition is satisfied: \n* a system in which the morphisms ''f''<sub>''ij''</sub> are surjective\n* a system of finite-dimensional vector spaces or finite abelian groups or modules of finite length or Artinian modules.\n\nAn example where <math>\\varprojlim{}^1</math> is non-zero is obtained by taking ''I'' to be the non-negative [[integer]]s, letting ''A''<sub>''i''</sub> = ''p''<sup>''i''</sup>'''Z''', ''B''<sub>''i''</sub> = '''Z''', and ''C''<sub>''i''</sub> = ''B''<sub>''i''</sub> / ''A''<sub>''i''</sub> = '''Z'''/''p''<sup>''i''</sup>'''Z'''. Then\n:<math>\\varprojlim{}^1A_i=\\mathbf{Z}_p/\\mathbf{Z}</math>\nwhere '''Z'''<sub>''p''</sub> denotes the [[p-adic integers]].\n\n===Further results===\n\nMore generally, if ''C'' is an arbitrary abelian category that has [[Injective object#Enough injectives|enough injectives]], then so does ''C''<sup>''I''</sup>, and the right [[derived functors]] of the inverse limit functor can thus be defined. The ''n''th right derived functor is denoted\n:<math>R^n\\varprojlim:C^I\\rightarrow C.</math>\nIn the case where ''C'' satisfies [[Grothendieck]]'s axiom [[Abelian category#Grothendieck's axioms|(AB4*)]], [[Jan-Erik Roos]] generalized the functor lim<sup>1</sup> on '''Ab'''<sup>''I''</sup> to series of functors lim<sup>n</sup> such that\n:<math>\\varprojlim{}^n\\cong R^n\\varprojlim.</math>\nIt was thought for almost 40 years that Roos had proved (in ''Sur les foncteurs dérivés de lim. Applications. '') that lim<sup>1</sup> ''A''<sub>''i''</sub> = 0 for (''A''<sub>''i''</sub>, ''f''<sub>''ij''</sub>) an inverse system with surjective transition morphisms and ''I'' the set of non-negative integers (such inverse systems are often called \"[[Mittag-Leffler]] sequences\"). However, in 2002, [[Amnon Neeman]] and [[Pierre Deligne]] constructed an example of such a system in a category satisfying (AB4) (in addition to (AB4*)) with lim<sup>1</sup> ''A''<sub>''i''</sub> ≠ 0. Roos has since shown (in \"Derived functors of inverse limits revisited\") that his result is correct if ''C'' has a set of generators (in addition to satisfying (AB3) and (AB4*)).\n\n[[Barry Mitchell (mathematician)|Barry Mitchell]] has shown (in \"The cohomological dimension of a directed set\") that if ''I'' has [[cardinality]] <math>\\aleph_d</math> (the ''d''th [[Aleph number|infinite cardinal]]), then ''R''<sup>''n''</sup>lim is zero for all ''n'' ≥ ''d'' + 2. This applies to the ''I''-indexed diagrams in the category of ''R''-modules, with ''R'' a commutative ring; it is not necessarily true in an arbitrary abelian category (see Roos' \"Derived functors of inverse limits revisited\" for examples of abelian categories in which lim<sup>''n''</sup>, on diagrams indexed by a countable set, is nonzero for&nbsp;''n''&nbsp;>&nbsp;1).\n\n== Related concepts and generalizations ==\n\nThe [[dual (category theory)|categorical dual]] of an inverse limit is a [[direct limit]] (or inductive limit). More general concepts are the [[limit (category theory)|limits and colimits]] of category theory. The terminology is somewhat confusing: inverse limits are a class of limits, while direct limits are a class of colimits.\n\n==See also==\n\n*[[Direct limit|Direct, or inductive limit]]\n* [[Protorus]]\n\n== Notes ==\n<references />\n\n==References==\n*{{citation|first=Nicolas|last=Bourbaki|authorlink=Nicolas Bourbaki|title=Algebra I|publisher=Springer|year=1989|isbn=978-3-540-64243-5|oclc=40551484}}\n*{{citation|first=Nicolas|last=Bourbaki|authorlink=Nicolas Bourbaki|title=General topology: Chapters 1-4|publisher=Springer|year=1989|isbn=978-3-540-64241-1|oclc=40551485}}\n*{{citation|first=Saunders |last=Mac Lane |authorlink=Saunders Mac Lane|title=[[Categories for the Working Mathematician]] | edition=2nd |date=September 1998 |publisher=Springer|isbn=0-387-98403-8}}\n*{{Citation | last=Mitchell | first=Barry | author-link=Barry Mitchell (mathematician) | title=Rings with several objects | journal=[[Advances in Mathematics]] | mr=0294454  | year=1972 | volume=8 | pages=1–161 | doi=10.1016/0001-8708(72)90002-3}}\n*{{Citation | last=Neeman | first=Amnon | author-link=Amnon Neeman | title=A counterexample to a 1961 \"theorem\" in homological algebra (with appendix by Pierre Deligne) | journal=[[Inventiones Mathematicae]] | mr=1906154  | year=2002 | volume=148 | issue=2 | pages=397–420 | doi=10.1007/s002220100197}}\n*{{Citation | last=Roos | first=Jan-Erik | author-link=Jan-Erik Roos | title=Sur les foncteurs dérivés de lim. Applications | journal=C. R. Acad. Sci. Paris | mr=0132091  | year=1961 | volume=252 | pages=3702–3704}}\n*{{Citation | last=Roos | first=Jan-Erik | author-link=Jan-Erik Roos | title=Derived functors of inverse limits revisited | journal=[[London Mathematical Society|J. London Math. Soc.]] |series=Series 2 | mr=2197371  | year=2006 | volume=73 | issue=1 | pages=65–83 | doi=10.1112/S0024610705022416}}\n* Section 3.5 of {{Weibel IHA}}\n\n{{Category theory}}\n\n[[Category:Limits (category theory)]]\n[[Category:Abstract algebra]]\n\n[[de:Limes (Kategorientheorie)]]"
    },
    {
      "title": "Involution (mathematics)",
      "url": "https://en.wikipedia.org/wiki/Involution_%28mathematics%29",
      "text": "{{for|the archaic use of this term|exponentiation}}\n[[Image:Involution.svg|right|thumb|An involution is a function <math>f:X\\to X</math> that, when applied twice, brings one back to the starting point.]]\nIn [[mathematics]], an '''involution''', or an '''involutory function''', is a [[function (mathematics)|function]] {{mvar|f}} that is its own [[inverse function|inverse]],\n\n: {{math|''f''(''f''(''x'')) {{=}} ''x''}}\n\nfor all {{mvar|x}} in the [[domain of a function|domain]] of {{mvar|f}}.<ref>{{Citation|last=Russell|first=Bertrand|title=Principles of mathematics|year=1903|publisher=W. W. Norton & Company, Inc|page=426|url=https://books.google.com/books?id=63ooitcP2osC&lpg=PR3&dq=involution%20subject%3A%|edition=2nd|isbn=9781440054167}}</ref>\n\nThe term '''anti-involution''' refers to involutions based on [[antihomomorphism]]s (see below the section on [[Involution (mathematics)#Quaternion algebra.2C groups.2C semigroups|Quaternion algebra, groups, semigroups]])\n\n: {{math|''f''(''xy'') {{=}} ''f''(''y'') ''f''(''x'')}}\n\nsuch that\n\n: {{math|''xy'' {{=}} ''f''(''f''(''xy'')) {{=}} ''f''( ''f''(''y'') ''f''(''x'') ) {{=}} ''f''(''f''(''x''))  ''f''(''f''(''y'')) {{=}} ''xy''}}.\n\n==General properties==\nAny involution is a [[bijection]].\n\nThe [[identity function|identity map]] is a trivial example of an involution. Common examples in mathematics of nontrivial involutions include [[multiplication]] by &minus;1 in [[arithmetic]], the taking of [[multiplicative inverse|reciprocals]], [[complement (set theory)|complementation]] in [[set theory]] and [[complex conjugate|complex conjugation]]. Other examples include [[circle inversion]], rotation by a half-turn, and [[reciprocal cipher]]s such as the [[ROT13]] transformation and the [[Beaufort cipher|Beaufort]] [[polyalphabetic cipher]].\n\nThe number of involutions, including the identity involution, on a set with {{nowrap|1=''n'' = 0, 1, 2, ...}} elements is given by a [[recurrence relation]] found by [[Heinrich August Rothe]] in 1800:\n:''a''<sub>0</sub> = ''a''<sub>1</sub> = 1;\n:''a''<sub>''n''</sub> = ''a''<sub>''n'' &minus; 1</sub> + (''n'' &minus; 1)''a''<sub>''n'' &minus; 2</sub>, for {{nowrap|''n'' > 1}}.\nThe first few terms of this sequence are [[1 (number)|1]], 1, [[2 (number)|2]], [[4 (number)|4]], [[10 (number)|10]], [[26 (number)|26]], [[76 (number)|76]], [[232 (number)|232]] {{OEIS|id=A000085}}; these numbers are called the [[Telephone number (mathematics)|telephone numbers]], and they also count the number of [[Young tableau]]x with a given number of cells.<ref>{{citation\n | last = Knuth | first = Donald E. | author-link = Donald Knuth\n | location = Reading, Mass.\n | mr = 0445948\n | pages = 48, 65\n | publisher = Addison-Wesley\n | title = [[The Art of Computer Programming]], Volume 3: Sorting and Searching\n | year = 1973}}.</ref>\nThe [[Function composition|composition]] {{nowrap|''g'' ∘ ''f''}} of two involutions ''f'' and ''g'' is an involution if and only if they commute: {{nowrap|1=''g'' ∘ ''f'' = ''f'' ∘ ''g''}}.<ref>{{citation|title=The Elements of Operator Theory|first=Carlos S.|last=Kubrusly|publisher=Springer Science & Business Media|year=2011|isbn=9780817649982|at=Problem&nbsp;1.11(a), p.&nbsp;27|url=https://books.google.com/books?id=g-UFYTO8SbMC&pg=PA27}}.</ref>\n\nEvery involution on an [[odd number]] of elements has at least one [[Fixed point (mathematics)|fixed point]]. More generally, for an involution on a finite set of elements, the number of elements and the number of fixed points have the same [[parity (mathematics)|parity]].<ref>{{citation\n | last = Zagier | first = D. | authorlink = Don Zagier\n | doi = 10.2307/2323918\n | issue = 2\n | journal = [[American Mathematical Monthly]]\n | mr = 1041893\n | page = 144\n | title = A one-sentence proof that every prime ''p''≡ 1 (mod 4) is a sum of two squares\n | volume = 97\n | year = 1990}}.</ref>\n\n==Involution throughout the fields of mathematics==\n\n===Pre-calculus ===\nBasic examples of involutions are the functions:\n\n:<math> f_1(x) = -x </math>, &nbsp; or &nbsp; <math> f_2(x) = \\frac {1}{x} </math>, as well as their composition <math> (f_1\\circ f_2)(x) = (f_2\\circ f_1)(x) = f_3(x) = -\\frac {1}{x}. </math> \n\nThese are not the only pre-calculus involutions. Another one within the positive reals is:\n\n:<math> f(x) = \\ln\\left(\\frac {e^x+1}{e^x-1}\\right) . </math>\n\nThe [[Graph_of_a_function|graph]] of an involution (on the real numbers) is [[Reflection_symmetry|line-symmetric]] over the line <math>y=x</math>. This is due to the fact that the inverse of any ''general'' function will be its reflection over the 45° line <math>y=x</math>. This can be seen by \"swapping\" <math>x</math> with <math>y</math>. If, in particular, the function is an ''involution'', then it will serve as its own reflection.\n\nOther elementary involutions are useful in [[Functional equation#Solving functional equations|solving functional equations]].\n\n===Euclidean geometry===\nA simple example of an involution of the three-dimensional [[Euclidean space]] is [[Reflection (mathematics)|reflection]] through a [[plane (mathematics)|plane]]. Performing a reflection twice brings a point back to its original coordinates.\n\nAnother involution is [[reflection through the origin]]; not a reflection in the above sense, and so, a distinct example.\n\nThese transformations are examples of [[affine involution]]s.\n\n===Projective geometry===\nAn involution is a [[projectivity]]  of period 2, that is, a projectivity that interchanges pairs of points. Coxeter relates three theorems on involutions:\n* Any projectivity that interchanges two points is an involution.\n* The three pairs of opposite sides of a [[complete quadrangle]] meet any line (not through a vertex) in three pairs of an involution (this is [[Desargues]]'s Involution Theorem,<ref>J. V. Field and J. J. Gray, ''The Geometrical Work of Girard Desargues,'' (New York: Springer, 1987), p. 54</ref> whose origins can be seen in Lemma IV of the lemmas to the ''Porisms'' of Euclid in Volume VII of the ''Collection'' of [[Pappus of Alexandria]] <ref>Ivor Thomas (ed.), ''Selections Illustrating the History of Greek Mathematics,'' Volume II, number 362 in the [[Loeb Classical Library]] (Cambridge and London: Harvard and Heinemann, 1980), pp. 610&ndash;3</ref>).\n* If an involution has one [[fixed point (mathematics)|fixed point]], it has another, and consists of the correspondence between [[projective harmonic conjugate|harmonic conjugates]] with respect to these two points. In this instance the involution is termed \"hyperbolic\", while if there are no fixed points it is \"elliptic\".\n\nAnother type of involution occurring in projective geometry is a '''polarity''' which is a [[correlation (projective geometry)|correlation]] of period 2.\n<ref>[[H. S. M. Coxeter]] (1969) Introduction to Geometry, pp 244&ndash;8, [[John Wiley & Sons]]</ref>\n\n===Linear algebra===\n{{details|Involutory matrix}}\nIn linear algebra, an involution is a linear operator ''T'' on a vector space, such that <math>T^2=I</math>. Except for in characteristic 2, such operators are diagonalizable for a given basis with just 1s and &minus;1s on the diagonal of the corresponding matrix. If the operator is orthogonal (an '''orthogonal involution'''), it is orthonormally diagonalizable.\n\nFor example, suppose that a basis for a vector space ''V'' is chosen, and that ''e''<sub>1</sub> and ''e''<sub>2</sub> are basis elements. There exists a linear transformation ''f'' which sends ''e''<sub>1</sub> to ''e''<sub>2</sub>, and sends ''e''<sub>2</sub> to ''e''<sub>1</sub>, and which is the identity on all other basis vectors. It can be checked that {{nowrap|1=''f''(''f''(''x'')) = ''x''}} for all ''x'' in ''V''. That is, ''f'' is an involution of ''V''.\n\nFor a specific basis, any linear operator can be represented by a [[matrix (mathematics)|matrix]] ''T''. Every matrix has a [[transpose]], obtained by swapping rows for columns. This transposition is an involution on the set of matrices.\n\nThe definition of involution extends readily to [[module (mathematics)|modules]]. Given a module ''M'' over a [[ring (mathematics)|ring]] ''R'', an ''R'' [[endomorphism]] ''f'' of ''M'' is called an involution if ''f''&nbsp;<sup>2</sup> is the identity homomorphism on ''M''.\n\n[[Idempotent element (ring theory)#Relation with involutions|Involutions are related to idempotent]]s; if 2 is invertible then they [[bijection|correspond]] in a one-to-one manner.\n\n===Quaternion algebra, groups, semigroups===\nIn a [[quaternion algebra]], an (anti-)involution is defined by the following axioms: if we consider a transformation <math>x \\mapsto f(x)</math> then it is an involution if\n* <math> f(f(x))=x </math> (it is its own inverse)\n* <math> f(x_1+x_2)=f(x_1)+f(x_2) </math> and <math> f(\\lambda x)=\\lambda f(x) </math> (it is linear)\n* <math> f(x_1 x_2)=f(x_1) f(x_2) </math>\n\nAn anti-involution does not obey the last axiom but instead\n* <math> f(x_1 x_2)=f(x_2) f(x_1) </math>\n\nThis former law is sometimes called [[antidistributive]]. It also appears in [[group (mathematics)|groups]] as {{nowrap|1=(''xy'')<sup>−1</sup> = ''y''<sup>−1</sup>''x''<sup>−1</sup>}}. Taken as an axiom, it leads to the notion of [[semigroup with involution]], of which there are natural examples that are not groups, for example square matrix multiplication (i.e. the [[full linear monoid]]) with [[transpose]] as the involution.\n\n===Ring theory===\n{{details|*-algebra}}\nIn [[ring theory]], the word ''involution'' is customarily taken to mean an [[antihomomorphism]] that is its own inverse function.\nExamples of involutions in common rings:\n* [[complex conjugation]] on the [[complex plane]]\n* multiplication by j in the [[split-complex number]]s\n* taking the transpose in a matrix ring.\n\n===Group theory===\nIn [[group theory]], an element of a [[group (mathematics)|group]] is an involution if it has [[order (group theory)|order]] 2; i.e. an involution is an element ''a'' such that ''a'' ≠ ''e'' and ''a''<sup>2</sup> = ''e'', where ''e'' is the [[identity element]].<ref>\nJohn S. Rose.\n[https://books.google.com/books?id=j-I7Zpq3GdIC \"A Course on Group Theory\"].\np. 10, section 1.13.\n</ref>\n\nOriginally, this definition agreed with the first definition above, since members of groups were always bijections from a set into itself; i.e., ''group'' was taken to mean ''[[permutation group]]''. By the end of the 19th century, ''group'' was defined more broadly, and accordingly so was ''involution''.\n\nA [[permutation]] is an involution precisely if it can be written as a product of one or more non-overlapping [[transposition (mathematics)|transposition]]s.\n\nThe involutions of a group have a large impact on the group's structure. The study of involutions was instrumental in the [[classification of finite simple groups]].\n\nAn element ''x'' of a group ''G'' is called [[strongly real element|strongly real]] if there is an involution ''t'' {{nowrap begin}}with ''x''<sup>''t''</sup> = ''x''<sup>−1</sup>{{nowrap end}} {{nowrap begin}}(where ''x''<sup>''t''</sup> = ''t''<sup>−1</sup>⋅''x''⋅''t'').{{nowrap end}}\n\n[[Coxeter group]]s are groups generated by involutions with the relations determined only by relations given for pairs of the generating involutions. Coxeter groups can be used, among other things, to describe the possible [[Platonic solid|regular polyhedra]] and their [[regular polytope|generalizations to higher dimensions]].\n\n=== Mathematical logic ===\nThe operation of complement in [[Boolean algebra (structure)|Boolean algebra]]s is an involution. Accordingly, [[negation]] in classical logic satisfies the ''law of double negation:'' ¬¬''A'' is equivalent to ''A''.\n\nGenerally in non-classical logics, negation that satisfies the law of double negation is called ''involutive.'' In algebraic semantics, such a negation is realized as an involution on the algebra of [[truth value]]s. Examples of logics which have involutive negation are Kleene and Bochvar [[three-valued logic]]s, [[Łukasiewicz logic|Łukasiewicz many-valued logic]], [[fuzzy logic]] IMTL, etc. Involutive negation is sometimes added as an additional connective to logics with non-involutive negation; this is usual, for example, in [[t-norm fuzzy logics]].\n\nThe involutiveness of negation is an important characterization property for logics and the corresponding [[variety (universal algebra)|varieties of algebras]]. For instance, involutive negation characterizes [[Boolean algebra (structure)|Boolean algebra]]s among [[Heyting algebra]]s. Correspondingly, classical [[classical logic|Boolean logic]] arises by adding the law of double negation to [[intuitionistic logic]]. The same relationship holds also between [[MV-algebra]]s and [[BL-algebra]]s (and so correspondingly between [[Łukasiewicz logic]] and fuzzy logic [[BL (logic)|BL]]), IMTL and [[Monoidal t-norm logic|MTL]], and other pairs of important varieties of algebras (resp. corresponding logics).\n\nIn the study of [[binary relation]]s, every relation has a [[converse relation]]. Since the converse of the converse is the original relation, the conversion operation is an involution on the [[category of relations]]. Binary relations are [[partial order|ordered]] through [[inclusion (set theory)|inclusion]]. While this ordering is reversed with the [[complementation (mathematics)|complementation]] involution, it is preserved under conversion.\n\n=== Computer science ===\nThe [[XOR]] [[bitwise operation]] with a given value for one parameter is an involution. XOR [[Mask (computing)|masks]] were once used to draw graphics on images in such a way that drawing them twice on the background reverts the background to its original state. The [[Bitwise NOT|NOT]] bitwise operation is also an involution, and is a special case of the XOR operation where one parameter has all bits set to 1.\n\nAnother example is a bit mask and shift function operating on color values stored as integers, say in the form RGB, that swaps R and B, resulting in the form BGR.\nf(f(RGB))=RGB, f(f(BGR))=BGR.\n\nThe [[RC4]] cryptographic cipher is an involution, as encryption and decryption operations use the same function.\n\n== See also ==\n* [[Automorphism]]\n* [[Idempotence]]\n* [[ROT13]]\n\n== References ==\n{{reflist}}\n\n==Further reading==\n* {{cite journal |title=Quaternion involutions and anti-involutions |first1=Todd A. |last1=Ell |first2=Stephen J. |last2=Sangwine |journal=Computers & Mathematics with Applications |volume=53 |issue=1 |year=2007 |pages=137–143 |doi=10.1016/j.camwa.2006.10.029}}\n* {{citation | last1=Knus | first1=Max-Albert | last2=Merkurjev | first2=Alexander | author2-link=Alexander Merkurjev | last3=Rost | first3=Markus | author3-link=Markus Rost | last4=Tignol | first4=Jean-Pierre | title=The book of involutions | others=With a preface by J. Tits | zbl=0955.16001 | series=Colloquium Publications | publisher=[[American Mathematical Society]] | volume=44 | location=Providence, RI | year=1998 | isbn=0-8218-0904-0 }}\n* {{springer|title=Involution|id=p/i052510}}\n\n[[Category:Abstract algebra]]\n[[Category:Functions and mappings]]"
    },
    {
      "title": "Isomorphism class",
      "url": "https://en.wikipedia.org/wiki/Isomorphism_class",
      "text": "{{Use American English|date=January 2019}}{{Short description|Equivalence class of isomorphic mathematical objects\n}}\n{{Refimprove|date=June 2016}}\nAn '''isomorphism class''' is a collection of [[mathematical object]]s [[isomorphic]] to each other.<ref>{{cite book|author=Awodey, Steve|chapter=Isomorphisms|title=Category theory|publisher=Oxford University Press|year=2006|isbn=9780198568612|page=11|url=https://books.google.com/books?id=IK_sIDI2TCwC&pg=PA11}}</ref>\n\nIsomorphism classes are often defined if the exact identity of the elements of the set is considered irrelevant, and the properties of the structure of the mathematical object are studied.  Examples of this are [[ordinal number|ordinals]] and [[graph theory|graphs]].  However, there are circumstances in which the isomorphism class of an object conceals vital internal information about it; consider these examples:\n* The [[associative algebras]] consisting of [[coquaternion]]s and [[2 × 2 real matrices]] are isomorphic as [[ring theory|rings]]. Yet they appear in different contexts for application (plane mapping and kinematics) so the isomorphism is insufficient to merge the concepts.\n*In [[homotopy theory]], the [[fundamental group]] of a space <math>X</math> at a point <math>p</math>, though technically denoted <math>\\pi_1(X,p)</math> to emphasize the dependence on the base point, is often written lazily as simply <math>\\pi_1(X)</math> if <math>X</math> is [[connected space#Path connectedness|path connected]].  The reason for this is that the existence of a path between two points allows one to identify loops at one with loops at the other; however, unless <math>\\pi_1(X,p)</math> is [[abelian group|abelian]] this isomorphism is non-unique.  Furthermore, the classification of [[covering space]]s makes strict reference to particular subgroups of <math>\\pi_1(X,p)</math>, specifically distinguishing between isomorphic but [[conjugacy class|conjugate]] subgroups, and therefore amalgamating the elements of an isomorphism class into a single featureless object seriously decreases the level of detail provided by the theory.\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Isomorphism Class}}\n[[Category:Abstract algebra]]"
    },
    {
      "title": "Left and right (algebra)",
      "url": "https://en.wikipedia.org/wiki/Left_and_right_%28algebra%29",
      "text": "{{other uses|Left and right (disambiguation)}}\n{{refimprove|date=November 2012}}\n{| align=right class=\"wikitable\" style=\"margin-left:1em\"\n |{{bigmath|''s a''}}<br/>{{bigmath|''s b''}}<br/>{{bigmath|''s c''}}<br/>{{bigmath|''s d''}}<br/>{{bigmath|''s e''}}<br/>{{bigmath|''s f''}}<br/>{{bigmath|''s g''}}<br/>…\n | align=right |{{bigmath|''a t''}}<br/>{{bigmath|''b t''}}<br/>{{bigmath|''c t''}}<br/>{{bigmath|''d t''}}<br/>{{bigmath|''e t''}}<br/>{{bigmath|''f t''}}<br/>{{bigmath|''g t''}}<br/>…\n |-\n | colspan=2 style=\"font-size:87%; width:10em\" |Left multiplication to&nbsp;{{mvar|s}} and right multiplication to&nbsp;{{mvar|t}}. An abstract notation without any specific sense.\n |}\nIn [[algebra]], the terms '''left''' and '''right''' denote the order of a [[binary operation]] (usually, but not always called \"[[multiplication]]\") in non-[[commutative property|commutative]] [[algebraic structure]]s.\nA binary operation&nbsp;∗ is usually written [[infix notation|in the infix form]]:\n:{{bigmath|''s'' ∗ ''t''}}\nThe [[argument of a function|argument]]&nbsp;{{mvar|s}} is placed on the left side, and the argument&nbsp;{{mvar|t}} is on the right side. Even if the symbol of the operation is omitted, the order of {{mvar|s}} and {{mvar|t}} does matter unless ∗ is commutative.\n\nA '''two-sided''' property is fulfilled on both sides. A '''one-sided''' property is related to one (unspecified) of two sides.\n\nAlthough terms are similar, left–right distinction in algebraic parlance is not related either to [[one-sided limit|left and right limits]] in calculus, or to [[orientation (geometry)|left and right in geometry]].\n\n== Binary operation as an operator ==\nA binary operation&nbsp;{{bigmath|∗}} may be considered as a [[parametric family|family]] of [[unary operation|unary]] [[operator (mathematics)|operators]] through [[currying]]\n:{{bigmath|1=''R''<sub>''t''</sub>(''s'') = ''s'' ∗ ''t''}},\ndepending on&nbsp;{{mvar|t}} as a parameter. It is the family of ''right'' operations. Similarly,\n:{{bigmath|1=''L''<sub>''s''</sub>(''t'') = ''s'' ∗ ''t''}}\ndefines the family of ''left'' operations parametrized with&nbsp;{{mvar|s}}.\n\nIf for some&nbsp;{{mvar|e}}, the left operation&nbsp;{{math|''L''<sub>''e''</sub>}} is [[identity function|identical]], then {{mvar|e}} is called a left [[identity element|identity]]. Similarly, if {{math|1=''R''<sub>''e''</sub> = ''id''}}, then {{mvar|e}} is a right identity.\n\nIn [[ring theory]], a subring which is [[invariant set|invariant]] under ''any'' left multiplication in a ring, is called a left [[ideal (ring theory)|ideal]]. Similarly, a right multiplications-invariant subring is a right ideal.\n\n== Left and right modules ==\nOver [[non-commutative ring]]s, the left–right distinction is applied to [[module (mathematics)|modules]], namely to specify the side where a scalar (module element) appear in the [[scalar multiplication]]. \n{| align=center class=\"wikitable\"\n !Left module\n !Right module\n |- align=center\n |{{bigmath|1=''s''('''x''' + '''y''') = ''s'''''x''' + ''s'''''y'''}}<br/>{{bigmath|1=(''s''<sub>1</sub> + ''s''<sub>2</sub>)'''x''' = ''s''<sub>1</sub>'''x''' + ''s''<sub>2</sub>'''x'''}} <br/>{{bigmath|1=''s''(''t'''''x''') = (''s t'')'''x'''}}\n |{{bigmath|1=('''x''' + '''y''')''t'' = '''x'''''t'' + '''y'''''t''}}<br/> {{bigmath|1='''x'''(''t''<sub>1</sub> + ''t''<sub>2</sub>) = '''x'''''t''<sub>1</sub> + '''x'''''t''<sub>2</sub>}}<br/>{{bigmath|1=('''x'''''s'')''t'' = '''x'''(''s t'')}}\n |}\nThe distinction is not purely syntactical because implies two different associativity rules (the lowest row in the table) which link multiplication in a module with multiplication in a ring.\n\nA [[bimodule]] is simultaneously a left and right module, with two ''different'' scalar multiplication operations, obeying an obvious associativity condition on them.\n\n== Other examples ==\n* [[left eigenvector]]s\n* left and right [[Group action (mathematics)|group action]]s\n\n== In category theory ==\nIn [[category theory]] the usage of \"left\" is \"right\" has some algebraic resemblance, but refers to left and right sides of [[morphism]]s. See [[adjoint functors]].\n\n== See also ==\n* [[Operator associativity]]\n\n== External links ==\n* {{MathWorld|RightIdeal|title=right ideal|author=[[Margherita Barile|Barile, Margherita]]}}\n* {{MathWorld|LeftIdeal|title=left ideal|author=[[Margherita Barile|Barile, Margherita]]}}\n* {{MathWorld|LeftEigenvector|title=left eigenvector}}\n\n[[Category:Abstract algebra]]\n[[Category:Mathematical terminology]]"
    },
    {
      "title": "Light's associativity test",
      "url": "https://en.wikipedia.org/wiki/Light%27s_associativity_test",
      "text": "In [[mathematics]], '''Light's associativity test''' is a procedure invented by F. W. Light for testing whether a [[binary operation]] defined in a [[finite set]] by a [[Cayley table|Cayley multiplication table]] is [[associative]]. The naive procedure for verification of the associativity of a binary operation specified by a Cayley table, which compares the two products that can be formed from each triple of elements, is cumbersome. Light's associativity test simplifies the task in some instances (although it does not improve the worst-case runtime of the naive algorithm, namely <math>\\mathcal{O} \\left( n^{3} \\right)</math> for sets of size <math>n</math>).\n\n==Description of the procedure==\n\nLet a binary operation ' · ' be defined in a finite set ''A'' by a Cayley table. Choosing some element ''a'' in ''A'',  two new binary operations are defined in ''A'' as follows:\n:''x'' <math>\\star</math> ''y'' = ''x'' &middot; ( ''a'' &middot; ''y'' )\n:''x'' <math>\\circ</math> ''y'' = ( ''x'' &middot;  ''a'' ) &middot; ''y''\nThe Cayley tables of these operations are  constructed and compared. If the tables coincide then ''x'' · ( ''a'' · ''y'' ) = ( ''x'' ·  ''a'' ) · ''y'' for all ''x'' and ''y''. This is repeated for every element of the set ''A''.\n\nThe example below illustrates a further simplification in the procedure for the construction and comparison of the Cayley tables of the operations ' <math>\\star</math> ' and ' <math>\\circ</math> '.\n\nIt is not even necessary to construct the Cayley tables of ' <math>\\star</math> ' and ' <math>\\circ</math> ' for ''all'' elements of ''A''. It is enough to compare Cayley tables of ' <math>\\star</math> ' and ' <math>\\circ</math> ' corresponding to the elements in a proper generating subset of ''A''.\n\nWhen the operation ' . ' is [[Commutative property|commutative]], then x <math>\\star</math> y = y <math>\\circ</math> x. As a result, only part of each Cayley table must be computed, because x <math>\\star</math> x = x <math>\\circ</math> x always holds, and x <math>\\star</math> y = x <math>\\circ</math> y implies y <math>\\star</math> x = y <math>\\circ</math> x.\n\nWhen there is an [[identity element]] e, it does not need to be included in the Cayley tables becaue x <math>\\star</math> y = x <math>\\circ</math> y always holds if at least one of x and y are equal to e. \n\n==Example==\n\nConsider the binary operation ' · ' in the set ''A'' = { ''a'', ''b'', ''c'', ''d'', ''e'' } defined by the following Cayley table (Table 1):\n\n<center>\n{| class=\"wikitable\" border=\"1\" width=\"20%\"\n|+Table 1\n|-\n! ·\n! ''a''\n! ''b''\n! ''c''\n! ''d''\n! ''e''\n|-\n| &nbsp; ''' ''a'' '''\n| &nbsp; ''a'' \n| &nbsp; ''a''\n| &nbsp; ''a''\n| &nbsp; ''d''\n| &nbsp; ''d''\n|-\n| &nbsp; ''' ''b'' '''\n| &nbsp; ''a''\n| &nbsp; ''b''\n| &nbsp; ''c''\n| &nbsp; ''d''\n| &nbsp; ''d''\n|-\n| &nbsp; ''' ''c'' '''\n| &nbsp; ''a''\n| &nbsp; ''c''\n| &nbsp; ''b''\n| &nbsp; ''d''\n| &nbsp; ''d''\n|-\n| &nbsp; ''' ''d'' '''\n| &nbsp; ''d''\n| &nbsp; ''d''\n| &nbsp; ''d''\n| &nbsp; ''a''\n| &nbsp; ''a''\n|-\n| &nbsp; ''' ''e'' '''\n| &nbsp; ''d''\n| &nbsp; ''e''\n| &nbsp; ''e''\n| &nbsp; ''a''\n| &nbsp; ''a''\n|}\n</center>\n\nThe set { ''c'', ''e'' } is a generating set for the set ''A'' under the binary operation defined by the above table, for,  ''a'' = ''e'' · ''e'', ''b'' = ''c'' · ''c'', ''d'' = ''c'' · ''e''. Thus it is enough to verify that the binary operations ' <math>\\star</math> ' and ' <math>\\circ</math> ' corresponding to ''c'' coincide and also that  the binary operations ' <math>\\star</math> ' and ' <math>\\circ</math> ' corresponding to ''e'' coincide.\n\nTo verify that the binary operations ' <math>\\star</math> ' and ' <math>\\circ</math> ' corresponding to ''c'' coincide, choose the row in Table 1 corresponding to the element ''' ''c'' ''':\n\n<center>\n{| class=\"wikitable\" border=\"1\" width=\"20%\"\n|+Table 2\n|-\n! ·\n! ''a''\n! ''b''\n! ''c''\n! ''d''\n! ''e''\n|-\n| &nbsp; ''' ''a'' '''\n| &nbsp; ''a'' \n| &nbsp; ''a''\n| &nbsp; ''a''\n| &nbsp; ''d''\n| &nbsp; ''d''\n|-\n| &nbsp; ''' ''b'' '''\n| &nbsp; ''a''\n| &nbsp; ''b''\n| &nbsp; ''c''\n| &nbsp; ''d''\n| &nbsp; ''d''\n|-\n| &nbsp; ''' ''c'' '''\n|style=\"background:#ADFF2F\"| &nbsp; ''a''\n|style=\"background:#ADFF2F\"| &nbsp; ''c''\n|style=\"background:#ADFF2F\"| &nbsp; ''b''\n|style=\"background:#ADFF2F\"| &nbsp; ''d''\n|style=\"background:#ADFF2F\"| &nbsp; ''d''\n|-\n| &nbsp; ''' ''d'' '''\n| &nbsp; ''d''\n| &nbsp; ''d''\n| &nbsp; ''d''\n| &nbsp; ''a''\n| &nbsp; ''a''\n|-\n| &nbsp; ''' ''e'' '''\n| &nbsp; ''d''\n| &nbsp; ''e''\n| &nbsp; ''e''\n| &nbsp; ''a''\n| &nbsp; ''a''\n|}\n</center>\n\nThis row is copied as the header row of a new table (Table 3):\n\n<center>\n{| class=\"wikitable\" border=\"1\" width=\"20%\"\n|+Table 3\n|-\n! &nbsp;&nbsp;&nbsp;\n! style=\"background:#ADFF2F\"| &nbsp; ''a''\n! style=\"background:#ADFF2F\"| &nbsp; ''c''\n! style=\"background:#ADFF2F\"| &nbsp; ''b'' \n! style=\"background:#ADFF2F\"| &nbsp; ''d'' \n! style=\"background:#ADFF2F\"| &nbsp; ''d''\n|-\n| &nbsp;&nbsp;&nbsp;\n|\n|\n|\n|\n|\n|-\n| &nbsp;&nbsp;&nbsp;\n|\n|\n|\n|\n|\n|-\n| &nbsp;&nbsp;&nbsp;\n|\n|\n|\n|\n|\n|-\n| &nbsp;&nbsp;&nbsp;\n|\n|\n|\n|\n|\n|-\n| &nbsp;&nbsp;&nbsp;\n|\n|\n|\n|\n|\n|}\n</center>\n\nUnder the header ''' ''a'' ''' copy the corresponding column in Table 1, under the header ''' ''b'' ''' copy the corresponding column in Table 1, etc., and construct Table 4.\n\n<center>\n{| class=\"wikitable\" border=\"1\" width=\"20%\"\n|+Table 4\n|-\n! &nbsp;&nbsp;&nbsp;\n! style=\"background:#ADFF2F\"| &nbsp; ''a''\n! style=\"background:#ADFF2F\"| &nbsp; ''c''\n! style=\"background:#ADFF2F\"| &nbsp; ''b'' \n! style=\"background:#ADFF2F\"| &nbsp; ''d'' \n! style=\"background:#ADFF2F\"| &nbsp; ''d''\n|-\n| \n| &nbsp; ''a'' \n| &nbsp; ''a''\n| &nbsp; ''a'' \n| &nbsp; ''d''\n| &nbsp; ''d'' \n|-\n| \n| &nbsp; ''a''\n| &nbsp; ''c''\n| &nbsp; ''b'' \n| &nbsp; ''d'' \n| &nbsp; ''d'' \n|-\n| \n| &nbsp; ''a''\n| &nbsp; ''b''\n| &nbsp; ''c''\n| &nbsp; ''d''\n| &nbsp; ''d''\n|-\n| \n| &nbsp; ''d''\n| &nbsp; ''d'' \n| &nbsp; ''d'' \n| &nbsp; ''a'' \n| &nbsp; ''a'' \n|-\n| \n| &nbsp; ''d''\n| &nbsp; ''e'' \n| &nbsp; ''e'' \n| &nbsp; ''a'' \n| &nbsp; ''a'' \n|}\n</center>\n\nThe column headers of Table 4 are now deleted to get Table 5:\n\n<center>\n{| class=\"wikitable\" border=\"1\" width=\"20%\"\n|+Table 5\n|-\n! &nbsp;&nbsp;&nbsp;\n! &nbsp;&nbsp;&nbsp;\n! &nbsp;&nbsp;&nbsp;\n! &nbsp;&nbsp;&nbsp;\n! &nbsp;&nbsp;&nbsp;\n! &nbsp;&nbsp;&nbsp;\n|-\n| \n| &nbsp; ''a'' \n| &nbsp; ''a''\n| &nbsp; ''a'' \n| &nbsp; ''d''\n| &nbsp; ''d'' \n|-\n| \n| &nbsp; ''a''\n| &nbsp; ''c''\n| &nbsp; ''b'' \n| &nbsp; ''d'' \n| &nbsp; ''d'' \n|-\n| \n| &nbsp; ''a''\n| &nbsp; ''b''\n| &nbsp; ''c''\n| &nbsp; ''d''\n| &nbsp; ''d''\n|-\n| \n| &nbsp; ''d''\n| &nbsp; ''d'' \n| &nbsp; ''d'' \n| &nbsp; ''a'' \n| &nbsp; ''a'' \n|-\n| \n| &nbsp; ''d''\n| &nbsp; ''e'' \n| &nbsp; ''e'' \n| &nbsp; ''a'' \n| &nbsp; ''a'' \n|}\n</center>\n\nThe Cayley table of the binary operation  ' <math>\\star</math> ' corresponding to the element ''c'' is given by Table 6.\n\n<center>\n{| class=\"wikitable\" border=\"1\" width=\"20%\"\n|+Table 6\n|-\n! &nbsp;<math>\\star</math> (c)\n! &nbsp; ''a''\n! &nbsp; ''b''\n! &nbsp; ''c'' \n! &nbsp; ''d'' \n! &nbsp; ''e''\n|-\n| &nbsp; ''' ''a'' '''\n| &nbsp; ''a'' \n| &nbsp; ''a''\n| &nbsp; ''a'' \n| &nbsp; ''d''\n| &nbsp; ''d'' \n|-\n| &nbsp; ''' ''b'' ''' \n| &nbsp; ''a''\n| &nbsp; ''c''\n| &nbsp; ''b'' \n| &nbsp; ''d'' \n| &nbsp; ''d'' \n|-\n| &nbsp; ''' ''c'' ''' \n| &nbsp; ''a''\n| &nbsp; ''b''\n| &nbsp; ''c''\n| &nbsp; ''d''\n| &nbsp; ''d''\n|-\n| &nbsp; ''' ''d'' ''' \n| &nbsp; ''d''\n| &nbsp; ''d'' \n| &nbsp; ''d'' \n| &nbsp; ''a'' \n| &nbsp; ''a'' \n|-\n| &nbsp; ''' ''e'' ''' \n| &nbsp; ''d''\n| &nbsp; ''e'' \n| &nbsp; ''e'' \n| &nbsp; ''a'' \n| &nbsp; ''a'' \n|}\n</center>\n\nNext choose the ''' ''c'' ''' column of Table 1:\n\n<center>\n{| class=\"wikitable\" border=\"1\" width=\"20%\"\n|+Table 7\n|-\n! ·\n! ''a''\n! ''b''\n! ''c''\n! ''d''\n! ''e''\n|-\n| &nbsp; ''' ''a'' '''\n| &nbsp; ''a'' \n| &nbsp; ''a''\n|style=\"background:#87CEEB\"| &nbsp; ''a''\n| &nbsp; ''d''\n| &nbsp; ''d''\n|-\n| &nbsp; ''' ''b'' '''\n| &nbsp; ''a''\n| &nbsp; ''b''\n|style=\"background:#87CEEB\"| &nbsp; ''c''\n| &nbsp; ''d''\n| &nbsp; ''d''\n|-\n| &nbsp; ''' ''c'' '''\n| &nbsp; ''a''\n| &nbsp; ''c''\n|style=\"background:#87CEEB\"| &nbsp; ''b''\n| &nbsp; ''d''\n| &nbsp; ''d''\n|-\n| &nbsp; ''' ''d'' '''\n| &nbsp; ''d''\n| &nbsp; ''d''\n|style=\"background:#87CEEB\"| &nbsp; ''d''\n| &nbsp; ''a''\n| &nbsp; ''a''\n|-\n| &nbsp; ''' ''e'' '''\n| &nbsp; ''d''\n| &nbsp; ''e''\n|style=\"background:#87CEEB\"| &nbsp; ''e''\n| &nbsp; ''a''\n| &nbsp; ''a''\n|}\n</center>\n\nCopy this column to the index column to get Table 8:\n\n<center>\n{| class=\"wikitable\" border=\"1\" width=\"20%\"\n|+Table 8\n|-\n! &nbsp;&nbsp;&nbsp;\n! &nbsp;&nbsp;&nbsp;\n! &nbsp;&nbsp;&nbsp;\n! &nbsp;&nbsp;&nbsp;\n! &nbsp;&nbsp;&nbsp;\n! &nbsp;&nbsp;&nbsp;\n|-\n|style=\"background:#87CEEB\"| &nbsp;''' ''a'' '''\n| \n| \n| \n| \n| \n|-\n|style=\"background:#87CEEB\"| &nbsp;''' ''c'' '''\n| \n| \n| \n| \n| \n|-\n|style=\"background:#87CEEB\"| &nbsp;''' ''b'' '''\n|\n|\n|\n|\n|\n|-\n|style=\"background:#87CEEB\"| &nbsp;''' ''d'' '''\n|\n|\n|\n|\n|\n|-\n|style=\"background:#87CEEB\"| &nbsp;''' ''e'' '''\n|\n|\n|\n|\n|\n|}\n</center>\n\nAgainst the index entry ''' ''a'' ''' in Table 8 copy the corresponding row in Table 1, against  the index entry ''' ''b'' ''' copy the corresponding row in Table 1, etc., and construct Table 9.\n\n<center>\n{| class=\"wikitable\" border=\"1\" width=\"20%\"\n|+Table 9\n|-\n! &nbsp;&nbsp;&nbsp;\n! &nbsp;&nbsp;&nbsp;\n! &nbsp;&nbsp;&nbsp;\n! &nbsp;&nbsp;&nbsp;\n! &nbsp;&nbsp;&nbsp;\n! &nbsp;&nbsp;&nbsp;\n|-\n| style=\"background:#87CEEB\"| &nbsp; ''' ''a'' '''\n| &nbsp; ''a'' \n| &nbsp; ''a''\n| &nbsp; ''a'' \n| &nbsp; ''d''\n| &nbsp; ''d'' \n|-\n| style=\"background:#87CEEB\"|&nbsp; ''' ''c'' ''' \n| &nbsp; ''a''\n| &nbsp; ''c''\n| &nbsp; ''b'' \n| &nbsp; ''d'' \n| &nbsp; ''d'' \n|-\n| style=\"background:#87CEEB\"|&nbsp; ''' ''b'' ''' \n| &nbsp; ''a''\n| &nbsp; ''b''\n| &nbsp; ''c''\n| &nbsp; ''d''\n| &nbsp; ''d''\n|-\n| style=\"background:#87CEEB\"|&nbsp; ''' ''d'' ''' \n| &nbsp; ''d''\n| &nbsp; ''d'' \n| &nbsp; ''d'' \n| &nbsp; ''a'' \n| &nbsp; ''a'' \n|-\n| style=\"background:#87CEEB\"|&nbsp; ''' ''e'' ''' \n| &nbsp; ''d''\n| &nbsp; ''e'' \n| &nbsp; ''e'' \n| &nbsp; ''a'' \n| &nbsp; ''a'' \n|}\n</center>\n\nThe index entries in the first column of Table 9 are now deleted to get Table 10:\n\n<center>\n{| class=\"wikitable\" border=\"1\" width=\"20%\"\n|+Table 10\n|-\n! &nbsp;&nbsp;&nbsp;\n! &nbsp;&nbsp;&nbsp;\n! &nbsp;&nbsp;&nbsp;\n! &nbsp;&nbsp;&nbsp;\n! &nbsp;&nbsp;&nbsp;\n! &nbsp;&nbsp;&nbsp;\n|-\n|&nbsp;&nbsp;&nbsp;\n| &nbsp; ''a'' \n| &nbsp; ''a''\n| &nbsp; ''a'' \n| &nbsp; ''d''\n| &nbsp; ''d'' \n|-\n| &nbsp;&nbsp;&nbsp;\n| &nbsp; ''a''\n| &nbsp; ''c''\n| &nbsp; ''b'' \n| &nbsp; ''d'' \n| &nbsp; ''d'' \n|-\n| &nbsp;&nbsp;&nbsp;\n| &nbsp; ''a''\n| &nbsp; ''b''\n| &nbsp; ''c''\n| &nbsp; ''d''\n| &nbsp; ''d''\n|-\n| &nbsp;&nbsp;&nbsp;\n| &nbsp; ''d''\n| &nbsp; ''d'' \n| &nbsp; ''d'' \n| &nbsp; ''a'' \n| &nbsp; ''a'' \n|-\n| &nbsp;&nbsp;&nbsp;\n| &nbsp; ''d''\n| &nbsp; ''e'' \n| &nbsp; ''e'' \n| &nbsp; ''a'' \n| &nbsp; ''a'' \n|}\n</center>\n\nThe Cayley table of the binary operation  ' <math>\\circ</math> ' corresponding to the element ''c'' is given by Table 11.\n\n<center>\n{| class=\"wikitable\" border=\"1\" width=\"20%\"\n|+Table 11\n|-\n! <math>\\circ</math>(c)\n! &nbsp; ''a'' \n! &nbsp; ''b'' \n! &nbsp; ''c'' \n! &nbsp; ''d'' \n! &nbsp; ''e'' \n|-\n| &nbsp; ''' ''a'' '''\n| &nbsp; ''a'' \n| &nbsp; ''a''\n| &nbsp; ''a'' \n| &nbsp; ''d''\n| &nbsp; ''d'' \n|-\n| &nbsp; ''' ''b'' '''\n| &nbsp; ''a''\n| &nbsp; ''c''\n| &nbsp; ''b'' \n| &nbsp; ''d'' \n| &nbsp; ''d'' \n|-\n| &nbsp; ''' ''c'' '''\n| &nbsp; ''a''\n| &nbsp; ''b''\n| &nbsp; ''c''\n| &nbsp; ''d''\n| &nbsp; ''d''\n|-\n| &nbsp; ''' ''d'' '''\n| &nbsp; ''d''\n| &nbsp; ''d'' \n| &nbsp; ''d'' \n| &nbsp; ''a'' \n| &nbsp; ''a'' \n|-\n| &nbsp; ''' ''e'' '''\n| &nbsp; ''d''\n| &nbsp; ''e'' \n| &nbsp; ''e'' \n| &nbsp; ''a'' \n| &nbsp; ''a'' \n|}\n</center>\n\nOne can verify that the entries in the various cells in Table 6 agrees with the entries in the corresponding cells of Table 11. This shows that ''x'' · ( ''c'' · ''y'' ) = ( ''x'' · ''c'' ) · ''y'' for all ''x'' and ''y'' in ''A''. If there were some discrepancy then it would not be true that ''x'' · ( ''c'' · ''y'' ) = ( ''x'' · ''c'' ) · ''y'' for all ''x'' and ''y'' in ''A''.\n\nThat ''x'' · ( ''e'' · ''y'' ) = ( ''x'' · ''e'' ) · ''y'' for all ''x'' and ''y'' in ''A'' can be verified in a similar way by constructing the following tables (Table 12 and Table 13):\n\n<center>\n{| class=\"wikitable\" border=\"1\" width=\"20%\"\n|+Table 12\n|-\n! &nbsp;<math>\\star</math>(''e'')\n! style=\"background:#ADFF2F\"| &nbsp; ''a''\n! style=\"background:#ADFF2F\"| &nbsp; ''b''\n! style=\"background:#ADFF2F\"| &nbsp; ''c'' \n! style=\"background:#ADFF2F\"| &nbsp; ''d'' \n! style=\"background:#ADFF2F\"| &nbsp; ''e''\n|-\n|style=\"background:#87CEEB\"| &nbsp; ''' ''a'' ''' \n|&nbsp; ''d''\n|&nbsp; ''d''\n|&nbsp; ''d''\n|&nbsp; ''a''\n|&nbsp; ''a''\n|-\n|style=\"background:#87CEEB\"| &nbsp; ''' ''b'' ''' \n|&nbsp; ''d''\n|&nbsp; ''d''\n|&nbsp; ''d''\n|&nbsp; ''a''\n|&nbsp; ''a''\n|-\n|style=\"background:#87CEEB\"| &nbsp; ''' ''c'' ''' \n|&nbsp; ''d''\n|&nbsp; ''d''\n|&nbsp; ''d''\n|&nbsp; ''a''\n|&nbsp; ''a''\n|-\n|style=\"background:#87CEEB\"| &nbsp; ''' ''d'' ''' \n|&nbsp; ''a''\n|&nbsp; ''a''\n|&nbsp; ''a''\n|&nbsp; ''d''\n|&nbsp; ''d''\n|-\n|style=\"background:#87CEEB\"| &nbsp; ''' ''e'' ''' \n|&nbsp; ''a''\n|&nbsp; ''a''\n|&nbsp; ''a''\n|&nbsp; ''d''\n|&nbsp; ''d''\n|}\n</center>\n\n<center>\n{| class=\"wikitable\" border=\"1\" width=\"20%\"\n|+Table 13\n|-\n! &nbsp;<math>\\circ</math>(''e'')\n! style=\"background:#ADFF2F\"| &nbsp; ''a''\n! style=\"background:#ADFF2F\"| &nbsp; ''b''\n! style=\"background:#ADFF2F\"| &nbsp; ''c'' \n! style=\"background:#ADFF2F\"| &nbsp; ''d'' \n! style=\"background:#ADFF2F\"| &nbsp; ''e''\n|-\n|style=\"background:#87CEEB\"| &nbsp; ''' ''a'' ''' \n|&nbsp; ''d''\n|&nbsp; ''d''\n|&nbsp; ''d''\n|&nbsp; ''a''\n|&nbsp; ''a''\n|-\n|style=\"background:#87CEEB\"| &nbsp; ''' ''b'' ''' \n|&nbsp; ''d''\n|&nbsp; ''d''\n|&nbsp; ''d''\n|&nbsp; ''a''\n|&nbsp; ''a''\n|-\n|style=\"background:#87CEEB\"| &nbsp; ''' ''c'' ''' \n|&nbsp; ''d''\n|&nbsp; ''d''\n|&nbsp; ''d''\n|&nbsp; ''a''\n|&nbsp; ''a''\n|-\n|style=\"background:#87CEEB\"| &nbsp; ''' ''d'' ''' \n|&nbsp; ''a''\n|&nbsp; ''a''\n|&nbsp; ''a''\n|&nbsp; ''d''\n|&nbsp; ''d''\n|-\n|style=\"background:#87CEEB\"| &nbsp; ''' ''e'' ''' \n|&nbsp; ''a''\n|&nbsp; ''a''\n|&nbsp; ''a''\n|&nbsp; ''d''\n|&nbsp; ''d''\n|}\n</center>\n\n===A further simplification ===\n\nIt is not necessary to construct the Cayley tables (Table 6 and table 11) of the binary operations ' <math>\\star</math> ' and ' <math>\\circ</math> '.  It is enough to copy the column corresponding to the header ''' ''c'' '''  in Table 1 to the index column in Table 5 and form the following table (Table 14) and verify that the ''' ''a'' '''-row of Table 14 is identical with the ''' ''a'' '''-row of Table 1,  the ''' ''b'' '''-row of Table 14 is identical with the ''' ''b'' '''-row of Table 1, etc. This is to be repeated ''mutatis mutandis'' for all the elements of the generating set of ''A''.\n\n<center>\n{| class=\"wikitable\" border=\"1\" width=\"20%\"\n|+Table 14\n|-\n! &nbsp;&nbsp;&nbsp;\n! style=\"background:#ADFF2F\"| &nbsp; ''a''\n! style=\"background:#ADFF2F\"| &nbsp; ''c''\n! style=\"background:#ADFF2F\"| &nbsp; ''b'' \n! style=\"background:#ADFF2F\"| &nbsp; ''d'' \n! style=\"background:#ADFF2F\"| &nbsp; ''d''\n|-\n| style=\"background:#87CEEB\"| &nbsp; ''' ''a'' '''\n| &nbsp; ''a'' \n| &nbsp; ''a''\n| &nbsp; ''a'' \n| &nbsp; ''d''\n| &nbsp; ''d'' \n|-\n| style=\"background:#87CEEB\"| &nbsp; ''' ''c'' '''\n| &nbsp; ''a''\n| &nbsp; ''c''\n| &nbsp; ''b'' \n| &nbsp; ''d'' \n| &nbsp; ''d'' \n|-\n| style=\"background:#87CEEB\"| &nbsp; ''' ''b'' '''\n| &nbsp; ''a''\n| &nbsp; ''b''\n| &nbsp; ''c''\n| &nbsp; ''d''\n| &nbsp; ''d''\n|-\n| style=\"background:#87CEEB\"| &nbsp; ''' ''d'' '''\n| &nbsp; ''d''\n| &nbsp; ''d'' \n| &nbsp; ''d'' \n| &nbsp; ''a'' \n| &nbsp; ''a'' \n|-\n| style=\"background:#87CEEB\"| &nbsp; ''' ''e'' '''\n| &nbsp; ''d''\n| &nbsp; ''e'' \n| &nbsp; ''e'' \n| &nbsp; ''a'' \n| &nbsp; ''a'' \n|}\n</center>\n\n==Program==\n[[Computer software]] can be written to carry out Light's associativity test. Kehayopulu and Argyris have developed such a program for [[Mathematica]].<ref>{{cite journal|last=Kehayopulu|first=Niovi|author2=Philip Argyris |year=1993|title=An algorithm for Light's associativity test using Mathematica|journal=J. Comput. Inform.|volume=3|issue=1|pages=87&ndash;98|issn=1180-3886}}</ref>\n\n==Extension==\n\nLight's associativity test can be extended to test associativity in a more general context.<ref>{{cite journal|doi=10.2307/2314731|last=Bednarek|first=A R|year=1968|title=An extension of Light's associativity test|journal=[[American Mathematical Monthly]]|volume=75|issue=5|pages=531&ndash;532|jstor=2314731}}</ref><ref>{{cite journal|doi=10.1007/BF02572966|last=Kalman|first=J A|year=1971|title=Bednarek's extension of Light's associativity test|journal=[[Semigroup Forum]]|volume=3|issue=1|pages=275–276}}</ref>\n\nLet ''T'' = { ''t''<sub>1</sub>, ''t''<sub>2</sub>, <math>\\ldots</math>, ''t''<sub>''m''</sub> } be a [[Magma (algebra)|magma]] in which the operation is denoted by [[Juxtaposition (literary)|juxtaposition]]. Let ''X'' = { ''x''<sub>1</sub>, ''x''<sub>2</sub>, <math>\\ldots</math>, ''x''<sub>''n''</sub> } be a set. Let there be a mapping from the [[Cartesian product]] ''T'' &times; ''X'' to ''X'' denoted by  (''t'', ''x'') {{mapsto}} ''tx'' and let it be required to test whether this map has the property\n\n:(''st'')''x'' = ''s''(''tx'')  for all ''s'', ''t'' in ''T''  and all ''x'' in ''X''.\n\nA generalization of Light's associativity test can be applied to verify whether the above property holds or not. In mathematical notations, the generalization runs as follows: For each ''t'' in ''T'', let ''L''(''t'') be the ''m'' &times; ''n'' matrix of elements of ''X'' whose ''i'' - th row is\n\n:( (''t''<sub>''i''</sub>''t'')''x''<sub>1</sub>, (''t''<sub>''i''</sub>''t'')''x''<sub>2</sub>, <math>\\ldots</math> , (''t''<sub>''i''</sub>''t'')''x''<sub>n</sub> )  for ''i'' = 1, <math>\\ldots</math>, ''m''\n\nand let ''R''(''t'') be the ''m'' &times; ''n'' matrix of elements of ''X'', the elements of whose ''j'' - th column are\n\n:( ''t''<sub>1</sub>(''tx''<sub>''j''</sub>),  ''t''<sub>2</sub>(''tx''<sub>''j''</sub>), <math>\\ldots</math> ,  ''t''<sub>''m''</sub>(''tx''<sub>j</sub>) )  for ''j'' = 1, <math>\\ldots</math>, ''n''.\n\nAccording to the generalised test (due to Bednarek),  that the property to be verified holds if and only if ''L''(''t'') = ''R''(''t'') for all ''t'' in ''T''. When ''X'' = ''T'', Bednarek's test reduces to Light's test.\n\n==More advanced algorithms==\nThere is a randomized algorithm by Rajagopalan and [[Leonard Schulman|Schulman]] to test associativity in time proportional to the input size. (The method also works for testing certain other identities.) Specifically, the runtime is <math>O(n^2 \\log \\frac1\\delta)</math> for an <math>n\\times n</math> table and error probability <math>\\delta</math>. \nThe algorithm can be modified to produce a triple <math>\\langle a,b,c\\rangle</math> for which <math>(ab)c\\ne a(bc)</math>, if there is one, in time <math>O(n^2 \\log n \\cdot\\log \\frac1\\delta)</math>.<ref>{{cite journal | doi = 10.1137/S0097539797325387 | volume=29 | issue=4 | title=Verification of Identities | journal=SIAM Journal on Computing | pages=1155–1163| year=2000 | last1=Rajagopalan | first1=Sridhar | last2=Schulman | first2=Leonard J. | citeseerx=10.1.1.4.6898 }}</ref>\n\n==Notes==\n{{reflist}}\n\n==References==\n* {{Cite book | last1=Clifford | first1=Alfred Hoblitzelle | author1-link=Alfred H. Clifford | last2=Preston | first2=Gordon Bamford | author2-link=Gordon Preston | title=The algebraic theory of semigroups. Vol. I | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=Mathematical Surveys, No. 7 | isbn=978-0-8218-0272-4 |mr=0132791 | year=1961}} (pp.&nbsp;7&ndash;9)\n\n[[Category:Abstract algebra]]\n[[Category:Semigroup theory]]\n[[Category:Binary operations]]\n[[Category:Elementary algebra]]"
    },
    {
      "title": "Linear independence",
      "url": "https://en.wikipedia.org/wiki/Linear_independence",
      "text": "<!--{{technical|date=April 2014}}-->\n{{For|linear dependence of random variables|Covariance}}{{More citations needed|date=January 2019}}[[File:Vec-indep.png|thumb|right|Linearly independent vectors in <math>\\R^3</math>]]\n[[File:Vec-dep.png|thumb|right|Linearly dependent vectors in a plane in <math>\\R^3</math>.]]\n\nIn the theory of [[vector space]]s,  a [[set (mathematics)|set]] of [[vector (mathematics)|vector]]s is said to be '''{{visible anchor|linearly dependent}}''' if at least one of the vectors in the set can be defined as a [[linear combination]] of the others;  if no vector in the set can be written in this way, then the vectors are said to be '''{{visible anchor|linearly independent}}'''. These concepts are central to the definition of [[Dimension (vector space)|dimension]].<ref>G. E. Shilov, ''Linear Algebra'' (Trans. R. A. Silverman), Dover Publications, New York, 1977.</ref>\n<!-- these distinctions are not useful\n* An [[indexed family]] of [[vector space|vector]]s is a '''linearly independent family''' if none of them can be written as a [[linear combination]] of finitely many other vectors in the family. A family of vectors which is not linearly independent is called '''linearly dependent'''.\n* A [[set (mathematics)|set]] of vectors is a '''linearly independent set''' if the set (regarded as a family indexed by itself) is a linearly independent family.\n\nThese two notions are not equivalent: the difference being that in a family we allow repeated elements, while in a set we do not.  For example if <math>V</math> is a vector space, then the family <math>F : \\{1,2\\}\\to V</math> such that <math>f(1)=v</math> and <math>f(2)=v</math> is a ''linearly dependent family'', but the singleton set of the images of that family is <math>\\{v\\}</math> which is a ''linearly independent set''.\n\nBoth notions are important and used in common, and sometimes even confused in the literature.\n-->\n<!-- this too early\nFor instance, in the [[3 dimensional space|three-dimensional]] [[real vector space]] <math>\\R^3</math> we have the following example:\n\n:<math>\n\\begin{matrix}\n\\mbox{independent}\\qquad\\\\\n\\underbrace{\n  \\overbrace{\n    \\begin{bmatrix}0\\\\0\\\\1\\end{bmatrix},\n    \\begin{bmatrix}0\\\\2\\\\-2\\end{bmatrix},\n    \\begin{bmatrix}1\\\\-2\\\\1\\end{bmatrix}\n  },\n  \\begin{bmatrix}4\\\\2\\\\3\\end{bmatrix}\n}\\\\\n\\mbox{dependent}\\\\\n\\end{matrix}\n</math>--><!-- weights 9, 5, 4 \nHere the first three vectors are linearly independent; but the fourth vector equals 9 times the first plus 5 times the second plus 4 times the third, so the four vectors together are linearly dependent. Linear dependence is a property of the set of vectors, not of any particular vector.  For example in this case we could just as well write the first vector as a linear combination of the last three.\n:<math>\\mathbf{v}_1=\\left(-\\frac{5}{9}\\right)\\mathbf{v}_2+\\left(-\\frac{4}{9}\\right)\\mathbf{v}_3+\\frac{1}{9}\\mathbf{v}_4 .</math>\n-->\n<!-- In [[probability theory]] and [[statistics]] there is an unrelated measure of linear dependence between [[random variable]]s. -->\n\nA vector space can be of [[finite-dimension]] or [[dimension (vector space)|infinite-dimension]] depending on the number of linearly independent [[basis vectors]].  The definition of linear dependence and the ability to determine whether a subset of vectors in a vector space is linearly dependent are central to determining a [[Basis (linear algebra)|basis]] for a vector space.\n\n== Definition ==\nThe vectors in a subset <math>S=\\{\\vec v_1,\\vec v_2,\\dots,\\vec v_k\\}</math> of a [[vector space]] ''V'' are said to be ''linearly dependent'', if there exist scalars <math>a_1,a_2,\\dots,a_k</math> , not all zero, such that\n:<math>a_1\\vec v_1+a_2\\vec v_2+\\cdots+a_k\\vec v_k= \\vec 0,</math>\nwhere <math>\\vec 0</math> denotes the zero vector.\n\nNotice that if not all of the scalars are zero, then at least one is non-zero, say <math>a_1</math>, in which case this equation can be written in the form\n:<math>\\vec v_1=\\frac{-a_2}{a_1}\\vec v_2+\\cdots+\\frac{-a_k}{a_1}\\vec v_k.</math>\nThus, <math>\\vec v_1</math> is shown to be a linear combination of the remaining vectors.\n<!-- this is not helpful\nFor any vectors <math>\\vec u_1,\\vec u_2,\\dots,\\vec u_n,</math> we have that\n:<math>0 u_1+0 u_2+\\cdots+0 u_n=0,</math>\nThis is called the [[Triviality (mathematics)|trivial]] representation of 0 as a linear combination of <math>\\vec u_1,\\vec u_2,\\dots,\\vec u_n,</math> , this motivates a very simple definition of both linear independence and linear dependence, for a set to be linearly dependent, there must exist a non-trivial representation of 0 as a linear combination of vectors in the set.\n-->\n\nThe vectors in a set <math>T=\\{\\vec v_1,\\vec v_2,\\dots,\\vec v_n\\}</math> are said to be ''linearly independent'' if the equation\n:<math>a_1\\vec v_1+a_2\\vec v_2+\\cdots+a_n\\vec v_n= \\vec 0,</math>\ncan only be satisfied by <math>a_i=0</math> for <math>i=1,\\dots,n</math>. This implies that no vector in the set can be represented as a linear combination of the remaining vectors in the set.  In other words, a set of vectors is linearly independent if the only representations of <math>\\vec 0</math> as a linear combination of its vectors is the trivial representation in which all the scalars <math>a_i</math> are zero.<ref>{{cite book|last=Friedberg, Insel, Spence|first=Stephen, Arnold, Lawrence|title=Linear Algebra|publisher=Pearson, 4th Edition|isbn=0130084514|pages=48–49}}</ref>\n\nThe alternate definition, that a set of vectors is linearly dependent if and only if some vector in that set can be written as a linear combination of the other vectors, is only useful when the set contains two or more vectors. When the set contains zero or one vector, the original definition is used.\n\n===Infinite dimensions===\nIn order to allow the number of linearly independent vectors in a vector space to be [[countably infinite]], it is useful to define linear dependence as follows. More generally, let ''V'' be a vector space over a [[field (mathematics)|field]] ''K'', and let {''v''<sub>''i''</sub> | ''i''∈''I''} be a [[indexed family|family]] of elements of ''V''. The family is ''linearly dependent'' over ''K'' if there exists a ''finite'' family {''a''<sub>''j''</sub>&nbsp;|&nbsp;''j''&nbsp;∈&nbsp;''J''} of elements of ''K'', all non-zero, such that\n:<math> \\sum_{j \\in J} a_j v_j = 0. </math>\n\nA set ''X'' of elements of ''V'' is ''linearly independent'' if the corresponding family {''x''}<sub>'''x'''∈''X''</sub> is linearly independent.  Equivalently, a family is dependent if a member is in the closure of the [[linear span]] of the rest of the family, i.e., a member is a [[linear combination]] of the rest of the family.  The trivial case of the empty family must be regarded as linearly independent for theorems to apply.\n\nA set of vectors which is linearly independent and [[linear span|spans]] some vector space, forms a [[basis (linear algebra)|basis]] for that vector space. For example, the vector space of all polynomials in ''x'' over the reals has the (infinite) subset {1, ''x'', ''x''<sup>2</sup>, ...} as a basis.\n\n== Geometric meaning ==\n\nA geographic example may help to clarify the concept of linear independence.  A person describing the location of a certain place might say, \"It is 3 miles north and 4 miles east of here.\"  This is sufficient information to describe the location, because the geographic coordinate system may be considered as a 2-dimensional vector space (ignoring altitude and the curvature of the Earth's surface).  The person might add, \"The place is 5 miles northeast of here.\"  Although this last statement is ''true'', it is not necessary.\n\nIn this example the \"3 miles north\" vector and the \"4 miles east\" vector are linearly independent.  That is to say, the north vector cannot be described in terms of the east vector, and vice versa.  The third \"5 miles northeast\" vector is a [[linear combination]] of the other two vectors, and it makes the set of vectors ''linearly dependent'', that is, one of the three vectors is unnecessary.\n\nAlso note that if altitude is not ignored, it becomes necessary to add a third vector to the linearly independent set.  In general, ''n'' linearly independent vectors are required to describe all locations in ''n''-dimensional space.\n\n== Evaluating linear independence ==\n\n===Vectors in R<sup>2</sup>===\n'''Three vectors:'''  Consider the set of vectors ''v''<sub>1</sub> = (1, 1), ''v''<sub>2</sub> = (−3, 2) and ''v''<sub>3</sub> = (2, 4), then the condition for linear dependence seeks a set of non-zero scalars, such that\n::<math> a_1 \\begin{Bmatrix} 1\\\\1\\end{Bmatrix} + a_2 \\begin{Bmatrix} -3\\\\2\\end{Bmatrix} + a_3 \\begin{Bmatrix} 2\\\\4\\end{Bmatrix} =\\begin{Bmatrix} 0\\\\0\\end{Bmatrix},</math>\nor\n::<math> \\begin{bmatrix} 1 & -3 & 2 \\\\ 1 & 2 & 4 \\end{bmatrix}\\begin{Bmatrix} a_1\\\\ a_2 \\\\ a_3 \\end{Bmatrix}= \\begin{Bmatrix} 0\\\\0\\end{Bmatrix}.</math>\n\n[[Row reduction|Row reduce]] this matrix equation by subtracting the first row from the second to obtain,\n::<math> \\begin{bmatrix} 1 & -3 & 2 \\\\ 0 & 5 & 2 \\end{bmatrix}\\begin{Bmatrix} a_1\\\\ a_2 \\\\ a_3 \\end{Bmatrix}= \\begin{Bmatrix} 0\\\\0\\end{Bmatrix}.</math>\nContinue the row reduction by (i) dividing the second row by 5, and then (ii) multiplying by 3 and adding to the first row, that is\n::<math> \\begin{bmatrix} 1 & 0 & 16/5 \\\\ 0 & 1 & 2/5 \\end{bmatrix}\\begin{Bmatrix} a_1\\\\ a_2 \\\\ a_3 \\end{Bmatrix}= \\begin{Bmatrix} 0\\\\0\\end{Bmatrix}.</math>\n\nWe can now rearrange this equation to obtain \n::<math> \\begin{bmatrix} 1 & 0  \\\\ 0 & 1 \\end{bmatrix}\\begin{Bmatrix} a_1\\\\ a_2 \\end{Bmatrix}= \\begin{Bmatrix} a_1\\\\ a_2 \\end{Bmatrix}=-a_3\\begin{Bmatrix} 16/5\\\\2/5\\end{Bmatrix}.</math>\nwhich shows that non-zero ''a''<sub>''i''</sub> exist such that ''v''<sub>3</sub> = (2, 4) can be defined in terms of ''v''<sub>1</sub> = (1, 1), ''v''<sub>2</sub> = (−3, 2).  Thus, the three vectors are linearly dependent.\n\n'''Two vectors:'''  Now consider the linear dependence of the two vectors ''v''<sub>1</sub> = (1, 1), ''v''<sub>2</sub> = (−3, 2), and check, \n::<math> a_1 \\begin{Bmatrix} 1\\\\1\\end{Bmatrix} + a_2 \\begin{Bmatrix} -3\\\\2\\end{Bmatrix}  =\\begin{Bmatrix} 0\\\\0\\end{Bmatrix},</math>\nor\n::<math> \\begin{bmatrix} 1 & -3  \\\\ 1 & 2  \\end{bmatrix}\\begin{Bmatrix} a_1\\\\ a_2 \\end{Bmatrix}= \\begin{Bmatrix} 0\\\\0\\end{Bmatrix}.</math>\n\nThe same row reduction presented above yields,\n::<math> \\begin{bmatrix} 1 & 0  \\\\ 0 & 1 \\end{bmatrix}\\begin{Bmatrix} a_1\\\\ a_2 \\end{Bmatrix}= \\begin{Bmatrix} 0\\\\0\\end{Bmatrix}.</math>\nThis shows that ''a''<sub>i</sub> = 0, which means that the vectors ''v''<sub>1</sub> = (1, 1) and ''v''<sub>2</sub> = (−3, 2) are linearly independent.\n\n===Vectors in R<sup>4</sup>===\nIn order to determine if the three vectors in '''R'''<sup>4</sup>,\n::<math> \\mathbf{v}_1= \\begin{Bmatrix}1\\\\4\\\\2\\\\-3\\end{Bmatrix},  \\mathbf{v}_2=\\begin{Bmatrix}7\\\\10\\\\-4\\\\-1\\end{Bmatrix}, \\mathbf{v}_3=\\begin{Bmatrix}-2\\\\1\\\\5\\\\-4\\end{Bmatrix}. </math>\nare linearly dependent, form the matrix equation,\n\n::<math>\\begin{bmatrix}1&7&-2\\\\4& 10& 1\\\\2&-4&5\\\\-3&-1&-4\\end{bmatrix}\\begin{Bmatrix} a_1\\\\ a_2 \\\\ a_3 \\end{Bmatrix} =  \\begin{Bmatrix}0\\\\0\\\\0\\\\0\\end{Bmatrix}.</math>\n\nRow reduce this equation to obtain,\n::<math> \\begin{bmatrix} 1& 7 & -2 \\\\ 0& -18& 9\\\\ 0 & 0 & 0\\\\ 0& 0& 0\\end{bmatrix} \\begin{Bmatrix} a_1\\\\ a_2 \\\\ a_3 \\end{Bmatrix} =  \\begin{Bmatrix}0\\\\0\\\\0\\\\0\\end{Bmatrix}.</math>\nRearrange to solve for v<sub>3</sub> and obtain,\n::<math> \\begin{bmatrix} 1& 7 \\\\ 0& -18& \\end{bmatrix} \\begin{Bmatrix} a_1\\\\ a_2  \\end{Bmatrix} =  -a_3\\begin{Bmatrix}-2\\\\9\\end{Bmatrix}.</math>\nThis equation is easily solved to define non-zero ''a''<sub>i</sub>,\n::<math> a_1 = -3 a_3 /2,  a_2 = a_3/2,\n</math>\nwhere ''a''<sub>3</sub> can be chosen arbitrarily.  Thus, the vectors ''v''<sub>1</sub>, ''v''<sub>2</sub> and ''v''<sub>3</sub> are linearly dependent.\n\n=== Alternative method using determinants ===\n\nAn alternative method relies on the fact that ''n'' vectors in <math>\\mathbb{R}^n</math> are linearly '''independent''' [[if and only if]] the [[determinant]] of the [[matrix (mathematics)|matrix]] formed by taking the vectors as its columns is non-zero.\n\nIn this case, the matrix formed by the vectors is\n:<math>A = \\begin{bmatrix}1&-3\\\\1&2\\end{bmatrix} . </math>\nWe may write a linear combination of the columns as\n:<math> A \\Lambda = \\begin{bmatrix}1&-3\\\\1&2\\end{bmatrix} \\begin{bmatrix}\\lambda_1 \\\\ \\lambda_2 \\end{bmatrix} . </math>\nWe are interested in whether ''A''Λ&nbsp;= '''0''' for some nonzero vector Λ. This depends on the determinant of ''A'', which is\n:<math> \\det A = 1\\cdot2 - 1\\cdot(-3) = 5 \\ne 0 . </math>\nSince the [[determinant]] is non-zero, the vectors (1, 1) and (&minus;3, 2) are linearly independent.\n\nOtherwise, suppose we have ''m'' vectors of ''n'' coordinates, with ''m''&nbsp;&lt;&nbsp;''n''. Then ''A'' is an ''n''×''m'' matrix and Λ is a column vector with ''m'' entries, and we are again interested in ''A''Λ&nbsp;= '''0'''. As we saw previously, this is equivalent to a list of ''n'' equations. Consider the first ''m'' rows of ''A'', the first ''m'' equations; any solution of the full list of equations must also be true of the reduced list. In fact, if 〈''i''<sub>1</sub>,...,''i''<sub>''m''</sub>〉 is any list of ''m'' rows, then the equation must be true for those rows.\n:<math> A_{{\\lang i_1,\\dots,i_m} \\rang} \\Lambda = \\mathbf{0} . </math>\nFurthermore, the reverse is true. That is, we can test whether the ''m'' vectors are linearly dependent by testing whether\n:<math> \\det A_{{\\lang i_1,\\dots,i_m} \\rang} = 0 </math>\nfor all possible lists of ''m'' rows. (In case ''m''&nbsp;= ''n'', this requires only one determinant, as above. If ''m''&nbsp;&gt;&nbsp;''n'', then it is a theorem that the vectors must be linearly dependent.) This fact is valuable for theory; in practical calculations more efficient methods are available.\n\n===More vectors than dimensions===\nIf there are more vectors than dimensions, the vectors are linearly dependent. This is illustrated in the example above of three vectors in '''R'''<sup>2</sup>.\n\n== Natural basis vectors ==\n\nLet ''V''&nbsp;=&nbsp;'''R'''<sup>''n''</sup> and consider the following elements in ''V'', known as the [[Standard basis|natural basis]] vectors:\n\n:<math>\\begin{matrix}\n\\mathbf{e}_1 & = & (1,0,0,\\ldots,0) \\\\\n\\mathbf{e}_2 & = & (0,1,0,\\ldots,0) \\\\\n& \\vdots \\\\\n\\mathbf{e}_n & = & (0,0,0,\\ldots,1).\\end{matrix}</math>\n\nThen '''e'''<sub>1</sub>, '''e'''<sub>2</sub>, ..., '''e<sub>n</sub>''' are linearly independent.\n\n=== Proof ===\n\nSuppose that ''a''<sub>1</sub>, ''a''<sub>2</sub>, ..., ''a''<sub>''n''</sub> are elements of '''R''' such that\n\n:<math> a_1 \\mathbf{e}_1 + a_2 \\mathbf{e}_2 + \\cdots + a_n \\mathbf{e}_n = \\mathbf{0} . </math>\n\nSince\n:<math> a_1 \\mathbf{e}_1 + a_2 \\mathbf{e}_2 + \\cdots + a_n \\mathbf{e}_n = (a_1 ,a_2 ,\\ldots, a_n) , </math>\n\nthen ''a''<sub>''i''</sub> = 0 for all ''i'' in {1, ..., ''n''}.\n\n== Linear independence of basis functions ==\n\nLet <math>V</math> be the [[vector space]] of all differentiable [[function (mathematics)|function]]s of a real variable <math>t</math>. Then the functions <math>e^t</math> and <math>e^{2t}</math> in <math>V</math> are linearly independent.\n\n=== Proof ===\nSuppose <math>a</math> and <math>b</math> are two real numbers such that\n\n:<math> ae ^ t + be ^ {2t} = 0 </math>\n\nTake the first derivative of the above equation such that\n\n:<math> ae ^ t + 2be ^ {2t} = 0 </math>\n\nfor ''all'' values of ''t''. We need to show that <math>a = 0</math> and <math>b = 0 </math>. In order to do this, we subtract the first equation from the second, giving <math> be ^ {2t} = 0 </math>. Since <math> e^{2t} </math> is not zero for some ''t'', <math> b=0 </math>. It follows that <math>a=0</math> too. Therefore, according to the definition of linear independence, <math> e^{t} </math> and <math> e^{2t} </math> are linearly independent.\n\n== Projective space of linear dependencies ==\n\nA '''linear dependence''' among vectors '''v'''<sub>1</sub>, ..., '''v'''<sub>''n''</sub> is a [[tuple]] (''a''<sub>1</sub>, ..., ''a''<sub>''n''</sub>) with ''n'' [[scalar (mathematics)|scalar]] components, not all zero, such that\n\n:<math>a_1 \\mathbf{v}_1 + \\cdots + a_n \\mathbf{v}_n= \\mathbf{0}. </math>\n\nIf such a linear dependence exists, then the ''n'' vectors are linearly dependent. It makes sense to identify two linear dependencies if one arises as a non-zero multiple of the other, because in this case the two describe the same linear relationship among the vectors. Under this identification, the set of all linear dependences among '''v'''<sub>1</sub>, ...., '''v'''<sub>''n''</sub> is a [[projective space]].\n\n== See also ==\n* [[Gramian matrix]]\n* [[Matroid]]\n* [[Orthogonality]]\n* [[Wronskian]]\n* [[Multicollinearity]]\n\n== References ==\n{{reflist}}\n\n== External links ==\n* {{springer|title=Linear independence|id=p/l059290}}\n* [http://mathworld.wolfram.com/LinearlyDependentFunctions.html Linearly Dependent Functions] at WolframMathWorld.\n* [http://people.revoledu.com/kardi/tutorial/LinearAlgebra/LinearlyIndependent.html Tutorial and interactive program] on Linear Independence.\n* [https://www.khanacademy.org/math/linear-algebra/vectors_and_spaces/linear_independence/v/linear-algebra-introduction-to-linear-independence Introduction to Linear Independence] at KhanAcademy.\n\n{{linear algebra}}\n\n{{DEFAULTSORT:Linear Independence}}\n[[Category:Abstract algebra]]\n[[Category:Linear algebra]]\n[[Category:Articles containing proofs]]"
    },
    {
      "title": "Linear map",
      "url": "https://en.wikipedia.org/wiki/Linear_map",
      "text": "{{Redirect|Linear transformation|fractional linear transformations|Möbius transformation}}\n{{distinguish|linear function}}\nIn [[mathematics]], a '''linear map''' (also called a '''linear mapping''', '''linear [[Transformation (function)|transformation]]''' or, in some contexts, '''linear function''') is a [[Map (mathematics)|mapping]] {{math|''V'' → ''W''}} between two [[Module (mathematics)|module]]s (including [[vector space]]s) that preserves (in the sense defined below) the operations of addition and [[scalar (mathematics)|scalar]] multiplication.  \n\nAn important special case is when {{math|1=''V'' = ''W''}}, an [[endomorphism]] of&nbsp;{{math|''V''}}, sometimes the term '''linear operator''' refers to this case<ref>Linear transformations of {{mvar|V}} into {{mvar|V}} are often called ''linear operators'' on {{mvar|V}} {{harvnb|Rudin|1976|page=207}}</ref>. In another convention, ''linear operator'' allows {{mvar|V}} and {{mvar|W}} to differ, while requiring them to be real vector spaces<ref>Let {{mvar|V}} and {{mvar|W}} be two real vector spaces. A mapping a from {{mvar|V}} into {{mvar|W}} Is called a 'linear mapping' or 'linear transformation' or 'linear operator' [...] from {{mvar|V}} into {{mvar|W}}, if <br> <math display='inline'>a(u+v)=au+av</math> for all <math display='inline'>u,v \\in V</math>, <br> <math display='inline'> a(\\lambda u)= \\lambda au </math> for all <math>u \\in V</math> and all real {{mvar|λ}}. {{harvnb|Bronshtein, Semendyayev|2004|page=316}}</ref>. Sometimes the term ''[[linear function]]'' has the same meaning as ''linear map'', while in [[analytic geometry]] it does not.\n\nA linear map always [[map (mathematics)|maps]] linear subspaces onto linear subspaces (possibly of a lower dimension);<ref>{{harvnb|Rudin|1991|page=14}}<br> Here are some properties of linear mappings <math display=\"inline\">\\Lambda: X \\to Y</math> whose proofs are so easy that we omit them; it is assumed that <math display=\"inline\">A \\subset X</math> and <math display=\"inline\">B \\subset Y</math>:\n{{ordered list\n | list-style-type=lower-alpha\n | <math display=\"inline\">\\Lambda 0 = 0.</math>\n | If {{mvar|A}} is a subspace (or a [[convex set]], or a [[balanced set]]) the same is true of <math display=\"inline\">\\Lambda(A)</math>\n | If {{mvar|B}} is a subspace (or a convex set, or a balanced set) the same is true of <math display=\"inline\">\\Lambda^{-1}(B)</math>\n | In particular, the set:\n: <math>\\Lambda^{-1}(\\{0\\}) = \\{x \\in X: \\Lambda x = 0\\} = {N}(\\Lambda)</math>\n\nis a subspace of {{mvar|X}}, called the ''null space'' of <math display=\"inline\">\\Lambda</math>.\n}}</ref> for instance it maps a plane through the [[origin (geometry)|origin]] to a [[plane (geometry)|plane]], [[straight line]] or [[point (geometry)|point]]. Linear maps can often be represented as [[matrix (mathematics)|matrices]], and simple examples include [[rotation and reflection linear transformations]].\n\nIn the language of [[abstract algebra]], a linear map is a [[module homomorphism]].  In the language of [[category theory]] it is a [[morphism]] in the [[category of modules]] over a given [[Ring (mathematics)|ring]].\n\n== Definition and first consequences ==\nLet <math display=\"inline\">V</math> and <math display=\"inline\">W</math> be vector spaces over the same [[field (mathematics)|field]] <math display=\"inline\">\\mathbf{K}.</math> A function <math display=\"inline\">f : V \\to W</math> is said to be a ''linear map'' if for any two vectors <math display=\"inline\">\\mathbf{u}, \\mathbf{v} \\in V</math> and any scalar <math display=\"inline\">c \\in \\mathbf{K}</math> the following two conditions are satisfied:\n\n{|\n|-\n| style=\"padding:0 20pt\"|<math>f(\\mathbf{u}+\\mathbf{v}) = f(\\mathbf{u})+f(\\mathbf{v})</math>\n| [[Additive map|additivity]] / operation of addition\n|-\n| style=\"padding:0 20pt\"|<math>f(c \\mathbf{u}) = c f(\\mathbf{u})</math>\n| [[Homogeneous function|homogeneity]] of degree 1 / operation of scalar multiplication\n|}\n\nThus, a linear map is said to be ''operation preserving''.  In other words, it does not matter whether the linear map is applied before or after the operations of addition and scalar multiplication.\n\nThis is equivalent to requiring the same for any linear combination of vectors, i.e. that for any vectors <math display=\"inline\">\\mathbf{u}_1, \\ldots, \\mathbf{u}_n \\in V</math> and scalars <math display=\"inline\">c_1, \\ldots, c_n \\in \\mathbf{K},</math> the following equality holds:<ref>{{harvnb|Rudin|1991|page=14}}. Suppose now that {{mvar|X}} and {{mvar|Y}} are vector spaces ''over the same scalar field''. A mapping <math display=\"inline\">\\Lambda: X \\to Y</math> is said to be ''linear'' if <math display=\"inline\">\\Lambda(\\alpha x + \\beta y) = \\alpha\\Lambda x + \\beta\\Lambda y</math> for all <math display=\"inline\">x, y \\in X</math> and all scalars <math display=\"inline\">\\alpha</math> and <math display=\"inline\">\\beta</math>. Note that one often writes <math display=\"inline\">\\Lambda x</math>, rather than <math display=\"inline\">\\Lambda(x)</math>, when <math display=\"inline\">\\Lambda</math> is linear.</ref><ref>{{harvnb|Rudin|1976|page=206}}. A mapping {{mvar|A}} of a vector space {{mvar|X}} into a vector space {{mvar|Y}} is said to be a ''linear transformation'' if: <math display=\"inline\">A\\left(\\bf{x}_1 + \\bf{x}_2\\right) = A\\bf{x}_1 + A\\bf{x}_2,\\  A(c\\bf{x}) = cA\\bf{x}</math> for all <math display=\"inline\">\\bf{x}, \\bf{x}_1, \\bf{x}_2 \\in X</math> and all scalars {{mvar|c}}. Note that one often writes <math display=\"inline\">A\\bf{x}</math> instead of <math display=\"inline\">A(\\bf {x})</math> if {{mvar|A}} is linear.</ref>\n\n: <math>f\\left(c_1 \\mathbf{u}_1 + \\cdots + c_n \\mathbf{u}_n\\right) = c_1 f\\left(\\mathbf{u}_1\\right) + \\cdots + c_n f\\left(\\mathbf{u}_n\\right).</math>\n\nDenoting the zero elements of the vector spaces <math display=\"inline\">V</math> and <math display=\"inline\">W</math> by <math display=\"inline\">\\mathbf{0}_V</math> and <math display=\"inline\">\\mathbf{0}_W</math> respectively, it follows that <math display=\"inline\">f\\left(\\mathbf{0}_V\\right) = \\mathbf{0}_W.</math> Let <math display=\"inline\">c = 0</math> and <math display=\"inline\">\\mathbf{v} \\in V</math> in the equation for homogeneity of degree 1:\n\n:<math>f\\left(\\mathbf{0}_V\\right) = f\\left(0\\mathbf{v}\\right) = 0f(\\mathbf{v}) = \\mathbf{0}_W.</math>\n\nOccasionally, <math display=\"inline\">V</math> and <math display=\"inline\">W</math> can be considered to be vector spaces over different fields.  It is then necessary to specify which of these ground fields is being used in the definition of \"linear\". If <math display=\"inline\">V</math> and <math display=\"inline\">W</math> are considered as spaces over the field <math display=\"inline\">\\mathbf{K}</math> as above, we talk about <math display=\"inline\">\\mathbf{K}</math>-linear maps. For example, the [[complex conjugate|conjugation]] of [[complex numbers]] is an <math display=\"inline\">\\mathbf{R}</math>-linear map <math display=\"inline\">\\mathbf{C} \\to \\mathbf{C}</math>, but it is not <math display=\"inline\">\\mathbf{C}</math>-linear.\n\nA linear map <math display=\"inline\">V \\to \\mathbf{K}</math> with <math display=\"inline\">\\mathbf{K}</math> viewed as a vector space over itself is called a [[linear functional]].<ref>{{harvnb|Rudin|1991|page=14}}. Linear mappings of {{mvar|X}} onto its scalar field are called ''linear functionals''.</ref>\n\nThese statements generalize to any left-module <math display=\"inline\">{}_R M</math> over a ring <math display=\"inline\">R</math> without modification, and to any right-module upon reversing of the scalar multiplication.\n\n== Examples ==\n* The zero map {{nowrap|''x'' ↦ 0}} between two left-modules (or two right-modules) over the same ring is always linear.\n* The [[identity function|identity map]] on any module is a linear operator.\n* Any [[homothecy]] centered in the origin of a vector space, <math display=\"inline\">v \\mapsto cv</math> where ''c'' is a scalar, is a linear operator.  This does not hold in general for modules, where such a map might only be [[semilinear map|semilinear]].\n* For real numbers, the map {{nowrap|''x'' ↦ ''x''<sup>2</sup>}} is not linear.\n* For real numbers, the map {{nowrap|''x'' ↦ ''x'' + 1}} is not linear (but is an [[affine transformation]]; {{nowrap|1=''y'' = ''x'' + 1}} is a [[linear equation]], as the term is used in [[analytic geometry]].)\n* If ''A'' is a real {{nowrap|''m'' × ''n''}} [[matrix (mathematics)|matrix]], then ''A'' defines a linear map from  '''R'''<sup>''n''</sup> to '''R'''<sup>''m''</sup> by sending the [[column vector]] {{nowrap|'''x''' ∈ '''R'''<sup>''n''</sup>}} to the column vector {{nowrap|''A'''''x''' ∈ '''R'''<sup>''m''</sup>}}. Conversely, any linear map between [[finite-dimensional]] vector spaces can be represented in this manner; see the [[#Matrices|following section]].\n* [[Derivative|Differentiation]] defines a linear map from the space of all differentiable functions to the space of all functions. It also defines a linear operator on the space of all [[smooth function]]s.\n* The (definite) [[integral]] over some [[interval (mathematics)|interval]] ''I'' is a linear map from the space of all real-valued integrable functions on ''I'' to '''R'''.\n* The (indefinite) [[integral]] (or [[antiderivative]]) with a fixed starting point defines a linear map from the space of all real-valued integrable functions on '''R''' to the space of all real-valued, differentiable, functions on '''R'''. Without a fixed starting point, an exercise in group theory will show that the antiderivative maps to the [[quotient space (linear algebra)|quotient space]] of the differentiables over the [[equivalence relation]], \"differ by a constant\", which yields an identity class of the constant valued functions <math display=\"inline\">\\left(\\,\\int\\!:\\  I(\\Re) \\ \\to\\  D(\\Re)/\\Re\\,\\right)</math>.\n* If ''V'' and ''W'' are finite-dimensional vector spaces over a field ''F'', then functions that send linear maps {{nowrap|''f'' : ''V'' → ''W''}} to {{nowrap|dim<sub>''F''</sub>(''W'') × dim<sub>''F''</sub>(''V'')}} matrices in the way described in the sequel are themselves linear maps (indeed [[linear isomorphism]]s).\n* The [[expected value]] of a [[Random variable#Definition|random variable]] (which is in fact a function, and as such a member of a vector space) is linear, as for random variables ''X'' and ''Y'' we have {{nowrap|1=E[''X'' + ''Y''] = E[''X''] + E[''Y'']}} and {{nowrap|1=E[''aX''] = ''a''E[''X'']}}, but the [[variance]] of a random variable is not linear.\n\n<gallery widths=300 heights=200>\nFile:Streckung eines Vektors.gif|The function <math display=\"inline\">f:\\R^2 \\to \\R^2</math> with <math display=\"inline\">f(x, y) = (2x, y)</math> is a linear map. This function scales the <math display=\"inline\">x</math> component of a vector by the factor <math display=\"inline\">2</math>.\nFile:Streckung der Summe zweier Vektoren.gif|The function is additive: It doesn't matter whether first vectors are added and then mapped or whether they are mapped and finally added: <math display=\"inline\">f(a + b) = f(a) + f(b)</math>\nFile:Streckung homogenitaet Version 3.gif|The function is homogeneous: It doesn't matter whether a vector is first scaled and then mapped or first mapped and then scaled: <math display=\"inline\">f(\\lambda a) = \\lambda f(a)</math>\n</gallery>\n\n== Matrices ==\n{{main|Transformation matrix}}\n\nIf ''V'' and ''W'' are [[finite-dimensional]] vector spaces and a [[basis of a vector space|basis]] is defined for each vector space, then every linear map from ''V'' to ''W'' can be represented by a [[matrix (mathematics)|matrix]].<ref>{{harvnb|Rudin|1976|page=210}}\n\nSuppose <math display=\"inline\">\\left\\{\\bf{x}_1, \\ldots, \\bf{x}_n\\right\\}</math> and <math display=\"inline\">\\left\\{\\bf{y}_1, \\ldots, \\bf{y}_m\\right\\}</math> are bases of vector spaces {{mvar|X}} and {{mvar|Y}}, respectively. Then every <math display=\"inline\">A \\in L(X, Y)</math> determines a set of numbers <math display=\"inline\">a_{i,j}</math> such that\n: <math>A\\bf{x}_j = \\sum_{i=1}^m a_{i,j}\\bf{y}_i\\quad (1 \\leq j \\leq n).</math>\n\nIt is convenient to represent these numbers in a rectangular array of {{mvar|m}} rows and {{mvar|n}} columns, called an {{mvar|m}} ''by'' {{mvar|n}} ''matrix'':\n: <math>[A] = \\begin{bmatrix}\n  a_{1,1} & a_{1,2} & \\ldots & a_{1,n} \\\\\n  a_{2,1} & a_{2,2} & \\ldots & a_{2,n} \\\\ \n   \\vdots &  \\vdots & \\ddots &  \\vdots \\\\\n  a_{m,1} & a_{m,2} & \\ldots & a_{m,n}\n\\end{bmatrix}</math>\n\nObserve that the coordinates <math display=\"inline\">a_{i,j}</math> of the vector <math display=\"inline\">A{\\bf x}_j</math> (with respect to the basis <math display=\"inline\">\\{\\bf{y}_1, \\ldots, \\bf{y}_m\\}</math>) appear in the ''j''<sup>th</sup> column of <math display=\"inline\">[A]</math>. The vectors <math display=\"inline\">A{\\bf x}_j</math> are therefore sometimes called the ''column vectors'' of <math display=\"inline\">[A]</math>. With this terminology, the ''range'' of {{mvar|A}} ''is spanned by the column vectors of <math display=\"inline\">[A]</math>''.\n</ref> This is useful because it allows concrete calculations. Matrices yield examples of linear maps: if ''A'' is a real {{nowrap|''m'' × ''n''}} matrix, then {{nowrap|1=''f''('''x''') = ''A'''''x'''}} describes a linear map {{nowrap|'''R'''<sup>''n''</sup> → '''R'''<sup>''m''</sup>}} (see [[Euclidean space]]).\n\nLet {'''v'''<sub>1</sub>, …, '''v'''<sub>''n''</sub>} be a basis for ''V''.  Then every vector '''v''' in ''V'' is uniquely determined by the coefficients ''c''<sub>1</sub>, …, ''c''<sub>''n''</sub> in the field '''R''':\n\n: <math>c_1 \\mathbf{v}_1 + \\cdots + c_n \\mathbf{v}_n.</math>\n\nIf {{nowrap|''f'' : ''V'' → ''W''}} is a linear map,\n\n: <math>f\\left(c_1 \\mathbf{v}_1 + \\cdots + c_n \\mathbf{v}_n\\right) = c_1 f\\left(\\mathbf{v}_1\\right) + \\cdots + c_n f\\left(\\mathbf{v}_n\\right),</math>\n\nwhich implies that the function ''f'' is entirely determined by the vectors ''f''('''v'''<sub>1</sub>), …, ''f''('''v'''<sub>''n''</sub>). Now let {{nowrap|{'''w'''<sub>1</sub>, …, '''w'''<sub>''m''</sub>} }} be a basis for ''W''.  Then we can represent each vector ''f''('''v'''<sub>''j''</sub>) as\n\n: <math>f\\left(\\mathbf{v}_j\\right) = a_{1j} \\mathbf{w}_1 + \\cdots + a_{mj} \\mathbf{w}_m.</math>\n\nThus, the function ''f'' is entirely determined by the values of ''a''<sub>''ij''</sub>. If we put these values into an {{nowrap|''m'' × ''n''}} matrix ''M'', then we can conveniently use it to compute the vector output of ''f'' for any vector in ''V''.  To get ''M'', every column ''j'' of ''M'' is a vector\n\n: <math>\\begin{pmatrix} a_{1j} & \\cdots & a_{mj} \\end{pmatrix}^\\textsf{T}</math>\n\ncorresponding to ''f''('''v'''<sub>''j''</sub>) as defined above. To define it more clearly, for some column ''j'' that corresponds to the mapping ''f''('''v'''<sub>''j''</sub>),\n\n: <math>\\mathbf{M} = \\begin{pmatrix}\n  \\ \\cdots & a_{1j} & \\cdots\\  \\\\\n           & \\vdots &          \\\\\n           & a_{mj} &\n\\end{pmatrix}</math>\n\nwhere '''M''' is the matrix of ''f''. In other words, every column {{nowrap|1=''j'' = 1, …, ''n''}} has a corresponding vector ''f''('''v'''<sub>''j''</sub>) whose coordinates ''a''<sub>1''j''</sub>, …, ''a''<sub>''mj''</sub> are the elements of column ''j''. A single linear map may be represented by many matrices. This is because the values of the elements of a matrix depend on the bases chosen.\n\nThe matrices of a linear transformation can be represented visually:\n\n# Matrix for <math display=\"inline\">T</math> relative to <math display=\"inline\">B</math>: <math display=\"inline\">A</math>\n# Matrix for <math display=\"inline\">T</math> relative to <math display=\"inline\">B'</math>: <math display=\"inline\">A'</math>\n# Transition matrix from <math display=\"inline\">B'</math> to <math display=\"inline\">B</math>: <math display=\"inline\">P</math>\n# Transition matrix from <math display=\"inline\">B</math> to <math display=\"inline\">B'</math>: <math display=\"inline\">P^{-1}</math>\n\n[[File:Linear_transformation_visualization.svg|The relationship between matrices in a linear transformation]]\n\nSuch that starting in the bottom left corner <math display=\"inline\">\\left[\\vec{v}\\right]_{B'}</math> and looking for the bottom right corner <math display=\"inline\">\\left[T\\left(\\vec{v}\\right)\\right]_{B'}</math>, one would left-multiply—that is, <math display=\"inline\">A'\\left[\\vec{v}\\right]_{B'} = \\left[T\\left(\\vec{v}\\right)\\right]_{B'}</math>. The equivalent method would be the \"longer\" method going clockwise from the same point such that <math display=\"inline\">\\left[\\vec{v}\\right]_{B'}</math> is left-multiplied with <math display=\"inline\">P^{-1}AP</math>, or <math display=\"inline\">P^{-1}AP\\left[\\vec{v}\\right]_{B'} = \\left[T\\left(\\vec{v}\\right)\\right]_{B'}</math>.\n\n== Examples of linear transformation matrices ==\nIn two-[[dimension]]al space '''R'''<sup>2</sup> linear maps are described by [[2 × 2 real matrices]]. These are some examples:\n\n* [[Rotation (mathematics)|rotation]]\n** by 90 degrees counterclockwise:\n**: <math>\\mathbf{A} = \\begin{pmatrix} 0 & -1\\\\ 1 & 0\\end{pmatrix}</math>\n** by an angle ''θ'' counterclockwise:\n**: <math>\\mathbf{A} = \\begin{pmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{pmatrix}</math>\n* [[Reflection (mathematics)|reflection]]\n** about the ''x'' axis:\n**: <math>\\mathbf{A} = \\begin{pmatrix} 1 & 0\\\\ 0 & -1\\end{pmatrix}</math>\n** about the ''y'' axis:\n**: <math>\\mathbf{A} = \\begin{pmatrix}-1 & 0\\\\ 0 & 1\\end{pmatrix}</math>\n* [[Scaling (geometry)|scaling]] by 2 in all directions:\n*: <math>\\mathbf{A} = \\begin{pmatrix} 2 & 0\\\\ 0 & 2\\end{pmatrix} = 2\\mathbf{I}</math>\n* [[shear mapping|horizontal shear mapping]]:\n*: <math>\\mathbf{A} = \\begin{pmatrix} 1 & m\\\\ 0 & 1\\end{pmatrix}</math>\n* [[squeeze mapping]]:\n*: <math>\\mathbf{A} = \\begin{pmatrix} k & 0\\\\ 0 & \\frac{1}{k}\\end{pmatrix}</math>\n* [[Projection (linear algebra)|projection]] onto the ''y'' axis:\n*: <math>\\mathbf{A} = \\begin{pmatrix} 0 & 0\\\\ 0 & 1\\end{pmatrix}.</math>\n\n== Forming new linear maps from given ones ==\nThe composition of linear maps is linear: if {{nowrap|''f'' : ''V'' → ''W''}} and {{nowrap|''g'' : ''W'' → ''Z''}} are linear, then so is their [[Relation composition|composition]] {{nowrap|''g'' ∘ ''f'' : ''V'' → ''Z''}}. It follows from this that the [[class (set theory)|class]] of all vector spaces over a given field ''K'', together with ''K''-linear maps as [[morphism]]s, forms a [[category (mathematics)|category]].\n\nThe [[inverse function|inverse]] of a linear map, when defined, is again a linear map.\n\nIf {{nowrap|''f''<sub>1</sub> : ''V'' → ''W''}} and {{nowrap|''f''<sub>2</sub> : ''V'' → ''W''}} are linear, then so is their [[pointwise]] sum {{nowrap|''f''<sub>1</sub> + ''f''<sub>2</sub>}} (which is defined by {{nowrap|1=(''f''<sub>1</sub> + ''f''<sub>2</sub>)('''x''') = ''f''<sub>1</sub>('''x''') + ''f''<sub>2</sub>('''x'''))}}.\n\nIf {{nowrap|''f'' : ''V'' → ''W''}} is linear and ''a'' is an element of the ground field ''K'', then the map ''af'', defined by {{nowrap|1=(''af'')('''x''') = ''a''(''f''('''x'''))}}, is also linear.\n\nThus the set {{nowrap|''L''(''V'', ''W'')}} of linear maps from ''V'' to ''W'' itself forms a vector space over ''K'', sometimes denoted {{nowrap|Hom(''V'', ''W'')}}.  Furthermore, in the case that {{nowrap|1=''V'' = ''W''}}, this vector space (denoted End(''V'')) is an [[associative algebra]] under [[composition of maps]], since the composition of two linear maps  is again a linear map, and the composition of maps is always associative.  This case is discussed in more detail below.\n\nGiven again the finite-dimensional case, if bases have been chosen, then the composition of linear maps corresponds to the [[matrix multiplication]], the addition of linear maps corresponds to the [[matrix addition]], and the multiplication of linear maps with scalars corresponds to the multiplication of matrices with scalars.\n\n== Endomorphisms and automorphisms ==\n{{main|Endomorphism|Automorphism}}\nA linear transformation ''f'': ''V'' → ''V'' is an [[endomorphism]] of ''V''; the set of all such endomorphisms End(''V'')  together with addition, composition and scalar multiplication as defined above forms an [[associative algebra]] with identity element over the field ''K'' (and in particular a [[ring (algebra)|ring]]). The multiplicative identity element of this algebra is the [[identity function|identity map]] id: ''V'' → ''V''.\n\nAn endomorphism of ''V'' that is also an [[isomorphism]] is called an [[automorphism]] of ''V''. The composition of two automorphisms is again an automorphism, and the set of all automorphisms of ''V'' forms a [[group (math)|group]], the [[automorphism group]] of ''V'' which is denoted by Aut(''V'') or GL(''V''). Since the automorphisms are precisely those [[endomorphisms]] which possess inverses under composition, Aut(''V'') is the group of [[Unit (ring theory)|units]] in the ring End(''V'').\n\nIf ''V'' has finite dimension ''n'', then End(''V'') is [[isomorphism|isomorphic]] to the [[associative algebra]] of all ''n'' × ''n'' matrices with entries in ''K''. The automorphism group of ''V'' is [[group isomorphism|isomorphic]] to the  [[general linear group]] GL(''n'', ''K'') of all ''n'' × ''n'' invertible matrices with entries in ''K''.\n\n== Kernel, image and the rank–nullity theorem ==\n{{main|Kernel (linear operator)|Image (mathematics)|Rank of a matrix}}\nIf ''f'' : ''V'' → ''W'' is linear, we define the [[kernel (linear operator)|kernel]] and the [[image (mathematics)|image]] or [[range (mathematics)|range]] of ''f'' by\n\n: <math>\\begin{align}\n  \\operatorname{\\ker}(f) &= \\{\\,x \\in V: f(x) = 0\\,\\} \\\\\n    \\operatorname{im}(f) &= \\{\\,w \\in W: w = f(x), x \\in V\\,\\}\n\\end{align}</math>\n\nker(''f'') is a [[Linear subspace|subspace]] of ''V'' and im(''f'') is a subspace of ''W''.  The following [[dimension]] formula is known as the [[rank–nullity theorem]]:\n\n: <math>\\dim(\\ker( f )) + \\dim(\\operatorname{im}( f )) = \\dim( V ).</math><ref>{{harvnb|Horn|Johnson|2013|loc=0.2.3 Vector spaces associated with a matrix or linear transformation, p. 6}}</ref>\n\nThe number dim(im(''f'')) is also called the ''rank of f'' and written as rank(''f''), or sometimes, ρ(''f''); the number dim(ker(''f'')) is called the ''nullity of f'' and written as null(''f'') or ν(''f''). If ''V'' and ''W'' are finite-dimensional, bases have been chosen and ''f'' is represented by the matrix ''A'', then the rank and nullity of ''f'' are equal to the [[rank of a matrix|rank]] and [[Kernel (matrix)#Subspace properties|nullity]] of the matrix ''A'', respectively.\n\n== Cokernel ==\n{{main|Cokernel}}\n\nA subtler invariant of a linear transformation <math display=\"inline\">f: V \\to W</math> is the [[cokernel|''co''kernel]], which is defined as\n\n: <math>\\operatorname{coker}(f) := W/f(V) = W/\\operatorname{im}(f).</math>\n\nThis is the ''dual'' notion to the kernel: just as the kernel is a ''sub''space of the ''domain,'' the co-kernel is a [[quotient space (linear algebra)|''quotient'' space]] of the ''target.''\nFormally, one has the [[exact sequence]]\n\n: <math>0 \\to \\ker(f) \\to V \\to W \\to \\operatorname{coker}(f) \\to 0.</math>\n\nThese can be interpreted thus: given a linear equation ''f''('''v''') = '''w''' to solve,\n\n* the kernel is the space of ''solutions'' to the ''homogeneous'' equation ''f''('''v''') = 0, and its dimension is the number of ''degrees of freedom'' in a solution, if it exists;\n* the co-kernel is the space of ''constraints'' that must be satisfied if the equation is to have a solution, and its dimension is the number of constraints that must be satisfied for the equation to have a solution.\n\nThe dimension of the co-kernel and the dimension of the image (the rank) add up to the dimension of the target space. For finite dimensions, this means that the dimension of the quotient space ''W''/''f''(''V'') is the dimension of the target space minus the dimension of the image.\n\nAs a simple example, consider the map ''f'': '''R'''<sup>2</sup> → '''R'''<sup>2</sup>, given by ''f''(''x'', ''y'') = (0, ''y''). Then for an equation ''f''(''x'', ''y'') = (''a'', ''b'') to have a solution, we must have ''a'' = 0 (one constraint), and in that case the solution space is (''x'', ''b'')  or equivalently stated, (0, ''b'') + (''x'', 0), (one degree of freedom). The kernel may be expressed as the subspace (''x'', 0) < ''V'': the value of ''x'' is the freedom in a solution – while the cokernel may be expressed via the map ''W'' → '''R''', <math display=\"inline\"> (a, b) \\mapsto (a):</math> given a vector (''a'', ''b''), the value of ''a'' is the ''obstruction'' to there being a solution.\n\nAn example illustrating the infinite-dimensional case is afforded by the map ''f'': '''R'''<sup>∞</sup> → '''R'''<sup>∞</sup>, <math display=\"inline\">\\left\\{a_n\\right\\} \\mapsto \\left\\{b_n\\right\\}</math> with ''b''<sub>1</sub> = 0 and ''b''<sub>''n'' + 1</sub> = ''a<sub>n</sub>'' for ''n'' > 0. Its image consists of all sequences with first element 0, and thus its cokernel consists of the classes of sequences with identical first element. Thus, whereas its kernel has dimension 0 (it maps only the zero sequence to the zero sequence), its co-kernel has dimension 1. Since the domain and the target space are the same, the rank and the dimension of the kernel add up to the same [[cardinal number#Cardinal addition|sum]] as the rank and the dimension of the co-kernel ( <math display=\"inline\">\\aleph_0 + 0 = \\aleph_0 + 1</math> ), but in the infinite-dimensional case it cannot be inferred that the kernel and the co-kernel of an [[endomorphism]] have the same dimension (0 ≠ 1). The reverse situation obtains for the map ''h'': '''R'''<sup>∞</sup> → '''R'''<sup>∞</sup>, <math display=\"inline\">\\left\\{a_n\\right\\} \\mapsto \\left\\{c_n\\right\\}</math> with ''c<sub>n</sub>'' = ''a''<sub>''n'' + 1</sub>. Its image is the entire target space, and hence its co-kernel has dimension 0, but since it maps all sequences in which only the first element is non-zero to the zero sequence, its kernel has dimension 1.\n\n=== Index ===\nFor a linear operator with finite-dimensional kernel and co-kernel, one may define  ''index'' as:\n\n: <math>\\operatorname{ind}(f) := \\dim(\\ker(f)) - \\dim(\\operatorname{coker}(f)),</math>\n\nnamely the degrees of freedom minus the number of constraints.\n\nFor a transformation between finite-dimensional vector spaces, this is just the difference dim(''V'') − dim(''W''), by rank–nullity. This gives an indication of how many solutions or how many constraints one has: if mapping from a larger space to a smaller one, the map may be onto, and thus will have degrees of freedom even without constraints. Conversely, if mapping from a smaller space to a larger one, the map cannot be onto, and thus one will have constraints even without degrees of freedom.\n\nThe index of an operator is precisely the [[Euler characteristic]] of the 2-term complex 0 → ''V'' → ''W'' → 0. In [[operator theory]], the index of [[Fredholm operator]]s is an object of study, with a major result being the [[Atiyah–Singer index theorem]].<ref>{{SpringerEOM|title=Index theory|id=Index_theory&oldid=23864|first=Victor|last=Nistor}}: \"The main question in index theory is to provide index formulas for classes of Fredholm operators ... Index theory has become a subject on its own only after M. F. Atiyah and I. Singer published their index theorems\"</ref>\n\n== Algebraic classifications of linear transformations ==\nNo classification of linear maps could hope to be exhaustive. The following incomplete list enumerates some important classifications that do not require any additional structure on the vector space.\n\nLet ''V'' and ''W'' denote vector spaces over a field, ''F''. Let ''T'': ''V'' → ''W'' be a linear map.\n\n* ''T'' is said to be ''[[injective]]'' or a ''[[monomorphism]]'' if any of the following equivalent conditions are true:\n** ''T'' is [[injective|one-to-one]] as a map of [[set (mathematics)|sets]].\n** ker''T'' = {0<sub>''V''</sub>}\n** dim(ker''T'') = 0\n** ''T'' is [[monic morphism|monic]] or left-cancellable, which is to say, for any vector space ''U'' and any pair of linear maps ''R'': ''U'' → ''V'' and ''S'': ''U'' → ''V'', the equation ''TR'' = ''TS'' implies ''R'' = ''S''.\n** ''T'' is [[inverse (ring theory)|left-invertible]], which is to say there exists a linear map ''S'': ''W'' → ''V'' such that ''ST'' is the [[Identity function|identity map]] on ''V''.\n* ''T'' is said to be ''[[surjective]]'' or an ''[[epimorphism]]'' if any of the following equivalent conditions are true:\n** ''T'' is [[surjective|onto]] as a map of sets.\n** [[cokernel|coker]] ''T'' = {0<sub>''W''</sub>}\n** ''T'' is [[epimorphism|epic]] or right-cancellable, which is to say, for any vector space ''U'' and any pair of linear maps ''R'': ''W'' → ''U'' and ''S'': ''W'' → ''U'', the equation ''RT'' = ''ST'' implies ''R'' = ''S''.\n** ''T'' is [[inverse (ring theory)|right-invertible]], which is to say there exists a linear map ''S'': ''W'' → ''V'' such that ''TS'' is the [[Identity function|identity map]] on ''W''.\n* ''T'' is said to be an ''[[isomorphism]]'' if it is both left- and right-invertible. This is equivalent to ''T'' being both one-to-one and onto (a [[bijection]] of sets) or also to ''T'' being both epic and monic, and so being a [[bimorphism]].\n* If ''T'': ''V'' → ''V'' is an endomorphism, then:\n** If, for some positive integer ''n'', the ''n''-th iterate of ''T'', ''T<sup>n</sup>'', is identically zero, then ''T'' is said to be [[nilpotent]].\n** If ''T''<sup>2</sup> = ''T'', then ''T'' is said to be [[idempotent]]\n** If ''T'' = ''kI'', where ''k'' is some scalar, then ''T'' is said to be a scaling transformation or scalar multiplication map; see [[scalar matrix]].\n\n== Change of basis ==\n{{main|Basis (linear algebra)|Change of basis}}\nGiven a linear map which is an [[endomorphism]] whose matrix is ''A'', in the basis ''B'' of the space it transforms vector coordinates [u] as [v] = ''A''[u]. As vectors change with the inverse of ''B'' (vectors are [[Covariance and contravariance of vectors|contravariant]]) its inverse transformation is [v] = ''B''[v'].\n\nSubstituting this in the first expression\n:<math>B\\left[v'\\right] = AB\\left[u'\\right]</math>\n\nhence\n:<math>\\left[v'\\right] = B^{-1}AB\\left[u'\\right] = A'\\left[u'\\right].</math>\n\nTherefore, the matrix in the new basis is ''A′'' = ''B''<sup>−1</sup>''AB'', being ''B'' the matrix of the given basis.\n\nTherefore, linear maps are said to be 1-co- 1-contra-[[covariance and contravariance of vectors|variant]] objects, or type (1, 1) [[tensor]]s.\n\n== Continuity ==\n{{main|Discontinuous linear map}}\n\nA ''linear transformation'' between [[topological vector space]]s, for example [[normed space]]s, may be [[continuous function (topology)|continuous]].  If its domain and codomain are the same, it will then be a [[continuous linear operator]].  A linear operator on a normed linear space is continuous if and only if it is [[bounded operator|bounded]], for example, when the domain is finite-dimensional.<ref>{{harvnb|Rudin|1991|page=15}}\n\n'''1.18 Theorem''' ''Let <math display=\"inline\">\\Lambda</math> be a linear functional on a topological vector space {{mvar|X}}. Assume <math display=\"inline\">\\Lambda x \\neq 0</math> for some <math display=\"inline\">x \\in X</math>. Then each of the following four properties implies the other three:\n{{ordered list\n | list-style-type=lower-alpha\n | <math display=\"inline\">\\Lambda</math> is continuous\n | The null space <math display=\"inline\">N(\\Lambda)</math> is closed.\n | <math display=\"inline\">N(\\Lambda)</math> is not dense in {{mvar|X}}.\n | <math display=\"inline\">\\Lambda</math> is bounded in some neighbourhood {{mvar|V}} of 0.\n}}</ref>  An infinite-dimensional domain may have [[discontinuous linear operator]]s.\n\nAn example of an unbounded, hence discontinuous, linear transformation is differentiation on the space of smooth functions equipped with the supremum norm (a function with small values can have a derivative with large values, while the derivative of 0 is 0).  For a specific example, sin(''nx'')/''n'' converges to 0, but its derivative cos(''nx'') does not, so differentiation is not continuous at 0 (and by a variation of this argument, it is not continuous anywhere).\n\n== Applications ==\nA specific application of linear maps is for geometric transformations, such as those performed in [[computer graphics]], where the translation, rotation and scaling of 2D or 3D objects is performed by the use of a [[transformation matrix]]. Linear mappings also are used as a mechanism for describing change: for example in calculus correspond to derivatives; or in relativity, used as a device to keep track of the local transformations of reference frames.\n\nAnother application of these transformations is in [[compiler optimizations]] of nested-loop code, and in [[parallelizing compiler]] techniques.\n\n== See also ==\n{{Wikibooks|Linear Algebra/Linear Transformations}}\n* [[Antilinear map]]\n* [[Bent function]]\n* [[Bounded operator]]\n\n== Notes==\n{{reflist}}\n\n== References ==\n* {{Cite book | last1=Halmos | first1=Paul R. | author1-link=Paul R. Halmos | title=Finite-Dimensional Vector Spaces | publisher=[[Springer-Verlag]] | location=New York | isbn=0-387-90093-4 | year=1974}}\n* {{Cite book | last1=Horn | first1=Roger A. | last2=Johnson | first2=Charles R. | title=Matrix Analysis |edition=Second |publisher=[[Cambridge University Press]] | isbn=978-0-521-83940-2 | year=2013 }}\n* {{Citation | last1=Lang | first1=Serge | author1-link=Serge Lang | title=Linear Algebra | publisher=[[Springer-Verlag]] | location=New York | isbn=0-387-96412-6 |edition=Third | year=1987}}\n* {{cite book| last=Rudin | first=Walter| author1-link=Walter Rudin| title=Principles of Mathematical Analysis |edition=Third |publisher=[[McGraw-Hill]]|year=1976|isbn=0-07-085613-3 }}\n* {{cite book| last=Rudin | first=Walter| author1-link=Walter Rudin| title=Functional Analysis |edition=Second | publisher=[[McGraw-Hill]] |year=1991 |isbn=0-07-054236-8 }}\n\n{{linear algebra}}\n{{tensors}}\n\n[[Category:Abstract algebra]]\n[[Category:Functions and mappings]]\n[[Category:Linear algebra]]\n[[Category:Transformation (function)]]"
    },
    {
      "title": "Linear span",
      "url": "https://en.wikipedia.org/wiki/Linear_span",
      "text": "In [[linear algebra]], the '''linear span''' (also called the '''linear hull''' or just '''span''') of a [[Set (mathematics)|set]] of [[vector space|vectors]] in a [[vector space]] is the [[intersection (set theory)|intersection]] of all [[linear subspace]]s which each contain every vector in that set. The linear span of a set of vectors is therefore a vector space. Spans can be generalized to [[matroid]]s and [[Module (mathematics)|modules]].\n\nFor expressing that a vector space {{mvar|V}} is a span of a set {{mvar|S}}, one commonly uses the following phrases: {{mvar|S}} spans {{mvar|V}}; {{mvar|V}} is spanned by {{mvar|S}}; {{mvar|S}} is a '''spanning set''' of {{mvar|V}}; {{mvar|S}} is a generating set of {{mvar|V}}.\n\n==Definition==\n\nGiven a [[vector space]] ''V'' over a [[field (mathematics)|field]] ''K'', the span of a [[Set (mathematics)|set]] ''S'' of vectors (not necessarily infinite) is defined to be the intersection ''W'' of all [[linear subspace|subspaces]] of ''V'' that contain ''S''. ''W'' is referred to as the subspace ''spanned by'' ''S'', or by the vectors in ''S''. Conversely, ''S'' is called a ''spanning set'' of ''W'', and we say that ''S'' ''spans'' ''W''.\n\nAlternatively, the span of ''S'' may be defined as the set of all finite [[linear combinations ]] of elements (vectors)  of ''S'', which follows from the above definition.\n\n:<math>\\operatorname{span}(S) =  \\left \\{ {\\left.\\sum_{i=1}^k \\lambda_i v_i \\right| k \\in \\mathbb{N}, v_i  \\in S, \\lambda _i  \\in K} \\right \\}.</math>\n\nIn particular, if ''S'' is a [[finite set|finite]] subset of ''V'', then the span of ''S'' is the set of all linear combinations of the elements of ''S''. In the case of infinite ''S'', infinite linear combinations (i.e. where a combination may involve an infinite sum, assuming such sums are defined somehow, e.g. if ''V'' is a [[Banach space]]) are excluded by the definition; a [[Linear combination#Generalizations|generalization]] that allows these is not equivalent.\n\n== Examples ==\n[[File:Basis for a plane.svg|thumb|280px|right|The cross-hatched plane is the linear span of '''u''' and '''v''' in '''R'''<sup>3</sup>.]]\n\nThe [[real number|real]] vector space '''R'''<sup>3</sup> has {(-1,0,0), (0,1,0), (0,0,1)} as a spanning set. This particular spanning set is also a [[Basis (linear algebra)|basis]]. If (-1,0,0) were replaced by (1,0,0), it would also form the [[standard basis|canonical basis]] of '''R'''<sup>3</sup>.\n\nAnother spanning set for the same space is given by {(1,2,3), (0,1,2), (−1,1/2,3), (1,1,1)}, but this set is not a basis, because it is [[Linear dependency|linearly dependent]].\n\nThe set {(1,0,0), (0,1,0), (1,1,0)} is not a spanning set of '''R'''<sup>3</sup>; instead its span is the space of all vectors in '''R'''<sup>3</sup> whose last component is zero. That space (the space of all vectors in '''R'''<sup>3</sup> whose last component is zero) is also spanned by the set {(1,0,0), (0,1,0)}, as (1,1,0) is a linear combination of (1,0,0) and (0,1,0). It does, however, span '''R'''<sup>2</sup>.\n\nThe empty set is a spanning set of {(0, 0, 0)} since the empty set is a subset of all possible vector spaces in '''R'''<sup>3</sup>, and {(0, 0, 0)} is the intersection of all of these vector spaces.\n\nThe set of functions ''x<sup>n</sup>'' where ''n'' is a non-negative integer spans the space of polynomials.\n\n==Theorems==\n\n'''Theorem 1:''' The subspace spanned by a non-empty subset ''S'' of a vector space ''V'' is the set of all linear combinations of vectors in ''S''.\n\nThis theorem is so well known that at times it is referred to as the definition of span of a set.\n\n'''Theorem 2:''' Every spanning set ''S'' of a vector space ''V'' must contain at least as many elements as any [[Linear independence|linearly independent]] set of vectors from ''V''.\n\n'''Theorem 3:''' Let ''V'' be a finite-dimensional vector space. Any set of vectors that spans ''V'' can be reduced to a basis for ''V'' by discarding vectors if necessary (i.e. if there are linearly dependent vectors in the set). If the [[axiom of choice]] holds, this is true without the assumption that ''V'' has finite dimension.\n\nThis also indicates that a basis is a minimal spanning set when ''V'' is finite-dimensional.\n\n== Generalizations ==\nGeneralizing the definition of the span of points in space, a subset ''X'' of the ground set of a [[matroid]] is called a ''spanning set'' if the rank of ''X'' equals the rank of the entire ground set{{Citation needed|date=May 2016}}.\n\nThe vector space definition can also be generalized to modules.<ref>{{Cite book|url=https://www.amazon.co.uk/Algebra-Third-AMS-Chelsea-Publishing/dp/0821816462|title=Algebra: Third Edition|last=Lane|first=Saunders Mac|last2=Birkhoff|first2=Garrett|date=1999-02-28|publisher=EDS Publications Ltd.|isbn=9780821816462|pages=168|language=English}}</ref> Given an ''R''-module ''A'' and a collection of elements a<sub>1</sub>,…,a<sub>n</sub> of A, the [[submodule]] of ''A'' spanned by a<sub>1</sub>,…,a<sub>n</sub> is the sum of [[cyclic module]]s\n\n:<math>Ra_1 + \\cdots + Ra_n = \\left \\{ \\sum_{k=1}^n r_k a_k \\Big  |   r_k \\in R \\right \\} </math>\n\nconsisting of all ''R''-linear combinations of the elements a<sub>i</sub>. As with the case of vector spaces, the submodule of A spanned by any subset of A is the intersection of all submodules containing that subset.\n\n==Closed linear span (functional analysis)==\n\n{{Split section|date=May 2016}}In [[functional analysis]], a closed linear span of a [[Set (mathematics)|set]] of [[vector space|vectors]] is the minimal closed set which contains the linear span of that set.\n\nSuppose that ''X'' is a normed vector space and let ''E'' be any non-empty subset of ''X''. The '''closed linear span''' of ''E'', denoted by <math>\\overline{\\operatorname{Sp}}(E)</math> or <math>\\overline{\\operatorname{Span}}(E)</math>, is the intersection of all the closed linear subspaces of ''X'' which contain ''E''.\n\nOne mathematical formulation of this is\n\n:<math>\\overline{\\operatorname{Sp}}(E)=\\{u\\in X | \\forall\\epsilon>0\\,\\exists x\\in\\operatorname{Sp}(E) : \\|x-u\\|<\\epsilon\\}.</math>\n\nThe closed linear span of the set of functions ''x<sup>n</sup>'' on the interval [0, 1], where ''n'' is a non-negative integer, depends on the norm used. If the [[Lp space#Lp spaces|''L''<sup>2</sup> norm]] is used, then the closed linear span is the [[Hilbert space]] of [[square-integrable function]]s on the interval. But if the [[maximum norm]] is used, the closed linear span will be the space of continuous functions on the interval. In either case, the closed linear span contains functions that are not polynomials, and so are not in the linear span itself. However, the [[cardinality]] of the set of functions in the closed linear span is the [[cardinality of the continuum]], which is the same cardinality as for the set of polynomials.\n\n===Notes===\n\nThe linear span of a set is dense in the closed linear span. Moreover, as stated in the lemma below, the closed linear span is indeed the [[closure (mathematics)|closure]] of the linear span.\n\nClosed linear spans are important when dealing with closed linear subspaces (which are themselves highly important, consider [[Riesz's lemma]]).\n\n===A useful lemma===\n\nLet ''X'' be a normed space and let ''E'' be any non-empty subset of ''X''. Then\n\n(a) <math>\\overline{\\operatorname{Sp}}(E)</math> is a closed linear subspace of ''X'' which contains ''E'',\n\n(b) <math>\\overline{\\operatorname{Sp}}(E)=\\overline{\\operatorname{Sp}(E)}</math>, viz. <math>\\overline{\\operatorname{Sp}}(E)</math> is the closure of <math>\\operatorname{Sp}(E)</math>,\n\n(c) <math>E^\\perp=(\\operatorname{Sp}(E))^\\perp=(\\overline{\\operatorname{Sp}(E)})^\\perp.</math>\n\n(So the usual way to find the closed linear span is to find the linear span first, and then the closure of that linear span.)\n\n==See also==\n*[[Affine hull]]\n*[[Convex hull]]\n\n==Notes==\n<references />\n\n== References ==\n* {{springer|author=M.I. Voitsekhovskii|title=Linear hull|id=L/l059260}}\n* {{cite web |url=https://www.math.ucdavis.edu/~anne/linear_algebra/mat67_course_notes.pdf |title=Linear Algebra - As an Introduction to Abstract Mathematics|last1=Lankham |first1=Isaiah |last2=Nachtergaele |first2=Bruno |author2-link=Bruno Nachtergaele|last3=Schilling |first3=Anne|author3-link=Anne Schilling |publisher=University of California, Davis |date= 13 February 2010 |accessdate=27 September 2011 }}\n* Brian P. Rynne & Martin A. Youngson (2008). ''Linear Functional Analysis'', page 4, Springer {{isbn|978-1848000049}}.\n\n== External links ==\n* [https://www.khanacademy.org/math/linear-algebra/vectors_and_spaces/linear_combinations/v/linear-combinations-and-span Linear Combinations and Span: Understanding linear combinations and spans of vectors], khanacademy.org.\n* {{Cite web |title=Linear combinations, span, and basis vectors |work=Essence of linear algebra |date=August 6, 2016 |via=[[YouTube]] |url=https://www.youtube.com/watch?v=k7RM-ot2NWY&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&index=3 }}\n\n{{Linear algebra}}\n\n[[Category:Abstract algebra]]\n[[Category:Linear algebra]]"
    },
    {
      "title": "List of problems in loop theory and quasigroup theory",
      "url": "https://en.wikipedia.org/wiki/List_of_problems_in_loop_theory_and_quasigroup_theory",
      "text": "In [[mathematics]], especially [[abstract algebra]], [[loop (algebra)|loop]] theory and [[quasigroup]] theory are active research areas with many [[open problems]].  As in other areas of mathematics, such problems are often made public at professional conferences and meetings. Many of the problems posed here first appeared in the ''Loops (Prague)'' conferences and the ''Mile High (Denver)'' conferences.\n\n== Open problems (Moufang loops) ==\n\n=== Abelian by cyclic groups resulting in Moufang loops ===\n<blockquote>\nLet ''L'' be a [[Moufang loop]] with normal [[Abelian group|abelian]] [[subgroup]] (associative subloop) ''M'' of odd order such that ''L''/''M'' is a [[cyclic group]] of order bigger than 3. (i) Is ''L'' a [[Group (mathematics)|group]]? (ii) If the orders of ''M'' and ''L''/''M'' are [[relatively prime]], is L a group?\n</blockquote>\n*''Proposed:'' by Michael Kinyon, based on (Chein and Rajah, 2000)\n*''Comments:'' The assumption that ''L''/''M'' has order bigger than 3 is important, as there is a (commutative) Moufang loop ''L'' of order 81 with normal commutative subgroup of order 27.\n\n=== Embedding CMLs of period 3 into alternative algebras ===\n<blockquote>\nConjecture: Any finite [[Moufang loop|commutative Moufang loop]] of period 3 can be embedded into a commutative [[alternative algebra]].\n</blockquote>\n*''Proposed:'' by Alexander Grishkov at Loops '03, Prague 2003\n\n=== Frattini subloop for Moufang loops ===\n<blockquote>\nConjecture: Let ''L'' be a finite Moufang loop and Φ(''L'') the intersection of all maximal subloops of ''L''. Then Φ(''L'') is a normal nilpotent subloop of ''L''.\n</blockquote>\n*''Proposed:'' by Alexander Grishkov at Loops '11, Třešť 2011\n\n=== Minimal presentations for loops M(G,2) ===\n<blockquote>\nFor a group <math>G</math>, define <math>M(G,2)</math> on <math>G</math> x <math>C_2</math> by\n<math>(g,0)(h,0)=(gh,0)</math>, <math>(g,0)(h,1)=(hg,1)</math>, <math>(g,1)(h,0)=(gh^{-1},1)</math>, <math>(g,1)(h,1)=(h^{-1}g,0)</math>. Find a minimal presentation for the Moufang loop <math>M(G,2)</math> with respect to a [[Presentation (group theory)|presentation]] for <math>G</math>.\n</blockquote>\n*''Proposed:'' by Petr Vojtěchovský at Loops '03, Prague 2003\n*''Comments:'' Chein showed in (Chein, 1974) that <math>M(G,2)</math> is a Moufang loop that is nonassociative if and only if <math>G</math> is nonabelian. Vojtěchovský (Vojtěchovský, 2003) found a minimal presentation for <math>M(G,2)</math> when <math>G</math> is a 2-generated group.\n\n=== Moufang loops of order ''p''<sup>2</sup>''q''<sup>3</sup> and ''pq''<sup>4</sup> ===\n<blockquote>\nLet ''p'' and ''q'' be distinct odd primes. If ''q'' is not congruent to 1 [[Modular arithmetic|modulo]] ''p'', are all Moufang loops of order ''p''<sup>2</sup>''q''<sup>3</sup> groups? What about ''pq''<sup>4</sup>?\n</blockquote>\n*''Proposed:'' by Andrew Rajah at Loops '99, Prague 1999\n*''Comments:'' The former has been solved by Rajah and Chee (2011) where they showed that for distinct odd primes ''p''<sub>1</sub> < ··· < ''p<sub>m</sub>'' < ''q'' < ''r''<sub>1</sub> < ··· < ''r<sub>n</sub>'', all Moufang loops of order ''p''<sub>1</sub><sup>2</sup>···''p<sub>m</sub>''<sup>2</sup>''q''<sup>3</sup>''r''<sub>1</sub><sup>2</sup>···''r<sub>n</sub>''<sup>2</sup> are groups if and only if ''q'' is not congruent to 1 modulo ''p<sub>i</sub>'' for each ''i''.\n\n=== (Phillips' problem) Odd order Moufang loop with trivial nucleus ===\n<blockquote>\nIs there a Moufang loop of odd order with trivial nucleus?\n</blockquote>\n*''Proposed:'' by Andrew Rajah at Loops '03, Prague 2003\n\n=== Presentations for finite simple Moufang loops ===\n<blockquote>\nFind presentations for all nonassociative finite simple Moufang loops in the variety of Moufang loops.\n</blockquote>\n*''Proposed:'' by Petr Vojtěchovský at Loops '03, Prague 2003\n*''Comments:'' It is shown in (Vojtěchovský, 2003) that every nonassociative finite simple Moufang loop is generated by 3 elements, with explicit formulas for the generators.\n\n=== The restricted Burnside problem for Moufang loops ===\n<blockquote>\nConjecture: Let ''M'' be a finite Moufang loop of exponent ''n'' with ''m'' generators. Then there exists a function ''f''(''n'',''m'') such that |''M''| < ''f''(''n'',''m'').\n</blockquote>\n*''Proposed:'' by Alexander Grishkov at Loops '11, Třešť 2011\n*''Comments:'' In the case when n is a prime different from 3 the conjecture was proved by Grishkov. If ''p'' = 3 and ''M'' is commutative, it was proved by Bruck. The general case for ''p'' = 3 was proved by G. Nagy. The case ''n'' = ''p''<sup>''m''</sup> holds by the Grishkov–Zelmanov Theorem.\n\n=== The Sanov and M. Hall theorems for Moufang loops ===\n<blockquote>\nConjecture: Let ''L'' be a finitely generated Moufang loop of exponent 4 or 6. Then ''L'' is finite.\n</blockquote>\n*''Proposed:'' by Alexander Grishkov at Loops '11, Třešť 2011\n\n=== Torsion in free Moufang loops ===\n<blockquote>\nLet MF<sub>''n''</sub> be the [[Free object|free]] Moufang loop with ''n'' generators.\n\nConjecture: MF<sub>3</sub> is [[Torsion (algebra)|torsion]] free but MF<sub>''n''</sub> with ''n''&nbsp;>&nbsp;4 is not.\n</blockquote>\n*''Proposed:'' by Alexander Grishkov at Loops '03, Prague 2003\n\n== Open problems (Bol loops) ==\n\n=== Nilpotency degree of the left multiplication group of a left Bol loop ===\n<blockquote>\nFor a left Bol loop ''Q'', find some relation between the [[Nilpotent group|nilpotency]] degree of the left multiplication group of ''Q'' and the structure of ''Q''.\n</blockquote>\n*''Proposed:'' at Milehigh conference on quasigroups, loops, and nonassociative systems, Denver 2005\n\n=== Are two Bol loops with similar multiplication tables isomorphic? ===\n<blockquote>\nLet <math>(Q,*)</math>, <math>(Q,+)</math> be two [[quasigroup]]s defined on the same underlying [[Set (mathematics)|set]] <math>Q</math>. The distance <math>d(*,+)</math> is the number of pairs <math>(a,b)</math> in <math>Q\\times Q</math> such that <math>a*b\\ne a+b </math>. Call a class of finite quasigroups ''quadratic'' if there is a positive real number <math>\\alpha</math> such that any two quasigroups <math>(Q,*)</math>, <math>(Q,+)</math> of order <math>n</math> from the class satisfying <math>d(*,+) < \\alpha\\,n^2</math> are isomorphic. Are Moufang loops quadratic? Are [[Bol loop]]s quadratic?\n</blockquote>\n*''Proposed:'' by Aleš Drápal at Loops '99, Prague 1999\n*''Comments:'' Drápal proved in (Drápal, 1992) that groups are quadratic with <math>\\alpha=1/9</math>, and in (Drápal, 2000) that 2-groups are quadratic with <math>\\alpha=1/4</math>.\n\n=== Campbell–Hausdorff series for analytic Bol loops ===\n<blockquote>\nDetermine the [[Campbell–Hausdorff series]] for analytic Bol loops.\n</blockquote>\n*''Proposed:'' by M. A. Akivis and V. V. Goldberg at Loops '99, Prague 1999\n*''Comments:'' The problem has been partially solved for local analytic Bruck loops in (Nagy, 2002).\n\n=== Universally flexible loop that is not middle Bol ===\n<blockquote>\nA loop is ''universally flexible'' if every one of its loop isotopes is [[Alternative algebra|flexible]], that is, satisfies (''xy'')''x''&nbsp;=&nbsp;''x''(''yx''). A loop is ''middle Bol'' if every one of its loop isotopes has the antiautomorphic inverse property, that is, satisfies (''xy'')<sup>&minus;1</sup>&nbsp;=&nbsp;''y''<sup>&minus;1</sup>''x''<sup>&minus;1</sup>. Is there a finite, universally flexible loop that is not middle Bol?\n</blockquote>\n*''Proposed:'' by Michael Kinyon at Loops '03, Prague 2003\n\n=== Finite simple Bol loop with nontrivial conjugacy classes ===\n<blockquote>\nIs there a finite simple nonassociative Bol loop with nontrivial conjugacy classes?\n</blockquote>\n*''Proposed:'' by Kenneth W. Johnson and Jonathan D. H. Smith at the 2nd Mile High Conference on Nonassociative Mathematics, Denver 2009\n\n== Open problems (Nilpotency and solvability) ==\n\n=== Niemenmaa's conjecture and related problems ===\n<blockquote>\nLet ''Q'' be a loop whose inner mapping group is nilpotent. Is ''Q'' nilpotent? Is ''Q'' solvable?\n</blockquote>\n*''Proposed:'' at Loops '03 and '07, Prague 2003 and 2007\n*''Comments:'' The answer to the first question is affirmative if ''Q'' is finite (Niemenmaa 2009). The problem is open in the general case.\n\n=== Loops with abelian inner mapping group ===\n<blockquote>\nLet ''Q'' be a loop with abelian inner mapping group. Is ''Q'' nilpotent? If so, is there a bound on the nilpotency class of ''Q''? In particular, can the nilpotency class of ''Q'' be higher than 3?\n</blockquote>\n*''Proposed:'' at Loops '07, Prague 2007\n*''Comments:'' When the inner mapping group Inn(''Q'') is finite and abelian, then ''Q'' is nilpotent (Niemenaa and Kepka). The first question is therefore open only in the infinite case. Call loop ''Q'' of ''Csörgõ type'' if it is nilpotent of class at least 3, and Inn(''Q'') is abelian. No loop of Csörgõ type of nilpotency class higher than 3 is known. Loops of Csörgõ type exist (Csörgõ, 2004), Buchsteiner loops of Csörgõ type exist (Csörgõ, Drápal and Kinyon, 2007), and Moufang loops of Csörgõ type exist (Nagy and Vojtěchovský, 2007). On the other hand, there are no groups of Csörgõ type (folklore), there are no commutative Moufang loops of Csörgõ type (Bruck), and there are no Moufang ''p''-loops of Csörgõ type for ''p''&nbsp;>&nbsp;3 (Nagy and Vojtěchovský, 2007).\n\n=== Number of nilpotent loops up to isomorphism ===\n<blockquote>\nDetermine the number of nilpotent loops of order 24 up to isomorphism.\n</blockquote>\n*''Proposed:'' by Petr Vojtěchovský at the 2nd Mile High Conference on Nonassociative Mathematics, Denver 2009\n*''Comment:'' The counts are known for ''n''&nbsp;<&nbsp;24, see (Daly and Vojtěchovský, 2010).\n\n== Open problems (quasigroups) ==\n\n=== Classification of finite simple paramedial quasigroups ===\n<blockquote>\nClassify the finite simple paramedial quasigroups.\n</blockquote>\n*''Proposed:'' by Jaroslav Ježek and Tomáš Kepka at Loops '03, Prague 2003\n\n=== Existence of infinite simple paramedial quasigroups ===\n<blockquote>\nAre there infinite simple paramedial quasigroups?\n</blockquote>\n*''Proposed:'' by Jaroslav Ježek and Tomáš Kepka at Loops '03, Prague 2003\n\n=== Minimal isotopically universal varieties of quasigroups ===\n<blockquote>\nA variety ''V'' of quasigroups is ''isotopically universal'' if every quasigroup is isotopic to a member of ''V''. Is the variety of loops a minimal isotopically universal variety? Does every isotopically universal variety contain the variety of loops or its parastrophes?\n</blockquote>\n\n*''Proposed:'' by Tomáš Kepka and Petr Němec at Loops '03, Prague 2003\n*''Comments:'' Every quasigroup is isotopic to a loop, hence the variety of loops is isotopically universal.\n\n=== Small quasigroups with quasigroup core ===\n<blockquote>\nDoes there exist a quasigroup ''Q'' of order ''q''&nbsp;=&nbsp;14, 18, 26 or 42 such that the operation * defined on ''Q'' by ''x''&nbsp;*&nbsp;''y''&nbsp;=&nbsp;''y''&nbsp;&minus;&nbsp;''xy'' is a quasigroup operation?\n</blockquote>\n\n*''Proposed:'' by Parascovia Syrbu at Loops '03, Prague 2003\n*''Comments:'' see (Conselo et al., 1998)\n\n=== Uniform construction of Latin squares? ===\n<blockquote>\nConstruct a latin square ''L'' of order ''n'' as follows: Let G = K<sub>n,n</sub> be the complete bipartite graph with distinct weights on its n<sup>2</sup> edges. Let M<sub>1</sub> be the cheapest matching in ''G'', ''M''<sub>2</sub> the cheapest matching in G with M<sub>1</sub> removed, and so on. Each matching ''M''<sub>''i''</sub> determines a permutation p<sub>i</sub> of 1,&nbsp;...,&nbsp;''n''. Let ''L'' be obtained from ''G'' by placing the permutation ''p''<sub>''i''</sub> into row ''i'' of ''L''. Does this procedure result in a uniform distribution on the space of latin squares of order ''n''?\n</blockquote>\n\n*''Proposed:'' by Gábor Nagy at the 2nd Mile High Conference on Nonassociative Mathematics, Denver 2009\n\n== Open problems (miscellaneous) ==\n\n=== Bound on the size of multiplication groups ===\n<blockquote>\nFor a loop ''Q'', let Mlt(Q) denote the multiplication group of ''Q'', that is, the group [[Generating set of a group|generated]] by all left and right translations. Is |Mlt(''Q'')|&nbsp;<&nbsp;''f''(|''Q''|) for some [[Variety (universal algebra)|variety]] of loops and for some [[polynomial]]&nbsp;''f''?\n</blockquote>\n\n*''Proposed:'' at the Milehigh conference on quasigroups, loops, and nonassociative systems, Denver 2005\n\n=== Does every finite alternative loop have 2-sided inverses? ===\n<blockquote>\nDoes every finite [[Alternative algebra|alternative]] loop, that is, every loop satisfying ''x''(''xy'')&nbsp;=&nbsp;(''xx'')''y'' and ''x''(''yy'')&nbsp;=&nbsp;(''xy'')''y'', have 2-sided inverses?\n</blockquote>\n\n*''Proposed:'' by Warren D. Smith\n*''Comments:'' There are infinite alternative loops without 2-sided inverses, cf. (Ormes and Vojtěchovský, 2007)\n\n=== Finite simple nonassociative automorphic loop ===\n<blockquote>\nFind a nonassociative finite simple [[automorphic loop]], if such a loop exists.\n</blockquote>\n\n*''Proposed:'' by Michael Kinyon at Loops '03, Prague 2003\n*''Comments:'' It is known that such a loop cannot be commutative (Grishkov, Kinyon and Nagý, 2013) nor have odd order (Kinyon, Kunen, Phillips and Vojtěchovský, 2013).\n\n=== Moufang theorem in non-Moufang loops ===\n<blockquote>\nWe say that a variety ''V'' of loops satisfies the Moufang theorem if for every loop ''Q'' in ''V'' the following implication holds: for every ''x'', ''y'', ''z'' in ''Q'', if ''x''(''yz'') = (''xy'')''z'' then the subloop generated by ''x'', ''y'', ''z'' is a group. Is every variety that satisfies Moufang theorem contained in the variety of Moufang loops?\n</blockquote>\n\n*''Proposed by:'' Andrew Rajah at Loops '11, Třešť 2011\n\n=== Universality of Osborn loops ===\n<blockquote>\nA loop is ''Osborn'' if it satisfies the identity ''x''((''yz'')''x'') = (''x''<sup>''&lambda;''</sup>\\''y'')(''zx''). Is every Osborn loop universal, that is, is every isotope of an Osborn loop Osborn? If not, is there a nice identity characterizing universal Osborn loops?\n</blockquote>\n\n*''Proposed:'' by Michael Kinyon at Milehigh conference on quasigroups, loops, and nonassociative systems, Denver 2005\n*''Comments:'' Moufang and conjugacy closed loops are Osborn. See (Kinyon, 2005) for more.\n\n== Solved problems ==\n\nThe following problems were posed as open at various conferences and have since been solved.\n\n=== Buchsteiner loop that is not conjugacy closed ===\n<blockquote>\nIs there a [[Buchsteiner loop]] that is not conjugacy closed? Is there a finite simple Buchsteiner loop that is not conjugacy closed?\n</blockquote>\n\n*''Proposed:'' at Milehigh conference on quasigroups, loops, and nonassociative systems, Denver 2005\n*''Solved by:'' Piroska Csörgõ, Aleš Drápal, and Michael Kinyon\n*''Solution:'' The quotient of a Buchsteiner loop by its nucleus is an abelian group of exponent 4. In particular, no nonassociative Buchsteiner loop can be simple. There exists a Buchsteiner loop of order 128 which is not conjugacy closed.\n\n=== Classification of Moufang loops of order 64 ===\n<blockquote>\nClassify nonassociative Moufang loops of order 64.\n</blockquote>\n\n*''Proposed:'' at Milehigh conference on quasigroups, loops, and nonassociative systems, Denver 2005\n*''Solved by:'' Gábor P. Nagy and Petr Vojtěchovský\n*''Solution:'' There are 4262 nonassociative Moufang loops of order 64. They were found by the method of group modifications in (Vojtěchovský, 2006), and it was shown in (Nagy and Vojtěchovský, 2007) that the list is complete. The latter paper uses a [[Linear algebra|linear-algebraic]] approach to Moufang loop [[Abelian extension|extensions]].\n\n=== Conjugacy closed loop with nonisomorphic one-sided multiplication groups ===\n<blockquote>\nConstruct a conjugacy closed loop whose left multiplication group is not isomorphic to its right multiplication group.\n</blockquote>\n\n*''Proposed:'' by Aleš Drápal at Loops '03, Prague 2003\n*''Solved by:'' Aleš Drápal\n*''Solution:'' There is such a loop of order 9. In can be obtained in the\n*[http://www.math.du.edu/loops LOOPS package] by the command <tt>CCLoop(9,1)</tt>.\n\n=== Existence of a finite simple Bol loop ===\n<blockquote>\nIs there a finite simple [[Bol loop]] that is not Moufang?\n</blockquote>\n\n* ''Proposed at:'' Loops '99, Prague 1999\n* ''Solved by:'' Gábor P. Nagy, 2007.\n* ''Solution:'' A simple Bol loop that is not Moufang will be called ''proper''.\n*: There are several families of proper simple Bol loops. A smallest proper simple Bol loop is of order 24 (Nagy 2008).\n*: There is also a proper simple Bol loop of exponent 2 (Nagy 2009), and a proper simple Bol loop of odd order (Nagy 2008).\n* ''Comments:'' The above constructions solved two additional open problems:\n** Is there a finite simple Bruck loop that is not Moufang? Yes, since any proper simple Bol loop of exponent 2 is Bruck.\n** Is every Bol loop of odd order solvable? No, as witnessed by any proper simple Bol loop of odd order.\n\n=== Left Bol loop with trivial right nucleus ===\n<blockquote>\nIs there a finite non-Moufang left [[Bol loop]] with trivial right nucleus?\n</blockquote>\n\n* ''Proposed:'' at Milehigh conference on quasigroups, loops, and nonassociative systems, Denver 2005\n* ''Solved by:'' Gábor P. Nagy, 2007\n* ''Solution:'' There is a finite simple left Bol loop of exponent 2 of order 96 with trivial right nucleus. Also, using an [[Zappa–Szép product|exact factorization]] of the [[Mathieu group]] M<sub>24</sub>, it is possible to construct a non-Moufang simple Bol loop which is a [[isotopy of loops|G-loop]].\n\n=== Lagrange property for Moufang loops ===\n<blockquote>\nDoes every finite Moufang loop have the strong Lagrange property?\n</blockquote>\n\n* ''Proposed:'' by Orin Chein at Loops '99, Prague 1999\n* ''Solved by:'' Alexander Grishkov and Andrei Zavarnitsine, 2003\n* ''Solution:'' Every finite Moufang loop has the strong Lagrange property (SLP). Here is an outline of the proof:\n** According to (Chein et al. 2003), it suffices to show SLP for nonassociative finite simple Moufang loops (NFSML).\n** It thus suffices to show that the order of a maximal subloop of an NFSML L divides the order of L.\n** A countable class of NFSMLs <math>M(q)</math> was discovered in (Paige 1956), and no other NSFMLs exist by (Liebeck 1987).\n** Grishkov and Zavarnitsine matched maximal subloops of loops <math>M(q)</math> with certain subgroups of groups with triality in (Grishkov and Zavarnitsine, 2003).\n\n=== Moufang loops with non-normal commutant ===\n<blockquote>\nIs there a Moufang loop whose commutant is not normal?\n</blockquote>\n\n* ''Proposed:'' by Andrew Rajah at Loops '03, Prague 2003\n* ''Solved by:'' Stephen Gagola III (Gagola 2012)\n* ''Solution:'' No, in every Moufang loop the commutant is a normal subloop. The solution subsumes a conjecture of Doro that a Moufang loop with trivial nucleus has a normal commutant.\n\n=== Quasivariety of cores of Bol loops ===\n<blockquote>\nIs the class of cores of Bol loops a quasivariety?\n</blockquote>\n\n* ''Proposed:'' by Jonathan D. H. Smith and Alena Vanžurová at Loops '03, Prague 2003\n* ''Solved by:'' Alena Vanžurová, 2004.\n* ''Solution:'' No, the class of cores of Bol loops is not closed under subalgebras. Furthermore, the class of cores of groups is not closed under subalgebras. Here is an outline of the proof:\n** Cores of abelian groups are [[Medial magma|medial]], by (Romanowska and Smith, 1985), (Rozskowska-Lech, 1999).\n** The smallest nonabelian group <math>S_3</math> has core containing a sub[[Magma (algebra)|magma]] <math>G</math> of order 4 that is not medial.\n** If <math>G</math> is a core of a Bol loop, it is a core of a Bol loop of order 4, hence a core of an abelian group, a contradiction.\n\n=== Parity of the number of quasigroups up to isomorphism ===\n<blockquote>\nLet I(n) be the number of isomorphism classes of quasigroups of order n. Is I(n) odd for every n?\n</blockquote>\n\n* ''Proposed:'' by Douglas S. Stones at 2nd Mile High Conference on Nonassociative Mathematics, Denver 2009\n* ''Solved by:'' Douglas S. Stones, 2010.\n* ''Solution:'' I(12) is even.  In fact, I(n) is odd for all ''n''&nbsp;&le;&nbsp;17 except&nbsp;12. (Stones 2010)\n\n=== Classification of finite simple paramedial quasigroups ===\n\n<blockquote>\nClassify the finite simple paramedial quasigroups.\n</blockquote>\n\n*''Proposed:'' by Jaroslav Ježek and Tomáš Kepka at Loops '03, Prague 2003.\n*''Solved by:'' Victor Shcherbacov and Dumitru  Pushkashu (2010).\n*''Solution:'' Any finite simple paramedial quasigroup is isotopic to elementary abelian p-group. Such quasigroup can be either a medial unipotent  quasigroup, or a medial commutative distributive quasigroup, or special kind isotope of (φ+ψ)-simple medial  distributive quasigroup.\n\n== See also ==\n* [[Problems in Latin squares]]\n\n==  References ==\n* {{Citation\n | last=Chein | first=Orin | title=Moufang Loops of Small Order I\n | journal=[[Transactions of the American Mathematical Society]] | volume=188 | year=1974 | pages=31–51\n | doi=10.2307/1996765\n | jstor=1996765\n}}.\n* {{Citation\n | last1=Chein | first1=Orin | title=Loops and the Lagrange property\n | last2=Kinyon | first2=Michael K. | last3=Rajah | first3=Andrew | last4=Vojtěchovský | first4=Petr\n | journal=[[Results in Mathematics]] | volume=43 | year=2003 | pages=74–78\n | arxiv=math/0205141 | doi=10.1007/bf03322722\n}}.\n* {{Citation\n | last1=Chein | first1=Orin | title=Possible orders of nonassociative Moufang loops\n | last2=Rajah | first2=Andrew\n | journal=Commentationes Mathematicae Universitatis Carolinae | volume=41 | year=2000 | pages=237–244 | issue=2\n}}.\n* {{Citation\n | last1=Conselo | first1=E. | first2=S. | last2=Conzales | first3=V. | last3=Markov | first4=A. | last4=Nechaev\n | title=Recursive MDS-codes and recursively differentiable quasigroups\n | journal=Diskretnaia Matematika | volume=10 | year=1998 | issue=2 | pages=3–29\n}}.\n* {{Citation\n | last1=Daly | first1=Dan | first2=Petr | last2=Vojtěchovský\n | title=Enumeration of nilpotent loops via cohomology\n | journal=[[Journal of Algebra]] | year=2009 | volume=322 | issue=11 | pages=4080–4098\n | doi=10.1016/j.jalgebra.2009.03.042\n| arxiv=1509.05713 }}.\n* {{Citation\n | last=Drápal | first=Aleš | title=How far apart can the group multiplication tables be?\n | journal=[[European Journal of Combinatorics]] | volume=13 | year=1992 | issue=5 | pages=335–343\n | doi=10.1016/S0195-6698(05)80012-5\n}}.\n* {{Citation\n | last=Drápal | first=Aleš | title=Non-isomorphic 2-groups coincide in at most three quarters of their multiplication tables\n | journal=[[European Journal of Combinatorics]] | volume=21 | year=2000\n | issue=3 | pages=301–321\n | doi=10.1006/eujc.1999.0347\n}}\n* {{Citation\n | last=Gagola III | first=Stephen | title=A Moufang loop's commutant\n | journal=[[Mathematical Proceedings of the Cambridge Philosophical Society]] | publisher=\n|volume = 152| issue=2|year = 2012|pages = 193–206|doi = 10.1017/S0305004111000181|bibcode=2012MPCPS.152..193G}}\n* {{Citation\n | last1=Grishkov | first1=Alexander N. | last2=Zavarnitsine | first2=Andrei V.\n | title=Lagrange's theorem for Moufang loops | journal=[[Mathematical Proceedings of the Cambridge Philosophical Society]]\n | volume=139 | year=2005 | issue=1 | pages=41–57\n | doi=10.1017/S0305004105008388\n| bibcode=2005MPCPS.139...41G}}\n* {{Citation\n | last1=Grishkov | first1=Alexander N. | last2=Kinyon | first2=Michael | last3=Nagý | first3=Gabor\n | title=Solvability of commutative automorphic loops | journal=[[Proceedings of the American Mathematical Society]]\n | volume= 142| issue=9 | year=2013 | arxiv=1111.7138 | doi = 10.1090/s0002-9939-2014-12053-3 | pages=3029–3037\n}}\n* {{Citation\n | last1=Kinyon | first1=Michael K.\n | title=A survey of Osborn loops | publisher=invited talk at Milehigh conference on quasigroups, loops and nonassociative systems, Denver, 2005 | url=http://web.cs.du.edu/~petr/milehigh/2005/kinyon_talk.pdf\n}}\n* {{Citation\n | last1=Kinyon | first1=Michael | last2=Kunen | first2=Kenneth | last3=Phillips | first3=J.D. | last4=Vojtěchovský | first4=Petr\n | title=The structure of automorphic loops | journal=[[Transactions of the American Mathematical Society]] \n | volume=368 | year=2016 | arxiv=1210.1642 | doi = 10.1090/tran/6622 | pages=8901–8927\n}}\n* {{Citation\n | last=Liebeck | first=M. W. | title=The classification of finite simple Moufang loops\n | journal=[[Mathematical Proceedings of the Cambridge Philosophical Society]] | year=1987 | volume=102\n | issue=1 | pages=33–47\n | doi=10.1017/S0305004100067025\n| bibcode=1987MPCPS.102...33L}}\n* {{Citation\n | last=Nagy | first=Gábor P. | title=The Campbell–Hausdorff series of local analytic Bruck loops\n | journal=Abh. Math. Sem. Univ. Hamburg | volume=72 | year=2002\n | issue=1 | pages=79–87\n | doi=10.1007/BF02941666\n}}.\n* {{Citation\n | last=Nagy | first=Gábor P. | last2=Vojtěchovský | first2=Petr\n | title=Moufang loops of order 64 and 81 | journal=Journal of Symbolic Computation | publisher=to appear | year=2007\n | doi=10.1016/j.jsc.2007.06.004\n | volume=42\n | issue=9\n | pages=871–883\n}}.\n* {{Citation\n | last=Nagy | first=Gábor P. | title=A class of simple proper Bol loops | journal=Manuscripta Mathematica | volume=127 | year=2008 | issue=1 | pages=81–88 \n | doi=10.1007/s00229-008-0188-5\n| arxiv=math/0703919 }}.\n* {{Citation\n | last=Nagy | first=Gábor P. | title=A class of finite simple Bol loops of exponent 2 | journal=[[Transactions of the American Mathematical Society]] | volume=361 | year=2009 | issue=10 | pages=5331–5343 \n | doi=10.1090/S0002-9947-09-04646-7\n| arxiv=0709.4544 }}.\n* {{Citation\n | last=Niemenmaa | first=Markku | title=Finite loops with nilpotent inner mapping groups are centrally nilpotent \n | journal=Bulletin of the Australian Mathematical Society | year=2009 | volume=79 | issue=1 | pages=109–114\n | doi=10.1017/S0004972708001093\n}}\n* {{Citation\n | last1=Ormes | first1=Nicholas | last2=Vojtěchovský | first2=Petr\n | title=Powers and alternative laws | journal=Commentationes Mathematicae Universitatis Carolinae\n | volume=48 | year=2007 | issue=1 | pages=25–40 \n}}.\n* {{Citation\n | last=Paige | first=L. | title=A class of simple Moufang loops\n | journal=Proceedings of the American Mathematical Society | volume=7 | issue=3 | pages=471–482 | year=1956\n | doi=10.2307/2032757\n | jstor=2032757\n}}.\n* {{Citation\n | last1=Rajah | first1=Andrew | last2=Chee | first2=Wing Loon | title=Moufang loops of odd order ''p''<sub>1</sub><sup>2</sup>''p''<sub>2</sub><sup>2</sup>···''p<sub>n</sub>''<sup>2</sup>''q''<sup>3</sup> | journal=International Journal of Algebra | volume=5 | issue=20 | pages=965–975 | year=2011\n}}.\n* {{Citation\n | last1=Rivin | first1=Igor | first2=Ilan | last2=Vardi | first3=Paul | last3=Zimmerman\n | title=The ''n''-queens problem | journal=[[American Mathematical Monthly]] | volume=101 | year=1994 | issue=7 | pages=629–639\n | doi=10.2307/2974691\n | jstor=2974691\n}}.\n* {{Citation\n | last1=Romanowska | first1=Anna | last2=Smith | first2=Jonathan D. H.\n | title=Modal Theory | publisher=Heldermann Verlag, Berlin | year=1985\n}}.\n* {{Citation\n | last1=Rozskowska-Lech | first1=B. | title=A representation of symmetric idempotent and entropic groupoids\n | journal=Demonstr. Math. | volume=32 |year=1999 | pages=248–262\n}}.\n* {{Citation\n | last1=Shcherbacov | first1=V.A.| last2=Pushkashu | first2=D.I.| title=On the structure of finite paramedial quasigroups\n | journal=Comment. Math. Univ. Carolin. | volume=51 |year=2010 | pages=357–370}}.\n* {{Citation\n | last1=Stones | first1=D. S. | title=The parity of the number of quasigroups\n | journal=[[Discrete Mathematics (journal)|Discrete Mathematics]] | volume=310 | issue=21 |year=2010 | pages=3033–3039\n | doi=10.1016/j.disc.2010.06.027\n}}.\n* {{Citation\n | last=Vojtěchovský | first=Petr | title=Generators for finite simple Moufang loops\n | journal=Journal of Group Theory | volume=6\n | issue=2 | year=2003 | pages=169–174\n | arxiv=math/0701701\n | doi=10.1515/jgth.2003.012\n}}.\n* {{Citation\n | last=Vojtěchovský | first=Petr | title=The smallest Moufang loop revisited\n | journal=Results in Mathematics | volume=44 | year=2003 | pages=189–193\n | arxiv=math/0701706 | doi=10.1007/bf03322924\n}}.\n* {{Citation\n | last=Vojtěchovský | first=Petr | title=Toward the classification of Moufang loops of order 64\n | journal=[[European Journal of Combinatorics]] | volume=27 | issue=3 | year=2006 | pages=444–460 | arxiv=math/0701712\n | doi=10.1016/j.ejc.2004.10.003\n}}.\n\n== External links ==\n* [http://www.karlin.mff.cuni.cz/~loops99 Loops '99 conference]\n* [http://www.karlin.mff.cuni.cz/~loops03 Loops '03 conference]\n* [http://www.karlin.mff.cuni.cz/~loops07 Loops '07 conference]\n* [http://www.karlin.mff.cuni.cz/~loops11 Loops '11 conference]\n* [http://www.math.du.edu/milehigh Milehigh conferences on nonassociative mathematics]\n* [http://www.math.du.edu/loops LOOPS package for GAP]\n* [http://www.math.du.edu/plq Problems in Loop Theory and Quasigroup Theory]\n\n[[Category:Unsolved problems in mathematics]]\n[[Category:Abstract algebra]]"
    },
    {
      "title": "Locally finite operator",
      "url": "https://en.wikipedia.org/wiki/Locally_finite_operator",
      "text": "{{unreferenced|date=November 2012}}\n\nIn [[mathematics]], a [[linear operator]] <math>f: V\\to V</math> is called '''locally finite''' if the [[linear space|space]] <math>V</math> is the union of a family of finite-dimensional <math>f</math>-[[invariant subspace]]s.\n\nIn other words, there exists a family <math>\\{ V_i\\vert i\\in I\\}</math> of linear subspaces of <math>V</math>,  such that we have the following:\n* <math>\\bigcup_{i\\in I} V_i=V</math>\n* <math>(\\forall i\\in I) f[V_i]\\subseteq V_i</math>\n* Each <math>V_i</math> is finite-dimensional.\n\n==Examples==\n* Every linear operator on a finite-dimensional space is trivially locally finite.\n* Every [[diagonalizable]] (i.e. there exists a [[Basis (linear algebra)|basis]] of <math>V</math> whose elements are all [[eigenvector]]s of <math>f</math>) linear operator is locally finite, because it is the union of subspaces spanned by finitely many eigenvectors of <math>f</math>.\n\n{{linear-algebra-stub}}\n\n\n[[Category:Abstract algebra]]\n[[Category:Functions and mappings]]\n[[Category:Linear algebra]]\n[[Category:Transformation (function)]]"
    },
    {
      "title": "Lulu smoothing",
      "url": "https://en.wikipedia.org/wiki/Lulu_smoothing",
      "text": "In [[signal processing]], '''Lulu [[smoothing]]''' is a [[nonlinear]] mathematical technique for removing impulsive [[Noise (signal processing)|noise]] from a data sequence such as a [[time series]]. It is a nonlinear equivalent to taking a [[moving average]] (or other smoothing technique) of a time series, and is similar to other [[nonlinear smoothing]] techniques, such as Tukey or [[Median filter|median smoothing]].<ref name=\"Tukey\" />[[File:LUSmoother width=1.png|thumb|LU smoother of width 1 applied to a noisy sequence]] LULU smoothers are compared in detail to median smoothers by Jankowitz and found to be superior in some aspects, particularly in mathematical properties like [[idempotence]].<ref name=\"jankowitz2007some\" />\n\n== Properties ==\nLulu operators have a  number of attractive mathematical properties, among them [[idempotence]] – meaning that repeated application of the operator yields the same result as a single application – and co-idempotence. \nAn interpretation of idempotence is that: 'Idempotence means that there is no “noise” left in the smoothed data and co-idempotence means that there is no “signal” left in the residual.'<ref name=\"conradie2006exact\" />\n\nWhen studying smoothers there are four properties that are useful to optimize:<ref name=\"rohwer2005nonlinear\" />\n\n# Effectiveness\n# Consistency\n# Stability\n# Efficiency\n\nThe operators can also be used to decompose a signal into various subcomponents similar to wavelet or Fourier decomposition.<ref name=\"fabris2009lulu\" />\n\n== History ==\nLulu smoothers were discovered  by C. H. Rohwer and have been studied for the last 30 years.<ref name=\"Rohwer1989\" /><ref name=\"Rohwer\" /> Their exact and asymptotic distributions have been derived.<ref name=\"conradie2006exact\" />\n\n== Operation ==\nApplying a Lulu smoother consists of repeated applications of the min and max operators over a given subinterval of the data.\nAs with other smoothers, a width or interval must be specified. The Lulu smoothers are composed of repeated applications of the ''L'' (lower) and ''U'' (Upper) operators, which are defined as follows:\n\n=== L operator ===\nFor an L operator of width ''n'' over an infinite sequence of ''x''s (..., ''x''<sub>''j''</sub>, ''x''<sub>''j''+1</sub>,...), the operation on ''x''<sub>''j''</sub> is calculated as follows:\n\n# Firstly we create (''n''&nbsp;+&nbsp;1) mini-sequences of length (''n''&nbsp;+&nbsp;1) each.  Each of these mini-sequences contains the element ''x''<sub>''j''</sub>. For example, for width 1, we create 2 mini-sequences of length 2 each. For width 1 these mini sequences are (''x''<sub>''j''&minus;1</sub>, ''x''<sub>''j''</sub>) and (''x''<sub>''j''</sub>, ''x''<sub>''j''+1</sub>). For width 2, the mini-sequences are (''x''<sub>''j''&minus;2</sub>, ''x''<sub>''j''&minus;1</sub>, ''x''<sub>''j''</sub>), (''x''<sub>''j''&minus;1</sub>, ''x''<sub>''j''</sub>, ''x''<sub>''j''+1</sub>) and (''x''<sub>''j''</sub>, ''x''<sub>''j''+1</sub>, ''x''<sub>''j''+2</sub>). For width 2, we refer to these mini-sequences as seq<sub>&minus;1</sub>, seq<sub>0</sub> and seq<sub>+1</sub>\n# Then we take the minimum of each of the mini sequences. Again for width 2 this gives: (Min(seq<sub>&minus;1</sub>), Min(seq<sub>0</sub>), Min(seq<sub>+1</sub>)). This gives us (''n''&nbsp;+&nbsp;1) numbers for each point.\n# Lastly we take the maximum of (the minimums of the mini sequences), or Max(Min(seq<sub>&minus;1</sub>), Min(seq<sub>0</sub>), Min(seq<sub>+1</sub>)) and this becomes ''L''(''x''<sub>''j''</sub>)\n\nThus for width 2, the ''L'' operator is:\n\n: ''L''(''x''<sub>''j''</sub>) =  Max(Min(seq<sub>&minus;1</sub>), Min(seq<sub>0</sub>), Min(seq<sub>+1</sub>))\n\n=== U Operator ===\nThis is identical to  the L operator, except that the order of Min and Max is reversed, i.e. for width 2:\n\n: ''U''(''x''<sub>''j''</sub>) =  Min(Max(seq<sub>&minus;1</sub>), Max(seq<sub>0</sub>), Max(seq<sub>+1</sub>))\n\n=== Examples ===\nExamples of the ''U'' and ''L'' operators, as well as combined ''UL'' and ''LU'' operators on a sample data set are shown in the following figures.\n\n[[File:LSmoother width=1.png|thumb|center|upright=2.0|L Smoother width 1]]\n[[File:USmoother width=1.png|thumb|center|upright=2.0|U Smoother width 1]]\n\nIt can be seen that the results of the ''UL'' and ''LU'' operators can be different. The combined operators are very effective at removing impulsive noise, the only cases where the noise is not removed effectively is where we get multiple noise signals very close together, in which case the filter 'sees' the multiple noises as part of the signal.\n\n[[File:LUSmoother width=1.png|thumb|center|upright=2.0|LU smoother width 1]]\n[[File:ULSmoother width=1.png|thumb|center|upright=2.0|UL smoother width 1]]\n\n== References ==\n<!--- See http://en.wikipedia.org/wiki/Wikipedia:Footnotes on how to create references using <ref></ref> tags which will then appear here automatically -->\n{{Reflist|\nrefs=\n\n<ref name=\"Rohwer1989\">\n{{cite journal       \n  |title=Idempotent one-sided approximation of median smoothers\n  |author=Rohwer, CH\n  |journal=Journal of Approximation Theory\n  |volume=58\n  |number=2\n  |pages=151–163\n  |year=1989\n  |doi=10.1016/0021-9045(89)90017-8\n}}\n</ref>\n<ref name=\"Rohwer\">\n{{cite journal       \n  |title=Projections and separators\n  |author=Rohwer, CH\n  |journal=Quaestiones Mathematicae\n  |volume=22\n  |number=2\n  |pages=219–230\n  |year=1999\n  |doi=10.1080/16073606.1999.9632077\n}}\n</ref>\n<ref name=\"rohwer2005nonlinear\">\n{{cite book\n  |title=Nonlinear smoothing and multiresolution analysis\n  |author=Rohwer, Carl\n  |volume=150\n  |year=2005\n  |publisher=Birkhauser Basel\n}}\n</ref>\n<ref name=\"Tukey\">\n{{cite journal       \n  |title=Nonlinear (nonsuperposable) methods for smoothing data\n  |author=Tukey, JW\n  |journal=Cong. Rec.\n  |pages=673\n  |year=1974\n  |publisher=EASCON\n}}\n</ref>\n\n<ref name=\"conradie2006exact\">\n{{cite journal\n  |title=Exact and asymptotic distributions of LULU smoothers\n  |author=Conradie, WJ and de Wet, T. and Jankowitz, M.\n  |journal=Journal of Computational and Applied Mathematics\n  |volume=186\n  |number=1\n  |pages=253–267\n  |year=2006\n  |doi=10.1016/j.cam.2005.03.073\n\n}}\n</ref>\n<ref name=\"jankowitz2007some\">\n{{cite thesis\n  |title=Some statistical aspects of LULU smoothers\n  |author=Jankowitz, M.D.\n  |type=PhD Thesis\n  |year=2007\n  |publisher=University of Stellenbosch\n}}\n</ref>\n\n<ref name=\"fabris2009lulu\">\n{{cite thesis\n  |title=LULU operators on multidimensional arrays and applications\n  |author=Fabris-Rotelli, Inger Nicolette\n  |type=MSc Thesis\n  |year=2009\n  |publisher=University of Pretoria\n}}\n</ref>\n\n}}\n\n[[Category:Abstract algebra]]\n[[Category:Theoretical computer science]]\n[[Category:Binary operations]]\n[[Category:Statistical signal processing]]"
    },
    {
      "title": "Maximal common divisor",
      "url": "https://en.wikipedia.org/wiki/Maximal_common_divisor",
      "text": "In [[abstract algebra]], particularly [[ring theory]], '''maximal common divisors''' are an abstraction of the [[number theory]] concept of [[greatest common divisor]] (GCD). This definition is slightly more general than GCDs, and may exist in rings in which GCDs do not. Halter-Koch (1998) provides the following definition.<ref name=\"Ideal Systems\">{{cite book|title=Ideal systems|publisher=Marcel Dekker|first=Franz|last=Halter-Koch|year=1998|isbn=0-8247-0186-0}}</ref>\n\n''d''&nbsp;∈&nbsp;''H'' is a maximal common divisor of a subset, ''B''&nbsp;⊂&nbsp;''H'', if the following criteria are met:\n# ''d''|''b'' for all ''b''&nbsp;∈&nbsp;''B''\n# Suppose ''c''&nbsp;∈&nbsp;''H'' ''d''|''c'' and ''c''|''b'' for all ''b''&nbsp;∈&nbsp;''B''. Then <math>c \\simeq d</math>.\n\n==References==\n{{Reflist}}\n\n[[Category:Abstract algebra]]\n\n\n{{algebra-stub}}"
    },
    {
      "title": "Minimal ideal",
      "url": "https://en.wikipedia.org/wiki/Minimal_ideal",
      "text": "In the branch of [[abstract algebra]] known as [[ring theory]], a '''minimal right ideal''' of a [[ring (mathematics)|ring]] ''R'' is a nonzero [[right ideal]] which contains no other nonzero right ideal. Likewise, a '''minimal left ideal''' is a nonzero left ideal of ''R'' containing no other nonzero left ideals of ''R'', and a '''minimal ideal''' of ''R'' is a nonzero ideal containing no other nonzero two-sided ideal of ''R''. {{harv|Isaacs|2009|loc=p.&nbsp;190}}\n\nIn other words, minimal right ideals are [[minimal element]]s of the [[poset]] of nonzero right ideals of ''R'' ordered by inclusion.  The reader is cautioned that outside of this context, some posets of ideals may admit the zero ideal, and so the zero ideal could potentially be a minimal element in that poset. This is the case for the poset of [[prime ideal]]s of a ring, which may include the zero ideal as a [[minimal prime ideal]].\n\n==Definition==\nThe definition of a minimal right ideal ''N'' of a ring ''R'' is equivalent to the following conditions:\n*''N'' is nonzero and if ''K'' is a right ideal of ''R'' with {{nowrap|{0} ⊆ ''K'' ⊆ ''N''}}, then either {{nowrap|1=''K'' = {0}{{null}}}} or {{nowrap|1=''K'' = ''N''}}.\n*''N'' is a [[simple module|simple]] right ''R''-module.\n\nMinimal right ideals are the [[duality (mathematics)|dual notion]] to [[maximal ideal|maximal right ideals]].\n\n==Properties==\nMany standard facts on minimal ideals can be found in standard texts such as {{harv|Anderson|Fuller|1992}}, {{harv|Isaacs|2009}}, {{harv|Lam|2001}}, and {{harv|Lam|1999}}.\n\n* In a [[ring with unity]], [[maximal ideal|maximal right ideals]] always exist. In contrast, minimal right, left, or two-sided ideals in a ring with unity need not exist.\n* The right [[Socle (mathematics)#Socle of a module|socle of a ring]] <math>\\mathrm{soc}(R_R)</math> is an important structure defined in terms of the minimal right ideals of ''R''.\n* Rings for which every right ideal contains a minimal right ideal are exactly the rings with an essential right socle.\n* Any right [[Artinian ring]] or right [[Kasch ring]] has a minimal right ideal.\n* [[domain (ring theory)|Domains]] that are not [[division ring]]s have no minimal right ideals.\n* In rings with unity, minimal right ideals are necessarily [[principal ideal|principal right ideals]], because for any nonzero ''x'' in a minimal right ideal ''N'', the set ''xR'' is a nonzero right ideal of ''R'' inside ''N'', and so {{nowrap|1=''xR'' = ''N''}}.\n* '''Brauer's lemma:''' Any minimal right ideal ''N'' in a ring ''R'' satisfies {{nowrap|1=''N''<sup>2</sup> = {0}{{null}}}} or {{nowrap|1=''N'' = ''eR''}} for some [[idempotent element (ring theory)|idempotent element]] ''e'' of ''R''. {{harv|Lam|2001|loc=p.&nbsp;162}}\n* If ''N''<sub>1</sub> and ''N''<sub>2</sub> are nonisomorphic minimal right ideals of ''R'', then the product {{nowrap|1=''N''<sub>1</sub>''N''<sub>2</sub>}} equals {0}.\n* If ''N''<sub>1</sub> and ''N''<sub>2</sub> are distinct minimal ideals of a ring ''R'', then {{nowrap|1=''N''<sub>1</sub>''N''<sub>2</sub> = {0}.}}\n* A [[simple ring]] with a minimal right ideal is a [[semisimple ring]].\n* In a [[semiprime ring]], there exists a minimal right ideal if and only if there exists a minimal left ideal. {{harv|Lam|2001|loc=p.&nbsp;174}}\n\n==Generalization==\nA nonzero submodule ''N'' of a right module ''M'' is called a '''minimal submodule''' if it contains no other nonzero submodules of ''M''. Equivalently, ''N'' is a nonzero submodule of ''M'' which is a [[simple module]]. This can also be extended to [[bimodules]] by calling a nonzero sub-bimodule ''N'' a '''minimal sub-bimodule''' of ''M'' if ''N'' contains no other nonzero sub-bimodules.\n\nIf the module ''M'' is taken to be the right ''R''-module ''R''<sub>''R''</sub>, then clearly the minimal submodules are exactly the minimal right ideals of ''R''. Likewise, the minimal left ideals of ''R'' are precisely the minimal submodules of the left module <sub>''R''</sub>''R''. In the case of two-sided ideals, we see that the minimal ideals of ''R'' are exactly the minimal sub-bimodules of the bimodule <sub>''R''</sub>''R''<sub>''R''</sub>.\n\nJust as with rings, there is no guarantee that minimal submodules exist in a module. Minimal submodules can be used to define the [[Socle (mathematics)#Socle of a module|socle of a module]].\n\n==References==\n{{Reflist}}\n*{{citation |last1=Anderson |first1=Frank W.  |last2=Fuller |first2=Kent R.  |title=Rings and categories of modules   |series=[[Graduate Texts in Mathematics]]   |volume=13   |edition=2   |publisher=Springer-Verlag  |place=New York   |year=1992   |pages=x+376   |isbn=0-387-97845-3  |mr=1245487 }}\n*{{citation |last=Isaacs |first=I. Martin  |title=Algebra: a graduate course |series=[[Graduate Studies in Mathematics]] |volume=100 |origyear=1994 |publisher=American Mathematical Society |place=Providence, RI |year=2009 |pages=xii+516 |isbn=978-0-8218-4799-2 |MR=2472787}}\n*{{citation | last=Lam | first=Tsit-Yuen | title=Lectures on modules and rings | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Graduate Texts in Mathematics No. 189 | isbn=978-0-387-98428-5 | mr=1653294 | year=1999}}\n*{{citation  |last=Lam |first=T. Y.   |title=A first course in noncommutative rings  |series=Graduate Texts in Mathematics   |volume=131   |edition=2   |publisher=Springer-Verlag   |place=New York   |year=2001   |pages=xx+385   |isbn=0-387-95183-0   |mr=1838439 }}\n\n==External links==\n* http://www.encyclopediaofmath.org/index.php/Minimal_ideal\n\n<!--- Categories --->\n[[Category:Abstract algebra]]\n[[Category:Ring theory]]\n[[Category:Ideals]]"
    },
    {
      "title": "Multilinear form",
      "url": "https://en.wikipedia.org/wiki/Multilinear_form",
      "text": "In [[abstract algebra]] and [[multilinear algebra]], a '''multilinear form''' on <math>V</math> is a [[Map (mathematics)|map]] of the type <blockquote><math>f: V^k \\to K  </math>,</blockquote>where <math>V</math> is a [[vector space]] over the [[field (mathematics)|field]] <math>K</math> (or more generally, a [[Module (mathematics)|module]] over a [[commutative ring]]), that is separately ''K''-[[linear]] in each of its <math>k</math> arguments.<ref>{{MathWorld|title=Multilinear Form|urlname=MultilinearForm}}</ref> (The rest of this article, however, will only consider multilinear forms on finite-dimensional vector spaces.)\n\nA multilinear ''k''-form on <math>V</math> over <math>\\mathbf{R}</math> is called a ('''covariant''') '''''k''-tensor''', and the vector space of such forms is usually denoted <math>\\mathcal{T}^k(V)</math> or <math>\\mathcal{L}^k(V)</math>.<ref>Many authors use the opposite convention, writing <math>\\mathcal{T}^k(V)</math> to denote the contravariant ''k''-tensors on <math>V</math> and <math>\\mathcal{T}_k(V)</math> to denote the covariant ''k''-tensors on <math>V</math>.</ref>\n\n== Tensor product ==\nGiven ''k''-tensor <math>f\\in\\mathcal{T}^k(V)</math> and ''ℓ''-tensor <math>g\\in\\mathcal{T}^\\ell(V)</math>, a product <math>f\\otimes g\\in\\mathcal{T}^{k+\\ell}(V)</math>, known as the '''tensor product''', can be defined by the property<blockquote><math>(f\\otimes g)(v_1,\\ldots,v_k,v_{k+1},\\ldots, v_{k+\\ell})=f(v_1,\\ldots,v_k)g(v_{k+1},\\ldots, v_{k+\\ell})</math>,</blockquote>for all <math>v_1,\\ldots,v_{k+\\ell}\\in V</math>.  The [[tensor product]] of multilinear forms is not commutative; however it is bilinear and associative: <blockquote><math>f\\otimes(ag_1+bg_2)=a(f\\otimes g_1)+b(f\\otimes g_2)</math>, <math>(af_1+bf_2)\\otimes g=a(f_1\\otimes g)+b(f_2\\otimes g)</math>, and</blockquote><blockquote><math>(f\\otimes g)\\otimes h=f\\otimes (g\\otimes h)</math>.</blockquote>If <math>(v_1,\\ldots, v_n)</math> forms a basis for ''n''-dimensional vector space <math>V</math> and <math>(\\phi^1,\\ldots,\\phi^n)</math> is the corresponding dual basis for the dual space <math>V^*=\\mathcal{T}^1(V)</math>, then the products <math>\\phi^{i_1}\\otimes\\cdots\\otimes\\phi^{i_k}</math>, with <math>1\\le i_1,\\ldots,i_k\\le n</math> form a basis for <math>\\mathcal{T}^k(V)</math>.  Consequently, <math>\\mathcal{T}^k(V)</math> has dimensionality <math>n^k</math>.\n\n==Examples==\n\n===Bilinear forms===\n\n''Main article: [[Bilinear form|Bilinear forms]]''\n\nIf <math>k=2\n</math>, <math>f:V\\times V\\to K</math> is referred to as a '''bilinear form'''.  A familiar and important example of a (symmetric) bilinear form is the [[Dot product|standard inner product]] (dot product) of vectors.\n===Alternating multilinear forms===\n\n''Main article: [[Alternating multilinear map|Alternating multilinear maps]]''\n\nAn important class of multilinear forms are the '''alternating multilinear forms''', which have the additional property that<ref name=\":0\">{{Cite book|title=An Introduction to Manifolds|last=Tu|first=Loring W.|publisher=Springer|year=2011|isbn=978-1-4419-7399-3|edition=2nd|location=New York|pages=22–23}}</ref><blockquote><math>f(x_{\\sigma(1)},\\ldots, x_{\\sigma(k)}) = \\mathrm{sgn}(\\sigma)f(x_1,\\ldots, x_k) </math>,</blockquote>where <math>\\sigma:\\mathbf{N}_k\\to\\mathbf{N}_k</math> is a [[permutation]] and <math>\\mathrm{sgn}(\\sigma)</math> denotes its [[Sign of a permutation|sign]] (+1 if even, –1 if odd).  As a consequence, [[Alternating multilinear map|alternating]] multilinear forms are antisymmetric with respect to swapping of any two arguments (i.e., <math>\\sigma(p)=q,\\sigma(q)=p </math> and <math>\\sigma(i)=i, 1\\le i\\le k, i\\neq p,q </math>):<blockquote><math>f(x_1,\\ldots, x_p,\\ldots, x_q,\\ldots, x_k) = -f(x_1,\\ldots, x_q,\\ldots, x_p,\\ldots, x_k) </math>.</blockquote>With the additional hypothesis that the [[Characteristic (field)|characteristic of the field]] <math>K</math> is not 2, setting <math>x_p=x_q=x </math> implies as a corollary that <math>f(x_1,\\ldots, x,\\ldots, x,\\ldots, x_k) = 0 </math>; that is, the form has a value of 0 whenever two of its arguments are equal.  Note, however, that some authors<ref>{{Cite book|title=Finite-Dimensional Vector Spaces|last=Halmos|first=Paul R.|publisher=Van Nostrand|year=1958|isbn=0-387-90093-4|edition=2nd|location=New York|pages=50}}</ref> use this last condition as the defining property of alternating forms.  This definition implies the property given at the beginning of the section, but as noted above, the converse implication holds only when <math>\\mathrm{char}(K)\\neq 2 </math>.\n\nAn alternating multilinear ''k''-form on <math>V</math> over <math>\\mathbf{R}</math> is called a  '''multicovector of degree ''k''''' or '''''k''-covector''', and the vector space of such alternating forms, a subspace of <math>\\mathcal{T}^k(V)</math>, is generally denoted <math>\\mathcal{A}^k(V)</math>, or, using the notation for the isomorphic ''k''th [[exterior power]] of <math>V^*</math>(the [[dual space]] of <math>V</math>), <math display=\"inline\">\\bigwedge^k V^*</math>.<ref>Spivak uses <math>\\Omega^k(V)</math> for the space of ''k''-covectors on <math>V</math>.  However, this notation is more commonly reserved for the space of differential ''k''-forms on <math>V</math>.  In this article, we use <math>\\Omega^k(V)</math> to mean the latter.</ref>  Note that linear functionals (multilinear 1-forms over <math>\\mathbf{R}</math>) are trivially alternating, so that <math>\\mathcal{A}^1(V)=\\mathcal{T}^1(V)=V^*</math>, while, by convention, 0-forms are defined to be scalars: <math>\\mathcal{A}^0(V)=\\mathcal{T}^0(V)=\\mathbf{R}</math>.\n\nThe [[determinant]] on <math>n\\times n</math> matrices, viewed as an <math>n</math> argument function of the column vectors, is an important example of an alternating multilinear form.\n\n==== Wedge product ====\nThe tensor product of alternating multilinear forms is, in general, no longer alternating.  However, by summing over all permutations of the tensor product, taking into account the parity of each term, the '''wedge product''' (<math>\\wedge</math>) of multicovectors can be defined, so that if <math>f\\in\\mathcal{A}^k(V)</math> and <math>g\\in\\mathcal{A}^\\ell(V)</math>, then <math>f\\wedge g\\in\\mathcal{A}^{k+\\ell}(V)</math>:<blockquote><math>(f\\wedge g)(v_1,\\ldots, v_{k+\\ell})=\\frac{1}{k!\\ell!}\\sum_{\\sigma\\in S_{k+\\ell}}\n(\\mathrm{sgn}(\\sigma))f(v_{\\sigma(1)},\\ldots,v_{\\sigma(k)})g(v_{\\sigma(k+1)}\n,\\ldots,v_{\\sigma(k+\\ell)})</math>,</blockquote>where the sum is taken over the set of all permutations over <math>k+\\ell</math> elements, <math>S_{k+\\ell}</math>.  The [[wedge product]] is bilinear, associative, and anticommutative: if <math>f\\in\\mathcal{A}^k(V)</math> and <math>g\\in\\mathcal{A}^\\ell(V)</math> then <math>f\\wedge g=(-1)^{k\\ell}g\\wedge f</math>. \n\nGiven a basis <math>(v_1,\\ldots, v_n)</math> for <math>V</math> and dual basis <math>(\\phi^1,\\ldots,\\phi^n)</math> for <math>V^*=\\mathcal{A}^1(V)</math>, the wedge products <math>\\phi^{i_1}\\wedge\\cdots\\wedge\\phi^{i_k}</math>, with <math>1\\leq i_1<\\cdots<i_k\\leq n</math> form a basis for <math>\\mathcal{A}^k(V)</math>.  Hence, the dimensionality of <math>\\mathcal{A}^k(V)</math> for ''n''-dimensional <math>V</math> is <math display=\"inline\">\\tbinom{n}{k}=\\frac{n!}{(n-k)!\\,k!}</math>.\n\n===Differential forms===\n''Main article: [[Differential form|Differential forms]]''\n\nDifferential forms are mathematical objects constructed via tangent spaces and multilinear forms that behave, in many ways, like [[Differential of a function|differentials]] in the classical sense.  Though conceptually and computationally useful, differentials are founded on ill-defined notions of infinitesimal quantities developed early in the [[history of calculus]].  Differential forms provide a mathematically rigorous and precise framework to modernize this long-standing idea.  Differential forms are especially useful in [[multivariable calculus]] (analysis) and [[differential geometry]] because they possess transformation properties that allow them be integrated on curves, surfaces, and their higher-dimensional analogues ([[Differentiable manifold|differentiable manifolds]]).  One far-reaching application is the modern statement of [[Stokes' theorem]], a sweeping generalization of the [[fundamental theorem of calculus]] to higher dimensions.  \n\nThe synopsis below is primarily based on Spivak (1965)<ref>{{Cite book|url=https://archive.org/details/SpivakM.CalculusOnManifoldsPerseus2006Reprint|title=Calculus on Manifolds|last=Spivak|first=Michael|publisher=W. A. Benjamin, Inc.|year=1965|isbn=0805390219|location=New York|pages=75–146}}</ref> and Tu (2011).<ref name=\":0\" />\n\n==== Definition and construction of differential 1-forms ====\nTo define differential forms on open subsets <math>U\\subset\\mathbf{R}^n</math>, we first need the notion of the '''tangent space''' of <math>\\mathbf{R}^n</math>at <math>p</math>, usually denoted <math>T_p\\mathbf{R}^n</math> or <math>\\mathbf{R}^n_p</math>. The vector space <math>\\mathbf{R}^n_p</math> can be defined most conveniently as the set of elements <math>v_p</math> (<math>v\\in\\mathbf{R}^n</math>, with <math>p\\in\\mathbf{R}^n</math> fixed) with vector addition and scalar multiplication defined by <math>v_p+w_p:=(v+w)_p</math> and <math>a\\cdot(v_p):=(a\\cdot v)_p</math>, respectively.  Moreover, if <math>(e_1,\\ldots,e_n)</math> is the standard basis for <math>\\mathbf{R}^n</math>, then <math>((e_1)_p,\\ldots,(e_n)_p)</math> is the analogous standard basis for <math>\\mathbf{R}^n_p</math>.  In other words, each tangent space <math>\\mathbf{R}^n_p</math> can simply be regarded as a copy of <math>\\mathbf{R}^n</math> (a set of tangent vectors) based at the point <math>p</math>. The collection (disjoint union) of tangent spaces of <math>\\mathbf{R}^n</math> at all <math>p\\in\\mathbf{R}^n</math> is known as the '''tangent bundle''' of <math>\\mathbf{R}^n</math> and is usually denoted <math display=\"inline\">T\\mathbf{R}^n:=\\bigcup_{p\\in\\mathbf{R}^n}\\mathbf{R}^n_p</math>.  While the definition given here provides a simple description of the tangent space of <math>\\mathbf{R}^n</math>, there are other, more sophisticated constructions that are better suited for defining the tangent spaces of [[Differentiable manifold|smooth manifolds]] in general (''see the article on [[Tangent space|tangent spaces]] for details'').      \n\nA '''differential ''k''-form''' on <math>U\\subset\\mathbf{R}^n</math> is defined as a function <math>\\omega</math> that assigns to every <math>p\\in U</math> a ''k''-covector on the tangent space of <math>\\mathbf{R}^n</math>at <math>p</math>, usually denoted <math>\\omega_p:=\\omega(p)\\in\\mathcal{A}^k(\\mathbf{R}^n_p)</math>.  In brief, a differential ''k-''form is a ''k''-covector field.  The space of ''k''-forms on <math>U</math> is usually denoted <math>\\Omega^k(U)</math>; thus if <math>\\omega</math> is a differential ''k''-form, we write <math>\\omega\\in\\Omega^k(U)</math>.  By convention, a continuous function on <math>U</math> is a differential 0-form: <math>f\\in C^0(U)=\\Omega^0(U)</math>.\n\nWe first construct differential 1-forms from 0-forms and deduce some of their basic properties.  To simplify the discussion below, we will only consider [[Smoothness|smooth]] differential forms constructed from smooth (<math>C^\\infty</math>) functions.  Let <math>f:\\mathbf{R}^n\\to\\mathbf{R}</math> be a smooth function.  We define the 1-form <math>df</math> on <math>U</math> for <math>p\\in U</math> and <math>v_p\\in\\mathbf{R}^n_p</math> by <math>(df)_p(v_p):=Df|_p(v)</math>, where <math>Df|_p:\\mathbf{R}^n\\to\\mathbf{R}</math> is the [[total derivative]] of <math>f</math> at <math>p</math>.  (Recall that the total derivative is a linear transformation.)  Of particular interest are the projection maps (also known as coordinate functions) <math>\\pi^i:\\mathbf{R}^n\\to\\mathbf{R}</math>, defined by <math>x\\mapsto x^i</math>, where <math>x^i</math> is the ''i''th standard coordinate of <math>x\\in\\mathbf{R}^n</math>.  The 1-forms <math>d\\pi^i</math> are known as the '''basic 1-forms'''; they are conventionally denoted <math>dx^i</math>.  If the standard coordinates of <math>v_p\\in\\mathbf{R}^n_p</math> are <math>(v^1,\\ldots, v^n)</math>, then application of the definition of <math>df</math> yields <math>dx^i_p(v_p)=v^i</math>, so that <math>dx^i_p((e_j)_p)=\\delta_j^i</math>, where <math>\\delta^i_j</math> is the [[Kronecker delta]].<ref>The Kronecker delta is usually denoted by <math>\\delta_{ij}=\\delta(i,j)</math> and defined as <math display=\"inline\">\\delta:X\\times X\\to\\{0,1\\},\\ (i,j)\\mapsto \\begin{cases} 1, & i=j \\\\ 0, & i\\neq j \\end{cases}</math>.  Here, the notation <math>\\delta^i_j</math> is used to conform to the tensor calculus convention on the use of upper and lower indices. </ref>  Thus, as the dual of the standard basis for <math>\\mathbf{R}^n_p</math>, <math>(dx^1_p,\\ldots,dx^n_p)</math> forms a basis for <math>\\mathcal{A}^1(\\mathbf{R}^n_p)=(\\mathbf{R}^n_p)^*</math>.  As a consequence, if <math>\\omega</math> is a 1-form on <math>U</math>, then <math>\\omega</math> can be written as <math display=\"inline\">\\sum a_i\\,dx^i</math> for smooth functions <math>a_i:U\\to\\mathbf{R}</math>.  Furthermore, we can derive an expression for <math>df</math> that coincides with the classical expression for a total differential:<blockquote><math>df=\\sum_{i=1}^n D_i f\\; dx^i={\\partial f\\over\\partial x^1}dx^1+\\cdots+{\\partial f\\over\\partial x^n}dx^n</math>.</blockquote>\n\n[''Comments on'' ''notation:'' In this article, we follow the convention from [[tensor calculus]] and differential geometry in which multivectors and multicovectors are written with lower and upper indices, respectively.  Since differential forms are multicovector fields, upper indices are employed to index them.<ref name=\":0\" />  The opposite rule applies to the ''components'' of multivectors and multicovectors, which instead are written with upper and lower indices, respectively.  For instance, we represent the standard coordinates of vector <math>v\\in\\mathbf{R}^n</math> as <math>(v^1,\\ldots,v^n)</math>, so that <math display=\"inline\">v=\\sum_{i=1}^n v^ie_i</math> in terms of the standard basis <math>(e_1,\\ldots,e_n)</math>.  In addition, superscripts appearing in the ''denominator'' of an expression (as in <math display=\"inline\">\\frac{\\partial f}{\\partial x^i}</math>) are treated as lower indices in this convention.  When indices are applied and interpreted in this manner, the number of upper indices minus the number of lower indices in each term of an expression is conserved, both within the sum and across an equal sign, a feature that serves as a useful mnemonic device and helps pinpoint errors made during manual computation.]\n\n==== Basic operations on differential ''k''-forms ====\nThe '''wedge product''' (<math>\\wedge</math>) and '''exterior differentiation''' (<math>d</math>) are two fundamental operations on differential forms.  The wedge product of a ''k''-form and an ''ℓ''-form is a <math>(k+\\ell)</math>-form, while the exterior derivative of a ''k''-form is a <math>(k+1)</math>-form.  Thus, both operations generate differential forms of higher degree from those of lower degree.\n\nThe [[wedge product]] <math>\\wedge:\\Omega^k(U)\\times\\Omega^{\\ell}(U)\\to\\Omega^{k+\\ell}(U)</math> of differential forms is a special case of the wedge product of multicovectors in general (''see above'').  As is true in general for the wedge product, the wedge product of differential forms is bilinear, associative, and anticommutative.\n\nMore concretely, if <math>\\omega=a_{i_1\\ldots i_k} dx^{i_1}\\wedge\\cdots\\wedge dx^{i_k}</math> and <math>\\eta=a_{j_1\\ldots i_{\\ell}} dx^{j_1}\\wedge\\cdots\\wedge dx^{j_{\\ell}}</math>, then   <blockquote><math>\\omega\\wedge\\eta=a_{i_1\\ldots i_k}a_{j_1\\ldots j_{\\ell}}dx^{i_1}\\wedge\\cdots\\wedge dx^{i_k}\\wedge dx^{j_1}\\wedge\\cdots\\wedge dx^{j_{\\ell}}</math>. </blockquote>Furthermore, for any set of indices <math>\\{\\alpha_1\\ldots,\\alpha_m\\}</math>,  <blockquote><math>dx^{\\alpha_1}\\wedge\\cdots\\wedge dx^{\\alpha_p}\\cdots\\wedge \\cdots dx^{\\alpha_q}\\wedge\\cdots\\wedge dx^{\\alpha_m}=\n-dx^{\\alpha_1}\\wedge\\cdots\\wedge dx^{\\alpha_q}\\cdots\\wedge \\cdots dx^{\\alpha_p}\\wedge\\cdots\\wedge dx^{\\alpha_m}</math>.  </blockquote>If <math>I=\\{i_1,\\ldots,i_k\\}</math>, <math>J=\\{j_1,\\ldots,j_{\\ell}\\}</math>, and <math>I\\cap J=\\emptyset</math>, then the indices of <math>\\omega\\wedge\\eta</math> can be arranged in ascending order by a (finite) sequence of such swaps.  Since <math>dx^\\alpha\\wedge dx^\\alpha=0</math>, <math>I\\cap J\\neq\\emptyset</math> implies that <math>\\omega\\wedge\\eta=0</math>.  Finally, as a consequence of bilinearity, if <math>\\omega</math> and <math>\\eta</math> are the sums of several terms, their wedge product obeys distributivity with respect to each of these terms.  \n\nThe collection of the wedge products of basic 1-forms <math>\\{dx^{i_1}\\wedge\\cdots\\wedge dx^{i_k}|1\\leq i_1<\\cdots< i_k\\leq n\\}</math> constitutes a basis for the space of differential ''k''-forms.  Thus, any <math>\\omega\\in\\Omega^k(U)</math> can be written in the form<blockquote><math>\\omega=\\sum_{i_1<\\cdots<i_k} a_{i_1\\ldots i_k}dx^{i_1}\\wedge\\cdots\\wedge dx^{i_k}</math> (*),</blockquote>where <math>a_{i_1\\ldots i_k}:U\\to\\mathbf{R}</math> are smooth functions.  With each set of indices <math>\\{i_1,\\ldots,i_k\\}</math> placed in ascending order, (*) is said to be the '''standard presentation''' of '''<math>\\omega</math>'''. <br>\n\nIn the previous section, the 1-form <math>df</math> was defined by taking the exterior derivative of the 0-form (continuous function) <math>f</math>.  We now extend this by defining the exterior derivative operator <math>d:\\Omega^k(U)\\to\\Omega^{k+1}(U)</math> for <math>k\\geq1</math>.  If the standard presentation of ''k''-form <math>\\omega</math> is given by (*), the <math>(k+1)</math>-form <math>d\\omega</math> is defined by<blockquote><math>d\\omega:=\\sum_{i_1<\\ldots <i_k} da_{i_1\\ldots i_k}\\wedge dx^{i_1}\\wedge\\cdots\\wedge dx^{i_k}</math>.</blockquote>A property of <math>d</math> that holds for all smooth forms is that the second exterior derivative of any <math>\\omega</math> vanishes identically: <math>d^2\\omega=d(d\\omega)\\equiv 0</math>.  This can be established directly from the definition of <math>d</math> and the [[Symmetry of second derivatives|equality of mixed second-order partial derivatives]] of <math>C^2</math> functions (''see the article on [[Closed and exact differential forms|closed and exact forms]] for details''). \n\n==== Integration of differential forms and Stokes' theorem for chains ====\nTo integrate a differential form over a parameterized domain, we first need to introduce the notion of the '''pullback''' of a differential form.  Roughly speaking, when a differential form is integrated, applying the pullback transforms it in a way that correctly accounts for a change-of-coordinates.   \n\nGiven a differentiable function <math>f:\\mathbf{R}^n\\to\\mathbf{R}^m</math> and ''k''-form <math>\\eta\\in\\Omega^k(\\mathbf{R}^m)</math>, we call <math>f^*\\eta\\in\\Omega^k(\\mathbf{R}^n)</math> the [[Pullback (differential geometry)|pullback]] of <math>\\eta</math> by <math>f</math> and define it as the ''k''-form such that <blockquote><math>(f^*\\eta)_p(v_{1p},\\ldots, v_{kp}):=\\eta_{f(p)}(f_*(v_{1p}),\\ldots,f_*(v_{kp}))</math>, </blockquote>for <math>v_{1p},\\ldots,v_{kp}\\in\\mathbf{R}^n_p</math>, where <math>f_*:\\mathbf{R}^n_p\\to\\mathbf{R}^m_{f(p)}</math> is the map <math>v_p\\mapsto(Df|_p(v))_{f(p)}</math>.<br>\n\nIf <math>\\omega=f\\, dx^1\\wedge\\cdots\\wedge dx^n</math> is an ''n''-form on <math>\\mathbf{R}^n</math> (i.e., <math>\\omega\\in\\Omega^n(\\mathbf{R}^n)</math>), we define its integral over the unit ''n''-cell as the iterated Riemann integral of <math>f</math>:<blockquote><math>\\int_{[0,1]^n} \\omega = \\int_{[0,1]^n} f\\,dx^1\\wedge\\cdots\n\\wedge dx^n:=\n\\int_0^1\\cdots\\int_0^1 f\\, dx^1\\cdots dx^n</math>.</blockquote>Next, we consider a domain of integration parameterized by a differentiable function <math>c:[0,1]^n\\to A\\subset\\mathbf{R}^m</math>, known as an '''''n''-cube'''.  To define the integral of <math>\\omega\\in\\Omega^n(A)</math> over <math>c</math>, we \"pull back\" from <math>A</math> to the unit ''n''-cell:<blockquote><math>\\int_c \\omega :=\\int_{[0,1]^n}c^*\\omega</math>.</blockquote>To integrate over more general domains, we define an '''''n-''chain <math display=\"inline\">C=\\sum_i n_ic_i</math>''' as the formal sum of ''n-''cubes and set<blockquote><math>\\int_C \\omega :=\\sum_i n_i\\int_{c_i}\\omega</math>.</blockquote>An appropriate definition of the <math>(n-1)</math>-[[Chain (algebraic topology)|chain]] <math>\\partial C</math>, known as the boundary of <math>C</math>,<ref>The formal definition of the boundary of a chain is somewhat involved and is omitted here (''see Spivak (1965), pp.&nbsp;98-99 for a discussion'').  Intuitively, if <math>C</math> maps to a square, then <math>\\partial C</math> is a linear combination of functions that maps to its edges in a counterclockwise manner.  It should be noted that the boundary of a chain is distinct from the notion of a boundary in point-set topology.</ref> allows us to state the celebrated '''Stokes' theorem''' (Stokes–Cartan theorem) for chains in a subset of <math>\\mathbf{R}^m</math>: <blockquote>''If <math>\\omega</math> is a'' ''smooth'' <math>(n-1)</math>''-form on an open set <math>A\\subset\\mathbf{R}^m</math>'' ''and <math>C</math>'' ''is a smooth'' <math>n\n</math>''-chain in <math>A</math>, then<math>\\int_C d\\omega=\\int_{\\partial C} \\omega</math>.''</blockquote>Using more sophisticated machinery (e.g., [[Germ (mathematics)|germs]] and [[Derivation (differential algebra)|derivations]]), the tangent space <math>T_p M</math> of any smooth manifold <math>M</math> (not necessarily embedded in <math>\\mathbf{R}^m</math>) can be defined.  Analogously, a differential form <math>\\omega\\in\\Omega^k(M)</math> on a general smooth manifold is a map <math>\\omega:p\\in M\\mapsto\\omega_p\\in \\mathcal{A}^k(T_pM)</math>.  [[Stokes' theorem]] can be further generalized to arbitrary smooth manifolds-with-boundary and even certain \"rough\" domains (''see the article on [[Stokes' theorem]] for details'').\n\n==See also==\n*[[Bilinear map]]\n*[[Exterior algebra]]\n*[[Homogeneous polynomial]]\n*[[Multilinear map]]\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Multilinear Form}}\n\n[[Category:Abstract algebra]]\n[[Category:Linear algebra]]\n[[Category:Multilinear algebra]]"
    },
    {
      "title": "Multiplicative inverse",
      "url": "https://en.wikipedia.org/wiki/Multiplicative_inverse",
      "text": "[[Image:Hyperbola one over x.svg|thumbnail|right|300px|alt=Graph showing the diagrammatic representation of limits approaching infinity|The reciprocal function: {{nowrap|1=''y'' = 1/''x''}}. For every ''x'' except 0, ''y'' represents its multiplicative inverse. The graph forms a [[rectangular hyperbola]].]]\nIn [[mathematics]], a '''multiplicative inverse''' or '''reciprocal''' for a number ''x'', denoted by 1/''x'' or ''x''<sup>&minus;1</sup>, is a number which when [[Multiplication|multiplied]] by ''x'' yields the [[multiplicative identity]], 1. The multiplicative inverse of a [[rational number|fraction]] ''a''/''b'' is ''b''/''a''. For the multiplicative inverse of a real number, divide 1 by the number. For example, the reciprocal of 5 is one fifth (1/5 or 0.2), and the reciprocal of 0.25 is 1 divided by 0.25, or 4. The '''reciprocal function''', the function ''f''(''x'') that maps ''x'' to 1/''x'', is one of the simplest examples of a function which is its own inverse (an [[Involution (mathematics)|involution]]).\n\nThe term ''reciprocal'' was in common use at least as far back as the third edition of ''[[Encyclopædia Britannica]]'' (1797) to describe two numbers whose product is 1; geometrical quantities in inverse proportion are described as ''reciprocall'' in a 1570 translation of [[Euclid]]'s ''[[Euclid's Elements|Elements]]''.<ref>\" In equall Parallelipipedons the bases are reciprokall to their altitudes\". ''OED'' \"Reciprocal\" §3a. Sir [[Henry Billingsley]] translation of Elements XI, 34.</ref>\n\nIn the phrase ''multiplicative inverse'', the qualifier ''multiplicative'' is often omitted and then tacitly understood (in contrast to the [[additive inverse]]). Multiplicative inverses can be defined over many mathematical domains as well as numbers. In these cases it can happen that {{nowrap|''ab'' ≠ ''ba''}}; then \"inverse\" typically implies that an element is both a left and right [[inverse element|inverse]].\n\nThe notation ''f'' <sup>−1</sup> is sometimes also used for the [[inverse function]] of the function ''f'', which is not in general equal to the multiplicative inverse. For example, the multiplicative inverse {{nowrap|1=1/(sin ''x'') = (sin ''x'')<sup>−1</sup>}} is the [[cosecant]] of x, and not the [[Inverse trigonometric functions|inverse sine of ''x'']] denoted by {{nowrap|sin<sup>−1</sup> ''x''}} or {{nowrap|arcsin ''x''}}. Only for linear maps are they strongly related (see below). The terminology difference ''reciprocal'' versus ''inverse'' is not sufficient to make this distinction, since many authors prefer the opposite naming convention, probably for historical reasons (for example in [[French language|French]], the inverse function is preferably called [[w:fr:Bijection réciproque|bijection réciproque]]).\n\n==Examples and counterexamples==\nIn the real numbers, [[0 (number)|zero]] [[Division by zero|does not have a reciprocal]] because no real number multiplied by 0 produces 1 (the product of any number with zero is zero). With the exception of zero, reciprocals of every [[real number]] are real, reciprocals of every [[rational number]] are rational, and reciprocals of every [[complex number]] are complex. The property that every element other than zero has a multiplicative inverse is part of the definition of a [[Field (mathematics)|field]], of which these are all examples. On the other hand, no [[integer]] other than 1 and −1 has an integer reciprocal, and so the integers are not a field.\n\nIn [[modular arithmetic]], the [[modular multiplicative inverse]] of ''a'' is also defined: it is the number ''x'' such that ''ax''&nbsp;≡&nbsp;1&nbsp;(mod&nbsp;''n'').  This multiplicative inverse exists [[if and only if]] ''a'' and ''n'' are [[coprime]].  For example, the inverse of 3 modulo 11 is 4 because 4&nbsp;·&nbsp;3&nbsp;≡&nbsp;1&nbsp;(mod&nbsp;11).  The [[extended Euclidean algorithm]] may be used to compute it.\n\nThe [[sedenion]]s are an algebra in which every nonzero element has a multiplicative inverse, but which nonetheless has divisors of zero, i.e. nonzero elements ''x'', ''y'' such that ''xy''&nbsp;= 0.\n\nA [[square matrix]] has an inverse [[if and only if]] its [[determinant]] has an inverse in the coefficient [[Ring (mathematics)|ring]]. The linear map that has the matrix ''A''<sup>&minus;1</sup> with respect to some base is then the reciprocal function of the map having ''A'' as matrix in the same base. Thus, the two distinct notions of the inverse of a function are strongly related in this case, while they must be carefully distinguished in the general case (as noted above).\n\nThe [[trigonometric functions]] are related by the reciprocal identity: the cotangent is the reciprocal of the tangent; the secant is the reciprocal of the cosine; the cosecant is the reciprocal of the sine.\n\nA ring in which every nonzero element has a multiplicative inverse is a [[division ring]]; likewise an [[algebra (ring theory)|algebra]] in which this holds is a [[division algebra]].\n\n== Complex numbers ==\nAs mentioned above, the reciprocal of every nonzero complex number {{math|1=''z'' = ''a'' + ''bi''}} is complex. It can be found by multiplying both top and bottom of 1/''z'' by its [[complex conjugate]] <math>\\bar z = a - bi</math> and using the property that <math>z\\bar z = \\|z\\|^2</math>, the [[absolute value]] of ''z'' squared, which is the real number {{math|''a''<sup>2</sup> + ''b''<sup>2</sup>}}:\n\n:<math>\\frac{1}{z} = \\frac{\\bar z}{z \\bar z} = \\frac{\\bar z}{\\|z\\|^2} = \\frac{a - bi}{a^2 + b^2} = \\frac{a}{a^2 + b^2} - \\frac{b}{a^2+b^2}i.</math>\n\nIn particular, if ||''z''||=1 (''z'' has unit magnitude), then <math>1/z = \\bar z</math>. Consequently, the [[imaginary unit]]s, ±{{math|''i''}}, have [[additive inverse]] equal to multiplicative inverse, and are the only complex numbers with this property. For example, additive and multiplicative inverses of {{math|''i''}} are &minus;({{math|''i''}}) = &minus;{{math|''i''}} and 1/{{math|''i''}} = &minus;{{math|''i''}}, respectively.\n\nFor a complex number in polar form {{math|1=''z'' = ''r''(cos φ + ''i'' sin φ)}}, the reciprocal simply takes the reciprocal of the magnitude and the negative of the angle:\n\n:<math>\\frac{1}{z} = \\frac{1}{r}\\left(\\cos(-\\varphi) + i \\sin(-\\varphi)\\right).</math>\n\n[[File:Reciprocal integral.svg|thumb|Geometric intuition for the integral of 1/''x''. The three integrals from 1 to 2, from 2 to 4, and from 4 to 8 are all equal. Each region is the previous region scaled vertically down by 50%, then horizontally by 200%. Extending this, the integral from 1 to 2<sup>''k''</sup> is ''k'' times the integral from 1 to 2, just as ln 2<sup>''k''</sup> = ''k'' ln 2.]]\n\n== Calculus ==\nIn real [[calculus]], the [[derivative]] of {{math|1= 1/''x'' = ''x''<sup>−1</sup>}} is given by the [[power rule]] with the power −1:\n\n:<math> \\frac{d}{dx} x^{-1} = (-1)x^{(-1)-1} = -x^{-2} = -\\frac{1}{x^2}.</math>\n\nThe power rule for integrals ([[Cavalieri's quadrature formula]]) cannot be used to compute the integral of 1/''x'', because doing so would result in division by 0:\n\n:<math>\\int \\frac{1}{x}\\,dx = \\frac{x^0}{0}\\ + C </math>\n\nInstead the integral is given by:\n\n:<math>\\int_1^a \\frac{1}{x}\\,dx = \\ln a,</math>\n:<math>\\int \\frac{1}{x}\\,dx = \\ln x + C.</math>\n\nwhere ln is the [[natural logarithm]]. To show this, note that <math>\\frac{d}{dx} e^x = e^x</math>, so if <math>y = e^x</math> and <math>x = \\ln y</math>, we have:<ref>{{cite web|last=Anthony|first=Dr.|title=Proof that INT(1/x)dx = lnx|url=http://mathforum.org/library/drmath/view/53562.html|work=Ask Dr. Math|publisher=Drexel University|accessdate=22 March 2013}}</ref>\n:<math>\\frac{dy}{dx} = y\\quad \\Rightarrow \\quad \\frac{dy}{y} = dx \\quad\\Rightarrow\\quad \\int \\frac{1}{y}\\,dy = \\int 1\\,dx \\quad\\Rightarrow\\quad \\int \\frac{1}{y}\\,dy = x + C = \\ln y + C.</math>\n\n== Algorithms ==\nThe reciprocal may be computed by hand with the use of [[long division]].\n\nComputing the reciprocal is important in many [[division algorithm]]s, since the quotient ''a''/''b'' can be computed by first computing 1/''b'' and then multiplying it by ''a''. Noting that <math>f(x) = 1/x - b</math> has a [[Zero of a function|zero]] at ''x'' = 1/''b'', [[Newton's method]] can find that zero, starting with a guess <math>x_0</math> and iterating using the rule:\n\n:<math>x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)} = x_n - \\frac{1/x_n - b}{-1/x_n^2} = 2x_n - bx_n^2 = x_n(2 - bx_n).</math>\n\nThis continues until the desired precision is reached. For example, suppose we wish to compute 1/17 ≈ 0.0588 with 3 digits of precision. Taking ''x''<sub>0</sub> = 0.1, the following sequence is produced:\n:''x''<sub>1</sub> = 0.1(2 − 17 × 0.1) = 0.03\n:''x''<sub>2</sub> = 0.03(2 − 17 × 0.03) = 0.0447\n:''x''<sub>3</sub> = 0.0447(2 − 17 × 0.0447) ≈ 0.0554\n:''x''<sub>4</sub> = 0.0554(2 − 17 × 0.0554) ≈ 0.0586\n:''x''<sub>5</sub> = 0.0586(2 − 17 × 0.0586) ≈ 0.0588\nA typical initial guess can be found by rounding ''b'' to a nearby power of 2, then using [[bit shift]]s to compute its reciprocal.\n\nIn [[constructive mathematics]], for a real number ''x'' to have a reciprocal, it is not sufficient that ''x'' ≠ 0. There must instead be given a ''rational'' number ''r'' such that 0&nbsp;&lt;&nbsp;''r''&nbsp;&lt;&nbsp;|''x''|. In terms of the approximation [[algorithm]] described above, this is needed to prove that the change in ''y'' will eventually become arbitrarily small.\n\n[[File:X to x power showing minimum.svg|thumb|Graph of f(''x'') = ''x''<sup>''x''</sup> showing the minimum at (1/e, e<sup>−1/e</sup>).]]\n\nThis iteration can also be generalised to a wider sort of inverses, e.g. [[Invertible matrix#Newton's method|matrix inverses]].\n\n==Reciprocals of irrational numbers==\nEvery number excluding zero has a reciprocal, and reciprocals of certain [[irrational number]]s can have important special properties. Examples include the reciprocal of [[e (mathematical constant)|e]] (≈&nbsp;0.367879) and the [[Golden ratio#Golden ratio conjugate|golden ratio's reciprocal]] (≈&nbsp;0.618034). The first reciprocal is special because no other positive number can produce a lower number when put to the power of itself; <math>f(1/e)</math> is the [[global optimum|global minimum]] of <math>f(x)=x^x</math>. The second number is the only positive number that is equal to its reciprocal plus one:<math>\\varphi = 1/\\varphi + 1</math>. Its [[additive inverse]] is the only negative number that is equal to its reciprocal minus one:<math>-\\varphi = -1/\\varphi - 1</math>.\n\nThe function <math>f(n)=n+\\sqrt {(n^2+1)}, n \\in \\mathbb{N}, n>0</math> gives an infinite number of irrational numbers that differ with their reciprocal by an integer. For example, <math>f(2)</math> is the irrational <math>2+\\sqrt 5</math>. Its reciprocal <math>1 / (2 + \\sqrt 5)</math> is <math>-2 + \\sqrt 5</math>, exactly <math>4</math> less. Such irrational numbers share a curious property: they have the same [[fractional part]] as their reciprocal.\n\n==Further remarks==\nIf the multiplication is associative, an element ''x'' with a multiplicative inverse cannot be a [[zero divisor]] (''x'' is a zero divisor if some nonzero ''y'', {{nowrap|1=''xy'' = 0}}). To see this, it is sufficient to multiply the equation {{nowrap|1=''xy'' = 0}} by the inverse of ''x'' (on the left), and then simplify using associativity. In the absence of associativity, the [[sedenion]]s provide a counterexample.\n\nThe converse does not hold: an element which is not a [[zero divisor]] is not guaranteed to have a multiplicative inverse.\nWithin '''Z''', all integers except −1, 0, 1 provide examples; they are not zero divisors nor do they have inverses in '''Z'''.\nIf the ring or algebra is [[finite set|finite]], however, then all elements ''a'' which are not zero divisors do have a (left and right) inverse. For, first observe that the map {{nowrap|1=''f''(''x'') = ''ax''}} must be [[injective]]: {{nowrap|1=''f''(''x'') = ''f''(''y'')}} implies {{nowrap|1=''x'' = ''y''}}:\n:<math>\\begin{align}\n ax &= ay &\\quad \\rArr & \\quad  ax-ay = 0 \\\\\n & &\\quad \\rArr  &\\quad  a(x-y) = 0 \\\\\n & &\\quad \\rArr  &\\quad  x-y = 0 \\\\\n & &\\quad \\rArr  &\\quad  x = y.\n\\end{align}</math>\nDistinct elements map to distinct elements, so the image consists of the same finite number of elements, and the map is necessarily [[surjective]]. Specifically, ƒ (namely multiplication by ''a'') must map some element ''x'' to 1, {{nowrap|1=''ax'' = 1}}, so that ''x'' is an inverse for ''a''.\n\n==Applications==\n\nThe expansion of the reciprocal 1/''q'' in any base can also act <ref name=\"Mitchell\">Mitchell, Douglas W., \"A nonlinear random number generator with known, long cycle length,\" ''[[Cryptologia]]'' 17, January 1993, 55–62.</ref> as a source of [[pseudo-random numbers]], if ''q'' is a \"suitable\" [[safe prime]], a prime of the form 2''p''&nbsp;+&nbsp;1 where ''p'' is also a prime. A sequence of pseudo-random numbers of length ''q''&nbsp;&minus;&nbsp;1 will be produced by the expansion.\n\n==See also==\n* [[Division (mathematics)]]\n* [[Exponential decay]]\n* [[Fraction (mathematics)]]\n* [[Group (mathematics)]]\n* [[Hyperbola]]\n* [[List of sums of reciprocals]]\n* [[Repeating decimal]]\n* [[Six-sphere coordinates]]\n* [[Unit fraction]]s – reciprocals of integers\n\n==Notes==\n\n<references/>\n\n==References==\n\n*Maximally Periodic Reciprocals, Matthews R.A.J. ''Bulletin of the Institute of Mathematics and its Applications'' vol 28 pp 147–148 1992\n\n[[Category:Elementary special functions]]\n[[Category:Abstract algebra]]\n[[Category:Elementary algebra]]\n[[Category:Multiplication]]\n[[Category:Unary operations]]"
    },
    {
      "title": "N-ary associativity",
      "url": "https://en.wikipedia.org/wiki/N-ary_associativity",
      "text": "{{DISPLAYTITLE:''n''-ary associativity}}\nIn [[algebra]], '''''n''-ary associativity''' is a [[generalization]] of the [[associative law]] to [[arity|''n''-ary operations]]. '''Ternary associativity''' is\n\n: (''abc'')''de'' = ''a''(''bcd'')''e'' = ''ab''(''cde''),\n\ni.e. the [[String (computer science)|string]] ''abcde'' with any three adjacent elements bracketed.  ''n''-ary associativity is a string of length ''n''&nbsp;+&nbsp;(''n''&nbsp;&minus;&nbsp;1) with any ''n'' adjacent elements bracketed.<ref>{{Citation |last=Dudek |first=W.A. |title=On some old problems in n-ary groups |url=http://www.quasigroups.eu/contents/contents8.php?m=trzeci |archive-url=https://web.archive.org/web/20090714003319/http://www.quasigroups.eu/contents/contents8.php?m=trzeci |dead-url=yes |archive-date=2009-07-14 |journal=Quasigroups and Related Systems |year=2001 |volume=8 |pages=15–36 }}.</ref>\n\n==References==\n{{reflist}}\n\n[[Category:Abstract algebra]]\n\n{{algebra-stub}}"
    },
    {
      "title": "Near sets",
      "url": "https://en.wikipedia.org/wiki/Near_sets",
      "text": "[[File:Descriptively Near Sets.svg|right|thumb|'''Figure 1.''' Descriptively, very near sets]]\n\nIn mathematics, '''near sets''' are either spatially [[closeness (mathematics)|close]] or descriptively close. Spatially close sets have nonempty [[intersection (set theory)|intersection]]. In other words, spatially close sets are not [[disjoint sets]], since they always have at least one element in common. Descriptively close sets contain elements that have matching descriptions. Such sets can be either disjoint or non-disjoint sets. Spatially near sets are also descriptively near sets.\n\n[[File:Minimally Descriptive Near Sets.svg|right|thumb|'''Figure 2.''' Descriptively, minimally near sets]]\n\nThe underlying assumption with descriptively close sets is that such sets contain elements that have location and measurable features such as colour and frequency of occurrence. The description of the element of a [[set (mathematics)|set]] is defined by a [[feature vector]]. Comparison of feature vectors provides a basis for measuring the closeness of descriptively near sets. Near set theory provides a formal basis for the observation, comparison, and classification of elements in sets based on their closeness, either spatially or descriptively. Near sets offer a framework for solving problems based on [[perception|human perception]] that arise in areas such as [[image processing]], [[computer vision]] as well as engineering and science problems.\n\nNear sets have a variety of applications in areas such as [[topology]]{{cref2|37|1}}, [[pattern recognition|pattern detection]] and [[statistical classification|classification]]{{cref2|50|1}}, [[abstract algebra]]{{cref2|51|1}}, mathematics in computer science{{cref2|38|1}}, and solving a variety of problems based on human perception{{cref2|42|1}}{{cref2|82|1}}{{cref2|47|1}}{{cref2|52|1}}{{cref2|56|1}} that arise in areas such as [[image analysis]]{{cref2|54|1}}{{cref2|14|1}}{{cref2|46|1}}{{cref2|17|1}}{{cref2|18|1}}, image processing{{cref2|40|1}}, [[Facial recognition system|face recognition]]{{cref2|13|1}}, [[ethology]]{{cref2|64|1}}, as well as engineering and science problems{{cref2|55|1}}{{cref2|64|2}}{{cref2|42|2}}{{cref2|19|1}}{{cref2|17|2}}{{cref2|18|2}}. From the beginning, descriptively near sets have proved to be useful in applications of topology{{cref2|37|2}}, and visual pattern recognition {{cref2|50|2}}, spanning a broad spectrum of applications that include [[camouflage]] detection, [[micropaleontology]], handwriting forgery detection, biomedical image analysis, [[content-based image retrieval]], [[population dynamics]], [[quotient space (topology)|quotient topology]], [[textile design]], [[visual merchandising]], and topological psychology.\n\nAs an illustration of the degree of descriptive nearness between two sets, consider an example of the Henry colour model for varying degrees of nearness\nbetween sets of picture elements in pictures (see, ''e.g.'',{{cref2|17|3}} §4.3). The two pairs of ovals in Fig. 1 and Fig. 2 contain coloured segments. Each segment in the figures corresponds to an equivalence class where all pixels in the class have similar descriptions, ''i.e.'', picture elements with similar colours. The ovals in Fig.1 are closer to each other descriptively than the ovals in Fig. 2.\n\n== History ==\nIt has been observed that the simple concept of ''nearness'' unifies various concepts of topological structures{{cref2|20|1}} inasmuch as the [[category (mathematics)|category]] '''Near''' of all nearness spaces and nearness preserving maps contains categories '''sTop''' (symmetric topological spaces and continuous maps{{cref2|3|1}}), '''Prox''' ([[proximity space]]s and <math>\\delta</math>-maps{{cref2|8|1}}{{cref2|67|1}}), '''Unif''' ([[uniform space]]s and uniformly continuous maps{{cref2|81|1}}{{cref2|77|1}}) and '''Cont''' (contiguity spaces and contiguity maps{{cref2|24|1}}) as embedded full subcategories{{cref2|20|2}}{{cref2|59|1}}. The categories <math>\\boldsymbol{\\varepsilon{ANear}}</math> and <math>\\boldsymbol{\\varepsilon{AMer}}</math> are shown to be full supercategories of various well-known categories, including the category <math>\\boldsymbol{sTop}</math> of symmetric topological spaces and continuous maps, and the category <math>\\boldsymbol{Met^\\infty}</math> of extended metric spaces and nonexpansive maps. The notation <math>\\boldsymbol{A}\\hookrightarrow\\boldsymbol{B}</math> reads ''category'' <math>\\boldsymbol{A}</math> ''is embedded in category'' <math>\\boldsymbol{B}</math>. The categories <math>\\boldsymbol{\\varepsilon AMer}</math> and <math>\\boldsymbol{\\varepsilon ANear}</math> are supercategories for a variety of familiar categories{{cref2|76|1}} shown in Fig. 3. Let <math>\\boldsymbol{\\varepsilon{ANear}}</math> denote the category of all <math>\\varepsilon</math>-approach nearness spaces and contractions, and let <math>\\boldsymbol{\\varepsilon AMer}</math> denote the category of all <math>\\varepsilon</math>-approach merotopic spaces and contractions.\n\n[[File:EANear and eAMer Supercategories.png|thumb|'''Figure 3.''' Supercats]]\n\nAmong these familiar categories is <math>\\boldsymbol{sTop}</math>, the symmetric form of <math>\\boldsymbol{Top}</math> (see [[category of topological spaces]]), the category with objects that are topological spaces and morphisms that are continuous maps between them{{cref2|1|1}}{{cref2|32|1}}. <math>\\boldsymbol{Met^{\\infty}}</math> with objects that are extended metric spaces is a subcategory of <math>\\boldsymbol{\\varepsilon AP}</math> (having objects <math>\\varepsilon</math>-approach spaces and contractions)  (see also{{cref2|57|1}}{{cref2|75|1}}). Let <math>\\rho_X,\\rho_Y</math> be extended pseudometrics on nonempty sets <math>X,Y</math>, respectively. The map <math>f:(X,\\rho_X)\\longrightarrow(Y,\\rho_Y)</math> is a contraction if and only if <math>f:(X,\\nu_{D_{\\rho_X}})\\longrightarrow(Y,\\nu_{D_{\\rho_Y}})</math> is a contraction. For nonempty subsets <math>A,B\\in 2^X</math> , the distance function <math>D_{\\rho}:2^X\\times 2^X\\longrightarrow [0,\\infty]</math> is defined by\n\n:<math>D_{\\rho}(A,B) =\\begin{cases}\n\\inf{\\{\\rho(a,b): a\\in A, b\\in B\\}}, &\\text{if }A\\text{ and }B\\text{ are not empty},\\\\ \n\\infty, &\\text{if }A\\text{ or }B\\text{ is empty}. \n\\end{cases} </math>\n\nThus <math>\\boldsymbol{\\varepsilon}</math>'''AP''' is embedded as a full subcategory in <math>\\boldsymbol{\\varepsilon{ANear}}</math> by the functor <math>F: \\boldsymbol{\\varepsilon{AP}}\\longrightarrow \\boldsymbol{\\varepsilon{ANear}}</math> defined by <math>F((X,\\rho))=(X,\\nu_{D_{\\rho}})</math> and <math>F(f)=f</math>. Then <math>f:(X,\\rho_X)\\longrightarrow(Y,\\rho_Y)</math> is a contraction if and only if <math>f:(X,\\nu_{D_{\\rho_X}})\\longrightarrow(Y,\\nu_{D_{\\rho_Y}})</math> is a contraction. Thus <math>\\boldsymbol{\\varepsilon{AP}}</math> is embedded as a full subcategory in <math>\\boldsymbol{\\varepsilon{ANear}}</math> by the functor <math>F: \\boldsymbol{\\varepsilon{AP}}\\longrightarrow \\boldsymbol{\\varepsilon{ANear}}</math> defined by <math>F((X,\\rho))=(X,\\nu_{D_{\\rho}})</math> and <math>F(f)=f.</math> Since the category <math>\\boldsymbol{Met^\\infty}</math> of extended metric spaces and nonexpansive maps is a full subcategory of <math>\\boldsymbol{\\varepsilon{AP}}</math>, therefore, <math>\\boldsymbol{\\varepsilon{ANear}}</math> is also a full supercategory of <math>\\boldsymbol{Met^\\infty}</math>. The category <math>\\boldsymbol{\\varepsilon{ANear}}</math> is a topological construct{{cref2|76|2}}.\n\n[[File:Frigyes Riesz.jpeg|thumb|'''Figure 4.''' [[Frigyes Riesz]], 1880-1956]]\n\nThe notions of near and far{{cref2|A|1|group=2}} in mathematics can be traced back to works by [[Johann Benedict Listing]] and [[Felix Hausdorff]]. The related notions of resemblance and similarity can be traced back to [[Henri Poincaré|J.H. Poincaré]], who introduced sets of similar sensations (nascent tolerance classes) to represent the results of G.T. Fechner's sensation sensitivity experiments{{cref2|10|1}} and a framework for the study of resemblance in representative spaces as models of what he termed physical continua{{cref2|63|1}}{{cref2|60|1}}{{cref2|61|1}}. The elements of a physical continuum (pc) are sets of sensations. The notion of a pc and various representative spaces (tactile, visual, motor spaces) were introduced by Poincaré in an 1894 article on the mathematical continuum{{cref2|63|2}}, an 1895 article on space and geometry{{cref2|60|2}} and a compendious 1902 book on science and hypothesis{{cref2|61|2}} followed by a number of elaborations, ''e.g.'',{{cref2|62|1}}. The 1893 and 1895 articles on continua (Pt. 1, ch. II) as well as representative spaces and geometry (Pt. 2, ch IV) are included as chapters in{{cref2|61|3}}. Later, F. Riesz introduced the concept of proximity or nearness of pairs of sets at the [[International Congress of Mathematicians]] (ICM) in 1908{{cref2|65|1}}.\n\nDuring the 1960s, [[Christopher Zeeman|E.C. Zeeman]] introduced tolerance spaces in modelling visual perception{{cref2|83|1}}. A.B. Sossinsky observed in 1986{{cref2|71|1}} that the main idea underlying tolerance space theory comes from Poincaré, especially{{cref2|60|3}}. In 2002, Z. Pawlak and J. Peters{{cref2|B|1|group=2}} considered an informal approach to the perception of the nearness of physical objects such as snowflakes that was not limited to spatial nearness. In 2006, a formal approach to the descriptive nearness of objects was considered by J. Peters, A. Skowron and J. Stepaniuk{{cref2|C|1|group=2}} in the context of proximity spaces{{cref2|39|1}}{{cref2|33|1}}{{cref2|35|1}}{{cref2|21|1}}. In 2007, descriptively near sets were introduced by J. Peters{{cref2|D|1|group=2}}{{cref2|E|1|group=2}} followed by the introduction of tolerance near sets{{cref2|41|1}}{{cref2|45|1}}. Recently, the study of descriptively near sets has led to algebraic{{cref2|22|1}}{{cref2|51|2}}, topological and proximity space{{cref2|37|4}} foundations of such sets.\n\n== Nearness of sets ==\nThe adjective ''near'' in the context of near sets is used to denote the fact that observed feature value differences of distinct objects are small enough to be\nconsidered indistinguishable, ''i.e.'', within some tolerance.\n\nThe exact idea of closeness or 'resemblance' or of 'being within tolerance' is universal enough to appear, quite naturally, in almost any mathematical setting\n(see, ''e.g.'',{{cref2|66|1}}). It is especially natural in mathematical applications: practical problems, more often than not, deal with approximate input data and only require viable results with a tolerable level of error{{cref2|71|2}}.\n\nThe words ''near'' and ''far'' are used in daily life and it was an incisive suggestion of [[Frigyes Riesz|F. Riesz]]{{cref2|65|2}} that these intuitive concepts be made rigorous. He introduced the concept of nearness of pairs of sets at the ICM in Rome in 1908. This concept is useful in simplifying teaching calculus and advanced calculus. For example, the passage from an intuitive definition of continuity of a function at a point to its rigorous epsilon-delta definition is sometime difficult for teachers to explain and for students to understand. Intuitively, [[continuous function|continuity]] can be explained using nearness language, ''i.e.'', a function <math>f:\\mathbb{R}\\rightarrow\\mathbb{R}</math> is continuous at a point <math>c</math>, provided points <math>\\{x\\}</math> near <math>c</math> go into points <math>\\{f(x)\\}</math> near <math>f(c)</math>. Using Riesz's idea, this definition can be made more precise and its contrapositive is the familiar definition{{cref2|4|1}}{{cref2|36|1}}.\n\n== Generalization of set intersection ==\nFrom a spatial point of view, nearness (a.k.a. proximity) is considered a generalization of set [[intersection (set theory)|intersection]]. For disjoint sets, a form of nearness set intersection is defined in terms of a set of objects (extracted from disjoint sets) that have similar features within some\ntolerance (see, ''e.g.'', §3 in{{cref2|80|1}}). For example, the ovals in Fig. 1 are considered near each other, since these ovals contain pairs of classes that display similar (visually indistinguishable) colours.\n\n== Efremovič proximity space ==\nLet <math>X</math> denote a [[metric space|metric]] [[topological space]] that is endowed with one or more proximity relations and let <math>2^X</math> denote the collection of all subsets of <math>X</math>.  The collection <math>2^X</math> is called the [[power set]] of <math>X</math>.\n\nThere are many ways to define Efremovič proximities on topological spaces (discrete proximity, standard proximity, metric proximity, Čech proximity, Alexandroff proximity, and Freudenthal proximity),  For details, see § 2, pp.&nbsp;93–94 in{{cref2|6|1}}.\nThe focus here is on ''standard proximity'' on a topological space.  For <math>A,B\\subset X</math>, <math>A</math> is near <math>B</math> (denoted by <math>A\\ \\delta\\ B</math>), provided their closures share a common point.\n\nThe ''closure'' of a subset <math>A\\in 2^X</math> (denoted by <math>\\mbox{cl}(A)</math>) is the usual [[Kuratowski closure axioms|Kuratowski closure]] of a set{{cref2|F|1|group=2}}, introduced in § 4, p.&nbsp;20{{cref2|27}}, is defined by\n\n:<math>\\begin{align}\n\\mbox{cl}(A) &= \\left\\{x\\in X: D(x,A)=0\\right\\},\\ \\mbox{where}\\\\\nD(x,A) &= inf\\left\\{d(x,a): a\\in A\\right\\}.\n\\end{align}</math>\n\n''i.e.'' <math>\\mbox{cl}(A)</math> is the set of all points <math>x</math> in <math>X</math> that are close to <math>A</math> (<math>D(x,A)</math> is the Hausdorff distance (see § 22, p.&nbsp;128, in{{cref2|15|1}}) between <math>x</math> and the set <math>A</math> and <math>d(x,a) = \\left|x - a \\right|</math> (standard distance)). A ''standard''  proximity relation is defined by\n\n:<math>\\delta = \\left\\{(A,B)\\in 2^X\\times 2^X: \\mbox{cl}(A)\\ \\cap\\ \\mbox{cl}(B)\\neq\\emptyset\\right\\}.</math>\n\nWhenever sets <math>A</math> and <math>B</math> have no points in common, the sets are ''far''from each other (denoted <math>A\\ \\underline{\\delta}\\ B</math>).\n\nThe following EF-proximity{{cref2|G|1|group=2}} space axioms are given by Jurij Michailov Smirnov{{cref2|67|2}} based on what [[Vadim Arsenyevich Efremovich|Vadim Arsenyevič Efremovič]] introduced during the first half of the 1930s{{cref2|8|2}}. Let <math>A,B,E\\in 2^X</math>.\n\n;EF.1 : If the set <math>A</math> is close to <math>B</math>, then <math>B</math> is close to <math>A</math>.\n;EF.2 : <math>A\\cup B</math> is close to <math>E</math>, if and only if, at least one of the sets <math>A</math> or <math>B</math> is close to <math>E</math>.\n;EF.3 : Two points are close, if and only if, they are the same point.\n;EF.4 : All sets are far from the empty set <math>\\emptyset</math>.\n;EF.5 : For any two sets <math>A</math> and <math>B</math> which are far from each other, there exists <math>C,D\\in 2^X</math>, <math>C\\cup D = X</math>, such that <math>A</math> is far from <math>C</math> and <math>B</math> is far from <math>D</math> (''Efremovič-axiom'').\n\nThe pair <math>(X, \\delta)</math> is called an EF-[[proximity space]].   In this context, a [[space]] is a set with some added structure.  With a proximity space <math>X</math>, the structure of <math>X</math> is induced by the EF-proximity relation <math>\\delta</math>. In a proximity space <math>X</math>, the closure of <math>A</math> in <math>X</math> coincides with the intersection of all closed sets that contain <math>A</math>.\n\n;Theorem 1{{cref2|67|3}} : The closure of any set <math>A</math> in the proximity space <math>X</math> is the set of points <math>x\\in X</math> that are close to <math>A</math>.\n\n== Visualization of EF-axiom ==\n\n[[File:Sample EF-space.svg|thumb|'''Figure 5.''' Example of a descriptive EF-proximity relation between sets <math>A, B</math>, and <math>C^c</math>]]\n\nLet the set <math>X</math> be represented by the points inside the rectangular region in Fig. 5.  Also, let <math>A,B</math> be any two non-intersection subsets (''i.e.'' subsets spatially far from each other) in <math>X</math>, as shown in Fig. 5.  Let <math>C^c = X \\backslash C</math> ([[complement (set theory)|complement]] of the set <math>C</math>). Then from the EF-axiom, observe the following:\n\n:<math>\\begin{align}\nA\\ &\\underline{\\delta}\\ B,\\\\\nB &\\subset C,\\\\\nD &= C^c,\\\\\nX &= D \\cup C,\\\\\n A &\\subset D,\\ \\mbox{hence, we can write}\\\\\nA\\ \\underline{\\delta}\\ B\\ &\\Rightarrow\\ A\\ \\underline{\\delta}\\ C\\ \\mbox{and}\\ B\\ \\underline{\\delta}\\ D,\\ \\mbox{for some}\\ C,D\\ \\mbox{in}\\ X \\mbox{ so that } C\\cup D = X. \\qquad \\blacksquare\n\\end{align}</math>\n\n== Descriptive proximity space ==\nDescriptively near sets were introduced as a means of solving classification and pattern recognition problems arising from disjoint sets that resemble each other{{cref2|44|1}}{{cref2|43|1}}.  Recently, the connections between near sets in EF-spaces and near sets in descriptive EF-proximity spaces have been explored in{{cref2|53|1}}{{cref2|48|1}}.\n\nAgain, let <math>X</math> be a metric topological space and let <math>\\Phi = \\left\\{\\phi_1,\\dots,\\phi_n\\right\\}</math> a set of probe functions that represent features of each <math>x\\in X</math>.  The assumption made here is <math>X</math> contains non-abstract points that have measurable features such as gradient orientation.  A non-abstract point has a location and features that can be measured (see § 3 in {{cref2|26|1}}).\n\nA ''probe function'' <math>\\phi:X\\rightarrow \\mathbb{R}</math> represents a feature of a sample point in <math>X</math>. The mapping <math>\\Phi:X\\longrightarrow \\mathbb{R}^n</math> is defined by <math>\\Phi(x) = (\\phi_1(x),\\dots,\\phi_n(x))</math>, where <math>\\mathbb{R}^n</math> is an n-dimensional real Euclidean [[vector space]].  <math>\\Phi(x)</math> is a feature vector for <math>x</math>, which provides a description of <math>x\\in X</math>. For example, this leads to a proximal view of sets of picture points in digital images{{cref2|48|2}}.\n\nTo obtain a descriptive proximity relation (denoted by <math>\\delta_{\\Phi}</math>), one first chooses a set of probe functions.  Let <math>\\mathcal{Q}: 2^X\\longrightarrow 2^{R^n}</math> be a mapping on a subset of <math>2^X</math> into a subset of <math>2^{R^n}</math>.   For example, let <math>A,B\\in 2^X</math> and <math>\\mathcal{Q}(A),\\mathcal{Q}(B)</math> denote sets of descriptions of points in <math>A,B</math>, respectively.  That is,\n\n:<math>\\begin{align}\n\\mathcal{Q}(A) &= \\left\\{\\Phi(a): a\\in A\\right\\},\\\\\n\\mathcal{Q}(B) &= \\left\\{\\Phi(b): b\\in B\\right\\}.\n\\end{align}</math>\n\nThe expression <math>A\\  \\delta_{\\Phi}\\ B</math> reads <math>A</math> ''is descriptively near'' <math>B</math>.  Similarly, <math>A\\  \\underline{\\delta}_{\\Phi}\\ B</math> reads <math>A</math> ''is descriptively far from'' <math>B</math>.  The descriptive proximity of <math>A</math> and <math>B</math> is defined by\n\n:<math>\nA\\ \\delta_{\\Phi}\\ B \\Leftrightarrow \\mathcal{Q}(\\mbox{cl}(A))\\; \\delta \\;\\mathcal{Q}(\\mbox{cl}(B)) \\neq \\emptyset.\n</math>\nThe ''descriptive intersection'' <math>\\mathop{\\cap}_{\\Phi}</math> of <math>A</math> and <math>B</math> is defined by\n\n:<math>\nA\\ \\mathop{\\cap}_{\\Phi}\\ B = \\left\\{x\\in A\\cup B:\\mathcal{Q}(A)\\; \\delta \\;\\mathcal{Q}(B)\\right\\}.\n</math>\n\nThat is, <math>x\\in A\\cup B</math> is in <math>A\\ \\mathop{\\cap}_{\\Phi}\\ B</math>, provided <math>\\Phi(x) = \\Phi(a) = \\Phi(b)</math> for some <math>a\\in A, b\\in B</math>. Observe that <math>A</math> and <math>B</math> can be disjoint and yet <math>A\\ \\mathop{\\cap}_{\\Phi}\\ B</math> can be nonempty.\nThe descriptive proximity relation <math>\\delta_{\\Phi}</math> is defined by\n\n:<math>\n\\delta_{\\Phi} = \\left\\{(A,B)\\in 2^X\\times 2^X:\n                       \\mbox{cl}(A)\\ \\mathop{\\cap}_{\\Phi}\\ \\mbox{cl}(B)\\neq\\emptyset\\right\\}.\n</math>\n\nWhenever sets <math>A</math> and <math>B</math> have no points with matching descriptions, the sets are ''descriptively far'' from each other \n(denoted by <math>A\\ \\underline{\\delta}_{\\Phi}\\ B</math>).\n\nThe binary relation <math>\\delta_{\\Phi}</math> is a ''descriptive EF-proximity'', provided the following axioms are satisfied for <math>A,B,E\\subset X</math>.\n\n;dEF.1 : If the set <math>A</math> is descriptively close to <math>B</math>, then <math>B</math> is descriptively close to <math>A</math>.\n;dEF.2 : <math>A\\cup B</math> is descriptively close to <math>E</math>, if and only if, at least one of the sets <math>A</math> or <math>B</math> is descriptively close to <math>E</math>.\n;dEF.3 : Two points <math>x,y\\in X</math> are descriptively close, if and only if, the description of <math>x</math> matches the description of <math>y</math>.\n;dEF.4 : All nonempty sets are descriptively far from the empty set <math>\\emptyset</math>.\n;dEF.5 : For any two sets <math>A</math> and <math>B</math> which are descriptively far from each other, there exists <math>C,D\\in 2^X</math>, <math>C\\cup D = X</math>, such that <math>A</math> is descriptively far from <math>C</math> and <math>B</math> is descriptively far from <math>D</math> (''Descriptive Efremovič axiom'').\n\nThe pair <math>(X, \\delta_{\\Phi})</math> is called a descriptive proximity space.\n\n== Proximal relator spaces ==\nA ''relator'' is a nonvoid family of relations  <math>\\mathcal{R}</math> on a nonempty set <math>X</math>{{cref2|72|1}}.  The pair <math>(X,\\mathcal{R})</math> (also denoted <math>X(\\mathcal{R})</math>) is called a relator space. Relator spaces are natural generalizations of ordered sets and uniform spaces{{cref2|73|1}}{{cref2|74|1}}}. With the introduction of a family of proximity relations <math>\\mathcal{R}_{\\delta}</math> on <math>X</math>, we obtain a proximal relator space <math>(X,\\mathcal{R}_{\\delta})</math>.  For simplicity, we consider only two proximity relations, namely, the Efremovič proximity <math>\\delta</math>{{cref2|8|3}} and the descriptive proximity <math>\\delta_{\\Phi}</math> in defining the ''descriptive relator'' <math>\\mathcal{R}_{\\delta_{\\Phi}}</math>{{cref2|53|2}}{{cref2|48|3}}. The pair <math>(X,\\mathcal{R}_{\\delta_{\\Phi}})</math> is called a ''proximal relator space ''{{cref2|49|1}}. In this work, <math>X</math> denotes a metric topological space that is endowed with the relations in a proximal relator. With the introduction of <math>(X,\\mathcal{R}_{\\delta_{\\Phi}})</math>, the traditional closure of a subset (''e.g.'', {{cref2|9|1}}{{cref2|7|1}}) can be compared with the more recent descriptive closure of a subset.\n\nIn a proximal relator space <math>X</math>, the ''descriptive closure of a set'' <math>A</math> (denoted by <math>\\mbox{cl}_{\\Phi}(A)</math>) is defined by\n\n:<math>\n\\mbox{cl}_{\\Phi}(A) = \\left\\{x\\in X: {\\Phi(x)} \\delta \\mathcal{Q}(\\mbox{cl}(A))\\right\\}.\n</math>\n\nThat is, <math>x\\in X</math> is in the descriptive closure of <math>A</math>, provided the closure of <math>\\Phi(x)</math> and the closure of <math>\\mathcal{Q}(\\mbox{cl}(A))</math> have at least one element in common.\n\n;Theorem 2 {{cref2|50|3}} : The descriptive closure of any set <math>A</math> in the descriptive EF-proximity space <math>(X,\\mathcal{R}_{\\delta_{\\Phi}})</math> is the set of points <math>x\\in X</math> that are descriptively close to <math>A</math>.\n\n;Theorem 3 {{cref2|50|4}} : Kuratowski closure of a set <math>A</math> is a subset of the descriptive closure of <math>A</math> in a descriptive EF-proximity space.\n\n;Theorem 4 {{cref2|49|2}} : Let <math>(X,\\mathcal{R}_{\\delta_{\\Phi}})</math> be a proximal relator space, <math>A\\subset X</math>.  Then <math>\\mbox{cl}(A)\\subseteq \\mbox{cl}_{\\Phi}(A)</math>.\n\n; Proof : Let <math>\\Phi(x)\\in\\mathcal{Q}(X\\setminus \\mbox{cl}(A))</math> such that <math>\\Phi(x) = \\Phi(a)</math>  for some <math>a\\in \\mbox{cl}A</math>. Consequently, <math>\\Phi(x)\\in\\mathcal{Q}(\\mbox{cl}_{\\Phi}(A))</math>.  Hence, <math>\\mbox{cl}(A)\\subseteq \\mbox{cl}_{\\Phi}(A)</math>\n\nIn a proximal relator space, EF-proximity <math>\\delta</math> leads to the following results for descriptive proximity <math>\\delta_{\\Phi}</math>.\n\n;Theorem 5 {{cref2|49|3}} : Let <math>(X,\\mathcal{R}_{\\delta_{\\Phi}})</math> be a proximal relator space, <math>A,B,C\\subset X</math>.  Then\n\n;1<math>^\\circ</math>: <math>A\\ \\delta\\ B\\ \\mbox{implies}\\ A\\ \\delta_{\\Phi}\\ B</math>.\n;2<math>^\\circ</math>: <math> (A\\cup B)\\ \\delta\\ C\\ \\mbox{implies}\\ (A\\cup B)\\ \\delta_{\\Phi}\\ C </math>.\n;3<math>^\\circ</math>: <math> \\mbox{cl}A\\ \\delta\\ \\mbox{cl}B\\ \\mbox{implies}\\ \\mbox{cl}A\\ \\delta_{\\Phi}\\ \\mbox{cl}B</math>.\n\n; Proof : \n;1<math>^\\circ</math>: <math>A\\ \\delta\\ B\\Leftrightarrow A\\cap B\\neq \\emptyset</math>.  For <math>x\\in A\\cap B, \\Phi(x)\\in \\mathcal{Q}(A)</math> and <math>\\Phi(x)\\in \\mathcal{Q}(B)</math>.  Consequently, <math>A\\ \\delta_{\\Phi}\\ B</math>.\n<math>1^\\circ \\Rightarrow\\ 2^\\circ</math>\n;3<math>^\\circ</math>: <math>\\mbox{cl}A\\ \\delta\\ \\mbox{cl}B</math> implies that <math>\\mbox{cl}A</math> and <math>\\mbox{cl}A</math> have at least one point in common. Hence, 1<math>^o\\Rightarrow\\ 3^o</math>.\n\n<math>\\qquad \\blacksquare</math>\n\n== Descriptive <math>\\delta</math>-neighbourhoods ==\n\n[[File:Descriptive.svg|thumb|'''Figure 6.''' Example depicting <math>\\delta</math>-neighbourhoods]]\n\nIn a pseudometric proximal relator space <math>X</math>, the neighbourhood of a point <math>x\\in X</math> (denoted by <math>N_{x,\\varepsilon}</math>), for <math>\\varepsilon > 0</math>, is defined by\n\n:<math>\nN_{x,\\varepsilon} = \\left\\{y\\in X: d(x,y) < \\varepsilon\\right\\}.\n</math>\n\nThe interior of a set <math>A</math> (denoted by <math>\\mbox{int}(A)</math>) and boundary of <math>A</math> (denoted by <math>\\mbox{bdy}(A)</math>) in a proximal relator space <math>X</math> are defined by\n\n:<math>\n\\mbox{int}(A) = \\left\\{x\\in X: N_{x,\\varepsilon}\\subseteq A\\right\\}.\n</math>\n\n:<math>\n\\mbox{bdy}(A) = \\mbox{cl}(A)\\setminus\\mbox{int}(A).\n</math>\n\nA set <math>A</math> has a ''natural strong inclusion'' in a set <math>B</math> associated with <math>\\delta</math>{{cref2|5|1}}{{cref2|6|2}}} (denoted by <math>A\\ \\ll_{\\delta}\\ B</math>), provided <math>A\\subset\\ \\mbox{int}B</math>, ''i.e.'', <math>A\\ \\underline{\\delta}\\ X\\setminus \\mbox{int}B</math> (<math>A</math> is far from the complement of <math>\\mbox{int}B</math>).  Correspondingly, a set <math>A</math> has a ''descriptive strong inclusion'' in a set <math>B</math> associated with <math>\\delta_{\\Phi}</math> (denoted by <math>A\\ \\mathop{\\ll}_{\\Phi}\\ B</math>), provided <math>\\mathcal{Q}(A)\\subset\\ \\mathcal{Q}(\\mbox{int}B)</math>, ''i.e.'', <math>A\\ \\underline{\\delta}_{\\Phi}\\ X\\setminus \\mbox{int}B</math> (<math>\\mathcal{Q}(A)</math> is far from the complement of <math>\\mbox{int}B</math>).\n\nLet <math>\\mathop{\\ll}_{\\Phi}</math> be a descriptive <math>\\delta</math>-neighbourhood relation defined by\n\n:<math>\n\\mathop{\\ll}_{\\Phi} = \\left\\{(A,B)\\in 2^X\\times 2^X: \\mathcal{Q}(A)\\subset \\mathcal{Q}(\\mbox{int}B)\\right\\}.\n</math>\n\nThat is, <math>A\\ \\mathop{\\ll}_{\\Phi}\\ B</math>, provided the description of each <math>a\\in A</math> is contained in the set of descriptions of the points <math>b\\in \\mbox{int}B</math>.  Now observe that any <math>A,B</math> in the proximal relator space <math>X</math> such that <math>A\\ \\underline{\\delta}_{\\Phi}\\ B</math> have disjoint <math>\\delta_{\\Phi}</math>-neighbourhoods,  ''i.e.'',\n \n:<math>\nA\\ \\underline{\\delta}_{\\Phi}\\ B\\Leftrightarrow A\\ \\mathop{\\ll}_{\\Phi}\\ E1, B\\ \\mathop{\\ll}_{\\Phi}\\ E2,\\ \\mbox{for some}\\ E1,E2\\subset X\\ \\mbox{(See Fig. 6).}\n</math>\n\n; Theorem 6 {{cref2|50|5}} : Any two sets descriptively far from each other belong to disjoint descriptive <math>\\delta_{\\Phi}</math>-neighbourhoods in a descriptive proximity space <math>X</math>.\n\nA consideration of strong containment of a nonempty set in another set leads to the study of hit-and-miss topologies and the Wijsman topology{{cref2|2|1}}.\n\n== Tolerance near sets ==\nLet <math>\\varepsilon</math> be a real number greater than zero.  In the study of sets that are proximally near within some tolerance, the set of proximity relations <math>\\mathcal{R}_{\\delta_{\\Phi}}</math> is augmented with a [[pseudometric space|pseudometric]] tolerance proximity relation (denoted by <math>\\delta_{\\Phi,\\varepsilon}</math>) defined by\n\n:<math>\\begin{align}\nD_{\\Phi}(A,B) &= inf\\left\\{d(\\Phi(a),\\Phi(a)): \\Phi(a)\\in\\mathcal{Q}(A), \\Phi(a)\\in \\mathcal{Q}(B)\\right\\},\\\\\nd(\\Phi(a),\\Phi(a)) &= \\mathop{\\sum}_{i=1}^n |\\phi_i(a)-\\phi_i(b)|,\\\\\n\\delta_{\\Phi,\\varepsilon} &= \\left\\{(A,B)\\in 2^X\\times 2^X:\n                       |D(\\mbox{cl}(A), \\mbox{cl}(B))| < \\varepsilon\\right\\}.\n\\end{align}</math>\n\nLet <math>\\mathcal{R}_{\\delta_{\\Phi,\\varepsilon}} = \\mathcal{R}_{\\delta_{\\Phi}}\\cup\\left\\{\\delta_{\\Phi,\\varepsilon}\\right\\}</math>.  In other words, a nonempty set equipped with the proximal relator <math>\\mathcal{R}_{\\delta_{\\Phi,\\varepsilon}}</math> has underlying [[mathematical structure|structure]] provided by the proximal relator <math>\\mathcal{R}_{\\delta_{\\Phi}}</math> and provides a basis for the study of tolerance near sets in <math>X</math> that are near within some tolerance.  Sets <math>A,B</math> in a descriptive pseudometric proximal relator space <math>(X,\\mathcal{R}_{\\delta_{\\Phi,\\varepsilon}})</math> are tolerance near sets (''i.e.'', <math>A\\ \\delta_{\\Phi,\\varepsilon}\\ B</math>), provided\n\n:<math>\nD_{\\Phi}(A,B) < \\varepsilon.\n</math>\n\n== Tolerance classes and preclasses ==\nRelations with the same formal properties as similarity relations of sensations considered by Poincaré{{cref2|62|2}} are nowadays, after [[Christopher Zeeman|Zeeman]]{{cref2|83|2}}, called ''[[tolerance relation]]s''. A ''tolerance <math>\\tau</math> on a set <math>O</math>'' is a relation <math>\\tau \\subseteq O \\times O</math> that is reflexive and symmetric.  In algebra, the term ''tolerance relation'' is also used in a narrow sense to denote reflexive and symmetric relations defined on universes of algebras that are also compatible with operations of a given algebra, ''i.e.'', they are generalizations of congruence relations (see ''e.g.'',{{cref2|12|1}}). In referring to such relations, the term ''algebraic tolerance'' or the term ''algebraic tolerance relation'' is used.\nTransitive tolerance relations are equivalence relations. A set <math>O</math> together with a tolerance <math>\\tau</math> is called a ''tolerance space'' (denoted <math>(O, \\tau)</math>). A set <math>A \\subseteq O</math> is a ''<math>\\tau</math>-preclass'' (or briefly ''preclass'' when <math>\\tau</math> is understood) if and only if for any <math>x,y \\in A</math>, <math>(x,y) \\in \\tau</math>.\n\nThe family of all preclasses of a tolerance space is naturally ordered by set inclusion and preclasses that are maximal with respect to set inclusion are called ''<math>\\tau</math>-classes'' or just ''classes'', when <math>\\tau</math> is understood. The family of all classes of the space <math>(O, \\tau)</math> is particularly interesting and is denoted by <math>H_{\\tau}(O)</math>. The family <math>H_{\\tau}(O)</math> is a covering of <math>O</math>{{cref2|58|1}}.\n\nThe work on similarity by Poincaré and Zeeman presage the introduction of near sets{{cref2|44|2}}{{cref2|43|1}} and research on similarity relations, ''e.g.'',{{cref2|79|1}}.  In science and engineering, tolerance near sets are a practical application of the study of sets that are near within some tolerance.  A tolerance <math>\\varepsilon\\in(0,\\infty]</math> is directly related to the idea of closeness or resemblance (''i.e.'', being within some tolerance) in comparing objects.\nBy way of application of Poincaré's approach in defining visual spaces and Zeeman's approach to tolerance relations, the basic idea is to compare objects such as image patches in the interior of digital images.\n\n=== Examples ===\n\n'''Simple Example'''\n\nThe following simple example demonstrates the construction of tolerance classes from real data. Consider the 20 objects in the table below with <math>|\\Phi| = 1</math>.\n\n:{| class=\"wikitable\" style=\"text-align:center; width:30%\" border=\"1\"\n|+ Sample Perceptual System\n!<math>x_i</math> !! <math>\\phi(x)</math> !! <math>x_i</math> !! <math>\\phi(x)</math> !! <math>x_i</math> !! <math>\\phi(x)</math> !!<math>x_i</math> !! <math>\\phi(x)</math>\n|-\n|<math>x_1</math> || .4518 || <math>x_6</math>    || .6943 || <math>x_{11}</math> ||  .4002 || <math>x_{16}</math> || .6079\n|-\n|<math>x_2</math> || .9166 || <math>x_7</math>    || .9246 || <math>x_{12}</math> ||  .1910 || <math>x_{17}</math> || .1869\n|-\n|<math>x_3</math> || .1398 || <math>x_8</math>    || .3537 || <math>x_{13}</math> ||  .7476 || <math>x_{18}</math> || .8489\n|-\n|<math>x_4</math> || .7972 || <math>x_9</math>    || .4722 || <math>x_{14}</math> ||  .4990 || <math>x_{19}</math> || .9170\n|-\n|<math>x_5</math> || .6281 || <math>x_{10}</math> || .4523 || <math>x_{15}</math> ||  .6289 || <math>x_{20}</math> || .7143\n|}\n\nLet a [[tolerance relation]] be defined as\n\n:<math>\\cong_{\\varepsilon} = \\{(x,y)\\in O \\times O :\\; \\parallel\\Phi(x) - \\Phi(y)\\parallel_{_2} \\leq \\varepsilon\\} </math>\n\nThen, setting <math>\\varepsilon = 0.1</math> gives the following tolerance classes:\n\n:<math>\n\\begin{align}\nH_{\\cong_{\\varepsilon}}(O) = & \\{ \\{x_1, x_8, x_{10}, x_{11}\\},\\{x_1, x_9, x_{10}, x_{11}, x_{14}\\},\\\\\n& \\{x_2, x_7, x_{18}, x_{19}\\},\\\\\n& \\{x_3, x_{12}, x_{17}\\},\\\\\n& \\{x_4, x_{13}, x_{20}\\},\\{x_4, x_{18}\\},\\\\\n& \\{x_5, x_6, x_{15}, x_{16}\\},\\{x_5, x_6, x_{15}, x_{20}\\},\\\\\n& \\{x_6, x_{13}, x_{20}\\}\\}.\n\\end{align}\n</math>\n\nObserve that each object in a tolerance class satisfies the condition <math>\\parallel\\Phi(x) -\\Phi(y)\\parallel_2\\leq\\varepsilon</math>, and that almost all of the objects appear in more than one class. Moreover, there would be twenty classes if the indiscernibility relation was used since there are no two objects with matching descriptions.\n\n'''Image Processing Example'''\n\n[[File:NearImages.svg|thumb|500px|'''Figure 7.''' Example of images that are near each other. (a) and (b) Images from the freely available LeavesDataset (see, ''e.g.'', www.vision.caltech.edu/archive.html).]]\n\nThe following example provides an example based on digital images. Let a subimage be defined as a small subset of [[pixel]]s belonging to a digital image such that the pixels contained in the subimage form a square. Then, let the sets <math> X</math> and <math>Y</math> respectively represent the subimages obtained from two different images, and let <math>O = \\{X \\cup Y\\}</math>. Finally, let the description of an object be given by the Green component in the [[RGB color model]].  The next step is to find all the tolerance classes using the tolerance relation defined in the previous example. Using this information, tolerance classes can be formed containing objects that have similar (within some small <math>\\varepsilon</math>) values for the Green component in the RGB colour model. Furthermore, images that are near (similar) to each other should have tolerance classes divided among both images (instead of a tolerance classes contained solely in one of the images). For example, the  figure accompanying this example shows a subset of the tolerance classes obtained from two leaf images. In this figure, each tolerance class is assigned a separate colour. As can be seen, the two leaves share similar tolerance classes. This example highlights a need to measure the degree of nearness of two sets.\n\n== Nearness measure ==\nLet <math>(U,\\mathcal{R}_{\\delta_{\\Phi,\\varepsilon}})</math> denote a particular descriptive pseudometric EF-proximal relator space equipped with the proximity relation <math>\\delta_{\\Phi,\\varepsilon}</math> and with nonempty subsets <math>X,Y\\in 2^U</math> and with the tolerance relation <math>\\cong_{\\Phi,\\varepsilon}</math> defined in terms of a set of probes <math>\\Phi</math> and with <math>\\varepsilon\\in (0,\\infty]</math>, where\n\n[[File:Visulization of nearness measure.jpg|thumb|500px|'''Figure 8.''' Examples of degree of nearness between two sets: (a) High degree of nearness, and (b) Low degree of nearness.]]\n\n:<math>\n\\simeq_{\\Phi,\\varepsilon} = \\{(x, y)\\in U\\times U\\mid\\ |\\Phi(x) - \\Phi(y)| \\leq\\varepsilon\\}.\n</math>\n\nFurther, assume <math>Z = X\\cup Y</math> and let <math>H_{\\tau_{\\Phi,\\varepsilon}}(Z)</math> denote the family of all classes in the space <math>(Z, \\simeq_{\\Phi,\\varepsilon})</math>.\n\nLet <math>A\\subseteq X, B\\subseteq Y</math>.  The distance <math>D_{_{tNM}}:2^U\\times 2^U:\\longrightarrow [0,\\infty]</math> is defined by\n\n:<math> D_{_{tNM}}(X,Y) =\n\\begin{cases}\n   1-tNM(A,B), &\\mbox{if }X\\mbox{ and }Y\\mbox{ are not empty},\\\\\n   \\infty,&\\mbox{if }X\\mbox{ or }Y\\mbox{ is empty},\n\t\\end{cases}\n</math>\n\nwhere\n\n:<math>\ntNM(A, B) = \\Biggl(\\sum_{C\\in H_{\\tau_{\\Phi,\\varepsilon}}(Z)} |C|\\Biggr)^{-1} \\cdot \\sum_{C\\in H_{\\tau_{\\Phi,\\varepsilon}}(Z)} |C| \\frac{ \\min (|C\\cap A |,|[C\\cap B|)}{\\max (|C\\cap A |,|C\\cap B|)}.\n</math>\n\nThe details concerning <math>tNM</math> are given in{{cref2|14|2}}{{cref2|16|1}}{{cref2|17|4}}. The idea behind <math>tNM</math> is that sets that are similar should have a similar number of objects in each tolerance class. Thus, for each tolerance class obtained from the covering of <math>Z=X\\cup Y</math>, <math>tNM</math> counts the number of objects that belong to <math>X</math> and <math>Y</math> and takes the ratio (as a proper fraction) of their cardinalities. Furthermore, each ratio is weighted by the total size of the tolerance class (thus giving importance to the larger classes) and the final result is normalized by dividing by the sum of all the cardinalities. The range of <math>tNM</math> is in the interval [0,1], where a value of 1 is obtained if the sets are equivalent (based on object descriptions) and a value of 0 is obtained if they have no descriptions in common.\n\nAs an example of the degree of nearness between two sets, consider figure below in which each image consists of two sets of objects, <math>X</math> and <math>Y</math>. Each colour in the figures corresponds to a set where all the objects in the class share the same description. The idea behind <math>tNM</math> is that the nearness of sets in a perceptual system is based on the cardinality of tolerance classes that they share. Thus, the sets in left side of the figure are closer (more near) to each other in terms of their descriptions than the sets in right side of the figure.\n\n== Near set evaluation and recognition (NEAR) system ==\n\n[[File:Near gui.jpg|thumb|'''Figure 9.''' NEAR system GUI.]]\n\nThe Near set Evaluation and Recognition (NEAR) system, is a system developed to demonstrate practical applications of near set theory to the problems of image segmentation evaluation and image correspondence. It was motivated by a need for a freely available software tool that can provide results for research and to generate interest in near set theory. The system implements a Multiple Document Interface (MDI) where each separate processing task is performed in its own child frame. The objects (in the near set sense) in this system are subimages of the images being processed and the probe functions (features) are image processing functions defined on the subimages. The system was written in C++ and was designed to facilitate the addition of new processing tasks and probe functions. Currently, the system performs six major tasks, namely, displaying equivalence and tolerance classes for an image, performing segmentation evaluation, measuring the nearness of two images, performing Content Based Image Retrieval (CBIR), and displaying the output of processing an image using a specific probe function.\n\n== Proximity System ==\n\n[[File:Proximity System.png|thumb|'''Figure 10.''' The Proximity System.]]\n\nThe Proximity System is an application developed to demonstrate descriptive-based topological approaches to nearness and proximity within the context of digital image analysis. The Proximity System grew out of the work of S. Naimpally and J. Peters on Topological Spaces. The Proximity System was written in Java and is intended to run in two different operating environments, namely on Android smartphones and tablets, as well as desktop platforms running the Java Virtual Machine. With respect to the desktop environment, the Proximity System is a cross-platform Java application for Windows, OSX, and Linux systems, which has been tested on Windows 7 and Debian Linux using the Sun Java 6 Runtime. In terms of the implementation of the theoretical approaches, both the Android and the desktop based applications use the same back-end libraries to perform the description-based calculations, where the only differences are the user interface and the Android version has less available features due to restrictions on system resources.\n\n== See also ==\n{{div col|colwidth=20em}}\n* [[Alternative set theory]]\n* [[:Category:Mathematical relations|Category:Mathematical relations]]\n* [[:Category:Topology|Category:Topology]]\n* [[Feature vector]]\n* [[Proximity space]]\n* [[Rough set]]\n* [[Topology]]\n{{div col end}}\n\n== Notes ==\n\n{{Cnote2 Begin}}\n\n{{Cnote2|A|n=1|group=2|J.R. Isbell observed that the notions ''near'' and ''far'' are important in a uniform space. Sets <math>A,B</math> are far ('''uniformaly distal'''), provided the <math>\\{A,B\\}</math> is a discrete collection. A nonempty set <math>U</math> is a ''uniform neighbourhood'' of a set <math>A</math>, provided the complement of <math>U</math> is far from <math>U</math>. See, §33 in {{cref2|23|1}} }}\n\n{{Cnote2|B|n=1|group=2| The intuition that led to the discovery of descriptively near sets is given in Pawlak, Z.;Peters, J.F. (2002, 2007) \"Jak blisko (How Near)\". ''Systemy Wspomagania Decyzji I'' '''57''' (109)}}\n\n{{Cnote2|C|n=1|group=2| Descriptively near sets are introduced in{{cref2|48|4}}. The connections between traditional EF-proximity and descriptive EF-proximity are explored in {{cref2|37|3}}.}}\n\n{{Cnote2|D|n=1|group=2|Reminiscent of M. Pavel's approach, descriptions of members of sets objects are defined relative to vectors of values obtained from real-valued functions called probes. See, Pavel, M. (1993). ''Fundamentals of pattern recognition''. 2nd ed. New York: Marcel Dekker, for the introduction of probe functions considered in the context of image registration.}}\n\n{{Cnote2|E|n=1|group=2|A non-spatial view of near sets appears in, C.J. Mozzochi, M.S. Gagrat, and S.A. Naimpally, Symmetric generalized topological structures, Exposition Press, Hicksville, NY, 1976., and, more recently, nearness of disjoint sets <math>X</math> and <math>Y</math> based on resemblance between pairs of elements <math>x\\in X, y\\in Y</math> (''i.e.'' <math>x</math> and <math>y</math> have similar feature vectors <math>\\boldsymbol{\\phi(x)},\\boldsymbol{\\phi(y)}</math> and the norm <math>\\parallel\\boldsymbol{\\phi(x)} - \\boldsymbol{\\phi(y)}\\parallel_{_p} < \\varepsilon</math>) See, ''e.g.'',{{cref2|43|3}}{{cref2|42|3}}{{cref2|53|3}}.}}\n\n{{Cnote2|F|n=1|group=2|The basic facts about closure of a set were first pointed out by M. Fréchet in{{cref2|11|1}}, and elaborated by B. Knaster and C. Kuratowski in{{cref2|25|1}}.}}\n\n{{Cnote2|G|n=1|group=2|Observe that up to the 1970s, ''proximity'' meant EF-proximity, since this is the one that was studied intensively. The pre-1970 work on proximity spaces is exemplified by the series of papers by J. M. Smirnov during the first half of the 1950s{{cref2|68|1}}{{cref2|67|4}}{{cref2|69|1}}{{cref2|70|1}}, culminating in the compendious collection of results by S.A. Naimpally and B.D. Warrack{{cref2|34|1}}. But in view of later developments, there is a need to distinguish between various proximities. A ''basic proximity'' or ''Čech-proximity'' was introduced by E. Čech during the late 1930s (see §25 A.1, pp. 439-440 in {{cref2|78|1}}). The conditions for the non-symmetric case for a proximity were introduced by S. Leader{{cref2|28|n=1}} and for the symmetric case by M.W. Lodato{{cref2|29|1}}{{cref2|30|1}}{{cref2|31|1}}.}}\n\n{{Cnote2 End}}\n\n== References ==\n\n{{Cnote2 Begin|liststyle=decimal|2}}\n\n{{Cnote2|1|n=1|value=1|{{cite book\n| first1 = J. | last1 = Adámek\n| first2 = H. | last2 = Herrlich\n| first3 = G. E. | last3 = Strecker\n| title = Abstract and concrete categories\n| publisher = Wiley-Interscience\n| location = London\n| date = 1990\n| pages = ix+482}}}}\n\n{{Cnote2|2|n=1|value=2|{{Citation\n| first = G. | last = Beer\n| contribution = Topologies on closed and closed convex sets\n| location = London, UK\n| publisher = Kluwer Academic Pub.\n| date = 1993\n| pages = xi + 340pp}}}}\n\n{{Cnote2|3|n=1|value=3|{{Citation\n| first1 = H. L. | last1 = Bentley\n| first2 = E. | last2 = Colebunders\n| first3 = E. | last3 = Vandermissen\n| contribution = A convenient setting for completions and function spaces\n| series = Contemporary Mathematics \n| editor-first1 = F. | editor-last1 = Mynard \n| editor-first2 =  E. | editor-last2 = Pearl\n| publisher = American Mathematical Society\n| location = Providence, RI\n| date = 2009\n| pages =  37–88}}}}\n\n{{Cnote2|4|n=1|value=4|{{cite journal\n| first1 = P. | last1 = Cameron\n| first2 = J. G. | last2 = Hockingand \n| first3 = S. A. | last3 = Naimpally\n| title = Nearness–a better approach to continuity and limits\n| journal = American Mathematical Monthly \n| volume = 81 \n| date = 1974\n| issue = 7\n| pages = 739–745 | doi=10.2307/2319561| jstor = 2319561\n}}}}\n\n{{Cnote2|5|n=1|value=5|{{Citation\n| first = A. | last = Di Concilio\n| contribution = Action, uniformity and proximity\n| location = Seconda Università di Napoli, Napoli\n| editor-first1 =  S. A. | editor-last1 = Naimpally\n| editor-first2 =  G. | editor-last2 = Di Maio\n| series = Theory and Applications of Proximity, Nearness and Uniformity\n| publisher = Prentice-Hall\n| date = 2008\n| pages = 71–88}}}}\n\n{{Cnote2|6|n=2|value=6|{{cite book\n | last = Di Concilio | first = Anna\n | contribution = Proximity: a powerful tool in extension theory, function spaces, hyperspaces, Boolean algebras and point-free geometry\n | doi = 10.1090/conm/486/09508\n | mr = 2521943\n | pages = 89–114\n | publisher = American Mathematical Society | location = Providence, RI\n | series = Contemporary Mathematics\n | title = Beyond topology\n | volume = 486\n | year = 2009| isbn = 9780821842799\n }}}}\n\n{{Cnote2|7|n=1|value=7|{{cite journal\n| first1 = R. | last1 = Devi\n| first2 = A. | last2 = Selvakumar\n| first3 = M. | last3 = Vigneshwaran\n| title = <math>(I,\\gamma)</math>-generalized semi-closed sets in topological spaces\n| journal = Filomat\n| date = 2010\n| volume = 24\n| issue = 1\n| pages = 97–100 | doi=10.2298/fil1001097d| citeseerx = 10.1.1.430.5991\n}}}}\n\n{{Cnote2|8|n=3|value=8|{{cite journal\n| first = V. A. | last = Efremovič | authorlink = Vadim Arsenyevich Efremovich\n| title = The geometry of proximity I (in Russian)\n| journal = Mat. Sb. (N.S.) \n| volume = 31\n| issue = 73\n| date = 1952\n| pages = 189–200}}}}\n\n{{Cnote2|9|n=1|value=9|{{cite journal\n| first = J. F. | last = Peters\n| title = A note on a-open sets and e<math>^*</math>-sets\n| journal = Filomat\n| date = 2008\n| volume = 22\n| issue = 1\n| pages = 89–96| doi = 10.2298/FIL0801087E\n}}}}\n\n{{Cnote2|10|n=1|value=10|{{cite book\n| first = G. T. | last = Fechner\n| title = Elements of Psychophysics, vol. I \n| publisher = Hold, Rinehart & Winston\n| location = London, UK\n| date = 1966\n| pages = H. E. Adler's trans. of Elemente der Psychophysik, 1860}}}}\n\n{{Cnote2|11|n=1|value=11|{{cite journal \n| first = M. | last = Fréchet\n| title = Sur quelques points du calcul fonctionnel\n| journal = Rend. Circ. Mat. Palermo \n| volume = 22\n| date = 1906\n| pages = 1–74\n | doi=10.1007/bf03018603| hdl = 10338.dmlcz/100655\n}}}}\n\n{{Cnote2|12|n=1|value=12|{{cite journal\n| first1 = G. | last1 = Grätzer \n| first2 = G. H. | last2 = Wenzel\n| title = Tolerances, covering systems, and the axiom of choice\n| journal = Archivum Mathematicum \n| volume = 25 \n| date = 1989\n| issue = 1–2\n| pages = 27–34}}}}\n\n{{Cnote2|13|n=1|value=13|{{cite journal\n| first1 = S. | last1 = Gupta \n| first2 = K. | last2 = Patnaik\n| title = Enhancing performance of face recognition systems by using near set approach for selecting facial features\n| journal = Journal of Theoretical and Applied Information Technology\n| volume = 4 \n| date = 2008\n| issue = 5\n| pages = 433–441}}}}\n\n{{Cnote2|14|n=2|value=14|{{cite journal\n| first1 = A. E. | last1 = Hassanien\n| first2 = A. | last2 = Abraham\n| first3 = J. F. | last3 = Peters\n| first4 = G. | last4 = Schaefer\n| first5 = C. | last5 = Henry\n| title = Rough sets and near sets in medical imaging: A review, IEEE \n| journal = Transactions on Information Technology in Biomedicine\n| volume = 13 \n| date = 2009\n| issue = 6\n| pages = 955–968\n| doi = 10.1109/TITB.2009.2017017| pmid = 19304490\n| citeseerx = 10.1.1.475.6138\n}}}}\n\n{{Cnote2|15|n=1|value=15|{{cite book\n| first = F. | last = Hausdorff\n| title = Grundz¨uge der mengenlehre\n| publisher = Veit and Company\n| location = Leipzig\n| date = 1914\n| pages = viii + 476}}}}\n\n{{Cnote2|16|n=1|value=16|{{cite journal\n| first1 = C. | last1 = Henry \n| first2 = J. F. | last2 = Peters\n| title = Perception-based image classification, International\n| journal = Journal of Intelligent Computing and Cybernetics \n| volume = 3 \n| date = 2010\n| issue = 3\n| pages = 410–430\n | doi=10.1108/17563781011066701}}}}\n\n{{Cnote2|17|n=4|value=17|{{Citation\n| first = C. J. | last = Henry\n| title = Near sets: Theory and applications\n| journal = Ph.D. Thesis, Dept. Elec. Comp. Eng., Uni. Of MB, Supervisor: J.F. Peters\n| date = 2010}}}}\n\n{{Cnote2|18|n=2|value=18|{{cite journal\n| first1 = C. | last1 = Henry \n| first2 = J. F. | last2 = Peters\n| title = Arthritic hand-finger movement similarity measurements: Tolerance near set approach\n| journal = Computational and Mathematical Methods in Medicine \n| date = 2011\n| pages = 569898\n| doi = 10.1155/2011/569898\n | pmid = 21559241 \n| pmc = 3087412 \n| volume=2011}}}}\n\n{{Cnote2|19|n=1|value=19|{{cite journal\n| first1 = C. J. | last1 = Henry \n| first2 = S. | last2 = Ramanna\n| title = Parallel Computation in Finding Near Neighbourhoods\n| journal = Lecture Notes in Computer Science \n| date = 2011\n| pages = 523–532}}}}\n\n{{Cnote2|20|n=2|value=20|{{cite journal\n| first = H. | last = Herrlich\n| title = A concept of nearness\n| journal =  General Topology and Its Applications\n| volume = 4 \n| issue = 3\n| date = 1974\n| pages = 191–212\n | doi=10.1016/0016-660x(74)90021-x}}}}\n\n{{Cnote2|21|n=1|value=21|{{Citation\n| first1 = J. G. | last1 = Hocking\n| first2 = S. A. | last2 = Naimpally\n| contribution = Nearness—a better approach to continuity and limits\n| series = Allahabad Mathematical Society Lecture Note Series\n| volume = 3\n| publisher = The Allahabad Mathematical Society\n| location = Allahabad\n| date = 2009\n| pages = iv+66\n| isbn = 978-81-908159-1-8}}}}\n\n{{Cnote2|22|n=1|value=22|{{cite journal\n| first = E. | last = Ïnan \n| first2 = M. A. | last2 = Öztürk\n| title = Near groups on nearness approximation spaces\n| journal = Hacettepe Journal of Mathematics and Statistics\n| volume = 41\n| date = 2012\n| issue = 4\n| pages = 545–558}}}}\n\n{{Cnote2|23|n=1|value=23|{{cite book\n| first = J. R. | last = Isbell\n| title = Uniform spaces\n| publisher = American Mathematical Society\n| location = Providence, Rhode Island\n| date = 1964\n| pages = xi + 175}}}}\n\n{{Cnote2|24|n=1|value=24|{{cite journal \n| first1 = V. M. | last1 = Ivanova \n| first2 = A. A. | last2 = Ivanov\n| title = Contiguity spaces and bicompact extensions of topological spaces (russian)\n| journal = Dokl. Akad. Nauk SSSR \n| volume = 127 \n| date = 1959\n| pages = 20–22}}}}\n\n{{Cnote2|25|n=1|value=25|{{cite journal\n| first1 = B. | last1 = Knaster \n| first2 = C. | last2 = Kuratowski\n| title = Sur les ensembles connexes\n| journal = Fundamenta Mathematicae\n| volume = 2\n| date = 1921\n| pages = 206–255| doi = 10.4064/fm-2-1-206-255 \n}}}}\n\n{{Cnote2|26|n=1|value=26|{{cite arXiv\n| first = M. M. | last = Kovár\n| title = A new causal topology and why the universe is co-compact\n| eprint=1112.0817\n| class=math-ph\n| date = 2011\n}}}}\n\n{{Cnote2|27|n=1|value=27|{{Citation\n| first = C. | last = Kuratowski\n| contribution = Topologie i\n| location = Warsaw\n| publisher = Panstwowe Wydawnictwo Naukowe\n| date = 1958\n| pages = XIII + 494pp}}}}\n\n{{Cnote2|28|n=1|value=28|{{cite journal\n| first = S. | last = Leader\n| title = Metrization of proximity spaces\n| journal = Proceedings of the American Mathematical Society\n| volume = 18\n| issue = 6\n| date = 1967\n| pages = 1084–1088\n | doi=10.2307/2035803| jstor = 2035803\n}}}}\n\n{{Cnote2|29|n=1|value=29|{{Citation\n| first = M. W. | last = Lodato\n| title = On topologically induced generalized proximity relations\n| journal = Ph.D. Thesis, Rutgers University\n| date = 1962}}}}\n\n{{Cnote2|30|n=1|value=30|{{cite journal\n| first = M. W. | last = Lodato\n| title = On topologically induced generalized proximity relations I\n| journal =  Proceedings of the American Mathematical Society\n| volume = 15 \n| issue = 3\n| date = 1964\n| pages = 417–422\n | doi=10.2307/2034517| jstor = 2034517\n}}}}\n\n{{Cnote2|31|n=1|value=31|{{cite journal\n| first = M. W. | last = Lodato\n| title = On topologically induced generalized proximity relations II\n| journal =  Pacific Journal of Mathematics\n| volume = 17\n| date = 1966\n| pages = 131–135| doi = 10.2140/pjm.1966.17.131\n}}}}\n\n{{Cnote2|32|n=1|value=32|{{cite book\n| first = S. | last = MacLane\n| title = Categories for the working mathematician\n| publisher = Springer\n| location = Berlin\n| date = 1971\n| pages = v+262pp}}}}\n\n{{Cnote2|33|n=1|value=33|{{Citation\n| first1 = C. J. | last1 = Mozzochi \n| first2 = S. A. | last2 = Naimpally\n| contribution = Uniformity and proximity\n| series = Allahabad Mathematical Society Lecture Note Series\n| volume = 2\n| publisher = The Allahabad Mathematical Society\n| location = Allahabad\n| date = 2009\n| pages = xii+153\n| isbn = 978-81-908159-1-8}}}}\n\n{{Cnote2|34|n=1|value=34|{{cite book\n| first = S. A. | last = Naimpally\n| title = Proximity spaces\n| publisher = Cambridge University Press\n| location = Cambridge, UK\n| date = 1970\n| pages = x+128\n| isbn = 978-0-521-09183-1}}}}\n\n{{Cnote2|35|n=1|value=35|{{cite book\n| first = S. A. | last = Naimpally\n| title = Proximity approach to problems in topology and analysis\n| publisher = Oldenbourg Verlag\n| location = Munich, Germany\n| date = 2009\n| pages = ix + 204\n| isbn = 978-3-486-58917-7}}}}\n\n{{Cnote2|36|n=1|value=36|{{cite journal\n| first1 = S. A. | last1 = Naimpally \n| first2 = J. F. | last2 = Peters\n| title = Preservation of continuity\n| journal = Scientiae Mathematicae Japonicae \n| volume = 76\n| date = 2013\n| issue = 2\n| pages = 1–7}}}}\n\n{{Cnote2|37|n=4|value=37|{{cite book\n| last1 = Naimpally | first1 = S. A.\n| last2 = Peters | first2 = J. F. \n| title = Topology with Applications. Topological Spaces via Near and Far\n| publisher = World Scientific\n| location = Singapore\n| date = 2013}}}}\n\n{{Cnote2|38|n=1|value=38|{{cite book\n| last1 = Naimpally | first1 = S. A. \n| last2 = Peters | first2 = J. F.\n| last3 = Wolski | first3 = M. \n| title = Near set theory and applications\n| series = Special Issue in Mathematics in Computer Science\n| volume = 7\n| publisher = Springer\n| location = Berlin\n| date = 2013\n| pages = 136}}}}\n\n{{Cnote2|39|n=1|value=39|{{Citation\n| first1 = S. A. | last1 = Naimpally \n| first2 = B. D. | last2 = Warrack\n| contribution = Proximity spaces\n| series = Cambridge Tract in Mathematics\n| volume = 59\n| publisher = Cambridge University Press\n| location = Cambridge, UK\n| date = 1970\n| pages = x+128}}}}\n\n{{Cnote2|40|n=1|value=40|{{cite book\n| first1 = S. K. | last1 = Pal \n| first2 = J. F. | last2 = Peters\n| title = Rough fuzzy image analysis. Foundations and methodologies\n| publisher = CRC Press, Taylor & Francis Group\n| location = London, UK \n| date = 2010\n| isbn = 9781439803295}}}}\n\n{{Cnote2|41|n=1|value=41|{{cite journal\n| first = J. F. | last = Peters\n| title = Tolerance near sets and image correspondence\n| journal = International Journal of Bio-Inspired Computation \n| volume = 1 \n| date = 2009\n| issue = 4\n| pages = 239–245\n | doi=10.1504/ijbic.2009.024722}}}}\n\n{{Cnote2|42|n=3|value=42|{{cite journal\n| last1 = Peters | first1 = J. F. \n| last2 = Wasilewski | first2 = P. \n| title = Foundations of near sets\n| journal = Information Sciences\n| volume = 179 \n| issue = 18\n| pages = 3091–3109\n| date = 2009\n | doi=10.1016/j.ins.2009.04.018}}}}\n\n{{Cnote2|43|n=3|value=43|{{cite journal\n| first = J. F. | last = Peters \n| title = Near sets. General theory about nearness of objects\n| journal = Applied Mathematical Sciences\n| volume = 1\n| date = 2007\n| issue = 53\n| pages = 2609–2629}}}}\n\n{{Cnote2|44|n=2|value=44|{{cite journal\n| first1 = J. F. | last1 = Peters\n| title = Near sets. Special theory about nearness of objects\n| journal = Fundamenta Informaticae\n| volume = 75 \n| date = 2007\n| issue = 1–4\n| pages = 407–433}}}}\n\n{{Cnote2|45|n=1|value=45|{{cite journal\n| first = J. F. | last = Peters\n| title = Corrigenda and addenda: Tolerance near sets and image correspondence\n| journal = International Journal of Bio-Inspired Computation\n| volume = 2\n| issue = 5\n| pages = 310–318\n| date = 2010\n | doi=10.1504/ijbic.2010.036157}}}}\n\n{{Cnote2|46|n=1|value=46|{{Citation\n| first = J. F. | last = Peters\n| contribution = How near are Zdzisław Pawlak's paintings? Merotopic distance between regions of interest\n| series = Intelligent Systems Reference Library volume dedicated to Prof. Zdzisław Pawlak\n| editor-first1 = A. | editor-last1 = Skowron \n| editor-first2 = S. | editor-last2 = Suraj\n| publisher = Springer\n| location = Berlin\n| date = 2011\n| pages = 1–19}}}}\n\n{{Cnote2|47|n=1|value=47|{{Citation\n| last = Peters | first = J. F. \n| contribution = Sufficiently near sets of neighbourhoods\n| series = Lecture Notes in Artificial Intelligence 6954\n| editor-last1 = Yao | editor-first1 = J. T. \n| editor-last2 = Ramanna | editor-first2 = S.\n| editor-last3 = Wang | editor-first3 = G. \n|display-editors = 3 | editor-last4 = Suraj | editor-first4 = Z.\n| publisher = Springer\n| place = Berlin\n| pages = 17–24 \n| date = 2011}}}}\n\n{{Cnote2|48|n=4|value=48|{{cite journal\n| last = Peters | first = J. F.\n| date = 2013\n| title = Near sets: An introduction \n| journal = Mathematics in Computer Science\n| volume = 7\n| issue = 1\n| pages = 3–9\n| doi = 10.1007/s11786-013-0149-6}}}}\n\n{{Cnote2|49|n=3|value=49|{{cite journal\n| first = J. F. | last = Peters\n| title = Proximal relator spaces\n| journal = Filomat\n| date = 2014\n| pages = 1–5 (''in press'')}}}}\n\n{{Cnote2|50|n=5|value=50|{{cite book\n| last = Peters | first = J. F. \n| title = Topology of Digital Images. Visual Pattern Discovery in Proximity Spaces\n| publisher = Springer\n| date = 2014\n| volume = 63\n| isbn = 978-3-642-53844-5\n| pages = 342}}}}\n\n{{Cnote2|51|n=2|value=51|{{cite journal\n| last1 = Peters | first1 = J. F. \n| last2 = İnan | first2 = E.\n| last3 = Öztürk | first3 = M. A.\n| title = Spatial and descriptive isometries in proximity spaces\n| journal = General Mathematics Notes \n| volume = 21 \n| issue = 2\n| date = 2014\n| pages = 125–134}}}}\n\n{{Cnote2|52|n=1|value=52|{{cite journal\n| last1 = Peters | first1 = J. F. \n| last2 = Naimpally | first2 = S. A.\n| title = Approach spaces for near families\n| journal = General Mathematics Notes\n| volume = 2\n| issue = 1\n| pages = 159–164\n| date = 2011}}}}\n\n{{Cnote2|53|n=3|value=53|{{cite journal\n| first1 = J. F. | last1 = Peters\n| first2 = S. A. | last2 = Naimpally\n| journal = General Mathematics Notes\n| volume = 2\n| date = 2011\n| issue = 1\n| pages = 159–164}}}}\n\n{{Cnote2|54|n=1|value=54|{{cite journal\n| last1 = Peters | first1 = J. F.\n| last2 = Puzio | first2 = L. \n| title = Image analysis with anisotropic wavelet-based nearness measures\n| journal = International Journal of Computational Intelligence Systems \n| volume = 2 \n| date = 2009\n| issue = 3\n| pages = 168–183\n| doi = 10.1016/j.ins.2009.04.018}}}}\n\n{{Cnote2|55|n=1|value=55|{{Citation\n| first1 = J. F. | last1 = Peters\n| first2 = S. | last2 = Shahfar\n| first3 =  S. | last3 = Ramanna\n| first4 = T. | last4 = Szturm\n| contribution = Biologically-inspired adaptive learning: A near set approach\n| series = Frontiers in the Convergence of Bioscience and Information Technologies\n| location = Korea\n| date = 2007}}}}\n\n{{Cnote2|56|n=1|value=56|{{cite journal\n| last1 = Peters | first1 = J. F. \n| last2 = Tiwari | first2 = S. \n| title = Approach merotopies and near filters. Theory and application\n| journal = General Mathematics Notes\n| volume = 3\n| issue = 1\n| pages = 32–45\n| date = 2011}}}}\n\n{{Cnote2|57|n=1|value=57|{{cite journal\n| first1 = J. F. | last1 = Peters \n| first2 = S. | last2 = Tiwari\n| title = Approach merotopies and near filters. Theory and application\n| journal = General Mathematics Notes\n| volume = 3 \n| date = 2011\n| issue = 1\n| pages = 32–45}}}}\n\n{{Cnote2|58|n=1|value=58|{{cite journal\n| first1 = J. F. | last1 = Peters \n| first2 = P. | last2 = Wasilewski\n| title = Tolerance spaces: Origins, theoretical aspects and applications\n| journal = Information Sciences\n| volume = 195 \n| date = 2012\n| pages = 211–225\n | doi=10.1016/j.ins.2012.01.023}}}}\n\n{{Cnote2|59|n=1|value=59|{{cite journal\n| first = J. | last = Picado\n| title = Weil nearness spaces\n| journal = Portugaliae Mathematica\n| volume = 55\n| issue = 2\n| pages = 233–254}}}}\n\n{{Cnote2|60|n=3|value=60|{{cite journal\n| first = J. H. | last = Poincaré\n| title = L'espace et la géomètrie\n| journal = Revue de M'etaphysique et de Morale\n| volume = 3\n| date = 1895\n| pages = 631–646}}}}\n\n{{Cnote2|61|n=3|value=61|{{cite journal\n| first = J. H. | last = Poincaré\n| title = Sur certaines surfaces algébriques; troisième complément 'a l'analysis situs\n| journal = Bulletin de la Société Mathématique de France\n| volume = 30 \n| date = 1902\n| pages = 49–70}}}}\n\n{{Cnote2|62|n=2|value=62|{{cite book\n| first = J. H. | last = Poincaré\n| title = Dernières pensées, trans. by J.W. Bolduc as Mathematics and science: Last essays\n| publisher = Flammarion & Kessinger\n| location = Paris & NY\n| date = 1913 & 2009}}}}\n\n{{Cnote2|63|n=2|value=63|{{cite journal\n| first = J. H. | last = Poincaré\n| title = Sur la nature du raisonnement mathématique\n| journal = Revue de Méaphysique et de Morale\n| volume = 2 \n| date = 1894\n| pages = 371–384}}}}\n\n{{Cnote2|64|n=2|value=64|{{cite journal\n| first1 = S. | last1 = Ramanna\n| first2 = A. H. | last2 = Meghdadi\n| title = Measuring resemblances between swarm behaviours: A perceptual tolerance near set approach\n| journal = Fundamenta Informaticae\n| volume = 95 \n| date = 2009\n| issue = 4\n| pages = 533–552\n| doi = 10.3233/FI-2009-163}}}}\n\n{{Cnote2|65|n=2|value=65|{{cite journal\n| first = F. | last = Riesz | authorlink = Frigyes Riesz\n| title = Stetigkeitsbegriff und abstrakte mengenlehre\n| journal = Atti del IV Congresso Internazionale dei Matematici II \n| date = 1908\n| url = http://www.mathunion.org/ICM/ICM1908.2/Main/icm1908.2.0018.0024.ocr.pdf\n| pages = 18–24}}}}\n\n{{Cnote2|66|n=1|value=66|{{cite book\n| first = J. A. | last = Shreider\n| title = Equality, resemblance, and order\n| publisher = Mir Publishers\n| location = Russia\n| date = 1975\n| pages = 279}}}}\n\n{{Cnote2|67|n=4|value=67|{{cite journal\n| first = J. M. | last = Smirnov\n| title = On proximity spaces\n| journal = Mat. Sb. (N.S.)\n| volume = 31 \n| date = 1952\n| issue = 73\n| pages = 543–574 (English translation: Amer. Math. Soc. Trans. Ser. 2, 38, 1964, 5–35)}}}}\n\n{{Cnote2|68|n=1|value=68|{{cite journal\n| first = J. M. | last = Smirnov\n| title = On proximity spaces in the sense of V.A. Efremovič\n| journal = Math. Sb. (N.S.) \n| volume = 84\n| date = 1952\n| pages = 895–898, English translation: Amer. Math. Soc. Trans. Ser. 2, 38, 1964, 1–4}}}}\n\n{{Cnote2|69|n=1|value=69|{{cite journal\n| first = J. M. | last = Smirnov\n| title = On the completeness of proximity spaces. I.\n| journal = Trudy Moskov. Mat. Obšč \n| volume = 3\n| date = 1954\n| pages = 271–306, English translation: Amer. Math. Soc. Trans. Ser. 2, 38, 1964, 37–74}}}}\n\n{{Cnote2|70|n=1|value=70|{{cite journal\n| first = J. M. | last = Smirnov\n| title = On the completeness of proximity spaces. II.\n| journal = Trudy Moskov. Mat. Obšč \n| volume = 4\n| date = 1955\n| pages = 421–438, English translation: Amer. Math. Soc. Trans. Ser. 2, 38, 1964, 75–94}}}}\n\n{{Cnote2|71|n=2|value=71|{{cite journal\n| first = A. B. | last = Sossinsky\n| title = Tolerance space theory and some applications\n| journal = Acta Applicandae Mathematicae \n| volume = 5 \n| date = 1986\n| issue = 2\n| pages = 137–167\n | doi=10.1007/bf00046585}}}}\n\n{{Cnote2|72|n=1|value=72|{{cite journal\n| first = Á. | last = Száz\n| title = Uniformly, proximally and topologically compact relators\n| journal = Mathematica Pannonica\n| volume = 8\n| issue = 1\n| pages = 103–116\n| date = 1997}}}}\n\n{{Cnote2|73|n=1|value=73|{{cite journal\n| first = Á. | last = Száz\n| title = Basic tools and mild continuities in relator spaces\n| journal = Acta Mathematica Hungarica\n| volume = 50 \n| issue = 3–4\n| date = 1987\n| pages = 177–201\n | doi=10.1007/bf01903935}}}}\n\n{{Cnote2|74|n=1|value=74|{{cite journal\n| first = Á | last = Száz\n| title = An extension of Kelley's closed relation theorem to relator spaces\n| journal = Filomat\n| volume = 14\n| date = 2000\n| pages = 49–71}}}}\n\n{{Cnote2|75|n=1|value=75|{{Citation\n| first = S. | last = Tiwari\n| title = Some aspects of general topology and applications. Approach merotopic structures and applications\n| journal = Ph.D. Thesis, Dept. Of Math., Allahabad (U.P.), India, Supervisor: M. Khare\n| date = 2010}}}}\n\n{{Cnote2|76|n=2|value=76|{{cite journal\n| first1 =  S. | last1 = Tiwari \n| first2 = J. F. | last2 = Peters\n| title = A new approach to the study of extended metric spaces\n| journal = Mathematica Aeterna\n| volume = 3\n| date = 2013\n| issue = 7\n| pages = 565–577}}}}\n\n{{Cnote2|77|n=1|value=77|{{Citation\n| first = J. W. | last = Tukey\n| contribution = Convergence and uniformity in topology\n| publisher = Princeton Univ. Press\n| series = Annals of Mathematics Studies \n| volume = AM-2\n| location = Princeton, NJ\n| date = 1940\n| pages = 90}}}}\n\n{{Cnote2|78|n=1|value=78|{{cite book\n| first = E. | last = Čech\n| title = Topological spaces, revised ed. by Z. Frolik and M. Katětov\n| publisher = John Wiley & Sons\n| location = London\n| date = 1966\n| pages = 893}}}}\n\n{{Cnote2|79|n=1|value=79|{{Citation\n| first = P. | last = Wasilewski\n| title = On selected similarity relations and their applications into cognitive science\n| journal = Ph.D. Thesis, Dept. Logic\n| date = 2004}}}}\n\n{{Cnote2|80|n=1|value=80|{{cite journal\n| first1 = P. | last1 = Wasilewski\n| first2 = J. F. | last2 = Peters\n| first3 = S. | last3 = Ramanna\n| title = Perceptual tolerance intersection\n| journal = Transactions on Rough Sets XIII\n| date = 2011\n| pages = 159–174}}}}\n\n{{Cnote2|81|n=1|value=81|{{Citation\n| first = A. | last = Weil\n| contribution = Sur les espaces à structure uniforme et sur la topologie générale\n| publisher = Harmann & cie\n| series = Actualités scientifique et industrielles\n| location = Paris\n| date = 1938}}}}\n\n{{Cnote2|82|n=1|value=82|{{cite journal\n| last1 = Wolski | first1 = M.\n| title = Perception and classification. A note on near sets and rough sets\n| journal = Fundamenta Informaticae\n| volume = 101\n| pages = 143–155\n| date = 2010}}}}\n\n{{Cnote2|83|n=2|value=83|{{Citation\n| first = E. C. | last = Zeeman | authorlink = Christopher Zeeman\n| contribution = The topology of the brain and visual perception\n| location = University of Georgia Institute Conference Proceedings (1962)\n| editor-first1 =  M. K. | editor-last1 = Fort, Jr.\n| series = Topology of 3-Manifolds and Related Topics\n| publisher = Prentice-Hall\n| date = 1962\n| pages = 240–256}}}}\n\n{{Cnote2 End}}\n\n==Further reading==\n{{refbegin}}\n* {{cite book\n| first1 = S. A. | last1 = Naimpally \n| first2 = J. F. | last2 = Peters\n| url = http://www.ams.org/mathscinet/search/publdoc.html?pg1=INDI&s1=334268&vfpref=html&r=12&mx-pid=3075111 \n| title = Topology with Applications. Topological Spaces via Near and Far\n| publisher = World Scientific Publishing . Co. Pte. Ltd\n| date = 2013\n| isbn = 978-981-4407-65-6}}\n* {{Citation\n| first1 =  S. A. | last1 = Naimpally\n| first2 = J. F. | last2 = Peters\n| first3 = M. | last3 = Wolski\n| url = https://link.springer.com/journal/11786/7/1 \n| title = Near Set Theory and Applications\n| series = Mathematics in Computer Science\n| publisher = Springer\n| location = Berlin\n| volume = 7 \n| issue = 1\n| date = 2013}}\n* {{Citation\n| first = J. F. | last = Peters\n| url = https://link.springer.com/book/10.1007\\%2F978-3-642-53845-2 \n| title = Topology of Digital Images. Visual Pattern Discovery in Proximity Spaces\n| series = Intelligent Systems Reference Library\n| volume = 63\n| publisher = Springer\n| location = Berlin\n| date = 2014}}\n* {{Citation\n| first1 = C. J. | last1 = Henry\n| first2 = J. F. | last2 = Peters\n| url = http://wren.ee.umanitoba.ca/index.php?option=com_content&view=article&id=51&Itemid=78\n| title = Near set evaluation and recognition (NEAR) system V3.0\n| publisher = Computational Intelligence Laboratory, University of Manitoba\n| series = UM CI Laboratory Technical Report No. TR-2009-015\n| date = 2012}}\n* {{cite journal\n| first1 = A. Di | last1 = Concilio\n| title = Proximity: A powerful tool in extension theory, function spaces, hyperspaces, boolean algebras and point-free geometry\n| journal = Computational Intelligence Laboratory, University of Manitoba\n| series = UM CI Laboratory Technical Report No. TR-2009-021\n| date = 2014}}\n* {{cite journal\n| first1 = J. F. | last1 = Peters\n| first2 = S. A. | last2 = Naimpally \n| title =  Applications of near sets\n| journal = Notices of the American Mathematical Society \n| date = 2012\n| volume = 59\n| issue = 4\n| pages = 536–542\n| citeseerx = 10.1.1.371.7903\n | url=http://www.ams.org/notices/201204/rtx120400536p.pdf\n}}\n{{refend}}\n\n[[Category:Abstract algebra]]\n[[Category:Mathematical relations]]\n[[Category:Systems of set theory]]\n[[Category:Topology]]"
    },
    {
      "title": "Near-ring",
      "url": "https://en.wikipedia.org/wiki/Near-ring",
      "text": "In [[mathematics]], a '''near-ring''' (also '''near ring''' or '''nearring''') is an [[Abstract algebra|algebraic]] [[Algebraic structure|structure]] similar to a [[Ring (algebra)|ring]] but satisfying fewer [[axiom]]s. Near-rings arise naturally from [[Function (mathematics)|functions]] on [[Group (mathematics)|group]]s.\n\n{{Algebraic structures |Ring}}\n\n== Definition ==\nA [[set (mathematics)|set]] ''N'' together with two [[binary operation]]s + (called ''[[addition]]'') and ⋅ (called ''[[multiplication]]'') is called a (right) ''near-ring'' if:\n:A1: ''N'' is a [[group (mathematics)|group]] (not necessarily [[abelian group|abelian]]) under addition;\n:A2: multiplication is [[associative property|associative]] (so ''N'' is a [[semigroup]] under multiplication); and\n:A3: multiplication ''on the right'' [[distributive property|distribute]]s over addition: for any ''x'', ''y'', ''z'' in ''N'', it holds that (''x'' + ''y'')⋅''z'' = (''x''⋅''z'') + (''y''⋅''z'').<ref name=\"Pilz82-Appl\">G. Pilz, (1982), \"Near-Rings: What They Are and What They Are Good For\" in ''Contemp. Math.'', 9, pp. 97–119. Amer. Math. Soc., Providence, R.I., 1981.</ref>\n\nSimilarly, it is possible to define a ''[[left and right (algebra)|left]] near-ring'' by replacing the right distributive law A3 by the corresponding left distributive law. Both right and left near-rings occur in the literature; for instance, the book of Pilz<ref name=\"Pilz_book\">G. Pilz, \"Near-rings, the Theory and its Applications\", North-Holland, Amsterdam, 2nd edition, (1983).</ref> uses right near-rings, while the book of Clay<ref name=\"Clay\">J. Clay, \"Nearrings: Geneses and applications\", Oxford, (1992).</ref> uses left near-rings.\n\nAn immediate consequence of this ''one-sided distributive law'' is that it is true that 0⋅''x'' = 0 but it is not necessarily true that ''x''⋅0 = 0 for any ''x'' in ''N''.  Another immediate consequence is that (&minus;''x'')⋅''y'' = &minus;(''x''⋅''y'') for any ''x'', ''y'' in ''N'', but it is not necessary that ''x''⋅(&minus;''y'') = &minus;(''x''⋅''y''). A near-ring is a [[ring theory|ring]] (not necessarily with unity) [[if and only if]] addition is commutative and multiplication is also distributive over addition on the ''left''.  If the near-ring has a multiplicative identity, then distributivity on both sides is sufficient, and commutativity of addition follows automatically.\n\n== Mappings from a group to itself ==\n\nLet ''G'' be a group, written additively but not necessarily [[Abelian group|abelian]], and let ''M''(''G'') be the set {''f'' | ''f'' : ''G'' → ''G''} of all [[function (mathematics)|function]]s from ''G'' to ''G''.  An addition operation can be defined on ''M''(''G''): given ''f'', ''g'' in ''M''(''G''), then the mapping ''f'' + ''g'' from ''G'' to ''G'' is given by (''f'' + ''g'')(''x'') = ''f''(''x'') + ''g''(''x'') for all ''x'' in ''G''.  Then (''M''(''G''),&#8239;+) is also a group, which is abelian if and only if ''G'' is abelian.  Taking the composition of mappings as the product ⋅, ''M''(''G'') becomes a near-ring.\n\nThe 0 element of the near-ring ''M''(''G'') is the [[zero map]], i.e., the mapping which takes every element of ''G'' to the identity element of ''G''.  The additive inverse −''f'' of ''f'' in ''M''(''G'') coincides with the natural [[pointwise]] definition, that is, (−''f'')(''x'') = −(''f''(''x'')) for all ''x'' in ''G''.\n\nIf ''G'' has at least 2 elements, ''M''(''G'') is not a ring, even if ''G'' is abelian.  (Consider a [[constant function|constant mapping]] ''g'' from ''G'' to a fixed element ''g'' ≠ 0 of ''G''; then ''g''⋅0 = ''g'' ≠ 0.) However, there is a subset ''E''(''G'') of ''M''(''G'') consisting of all group [[endomorphism]]s of ''G'', that is, all maps ''f'' : ''G'' → ''G'' such that ''f''(''x'' + ''y'') = ''f''(''x'') + ''f''(''y'') for all ''x'', ''y'' in ''G''.  If (''G'',&#8239;+) is abelian, both near-ring operations on ''M''(''G'') are closed on ''E''(''G''), and (''E''(''G''),&#8239;+,&#8239;⋅) is a ring.  If (''G'',&#8239;+) is nonabelian, ''E''(''G'') is generally not closed under the near-ring operations; but the closure of ''E''(''G'') under the near-ring operations is a near-ring.\n\nMany subsets of ''M''(''G'') form interesting and useful near-rings.   For example:<ref name=\"Pilz82-Appl\"/>\n*The mappings for which ''f''(0) = 0.\n*The constant mappings, i.e., those that map every element of the group to one fixed element.\n*The set of maps generated by addition and negation from the [[endomorphism]]s of the group (the \"additive closure\" of the set of endomorphisms).  If G is abelian then the set of endomorphisms is already additively closed, so that the additive closure is just the set of endomorphisms of G, and it forms not just a near-ring, but a ring.\n\nFurther examples occur if the group has further structure, for example:\n*The continuous mappings in a [[topological group]].\n*The polynomial functions on a ring with identity under addition and polynomial composition.\n*The affine maps in a [[vector space]].\n\nEvery near-ring is [[Isomorphism|isomorphic]] to a subnear-ring of ''M''(''G'') for some ''G''.\n\n== Applications ==\nMany applications involve the subclass of near-rings known as [[Near-field (mathematics)|near-fields]]; for these see the article on near-fields.\n\nThere are various applications of proper near-rings, i.e., those that are neither rings nor near-fields.\n\nThe best known is to [[Block design|balanced incomplete block designs]]<ref name=\"Pilz_book\"/> using planar near-rings. These are a way to obtain [[Difference set|difference families]] using the orbits of a fixed point free automorphism group of a group. Clay and others have extended these ideas to more general geometrical constructions<ref name=\"Clay\"/>.\n\n==See also==\n* [[Near-field (mathematics)]]\n* [[Semiring]]\n* [[Near-semiring]]\n\n==References==\n{{reflist}}\n* {{cite book|author1=Celestina Cotti Ferrero|author2=Giovanni Ferrero|title=Nearrings: Some Developments Linked to Semigroups and Groups|year=2002|publisher=Kluwer Academic Publishers|isbn=978-1-4613-0267-4}}\n\n==External links==\n* The [http://www.algebra.uni-linz.ac.at/Nearrings/ Near Ring Main Page] at the [[Johannes Kepler Universität Linz]]\n\n[[Category:Abstract algebra]]"
    },
    {
      "title": "Emmy Noether bibliography",
      "url": "https://en.wikipedia.org/wiki/Emmy_Noether_bibliography",
      "text": "[[Emmy Noether]] was a German mathematician. This article lists the publications upon which her reputation is built (in part).\n\n==First epoch (1908–1919)==\n\n{| class=\"wikitable sortable\" id=\"Noether_publications_first_epoch\"\n! Index<ref name=\"index_numbers\">These Index numbers are used for cross-referencing in the \"Classification and notes\" column.  The numbers are taken from the Brewer and Smith reference cited in the Bibliography, pp. 175–177.</ref> !! Year !! Title and English translation<ref name=\"translation\" >The translations shown in black are taken from the Kimberling source.  Unofficial translations are given in purple font.</ref> !! Journal, volume, pages !! Classification and notes\n|-\n| <span id=\"noether_1\"></span>1 || 1907 || [http://gdz.sub.uni-goettingen.de/dms/load/img/?IDDOC=39728 Über die Bildung des Formensystems der ternären biquadratischen Form]\n{|\n| On Complete Systems of Invariants for Ternary Biquadratic Forms\n|}\n| ''Sitzung Berichte der Physikal.-mediz. Sozietät in Erlangen'', '''39''', 176–179 ||  '''[[Algebraic invariant]]s'''.  Preliminary 4-page report on her dissertation results.\n|-\n| <span id=\"noether_2\"></span>2 || 1908 || [http://gdz.sub.uni-goettingen.de/no_cache/dms/load/img/?IDDOC=261200 Über die Bildung des Formensystems der ternären biquadratischen Form]\n{|\n| On Complete Systems of Invariants for Ternary Biquadratic Forms\n|}\n| ''Journal für die reine und angewandte Mathematik'', '''134''', 23–90 + 2 tables ||  '''[[Algebraic invariant]]s'''.  Main description of her dissertation, including 331 explicitly calculated ternary invariants.\n|-\n| <span id=\"noether_3\"></span>3 || 1910 || [http://gdz.sub.uni-goettingen.de/no_cache/dms/load/img/?IDDOC=244194 Zur Invariantentheorie der Formen von ''n'' Variabeln]\n{|\n| On the Theory of Invariants for Forms of ''n'' Variables<sup>§</sup>\n|}\n| ''Jahresbericht der Deutschen Mathematiker-Vereinigung'', '''19''', 101–104 ||  '''[[Algebraic invariant]]s'''.  Short communication describing the following paper.\n|-\n| <span id=\"noether_4\"></span>4 || 1911 || [http://gdz.sub.uni-goettingen.de/no_cache/dms/load/img/?IDDOC=261149 Zur Invariantentheorie der Formen von ''n'' Variabeln]\n{|\n| On the Theory of Invariants for Forms of ''n'' Variables<sup>§</sup>\n|}\n| ''Journal für die reine und angewandte Mathematik'', '''139''', 118–154  ||  '''[[Algebraic invariant]]s'''.  Extension of the formal algebraic-invariant methods to forms of an arbitrary number ''n'' of variables.  Noether applied these results in her publications [[#noether_8|#8]] and [[#noether_16|#16]].\n|-\n| <span id=\"noether_5\"></span>5 || 1913 || [http://gdz.sub.uni-goettingen.de/no_cache/dms/load/img/?IDDOC=244058 Rationale Funktionenkörper]\n{|\n| Rational Function Fields<sup>§</sup>\n|}\n| ''Jahresbericht der Deutschen Mathematiker-Vereinigung'', '''22''', 316–319 ||  '''[[Field theory (mathematics)|Field theory]]'''.  See the following paper.\n|-\n| <span id=\"noether_6\"></span>6 || 1915 || [http://www.digizeitschriften.de/index.php?id=loader&tx_jkDigiTools_pi1%5BIDDOC%5D=362546 Körper und Systeme rationaler Funktionen]\n{|\n| Fields and Systems of Rational Functions\n|}\n| ''Mathematische Annalen'', '''76''', 161–191 ||  '''[[Field theory (mathematics)|Field theory]]'''.  In this and the preceding paper, Noether investigates [[field (mathematics)|fields]] and systems of [[rational function]]s of ''n'' variables, and demonstrates that they have a rational [[Basis (linear algebra)|basis]].  In this work, she combined then-recent work of [[Ernst Steinitz]] on fields, with the methods for proving finiteness developed by [[David Hilbert]].  The methods she developed in this paper appeared again in her publication [[#noether_11|#11]] on the  [[inverse Galois problem]].\n|-\n|-\n| <span id=\"noether_7\"></span>7 || 1915 || [http://www.digizeitschriften.de/index.php?id=loader&tx_jkDigiTools_pi1%5BIDDOC%5D=461158 Der Endlichkeitssatz der Invarianten endlicher Gruppen]\n{|\n| The Finiteness Theorem for Invariants of Finite Groups\n|}\n| ''Mathematische Annalen'', '''77''', 89–92 ||  '''[[Group theory]]'''.  Proof that the invariants of a finite group are themselves finite, following the methods of [[David Hilbert]].\n|-\n| <span id=\"noether_8\"></span>8 || 1915 || [http://www.digizeitschriften.de/index.php?id=loader&tx_jkDigiTools_pi1%5BIDDOC%5D=461159 Über ganze rationale Darstellung der Invarianten eines Systems von beliebig vielen Grundformen]\n{|\n| On an Integral Rational Representation of the Invariants of a System of Arbitrarily Many Basis Forms<sup>§</sup>\n|}\n| ''Mathematische Annalen'', '''77''', 93–102 ||  Applies her earlier work on ''n''-forms.<ref>vdW, p. 102</ref>\n|-\n| <span id=\"noether_9\"></span>9 || 1916 || [http://www.digizeitschriften.de/index.php?id=loader&tx_jkDigiTools_pi1%5BIDDOC%5D=461160 Die allgemeinsten Bereiche aus ganzen transzendenten Zahlen]\n{|\n| The Most General Domains of Completely Transcendental Numbers\n|}\n| ''Mathematische Annalen'', '''77''', 103–128 (corrig., '''81''', 30) ||  \n|-\n| <span id=\"noether_10\"></span>10 || 1916 || [http://www.digizeitschriften.de/index.php?id=loader&tx_jkDigiTools_pi1%5BIDDOC%5D=461203 Die Funktionalgleichungen der isomorphen Abbildung]\n{|\n| Functional Equations of the Isomorphic Mapping\n|}\n| ''Mathematische Annalen'', '''77''', 536–545 ||  \n|-\n| <span id=\"noether_11\"></span>11 || 1918 || [http://www.digizeitschriften.de/index.php?id=loader&tx_jkDigiTools_pi1%5BIDDOC%5D=362593 Gleichungen mit vorgeschriebener Gruppe]\n{|\n| Equations with Prescribed Group\n|}\n| ''Mathematische Annalen'', '''78''', 221–229 (corrig., '''81''', 30) ||  '''[[Galois theory]]'''.  Important paper on the [[inverse Galois problem]]&nbsp;— as assessed by [[Bartel Leendert van der Waerden|B. L. van der Waerden]] in 1935, her work was  \"the most significant contribution made by anyone so far\" to this still-unsolved problem.\n|-\n| <span id=\"noether_12\"></span>12 || 1918 || [http://gdz.sub.uni-goettingen.de/no_cache/dms/load/img/?IDDOC=63702 Invarianten beliebiger Differentialausdrücke]\n{|\n| Invariants of Arbitrary Differential Expressions<sup>§</sup>\n|}\n| ''Nachrichten der Königlichen Gesellschaft der Wissenschaften zu Göttingen, Math.-phys. Klasse'', '''1918''', 38–44 ||  '''Differential invariants'''.  Introduces the concept of a reduced system, in which some differential invariants are reduced to algebraic invariants.\n|-\n| <span id=\"noether_13\"></span>13 || 1918 || [https://web.archive.org/web/20080705175409/http://www.physics.ucla.edu/~cwp/articles/noether.trans/german/emmy235.html Invariante Variationsprobleme]\n{|\n| [http://www.physics.ucla.edu/~cwp/articles/noether.trans/english/mort186.html Invariant Variation Problems]\n|}\n| ''Nachrichten der Königlichen Gesellschaft der Wissenschaften zu Göttingen, Math.-phys. Klasse'', '''1918''', 235–257 ||  '''Differential invariants'''.  Seminal paper introducing [[Noether's theorem]]s, which allow differential invariants to be developed from symmetries in the [[calculus of variations]].\n|-\n| <span id=\"noether_14\"></span>14 || 1919 || [http://gdz.sub.uni-goettingen.de/no_cache/dms/load/img/?IDDOC=249069 Die arithmetische Theorie der algebraischen Funktionen einer Veränderlichen in ihrer Beziehung zu den übrigen Theorien und zu der Zahlkörpertheorie]\n{|\n| The Arithmetic Theory of Algebraic Functions of One Variable in its Relationship to the Other Theories and to Number Field Theory<sup>§</sup>\n|}\n| ''Jahresbericht der Deutschen Mathematiker-Vereinigung'', '''28 (Abt. 1)''', 182–203 ||  \n|-\n| <span id=\"noether_15\"></span>15 || 1919 || [http://gdz.sub.uni-goettingen.de/no_cache/dms/load/img/?IDDOC=63745 Die Endlichkeit des Systems der ganzzahligen Invarianten binärer Formen]\n{|\n| A Proof of Finiteness for Integral Binary Invariants\n|}\n| ''Nachrichte der Königlichen Gesellschaft der Wissenschaften zu Göttingen, Math.-phys. Klasse'', '''1919''', 138–156 ||  '''[[Algebraic invariant]]s'''.  Proof that the integral invariants of binary forms are themselves finite.  Similar to publication [[#noether_7|#7]], this paper is devoted to the research area of [[David Hilbert|Hilbert]].\n|-\n| <span id=\"noether_16\"></span>16 || 1920 || [http://gdz.sub.uni-goettingen.de/no_cache/dms/load/img/?IDDOC=36824 Zur Reihenentwicklung in der Formentheorie]\n{|\n| On Series Expansions in the Theory of Forms<sup>§</sup>\n|}\n| ''Mathematische Annalen'', '''81''', 25–30 ||  Another application of her work in publication [[#noether_4|#4]] on the algebraic invariants of forms with ''n'' variables.\n|}\n\n==Second epoch (1920–1926)==\n\nIn the second epoch, Noether turned her attention to the theory of rings.  With her paper ''Moduln in nichtkommutativen Bereichen, insbesondere aus Differential- und Differenzenausdrücken'', [[Hermann Weyl]] states, \"It is here for the first time that the Emmy Noether appears whom we all know, and who changed the face of algebra by her work.\"\n\n{| class=\"wikitable sortable\" id=\"Noether_publications_second_epoch\"\n! Index<ref name=\"index_numbers\" /> !! Year !! Title and English translation<ref name=\"translation\" /> !! Journal, volume, pages !! Classification and notes\n|-\n| <span id=\"noether_17\"></span>17 || 1920 || [http://gdz.sub.uni-goettingen.de/no_cache/dms/load/img/?IDDOC=16862 Moduln in nichtkommutativen Bereichen, insbesondere aus Differential- und Differenzenausdrücken]\n{|\n| Modules in Non-commutative Domains, especially Those Composed of Differential and Difference Expressions<sup>§</sup>\n|}\n| ''Mathematische Zeitschrift'', '''8''', 1–35 ||  '''[[Ideal (ring theory)|Ideals]] and [[Module (mathematics)|modules]]'''.  Written with W. Schmeidler.  Seminal paper that introduces the concepts of left and right ideals, and develops various ideas of modules: direct sums and intersections, residue class modules and isomorphy of modules.  First use of the exchange method for proving uniqueness, and first representation of modules as intersections obeying an [[ascending chain condition]].\n|-\n| <span id=\"noether_18\"></span>18 || 1921 || [http://gdz.sub.uni-goettingen.de/no_cache/dms/load/img/?IDDOC=249024 Über eine Arbeit des im Kriege gefallenen K. Hentzelt zur Eliminationstheorie]<ref name=\"Scroll forward to page 101\">Scroll forward to page 101.</ref>\n{|\n| On a Work on Elimination Theory by K. Hentzelt, who Fell in the War<sup>§</sup>\n|}\n| ''Jahresbericht der Deutschen Mathematiker-Vereinigung'', '''30 (Abt. 2)''', 101 ||  '''[[Elimination theory]]'''.  Preliminary report of the dissertation of Kurt Hentzelt, who died during [[World War I]].  The full description of Hentzelt's work came in publication [[#noether_22|#22]].\n|-\n| <span id=\"noether_19\"></span>19 || 1921 ||  [http://www.digizeitschriften.de/index.php?id=loader&tx_jkDigiTools_pi1%5BIDDOC%5D=362701 Idealtheorie in Ringbereichen]\n{|\n| The Theory of Ideals in Ring Domains<sup>§</sup>\n|}\n| ''Mathematische Annalen'', '''83''', 24–66 || '''[[Ideal (ring theory)|Ideals]]'''.  Considered by many mathematicians to be Noether's most important paper.  In it, Noether shows the equivalence of the [[ascending chain condition]] with previous concepts such as Hilbert's theorem of a finite ideal basis.  She also shows that any ideal that satisfies this condition can be represented as an intersection of primary ideals, which are a generalization of the ''einartiges Ideal'' defined by [[Richard Dedekind]].  Noether also defines irreducible ideals and proves four uniqueness theorems by the exchange method, as in publication [[#noether_17|#17]].\n|-\n| <span id=\"noether_20\"></span>20 || 1922 || [http://www.digizeitschriften.de/index.php?id=loader&tx_jkDigiTools_pi1%5BIDDOC%5D=362772 Ein algebraisches Kriterium für absolute Irreduzibilität]\n{|\n| An Algebraic Criterion for Absolute Irreducibility<sup>§</sup>\n|}\n| ''Mathematische Annalen'', '''85''', 26–33 ||  \n|-\n| <span id=\"noether_21\"></span>21 || 1922 || Formale Variationsrechnung und Differentialinvarianten\n{|\n| Formal Calculus of Variations and Differential Invariants<sup>§</sup>\n|}\n| ''Encyklopädie der math. Wiss.'', '''III, 3, E''', 68–71 (in: R. Weitzenböck, ''Differentialinvarianten'') ||  \n|-\n| <span id=\"noether_22\"></span>22 || 1923 || [http://www.digizeitschriften.de/index.php?id=loader&tx_jkDigiTools_pi1%5BIDDOC%5D=362882 Zur Theorie der Polynomideale und Resultanten]\n{|\n| On the Theory of Polynomial Ideals and Resultants<sup>§</sup>\n|}\n| ''Mathematische Annalen'', '''88''', 53–79 ||  '''[[Elimination theory]]'''.  Based on the dissertation of Kurt Hentzelt, who died before this paper was presented.  In this work, and in publications [[#noether_24|#24]] and [[#noether_25|#25]], Noether subsumes [[elimination theory]] within her general theory of ideals.\n|-\n| <span id=\"noether_23\"></span>23 || 1923 || [http://gdz.sub.uni-goettingen.de/no_cache/dms/load/img/?IDDOC=248924 Algebraische und Differentialinvarianten]\n{|\n| Algebraic and Differential Invariants<sup>§</sup>\n|}\n| ''Jahresbericht der Deutschen Mathematiker-Vereinigung'', '''32''', 177–184 ||  \n|-\n| <span id=\"noether_24\"></span>24 || 1923 || [http://www.digizeitschriften.de/index.php?id=loader&tx_jkDigiTools_pi1%5BIDDOC%5D=362964 Eliminationstheorie und allgemeine Idealtheorie]\n{|\n| Elimination Theory and the General Ideal Theory<sup>§</sup>\n|}\n| ''Mathematische Annalen'', '''90''', 229–261 ||  '''[[Elimination theory]]'''.  Based on the dissertation of Kurt Hentzelt, who died before this paper was presented.  In this work, and in publications [[#noether_24|#24]] and [[#noether_25|#25]], Noether subsumes [[elimination theory]] within her general theory of ideals.\n|-\n| <span id=\"noether_25\"></span>25 || 1924 || [http://gdz.sub.uni-goettingen.de/no_cache/dms/load/img/?IDDOC=248880 Eliminationstheorie und Idealtheorie]\n{|\n| Elimination Theory and Ideal Theory<sup>§</sup>\n|}\n| ''Jahresbericht der Deutschen Mathematiker-Vereinigung'', '''33''', 116–120 ||  '''[[Elimination theory]]'''.  Based on the dissertation of Kurt Hentzelt, who died before this paper was presented.  In this work, and in publications [[#noether_24|#24]] and [[#noether_25|#25]], Noether subsumes [[elimination theory]] within her general theory of ideals. She developed a final proof during a lecture in 1923/1924.  When her colleague [[Bartel Leendert van der Waerden|van der Waerden]] developed the same proof independently (but working from her publications), Noether allowed him to publish.\n|-\n| <span id=\"noether_26\"></span>26 || 1924 || [http://gdz.sub.uni-goettingen.de/no_cache/dms/load/img/?IDDOC=248903 Abstrakter Aufbau der Idealtheorie im algebraischen Zahlkörper]<ref>Scroll forward to page 102.</ref>\n{|\n| Abstract Structure of the Theory of Ideals in Algebraic Number Fields<sup>§</sup>\n|}\n| ''Jahresbericht der Deutschen Mathematiker-Vereinigung'', '''33''', 102 ||  \n|-\n| <span id=\"noether_27\"></span>27 || 1925 || [http://gdz.sub.uni-goettingen.de/no_cache/dms/load/img/?IDDOC=248861 Hilbertsche Anzahlen in der Idealtheorie]<ref name=\"Scroll forward to page 101\"/>\n{|\n| Hilbert Counts in the Theory of Ideals<sup>§</sup>\n|}\n| ''Jahresbericht der Deutschen Mathematiker-Vereinigung'', '''34 (Abt. 2)''', 101 ||  \n|-\n| <span id=\"noether_28\"></span>28 || 1926 || [http://gdz.sub.uni-goettingen.de/no_cache/dms/load/img/?IDDOC=248861 Ableitung der Elementarteilertheorie aus der Gruppentheorie]<ref>Scroll forward to page 104.</ref>\n{|\n| Derivation of the Theory of Elementary Divisors from Group Theory<sup>§</sup>\n|}\n| ''Jahresbericht der Deutschen Mathematiker-Vereinigung'', '''34 (Abt. 2)''', 104 ||  \n|-\n| <span id=\"noether_29\"></span>29 || 1925 || [http://gdz.sub.uni-goettingen.de/no_cache/dms/load/img/?IDDOC=248861 Gruppencharaktere und Idealtheorie]<ref>Scroll forward to page 144.</ref>\n{|\n| Group Characters and the Theory of Ideals<sup>§</sup>\n|}\n| ''Jahresbericht der Deutschen Mathematiker-Vereinigung'', '''34 (Abt. 2)''', 144 ||  '''[[Group representation]]s''', '''[[module (mathematics)|modules]]''' and '''[[ideal (ring theory)|ideals]]'''.  First of four papers showing the close connection between these three subjects.  See also publications [[#noether_32|#32]], [[#noether_33|#33]], and [[#noether_35|#35]].\n|-\n| <span id=\"noether_3\"></span>30 || 1926 || [http://gdz.sub.uni-goettingen.de/no_cache/dms/load/img/?IDDOC=63971 Der Endlichkeitssatz der Invarianten endlicher linearer Gruppen der Charakteristik ''p'']\n{|\n| Proof of the Finiteness of the Invariants of Finite Linear Groups of Characteristic ''p''<sup>§</sup>\n|}\n| ''Nachrichten der Königlichen Gesellschaft der Wissenschaften zu Göttingen, Math.-phys. Klasse'', '''1926''', 28–35 ||  By applying ascending and descending chain conditions to finite extensions of a ring, Noether shows that the algebraic invariants of a finite group are finitely generated even in positive characteristic.\n|-\n| <span id=\"noether_31\"></span>31 || 1926 || [http://www.digizeitschriften.de/index.php?id=loader&tx_jkDigiTools_pi1%5BIDDOC%5D=363121 Abstrakter Aufbau der Idealtheorie in algebraischen Zahl- und Funktionenkörpern]\n{|\n| Abstract Structure of the Theory of Ideals in Algebraic Number Fields and Function Fields<sup>§</sup>\n|}\n| ''Mathematische Annalen'', '''96''', 26–61 || '''[[Ideal (ring theory)|Ideals]].''' Seminal paper in which Noether determined the minimal set of conditions required that a primary ideal be representable as a power of [[prime ideal]]s, as [[Richard Dedekind]] had done for [[algebraic number]]s. Three conditions were required: an ascending chain condition, a dimension condition, and the condition that the ring be [[integrally closed domain|integrally closed]].\n|}\n\n==Third epoch (1927–1935)==\n\nIn the third epoch, Emmy Noether focused on non-commutative algebras, and unified much earlier work on the representation theory of groups.\n\n{| class=\"wikitable sortable\" id=\"Noether_publications_third_epoch\"\n! Index<ref name=\"index_numbers\" /> !! Year !! Title and English translation<ref name=\"translation\" /> !! Journal, volume, pages !! Classification and notes\n|-\n| <span id=\"noether_32\"></span>32 || 1927 || [http://gdz.sub.uni-goettingen.de/no_cache/dms/load/img/?IDDOC=252018 Der Diskriminantensatz für die Ordnungen eines algebraischen Zahl- oder Funktionenkörpers]\n{|\n| The Discriminant theorem for the Orders of an Algebraic Number Field or Function Field<sup>§</sup>\n|}\n| ''Journal für die reine und angewandte Mathematik'', '''157''', 82–104  ||  '''[[Group representation]]s''', '''[[module (mathematics)|modules]]''' and '''[[ideal (ring theory)|ideals]]'''.  Second of four papers showing the close connection between these three subjects.  See also publications [[#noether_29|#29]], [[#noether_33|#33]], and [[#noether_35|#35]].\n|-\n| <span id=\"noether_33\"></span>33 || 1927 || Über minimale Zerfällungskörper irreduzibler Darstellungen\n{|\n| On the Minimum Splitting Fields of Irreducible Representations<sup>§</sup>\n|}\n| ''Sitzungsberichte der Preussischen Akademie der Wissenschaften'', '''1927''', 221–228 || '''[[Group representation]]s''', '''[[module (mathematics)|modules]]''' and '''[[ideal (ring theory)|ideals]]'''.  Written with R. Brauer.  Third of four papers showing the close connection between these three subjects.  See also publications [[#noether_29|#29]], [[#noether_32|#32]], and [[#noether_35|#35]].   This paper shows that the splitting fields of a division algebra are embedded in the algebra itself; the [[splitting field]]s are maximal commutative subfields either over the algebra, or over a full matrix ring over the algebra.\n|-\n| <span id=\"noether_34\"></span>34 || 1928 || Hyperkomplexe Größen und Darstellungstheorie, in arithmetischer Auffassung\n{|\n| Hypercomplex Quantities and the Theory of Representations, from an Arithmetic Perspective<sup>§</sup>\n|}\n| ''Atti Congresso Bologna'', '''2''', 71–73 ||  '''[[Group representation]]s''', '''[[module (mathematics)|modules]]''' and '''[[ideal (ring theory)|ideals]]'''.  Synopsis of her papers showing the close connection between these three subjects.  See also publications [[#noether_29|#29]], [[#noether_32|#32]], [[#noether_32|#33]], and [[#noether_35|#35]].\n|-\n| <span id=\"noether_35\"></span>35 || 1929 || [http://gdz.sub.uni-goettingen.de/no_cache/dms/load/img/?IDDOC=286988 Hyperkomplexe Größen und Darstellungstheorie]\n{|\n| Hypercomplex Quantities and the Theory of Representations\n|}\n| ''Mathematische Zeitschrift'', '''30''', 641–692 ||  '''[[Group representation]]s''', '''[[module (mathematics)|modules]]''' and '''[[ideal (ring theory)|ideals]]'''.  Final paper of four showing the close connection between these three subjects.  See also publications [[#noether_29|#29]], [[#noether_32|#32]], and [[#noether_33|#33]].\n|-\n| <span id=\"noether_36\"></span>36 || 1929 || Über Maximalbereiche von ganzzahligen Funktionen\n{|\n| On the Maximal Domains of Integral Functions<sup>§</sup>\n|}\n| ''Rec. Soc. Math. Moscou'', '''36''', 65–72 ||  \n|-\n| <span id=\"noether_37\"></span>37 || 1929 || Idealdifferentiation und Differente |\n{|\n| Differents and Ideal Differentiation<sup>§</sup>\n|}\n| ''Jahresbericht der Deutschen Mathematiker-Vereinigung'', '''39 (Abt. 2)''', 17 ||    \n|-\n| <span id=\"noether_38\"></span>38 || 1932 || [http://gdz.sub.uni-goettingen.de/no_cache/dms/load/img/?IDDOC=260827 Normalbasis bei Körpern ohne höhere Verzweigung]\n{|\n| Normal Basis in Fields without Higher Ramification<sup>§</sup>\n|}\n| ''Journal für die reine und angewandte Mathematik'', '''167''', 147–152 ||  \n|-\n| <span id=\"noether_39\"></span>39 || 1932 || [http://gdz.sub.uni-goettingen.de/no_cache/dms/load/img/?IDDOC=260847 Beweis eines Hauptsatzes in der Theorie der Algebren]\n{|\n| Proof of a Main Theorem in the Theory of Algebras<sup>§</sup>\n|}\n| ''Journal für die reine und angewandte Mathematik'', '''167''', 399–404 ||  Written with R. Brauer and H. Hasse.\n|-\n| <span id=\"noether_4\"></span>40 || 1932 || Hyperkomplexe Systeme in ihren Beziehungen zur kommutativen Algebra und zur Zahlentheorie\n{|\n| Hypercomplex Systems in Their Relationship to Commutative Algebra and to Number Theory<sup>§</sup>\n|}\n| ''Verhandl. Internat. Math. Kongress Zürich '''1''', 189–194 ||  \n|-\n| <span id=\"noether_41\"></span>41 || 1933 || [http://gdz.sub.uni-goettingen.de/no_cache/dms/load/img/?IDDOC=85341 Nichtkommutative Algebren]\n{|\n| Non-commutative Algebras<sup>§</sup>\n|}\n| ''Mathematische Zeitschrift'', '''37''', 514–541 ||  \n|-\n| <span id=\"noether_42\"></span>42 || 1933 || [http://www.digizeitschriften.de/index.php?id=loader&tx_jkDigiTools_pi1%5BIDDOC%5D=363603 Der Hauptgeschlechtsatz für relativ-galoissche Zahlkörper]\n{|\n| The Principal Genus Theorem for Relatively Galois Fields of Numbers<sup>§</sup>\n|}\n| ''Mathematische Annalen'', '''108''', 411–419 ||  \n|-\n| <span id=\"noether_43\"></span>43 || 1934 || Zerfallende verschränkte Produkte und ihre Maximalordnungen, Exposés mathématiques publiés à la mémoire de J. Herbrand IV\n{|\n| Decomposing Crossed Products and Their Maximal Orders, in memory of J. Herbrand IV<sup>§</sup>\n|}\n| Actualités scient. et industr., '''148''' ||  \n|-\n| <span id=\"noether_44\"></span>44 || 1950 || [http://gdz.sub.uni-goettingen.de/no_cache/dms/load/img/?IDDOC=260694 Idealdifferentiation und Differente]\n{|\n| Differents and Ideal Differentiation<sup>§</sup>\n|}\n| ''Journal für die reine und angewandte Mathematik'', '''188''', 1–21 ||  \n|}\n\n==References==\n\n{{reflist|1}}\n\n==Bibliography==\n\n* {{cite book | veditors = Brewer JW, Smith MK | year = 1981 | title = Emmy Noether: A Tribute to Her Life and Work | publisher = Marcel Dekker | location = New York | isbn = 0-8247-1550-0}}\n* {{cite book | author = Dick A | year = 1970 | title = Emmy Noether 1882–1935 | edition = (Beihft Nr. 13 zur Zeitschrift ''Elemente der Mathematik'') | publisher = Birkhäuser Verlag | location = Basel | pages = 40–42}}\n* {{citation |last=Kimberling |first=Clark |chapter=Emmy Noether and Her Influence |pages=3–61 |title=Emmy Noether: A Tribute to Her Life and Work | editor=James W. Brewer | editor2=Martha K. Smith |place=New York |publisher= Marcel Dekker, Inc. |year=1981 |isbn=0-8247-1550-0}}.\n*{{Citation | last1=Noether | first1=Emmy | author1-link=Emmy Noether | editor1-last=Jacobson | editor1-first=Nathan | editor1-link=Nathan Jacobson | title=Gesammelte Abhandlungen (Collected Works) | publisher=[[Springer-Verlag]] | location=Berlin, New York | isbn=978-3-540-11504-5 | mr=703862 | year=1983 | pages = 773–775 | url = https://books.google.com/books?id=bsm5lmprAQQC }}\n\n==External links==\n* [http://physikerinnen.de/noetherpublikationen.html List of Emmy Noether's publications by Dr. Cordula Tollmien]\n* [http://www.rzuser.uni-heidelberg.de/~ci3/hasse-noether/noether-vdw.pdf List of Emmy Noether's publications in the eulogy] by [[Bartel Leendert van der Waerden]]\n* [http://cwp.library.ucla.edu/Phase2/Noether,_Amalie_Emmy@861234567.html Partial listing of important works] at the [http://cwp.library.ucla.edu/exp.html Contributions of 20th century Women to Physics] at [[University of California, Los Angeles|UCLA]]\n* [http://www-history.mcs.st-andrews.ac.uk/history/Biographies/Noether_Emmy.html MacTutor biography of Emmy Noether]\n\n{{DEFAULTSORT:Noether, Emmy}}\n[[Category:Abstract algebra]]\n[[Category:Bibliographies by writer]]\n[[Category:Bibliographies of German writers]]\n[[Category:Science bibliographies]]"
    },
    {
      "title": "Normal basis",
      "url": "https://en.wikipedia.org/wiki/Normal_basis",
      "text": "In [[mathematics]], specifically the [[Abstract algebra|algebraic]] theory of [[field theory (mathematics)|fields]], a '''normal basis''' is a special kind of [[basis (linear algebra)|basis]] for [[Galois extension]]s of finite degree, characterised as forming a single [[orbit (group theory)|orbit]] for the [[Galois group]].  The '''normal basis theorem''' states that any finite Galois extension of fields has a normal basis. In [[algebraic number theory]], the study of the more refined question of the existence of a [[normal integral basis]] is part of [[Galois module]] theory.\n\n==Normal basis theorem==\nLet <math>F\\subset K</math> be a Galois extension with Galois group <math>G</math>. The classical '''normal basis theorem''' states that there is an element <math>\\beta\\in K</math> such that <math>\\{g(\\beta) \\ \\textrm{ for }\\ g\\in G\\}</math> forms a basis of ''K'', considered as a vector space over ''F''. That is, any element <math>\\alpha \\in K</math> can be written uniquely as <math>\\textstyle\\alpha = \\sum_{g\\in G} a_g\\, g(\\beta)</math> for coefficients <math>a_g\\in F.</math>\n\nA normal basis contrasts with a [[Primitive element theorem|primitive element]] basis of the form <math>\\{1,\\beta,\\beta^2,\\ldots,\\beta^{n-1}\\}</math>, where <math>\\beta\\in K</math> is an element whose minimal polynomial has degree <math>n=[K:F]</math>.\n\n== Case of finite fields ==\nFor [[Finite field|finite fields]] this can be stated as follows:<ref>{{citation|author1=Nader H. Bshouty|title=Generalizations of the normal basis theorem of finite fields|url=https://hotcrp-vee2014.cs.technion.ac.il/users/wwwb/cgi-bin/tr-get.cgi/1989/CS/CS0578.pdf|page=1|year=1989|author2=Gadiel Seroussi}}; ''SIAM J. Discrete Math.'' 3 (1990), no. 3, 330–337.</ref> Let <math>F = GF(q)=\\mathbb{F}_q</math> denote the field of ''q'' elements, where ''q = p<sup>m</sup>'' is a prime power, and let <math>K=GF(q^n)=\\mathbb{F}_{q^n}</math> denote its extension field of degree ''n'' ≥ 1.  Here the Galois group is <math>G = \\text{Gal}(K/F) = \\{1,\\Phi,\\Phi^2,\\ldots,\\Phi^{n-1}\\}</math> with <math>\\Phi^n = 1,</math> a [[Cyclic group|cyclic group]] generated by the relative [[Frobenius endomorphism|Frobenius automorphism]] <math>\\Phi(\\alpha)=\\alpha^q,</math>with <math>\\Phi^n = 1 =\\textrm{Id}_K.</math> Then there exists an element {{nowrap|''β'' ∈ ''K''}} such that\n<blockquote><math>\\{\\beta, \\Phi(\\beta), \\Phi^2(\\beta),\\ldots,\\Phi^{n-1}(\\beta)\\}\n\\ = \\ \n\\{\\beta, \\beta^q, \\beta^{q^2}, \\ldots,\\beta^{q^{n-1}}\\!\\}</math> </blockquote>\nis a basis of ''K'' over ''F''.\n\n=== Proof for finite fields ===\nIn case the Galois group is cyclic as above, generated by <math>\\Phi</math> with <math>\\Phi^n=1,</math> the Normal Basis Theorem follows from two basic facts. The first is the linear independence of characters: a [[Multiplicative character|''multiplicative'' ''character'']] is a mapping ''χ'' from a group ''H'' to a field ''K'' satisfying <math>\\chi(h_1h_2)=\\chi(h_1)\\chi(h_2)</math>; then any distinct characters <math>\\chi_1,\\chi_2,\\ldots </math> are linearly independent in the ''K''-vector space of mappings. We apply this to the Galois group automorphisms <math>\\chi_i=\\Phi^i: K \\to K,</math> thought of as mappings from the multiplicative group <math>H=K^\\times</math>. Now  <math>K\\cong F^n</math>as an ''F''-vector space, so we may consider <math>\\Phi : F^n\\to F^n</math> as an element of the matrix algebra <math>M_n(F);</math> since its powers <math>1,\\Phi,\\ldots,\\Phi^{n-1}</math> are linearly independent (over ''K'' and a fortiori over ''F''), its [[Minimal polynomial|minimal polynomial]]{{dn|date=May 2019}} must have degree at least ''n'', i.e. it must be <math>X^n-1</math>. We conclude that the [[group algebra]] of ''G'' is <math>F[G]\\cong F[X]/(X^n{-}\\,1),</math> a quotient of the polynomial ring ''F''[''X''], and the ''F''-vector space ''K'' is a [[Module (mathematics)|module]] (or [[Representation theory of finite groups|representation]]) for this algebra. \n\nThe second basic fact is the classification of [[Modules over a pid|modules over a PID]] such as ''F''[''G'']. These are just direct sums of cyclic modules of the form <math>F[X]/(f(x)),</math> where ''f''(''x'') must be divisible by ''X<sup>n</sup> −'' 1. (Here ''G'' acts by <math>\\Phi\\cdot X^i = X^{i+1}.</math>)  But since <math>\\dim_F F[X]/(X^n{-}\\,1) = \\dim_F(K) = n,</math> we can only have ''f''(''x'') =  ''X<sup>n</sup> −'' 1, and<blockquote><math>K \\ \\cong\\ F[X]/(X^n{-}\\,1)</math></blockquote>as ''F''[''G'']-modules, namely the [[Regular representation|regular representation]] of ''G''. (Note this is ''not'' an isomorphism of rings or ''F''-algebras!)  Now the basis <math>\\{1,X,X^2,\\ldots,X^{n-1}\\}</math> on the right side of this isomorphism corresponds to a normal basis <math>\\{\\beta, \\Phi(\\beta),\\Phi^2(\\beta),\\ldots,\\Phi^{m-1}(\\beta)\\}</math> of ''K'' on the left.\n\nNote that this proof would also apply in the case of a cyclic [[Kummer theory|Kummer extension]].\n\n=== Example ===\nConsider the field <math>K=GF(2^3)=\\mathbb{F}_{8}</math> over <math>F=GF(2)=\\mathbb{F}_{2}</math>, with Frobenius automorphism <math>\\Phi(\\alpha)=\\alpha^2</math>. The proof above clarifies the choice of normal bases in terms of the structure of ''K'' as a representation of ''G'' (or ''F''[''G'']-module). The irreducible factorization \n<blockquote><math>X^n-1 \\ =\\ X^3-1\\ = \\ (X{+}1)(X^2{+}X{+}1) \\ \\in\\ F[X]</math></blockquote> means we have a direct sum of ''F''[''G'']-modules (by the [[Chinese remainder theorem]]):<blockquote><math>K\\ \\cong\\ \\frac{F[X]}{(X^3{-}\\,1)} \\ \\cong\\ \\frac{F[X]}{(X{+}1)} \\oplus \\frac{F[X]}{(X^2{+}X{+}1)}.</math></blockquote>\nThe first component is just <math>F\\subset K</math>, while the second is isomorphic as an ''F''[''G'']-module to <math>\\mathbb{F}_{2^2} \\cong \\mathbb{F}_2[X]/(X^2{+}X{+}1)</math> under the action <math>\\Phi\\cdot X^i = X^{i+1}.</math> (Thus <math>K \\cong \\mathbb F_2\\oplus \\mathbb F_4</math> as ''F''[''G'']-modules, but ''not'' as ''F''-algebras.) \n\nThe elements <math>\\beta\\in K</math> which can be used for a normal basis are precisely those outside either of the submodules, so that <math>(\\Phi{+}1)(\\beta)\\neq 0</math> and <math>(\\Phi^2{+}\\Phi{+}1)(\\beta)\\neq 0</math>. In terms of the ''G''-orbits of ''K'', which correspond to the irreducible factors of:\n<blockquote><math>t^{2^3}-t \\ = \\ t(t{+}1)(t^3{+}t{+}1)(t^3{+}t^2{+}1)\\ \\in\\ F[t],</math></blockquote>\nthe elements of <math>F=\\mathbb{F}_2</math> are the roots of <math>t(t{+}1)</math>, the nonzero elements of the submodule <math>\\mathbb{F}_4</math> are the roots of <math>t^3{+}t{+}1</math>, while the normal basis, which in this case is unique, is given by the roots of the remaining factor <math>t^3{+}t^2{+}1</math>.\n\nBy contrast, for the extension field <math>L = GF(2^4)=\\mathbb{F}_{16}</math> in which ''n'' = 4 is divisible by ''p'' = 2, we have the ''F''[''G'']-module isomorphism \n<blockquote><math>L \\ \\cong\\ \\mathbb{F}_2[X]/(X^4{-}1)\n\\ =\\ \\mathbb{F}_2[X]/(X{+}1)^4.</math></blockquote>\nHere the operator <math>\\Phi\\cong X</math> is not [[Diagonalizable matrix|diagonalizable]], the module ''L'' has nested submodules given by [[Generalized eigenvector|generalized eigenspaces]] of <math>\\Phi</math>, and the normal basis elements ''β'' are those outside the largest proper generalized eigenspace, the elements with <math>(\\Phi{+}1)^3(\\beta)\\neq 0</math>.\n\n=== Application to cryptography ===\nThe normal basis is frequently used in [[cryptography|cryptographic]] applications based on the [[discrete logarithm problem]], such as [[elliptic curve cryptography]], since arithmetic using a normal basis is typically more computationally efficient than using other bases.\n\nFor example, in the field <math>K=GF(2^3)=\\mathbb{F}_{8}</math> above, we may represent elements as bit-strings:<blockquote><math>\\alpha \\ =\\ (a_2,a_1,a_0)\\ =\\ a_2\\Phi^2(\\beta) + a_1\\Phi(\\beta)+a_0\\beta\n\\ =\\ a_2\\beta^4 + a_1\\beta^2 +a_0\\beta,</math> </blockquote>where the coefficients are bits <math>a_i\\in GF(2)=\\{0,1\\}.</math> Now we can square elements by doing a left circular shift, <math>\\alpha^2=\\Phi(a_2,a_1,a_0) = (a_1,a_0,a_2)</math>, since squaring ''β''<sup>4</sup> gives ''β''<sup>8</sup> = ''β''.  This makes the normal basis especially attractive for cryptosystems that utilize frequent squaring.\n\n==Primitive normal basis==\nA '''primitive normal basis''' of an extension of finite fields ''E''/''F'' is a normal basis for ''E''/''F'' that is generated by a [[Primitive element (finite field)|primitive element]] of ''E'', that is a generator of the multiplicative group <math>K^\\times.</math> (Note that this is a more restrictive definition of primitive element than that mentioned above after the general Normal Basis Theorem: one requires powers of the element to produce every non-zero element of ''K'', not merely a basis.)  Lenstra and Schoof (1987) proved that every finite field extension possesses a primitive normal basis, the case when ''F'' is a [[prime field]] having been settled by [[Harold Davenport]].\n\n==Free elements==\nIf ''K''/''F'' is a Galois extension and ''x'' in ''E'' generates a normal basis over ''F'', then ''x'' is '''free''' in ''K''/''F''.  If ''x'' has the property that for every subgroup ''H'' of the Galois  group ''G'', with fixed field ''K''<sup>''H''</sup>, ''x'' is free for ''K''/''K''<sup>''H''</sup>, then ''x'' is said to be '''completely free''' in ''K''/''F''.  Every Galois extension has a completely free element.<ref>Dirk Hachenberger, ''Completely free elements'', in Cohen & Niederreiter (1996) pp.97-107 {{zbl|0864.11066}}</ref>\n\n==See also==\n*[[Dual basis in a field extension]]\n*[[Polynomial basis]]\n*[[Zech's logarithm]]\n\n==References==\n{{reflist}}\n* {{cite book | editor1-first=S. | editor1-last=Cohen | editor2-first=H. | editor2-last=Niederreiter|editor2-link = Harald Niederreiter | title=Finite Fields and Applications. Proceedings of the 3rd international conference, Glasgow, UK, July 11–14, 1995 | publisher=[[Cambridge University Press]] | isbn=978-0-521-56736-7 | year=1996 | series=London Mathematical Society Lecture Note Series | volume=233 | zbl=0851.00052 }}\n* {{cite journal | last1=Lenstra | first1=H.W., jr | author1-link=Hendrik Lenstra | last2=Schoof | first2=R.J. | author2-link=René Schoof | title=Primitive normal bases for finite fields | zbl=0615.12023 | journal=[[Mathematics of Computation]] | volume=48 | issue=177 | pages=217–231 | year=1987 | jstor=2007886 | doi=10.2307/2007886}}\n* {{cite book | editor1-last=Menezes | editor1-first=Alfred J. | editor1-link=Alfred Menezes | title=Applications of finite fields | zbl=0779.11059 | series=The Kluwer International Series in Engineering and Computer Science | volume=199 | location=Boston | publisher= Kluwer Academic Publishers | year=1993 | isbn=978-0792392828 }}\n\n[[Category:Linear algebra]]\n[[Category:Field theory]]\n[[Category:Abstract algebra]]\n[[Category:Cryptography]]"
    },
    {
      "title": "Operad",
      "url": "https://en.wikipedia.org/wiki/Operad",
      "text": "In [[mathematics]], an '''operad''' is concerned with prototypical [[algebra over a field|algebras]] that model properties such as [[commutativity]] or [[anticommutativity]] as well as various amounts of [[associativity]]. Operads generalize the various [[associativity]] properties already observed in [[Algebra over a field|algebras]] and [[coalgebra]]s such as [[Lie algebra]]s or [[Poisson algebra]]s by modeling computational trees within the algebra. Algebras are to operads as [[group representation]]s are to [[group (mathematics)|group]]s. An operad can be seen as a set of [[Operation (mathematics)|operations]], each one having a fixed finite number of inputs (arguments) and one output, which can be composed one with others. They form a [[Category theory|category-theoretic]] analog of [[universal algebra]].{{dubious|Seems ... wrong or at best inaccurate. The difference is viewpoint but I don’t think that’s category theory; for example, one can still talk about operads without category theory. —- Taku|date=January 2019}}\n\n== History ==\nOperads originate in [[algebraic topology]] from the study of iterated [[loop space]]s by [[Michael Boardman|J. Michael Boardman]] and [[Rainer M. Vogt]],<ref>{{Cite journal|last=Boardman|first=J. M.|author-link=Michael Boardman|last2=Vogt|first2=R. M.|date=1968-11-01|title=Homotopy-everything $H$-spaces|url=https://www.ams.org/journals/bull/1968-74-06/S0002-9904-1968-12070-1/home.html|journal=Bulletin of the American Mathematical Society|language=en-US|volume=74|issue=6|pages=1117–1123|doi=10.1090/S0002-9904-1968-12070-1|issn=0002-9904|via=|bibcode=1994BAMaS..30..205W}}</ref><ref>{{Cite book|last=Boardman|first=J. M.|author-link=Michael Boardman|last2=Vogt|first2=R. M.|date=1973|title=Homotopy Invariant Algebraic Structures on Topological Spaces|journal=Lecture Notes in Mathematics|language=en-gb|volume=347|pages=|doi=10.1007/bfb0068547|issn=0075-8434|isbn=978-3-540-06479-4}}</ref> and [[J. Peter May]].<ref>{{Cite book|last=May|first=J. P.|author-link=J. Peter May|date=1972|title=The Geometry of Iterated Loop Spaces|journal=Lecture Notes in Mathematics|language=en-gb|volume=271|pages=|doi=10.1007/bfb0067491|issn=0075-8434|isbn=978-3-540-05904-2|citeseerx=10.1.1.146.3172}}</ref> The word \"operad\" was created by May as a [[portmanteau]] of \"operations\" and \"[[monad (category theory)|monad]]\" (and also because his mother was an opera singer).<ref>{{Cite web|url=https://www.math.uchicago.edu/~may/PAPERS/mayi.pdf|title=Operads, Algebras, and Modules|last=May|first=J. Peter|authorlink=J. Peter May|date=|website=math.uchicago.edu|page=2|archive-url=|archive-date=|dead-url=|access-date={{date|2018-09-28}}}}</ref> Interest in operads was considerably renewed in the early [[1990s|90s]] when, based on early insights of [[Maxim Kontsevich]], [[Victor Ginzburg]] and [[Mikhail Kapranov]] discovered that some [[Duality (mathematics)|duality]] phenomena in [[rational homotopy theory]] could be explained using [[Koszul duality]] of operads.<ref>{{Cite journal|last=Ginzburg|first=Victor|author-link=Victor Ginzburg|last2=Kapranov|first2=Mikhail|date=1994|title=Koszul duality for operads|url=https://projecteuclid.org/euclid.dmj/1077286744|journal=Duke Mathematical Journal|language=en|volume=76|issue=1|pages=203–272|doi=10.1215/S0012-7094-94-07608-4|issn=0012-7094|mr=1301191|zbl=0855.18006|via=[[Project Euclid]]}}</ref><ref>{{Cite web|url=http://www.numdam.org/item/SB_1994-1995__37__47_0|title=La renaissance des opérades|last=Loday|first=Jean-Louis|authorlink=Jean-Louis Loday|year=1996|website=www.numdam.org|series=[[Séminaire Nicolas Bourbaki]]|language=en|mr=1423619|zbl=0866.18007|archive-url=|archive-date=|dead-url=|access-date=2018-09-27}}</ref> Operads have since found many applications, such as in [[deformation quantization]] of [[Poisson manifold]]s, the [[Deligne conjecture]],<ref name=Deligne>{{cite arxiv|last=Kontsevich|first=Maxim|last2=Soibelman|first2=Yan|date=2000-01-26|title=Deformations of algebras over operads and Deligne's conjecture|eprint=math/0001151}}</ref> or [[Graph (discrete mathematics)|graph]] [[Homology (mathematics)|homology]] in the work of [[Maxim Kontsevich]] and [[Thomas Willwacher]].\n\n==Definition==\n\n===Non-symmetric operad===\nA non-symmetric operad (sometimes called an ''operad without permutations'', or a ''non-<math>\\Sigma</math>'' or ''plain'' operad) consists of the following:\n* a sequence <math>(P(n))_{n\\in\\mathbb{N}}</math> of sets, whose elements are called ''<math>n</math>-ary operations'',\n* an element <math>1</math> in <math>P(1)</math> called the ''identity'',\n* for all positive integers <math>n</math>, <math display=\"inline\">k_1,\\ldots,k_n</math>, a ''composition'' function\n\n:<math>\n\\begin{align}\n\\circ: P(n)\\times P(k_1)\\times\\cdots\\times P(k_n) & \\to P(k_1+\\cdots+k_n)\\\\\n(\\theta,\\theta_1,\\ldots,\\theta_n) & \\mapsto \\theta\\circ(\\theta_1,\\ldots,\\theta_n),\n\\end{align}\n</math>\nsatisfying the following coherence axioms:\n* ''identity'': <math>\\theta\\circ(1,\\ldots,1)=\\theta=1\\circ\\theta</math>\n* ''associativity'':\n:: <math>\n\\begin{align}\n& \\theta \\circ (\\theta_1 \\circ (\\theta_{1,1}, \\ldots, \\theta_{1,k_1}), \\ldots, \\theta_n \\circ (\\theta_{n,1}, \\ldots,\\theta_{n,k_n})) \\\\\n= {} & (\\theta \\circ (\\theta_1, \\ldots, \\theta_n)) \\circ (\\theta_{1,1}, \\ldots, \\theta_{1,k_1}, \\ldots, \\theta_{n,1}, \\ldots, \\theta_{n,k_n})\n\\end{align}\n</math>\n\n(the number of arguments corresponds to the arities of the operations).\n\n===Symmetric operad===\nA symmetric operad (often just called ''operad'') is a non-symmetric operad <math>P</math> as above, together with a right action of the [[symmetric group]] <math>\\Sigma_n</math> on <math>P(n)</math>, satisfying the above associative and identity axioms, as well as\n\n*''equivariance'': given permutations <math>s_i \\in \\Sigma_{k_i}, t\\in \\Sigma_n</math>,\n:: <math>\n\\begin{align}\n& (\\theta*t)\\circ(\\theta_{t_1},\\ldots,\\theta_{t_n}) = (\\theta\\circ(\\theta_1,\\ldots,\\theta_n))*t; \\\\[2pt]\n& \\theta\\circ(\\theta_1*s_1,\\ldots,\\theta_n*s_n) = (\\theta\\circ(\\theta_1,\\ldots,\\theta_n))*(s_1,\\ldots,s_n)\n\\end{align}\n</math>\n(where by [[abuse of notation]], <math>t</math> on the right hand side of the first equivariance relation is the element\nof <math>\\Sigma_{k_1+\\dots+k_n}</math> that acts on the set <math>\\{1, 2, \\dots , k_1+\\dots +k_n\\}</math> by breaking it into <math>n</math> blocks, the first of size <math>k_1</math>, the second of size <math>k_2</math>, through the <math>n</math>th block of size <math>k_n</math>, and then permutes these <math>n</math> blocks by <math>t</math>).\n\nThe permutation actions in this definition are vital to most applications, including the original application to loop spaces.\n\n=== Morphisms ===\nA morphism of operads <math>f:P\\to Q</math> consists of a sequence\n:<math>(f_n:P(n)\\to Q(n))_{n\\in\\mathbb{N}}</math>\nwhich:\n* preserves the identity: <math>f(1)=1</math>\n* preserves composition: for every ''n''-ary operation <math>\\theta</math> and operations <math>\\theta_1 , \\ldots , \\theta_n</math>,\n:: <math>\nf(\\theta\\circ(\\theta_1,\\ldots,\\theta_n))\n= f(\\theta)\\circ(f(\\theta_1),\\ldots,f(\\theta_n))\n</math>\n* preserves the permutation actions: <math>f(x*s)=f(x)*s</math>.\n\nOperads therefore form a [[category (mathematics)|category]] denoted by <math>\\mathsf{Oper}</math>.\n\n=== In other categories ===\nSo far only operads have been considered in the [[Category theory|category]] of sets. It is actually possible to define operads in any [[symmetric monoidal category]] (or, for non-symmetric operads, any [[monoidal category]]).\n\nA common example would be given by the category of [[topological spaces]], with the monoidal product given by the [[cartesian product]]. In this case, a topological operad is given by a sequence of ''spaces'' (instead of sets) <math>\\{ P(n) \\}_{n \\ge 0}</math>. The structure maps of the operad (the composition and the actions of the symmetric groups) must then be assumed to be continuous. The result is called a ''topological operad''. Similarly, in the definition of a morphism, it would be necessary to assume that the maps involved are continuous.\n\nOther common settings to define operads include, for example, [[Module (mathematics)|module]] over a [[Ring (mathematics)|ring]], [[chain complex]]es, [[groupoid]]s (or even the category of categories itself), [[coalgebra]]s, etc.\n\n=== Algebraist definition ===\nBy definition, an [[associative algebra]] over a commutative ring ''R'' is a [[monoid object]] in the monoidal category <math>R-\\mathsf{Mod}</math> of modules over ''R''. This definition can be extended to give a definition of an operad: namely, an ''operad'' over ''R'' is a monoid object <math>(T, \\gamma, \\eta)</math> in the [[monoidal category of endofunctors]] on <math>R-\\mathsf{Mod}</math> (it is a [[monad (category theory)|monad]]) satisfying some finiteness condition.<ref group=note>”finiteness” refers to the fact that only a finite number of inputs are allowed in the definition of an operad. For example, the condition is satisfied if one can write\n:<math>T(V) = \\bigoplus_{n=1}^{\\infty} T_n \\otimes V^{\\otimes n}</math>,\n:<math>\\gamma(V): T_n \\otimes T_{i_1} \\otimes \\cdots \\otimes T_{i_n} \\to T_{i_1 + \\dots + i_n}</math>.</ref>\n\nFor example, a monoid object in the category of polynomial functors is an operad.<ref name=Deligne /> Similarly, a symmetric operad can be defined as a monoid object in the category of [[S-object|<math>\\mathbb{S}</math>-objects]].<ref>{{cite arxiv|last=Jones|first=J. D. S.|last2=Getzler|first2=Ezra|date=1994-03-08|title=Operads, homotopy algebra and iterated integrals for double loop spaces|eprint=hep-th/9403055|language=en}}</ref> A monoid object in the category of [[combinatorial species]] is an operad in finite sets.\n\nAn operad in the above sense is sometimes thought of as a '''generalized ring'''. For example, Nikolai Durov defines his generalized ring as a monoid object in the monoidal category of endofuctors that commutes with filtered colimit.<ref>N. Durov, New approach to Arakelov geometry, University of Bonn, PhD thesis, 2007; [http://www.arxiv.org/abs/0704.2030 arXiv:0704.2030].</ref> It is a generalization of a ring since each ordinary ring ''R'' defines a monad <math>\\Sigma_R: \\textbf{Set} \\to \\textbf{Set}</math> that sends a set ''X'' to the [[free module|free ''R''-module <math>R^{(X)}</math>]] generated by ''X''.\n\n== Understanding the axioms ==\n\n===Associativity axiom===\n\"Associativity\" means that ''composition'' of operations is associative\n(the function <math>\\circ</math> is associative), analogous to the axiom in category theory that <math>f \\circ (g \\circ h) = (f \\circ g) \\circ h</math>; it does ''not'' mean that the operations ''themselves'' are associative as operations.\nCompare with the [[#Associative_operad|associative operad]], below.\n\nAssociativity in operad theory means that [[Expression (mathematics)|expressions]] can be written involving operations without ambiguity from the omitted compositions, just as associativity for operations allows products to be written without ambiguity from the omitted parentheses.\n\nFor instance, if <math>\\theta</math> is a binary operation, which is written as <math>\\theta(a,b)</math> or <math>(ab)</math>. So that <math>\\theta</math> may or may not be associative.\n\nThen what is commonly written <math>((ab)c)</math> is unambiguously written operadically as <math>\\theta \\circ (\\theta,1)</math> . This sends <math>(a,b,c)</math> to <math>(ab,c)</math> (apply <math>\\theta</math> on the first two, and the identity on the third), and then the <math>\\theta</math> on the left \"multiplies\" <math>ab</math> by <math>c</math>.\nThis is clearer when depicted as a tree:\n\n[[File:OperadTreeCompose1.svg|Tree before composition]]\n\nwhich yields a 3-ary operation:\n\n[[File:OperadTreeCompose2.svg|Tree after composition]]\n{{Clear}}\n\nHowever, the expression <math>(((ab)c)d)</math> is ''a priori'' ambiguous:\nit could mean <math>\\theta \\circ ((\\theta,1) \\circ ((\\theta,1),1))</math>, if the inner compositions are performed first, or it could mean <math>(\\theta \\circ (\\theta,1)) \\circ ((\\theta,1),1)</math>,\nif the outer compositions are performed first (operations are read from right to left).\nWriting <math>x=\\theta, y=(\\theta,1), z=((\\theta,1),1)</math>, this is <math>x \\circ (y \\circ z)</math> versus <math>(x \\circ y) \\circ z</math>. That is, the tree is missing \"vertical parentheses\":\n\n[[File:OperadTreeCompose3.svg|Tree before composition]]\n\nIf the top two rows of operations are composed first (puts an upward parenthesis at the <math>(ab)c\\ \\ d</math> line; does the inner composition first), the following results:\n\n[[File:OperadTreeCompose4.svg|Intermediate tree]]\n\nwhich then evaluates unambiguously to yield a 4-ary operation.\nAs an annotated expression:\n:<math>\\theta_{(ab)c\\cdot d} \\circ ((\\theta_{ab \\cdot c},1_d) \\circ ((\\theta_{a\\cdot b},1_c),1_d))</math>\n\n[[File:OperadTreeCompose5.svg|Tree after composition]]\n\nIf the bottom two rows of operations are composed first (puts a downward parenthesis at the <math>ab\\quad c\\ \\ d</math> line; does the outer composition first), following results:\n\n[[File:OperadTreeCompose6.svg|Intermediate tree]]\n\nwhich then evaluates unambiguously to yield a 4-ary operation:\n\n[[File:OperadTreeCompose5.svg|Tree after composition]]\n\nThe operad axiom of associativity is that ''these yield the same result'', and thus that the expression <math>(((ab)c)d)</math> is unambiguous.\n\n===Identity axiom===\nThe identity axiom (for a binary operation) can be visualized in a tree as:\n\n[[Image:OperadIdentityAxiom.svg|The axiom of identity in an operad]]\n\nmeaning that the three operations obtained are equal: pre- or post- composing with the identity makes no difference. As for categories, <math>1 \\circ 1 = 1</math> is a corollary of the identity axiom.\n\n==Examples==\n=== Endomorphism operad ===\nLet ''V'' be a finite-dimensional vector space over a field ''k''. Then the ''endomorphism operad'' <math>\\mathcal{End}_V = \\{ \\mathcal{End}_V(n) \\}</math> of ''V'' consists of<ref>{{harvnb|Markl|loc=Example 2}}</ref>\n# <math>\\mathcal{End}_V(n)</math> = the space of linear maps <math>V^{\\otimes n} \\to V</math>,\n# (composition) <math>\\gamma(f, g_1, \\dots, g_n): V^{\\otimes i_1} \\otimes \\cdots \\otimes V^{\\otimes i_n} \\overset{g_1 \\otimes \\cdots \\otimes g_n}\\to V^{\\otimes n} \\overset{f}\\to V</math>,\n# (identity) <math>\\eta: k \\to \\mathcal{End}_V(1), \\, 1 \\mapsto \\operatorname{id}_V,</math>\n# (symmetric group action) <math>(\\gamma (f, g_1, \\dots, g_n)) * \\sigma = f \\circ g_{\\sigma^{-1}(1)} \\otimes \\dots \\otimes g_{\\sigma^{-1}(n)}, \\, \\sigma \\in \\Sigma_n.</math>\n\nIf <math>\\mathcal{O}</math> is another operad, each operad morphism <math>\\mathcal{O} \\to \\mathcal{End}</math> is called an [[operad algebra]] (notice this is analogous to the fact that each ''R''-module structure on an abelian group ''M'' amounts to a ring homomorphism <math>R \\to \\operatorname{End}(M)</math>.)\n\nDepending on applications, variations of the above are possible: for example, in algebraic topology, instead of vector spaces and tensor products between them, one uses [[reasonable topological space|(reasonable) topological spaces]] and cartesian products between them.\n\n===\"Little something\" operads===\n[[Image:Composition in the little discs operad.svg|thumb|Operadic composition in the '''little 2-discs operad'''.]]\n[[Image:Operadic composition in the operad of symmetries.svg|thumb|Operadic composition in the operad of symmetries.]]\n\nA ''little discs operad'' or, ''little balls operad'' or, more specifically, the ''little n-discs operad'' is a topological operad defined in terms of configurations of disjoint ''n''-dimensional [[disc (mathematics)|disc]]s inside a unit ''n''-disc centered in the [[Origin (mathematics)|origin]] of  ''R''<sup>''n''</sup>. The operadic composition for little 2-discs is illustrated in the figure.<ref>Giovanni Giachetta, Luigi Mangiarotti, [[Sardanashvily|Gennadi Sardanashvily]] (2005) ''Geometric and Algebraic Topological Methods in Quantum Mechanics,'' {{isbn|981-256-129-3}}, [https://books.google.com/books?id=fLbisfrkWpoC&pg=PA474&lpg=PA474&dq=%22Little+discs+operad%22&source=web&ots=NNKTqHPeX7&sig=KVdeG4dbMj1GfggbYd3zeNVs_zQ&hl=en&sa=X&oi=book_result&resnum=4&ct=result#PPA474,M1 pp. 474,475]</ref>{{clarify|I can see how the circles together but the meaning also needs to be explained in more down-to-earth terms.|date=December 2018}}\n\nOriginally the ''little n-cubes operad'' or the ''little intervals operad'' (initially called little ''n''-cubes [[PRO (category theory)|PROP]]s) was defined by [[Michael Boardman]] and [[Rainer Vogt]]  in a similar way, in terms of configurations of disjoint [[axis-aligned]] ''n''-dimensional [[hypercube]]s (n-dimensional [[interval (mathematics)|intervals]]) inside the [[unit hypercube]].<ref>{{Cite book|title=Axiomatic, Enriched and Motivic Homotopy Theory|last=Greenlees|first=J. P. C.|publisher=Springer Science & Business Media|year=2002|isbn=978-1-4020-1834-3|series=Proceedings of the NATO Advanced Study Institute on Axiomatic, Enriched and Motivic Homotopy Theory|location=Cambridge, [[United Kingdom]]|pages=154–156}}</ref> Later it was generalized by May<ref>{{cite journal | last1 = May | first1 = J. P. | year = 1977 | title = Infinite loop space theory | url = http://projecteuclid.org/DPubS?verb=Display&version=1.0&service=UI&handle=euclid.bams/1183538891&page=record| journal = Bull. Amer. Math. Soc. | volume = 83 | issue = 4| pages = 456–494 | doi=10.1090/s0002-9904-1977-14318-8}}</ref> to '''little convex bodies operad''', and \"little discs\" is a case of \"folklore\" derived from the \"little convex bodies\".<ref>{{Cite arxiv |eprint=math/9803156 |title=Grafting Boardman's Cherry Trees to Quantum Field Theory |last1=Stasheff |first1=Jim |year=1998}}</ref>\n\n===Associative operad===\nAnother class of examples of operads are those capturing the structures of algebraic structures, such as associative algebras, commutative algebras and Lie algebras. Each of these can be exhibited as a finitely presented operad, in each of these three generated by binary operations.\n\nThus, the associative operad is generated by a binary operation <math>\\psi</math>, subject to the condition that\n:<math>\\psi\\circ(\\psi,1)=\\psi\\circ(1,\\psi).</math>\n\nThis condition ''does'' correspond to [[associativity]] of the binary operation <math>\\psi</math>; writing <math>\\psi(a,b)</math> multiplicatively, the above condition is <math>(ab)c = a(bc)</math>. This associativity of the ''operation'' should not be confused with associativity of ''composition''; see the [[#Axiom_of_associativity|axiom of associativity]], above.\n\nThis operad is [[terminal object|terminal]] in the category of non-symmetric operads, as it has exactly one ''n''-ary operation for each ''n,'' corresponding to the unambiguous product of ''n'' terms: <math>x_1 \\dotsb x_n</math>. For this reason, it is sometimes written as 1 by category theorists (by analogy with the one-point set, which is terminal in the category of sets).\n\n===Terminal symmetric operad===\nThe terminal symmetric operad is the operad whose algebras are commutative monoids, which also has one ''n''-ary operation for each ''n'', with each <math>S_n</math> acting trivially; this triviality corresponds to commutativity, and whose ''n''-ary operation is the unambiguous product of ''n''-terms, where order does not matter:\n:<math>x_1 \\dotsb x_n = x_{\\sigma(1)} \\dotsb x_{\\sigma(n)}</math>\nfor any permutation <math>\\sigma \\in S_n</math>.\n\n===Operads from the symmetric and braid groups===\nThere is an operad for which each <math>P(n)</math> is given by the [[symmetric group]] <math>S_n</math>. The composite <math>\\sigma \\circ (\\tau_1, \\dots, \\tau_n)</math> permutes its inputs in blocks according to <math>\\sigma</math>, and within blocks according to the appropriate <math>\\tau_i</math>. Similarly, there is a non-<math>\\Sigma</math> operad for which each <math>P(n)</math> is given by the Artin [[braid group]] <math>B_n</math>. Moreover, this non-<math>\\Sigma</math> operad has the structure of a braided operad, which generalizes the notion of an operad from symmetric to braid groups.\n\n===Linear algebra===\nIn [[linear algebra]], vector spaces can be considered to be algebras over the operad <math>\\mathbf{R}^\\infty</math> (the infinite [[direct sum of modules|direct sum]], so only finitely many terms are non-zero; this corresponds to only taking finite sums), which parametrizes [[linear combinations]]: the vector <math>(2,3,-5,0,\\dots)</math> for instance corresponds to the linear combination\n:<math>2v_1 + 3v_2 -5v_3 + 0v_4 + \\cdots.</math>\n\nSimilarly, [[affine combination]]s, [[conical combination]]s, and [[convex combination]]s can be considered to correspond to the sub-operads where the terms sum to 1, the terms are all non-negative, or both, respectively. Graphically, these are the infinite affine hyperplane, the infinite hyper-octant, and the infinite simplex. This formalizes what is meant by <math>\\mathbf{R}^n</math> being or the standard simplex being model spaces, and such observations as that every bounded [[convex polytope]] is the image of a simplex. Here suboperads correspond to more restricted operations and thus more general theories.\n\nThis point of view formalizes the notion that linear combinations are the most general sort of operation on a vector space – saying that a vector space is an algebra over the operad of linear combinations is precisely the statement that ''all possible'' algebraic operations in a vector space are linear combinations. The basic operations of vector addition and scalar multiplication are a [[generating set]] for the operad of all linear combinations, while the linear combinations operad canonically encodes all possible operations on a vector space.\n\n=== Commutative-ring operad ===\nThe ''commutative-ring operad'' is an operad [[operad algebra|whose algebras]] are commutative rings (perhaps over some base field). The [[Koszul-dual]] of it is the [[Lie operad]] and conversely.\n\n== Constructs ==\nTypical algebraic constructions (e.g., free algebra construction) can be extended to operads. Let '''C''' denote a module category used in the definition of an operad; e.g., it can be the category of <math>\\mathbb{S}</math>-modules for symmetric operads.\n\n=== Free operad ===\nThere is the forgetful functor <math>\\mathsf{Oper} \\to \\textbf{C}</math>. The free operad functor <math>\\Gamma: \\textbf{C} \\to \\mathsf{Oper}</math> is defined as a left adjoint to the forgetful functor (this is the usual definition of [[free functor]]). Like a group or a ring, the free construction allows to express an operad in terms of generators and relations. By a ''free representation'' of an operad <math>\\mathcal{O}</math>, we mean writing <math>\\mathcal{O}</math> as a quotient of a free operad <math>\\mathcal{F} = \\Gamma(E)</math> generated by a module ''E'': then ''E'' is the generator of <math>\\mathcal{O}</math> and kernel of <math>\\mathcal{F} \\to \\mathcal{O}</math> is the relation.\n\nA (symmetric) operad <math>\\mathcal{O} = \\{ \\mathcal{O}(n) \\}</math> is called ''quadratic'' if it has a free presentation such that <math>E = \\mathcal{O}(2)</math> is the generator and the relation is contained in <math>\\Gamma(E)(3)</math>.<ref>{{harvnb|Markl|loc=Definition 37.}}</ref>\n\n== Operads in homotopy theory ==\n{{expand section|date=December 2018}}\nIn {{harvtxt|Stasheff|2004}}, Stasheff writes:\n:Operads are particularly important and useful in categories with a good notion of “homotopy”, where they play a key role in organizing hierarchies of higher homotopies.\n\n==See also==\n* [[PRO (category theory)]]\n* [[Algebra over an operad]]\n* [[Higher-order operad]]\n* [[E∞-operad]]\n\n==Notes==\n{{reflist|group=note}}\n\n===Citations===\n{{Reflist}}\n\n==References==\n\n*{{Cite book\n | author = Tom Leinster\n | year = 2004\n | title = Higher Operads, Higher Categories\n | publisher = Cambridge University Press\n | isbn = 978-0-521-53215-0\n | arxiv = math/0305049\n| bibcode = 2004hohc.book.....L\n }}\n*{{Cite book\n | author = Martin Markl, [[Steve Shnider]], [[Jim Stasheff]]\n | year = 2002\n | title = Operads in Algebra, Topology and Physics\n | publisher = American Mathematical Society\n | isbn = 978-0-8218-4362-8\n | url = http://www.ams.org/bookstore?fn=20&arg1=survseries&item=SURV-96\n}}\n* {{Cite arxiv\n| last = Markl\n| first = Martin\n| date = June 2006\n| title = Operads and PROPs\n| eprint = math/0601129\n}}\n* {{Cite journal\n   | last = Stasheff\n   | first = Jim | authorlink = Jim Stasheff\n   | title = What Is...an Operad?\n   | journal = [[Notices of the American Mathematical Society]]\n   |date=June–July 2004\n   | volume = 51\n   | issue = 6\n   | pages =630–631\n   | url = http://www.ams.org/notices/200406/what-is.pdf\n   | format = [[PDF]]\n   | accessdate = 2008-01-17\n}}\n*{{Citation | last1=Loday | first1=Jean-Louis | last2=Vallette | first2=Bruno |author-link=Jean-Louis Loday| title= Algebraic Operads | url=http://www-irma.u-strasbg.fr/~loday/PAPERS/LodayVallette.pdf | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Grundlehren der Mathematischen Wissenschaften  | isbn=978-3-642-30361-6 | year=2012 | volume=346|pages=}}\n*{{Citation | last=Zinbiel | first=Guillaume W. |author-link=Jean-Louis Loday| editor2-last=Guo | editor2-first=Li | editor1-last=Bai | editor1-first=Chengming | editor3-last=Loday | editor3-first=Jean-Louis | title=Operads and universal algebra | series =  Nankai Series in Pure, Applied Mathematics and Theoretical Physics  | isbn=9789814365116 | year=2012 | volume=9 | chapter=Encyclopedia of types of algebras 2010 | arxiv=1101.0267 | pages=217–298 |id=| bibcode=2011arXiv1101.0267Z }}\n*{{Citation|last=Fresse|first=Benoit|title=Homotopy of Operads and Grothendieck-Teichmüller Groups|volume=|pages=|series=Mathematical Surveys and Monographs|publisher=[[American Mathematical Society]]|isbn=978-1-4704-3480-9|mr=3643404|zbl=1373.55014|date=2017-05-17}}\n*Miguel A. Mendéz (2015). ''Set Operads in Combinatorics and Computer Science''. SpringerBriefs in Mathematics. [https://www.springer.com/us/book/9783319117126 ISBN 978-3-319-11712-6].\n*Samuele Giraudo (2018). ''Nonsymmetric Operads in Combinatorics''. Springer International Publishing. [https://www.springer.com/us/book/9783030020736 ISBN 978-3-030-02073-6].\n\n== External links ==\n*https://ncatlab.org/nlab/show/operad\n*https://golem.ph.utexas.edu/category/2011/05/an_operadic_introduction_to_en.html\n\n{{Use dmy dates|date=September 2010}}\n\n[[Category:Abstract algebra]]\n[[Category:Category theory]]"
    },
    {
      "title": "Operad algebra",
      "url": "https://en.wikipedia.org/wiki/Operad_algebra",
      "text": "In algebra, an '''operad algebra''' is an \"algebra\" over an [[operad]]. It is a generalization of an [[associative algebra]] over a commutative ring ''R'', with an operad replacing ''R''.\n\n== Definitions ==\nGiven an operad ''O'' (say, a [[symmetric sequence]] in a [[symmetric monoidal ∞-category]] ''C''), an '''algebra over an operad''', or '''''O''-algebra''' for short, is, roughly, a left module over ''O'' with multiplications parametrized by ''O''.\n\nIf ''O'' is a [[topological operad]], then one can say an algebra over an operad is an ''O''-monoid object in ''C''. If ''C'' is symmetric monoidal, this recovers the usual definition.\n\nLet ''C'' be symmetric monoidal ∞-category with monoidal structure distributive over colimits. If <math>f: O \\to O'</math> is a map of operads and, moreover, if ''f'' is a homotopy equivalence, then the ∞-category of algebras over ''O'' in ''C'' is equivalent to the ∞-category of algebras over ''O''' in ''C''.<ref>{{harvnb||Francis|loc=Proposition 2.9.}}</ref>\n\n== See also ==\n*[[En-ring]]\n*[[Homotopy Lie algebra]]\n\n== Notes ==\n{{reflist}}\n\n== References ==\n*John Francis, [http://www.math.northwestern.edu/~jnkf/writ/thezrev.pdf Derived Algebraic Geometry Over <math>\\mathcal{E}_n</math>-Rings]\n*{{cite arxiv|last=Hinich|first=Vladimir|date=1997-02-11|title=Homological algebra of homotopy algebras|eprint=q-alg/9702015}}\n\n== External links ==\n*http://ncatlab.org/nlab/show/operad\n*http://ncatlab.org/nlab/show/algebra+over+an+operad\n\n[[Category:Abstract algebra]]\n\n\n{{algebra-stub}}"
    },
    {
      "title": "Ordered exponential",
      "url": "https://en.wikipedia.org/wiki/Ordered_exponential",
      "text": "{{More citations needed|date=April 2018}}\nThe '''ordered exponential''', also called the '''path-ordered exponential''', is a [[mathematics|mathematical]] operation defined in [[non-commutative]] [[algebra over a field|algebras]], equivalent to the [[exponential function|exponential]] of the [[integral]] in the [[commutative]] algebras.  In practice the ordered exponential is used in [[matrix (math)|matrix]] and [[Operator (mathematics)|operator]] algebras.\n\n== Definition ==\n\nLet ''A'' be an [[algebra over a field|algebra]] over a [[real number|real]] or [[complex number|complex]] [[field (mathematics)|field]] ''K'', and ''a''(''t'') be a [[function (mathematics)|parameterized]] element of ''A'',\n\n: <math>a \\mathrel{:} K \\to A. \\, </math>\n\nThe parameter ''t'' in ''a''(''t'') is often referred to as the ''time parameter'' in this context.\n\nThe ordered exponential of ''a'' is denoted\n\n: <math>\\begin{align}\n\\operatorname{OE}[a](t) \\equiv \\mathcal{T} \\left\\{e^{\\int_0^t a(t') \\, dt'}\\right\\} & \\equiv \\sum_{n = 0}^\\infty \\frac{1}{n!} \\int_0^t \\cdots \\int_0^t \\mathcal{T} \\left\\{a(t'_1) \\cdots a(t'_{n})\\right\\} \\, dt'_1 \\cdots dt'_{n} \\\\\n& \\equiv \\sum_{n = 0}^\\infty \\int_0^t \\int_0^{t'_n} \\int_0^{t'_{n-1}} \\cdots \\int_0^{t'_{2}} a(t'_{n}) \\cdots a(t'_{1}) \\, dt'_1 \\cdots dt'_{n-2} dt'_{n-1} dt'_{n} \n\\end{align}</math>\n\nwhere the term ''n''&nbsp;=&nbsp;0 is equal to 1 and\nwhere <math>\\mathcal{T}</math> is a higher-order operation that ensures the exponential is [[Path-ordering#Time_ordering|time-ordered]]: any product of ''a''(''t'') that occurs in the expansion of the exponential must be ordered such that the value of ''t'' is increasing from right to left of the product; a schematic example:\n: <math>\\mathcal{T} \\left\\{a(1.2) a(9.5) a(4.1)\\right\\} = a(9.5) a(4.1) a(1.2).</math>\nThis restriction is necessary as products in the algebra are not necessarily commutative.\n\nThe operation maps a parameterized element onto another parameterized element, or symbolically,\n: <math>\\operatorname{OE} \\mathrel{:} \\left(K \\to A\\right) \\to \\left(K \\to A\\right). </math>\n\nThere are various ways to define this integral more rigorously.\n\n=== Product of exponentials ===\n\nThe ordered exponential can be defined as the left [[product integral]] of the [[infinitesimal]] exponentials, or equivalently, as an [[ordered product]] of exponentials in the [[Limit (mathematics)|limit]] as the number of terms grows to infinity:\n\n: <math>\\operatorname{OE}[a](t) = \\prod_0^t e^{a(t') \\, dt'} \\equiv\n    \\lim_{N \\rightarrow \\infty} \\left(\n      e^{a(t_N) \\, \\Delta t} e^{a(t_{N-1}) \\, \\Delta t} \\cdots\n      e^{a(t_1) \\, \\Delta t} e^{a(t_0) \\, \\Delta t}\n    \\right)\n  </math>\n\nwhere the time moments {{nowrap|&#123;''t''<sub>0</sub>, …, ''t''<sub>''N''</sub>&#125;}} are defined as {{nowrap|''t''<sub>''i''</sub> ≡ ''i'' Δ''t''}} for {{nowrap|''i'' {{=}} 0, …, ''N''}}, and {{nowrap|Δ''t'' ≡ ''t'' / ''N''}}.\n\nThe ordered exponential is in fact a [[Product integral#Type II|geometric integral]].<ref name=nnc>Michael Grossman and Robert Katz. [https://books.google.com/books?q=%22Non-Newtonian+Calculus%22&btnG=Search+Books&as_brr=0 ''Non-Newtonian Calculus''], {{ISBN|0912938013}}, 1972.</ref><ref>A. E. Bashirov, E. M. Kurpınar, A. Özyapıcı. [http://linkinghub.elsevier.com/retrieve/pii/S0022247X07003824 ''Multiplicative calculus and its applications''], Journal of Mathematical Analysis and Applications, 2008.</ref>   <ref name=FvA>Luc Florack and Hans van Assen.[http://www.springerlink.com/content/82057067845wx7v6/ \"Multiplicative calculus in biomedical image analysis\"], Journal of Mathematical Imaging and Vision, 2011. </ref>\n\n=== Solution to a differential equation ===\n\nThe ordered exponential is unique solution of the [[initial value problem]]:\n\n: <math>\\begin{align}\n    \\frac{d}{d t} \\operatorname{OE}[a](t) &= a(t) \\operatorname{OE}[a](t) \\text{,} \\\\[5pt]\n    \\operatorname{OE}[a](0) &= 1 \\text{.}\n  \\end{align}</math>\n\n=== Solution to an integral equation ===\n\nThe ordered exponential is the solution to the [[integral equation]]:\n\n:: <math>\\operatorname{OE}[a](t) = 1 + \\int_0^t a(t') \\operatorname{OE}[a](t') \\, dt'. </math>\n\nThis equation is equivalent to the previous initial value problem.\n\n=== Infinite series expansion ===\n\nThe ordered exponential can be defined as an infinite sum,\n\n: <math>\\operatorname{OE}[a](t) = 1 + \\int_0^t a(t_1) \\, dt_1+ \\int_0^t \\int_0^{t_1} a(t_1) a(t_2) \\, dt_2 \\, dt_1 + \\cdots.</math>\n\nThis can be derived by recursively substituting the integral equation into itself.\n\n== Example ==\n\nGiven a manifold <math>M</math> where for a <math>e \\in TM</math> with [[Group theory|group]] transformation <math>g: e \\mapsto g e</math> it holds at a point <math>x \\in M</math>:\n\n: <math>de(x) + \\operatorname{J}(x)e(x) = 0.</math>\n\nHere, <math>d</math> denotes [[exterior differentiation]] and <math>\\operatorname{J}(x)</math> is the connection operator (1-form field) acting on <math>e(x)</math>. When integrating above equation it holds (now, <math>\\operatorname{J}(x)</math> is the connection operator expressed in a coordinate basis)\n\n: <math>e(y) = \\operatorname{P} \\exp \\left(- \\int_x^y J(\\gamma (t)) \\gamma '(t) \\, dt \\right) e(x)</math>\n\nwith the path-ordering operator <math>\\operatorname{P}</math> that orders factors in order of the path <math>\\gamma(t) \\in M</math>. For the special case that <math>\\operatorname{J}(x)</math> is an [[Antisymmetry|antisymmetric]] operator and <math>\\gamma</math> is an infinitesimal rectangle with edge lengths <math>|u|,|v|</math> and corners at points <math>x,x+u,x+u+v,x+v,</math> above expression simplifies as follows :\n\n: <math>\n\\begin{align}\n& \\operatorname{OE}[- \\operatorname{J}]e(x) \\\\[5pt]\n= {} & \\exp [- \\operatorname{J}(x+v) (-v)] \\exp [- \\operatorname{J}(x+u+v) (-u)] \\exp [- \\operatorname{J}(x+u) v] \\exp [- \\operatorname{J}(x) u] e(x) \\\\[5pt]\n= {} & [1 - \\operatorname{J}(x+v) (-v)][1 - \\operatorname{J}(x+u+v) (-u)][1 - \\operatorname{J}(x+u) v][1 - \\operatorname{J}(x) u] e(x).\n\\end{align}\n</math>\n\nHence, it holds the group transformation identity <math>\\operatorname{OE}[- \\operatorname{J}] \\mapsto g \\operatorname{OE}[\\operatorname{J}] g^{-1}</math>. If <math>- \\operatorname{J}(x)</math> is a smooth connection, expanding above quantity to second order in infinitesimal quantities <math>|u|,|v|</math> one obtains for the ordered exponential the identity with a correction term that is proportional to the [[Riemann curvature tensor| curvature tensor]].\n\n==See also==\n\n* [[Path-ordering]] (essentially the same concept)\n* [[Magnus expansion]]\n* [[Product integral]]\n* [[Multiplicative calculus]]\n* [[List of derivatives and integrals in alternative calculi]]\n* [[Indefinite product]]\n*[[Fractal derivative]]\n\n==References==\n\n<references />\n\n==External links==\n* [https://sites.google.com/site/nonnewtoniancalculus/ Non-Newtonian calculus website]\n\n[[Category:Abstract algebra]]\n[[Category:Ordinary differential equations]]\n[[Category:Non-Newtonian calculus]]"
    },
    {
      "title": "Orthogonality",
      "url": "https://en.wikipedia.org/wiki/Orthogonality",
      "text": "{{redirect|Orthogonal|the trilogy of novels by [[Greg Egan]]|Orthogonal (novel)}}\n\n[[File:Perpendicular-coloured.svg|thumb|right|220px|The line segments AB and CD are orthogonal to each other.]]\n\nIn [[mathematics]], '''orthogonality''' is the generalization of the notion of [[perpendicularity]] to the [[linear algebra]] of [[bilinear form]]s.  Two elements ''u'' and ''v'' of a [[vector space]] with bilinear form ''B'' are '''orthogonal''' when {{nowrap|1=''B''(''u'', ''v'') = 0}}.  Depending on the bilinear form, the vector space may contain nonzero self-orthogonal vectors. In the case of [[function space]]s, families of [[orthogonal functions]] are used to form a [[basis (linear algebra)|basis]].\n\nBy extension, orthogonality is also used to refer to the separation of specific features of a system. The term also has specialized meanings in other fields including art and chemistry.\n\n==Etymology==\n\nThe word comes from the [[Ancient Greek|Greek]] ''{{lang|grc|ὀρθός}}'' (''orthos''), meaning \"upright\"<ref>Liddell and Scott, ''[[A Greek–English Lexicon]]'' [http://www.perseus.tufts.edu/hopper/morph?l=o%29rqos&la=greek#lexicon ''s.v.'' ὀρθός]</ref> , and ''{{lang|grc|γωνία}}'' (''gonia''), meaning \"angle\"<ref>Liddell and Scott, ''[[A Greek–English Lexicon]]'' [http://www.perseus.tufts.edu/hopper/morph?l=gwni%2Fa&la=greek#lexicon ''s.v.'' γωνία]</ref>.\nThe ancient Greek ὀρθογώνιον ''orthogōnion'' and classical Latin ''orthogonium'' originally denoted a [[rectangle]].<ref>Liddell and Scott, ''[[A Greek–English Lexicon]]'' [http://www.perseus.tufts.edu/hopper/morph?l=o%29rqog%2Fwnion&la=greek#lexicon ''s.v.'' ὀρθογώνιον]</ref> Later, they came to mean a [[right triangle]].  In the 12th century, the post-classical Latin word ''orthogonalis'' came to mean a right angle or something related to a right angle.<ref>[[Oxford English Dictionary]], Third Edition, September 2004, ''s.v.'' orthogonal</ref>\n\n==Mathematics and physics==\n[[File:Orthogonality and rotation.svg|thumb|350px|Orthogonality and rotation of coordinate systems compared between '''left:''' [[Euclidean space]] through circular [[angle]] ''ϕ'', '''right:''' in [[Minkowski spacetime]] through [[hyperbolic angle]] ''ϕ'' (red lines labelled ''c'' denote the [[worldline]]s of a light signal, a vector is orthogonal to itself if it lies on this line).<ref>{{cite book|title=Gravitation|author1=J.A. Wheeler |author2=C. Misner |author3=K.S. Thorne |publisher=W.H. Freeman & Co|page=58|year=1973|isbn=0-7167-0344-0}}</ref>]]\n\n===Definitions===\n*In [[geometry]], two [[Euclidean vector]]s are '''orthogonal''' if they are [[perpendicular]], ''i.e.'', they form a [[right angle]].\n*Two [[vector space|vectors]], ''x'' and ''y'', in an [[inner product space]], ''V'', are ''orthogonal'' if their inner product <math>\\langle x, y \\rangle</math> is zero.<ref>{{cite web|title=Wolfram MathWorld|url=http://mathworld.wolfram.com/Orthogonal.html}}</ref>  This relationship is denoted <math>x \\perp y</math>.\n*Two [[Linear subspace|vector subspaces]], ''A'' and ''B'', of an inner product space ''V'', are called '''orthogonal subspaces''' if each vector in ''A'' is orthogonal to each vector in ''B''.  The largest subspace of ''V'' that is orthogonal to a given subspace is its [[orthogonal complement]].\n*Given a [[Module (mathematics)|module]] ''M'' and its dual ''M''<sup>∗</sup>, an element ''m''′ of ''M''<sup>∗</sup> and an element ''m'' of ''M'' are ''orthogonal'' if their [[natural pairing]] is zero, i.e. {{nowrap|1={{langle}}''m''′, ''m''{{rangle}} = 0}}. Two sets {{nowrap|''S''′ ⊆ ''M''<sup>∗</sup>}} and {{nowrap|''S'' ⊆ ''M''}} are orthogonal if each element of ''S''′ is orthogonal to each element of ''S''.<ref>{{citation |author=Bourbaki |title=Algebra I |section=ch. II §2.4 |page=234}}</ref>\n*A [[term rewriting system]] is said to be [[orthogonality (term rewriting)|orthogonal]] if it is left-linear and is non-ambiguous. Orthogonal term rewriting systems are [[confluence (term rewriting)|confluent]].\n\nA set of vectors in an inner product space is called '''pairwise orthogonal''' if each pairing of them is orthogonal. Such a set is called an '''orthogonal set'''.\n\nIn certain cases, the word ''normal'' is used to mean ''orthogonal'', particularly in the geometric sense as in the [[surface normal|normal to a surface]]. For example, the ''y''-axis is normal to the curve {{nowrap|1=''y'' = ''x''<sup>2</sup>}} at the origin.  However, ''normal'' may also refer to the magnitude of a vector. In particular, a set is called [[Orthonormality|orthonormal]] (orthogonal plus normal) if it is an orthogonal set of [[unit vector]]s. As a result, use of the term ''normal'' to mean \"orthogonal\" is often avoided. The word \"normal\" also has a different meaning in [[probability]] and [[statistics]].\n\nA vector space with a [[bilinear form]] generalizes the case of an inner product. When the bilinear form applied to two vectors results in zero, then they are '''orthogonal'''. The case of a [[pseudo-Euclidean space|pseudo-Euclidean plane]] uses the term [[hyperbolic orthogonality]]. In the diagram, axes x′ and t′ are hyperbolic-orthogonal for any given ''ϕ''.\n\n===Euclidean vector spaces===\nIn [[Euclidean space]], two vectors are orthogonal [[if and only if]] their [[dot product]] is zero, i.e. they make an angle of 90° (π/2 [[radian]]s), or one of the vectors is zero.<ref>{{cite book|author1=Trefethen, Lloyd N. |author2=Bau, David |lastauthoramp=yes |title=Numerical linear algebra|publisher=SIAM|year=1997|isbn=978-0-89871-361-9|page=13|url=https://books.google.com/books?id=bj-Lu6zjWbEC&pg=PA13}}</ref> Hence orthogonality of vectors is an extension of the concept of [[perpendicular]] vectors to spaces of any dimension.\n\nThe [[orthogonal complement]] of a subspace is the space of all vectors that are orthogonal to every vector in the subspace.  In a three-dimensional Euclidean vector space, the orthogonal complement of a [[line (geometry)|line]] through the origin is the [[plane (mathematics)|plane]] through the origin perpendicular to it, and vice versa.<ref name=\"R. Penrose 2007 417–419\">{{cite book |author=R. Penrose| title=[[The Road to Reality]]| publisher= Vintage books|pages=417–419| year=2007 | isbn=0-679-77631-1}}</ref>\n\nNote that the geometric concept two planes being perpendicular does not correspond to the orthogonal complement, since in three dimensions a pair of vectors, one from each of a pair of perpendicular planes, might meet at any angle.\n\nIn four-dimensional Euclidean space, the orthogonal complement of a line is a [[hyperplane]] and vice versa, and that of a plane is a plane.<ref name=\"R. Penrose 2007 417–419\"/>\n\n===Orthogonal functions===\n{{Main|Orthogonal functions}}\nBy using [[integral calculus]], it is common to use the following to define the [[inner product]] of two [[function (mathematics)|functions]] ''f'' and ''g'' with respect to a nonnegative [[weight function]] ''w'' over an interval {{nowrap|[''a'', ''b'']}}:\n\n:<math>\\langle f, g\\rangle_w = \\int_a^b f(x)g(x)w(x)\\,dx.</math>\n\nIn simple cases, {{nowrap|1=''w''(''x'') = 1}}.\n\nWe say that functions ''f'' and ''g'' are '''orthogonal''' if their inner product (equivalently, the value of this integral) is zero:\n\n:<math>\\langle f, g\\rangle_w = 0.</math>\n\nOrthogonality of two functions with respect to one inner product does not imply orthogonality with respect to another inner product.\n\nWe write the [[norm (mathematics)|norm]] with respect to this inner product as\n:<math>\\|f\\|_w = \\sqrt{\\langle f, f\\rangle_w}</math>\n\nThe members of a set of functions {{nowrap|1={''f''<sub>''i''</sub> : ''i'' = 1, 2, 3, ...}{{null}}}} are ''orthogonal'' with respect to ''w'' on the interval {{nowrap|[''a'', ''b'']}} if\n:<math>\\langle f_i, f_j \\rangle_w=0\\quad i \\ne j.</math>\nThe members of such a set of functions are ''orthonormal'' with respect to ''w'' on the interval {{nowrap|[''a'', ''b'']}} if\n:<math>\\langle f_i, f_j \\rangle_w=\\delta_{i,j},</math>\nwhere\n:<math>\\delta_{i,j}=\\left\\{\\begin{matrix}1, & & i=j \\\\ 0, & & i\\neq j\\end{matrix}\\right.</math>\nis the [[Kronecker delta]].\nIn other words, every pair of them (excluding pairing of a function with itself) is orthogonal, and the norm of each is 1. See in particular the  [[orthogonal polynomials]].\n\n===Examples===\n* The vectors (1, 3, 2)<sup>T</sup>, (3, −1, 0)<sup>T</sup>, (1, 3, −5)<sup>T</sup>  are orthogonal to each other, since (1)(3) + (3)(−1) + (2)(0) = 0, (3)(1) + (−1)(3) + (0)(−5) = 0, and (1)(1) + (3)(3) + (2)(−5) = 0.\n* The vectors (1, 0, 1, 0, ...)<sup>T</sup> and (0, 1, 0, 1, ...)<sup>T</sup> are orthogonal to each other. The dot product of these vectors is 0. We can then make the generalization to consider the vectors in '''Z'''<sub>2</sub><sup>''n''</sup>:\n\n::<math>\\mathbf{v}_k = \\sum_{i=0\\atop ai+k < n}^{n/a} \\mathbf{e}_i</math>\n\n:for some positive integer ''a'', and for {{nowrap|1 ≤ ''k'' ≤ ''a'' − 1}}, these vectors are orthogonal, for example {{nowrap|(1, 0, 0, 1, 0, 0, 1, 0)<sup>T</sup>}}, {{nowrap|(0, 1, 0, 0, 1, 0, 0, 1)<sup>T</sup>}}, {{nowrap|(0, 0, 1, 0, 0, 1, 0, 0)<sup>T</sup>}} are orthogonal.\n\n* The functions {{nowrap|2''t'' + 3}} and {{nowrap|45''t''<sup>2</sup> + 9''t'' − 17}} are orthogonal with respect to a unit weight function on the interval from −1 to 1:<br/><math>\\int_{-1}^1 \\left(2t+3\\right)\\left(45t^2+9t-17\\right)\\,dt = 0</math>\n* The functions 1, sin(''nx''), cos(''nx'') : ''n'' = 1, 2, 3, ... are orthogonal with respect to [[Riemann integration]] on the intervals {{nowrap|[0, 2π]}}, {{nowrap|[−π, π]}}, or any other closed interval of length 2π. This fact is a central one in [[Fourier series]].\n\n====Orthogonal polynomials====\n* Various polynomial sequences named for [[mathematician]]s of the past are sequences of [[orthogonal polynomials]]. In particular:\n**The [[Hermite polynomials]] are orthogonal with respect to the [[Gaussian distribution]] with zero mean value.\n**The [[Legendre polynomials]] are orthogonal with respect to the [[uniform distribution (continuous)|uniform distribution]] on the interval {{nowrap|[−1, 1]}}.\n**The [[Laguerre polynomials]] are orthogonal with respect to the [[exponential distribution]]. Somewhat more general Laguerre polynomial sequences are orthogonal with respect to [[gamma distribution]]s.\n**The [[Chebyshev polynomials]] of the first kind are orthogonal with respect to the measure <math>1/\\sqrt{1-x^2}.</math>\n**The Chebyshev polynomials of the second kind are orthogonal with respect to the [[Wigner semicircle distribution]].\n\n====Orthogonal states in quantum mechanics====\n* In [[quantum mechanics]], a sufficient (but not necessary) condition that two [[eigenstates]] of a [[Hermitian operator]], <math> \\psi_m </math> and <math> \\psi_n </math>, are orthogonal is that they correspond to different eigenvalues. This means, in [[Dirac notation]], that <math> \\langle \\psi_m | \\psi_n \\rangle = 0 </math> if <math> \\psi_m </math> and <math> \\psi_n </math> correspond to different eigenvalues. This follows from the fact that [[Schrödinger equation|Schrödinger's equation]] is a [[Sturm–Liouville theory|Sturm–Liouville]] equation (in Schrödinger's formulation) or that observables are given by hermitian operators (in Heisenberg's formulation).{{citation needed|date=February 2012}}\n\n==Art==\nIn art, the [[Perspective (graphical)|perspective]] (imaginary) lines pointing to the [[vanishing point]] are referred to as \"orthogonal lines\".\n\nThe term \"orthogonal line\" often has a quite different meaning in the literature of modern art criticism. Many works by painters such as [[Piet Mondrian]] and [[Burgoyne Diller]] are noted for their exclusive use of \"orthogonal lines\" — not, however, with reference to perspective, but rather referring to lines that are straight and exclusively horizontal or vertical, forming right angles where they intersect. For example, an essay at the [[Web site]] of the [[Thyssen-Bornemisza Museum]] states that \"Mondrian ... dedicated his entire oeuvre to the investigation of the balance between orthogonal lines and primary colours.\" [http://www.museothyssen.org/thyssen_ing/coleccion/obras_ficha_texto_print497.html]\n\n==Computer science==<!-- This section is linked from [[Motorola 68000]] -->\nOrthogonality in programming language design is the ability to use various language features in arbitrary combinations with consistent results.<ref>Michael L. Scott, ''Programming Language Pragmatics'', p. 228</ref> This usage was introduced by [[Adriaan van Wijngaarden|Van Wijngaarden]] in the design of [[Algol 68]]:\n<blockquote>\nThe number of independent primitive concepts has been minimized in order that the language be easy to describe, to learn, and to implement. On the other hand, these concepts have been applied “orthogonally” in order to maximize the expressive power of the language while trying to avoid deleterious superfluities.<ref>1968, Adriaan van Wijngaarden et al., Revised Report on the Algorithmic Language ALGOL 68, section 0.1.2, Orthogonal design</ref>\n</blockquote>\n\nOrthogonality is a system design property which guarantees that modifying the technical effect produced by a component of a system neither creates nor propagates side effects to other components of the system. Typically this is achieved through the [[separation of concerns]] and [[Information Hiding#Encapsulation|encapsulation]], and it is essential for feasible and compact designs of complex systems. The emergent behavior of a system consisting of components should be controlled strictly by formal definitions of its logic and not by side effects resulting from poor integration, i.e., non-orthogonal design of modules and interfaces. Orthogonality reduces testing and development time because it is easier to verify designs that neither cause side effects nor depend on them.\n\nAn [[instruction set]] is said to be '''[[orthogonal instruction set|orthogonal]]''' if it lacks redundancy (i.e., there is only a single instruction that can be used to accomplish a given task)<ref>{{cite book|author1=Null, Linda  |author2=Lobur, Julia |lastauthoramp=yes |title=The essentials of computer organization and architecture|publisher=Jones & Bartlett Learning|edition=2nd|year=2006|isbn=978-0-7637-3769-6|page=257|url=https://books.google.com/books?id=QGPHAl9GE-IC&pg=PA257}}</ref> and is designed such that instructions can use any [[processor register|register]] in any [[addressing mode]]. This terminology results from considering an instruction as a vector whose components are the instruction fields.  One field identifies the registers to be operated upon and another specifies the addressing mode. An [[orthogonal instruction set]] uniquely encodes all combinations of registers and addressing modes.{{Citation needed|date=April 2011}}\n\n==Communications==\n{{unreferenced section|date=May 2019}}\nIn communications, multiple-access schemes are orthogonal when an ideal receiver can completely reject arbitrarily strong unwanted signals from the desired signal using different [[basis function]]s. One such scheme is [[time division multiple access|TDMA]], where the orthogonal basis functions are nonoverlapping rectangular pulses (\"time slots\").\n\nAnother scheme is [[orthogonal frequency-division multiplexing]] (OFDM), which refers to the use, by a single transmitter, of a set of frequency multiplexed signals with the exact minimum frequency spacing needed to make them orthogonal so that they do not interfere with each other.  Well known examples include ('''a''', '''g''', and '''n''') versions of [[802.11]] [[Wi-Fi]]; [[WiMAX]]; [[ITU-T]] [[G.hn]], [[DVB-T]], the terrestrial digital TV broadcast system used in most of the world outside North America; and DMT (Discrete Multi Tone), the standard form of [[ADSL]].\n\nIn OFDM, the [[subcarrier]] frequencies are chosen{{how|date=May 2019}} so that the subcarriers are orthogonal to each other, meaning that crosstalk between the subchannels is eliminated and intercarrier guard bands are not required. This greatly simplifies the design of both the transmitter and the receiver. In conventional FDM, a separate filter for each subchannel is required.\n\n==Statistics, econometrics, and economics==\nWhen performing statistical analysis, [[Dependent and independent variables|independent variables]] that affect a particular [[Dependent and independent variables|dependent variable]] are said to be orthogonal if they are uncorrelated,<ref>{{cite book |title=Probability, Random Variables and Stochastic Processes |author1=Athanasios Papoulis |author2=S. Unnikrishna Pillai |year=2002 |pages=211 |publisher=McGraw-Hill |isbn= 0-07-366011-6}}</ref> since the covariance forms an inner product. In this case the same results are obtained for the effect of any of the independent variables upon the dependent variable, regardless of whether one models the effects of the variables  individually with [[simple linear regression|simple regression]] or simultaneously with [[multiple regression]]. If [[correlation]] is present, the factors are not orthogonal and different results are obtained by the two methods. This usage arises from the fact that if centered by subtracting the [[expected value]] (the mean), uncorrelated variables are orthogonal in the geometric sense discussed above, both as observed data (i.e., vectors) and as random variables (i.e., density functions).\nOne [[econometrics|econometric]] formalism that is alternative to the [[maximum likelihood]] framework, the [[Generalized Method of Moments]], relies on orthogonality conditions. In particular, the [[Ordinary Least Squares]] estimator may be easily derived from an orthogonality condition between the explanatory variables and model residuals.\n\n==Taxonomy==\nIn [[Taxonomy (general)|taxonomy]], an orthogonal classification is one in which no item is a member of more than one group, that is, the classifications are mutually exclusive.\n\n==Combinatorics==\nIn [[combinatorics]], two ''n''×''n'' [[Latin squares]] are said to be orthogonal if their [[superimposition]] yields all possible ''n''<sup>2</sup> combinations of entries.<ref>{{cite book|author=Hedayat, A.|title=Orthogonal arrays: theory and applications|publisher=Springer|year=1999|isbn=978-0-387-98766-8|page=168|url=https://books.google.com/books?id=HrUYlIbI2mEC&pg=PA168|display-authors=etal}}</ref>\n\n==Chemistry and biochemistry==\nIn [[organic synthesis|synthetic organic chemistry]] orthogonal [[protecting group|protection]] is a strategy allowing the deprotection of [[functional group]]s independently of each other. In chemistry and biochemistry, an orthogonal interaction occurs when there are two pairs of substances and each substance can interact with their respective partner, but does not interact with either substance of the other pair. For example, [[DNA]] has two orthogonal pairs: cytosine and guanine form a base-pair, and adenine and thymine form another base-pair, but other base-pair combinations are strongly disfavored. As a chemical example, tetrazine reacts with transcyclooctene and azide reacts with cyclooctyne without any cross-reaction, so these are mutually orthogonal reactions, and so, can be performed simultaneously and selectively.<ref>{{cite journal|doi = 10.1002/anie.201104389|title = Bioorthogonal Reaction Pairs Enable Simultaneous, Selective, Multi-Target Imaging|year = 2012|last1 = Karver|first1 = Mark R.|last2 = Hilderbrand|first2 = Scott A.|journal = Angewandte Chemie International Edition|volume = 51|issue = 4|pages = 920–2|pmc=3304098|pmid=22162316}}</ref> [[Bioorthogonal chemistry]] refers to chemical reactions occurring inside living systems without reacting with naturally present cellular components. In [[supramolecular chemistry]] the notion of orthogonality refers to the possibility of two or more supramolecular, often [[non-covalent]], interactions being compatible; reversibly forming without interference from the other.\n\nIn [[analytical chemistry]], analyses are \"orthogonal\" if they make a measurement or identification in completely different ways, thus increasing the reliability of the measurement.  This is often required as a part of a [[new drug application]].\n\n==System reliability==\nIn the field of system reliability orthogonal redundancy is that form of redundancy where the form of backup device or method is completely different from the prone to error device or method. The failure mode of an orthogonally redundant back-up device or method does not intersect with and is completely different from the failure mode of the device or method in need of redundancy to safeguard the total system against catastrophic failure.\n\n==Neuroscience==\nIn [[neuroscience]], a sensory map in the brain which has overlapping stimulus coding (e.g. location and quality) is called an orthogonal map.\n\n==Gaming==\nIn board games such as [[chess]] which feature a grid of squares, 'orthogonal' is used to mean \"in the same row/'rank' or column/'file'\".  This is the counterpart to squares which are \"diagonally adjacent\".<ref>{{cite web|title=chessvariants.org chess glossary|url=http://www.chessvariants.org/misc.dir/coreglossary.html#orthogonal_direction}}</ref> In the ancient Chinese board game [[Go (game)|Go]] a player can capture the stones of an opponent by occupying all orthogonally-adjacent points.\n\n==Other examples==\nStereo vinyl records encode both the left and right stereo channels in a single groove.  The V-shaped groove in the vinyl has walls that are 90 degrees to each other, with variations in each wall separately encoding one of the two analogue channels that make up the stereo signal.  The cartridge senses the motion of the stylus following the groove in two orthogonal directions: 45 degrees from vertical to either side.<ref>For an illustration, see [https://www.youtube.com/watch?v=umu37m0qUiE YouTube].</ref>  A pure horizontal motion corresponds to a mono signal, equivalent to a stereo signal in which both channels carry identical (in-phase) signals.\n\n==See also==\n{{Wiktionary|orthogonal}}\n* [[Imaginary number]]\n* [[Isogonal (disambiguation)|Isogonal]]\n* [[Isogonal trajectory]]\n* [[Orthogonal complement]]\n* [[Orthogonal group]]\n* [[Orthogonal matrix]]\n* [[Orthogonal polynomials]]\n* [[Orthogonalization]]\n** [[Gram–Schmidt process]]\n* [[Orthonormal basis]]\n* [[Orthonormality]]\n* [[Orthogonal transform]]\n* Pan-orthogonality occurs in [[coquaternion]]s\n* [[Surface normal]]\n* [[Orthogonal ligand-protein pair]]\n\n==References==\n{{Reflist|30em}}\n\n==Further reading==\n* [http://www.faqs.org/docs/artu/ch04s02.html Chapter 4 – Compactness and Orthogonality] in ''[[The Art of Unix Programming]]''\n\n{{linear algebra}}\n\n[[Category:Abstract algebra]]\n[[Category:Linear algebra]]"
    },
    {
      "title": "Parallel (operator)",
      "url": "https://en.wikipedia.org/wiki/Parallel_%28operator%29",
      "text": "<!-- Please do not remove or change this AfD message until the discussion has been closed. -->\n{{Article for deletion/dated|page=Parallel (operator)|timestamp=20190622173409|year=2019|month=June|day=22|substed=yes|help=off}}\n<!-- Once discussion is closed, please place on talk page: {{Old AfD multi|page=Parallel (operator)|date=22 June 2019|result='''keep'''}} -->\n<!-- End of AfD message, feel free to edit beyond this point -->\n{{mi|\n{{Orphan|date=May 2016}}{{notability|date=May 2016}}\n{{expert needed|1=Mathematics|date=May 2016}}\n}}\nThe '''parallel operator''' <math>\\|</math> (pronounced \"parallel\") is a [[Function (mathematics)|mathematical function]] which is used especially as shorthand in [[electrical engineering]]. It represents the [[Multiplicative inverse|reciprocal]] value of a sum of reciprocal values and is defined by:<ref>{{cite book |last1=Basso |first1=Christophe P. |title=Linear Circuit Transfer Functions: An Introduction to Fast Analytical Techniques |date=2016 |publisher=John Wiley & Sons |isbn=9781119236368 |url=https://books.google.co.uk/books?id=gjHYCwAAQBAJ&pg=PA12&lpg=PA12&dq=%22parallel+operator%22+circuit+analysis&source=bl&ots=dk1N2dhmlw&sig=qYb6X6edbWYnj_awkvmliq0xeEE&hl=en&sa=X&ved=2ahUKEwiwhZ71wcHfAhUDzYUKHWsuBS04ChDoATAGegQIBhAB#v=onepage&q=%22parallel%20operator%22%20circuit%20analysis&f=false |accessdate=28 December 2018 |language=en}}</ref><ref>{{cite web |title=EECS 16A  Designing Information Devices and Systems I |url=https://inst.eecs.berkeley.edu/~ee16a/fa18/lectures/Note15.pdf#page=12 |accessdate=28 December 2018}}</ref><ref>{{cite web |title=ECE1250 COOKBOOK—NODES,SERIES,PARALLEL,SHORT CIRCUIT |url=https://utah.instructure.com/files/58567188/download?download_frd=1 |accessdate=2 March 2019}}</ref>\n\n<math>\\begin{array}{rlcl}\n\\|:\\ & \\overline{\\mathbb{C}} \\times \\overline{\\mathbb{C}} &\\to& \\overline{\\mathbb{C}}\\\\\n& (a,b) &\\mapsto& a \\| b = \\frac{1}{\\frac{1}{a} + \\frac{1}{b}}\n\\end{array}</math> \n\nThat is, it gives half of the [[harmonic mean]] of two numbers a and b.\n\nThe total resistance of [[parallel resistors|resistors connected in parallel]] is the reciprocal of the sum of the reciprocals of the individual resistors. \n:[[File:resistors in parallel.svg|A diagram of several resistors, side by side, both leads of each connected to the same wires]]\n:<math>\n\\frac{1}{R_\\mathrm{eq}} = \\frac{1}{R_1} + \\frac{1}{R_2} + \\cdots +  \\frac{1}{R_n}.\n</math>\n\n== References ==\n{{Reflist}}\n\n\n[[Category:Binary_operations]]\n[[Category:Abstract_algebra]]\n[[Category:Elementary_algebra]]\n[[Category:Multiplication]]\n\n\n{{applied-math-stub}}"
    },
    {
      "title": "Perfect complex",
      "url": "https://en.wikipedia.org/wiki/Perfect_complex",
      "text": "In algebra, a '''perfect complex''' of modules over a commutative ring ''A'' is an object in the derived category of ''A''-modules that is quasi-isomorphic to a bounded complex of finite projective ''A''-modules. A '''perfect module''' is a module that is perfect when it is viewed as a complex concentrated at degree zero. For example, if ''A'' is Noetherian, a module over ''A'' is perfect if and only if it has finite [[projective dimension]].\n\n== Other characterizations ==\nPerfect complexes are precisely the [[compact object (mathematics)|compact object]]s in the unbounded derived category <math>D(A)</math> of ''A''-modules.<ref>See, e.g., {{harvtxt|Ben-Zvi|Francis|Nadler|2010}}</ref> They are also precisely the [[dualizable object]]s in this category.<ref>Lemma 2.6. of https://arxiv.org/pdf/1611.08466.pdf</ref>\n\nA [[compact object (mathematics)|compact object]] in the ∞-category of (say right) [[module spectrum|module spectra]] over a [[ring spectrum]] is often called perfect; <ref>http://www.math.harvard.edu/~lurie/281notes/Lecture19-Rings.pdf</ref> see also [[module spectrum]].\n\n== Pseudo-coherent sheaf ==\nWhen the structure sheaf <math>\\mathcal{O}_X</math> is not coherent, working with coherent sheaves has awkwardness (namely the kernel of a finite presentation can fail to be coherent). Because of this, [[Séminaire de Géométrie Algébrique du Bois Marie|SGA 6 Expo I]] introduces the notion of a '''pseudo-coherent sheaf'''.\n\nBy definition, given a [[ringed space]] <math>(X, \\mathcal{O}_X)</math>, an [[sheaf of modules|<math>\\mathcal{O}_X</math>-module]] is called pseudo-coherent if for every integer <math>n \\ge 0</math>, locally, there is a [[free presentation]] of finite type of length ''n''; i.e., \n:<math>L_n \\to L_{n-1} \\to \\cdots \\to L_0 \\to F \\to 0</math>.\n\nA complex ''F'' of <math>\\mathcal{O}_X</math>-modules is called pseudo-coherent if, for every integer ''n'', there is locally a quasi-isomorphism <math>L \\to F</math> where ''L'' has degree bounded above and consists of finite free modules in degree <math>\\ge n</math>. If the complex consists only of the zero-th degree term, then it is pseudo-coherent if and only if it is so as a module.\n\nRoughly speaking, a pseudo-coherent complex may be thought of as a limit of perfect complexes.\n\n== See also ==\n*[[Hilbert–Burch theorem]]\n*[[elliptic complex]] (related notion; discussed at SGA 6 Exposé II, Appendix II.)\n\n== References ==\n{{reflist}}\n* {{Citation|last1=Ben-Zvi|first1=David|last2=Francis|first2=John|last3=Nadler|first3=David|title=Integral transforms and Drinfeld centers in derived algebraic geometry|journal=Journal of the American Mathematical Society|\nvolume=23|year=2010|issue=4|pages=909&ndash;966|mr=2669705|doi=10.1090/S0894-0347-10-00669-7|arxiv=0805.0157}}\n\n*{{cite book\n | editor-last = Berthelot\n | editor-first = Pierre\n | editor-link = Pierre Berthelot (mathematician)\n | editor2=[[Alexandre Grothendieck]]\n | editor3=[[Luc Illusie]]\n | title = Séminaire de Géométrie Algébrique du Bois Marie - 1966-67 - Théorie des intersections et théorème de Riemann-Roch - (SGA 6) (Lecture notes in mathematics '''225''')\n | year = 1971\n | publisher = [[Springer Science+Business Media|Springer-Verlag]]\n | location = Berlin; New York\n | language = French\n | pages = xii+700\n | nopp = true\n |doi=10.1007/BFb0066283\n |isbn= 978-3-540-05647-8 \n | mr = 0354655\n}}\n\n== External links ==\n*http://stacks.math.columbia.edu/tag/0656\n*http://ncatlab.org/nlab/show/perfect+module\n*https://mathoverflow.net/questions/200540/an-alternative-definition-of-pseudo-coherent-complex\n\n{{algebra-stub}}\n\n[[Category:Abstract algebra]]"
    },
    {
      "title": "Poincaré space",
      "url": "https://en.wikipedia.org/wiki/Poincar%C3%A9_space",
      "text": "In [[algebraic topology]], a '''Poincaré space''' is an ''n''-dimensional [[topological space]] with a distinguished element ''µ'' of its ''n''th [[homology group]] such that taking the [[cap product]] with an element of the ''k''th [[cohomology]] group yields an isomorphism to the (''n''&nbsp;&minus;&nbsp;''k'')th homology group.<ref>{{springer\n | title=Poincaré space\n | id=p/p073110\n | last=Rudyak\n | first=Yu.B.\n}}</ref>  The space is essentially one for which [[Poincaré duality]] is valid; more precisely, one whose singular chain complex forms a [[Poincaré complex]] with respect to the distinguished element ''µ''.\n\nFor example, any closed, orientable, connected manifold ''M'' is a Poincaré space, where the distinguished element is the [[fundamental class]] <math>[M].</math>\n\nPoincaré spaces are used in [[surgery theory]] to analyze and classify manifolds. Not every Poincaré space is a manifold, but the difference can be studied, first by having a [[Normal invariant|normal map]] from a manifold, and then via [[obstruction theory]].\n\n==Other uses==\nSometimes,<ref name=\"EGB\">{{Cite journal|title=''Locally Connected Spaces and Generalized Manifolds''|journal = American Journal of Mathematics|volume = 64|issue = 1|pages = 553–574|author= Edward G. Begle|year=1942|jstor = 2371704|doi = 10.2307/2371704}}</ref> ''Poincaré space'' means a [[homology sphere]] with non-trivial [[fundamental group]]&mdash;for instance, the Poincaré dodecahedral space in 3 dimensions.\n\n==See also==\n*[[Stable normal bundle]]\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Poincare space}}\n[[Category:Algebraic topology]]\n[[Category:Abstract algebra]]\n\n\n{{Topology-stub}}"
    },
    {
      "title": "Polarization identity",
      "url": "https://en.wikipedia.org/wiki/Polarization_identity",
      "text": "{{About|quadratic forms|formulas for higher-degree polynomials|Polarization of an algebraic form}}\n{{See also|Parallelogram law|Norm (mathematics)|Inner product space}}\n\n[[File:Parallelogram law.svg|thumb|Vectors involved in the polarization identity.]]\nIn [[mathematics]], the '''polarization identity''' is any one of a family of formulas that express the [[inner product]] of two [[vector (geometry)|vectors]] in terms of the [[Norm (mathematics)|norm]] of a [[normed vector space]]. Let <math>\\|x\\| </math> denote the norm of vector ''x'' and <math>\\langle x, \\ y \\rangle </math> the inner product of vectors ''x'' and ''y''. Then the underlying theorem, attributed to [[Fréchet]], [[John von Neumann|von Neumann]] and [[Pascual Jordan|Jordan]], is stated as:<ref name=Blanchard>\n{{cite book\n |author= [[Philippe Blanchard]], Erwin Brüning\n |chapter= Proposition 14.1.2 (Fréchet–von Neumann–Jordan)\n |url= https://books.google.com/books?id=1g2rikccHcgC&pg=PA192\n |page=192\n |title=Mathematical methods in physics: distributions, Hilbert space operators, and variational methods\n |year=2003\n |publisher=Birkhäuser\n |isbn= 0817642285\n}}</ref><ref name=Teschl>\n{{cite book\n |title=Mathematical methods in quantum mechanics: with applications to Schrödinger operators\n |author=[[Gerald Teschl]]\n |chapter=Theorem 0.19 (Jordan–von Neumann)\n |page=19\n |url=http://www.mat.univie.ac.at/~gerald/ftp/book-schroe/\n |isbn=0-8218-4660-4\n |year=2009\n |publisher=American Mathematical Society Bookstore\n}}</ref>\n: In a normed space (''V'', <math>\\| \\cdot \\|</math>), if the [[parallelogram law]] holds, then there is an inner product on ''V'' such that <math>\\|x\\|^2 = \\langle x,\\ x\\rangle</math> for all <math>x \\in V</math>.\n\n==Formula==\nThe various forms given below are all related by the parallelogram law:\n:<math>\n  2\\|\\textbf{u}\\|^2 + 2\\|\\textbf{v}\\|^2 =\n    \\|\\textbf{u} + \\textbf{v}\\|^2 + \\|\\textbf{u} - \\textbf{v}\\|^2.\n</math>\n\nThe polarization identity can be generalized to various other contexts in [[abstract algebra]], [[linear algebra]], and [[functional analysis]].\n\n===For vector spaces with real scalars===\nIf ''V'' is a real vector space, then the inner product is defined by the polarization identity\n:<math>\\langle x, \\ y \\rangle = \\frac{1}{4} \\left(\\|x + y \\|^2 - \\|x - y\\|^2 \\right)\\ \\forall\\ x, y \\in V \\ .</math>\n\n===For vector spaces with complex scalars===\nIf ''V'' is a complex vector space the inner product is given by the polarization identity\n:<math>\n  \\langle x, \\ y \\rangle =\n    \\frac{1}{4} \\left(\\|x + y \\|^2 - \\|x - y\\|^2 + i\\|x - iy\\|^2 - i\\|x + iy\\|^2\\right)\\ \\forall\\ x, y \\in V ,</math>\n\nwhere <math>i</math> is the [[imaginary unit]]. Note that this defines an inner product which is anti-linear in its first argument and linear in its second argument. For the convention using the reversed definition, one needs to take the complex conjugate:\n:<math>\n  \\langle x, \\ y \\rangle =\n    \\frac{1}{4} \\left(\\|x + y \\|^2 - \\|x - y\\|^2 - i\\|x - iy\\|^2 + i\\|x + iy\\|^2\\right)\\ \\forall\\ x, y \\in V</math>\n\n===Other forms for real vector spaces===\nThe parallelogram law can be used to derive other forms:\n:<math>\\begin{align}\n  \\textbf{u}\\cdot\\textbf{v} &=\n    \\frac{1}{2}\\left(\\|\\textbf{u} + \\textbf{v}\\|^2 - \\|\\textbf{u}\\|^2 - \\|\\textbf{v}\\|^2\\right),\n    & (1) \\\\[0.5em]\n  \\textbf{u}\\cdot\\textbf{v} &=\n    \\frac{1}{2}\\left(\\|\\textbf{u}\\|^2 + \\|\\textbf{v}\\|^2 - \\|\\textbf{u} - \\textbf{v}\\|^2 \\right),\n    & (2) \\\\[0.5em]\n  \\textbf{u}\\cdot\\textbf{v} &= \n    \\frac{1}{4}\\left(\\|\\textbf{u} + \\textbf{v}\\|^2 - \\|\\textbf{u} - \\textbf{v}\\|^2 \\right).\n    & (3)\n\\end{align}</math>\n\n==Application to dot products==\n\n===Relation to the law of cosines===\nThe second form of the polarization identity can be written as\n:<math>\n  \\|\\textbf{u}-\\textbf{v}\\|^2 =\n    \\|\\textbf{u}\\|^2 + \\|\\textbf{v}\\|^2 - 2(\\textbf{u}\\cdot\\textbf{v}).\n</math>\n\nThis is essentially a vector form of the [[law of cosines]] for the [[triangle]] formed by the vectors '''u''', '''v''', and '''u'''&nbsp;&ndash;&nbsp;'''v'''.  In particular,\n:<math>\\textbf{u}\\cdot\\textbf{v} = \\|\\textbf{u}\\|\\,\\|\\textbf{v}\\| \\cos\\theta,</math>\n\nwhere ''&theta;'' is the angle between the vectors '''u''' and '''v'''.\n\n===Derivation===\nThe basic relation between the norm and the dot product is given by the equation\n:<math>\\|\\textbf{v}\\|^2 = \\textbf{v} \\cdot \\textbf{v}.</math>\n\nThen\n:<math>\\begin{align}\n  \\|\\textbf{u} + \\textbf{v}\\|^2\n    &= (\\textbf{u}+\\textbf{v})\\cdot(\\textbf{u} + \\textbf{v}) \\\\[3pt]\n    &= (\\textbf{u}\\cdot\\textbf{u}) + (\\textbf{u}\\cdot\\textbf{v}) + (\\textbf{v}\\cdot\\textbf{u}) + (\\textbf{v}\\cdot\\textbf{v}) \\\\[3pt]\n    &= \\|\\textbf{u}\\|^2 + \\|\\textbf{v}\\|^2 + 2(\\textbf{u}\\cdot\\textbf{v}),\n\\end{align}</math>\n\nand similarly\n:<math>\n  \\|\\textbf{u} - \\textbf{v}\\|^2 =\n    \\|\\textbf{u}\\|^2 + \\|\\textbf{v}\\|^2 - 2(\\textbf{u}\\cdot\\textbf{v}).</math>\n\nForms (1) and (2) of the polarization identity now follow by solving these equations for '''u'''&nbsp;·&nbsp;'''v''', while form (3) follows from subtracting these two equations.  (Adding these two equations together gives the parallelogram law.)\n\n==Generalizations==\n\n===Norms===\nIn [[linear algebra]], the polarization identity applies to any [[norm (mathematics)|norm]] on a [[vector space]] defined in terms of an [[inner product]] by the equation\n:<math>\\|v\\| = \\sqrt{\\langle v, v \\rangle}.</math>\n\nAs noted for the dot product case above, for real vectors ''u'' and ''v'', an angle θ can be introduced using:<ref name= Hildebrand>\n{{cite book\n |title=Methods of applied mathematics\n |url=https://books.google.com/books?id=17EZkWPz_eQC&pg=PA24\n |page=24\n |chapter=Equation 66, the natural definition\n |author=Francis Begnaud Hildebrand\n |isbn=0-486-67002-3\n |year=1992\n |publisher=Courier Dover Publications\n |edition=Reprint of Prentice-Hall 1965 2nd\n}}</ref>\n:<math>\\langle u,\\ v \\rangle = \\|u\\|\\|v\\|\\cos\\theta\\ ;\\ (-\\pi < \\theta \\le \\pi)\\ ,</math>\n\nwhich is acceptable due to the [[Cauchy–Schwarz inequality]]:\n:<math>|\\langle u,\\ v \\rangle | \\le \\|u\\|\\|v\\|\\ .</math>\n\nThis inequality ensures that the magnitude of the above defined cosine ≤ 1. The choice of the cosine function ensures that when <math>\\langle u,\\ v \\rangle = 0 </math> (orthogonal vectors), the angle θ = π/2 or −π/2, where the sign is determined by an orientation on the vector space.\n\nIn this case, the identities become\n:<math>\\begin{align}\n  \\langle u, v \\rangle &= \\frac{1}{2}\\left(\\|u + v\\|^2 - \\|u\\|^2 - \\|v\\|^2    \\right), \\\\[3pt]\n  \\langle u, v \\rangle &= \\frac{1}{2}\\left(\\|u\\|^2     + \\|v\\|^2 - \\|u - v\\|^2\\right), \\\\[3pt]\n  \\langle u, v \\rangle &= \\frac{1}{4}\\left(\\|u + v\\|^2           - \\|u - v\\|^2\\right).\n\\end{align}</math>\n\nConversely, if a norm on a vector space satisfies the parallelogram law, then any one of the above identities can be used to define a compatible inner product.  In functional analysis, introduction of an inner product norm like this often is used to make a [[Banach space]] into a [[Hilbert space]].\n\n===Symmetric bilinear forms===\nThe polarization identities are not restricted to inner products.  If ''B'' is any [[symmetric bilinear form]] on a vector space, and ''Q'' is the [[quadratic form]] defined by\n:<math>Q(v) = B(v, v),</math>\n\nthen\n:<math>\\begin{align}\n  2 B(u, v) &= Q(u + v) - Q(u) - Q(v),     \\\\\n  2 B(u, v) &= Q(u)     + Q(v) - Q(u - v), \\\\\n  4 B(u, v) &= Q(u + v)        - Q(u - v).\n\\end{align}</math>\n\nThe so-called [[homogeneous polynomial|symmetrization map]] generalizes the latter formula, replacing ''Q'' by a homogeneous polynomial of degree ''k'' defined by ''Q''(''v'')&nbsp;=&nbsp;''B''(''v'', …, ''v''), where ''B'' is a symmetric ''k''-linear map.\n\nThe formulas above even apply in the case where the [[field (mathematics)|field]] of [[scalar (mathematics)|scalars]] has [[characteristic (algebra)|characteristic]] two, though the left-hand sides are all zero in this case.  Consequently, in characteristic two there is no formula for a symmetric bilinear form in terms of a quadratic form, and they are in fact distinct notions, a fact which has important consequences in [[L-theory]]; for brevity, in this context \"symmetric bilinear forms\" are often referred to as \"symmetric forms\".\n\nThese formulas also apply to bilinear forms on [[module (mathematics)|modules]] over a [[commutative ring]], though again one can only solve for ''B''(''u'',&nbsp;''v'') if 2 is invertible in the ring, and otherwise these are distinct notions. For example, over the integers, one distinguishes [[integral quadratic form]]s from integral ''symmetric'' forms, which are a narrower notion.\n\nMore generally, in the presence of a ring involution or where 2 is not invertible, one distinguishes [[ε-quadratic form]]s and [[ε-symmetric form]]s; a symmetric form defines a quadratic form, and the polarization identity (without a factor of 2) from a quadratic form to a symmetric form is called the \"symmetrization map\", and is not in general an isomorphism. This has historically been a subtle distinction: over the integers it was not until the 1950s that relation between \"twos out\" (integral ''quadratic'' form) and \"twos in\" (integral ''symmetric'' form) was understood – see discussion at [[integral quadratic form]]; and in the algebraization of [[surgery theory]], Mishchenko originally used ''symmetric'' ''L''-groups, rather than the correct ''quadratic'' ''L''-groups (as in Wall and Ranicki) – see discussion at [[L-theory]].\n\n===Complex numbers===\nIn linear algebra over the [[complex number]]s, it is customary to use a [[sesquilinear form|sesquilinear]] inner product, with the property that <math>\\langle v, u \\rangle</math> is the [[complex conjugate]] of <math>\\langle u, v \\rangle</math>.  In this case the standard polarization identities only give the real part of the inner product:\n:<math>\\begin{align}\n  \\text{Re}\\langle u, v \\rangle &= \\frac{1}{2}\\left(\\|u + v\\|^2 - \\|u\\|^2 - \\|v\\|^2\\right), \\\\[3pt]\n  \\text{Re}\\langle u, v \\rangle &= \\frac{1}{2}\\left(\\|u\\|^2 + \\|v\\|^2 - \\|u - v\\|^2\\right), \\\\[3pt]\n  \\text{Re}\\langle u, v \\rangle &= \\frac{1}{4}\\left(\\|u + v\\|^2       - \\|u - v\\|^2\\right).\n\\end{align}</math>\n\nUsing <math>\\text{Im}\\langle u, v \\rangle =\\text{Re}\\langle u, -iv \\rangle</math> (holds with the convention that the inner product is linear in the second variable), the imaginary part of the inner product can be retrieved as follows:\n:<math>\\begin{align}\n  \\text{Im}\\langle u, v \\rangle &= \\frac{1}{2}\\left(\\|u - iv\\|^2 - \\|u\\|^2 - \\|v\\|^2\\right), \\\\[3pt]\n  \\text{Im}\\langle u, v \\rangle &= \\frac{1}{2}\\left(\\|u\\|^2 + \\|v\\|^2 - \\|u + iv\\|^2\\right), \\\\[3pt]\n  \\text{Im}\\langle u, v \\rangle &= \\frac{1}{4}\\left(\\|u - iv\\|^2 - \\|u + iv\\|^2\\right).\n\\end{align}</math>\n\n===Homogeneous polynomials of higher degree===\nFinally, in any of these contexts these identities may be extended to [[homogeneous polynomial]]s (that is, [[algebraic form]]s) of arbitrary [[Degree of a polynomial|degree]], where it is known as the [[polarization formula]], and is reviewed in greater detail in the article on the [[polarization of an algebraic form]].\n\nThe polarization identity can be stated in the following way:\n:<math>\\langle u, v \\rangle = 4^{-1} \\sum_{k=0}^3 i^k\\left\\|u + i^k v\\right\\|^2.</math>\n\n==Notes and references==\n<references/>\n\n{{Functional Analysis}}\n\n[[Category:Abstract algebra]]\n[[Category:Linear algebra]]\n[[Category:Functional analysis]]\n[[Category:Vectors (mathematics and physics)]]\n[[Category:Norms (mathematics)]]\n[[Category:Mathematical identities]]"
    },
    {
      "title": "Polarization of an algebraic form",
      "url": "https://en.wikipedia.org/wiki/Polarization_of_an_algebraic_form",
      "text": "In [[mathematics]], in particular in [[algebra]], '''polarization''' is a technique for expressing a [[homogeneous polynomial]] in a simpler fashion by adjoining more variables.  Specifically, given a homogeneous polynomial, polarization produces a [[multilinear form]] from which the original polynomial can be recovered by evaluating along a certain diagonal.\n\nAlthough the technique is deceptively simple, it has applications in many areas of abstract mathematics: in particular to [[algebraic geometry]], [[invariant theory]], and [[representation theory]].  Polarization and related techniques form the foundations for [[Weyl's invariant theory]].\n\n==The technique==\nThe fundamental ideas are as follows.  Let ''f''('''u''') be a polynomial in ''n'' variables '''u''' = (''u''<sub>1</sub>, ''u''<sub>2</sub>, ..., ''u''<sub>n</sub>).  Suppose that ''f'' is homogeneous of degree ''d'', which means that\n:''f''(''t'' '''u''') = ''t''<sup>''d''</sup> ''f''('''u''')   for all ''t''.\n\nLet '''u'''<sup>(1)</sup>, '''u'''<sup>(2)</sup>, ..., '''u'''<sup>(d)</sup> be a collection of [[indeterminate (variable)|indeterminate]]s with '''u'''<sup>(i)</sup> = (''u''<sub>1</sub><sup>(i)</sup>, ''u''<sub>2</sub><sup>(i)</sup>, ..., ''u''<sub>n</sub><sup>(i)</sup>), so that there are ''dn'' variables altogether.  The '''polar form''' of ''f'' is a polynomial\n:''F''('''u'''<sup>(1)</sup>, '''u'''<sup>(2)</sup>, ..., '''u'''<sup>(d)</sup>)\nwhich is linear separately in each '''u'''<sup>(i)</sup> (i.e., ''F'' is multilinear), symmetric in the '''u'''<sup>(i)</sup>, and such that\n:''F''('''u''','''u''', ..., '''u''')=''f''('''u''').\n\nThe polar form of ''f'' is given by the following construction\n:<math>F({\\mathbf u}^{(1)},\\dots,{\\mathbf u}^{(d)})=\\frac{1}{d!}\\frac{\\partial}{\\partial\\lambda_1}\\dots\\frac{\\partial}{\\partial\\lambda_d}f(\\lambda_1{\\mathbf u}^{(1)}+\\dots+\\lambda_d{\\mathbf u}^{(d)})|_{\\lambda=0}.</math>\nIn other words, ''F'' is a constant multiple of the coefficient of λ<sub>1</sub> λ<sub>2</sub>...λ<sub>d</sub> in the expansion of ''f''(λ<sub>1</sub>'''u'''<sup>(1)</sup> + ... + λ<sub>d</sub>'''u'''<sup>(d)</sup>).\n\n==Examples==\n*Suppose that '''x'''=(''x'',''y'') and ''f''('''x''') is the [[quadratic form]]\n:<math>f({\\mathbf x}) = x^2 + 3 x y + 2 y^2.</math>\nThen the polarization of ''f'' is a function in '''x'''<sup>(1)</sup> = (''x''<sup>(1)</sup>, ''y''<sup>(1)</sup>) and '''x'''<sup>(2)</sup> = (''x''<sup>(2)</sup>, ''y''<sup>(2)</sup>) given by\n:<math>F({\\mathbf x}^{(1)},{\\mathbf x}^{(2)}) = x^{(1)}x^{(2)}+\\frac{3}{2}x^{(2)}y^{(1)}+\\frac{3}{2}x^{(1)}y^{(2)}+2 y^{(1)}y^{(2)}.</math>\n\n*More generally, if ''f'' is any quadratic form, then the polarization of ''f'' agrees with the conclusion of the [[polarization identity]].\n*'''A cubic example.'''  Let ''f''(''x'',''y'')=''x''<sup>3</sup> + 2''xy''<sup>2</sup>.  Then the polarization of ''f'' is given by\n:<math>F(x^{(1)},y^{(1)},x^{(2)},y^{(2)},x^{(3)},y^{(3)})= x^{(1)}x^{(2)}x^{(3)}+\\frac{2}{3}x^{(1)}y^{(2)}y^{(3)}+\\frac{2}{3}x^{(3)}y^{(1)}y^{(2)}+\\frac{2}{3}x^{(2)}y^{(3)}y^{(1)}.</math>\n\n==Mathematical details and consequences==\nThe polarization of a homogeneous polynomial of degree ''d'' is valid over any [[commutative ring]] in which ''d''! is a unit.  In particular, it holds over any [[Field (mathematics)|field]] of [[characteristic zero]] or whose characteristic is strictly greater than ''d''.\n\n===The polarization isomorphism (by degree)===\nFor simplicity, let ''k'' be a field of characteristic zero and let {{nowrap|1=''A'' = ''k''['''x''']}} be the [[polynomial ring]] in ''n'' variables over ''k''.  Then ''A'' is [[graded ring|graded]] by [[degree of a polynomial|degree]], so that\n:<math>A = \\bigoplus_d A_d.</math>\nThe polarization of algebraic forms then induces an isomorphism of vector spaces in each degree\n:<math>A_d \\cong Sym^d k^n</math>\nwhere ''Sym''<sup>''d''</sup> is the ''d''-th [[symmetric power]] of the ''n''-dimensional space ''k''<sup>''n''</sup>.\n\nThese isomorphisms can be expressed independently of a basis as follows.  If ''V'' is a finite-dimensional vector space and ''A'' is the ring of ''k''-valued polynomial functions on ''V'', graded by homogeneous degree, then polarization yields an isomorphism\n:<math>A_d \\cong Sym^d V^*.</math>\n\n===The algebraic isomorphism===\nFurthermore, the polarization is compatible with the algebraic structure on ''A'', so that\n:<math>A \\cong Sym^\\cdot V^*</math>\nwhere ''Sym''<sup>&sdot;</sup>''V''<sup>∗</sup> is the full [[symmetric algebra]] over ''V''<sup>∗</sup>.\n\n===Remarks===\n* For fields of [[positive characteristic]] ''p'', the foregoing isomorphisms apply if the graded algebras are truncated at degree ''p''-1.\n* There do exist generalizations when ''V'' is an infinite dimensional [[topological vector space]].\n\n==References==\n* [[Claudio Procesi]] (2007) ''Lie Groups: an approach through invariants and representations'', Springer, {{isbn|9780387260402}} .\n\n{{DEFAULTSORT:Polarization Of An Algebraic Form}}\n[[Category:Abstract algebra]]\n[[Category:Homogeneous polynomials]]"
    },
    {
      "title": "Predual",
      "url": "https://en.wikipedia.org/wiki/Predual",
      "text": "In [[mathematics]], the '''predual''' of an object ''D'' is an object ''P'' whose [[dual space]] is ''D''.\n\nFor example, the predual of the space of [[bounded operator]]s is the space of [[trace class]] operators. The predual of the space of  [[differential forms]] is the space of [[chainlets]]. A predual is not always guaranteed to be unique or exist, however.\n\n<!-- other, simpler, interesting examples? -->\n\n[[Category:Abstract algebra]]\n[[Category:Functional analysis]]\n\n{{mathanalysis-stub}}"
    },
    {
      "title": "Principle of distributivity",
      "url": "https://en.wikipedia.org/wiki/Principle_of_distributivity",
      "text": "The '''principle of distributivity''' states that the algebraic [[distributive law]] is [[Validity (logic)|valid]] for [[classical logic]], where both [[logical conjunction]] and [[logical disjunction]] are distributive over each other so that for any [[proposition]]s ''A'', ''B'' and ''C'' the [[Logical equivalence|equivalence]]s\n:<math>A \\land (B \\lor C) \\iff (A \\land B) \\lor (A \\land C)</math>\nand\n:<math>A \\lor (B \\land C) \\iff (A \\lor B) \\land (A \\lor C)</math>\nhold.\n\nThe principle of distributivity is valid in [[classical logic]], but invalid in [[quantum logic]].\n\nThe article \"[[Is Logic Empirical?]]\" discusses the case that quantum logic is the correct, empirical logic, on the grounds that the principle of distributivity is [[consistency|inconsistent]] with a reasonable [[Interpretation of quantum mechanics|interpretation of quantum phenomena]].<ref name=\"Is Logic Empirical?\">{{cite journal | last = Putnam | first = H. | authorlink = Hilary Putnam | year = 1969 | title = Is Logic Empirical? | journal = Boston Studies in the Philosophy of Science | volume = 5}}</ref>\n\n== References ==\n{{reflist}}\n\n[[Category:Abstract algebra]]\n[[Category:Principles]]\n[[Category:Propositional calculus]]\n\n{{mathlogic-stub}}"
    },
    {
      "title": "Radical polynomial",
      "url": "https://en.wikipedia.org/wiki/Radical_polynomial",
      "text": "In [[mathematics]], in the realm of [[abstract algebra]], a '''radical polynomial''' is a multivariate [[polynomial]] over a field that can be expressed as a polynomial in the sum of squares of the variables. That is, if \n\n:<math>k[x_1, x_2,\\ldots, x_n]</math> \n\nis a [[polynomial ring]], the ring of radical polynomials is the subring generated by the polynomial \n\n:<math>\\sum_{i=1}^n x_i^2.</math>\n\nRadical polynomials are characterized as precisely those polynomials that are [[invariant (mathematics)|invariant]] under the action of the [[orthogonal group]].\n\nThe ring of radical polynomials is a [[graded algebra|graded subalgebra]] of the ring of all polynomials.\n\nThe standard [[separation of variables theorem]] asserts that every polynomial can be expressed as a finite sum of terms, each term being a product of a radical polynomial and a [[harmonic polynomial]]. This is equivalent to the statement that the ring of all polynomials is a [[free module]] over the ring of radical polynomials.\n\n==References==\n{{unreferenced|date=June 2008}}\n\n[[Category:Abstract algebra]]\n[[Category:Polynomials]]\n[[Category:Invariant theory]]\n\n\n{{Abstract-algebra-stub}}"
    },
    {
      "title": "Rational series",
      "url": "https://en.wikipedia.org/wiki/Rational_series",
      "text": "In mathematics and computer science, a '''rational series''' is a generalisation of the concept of [[formal power series]] over a [[Ring (mathematics)|ring]] to the case when the basic algebraic structure is no longer a ring but a [[semiring]], and the [[Indeterminate (variable)|indeterminate]]s adjoined are not assumed to [[Commutative property|commute]].  They can be regarded as algebraic expressions of a [[formal language]] over a finite [[Alphabet (computer science)|alphabet]].\n\n==Definition==\nLet ''R'' be a [[semiring]] and ''A'' a finite alphabet.\n\nA ''noncommutative polynomial'' over ''A'' is a finite formal sum of words over ''A''.  They form a semiring <math>R\\langle A \\rangle</math>.\n\nA ''formal series'' is a ''R''-valued function ''c'', on the [[free monoid]] ''A''<sup>*</sup>, which may be written as\n\n:<math>\\sum_{w \\in A^*} c(w) w \\ . </math>\n\nThe set of formal series is denoted <math>R\\langle\\langle A \\rangle\\rangle</math> and becomes a semiring under the operations\n\n:<math>c+d : w \\mapsto c(w) + d(w) </math>\n:<math>c\\cdot d : w \\mapsto \\sum_{uv = w} c(u) \\cdot d(v) \\ . </math>\n\nA non-commutative polynomial thus corresponds to a function ''c'' on ''A''<sup>*</sup> of finite support.\n\nIn the case when ''R'' is a ring, then this is the ''Magnus ring'' over ''R''.<ref>{{cite book | first=Helmut | last=Koch | title=Algebraic Number Theory | publisher=[[Springer-Verlag]] | year=1997 | isbn=3-540-63003-1 | zbl=0819.11044 | series=Encycl. Math. Sci. | volume=62 | edition=2nd printing of 1st | page=167 }}</ref>\n\nIf ''L'' is a language over ''A'', regarded as a subset of ''A''<sup>*</sup> we can form the ''characteristic series'' of ''L'' as the formal series\n\n:<math>\\sum_{w \\in L} w </math>\n\ncorresponding to the [[Indicator function |characteristic function]] of ''L''.\n\nIn <math>R\\langle\\langle A \\rangle\\rangle</math> one can define an operation of [[Kleene star|iteration]] expressed as\n\n:<math> S^* = \\sum_{n \\ge 0} S^n </math>\n\nand formalised as\n\n:<math>c^*(w) = \\sum_{u_1 u_2 \\cdots u_n = w} c(u_1)c(u_2) \\cdots c(u_n) \\ . </math>\n\nThe ''rational operations'' are the addition and multiplication of formal series, together with iteration.\nA '''rational series''' is a formal series obtained by rational operations from <math>R\\langle A \\rangle</math>.\n\n==See also==\n* [[Formal power series]]\n* [[Rational language]] \n* [[Rational set]]\n* [[Hahn series]] (Malcev–Neumann series)\n* [[Weighted automaton]]\n\n==References==\n{{reflist}}\n* {{cite book | last1=Berstel | first1=Jean | last2=Reutenauer | first2=Christophe | title=Noncommutative rational series with applications | series=Encyclopedia of Mathematics and Its Applications | volume=137 | location=Cambridge | publisher=[[Cambridge University Press]] | year=2011 | isbn=978-0-521-19022-0 | zbl=1250.68007 }}\n\n== Further reading ==\n* {{cite book | last=Sakarovitch | first=Jacques | title=Elements of automata theory | others=Translated from the French by Reuben Thomas | location=Cambridge | publisher=[[Cambridge University Press]] | year=2009 | isbn=978-0-521-84425-3 | zbl=1188.68177 | at=Part IV (where they are called <math>\\mathbb{K}</math>-rational series)}}\n* Droste, M., & Kuich, W. (2009). Semirings and Formal Power Series. ''Handbook of Weighted Automata'', 3–28. {{doi|10.1007/978-3-642-01492-5_1}}\n* Sakarovitch, J. Rational and Recognisable Power Series. ''Handbook of Weighted Automata'', 105–174 (2009). {{doi|10.1007/978-3-642-01492-5_4}}\n* W. Kuich. Semirings and formal power series: Their relevance to formal languages and automata theory. In G. Rozenberg and A. Salomaa, editors, Handbook of Formal Languages, volume 1, Chapter 9, pages 609–677. Springer, Berlin, 1997\n\n[[Category:Abstract algebra]]\n[[Category:Formal languages]]\n[[Category:Mathematical series]]\n\n{{algebra-stub}}"
    },
    {
      "title": "Rayleigh's quotient in vibrations analysis",
      "url": "https://en.wikipedia.org/wiki/Rayleigh%27s_quotient_in_vibrations_analysis",
      "text": "{{Multiple issues|\n{{Orphan|date=January 2017}}\n{{cleanup|date=February 2017|reason=Needs linked into Wikipedia}}\n{{more footnotes|date=February 2017}}\n{{refimprove|date=February 2017}}\n}}\nThe [[Rayleigh's quotient]] represents a quick method to estimate the natural [[frequency]] of a multi-degree-of-freedom vibration system, in which the [[mass]] and the stiffness matrices are known.\n\nThe [[eigenvalue]] problem for a general system of the form\n\n: <math>M\\,\\ddot{\\textbf{q}}(t) + C\\,\\dot{\\textbf{q}}(t) + K\\,\\textbf{q}(t) = \\textbf{Q}(t)</math>\n\nin absence of damping and external forces reduces to\n\n: <math> M\\,\\ddot{\\textbf{q}}(t) + K\\,\\textbf{q}(t) = 0 </math>\n\nThe previous equation can be written also as\n\n: <math>K\\,\\textbf{u} = \\lambda\\,M\\,\\textbf{u}</math>\n\nwhere <math> \\lambda=\\omega^2 </math>, in which <math> \\omega </math> represents the natural frequency, M and K are the real positive symmetric mass and stiffness matrices respectively.\n\nFor an ''n''-degree-of-freedom system the equation has ''n'' solutions <math> \\lambda_m </math>, <math> \\textbf{u}_m </math> that satisfy the equation\n\n: <math> K\\,\\textbf{u}_m = \\lambda_m\\,M\\,\\textbf{u}_m </math>\n\nBy multiplying both sides of the equation  by <math> \\textbf{u}_{m}^{T} </math> and dividing by the scalar <math> \\textbf{u}_m^T\\,M\\,\\textbf{u}_m </math>, it is possible to express the eigenvalue problem as follow:\n\n: <math> \\lambda_m = \\omega_m^2 = \\frac{\\textbf{u}_m^T\\,K\\,\\textbf{u}_m}{\\textbf{u}_m^T\\,M\\,\\textbf{u}_m} </math>\nfor ''m'' = 1,2,3,...,''n''.\n\nIn the previous equation it is also possible to observe that the numerator is proportional to the potential energy while the denominator depicts a measure of the kinetic energy. Moreover, the equation allow us to calculate the natural frequency only if the eigenvector (as well as any other displacement vector) <math> \\textbf{u}_{m} </math> is known. For academic interests, if the modal vectors are not known, we can repeat the foregoing process but with <math> \\lambda = \\omega^2 </math> and <math> \\textbf{u} </math> taking the place of <math> \\lambda_{m} = \\omega_{m}^2 </math> and <math> \\textbf{u}_{m} </math>, respectively. By doing so we obtain the scalar <math> R(\\textbf{u}) </math>, also known as Rayleigh's quotient:\n\n<ref>{{cite book|last1=Meirovitch|first1=Leonard|title=Fundamentals of Vibration|date=2003|publisher=McGraw-Hill Education|isbn=9780071219839|pages=806}}</ref>\n\n: <math>R(\\textbf{u}) = \\lambda = \\omega^2 =  \\frac{\\textbf{u}^T\\,K\\,\\textbf{u}}{\\textbf{u}^T\\,M\\,\\textbf{u}}</math>\n\nTherefore, the Rayleigh's quotient is a scalar whose value depends on the vector <math> \\textbf{u} </math> and it can be calculated with good approximation for any arbitrary vector <math> \\textbf{u} </math> as long as it lays reasonably far from the modal vectors <math> \\textbf{u}_{i} </math>, ''i'' = 1,2,3,...,''n''.\n\nSince, is it possible to state that the vector <math> \\textbf{u} </math> differs from the modal vector <math> \\textbf{u}_m </math> by a small quantity of first order, the correct result of the Rayleigh's quotient will differ not sensitively from the estimated one and that's what makes this method very useful. A good way to estimate the lowest modal vector <math> (u_1) </math>, that generally works well for most structures (even though is not guaranteed), is to assume <math> (u_1) </math> equal to the static displacement from an applied force that has the same relative distribution of the diagonal mass matrix terms. The latter can be elucidated by the following 3-DOF example.\n\n== Example – 3DOF ==\n\nAs an example, we can consider a 3-degree-of-freedom system in which the mass and the stiffness matrices of them are known as follows:\n\n:<math>M = \n  \\begin{pmatrix}\n    1 & 0 & 0 \\\\\n    0 & 1 & 0 \\\\\n    0 & 0 & 3\n  \\end{pmatrix}\n\\; \\;\nK = \n  \\begin{pmatrix}\n    3 & -1 & 0 \\\\\n    -1 & 3 & -2 \\\\\n    0 & -2 & 2\n  \\end{pmatrix}\n</math>\n\nTo get an estimation of the lowest natural frequency we choose a trial vector of static displacement obtained by loading the system with a force proportional to the masses:\n\n: <math>\\textbf{F} = k[m_1\\, m_2\\, m_3]^T = 1[1\\, 1\\, 3]</math> \n\nThus, the trial vector will become\n\n: <math>\\textbf{u} = K^{-1}\\textbf{F} = [2.5;6.5;8]</math>\n\nthat allow us to calculate the Rayleigh's quotient:\n\n<math>R = \\frac{\\textbf{u}^T\\,K\\,\\textbf{u}}{\\textbf{u}^T\\,M\\,\\textbf{u}} = \\cdots = 0.137214</math>\n\nThus, the lowest natural frequency, calculated by means of Rayleigh's quotient is:\n\n: <math>w_\\text{Ray} = 0.370424</math>\n\nUsing a calculation tool is pretty fast to verify how much it differs from the \"real\" one. In this case, using MATLAB, it has been calculated that the lowest natural frequency is: <math>w_\\text{real} = 0.369308</math> that has led to an error of <math>0.302315 \\%</math> using the Rayleigh's approximation, that is a remarkable result.\n\nThe example shows how the Rayleigh's quotient is capable of getting an accurate estimation of the lowest natural frequency. The practice of using the static displacement vector as a trial vector is valid as the static displacement vector tends to resemble the lowest vibration mode.\n\n==References==\n{{reflist}}\n\n[[Category:Abstract algebra]]\n[[Category:Linear algebra]]\n[[Category:Mathematical physics]]\n[[Category:Mechanical vibrations]]"
    },
    {
      "title": "Row and column spaces",
      "url": "https://en.wikipedia.org/wiki/Row_and_column_spaces",
      "text": "[[File:Matrix Rows.svg|thumb|right|The row vectors of a [[matrix (mathematics)|matrix]]. The row space of this matrix is the vector space generated by linear combinations of the row vectors.]]\n\n[[Image:Matrix Columns.svg|thumb|right|The column vectors of a [[matrix (mathematics)|matrix]]. The column space of this matrix is the vector space generated by linear combinations of the column vectors.]]\n\nIn [[linear algebra]], the '''column space''' (also called the '''range''' or [[Image (mathematics)|'''image''']]) of a [[matrix (mathematics)|matrix]] ''A'' is the [[Linear span|span]] (set of all possible [[linear combination]]s) of its [[column vector]]s. The column space of a matrix is the [[image (mathematics)|image]] or [[range (mathematics)|range]] of the corresponding [[matrix transformation]].\n\nLet <math>\\mathbb{F}</math> be a [[field (mathematics)|field]]. The column space of an ''m''&#8239;&times;&#8239;''n'' matrix with components from <math>\\mathbb{F}</math> is a [[linear subspace]] of the [[Examples of vector spaces#Coordinate space|''m''-space]] <math>\\mathbb{F}^m</math>.  The [[dimension (linear algebra)|dimension]] of the column space is called the [[rank (linear algebra)|rank]] of the matrix and is at most min(''m'',&#8239;''n'').<ref name=\"ReferenceA\">Linear algebra, as discussed in this article, is a very well established mathematical discipline for which there are many sources. Almost all of the material in this article can be found in Lay 2005, Meyer 2001, and Strang 2005.</ref> A definition for matrices over a [[ring (mathematics)|ring]] <math>\\mathbb{K}</math> [[#For matrices over a ring|is also possible]].\n\nThe '''row space''' is defined similarly.\n\nThis article considers matrices of [[real number]]s.  The row and column spaces are subspaces of the [[real coordinate space|real spaces]] '''R'''<sup>''n''</sup> and '''R'''<sup>''m''</sup> respectively.<ref>{{harvtxt|Anton|1987|p=179}}</ref>\n\n==Overview==\nLet ''A'' be an ''m''-by-''n'' matrix. Then\n# rank(''A'') = dim(rowsp(''A'')) = dim(colsp(''A'')),<ref>{{harvtxt|Anton|1987|p=183}}</ref>\n# rank(''A'') = number of [[Pivot element|pivots]] in any echelon form of ''A'',\n# rank(''A'') = the maximum number of linearly independent rows or columns of ''A''.<ref>{{harvtxt|Beauregard|Fraleigh|1973|p=254}}</ref>\n\nIf one considers the matrix as a [[linear transformation]] from '''R'''<sup>''n''</sup> to '''R'''<sup>''m''</sup>, then the column space of the matrix equals the [[image (mathematics)|image]] of this linear transformation.\n\nThe column space of a matrix ''A'' is the set of all linear combinations of the columns in ''A''.  If ''A'' = ['''a'''<sub>1</sub>, ...., '''a'''<sub>''n''</sub>], then colsp(''A'') = span {'''a'''<sub>1</sub>, ...., '''a'''<sub>''n''</sub>}.\n\nThe concept of row space generalizes to matrices over '''C''', the field of [[complex number]]s, or over any [[field (mathematics)|field]].\n\nIntuitively, given a matrix ''A'', the action of the matrix ''A'' on a vector '''x''' will return a linear combination of the columns of ''A'' weighted by the coordinates of '''x''' as coefficients. Another way to look at this is that it will (1) first project '''x''' into the row space of ''A'', (2) perform an invertible transformation, and (3) place the resulting vector '''y''' in the column space of ''A''.  Thus the result '''y''' = ''A'' '''x''' must reside in the column space of ''A''. See [[singular value decomposition]] for more details on this second interpretation.{{ clarify | date = November 2015 | reason = This perspective is not covered on the SVD page. }}\n\n===Example===\nGiven a matrix ''J'':\n:<math>\n  J =\n  \\begin{bmatrix}\n    2 & 4 & 1 & 3 & 2\\\\\n    -1 & -2 & 1 & 0 & 5\\\\\n    1 & 6 & 2 & 2 & 2\\\\\n    3 & 6 & 2 & 5 & 1\n  \\end{bmatrix}\n</math>\nthe rows are\n'''r'''<sub>1</sub> = (2,4,1,3,2),\n'''r'''<sub>2</sub> = (−1,−2,1,0,5),\n'''r'''<sub>3</sub> = (1,6,2,2,2),\n'''r'''<sub>4</sub> = (3,6,2,5,1).\nConsequently, the row space of ''J'' is the subspace of '''R'''<sup>5</sup> [[linear span|spanned]] by { '''r'''<sub>1</sub>, '''r'''<sub>2</sub>, '''r'''<sub>3</sub>, '''r'''<sub>4</sub> }.   \nSince these four row vectors are [[Linear independence|linearly independent]], the row space is 4-dimensional.  Moreover, in this case it can be seen that they are all [[orthogonality|orthogonal]] to the vector '''n''' = (6,−1,4,−4,0), so it can be deduced that the row space consists of all vectors in '''R'''<sup>5</sup> that are orthogonal to '''n'''.\n\n==Column space==\n\n===Definition===\n\nLet ''K'' be a [[field (mathematics)|field]] of [[scalar (mathematics)|scalars]]. Let ''A'' be an ''m''&#8239;&times;&#8239;''n'' matrix, with column vectors '''v'''<sub>1</sub>,&nbsp;'''v'''<sub>2</sub>,&nbsp;...,&nbsp;'''v'''<sub>''n''</sub>.  A [[linear combination]] of these vectors is any vector of the form\n:<math>c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_n \\mathbf{v}_n,</math>\nwhere ''c''<sub>1</sub>,&nbsp;''c''<sub>2</sub>,&nbsp;...,&nbsp;''c<sub>n</sub>'' are scalars.  The set of all possible linear combinations of '''v'''<sub>1</sub>,&#8239;...&#8239;,'''v'''<sub>''n''</sub> is called the '''column space''' of ''A''.  That is, the column space of ''A'' is the [[linear span|span]] of the vectors '''v'''<sub>1</sub>,&#8239;...&#8239;,&#8239;'''v'''<sub>''n''</sub>.\n\nAny linear combination of the column vectors of a matrix ''A'' can be written as the product of ''A'' with a column vector:\n:<math>\\begin{array} {rcl}\nA \\begin{bmatrix} c_1 \\\\ \\vdots \\\\ c_n \\end{bmatrix} \n& = & \\begin{bmatrix} a_{11} & \\cdots & a_{1n} \\\\ \\vdots & \\ddots & \\vdots \\\\ a_{m1} & \\cdots & a_{mn} \\end{bmatrix} \\begin{bmatrix} c_1 \\\\ \\vdots \\\\ c_n \\end{bmatrix}\n= \\begin{bmatrix} c_1 a_{11} + & \\cdots & + c_{n} a_{1n} \\\\ \\vdots & \\vdots & \\vdots \\\\ c_{1} a_{m1} + & \\cdots & + c_{n} a_{mn} \\end{bmatrix} = c_1 \\begin{bmatrix} a_{11} \\\\ \\vdots \\\\ a_{m1} \\end{bmatrix} + \\cdots + c_n \\begin{bmatrix} a_{1n} \\\\ \\vdots \\\\ a_{mn} \\end{bmatrix} \\\\\n& = & c_1 \\mathbf{v}_1 + \\cdots + c_n \\mathbf{v}_n\n\\end{array}</math>\n\nTherefore, the column space of ''A'' consists of all possible products ''A'''''x''', for '''x'''&nbsp;&isin;&nbsp;'''C'''<sup>''n''</sup>.  This is the same as the [[image (mathematics)|image]] (or [[range (mathematics)|range]]) of the corresponding [[matrix transformation]].\n\n;Example\n:If <math>A = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 2 & 0 \\end{bmatrix}</math>, then the column vectors are '''v'''<sub>1</sub>&nbsp;=&nbsp;(1,&nbsp;0,&nbsp;2)<sup>T</sup> and '''v'''<sub>2</sub>&nbsp;=&nbsp;(0,&nbsp;1,&nbsp;0)<sup>T</sup>.\n:A linear combination of '''v'''<sub>1</sub> and '''v'''<sub>2</sub> is any vector of the form\n::<math>c_1 \\begin{bmatrix} 1 \\\\ 0 \\\\ 2 \\end{bmatrix} + c_2 \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} c_1 \\\\ c_2 \\\\ 2c_1 \\end{bmatrix}\\,</math>\n:The set of all such vectors is the column space of ''A''.  In this case, the column space is precisely the set of vectors (''x'',&nbsp;''y'',&nbsp;''z'')&nbsp;&isin;&nbsp;'''R'''<sup>3</sup> satisfying the equation ''z''&nbsp;=&nbsp;2''x'' (using [[Cartesian coordinates]], this set is a [[plane (mathematics)|plane]] through the origin in [[three-dimensional space]]).\n\n===Basis===\nThe columns of ''A'' span the column space, but they may not form a [[basis (linear algebra)|basis]] if the column vectors are not [[linearly independent]].  Fortunately, [[elementary row operations]] do not affect the dependence relations between the column vectors.  This makes it possible to use [[row reduction]] to find a [[basis (linear algebra)|basis]] for the column space.\n\nFor example, consider the matrix\n:<math>A = \\begin{bmatrix} 1 & 3 & 1 & 4 \\\\ 2 & 7 & 3 & 9 \\\\ 1 & 5 & 3 & 1 \\\\ 1 & 2 & 0 & 8 \\end{bmatrix}\\text{.}</math>\nThe columns of this matrix span the column space, but they may not be [[linearly independent]], in which case some subset of them will form a basis.  To find this basis, we reduce ''A'' to [[reduced row echelon form]]:\n:<math>\\begin{bmatrix} 1 & 3 & 1 & 4 \\\\ 2 & 7 & 3 & 9 \\\\ 1 & 5 & 3 & 1 \\\\ 1 & 2 & 0 & 8 \\end{bmatrix}\n\\sim \\begin{bmatrix} 1 & 3 & 1 & 4 \\\\ 0 & 1 & 1 & 1 \\\\ 0 & 2 & 2 & -3 \\\\ 0 & -1 & -1 & 4 \\end{bmatrix}\n\\sim  \\begin{bmatrix} 1 & 0 & -2 & 1 \\\\ 0 & 1 & 1 & 1 \\\\ 0 & 0 & 0 & -5 \\\\ 0 & 0 & 0 & 5 \\end{bmatrix}\n\\sim  \\begin{bmatrix} 1 & 0 & -2 & 0 \\\\ 0 & 1 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix}\\text{.}</math><ref>This computation uses the [[Gaussian elimination|Gauss–Jordan]] row-reduction algorithm.  Each of the shown steps involves multiple elementary row operations.</ref>\nAt this point, it is clear that the first, second, and fourth columns are linearly independent, while the third column is a linear combination of the first two.  (Specifically, '''v'''<sub>3</sub>&nbsp;=&nbsp;&ndash;2'''v'''<sub>1</sub>&nbsp;+&nbsp;'''v'''<sub>2</sub>.)  Therefore, the first, second, and fourth columns of the original matrix are a basis for the column space:\n:<math>\\begin{bmatrix} 1 \\\\ 2 \\\\ 1 \\\\ 1\\end{bmatrix},\\;\\;\n\\begin{bmatrix} 3 \\\\ 7 \\\\ 5 \\\\ 2\\end{bmatrix},\\;\\;\n\\begin{bmatrix} 4 \\\\ 9 \\\\ 1 \\\\ 8\\end{bmatrix}\\text{.}</math>\nNote that the independent columns of the reduced row echelon form are precisely the columns with [[Pivot element|pivots]].  This makes it possible to determine which columns are linearly independent by reducing only to [[row echelon form|echelon form]].\n\nThe above algorithm can be used in general to find the dependence relations between any set of vectors, and to pick out a basis from any spanning set.  A different algorithm for finding a basis from a spanning set is given in the [[row space]] article;  finding a basis for the column space of ''A'' is equivalent to finding a basis for the row space of the [[transpose]] matrix&nbsp;''A''<sup>T</sup>.\n\nTo find the basis in a practical setting (e.g., for large matrices), the [[singular-value decomposition]] is typically used.\n\n===Dimension===\n{{main|Rank (linear algebra)}}\nThe [[dimension (linear algebra)|dimension]] of the column space is called the '''[[rank (linear algebra)|rank]]''' of the matrix.  The rank is equal to the number of pivots in the [[reduced row echelon form]], and is the maximum number of linearly independent columns that can be chosen from the matrix.  For example, the 4&nbsp;&times;&nbsp;4 matrix in the example above has rank three.\n\nBecause the column space is the [[image (mathematics)|image]] of the corresponding [[matrix transformation]], the rank of a matrix is the same as the dimension of the image.  For example, the transformation '''R'''<sup>4</sup>&nbsp;&rarr;&nbsp;'''R'''<sup>4</sup> described by the matrix above maps all of '''R'''<sup>4</sup> to some three-dimensional [[Euclidean subspace|subspace]].\n\nThe '''nullity''' of a matrix is the dimension of the [[kernel (matrix)|null space]], and is equal to the number of columns in the reduced row echelon form that do not have pivots.<ref>Columns without pivots represent free variables in the associated homogeneous [[system of linear equations]].</ref>  The rank and nullity of a matrix ''A'' with ''n'' columns are related by the equation:\n:<math>\\text{rank}(A) + \\text{nullity}(A) = n.\\,</math>\nThis is known as the [[rank–nullity theorem]].\n\n===Relation to the left null space===\nThe [[left null space]] of ''A'' is the set of all vectors '''x''' such that '''x'''<sup>T</sup>''A''&nbsp;=&nbsp;'''0'''<sup>T</sup>.  It is the same as the [[kernel (matrix)|null space]] of the [[transpose]] of ''A''. The product of the matrix ''A''<sup>T</sup> and the vector '''x''' can be written in terms of the [[dot product]] of vectors:\n:<math>A^\\mathsf{T}\\mathbf{x} = \\begin{bmatrix} \\mathbf{v}_1 \\cdot \\mathbf{x} \\\\ \\mathbf{v}_2 \\cdot \\mathbf{x} \\\\ \\vdots \\\\ \\mathbf{v}_n \\cdot \\mathbf{x} \\end{bmatrix},</math>\nbecause [[row vector]]s of ''A''<sup>T</sup> are transposes of column vectors '''v'''<sub>''k''</sub> of ''A''.  Thus ''A''<sup>T</sup>'''x'''&nbsp;=&nbsp;'''0''' if and only if '''x''' is [[orthogonal]] (perpendicular) to each of the column vectors of ''A''.\n\nIt follows that the left null space (the null space of ''A''<sup>T</sup>) is the [[orthogonal complement]] to the column space of A.\n\nFor a matrix ''A'', the column space, row space, null space, and left null space are sometimes referred to as the [[four fundamental subspaces]].\n\n===For matrices over a ring===\nSimilarly the column space (sometimes disambiguated as ''right'' column space) can be defined for matrices over a [[ring (mathematics)|ring]] ''K'' as\n:<math>\\sum\\limits_{k=1}^n \\mathbf{v}_k c_k</math>\nfor any ''c''<sub>1</sub>,&nbsp;...,&nbsp;''c<sub>n</sub>'', with replacement of the vector ''m''-space with \"[[left and right (algebra)|right]] [[free module]]\", which changes the order of [[scalar multiplication]] of the vector '''v'''<sub>''k''</sub> to the scalar ''c<sub>k</sub>'' such that it is written in an unusual order ''vector''–''scalar''.<ref>Important only if ''K'' is not [[commutative ring|commutative]]. Actually, this form is merely a [[matrix multiplication|product]] ''A'''''c''' of the matrix ''A'' to the column vector '''c''' from ''K''<sup>''n''</sup> where the order of factors is ''preserved'', unlike [[#Definition|the formula above]].</ref>\n\n==Row space==\n\n===Definition===\nLet ''K'' be a [[field (mathematics)|field]] of [[scalar (mathematics)|scalars]]. Let ''A'' be an ''m''&#8239;&times;&#8239;''n'' matrix, with row vectors '''r'''<sub>1</sub>,&#8239;'''r'''<sub>2</sub>,&#8239;...&#8239;,&#8239;'''r'''<sub>''m''</sub>.  A [[linear combination]] of these vectors is any vector of the form\n:<math>c_1 \\mathbf{r}_1 + c_2 \\mathbf{r}_2 + \\cdots + c_m \\mathbf{r}_m,</math>\nwhere ''c''<sub>1</sub>,&#8239;''c''<sub>2</sub>,&#8239;...&#8239;,&#8239;''c<sub>m</sub>'' are scalars.  The set of all possible linear combinations of '''r'''<sub>1</sub>,&#8239;...&#8239;,&#8239;'''r'''<sub>''m''</sub> is called the '''row space''' of ''A''.  That is, the row space of ''A'' is the [[linear span|span]] of the vectors '''r'''<sub>1</sub>,&#8239;...&#8239;,&#8239;'''r'''<sub>''m''</sub>.\n\nFor example, if\n:<math>A = \\begin{bmatrix} 1 & 0 & 2 \\\\ 0 & 1 & 0 \\end{bmatrix},</math>\nthen the row vectors are '''r'''<sub>1</sub>&nbsp;=&nbsp;(1,&#8239;0,&#8239;2) and '''r'''<sub>2</sub>&nbsp;=&nbsp;(0,&#8239;1,&#8239;0).  A linear combination of '''r'''<sub>1</sub> and '''r'''<sub>2</sub> is any vector of the form\n:<math>c_1 (1,0,2) + c_2 (0,1,0) = (c_1,c_2,2c_1).\\,</math>\nThe set of all such vectors is the row space of ''A''.  In this case, the row space is precisely the set of vectors (''x'',&#8239;''y'',&#8239;''z'')&nbsp;∈&nbsp;''K''<sup>3</sup> satisfying the equation ''z''&nbsp;=&nbsp;2''x'' (using [[Cartesian coordinates]], this set is a [[plane (mathematics)|plane]] through the origin in [[three-dimensional space]]).\n\nFor a matrix that represents a homogeneous [[system of linear equations]], the row space consists of all linear equations that follow from those in the system.\n\nThe column space of ''A'' is equal to the row space of ''A''<sup>T</sup>.\n\n===Basis===\nThe row space is not affected by [[elementary row operations]].  This makes it possible to use [[row reduction]] to find a [[basis (linear algebra)|basis]] for the row space.\n\nFor example, consider the matrix\n:<math>A = \\begin{bmatrix} 1 & 3 & 2 \\\\ 2 & 7 & 4 \\\\ 1 & 5 & 2\\end{bmatrix}.</math>\nThe rows of this matrix span the row space, but they may not be [[linearly independent]], in which case the rows will not be a basis.  To find a basis, we reduce ''A'' to [[row echelon form]]:\n\n'''r<sub>1</sub>''', '''r<sub>2</sub>''', '''r<sub>3</sub>''' represents the rows.\n:<math>\n\\begin{bmatrix} 1 & 3 & 2 \\\\ 2 & 7 & 4 \\\\ 1 & 5 & 2\\end{bmatrix} \\underbrace{\\sim}_{r_2-2r_1}\n\\begin{bmatrix} 1 & 3 & 2 \\\\ 0 & 1 & 0 \\\\ 1 & 5 & 2\\end{bmatrix} \\underbrace{\\sim}_{r_3-r_1}\n\\begin{bmatrix} 1 & 3 & 2 \\\\ 0 & 1 & 0 \\\\ 0 & 2 & 0\\end{bmatrix} \\underbrace{\\sim}_{r_3-2r_2}\n\\begin{bmatrix} 1 & 3 & 2 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0\\end{bmatrix} \\underbrace{\\sim}_{r_1-3r_2}\n\\begin{bmatrix} 1 & 0 & 2 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0\\end{bmatrix}.\n</math>\nOnce the matrix is in echelon form, the nonzero rows are a basis for the row space.  In this case, the basis is {&nbsp;(1,&#8239;3,&#8239;2),&nbsp;(0,&#8239;1,&#8239;0)&nbsp;}. Another possible basis {&nbsp;(1,&#8239;0,&#8239;2),&nbsp;(0,&#8239;1,&#8239;0)&nbsp;} comes from a further reduction.<ref name=\"example\">The example is valid over the [[real number]]s, the [[rational number]]s, and other [[number field]]s. It is not necessarily correct over fields and rings with non-zero [[characteristic (algebra)|characteristic]].</ref>\n\nThis algorithm can be used in general to find a basis for the span of a set of vectors.  If the matrix is further simplified to [[reduced row echelon form]], then the resulting basis is uniquely determined by the row space.\n\nIt is sometimes convenient to find a basis for the row space from among the rows of the original matrix instead (for example, this result is useful in giving an elementary proof that the [[Rank (linear algebra)#Alternative definitions|determinantal rank]] of a matrix is equal to its rank). Since row operations can affect linear dependence relations of the row vectors, such a basis is instead found indirectly using the fact that the column space of ''A''<sup>T</sup> is equal to the row space of ''A''. Using the example matrix ''A'' above, find ''A''<sup>T</sup> and reduce it to row echelon form:\n\n:<math>\nA^T = \\begin{bmatrix} 1 & 2 & 1 \\\\ 3 & 7 & 5 \\\\ 2 & 4 & 2\\end{bmatrix} \\sim \n\\begin{bmatrix} 1 & 2 & 1 \\\\ 0 & 1 & 2 \\\\ 0 & 0 & 0\\end{bmatrix}. \n</math>\n\nThe pivots indicate that the first two columns of ''A''<sup>T</sup> form a basis of the column space of  ''A''<sup>T</sup>. Therefore, the first two rows of ''A'' (before any row reductions) also form a basis of the row space of ''A''.\n\n===Dimension===\n{{main|Rank (linear algebra)}}\nThe [[dimension (linear algebra)|dimension]] of the row space is called the '''[[rank (linear algebra)|rank]]''' of the matrix.  This is the same as the maximum number of linearly independent rows that can be chosen from the matrix, or equivalently the number of pivots.  For example, the 3&#8239;&times;&#8239;3 matrix in the example above has rank two.<ref name=\"example\"/>\n\nThe rank of a matrix is also equal to the dimension of the [[column space]].  The dimension of the [[null space]] is called the '''nullity''' of the matrix, and is related to the rank by the following equation:\n:<math>\\operatorname{rank}(A) + \\operatorname{nullity}(A) = n,</math>\nwhere ''n'' is the number of columns of the matrix ''A''.  The equation above is known as the [[rank–nullity theorem]].\n\n===Relation to the null space===\nThe [[null space]] of matrix ''A'' is the set of all vectors '''x''' for which ''A'''''x'''&nbsp;=&nbsp;'''0'''.  The product of the matrix ''A'' and the vector '''x''' can be written in terms of the [[dot product]] of vectors:\n:<math>A\\mathbf{x} = \\begin{bmatrix} \\mathbf{r}_1 \\cdot \\mathbf{x} \\\\ \\mathbf{r}_2 \\cdot \\mathbf{x} \\\\ \\vdots \\\\ \\mathbf{r}_m \\cdot \\mathbf{x} \\end{bmatrix},</math>\nwhere '''r'''<sub>1</sub>,&#8239;...&#8239;,&#8239;'''r'''<sub>''m''</sub> are the row vectors of ''A''.  Thus ''A'''''x'''&nbsp;=&nbsp;'''0''' if and only if '''x''' is [[orthogonal]] (perpendicular) to each of the row vectors of ''A''.\n\nIt follows that the null space of ''A'' is the [[orthogonal complement]] to the row space.  For example, if the row space is a plane through the origin in three dimensions, then the null space will be the perpendicular line through the origin.  This provides a proof of the [[rank–nullity theorem]] (see [[#Dimension|dimension]] above).\n\nThe row space and null space are two of the [[four fundamental subspaces]] associated with a matrix ''A'' (the other two being the [[column space]] and [[left null space]]).\n\n===Relation to coimage===\nIf ''V'' and ''W'' are [[vector spaces]], then the [[kernel (linear algebra)|kernel]] of a [[linear transformation]] ''T'':&nbsp;''V''&nbsp;→&nbsp;''W'' is the set of vectors '''v'''&nbsp;∈&nbsp;''V'' for which ''T''('''v''')&nbsp;=&nbsp;'''0'''.  The kernel of a linear transformation is analogous to the null space of a matrix.\n\nIf ''V'' is an [[inner product space]], then the orthogonal complement to the kernel can be thought of as a generalization of the row space.  This is sometimes called the [[coimage]] of ''T''.  The transformation ''T'' is one-to-one on its coimage, and the coimage maps [[isomorphism|isomorphically]] onto the [[image (mathematics)|image]] of ''T''.\n\nWhen ''V'' is not an inner product space, the coimage of ''T'' can be defined as the [[quotient space (linear algebra)|quotient space]] ''V''&nbsp;/&nbsp;ker(''T'').\n\n==See also==\n* [[Euclidean subspace]]\n\n==Notes==\n{{reflist}}\n\n==References==\n{{see also|Linear algebra#Further reading}}\n\n=== Textbooks ===\n* {{Citation\n | last = Anton\n | first = Howard\n | date = 1987\n | title = Elementary Linear Algebra\n | location = New York\n | publisher = [[John Wiley & Sons|Wiley]]\n | edition = 5th\n | isbn = 0-471-84819-0\n}}\n* {{Citation\n | last = Axler\n | first = Sheldon Jay\n | date = 1997\n | title = Linear Algebra Done Right\n | publisher = Springer-Verlag\n | edition = 2nd\n | isbn = 0-387-98259-0\n}}\n* {{Citation \n| last1 = Banerjee \n| first1 = Sudipto\n| last2 = Roy\n| first2 = Anindya \n| date = June 6, 2014 \n| title = Linear Algebra and Matrix Analysis for Statistics \n| publisher = CRC Press \n| edition = 1st\n| isbn = 978-1-42-009538-8\n}}\n* {{ citation \n| last1 = Beauregard \n| first1 = Raymond A. \n| last2 = Fraleigh \n| first2 = John B. \n| title = A First Course In Linear Algebra: with Optional Introduction to Groups, Rings, and Fields \n| location = Boston \n| publisher = [[Houghton Mifflin Company]] \n| year = 1973 \n| isbn = 0-395-14017-X }}\n* {{Citation\n | last = Lay\n | first = David C.\n | date = August 22, 2005\n | title = Linear Algebra and Its Applications\n | publisher = Addison Wesley\n | edition = 3rd\n | isbn = 978-0-321-28713-7\n}}\n* {{Citation\n | last = Leon\n | first = Steven J.\n | date = 2006\n | title = Linear Algebra With Applications\n | publisher = Pearson Prentice Hall\n | edition = 7th\n}}\n* {{Citation\n |last        = Meyer\n |first       = Carl D.\n |date        = February 15, 2001\n |title       = Matrix Analysis and Applied Linear Algebra\n |publisher   = Society for Industrial and Applied Mathematics (SIAM)\n |isbn        = 978-0-89871-454-8\n |url         = http://www.matrixanalysis.com/DownloadChapters.html\n |deadurl     = yes\n |archiveurl  = https://web.archive.org/web/20010301161440/http://matrixanalysis.com/DownloadChapters.html\n |archivedate = March 1, 2001\n |df          = \n}}\n* {{Citation\n | last = Poole\n | first = David\n | date = 2006\n | title = Linear Algebra: A Modern Introduction\n | publisher = Brooks/Cole\n | edition = 2nd\n | isbn = 0-534-99845-3\n}}\n* {{Citation \n| last = Strang \n| first = Gilbert \n| date = July 19, 2005 \n| title = Linear Algebra and Its Applications \n| publisher = Brooks Cole \n| edition = 4th \n| isbn = 978-0-03-010567-8\n}}\n\n==External links==\n\n{{wikibooks|Linear Algebra/Column and Row Spaces}}\n\n*{{MathWorld |title=Row Space |urlname=RowSpace}}\n*{{MathWorld |title=Column Space |urlname=ColumnSpace}}\n*{{aut|[[Gilbert Strang]]}}, [http://ocw.mit.edu/OcwWeb/Mathematics/18-06Spring-2005/VideoLectures/detail/lecture10.htm MIT Linear Algebra Lecture on the Four Fundamental Subspaces] at Google Video, from [[MIT OpenCourseWare]]\n*[http://www.khanacademy.org/video/column-space-of-a-matrix?playlist=Linear+Algebra Khan Academy video tutorial]\n*[http://mfile.akamai.com/7870/rm/mitstorage.download.akamai.com/7870/18/18.06/videolectures/strang-1806-lec06-20sep1999-80k.rm Lecture on column space and nullspace by Gilbert Strang of MIT]\n*[http://wps.prenhall.com/am_leon_linearalg_7/53/13727/3514210.cw/content/index.html Row Space and Column Space]\n\n{{linear algebra}}\n\n[[Category:Abstract algebra]]\n[[Category:Linear algebra]]\n[[Category:Matrices]]"
    },
    {
      "title": "Scalar (mathematics)",
      "url": "https://en.wikipedia.org/wiki/Scalar_%28mathematics%29",
      "text": "{{other uses2|Scalar}}\n[[File:Vector components.svg|thumb|Scalars are [[real numbers]] used in linear algebra, as opposed to [[vector (mathematics)|vectors]]. This image shows a [[Euclidean vector]]. Its coordinates ''x'' and ''y'' are scalars, as is its length, but '''v''' is not a scalar.]]\n\nA '''scalar''' is an element of a [[Field (mathematics)|field]] which is used to define a [[vector space]]. A quantity described by multiple scalars, such as having both direction and magnitude, is called a [[Vector (mathematics and physics)|vector]].<ref>[http://www.mathwords.com/s/scalar.htm Mathwords.com – Scalar]</ref>\n\nIn [[linear algebra]], real numbers or other elements of a field are called '''scalars''' and relate to vectors in a vector space through the operation of [[scalar multiplication]], in which a vector can be multiplied by a number to produce another vector.<ref>{{cite book |last=Lay |first=David C. |title=Linear Algebra and Its Applications |publisher=[[Addison–Wesley]] |year=2006 |edition=3rd |isbn=0-321-28713-4}}</ref><ref>{{cite book |last=Strang |first=Gilbert |authorlink=Gilbert Strang |title=Linear Algebra and Its Applications |publisher=[[Brooks Cole]] |year=2006 |edition=4th |isbn=0-03-010567-6}}</ref><ref>{{cite book |last=Axler |first=Sheldon |title=Linear Algebra Done Right |publisher=[[Springer Science+Business Media|Springer]] |year=2002 |edition=2nd |isbn=0-387-98258-2}}</ref> More generally, a vector space may be defined by using any [[Field (mathematics)|field]] instead of real numbers, such as [[complex number]]s. Then the scalars of that vector space will be the elements of the associated field.\n\nA [[Inner product|scalar product]] operation –&nbsp;not to be confused with [[scalar multiplication]]&nbsp;– may be defined on a vector space, allowing two vectors to be multiplied to produce a scalar. A vector space equipped with a scalar product is called an [[inner product space]].\n\nThe real component of a [[quaternion]] is also called its '''scalar part'''.\n\nThe term is also sometimes used informally to mean a vector, [[Matrix (mathematics)|matrix]], [[tensor]], or other usually \"compound\" value that is actually reduced to a single component.  Thus, for example, the product of a 1&times;''n'' matrix and an ''n''&times;1 matrix, which is formally a 1&times;1 matrix, is often said to be a '''scalar'''.\n\nThe term [[scalar matrix]] is used to denote a matrix of the form ''kI'' where ''k'' is a scalar and ''I'' is the [[identity matrix]].\n\n==Etymology==\n\nThe word ''scalar'' derives from the [[Latin language|Latin]]  word ''scalaris'', an adjectival form of ''scala'' (Latin for \"ladder\"), from which the English word ''scale'' also comes. The first recorded usage of the word \"scalar\" in mathematics occurs in [[François Viète]]'s ''Analytic Art'' (''In artem analyticem isagoge'') (1591):<ref>\n{{cite book\n| last1                 = Vieta\n| first1                = Franciscus\n| author-link1          = François Viète\n| title                 = In artem analyticem isagoge seorsim excussa ab Opere restitutae mathematicae analyseos, seu Algebra noua\n|trans-title=Guide to the analytic art [...] or new algebra\n| url                   = https://books.google.com/books?id=BWTyywN39KEC\n| language              = Latin\n| location              = Tours\n| publisher             = apud Iametium Mettayer typographum regium\n| publication-date      = 1591\n| accessdate            = 2015-06-24\n}}\n</ref>{{page needed|date=June 2015}}<ref>\nhttp://math.ucdenver.edu/~wcherowi/courses/m4010/s08/lcviete.pdf Lincoln Collins. Biography Paper: Francois Viete</ref>\n\n:Magnitudes that ascend or descend proportionally in keeping with their nature from one kind to another may be called scalar terms.\n:(Latin: ''Magnitudines quae ex genere ad genus sua vi proportionaliter adscendunt vel descendunt, vocentur Scalares.'')\n\nAccording to a citation in the ''[[Oxford English Dictionary]]'' the first recorded usage of the term \"scalar\" in English came with [[William Rowan Hamilton|W. R. Hamilton]] in 1846, referring to the real part of a [[quaternion]]:\n\n:''The algebraically real part may receive, according to the question in which it occurs, all values contained on the one scale of progression of numbers from negative to positive infinity; we shall call it therefore the scalar part.''\n\n==Definitions and properties==\n\n===Scalars of vector spaces===\nA [[vector space]] is defined as a set of vectors, a set of scalars, and a [[scalar multiplication]] operation that takes a scalar ''k'' and a vector '''v''' to another vector ''k'''''v'''.  For example, in a [[coordinate space]], the scalar multiplication <math>k(v_1, v_2, \\dots, v_n)</math> yields <math> (kv_1, kv_2, \\dots, k v_n)</math>.  In a (linear) [[function space]], ''kƒ'' is the function ''x'' {{Mapsto}} ''k''(''ƒ''(''x'')).\n\nThe scalars can be taken from any field, including the [[rational number|rational]], [[algebraic number|algebraic]], real, and complex numbers, as well as [[finite field]]s.\n\n===Scalars as vector components===\nAccording to a fundamental theorem of linear algebra, every vector space has a [[basis (linear algebra)|basis]].  It follows that every vector space over a scalar field ''K'' is [[isomorphism|isomorphic]] to a [[coordinate vector space]] where the coordinates are elements of ''K''.  For example, every real vector space of [[dimension (vector space)|dimension]] ''n'' is isomorphic to ''n''-dimensional real space '''R'''<sup>''n''</sup>.\n\n===Scalars in normed vector spaces===\nAlternatively, a vector space ''V'' can be equipped with a [[norm (mathematics)|norm]] function that assigns to every vector '''v''' in ''V'' a scalar ||'''v'''||. By definition, multiplying '''v''' by a scalar ''k'' also multiplies its norm by |''k''|. If ||'''v'''|| is interpreted as the ''length'' of '''v''', this operation can be described as '''scaling''' the length of '''v''' by ''k''.  A vector space equipped with a norm is called a [[normed vector space]] (or ''normed linear space'').\n\nThe norm is usually defined to be an element of ''V''<nowiki>'</nowiki>s scalar field ''K'', which restricts the latter to fields that support the notion of sign. Moreover, if ''V'' has dimension 2 or more, ''K'' must be closed under square root, as well as the four arithmetic operations; thus the rational numbers '''Q''' are excluded, but the [[quadratic surd|surd field]] is acceptable.  For this reason, not every scalar product space is a normed vector space.\n\n===Scalars in modules===\nWhen the requirement that the set of scalars form a field is relaxed so that it need only form a [[ring (mathematics)|ring]] (so that, for example, the division of scalars need not be defined, or the scalars need not be [[commutative]]), the resulting more general algebraic structure is called a [[module (mathematics)|module]].\n\nIn this case the \"scalars\" may be complicated objects.  For instance, if ''R'' is a ring, the vectors of the product space ''R''<sup>''n''</sup> can be made into a module with the ''n''×''n'' matrices with entries from ''R'' as the scalars. Another example comes from [[manifold|manifold theory]], where the space of [[Section (fiber bundle)|sections]] of the [[tangent bundle]] forms a module over the [[algebra]] of real functions on the manifold.\n\n===Scaling transformation===\nThe scalar multiplication of vector spaces and modules is a special case of [[scaling (geometry)|scaling]], a kind of [[linear transformation]].\n\n===Scalar operations (computer science)===\nOperations that apply to a single value at a time.\n* [[Scalar processor]]\n\n==See also==\n* [[Scalar (physics)]]\n\n==References==\n{{reflist}}\n\n== External links ==\n* {{springer|title=Scalar|id=p/s083240}}\n* {{MathWorld |urlname=Scalar |title=Scalar}}\n* [http://www.mathwords.com/s/scalar.htm Mathwords.com – Scalar]\n\n{{Linear algebra}}\n\n{{DEFAULTSORT:Scalar (Mathematics)}}\n[[Category:Abstract algebra]]\n[[Category:Linear algebra]]\n[[Category:Analytic geometry]]"
    },
    {
      "title": "Scalar multiplication",
      "url": "https://en.wikipedia.org/wiki/Scalar_multiplication",
      "text": "{{distinguish|scalar product}}\n[[Image:Scalar multiplication by r=3.svg|250px|thumb|right|Scalar multiplication of a vector by a factor of 3 stretches the vector out.]]\n[[Image:Scalar multiplication of vectors2.svg|250px|thumb|right|The scalar multiplications −'''a''' and 2'''a''' of a vector '''a''']]\n\nIn [[mathematics]], '''scalar multiplication''' is one of the basic operations defining a [[vector space]] in [[linear algebra]]<ref>{{cite book | last=Lay | first=David C. | title=Linear Algebra and Its Applications | publisher=[[Addison–Wesley]] | year=2006 | edition = 3rd | isbn=0-321-28713-4}}</ref><ref>{{cite book | last=Strang | first=Gilbert | authorlink=Gilbert Strang | title=Linear Algebra and Its Applications | publisher=[[Brooks Cole]] | year=2006 | edition = 4th | isbn=0-03-010567-6}}</ref><ref>{{cite book | last = Axler | first = Sheldon | title = Linear Algebra Done Right | publisher = [[Springer Science+Business Media|Springer]] | year = 2002 | edition = 2nd | isbn = 0-387-98258-2}}</ref> (or more generally, a [[module (mathematics)|module]] in [[abstract algebra]]<ref>{{cite book | last1=Dummit | first1=David S. | last2=Foote | first2=Richard M. | title=Abstract Algebra | publisher=[[John Wiley & Sons]] | year=2004 | edition=3rd | isbn=0-471-43334-9}}</ref><ref>{{cite book | last=Lang | first=Serge | authorlink=Serge Lang | title=Algebra | publisher=[[Springer Science+Business Media|Springer]] | series=[[Graduate Texts in Mathematics]] | year=2002 | isbn=0-387-95385-X}}</ref>). In common geometrical contexts, scalar multiplication of a [[real number|real]] [[Euclidean vector]] by a positive real number multiplies the magnitude of the vector without changing its direction. The term [[scalar (mathematics)|\"scalar\"]] itself derives from this usage: a scalar is that which [[uniform scaling|scales]] vectors. Scalar multiplication is the multiplication of a vector by a scalar (where the product is a vector), and must be distinguished from [[inner product]] of two vectors (where the product is a scalar).\n\n==Definition==\nIn general, if ''K'' is a [[field (algebra)|field]] and ''V'' is a vector space over ''K'', then scalar multiplication is a [[function (mathematics)|function]] from ''K'' &times; ''V'' to ''V''.\nThe result of applying this function to ''c'' in ''K'' and '''''v''''' in ''V'' is denoted ''c'''v'''''.\n\n===Properties===\n\nScalar multiplication obeys the following rules ''(vector in [[boldface]])'':\n* [[Additive map|Additivity]] in the scalar: (''c'' + ''d'')'''''v''''' = ''c'''v''''' + ''d'''v''''';\n* Additivity in the vector: ''c''('''''v''''' + '''''w''''') = ''c'''v''''' + ''c'''w''''';\n* Compatibility of product of scalars with scalar multiplication: (''cd'')'''''v''''' = ''c''(''d'''v''''');\n* Multiplying by 1 does not change a vector: 1'''''v''''' = '''''v''''';\n* Multiplying by 0 gives the [[zero vector]]: 0'''''v''''' = '''''0''''';\n* Multiplying by −1 gives the [[additive inverse]]: (−1)'''''v''''' = −'''''v'''''.\nHere + is [[addition]] either in the field or in the vector space, as appropriate; and 0 is the additive identity in either.\n[[Multiplication by juxtaposition|Juxtaposition]] indicates either scalar multiplication or the [[multiplication]] operation in the field.\n\n==Interpretation==\n\nScalar multiplication may be viewed as an [[external (mathematics)|external]] [[binary operation]] or as an [[Group action (mathematics)|action]] of the field on the vector space. A [[geometric]] interpretation of scalar multiplication is that it stretches, or contracts, vectors by a constant factor.\n\nAs a special case, ''V'' may be taken to be ''K'' itself and scalar multiplication may then be taken to be simply the multiplication in the field.\n\nWhen ''V'' is ''K''<sup>''n''</sup>, scalar multiplication is equivalent to multiplication of each component with the scalar, and may be defined as such.\n\nThe same idea applies if ''K'' is a [[commutative ring]] and ''V'' is a [[module (mathematics)|module]] over ''K''.\n''K'' can even be a [[rig (algebra)|rig]], but then there is no additive inverse.\nIf ''K'' is not [[commutative]], the distinct operations ''left scalar multiplication'' ''c'''v''''' and ''right scalar multiplication'' '''''v'''c'' may be defined.\n\n== Scalar multiplication of matrices ==\n\n{{main|Matrix (mathematics)}}\n\nThe '''left scalar multiplication''' of a matrix {{math|'''A'''}} with a scalar {{math|''λ''}} gives another matrix {{math|''λ'''''A'''}} of the same size as {{math|'''A'''}}. The entries of {{math|''λ'''''A'''}} are defined by\n\n:<math> (\\lambda \\mathbf{A})_{ij} = \\lambda\\left(\\mathbf{A}\\right)_{ij}\\,,</math>\n\nexplicitly:\n\n:<math> \\lambda \\mathbf{A} = \\lambda \\begin{pmatrix}\nA_{11} & A_{12} & \\cdots & A_{1m} \\\\\nA_{21} & A_{22} & \\cdots & A_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nA_{n1} & A_{n2} & \\cdots & A_{nm} \\\\\n\\end{pmatrix} = \\begin{pmatrix}\n\\lambda A_{11} & \\lambda A_{12} & \\cdots & \\lambda A_{1m} \\\\\n\\lambda A_{21} & \\lambda A_{22} & \\cdots & \\lambda A_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\lambda A_{n1} & \\lambda A_{n2} & \\cdots & \\lambda A_{nm} \\\\\n\\end{pmatrix}\\,.</math>\n\nSimilarly, the '''right scalar multiplication''' of a matrix {{math|'''A'''}} with a scalar {{math|''λ''}} is defined to be\n\n:<math> (\\mathbf{A}\\lambda)_{ij} = \\left(\\mathbf{A}\\right)_{ij} \\lambda\\,, </math>\n\nexplicitly:\n\n:<math> \\mathbf{A}\\lambda = \\begin{pmatrix}\nA_{11} & A_{12} & \\cdots & A_{1m} \\\\\nA_{21} & A_{22} & \\cdots & A_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nA_{n1} & A_{n2} & \\cdots & A_{nm} \\\\\n\\end{pmatrix}\\lambda = \\begin{pmatrix}\nA_{11} \\lambda & A_{12} \\lambda & \\cdots & A_{1m} \\lambda \\\\\n A_{21} \\lambda & A_{22} \\lambda & \\cdots & A_{2m} \\lambda \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nA_{n1} \\lambda & A_{n2} \\lambda & \\cdots & A_{nm} \\lambda \\\\\n\\end{pmatrix}\\,.</math>\n\nWhen the underlying [[ring (mathematics)|ring]] is [[commutative]], for example, the [[real numbers|real]] or [[complex number]] [[Field (mathematics)|field]], these two multiplications are the same, and are simply called ''scalar multiplication''. However, for matrices over a more general [[ring (mathematics)|ring]] that are ''not'' commutative, such as the [[quaternion]]s, they may not be equal.\n\nFor a real scalar and matrix:\n\n:<math> \\lambda = 2, \\quad \\mathbf{A} =\\begin{pmatrix}\na & b \\\\\nc & d \\\\\n\\end{pmatrix} </math>\n\n:<math> 2 \\mathbf{A} = 2 \\begin{pmatrix}\na & b \\\\\nc & d \\\\\n\\end{pmatrix} = \\begin{pmatrix}\n2 \\!\\cdot\\! a & 2 \\!\\cdot\\! b \\\\\n2 \\!\\cdot\\! c & 2 \\!\\cdot\\! d \\\\\n\\end{pmatrix} = \\begin{pmatrix}\na \\!\\cdot\\! 2 & b \\!\\cdot\\! 2 \\\\\nc \\!\\cdot\\! 2 & d \\!\\cdot\\! 2 \\\\\n\\end{pmatrix} = \\begin{pmatrix}\na & b \\\\\nc & d \\\\\n\\end{pmatrix}2= \\mathbf{A}2.</math>\n\nFor quaternion scalars and matrices:\n\n:<math> \\lambda = i, \\quad \\mathbf{A} = \\begin{pmatrix}\n    i & 0 \\\\\n    0 & j \\\\\n  \\end{pmatrix} </math>\n\n:<math>\n  i\\begin{pmatrix}\n    i & 0 \\\\\n    0 & j \\\\\n  \\end{pmatrix}\n= \\begin{pmatrix}\n    i^2 & 0 \\\\\n    0 & ij \\\\\n  \\end{pmatrix}\n= \\begin{pmatrix}\n    -1 & 0 \\\\\n     0 & k \\\\\n  \\end{pmatrix}\n\\ne \\begin{pmatrix}\n    -1 & 0 \\\\\n    0 & -k \\\\\n  \\end{pmatrix}\n= \\begin{pmatrix}\n    i^2 & 0 \\\\\n    0 & ji \\\\\n  \\end{pmatrix}\n= \\begin{pmatrix}\n    i & 0 \\\\\n    0 & j \\\\\n  \\end{pmatrix}i\\,,\n</math>\n\nwhere {{math|''i'', ''j'', ''k''}} are the quaternion units. The non-commutativity of quaternion multiplication prevents the transition of changing {{math|''ij'' {{=}} +''k''}} to {{math|''ji'' {{=}} −''k''}}.\n\n==See also==\n\n*[[Statics]]\n*[[Mechanics]]\n*[[Product (mathematics)]]\n\n==References==\n{{reflist}}\n\n{{Linear algebra}}\n{{Algebra-footer}}\n\n{{DEFAULTSORT:Scalar Multiplication}}\n[[Category:Linear algebra]]\n[[Category:Abstract algebra]]\n[[Category:Multiplication]]"
    },
    {
      "title": "Self-adjoint",
      "url": "https://en.wikipedia.org/wiki/Self-adjoint",
      "text": "In [[mathematics]], an element ''x'' of a [[*-algebra]] is '''self-adjoint''' if  <math>x^*=x</math>.\n\nA collection ''C'' of elements of a star-algebra is '''self-adjoint''' if it is closed under the [[Involution (mathematics)|involution]] operation. For example, if <math>x^*=y</math> then since <math>y^*=x^{**}=x</math> in a star-algebra, the set {''x'',''y''} is a self-adjoint set even though ''x'' and ''y'' need not be self-adjoint elements.\n\nIn [[functional analysis]], a [[linear operator]] ''A'' on a [[Hilbert space]] is called '''self-adjoint''' if it is equal to its own [[adjoint operator|adjoint]] ''A''{{sup|∗}} and that the domain of ''A'' is the same as that of ''A''{{sup|∗}}. See [[self-adjoint operator]] for a detailed discussion. If the Hilbert space is finite-dimensional and an [[orthonormal basis]] has been chosen, then the operator ''A'' is self-adjoint if and only if the [[matrix (mathematics)|matrix]] describing ''A'' with respect to this basis is [[Hermitian matrix|Hermitian]], i.e. if it is equal to its own [[conjugate transpose]]. Hermitian matrices are also called '''self-adjoint'''.\n\nIn a [[dagger category]], a [[morphism]] <math> f</math> is called '''self-adjoint''' if <math> f=f^\\dagger</math>; this is possible only for an [[endomorphism]] <math>f\\colon A \\to A</math>.\n\n==See also==\n*[[Symmetric matrix]]\n*[[Self-adjoint operator]]\n*[[Hermitian matrix]]\n\n==References==\n*{{cite book |authorlink=Michael C. Reed |first=M. |last=Reed |authorlink2=Barry Simon |first2=B. |last2=Simon |title=Methods of Mathematical Physics |others=Vol 2 |publisher=Academic Press |year=1972 |isbn= }}\n*{{cite book |authorlink=Gerald Teschl |first=G. |last=Teschl |title=Mathematical Methods in Quantum Mechanics; With Applications to Schrödinger Operators |publisher=American Mathematical Society |location=Providence |year=2009 |url=http://www.mat.univie.ac.at/~gerald/ftp/book-schroe/ }}\n\n{{DEFAULTSORT:Self-Adjoint}}\n[[Category:Abstract algebra]]\n[[Category:Linear algebra]]"
    },
    {
      "title": "Separating lattice homomorphism",
      "url": "https://en.wikipedia.org/wiki/Separating_lattice_homomorphism",
      "text": "{{Multiple issues|\n{{orphan|date=August 2017}}\n{{unreferenced|date=August 2017}}\n}}\n\nLet <math>\\mathbb{L}</math> and <math>\\mathbb{L}'</math> be two [[Lattice (order)|lattices]] with '''0''' and '''1'''. A homomorphism from <math>\\mathbb{L}</math> to <math>\\mathbb{L}'</math> is called '''0,1'''-''separating'' iff <math>f^{-1}\\{f(0)\\}=\\{0\\}</math> (<math>f</math> separates '''0''') and <math>f^{-1}\\{f(1)\\}=\\{1\\}</math> (<math>f</math> separates 1).\n\n\n{{abstract-algebra-stub}}\n\n\n\n[[Category:Abstract algebra]]"
    },
    {
      "title": "Sequential dynamical system",
      "url": "https://en.wikipedia.org/wiki/Sequential_dynamical_system",
      "text": "[[File:SDSphase1.png|thumbnail|right|Phase space of the sequential dynamical system]]\n\n'''Sequential dynamical systems''' ('''SDSs''') are a class of [[graph dynamical system]]s. They are discrete [[dynamical systems]] which generalize many aspects of for example classical [[cellular automata]], and they provide a framework for studying asynchronous processes over [[graph theory|graphs]]. The analysis of SDSs uses techniques from [[combinatorics]], [[abstract algebra]], [[graph theory]], [[dynamical system]]s and [[probability theory]].\n\n==Definition==\nAn SDS is constructed from the following components:\n\n<blockquote>\n* A finite ''graph'' ''Y'' with vertex set v[''Y''] = {1,2, ... , n}. Depending on the context the graph can be directed or undirected.\n* A state ''x<sub>v</sub>'' for each vertex ''i'' of ''Y'' taken from a finite set ''K''. The ''system state'' is the ''n''-tuple ''x'' = (''x''<sub>1</sub>, ''x''<sub>2</sub>, ... , ''x<sub>n</sub>''), and ''x''[''i''] is the [[tuple]] consisting of the states associated to the vertices in the 1-neighborhood of ''i'' in ''Y'' (in some fixed order).\n* A ''vertex function'' ''f<sub>i</sub>'' for each vertex ''i''. The vertex function maps the state of vertex ''i'' at time ''t'' to the vertex state at time ''t''&nbsp;+&nbsp;1 based on the states associated to the 1-neighborhood of ''i'' in ''Y''.\n* A word ''w'' = (''w''<sub>1</sub>, ''w''<sub>2</sub>, ... , ''w<sub>m</sub>'') over ''v''[''Y'']. \n</blockquote>\n\nIt is convenient to introduce the ''Y''-local maps ''F<sub>i</sub>'' constructed from the vertex functions by\n\n: <math>F_i (x) = (x_1, x_2,\\ldots, x_{i-1}, f_i(x[i]), x_{i+1}, \\ldots , x_n) \\;. </math>\n\nThe word ''w'' specifies the sequence in which the ''Y''-local maps are composed to derive the sequential dynamical system map ''F'': ''K<sup>n</sup> → K<sup>n</sup>'' as\n\n: <math>[F_Y ,w] = F_{w(m)} \\circ F_{w(m-1)} \\circ \\cdots \\circ F_{w(2)} \\circ F_{w(1)} \\;. </math>\n\nIf the update sequence is a permutation one frequently speaks of a ''permutation SDS'' to emphasize this point. \nThe ''phase space'' associated to a sequential dynamical system with map ''F'': ''K<sup>n</sup> → K<sup>n</sup>'' is the finite directed graph with vertex set ''K<sup>n</sup>'' and directed edges (''x'', ''F''(''x'')). The structure of the phase space is governed by the properties of the graph ''Y'', the vertex functions (''f<sub>i</sub>'')''<sub>i</sub>'', and the update sequence ''w''. A large part of SDS research seeks to infer phase space properties based on the structure of the system constituents.\n\n==Example==\nConsider the case where ''Y'' is the graph with vertex set {1,2,3} and undirected edges {1,2}, {1,3} and {2,3} (a triangle or 3-circle) with vertex states from ''K'' = {0,1}. For vertex functions use the symmetric, boolean function nor : ''K<sup>3</sup> → K'' defined by nor(''x'',''y'',''z'') = (1+''x'')(1+''y'')(1+''z'') with boolean arithmetic. Thus, the only case in which the function nor returns the value 1 is when all the arguments are 0. Pick ''w'' = (1,2,3) as update sequence. Starting from the initial system state (0,0,0) at time ''t'' = 0 one computes the state of vertex 1 at time ''t''=1 as nor(0,0,0) = 1. The state of vertex 2 at time ''t''=1 is nor(1,0,0) = 0. Note that the state of vertex 1 at time ''t''=1 is used immediately. Next one obtains the state of vertex 3 at time ''t''=1 as nor(1,0,0) = 0. This completes the update sequence, and one concludes that the Nor-SDS map sends the system state (0,0,0) to (1,0,0). The system state (1,0,0) is in turned mapped to (0,1,0) by an application of the SDS map.\n\n==See also==\n*[[Graph dynamical system]]\n*[[Boolean network]]\n*[[Gene regulatory network]]\n*[[Dynamic Bayesian network]]\n*[[Petri net]]\n\n==References==\n* {{cite book | author=Henning S. Mortveit, Christian M. Reidys | title=An Introduction to Sequential Dynamical Systems | publisher=Springer | year=2008 | isbn=0387306544}}\n*[http://www.emis.de/journals/DMTCS/pdfpapers/dmAB0106.pdf Predecessor and Permutation Existence Problems for Sequential Dynamical Systems]\n*[https://arxiv.org/pdf/math.DS/0603370 Genetic Sequential Dynamical Systems]\n\n[[Category:Combinatorics]]\n[[Category:Graph theory]]\n[[Category:Networks]]\n[[Category:Abstract algebra]]\n[[Category:Dynamical systems]]"
    },
    {
      "title": "Setoid",
      "url": "https://en.wikipedia.org/wiki/Setoid",
      "text": "{{Use American English|date = March 2019}}\n{{Short description|Mathematical construction of a set with an equivalence relation}}\n{{Redirect|E-set|the technique in fertility medicine|e-SET}}\n\nIn [[mathematics]], a '''setoid''' (''X'', ~) is a [[Set (mathematics)|set]] (or [[type (mathematics)|type]]) ''X'' equipped with an [[equivalence relation]] ~.  A Setoid may also be called '''E-set''', '''[[Errett Bishop|Bishop]] set''', or '''extensional set'''.<ref>Alexandre Buisse, Peter Dybjer, [http://ac.els-cdn.com/S1571066108003976/1-s2.0-S1571066108003976-main.pdf?_tid=2f6e8f24-5e62-11e7-8bda-00000aacb35e&acdnat=1498916325_9ee52b78dcb800a72e25dac0be00d6ac \"The Interpretation of Intuitionistic Type Theory in Locally Cartesian Closed Categories - an Intuitionistic Perspective\"], ''Electronic Notes in Theoretical Computer Science'' 218 (2008) 21–32.</ref>\n\nSetoids are studied especially in [[proof theory]] and in [[type-theoretic]] [[foundations of mathematics]]. Often in mathematics, when one defines an equivalence relation on a set, one immediately forms the [[quotient set]] (turning equivalence into [[equality (mathematics)|equality]]). In contrast, setoids may be used when a difference between identity and equivalence must be maintained, often with an interpretation of [[intension]]al equality (the equality on the original set) and [[Extension (semantics)|extensional]] equality (the equivalence relation, or the equality on the quotient set).\n\n==Proof theory==\nIn proof theory, particularly the proof theory of [[constructive mathematics]] based on the [[Curry–Howard correspondence]], one often identifies a mathematical [[Proposition (mathematics)|proposition]] with its set of [[proof (mathematics)|proof]]s (if any). A given proposition may have many proofs, of course; according to the principle of [[proof irrelevance]], normally only the truth of the proposition matters, not which proof was used. However, the Curry–Howard correspondence can turn proofs into [[algorithm]]s, and differences between algorithms are often important. So proof theorists may prefer to identify a proposition with a ''setoid'' of proofs, considering proofs equivalent if they can be converted into one another through [[beta conversion]] or the like.\n\n==Type theory==\nIn type-theoretic foundations of mathematics, setoids may be used in a type theory that lacks [[quotient type]]s to model general mathematical sets. For example, in [[Per Martin-Löf]]'s [[intuitionistic type theory]], there is no type of [[real number]]s, only a type of [[regular Cauchy sequence]]s of [[rational number]]s. To do [[real analysis]] in Martin-Löf's framework, therefore, one must work with a ''setoid'' of real numbers, the type of regular Cauchy sequences equipped with the usual notion of equivalence.  Predicates and functions of real numbers need to be defined for regular Cauchy sequences and proven to be compatible with the equivalence relation.  Typically (although it depends on the type theory used), the [[axiom of choice]] will hold for functions between types (intensional functions), but not for functions between setoids (extensional functions).{{clarify|date=October 2010}} The term \"set\" is variously used either as a synonym of \"type\" or as a synonym of \"setoid\".<ref>{{cite journal|page=9|url=http://www.cs.chalmers.se/Cs/Research/Logic/TypesSS05/Extra/palmgren.pdf|title=Bishop's set theory}}</ref>\n\n==Constructive mathematics==\nIn [[Constructivism (mathematics)|constructive mathematics]], one often takes a setoid with an [[apartness relation]] instead of an equivalence relation, called a '''constructive''' setoid. One sometimes also considers a '''partial''' setoid using a [[partial equivalence relation]] or partial apartness. (see e.g. Barthe ''et al.'', section 1)\n\n== See also ==\n{{Portal|Computer Science}}\n* [[Groupoid]]\n\n==Notes==\n<references/>\n\n== References ==\n*{{citation\n | last = Hofmann | first = Martin\n | contribution = A simple model for quotient types\n | doi = 10.1007/BFb0014055\n | location = Berlin\n | mr = 1477985\n | pages = 216–234\n | publisher = Springer\n | series = Lecture Notes in Comput. Sci.\n | title = Typed lambda calculi and applications (Edinburgh, 1995)\n | url = http://www.tcs.informatik.uni-muenchen.de/~mhofmann/tlca95.ps\n | volume = 902\n | year = 1995| isbn = 978-3-540-59048-4\n | citeseerx = 10.1.1.55.4629\n }}.\n*{{citation\n | last1 = Barthe | first1 = Gilles\n | last2 = Capretta | first2 = Venanzio\n | last3 = Pons | first3 = Olivier\n | doi = 10.1017/S0956796802004501\n | issue = 2\n | journal = Journal of Functional Programming\n | mr = 1985376\n | pages = 261–293\n | title = Setoids in type theory\n | url = http://www.cs.ru.nl/~venanzio/publications/Setoids_JFP_2003.pdf\n | volume = 13\n | year = 2003}}.\n\n==External links==\n* [http://coq.inria.fr/library/Coq.Classes.SetoidClass.html Implementation of setoids] in [[Coq]]\n*{{nlab|id=setoid|title=Setoid}}\n*{{nlab|id=Bishop+set|title=Bishop set}}\n\n[[Category:Abstract algebra]]\n[[Category:Category theory]]\n[[Category:Proof theory]]\n[[Category:Type theory]]"
    },
    {
      "title": "Simple (abstract algebra)",
      "url": "https://en.wikipedia.org/wiki/Simple_%28abstract_algebra%29",
      "text": "In [[mathematics]], the term '''simple''' is used to describe an [[algebraic structure]] which in some sense cannot be divided by a smaller structure of the same type. Put another way, an algebraic structure is simple if the [[Kernel (algebra)|kernel]] of every homomorphism is either the whole structure or a single element. Some examples are:\n\n* A [[group (mathematics)|group]] is called a [[simple group]] if it does not contain a nontrivial proper [[normal subgroup]].\n* A [[ring (mathematics)|ring]] is called a [[simple ring]] if it does not contain a nontrivial [[two sided ideal]].\n* A [[module (mathematics)|module]] is called a [[simple module]] if it does not contain a nontrivial [[submodule]].\n* An [[algebra (ring theory)|algebra]] is called a [[simple algebra]] if it does not contain a nontrivial [[two sided ideal]].\n\nThe general pattern is that the structure admits no non-trivial [[Congruence_relation#Universal_algebra|congruence relations]].\n\nThe term is used differently in [[semigroup]] theory. A semigroup is said to be ''simple'' if it has no nontrivial\n[[Semigroup#Subsemigroups_and_ideals|ideals]], or equivalently, if [[Green's relations|Green's relation]] ''J'' is\nthe universal relation. Not every congruence on a semigroup is associated with an ideal, so a simple semigroup may\nhave nontrivial congruences. A semigroup with no nontrivial congruences is called ''congruence simple''.\n\n==See also==\n* [[semisimple]]\n\n{{DEFAULTSORT:Simple (Abstract Algebra)}}\n[[Category:Abstract algebra]]"
    },
    {
      "title": "Skew-Hermitian matrix",
      "url": "https://en.wikipedia.org/wiki/Skew-Hermitian_matrix",
      "text": "{{for|matrices with antisymmetry over the [[real number]] field|Skew-symmetric matrix}} __NOTOC__\nIn [[linear algebra]], a [[square matrix]] with [[Complex number|complex]] entries is said to be '''skew-Hermitian''' or '''antihermitian''' if its [[conjugate transpose]] is the negative of the original matrix.<ref>{{harvtxt|Horn|Johnson|1985}}, §4.1.1; {{harvtxt|Meyer|2000}}, §3.2</ref> That is, the matrix <math>A</math> is skew-Hermitian if it satisfies the relation\n\n{{Equation box 1\n|indent =\n|title=\n|equation = <math>A \\text{ skew-Hermitian} \\quad \\iff \\quad A^\\mathsf{H} = -A</math>\n|cellpadding= 6\n|border\n|border colour = #0073CF\n|background colour=#F5FFFA}}\n\nwhere <math>A^\\textsf{H}</math> denotes the conjugate transpose of the matrix <math>A</math>.  In component form, this means that\n\n{{Equation box 1\n|indent =\n|title=\n|equation = <math>A \\text{ skew-Hermitian} \\quad \\iff \\quad a_{ij} = -\\overline{a_{ji}}</math>\n|cellpadding= 6\n|border\n|border colour = #0073CF\n|background colour=#F5FFFA}}\n\nfor all indices <math>i</math> and <math>j</math>, where <math>a_{ij}</math> is the element in the <math>j</math>-th row and <math>i</math>-th column of <math>A</math>, and the overline denotes [[complex conjugate|complex conjugation]].\n\nSkew-Hermitian matrices can be understood as the complex versions of real [[Skew-symmetric matrix|skew-symmetric matrices]], or as the matrix analogue of the purely imaginary numbers.<ref name=HJ85S412>{{harvtxt|Horn|Johnson|1985}}, §4.1.2</ref>  The set of all skew-Hermitian <math>n \\times n</math> matrices forms the <math>u(n)</math> [[Lie algebra]], which corresponds to the Lie group [[Unitary group|U(<var>n</var>)]].\nThe concept can be generalized to include [[linear transformation]]s of any [[complex number|complex]] [[vector space]] with a [[sesquilinear]] [[Norm (mathematics)|norm]].\n\nNote that the [[adjoint operator|adjoint]] of an operator depends on the [[scalar product]] considered on the <math>n</math> dimensional complex or real space <math>K^n</math>. If <math>(\\cdot|\\cdot) </math> denotes the scalar product on <math> K^n</math>, then saying <math> A</math> is skew-adjoint means that for all <math>u,v \\in K^n</math> one has\n<math> (Au|v) = - (u|Av) \\,</math>.\n\n[[Imaginary number]]s can be thought of as skew-adjoint (since they are like <math>1 \\times 1</math> matrices), whereas [[real number]]s correspond to [[self-adjoint]] operators.\n\n== Example ==\nFor example, the following matrix is skew-Hermitian\n:<math>A = \\begin{bmatrix} -i & 2 + i \\\\ -2 + i & 0 \\end{bmatrix}</math>\nbecause\n:<math>\n  -A =\n  \\begin{bmatrix} i & -2 - i \\\\ 2 - i & 0 \\end{bmatrix} =\n  \\begin{bmatrix}\n    \\overline{-i}    & \\overline{-2 + i} \\\\\n    \\overline{2 + i} & \\overline{0}\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    \\overline{-i}     & \\overline{2 + i} \\\\\n    \\overline{-2 + i} &     \\overline{0}\n  \\end{bmatrix}^\\mathsf{T} =\n  A^\\mathsf{H}\n</math>\n\n== Properties ==\n* The eigenvalues of a skew-Hermitian matrix are all purely imaginary (and possibly zero). Furthermore, skew-Hermitian matrices are [[normal matrix|normal]].  Hence they are diagonalizable and their eigenvectors for distinct eigenvalues must be orthogonal.<ref>{{harvtxt|Horn|Johnson|1985}}, §2.5.2, §2.5.4</ref>\n* All entries on the [[main diagonal]] of a skew-Hermitian matrix have to be pure [[imaginary number|imaginary]]; i.e., on the imaginary axis (the number zero is also considered purely imaginary).<ref>{{harvtxt|Meyer|2000}}, Exercise 3.2.5</ref>\n* If <math>A</math> and <math>B</math> are skew-Hermitian, then {{tmath|aA + bB}} is skew-Hermitian for all [[real number|real]] [[scalar (mathematics)|scalars]] <math>a</math> and <math>b</math>.<ref name=HJ85S411>{{harvtxt|Horn|Johnson|1985}}, §4.1.1</ref>\n* <math>A</math> is skew-Hermitian ''if and only if'' <math>i A</math> (or equivalently, <math>-i A</math>) is [[Hermitian matrix|Hermitian]].<ref name=HJ85S411/>\n*<math>A</math> is skew-Hermitian ''if and only if'' the real part <math>\\Re{(A)}</math> is [[skew-symmetric matrix|skew-symmetric]] and the imaginary part <math>\\Im{(A)}</math> is [[symmetric matrix|symmetric]].\n* If <math>A</math> is skew-Hermitian, then <math>A^k</math> is Hermitian if <math>k</math> is an even integer and skew-Hermitian if <math>k</math> is an odd integer.\n* <math>A</math> is skew-Hermitian if and only if <math>x^\\mathsf{H} A y = -y^\\mathsf{H} A x</math> for all vectors <math>x,y</math>.\n* If <math>A</math> is skew-Hermitian, then the [[matrix exponential]] <math>e^A</math> is [[unitary matrix|unitary]].\n* The space of skew-Hermitian matrices forms the [[Lie algebra]] <math>u(n)</math> of the [[Lie group]] <math>U(n)</math>.\n\n==Decomposition into Hermitian and skew-Hermitian==\n* The sum of a square matrix and its conjugate transpose <math>\\left(A + A^\\mathsf{H}\\right)</math> is Hermitian.\n* The difference of a square matrix and its conjugate transpose <math>\\left(A - A^\\mathsf{H}\\right)</math> is skew-Hermitian. This implies that [[commutator]] of two Hermitian matrices is skew-Hermitian.\n* An arbitrary square matrix <math>C</math> can be written as the sum of a Hermitian matrix <math>A</math> and a skew-Hermitian matrix <math>B</math>:\n::<math>C = A + B \\quad\\mbox{with}\\quad A = \\frac{1}{2}\\left(C + C^\\mathsf{H}\\right) \\quad\\mbox{and}\\quad B = \\frac{1}{2}\\left(C - C^\\mathsf{H}\\right)</math>\n\n==See also==\n*[[Bivector (complex)]]\n*[[Hermitian matrix]]\n*[[Normal matrix]]\n*[[Skew-symmetric matrix]]\n*[[Unitary matrix]]\n\n==Notes==\n<references/>\n\n==References==\n* {{Citation | last1=Horn | first1=Roger A. | last2=Johnson | first2=Charles R. | title=Matrix Analysis | publisher=[[Cambridge University Press]] | isbn=978-0-521-38632-6 | year=1985}}.\n* {{Citation | last1=Meyer | first1=Carl D. | title=Matrix Analysis and Applied Linear Algebra | url=http://www.matrixanalysis.com/ | publisher=[[Society for Industrial and Applied Mathematics|SIAM]] | isbn=978-0-89871-454-8 | year=2000}}.\n\n[[Category:Matrices]]\n[[Category:Abstract algebra]]\n[[Category:Linear algebra]]"
    },
    {
      "title": "Split exact sequence",
      "url": "https://en.wikipedia.org/wiki/Split_exact_sequence",
      "text": "In mathematics, a '''split exact sequence''' is a [[short exact sequence]] in which the middle term is built out of the two outer terms in the simplest possible way.\n\n==Equivalent characterizations==\n\nA short exact sequence of [[abelian group]]s or of [[module (mathematics)|modules]] over a fixed ring, or more generally of objects in an [[abelian category]]\n\n:<math>0 \\to A \\stackrel a \\to B \\stackrel b \\to C \\to 0</math>\n\nis called split exact if it is isomorphic to the sequence where the middle term is the [[direct sum]] of the outer ones:\n\n:<math>0 \\to A \\stackrel i \\to A \\oplus C \\stackrel p \\to C \\to 0</math>\n\nThe requirement that the sequence is isomorphic means that there is an isomorphism <math>f : B \\to A \\oplus C</math> such that the composite <math>f \\circ a</math> is the natural inclusion <math>i: A \\to A \\oplus C</math> and such that the composite <math>p \\circ f</math> equals ''b''.\n\nThe [[splitting lemma]] provides further equivalent characterizations of split exact sequences.\n==Examples==\n\nAny short exact sequence of [[vector space]]s is split exact. This is a rephrasal of the fact that any set of linearly independent vectors in a vector space can be extended to a basis.\n\nThe exact sequence\n\n:<math>0 \\to \\mathbf Z \\stackrel 2 \\to \\mathbf Z \\to \\mathbf Z / 2 \\to 0</math>\n\n(where the first map is multiplication by 2) is not split exact.\n\n==Related notions==\n\n[[Pure exact sequence]]s can be characterized as the [[filtered colimit]]s of split exact sequences.<ref>{{harvtxt|Fuchs|2015|loc=Ch. 5, Thm. 3.4}}</ref>\n\n==References==\n\n<references/>\n*{{Citation|title=Abelian Groups|author=Fuchs|first=László|isbn=9783319194226|series=Springer Monographs in Mathematics|year=2015|publisher=Springer}}\n[[Category:Abstract algebra]]"
    },
    {
      "title": "Subquotient",
      "url": "https://en.wikipedia.org/wiki/Subquotient",
      "text": "In the [[mathematics|mathematical]] fields of [[category theory]] and [[abstract algebra]], a '''subquotient''' is a [[quotient object]] of a [[subobject]]. Subquotients are particularly important in [[abelian categories]], and in [[group theory]], where they are also known as '''sections''', though this conflicts with a different meaning in [[section (category theory)|category theory]].\n\nFor example, of the 26 [[sporadic group]]s, 20 are subquotients of the [[monster group]], and are referred to as the \"Happy Family\", while the other 6 are [[pariah group]]s.\n\nA quotient of a subrepresentation of a representation (of, say, a group) might be called a subquotient representation; e.g., [[Harish-Chandra]]'s subquotient theorem.<ref>{{Citation | last1=Dixmier | first1=Jacques | title=Enveloping algebras | origyear=1974 | url=https://books.google.com/books?isbn=0821805606 | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=[[Graduate Studies in Mathematics]] | isbn=978-0-8218-0560-2 | mr=0498740 | year=1996 | volume=11}} p. 310</ref>\n\nIn constructive [[set theory]], where the [[law of excluded middle]] does not necessarily hold, one can consider the relation 'subquotient of' as replacing the usual [[order relation]](s) on [[cardinal number|cardinals]]. When one has the law of the excluded middle, then a subquotient <math>X</math> of <math>Y</math> is either the [[empty set]] or there is an onto function <math>Y\\to X</math>. This order relation is traditionally denoted <math>\\leq^\\ast</math>. If additionally the [[axiom of choice]] holds, then <math>X</math> has a one-to-one function to <math>Y</math> and this order relation is the usual <math>\\leq</math> on corresponding cardinals.\n\n==Transitive relation==\nThe relation »is subquotient of« is transitive.\n;Proof\nLet <math>G,H,J</math> groups and <math>\\phi\\colon G\\to H</math> and <math>\\psi\\colon H\\to J</math> be [[group homomorphism]]s, then also the composition\n:<math>\\psi\\circ\\phi\\colon G\\to J, g \\mapsto (\\psi\\circ\\phi)(g):=\\psi(\\phi(g))</math>\nis a homomorphism.\n\nIf <math>U</math> is a subgroup of <math>G</math> and <math>V</math> a subgroup of <math>\\phi(U)</math>, then <math>U':=\\phi^{-1}(V)</math> is a subgroup of <math>U\\leq G</math>. We have <math>\\phi(U')\\subseteq V</math>, indeed <math>\\phi(U')=V</math>, because every <math>v \\in V\\subseteq \\phi(U)</math> has a [[preimage]] in <math>U</math>. Thus <math>(\\psi\\circ\\phi)(U')=\\psi(V)</math>. This means that the image, say <math>\\psi(V)</math>, of a subgroup, say <math>V</math>, of <math>H</math> is also the image of a subgroup, namely <math>U'</math> under <math>\\psi\\circ\\phi</math>, of <math>G</math>.\n\nIn other words: If <math>\\psi(V)</math> is a subquotient of <math>\\phi(U)</math> and <math>\\phi(U)</math> is subquotient of <math>G</math> then <math>\\psi(V)</math> is subquotient of <math>G</math>.&nbsp;&nbsp;■\n\n==See also==\n* [[Homological algebra]]\n\n==References==\n<references/>\n\n[[Category:Category theory]]\n[[Category:Abstract algebra]]\n\n\n{{Categorytheory-stub}}\n{{Abstract-algebra-stub}}"
    },
    {
      "title": "Total algebra",
      "url": "https://en.wikipedia.org/wiki/Total_algebra",
      "text": "In [[abstract algebra]], the '''total algebra''' of a [[monoid]] is a generalization of the [[monoid ring]] that allows for [[series (mathematics)|infinite sums]] of elements of a ring.  Suppose that ''S'' is a monoid with the property that, for all <math>s\\in S</math>, there exist only finitely many ordered pairs <math>(t,u)\\in S\\times S</math> for which <math>tu=s</math>.  \nLet ''R'' be a ring.  Then the total algebra of ''S'' over ''R'' is the set <math>R^S</math> of all functions <math>\\alpha:S\\to R</math> with the addition law given by the (pointwise) operation:\n:<math>(\\alpha+\\beta)(s)=\\alpha(s)+\\beta(s)</math>\nand with the multiplication law given by:\n:<math>(\\alpha\\cdot\\beta)(s) = \\sum_{tu=s}\\alpha(t)\\beta(u).</math>\nThe sum on the right-hand side has finite support, and so is well-defined in ''R''.\n\nThese operations turn <math>R^S</math> into a ring.  There is an embedding of ''R'' into <math>R^S</math>, given by the constant functions, which turns <math>R^S</math> into an ''R''-algebra.\n\nAn example is the ring of [[formal power series]], where the monoid ''S'' is the [[natural numbers]].  The product is then the [[Cauchy product]].\n{{algebra-stub}}\n\n==References==\n* {{citation|author=[[Nicolas Bourbaki]]|title=Algebra|publisher=Springer|year=1989}}: §III.2\n\n[[Category:Abstract algebra]]"
    },
    {
      "title": "Transpose",
      "url": "https://en.wikipedia.org/wiki/Transpose",
      "text": "{{about|the transpose of a matrix||Transposition (disambiguation)}}\n{{hatnote|Note that this article assumes that matrices are taken over a commutative ring. These results may not hold in the non-commutative case.}}\n<!--This manual hatnote is inappropriate; mention of assumptions should be worked into the article proper.-->\n[[File:Matrix transpose.gif|thumb|200px|right|The transpose '''A'''<sup>T</sup> of a matrix '''A''' can be obtained by reflecting the elements along its main diagonal. Repeating the process on the transposed matrix returns the elements to their original position.]]\n\nIn [[linear algebra]], the '''transpose''' of a [[Matrix (mathematics)|matrix]] is an operator which flips a matrix over its diagonal, that is it switches the row and column indices of the matrix by producing another matrix denoted as '''A'''<sup>T</sup> (also written '''A'''′, '''A'''<sup>tr</sup>, <sup>t</sup>'''A''' or '''A'''<sup>t</sup>). It is achieved by any one of the following equivalent actions:\n* reflect '''A''' over its [[main diagonal]] (which runs from top-left to bottom-right) to obtain '''A'''<sup>T</sup>,\n* write the rows of '''A''' as the columns of '''A'''<sup>T</sup>,\n* write the columns of '''A''' as the rows of '''A'''<sup>T</sup>.\n\nFormally, the ''i''-th row, ''j''-th column element of '''A'''<sup>T</sup> is the ''j''-th row, ''i''-th column element of '''A''':\n\n:<math>\\left[\\mathbf{A}^\\operatorname{T}\\right]_{ij} = \\left[\\mathbf{A}\\right]_{ji}.</math>\n\nIf '''A''' is an {{nowrap|''m'' × ''n''}} matrix, then '''A'''<sup>T</sup> is an {{nowrap|''n'' × ''m''}} matrix.  To avoid confusing the reader between the transpose operation and a matrix raised to the t<sup>th</sup> power, the <math>\\mathbf{A}^\\top</math> symbol denotes the transpose operation.\n\nThe transpose of a matrix was introduced in 1858 by the British mathematician [[Arthur Cayley]].<ref>Arthur Cayley (1858) [https://books.google.com/books?id=flFFAAAAcAAJ&pg=PA31#v=onepage&q&f=false \"A memoir on the theory of matrices\"], ''Philosophical Transactions of the Royal Society of London'', '''148''' : 17–37.  The transpose (or \"transposition\") is defined on page 31.</ref> \n\n== Examples ==\n*<math>\\begin{bmatrix}\n    1 & 2\n  \\end{bmatrix}^{\\operatorname{T}}\n  = \\,\n  \\begin{bmatrix}\n    1   \\\\\n    2\n  \\end{bmatrix}\n</math>\n\n*<math>\n  \\begin{bmatrix}\n    1 & 2  \\\\\n    3 & 4\n  \\end{bmatrix}^{\\operatorname{T}}\n  =\n  \\begin{bmatrix}\n    1 & 3  \\\\\n    2 & 4\n  \\end{bmatrix}\n</math>\n\n* <math>\n  \\begin{bmatrix}\n    1 & 2 \\\\\n    3 & 4 \\\\\n    5 & 6\n  \\end{bmatrix}^{\\operatorname{T}}\n  =\n  \\begin{bmatrix}\n     1 & 3 & 5\\\\\n     2 & 4 & 6\n  \\end{bmatrix}\n</math>\n\n== Properties ==\nFor matrices '''A''', '''B''' and scalar ''c'' we have the following properties of transpose:\n\n{{ordered list\n|1= <math>\\left(\\mathbf{A}^\\operatorname{T} \\right)^\\operatorname{T} = \\mathbf{A}.</math>\n:The operation of taking the transpose is an [[Involution (mathematics)|involution]] (self-[[Inverse matrix|inverse]]).\n|2= <math>\\left(\\mathbf{A} + \\mathbf{B}\\right)^\\operatorname{T} = \\mathbf{A}^\\operatorname{T} + \\mathbf{B}^\\operatorname{T}.</math>\n:The transpose respects addition.\n|3= <math>\\left(\\mathbf{A B}\\right)^\\operatorname{T} = \\mathbf{B}^\\operatorname{T} \\mathbf{A}^\\operatorname{T}.</math>\n:Note that the order of the factors reverses. From this one can deduce that a [[square matrix]] '''A''' is [[Invertible matrix|invertible]] if and only if '''A'''<sup>T</sup> is invertible, and in this case we have ('''A'''<sup>−1</sup>)<sup>T</sup> = ('''A'''<sup>T</sup>)<sup>−1</sup>.  By induction this result extends to the general case of multiple matrices, where we find that ('''A'''<sub>1</sub>'''A'''<sub>2</sub>...'''A'''<sub>''k''−1</sub>'''A'''<sub>''k''</sub>)<sup>T</sup>&nbsp;=&nbsp;'''A'''<sub>''k''</sub><sup>T</sup>'''A'''<sub>''k''−1</sub><sup>T</sup>…'''A'''<sub>2</sub><sup>T</sup>'''A'''<sub>1</sub><sup>T</sup>.\n|4= <math>\\left(c \\mathbf{A}\\right)^\\operatorname{T} = c \\mathbf{A}^\\operatorname{T}.</math>\n:The transpose of a [[Scalar (mathematics)|scalar]] is the same scalar. Together with (2), this states that the transpose is a [[linear map]] from the [[Vector space|space]] of {{nowrap|''m'' × ''n''}} matrices to the space of all {{nowrap|''n'' × ''m''}} matrices.\n|5= <math>\\det\\left(\\mathbf{A}^\\operatorname{T}\\right) = \\det(\\mathbf{A}).</math>\n:The [[determinant]] of a square matrix is the same as the determinant of its transpose.\n|6= The [[dot product]] of two column [[vector space|vector]]s '''a''' and '''b''' can be computed as the single entry of the matrix product:\n:<math>\\left[ \\mathbf{a} \\cdot \\mathbf{b} \\right] = \\mathbf{a}^{\\operatorname{T}} \\mathbf{b},</math>\nwhich is written as '''a'''<sub>''i''</sub> '''b'''<sup>''i''</sup> in [[Einstein summation convention]].\n|7= If '''A''' has only real entries, then '''A'''<sup>T</sup>'''A''' is a [[positive-semidefinite matrix]].\n|8= <math>\\left(\\mathbf{A}^\\operatorname{T}\\right)^{-1} = \\left(\\mathbf{A}^{-1}\\right)^\\operatorname{T}.</math>\n: The transpose of an invertible matrix is also invertible, and its inverse is the transpose of the inverse of the original matrix.  The notation '''A'''<sup>−T</sup> is sometimes used to represent either of these equivalent expressions.\n|9= If '''A''' is a square matrix, then its [[Eigenvalue, eigenvector and eigenspace|eigenvalues]] are equal to the eigenvalues of its transpose, since they share the same [[characteristic polynomial]].\n}}\n\n== Special transpose matrices ==\nA square matrix whose transpose is equal to itself is called a [[symmetric matrix]]; that is, '''A''' is symmetric if\n:<math>\\mathbf{A}^{\\operatorname{T}} = \\mathbf{A}.</math>\n\nA square matrix whose transpose is equal to its negative is called a [[skew-symmetric matrix]]; that is, '''A''' is skew-symmetric if\n:<math>\\mathbf{A}^{\\operatorname{T}} = -\\mathbf{A}.</math>\n\nA square [[complex number|complex]] matrix whose transpose is equal to the matrix with every entry replaced by its [[complex conjugate]] (denoted here with an overline) is called a [[Hermitian matrix]] (equivalent to the matrix being equal to its [[conjugate transpose]]); that is, '''A''' is Hermitian if\n:<math>\\mathbf{A}^{\\operatorname{T}} = \\overline{\\mathbf{A}}.</math>\n\nA square [[complex number|complex]] matrix whose transpose is equal to the negation of its complex conjugate is called a [[skew-Hermitian matrix]]; that is, '''A''' is skew-Hermitian if\n:<math>\\mathbf{A}^{\\operatorname{T}} = -\\overline{\\mathbf{A}}.</math>\n\nA square matrix whose transpose is equal to its [[Inverse matrix|inverse]] is called an [[orthogonal matrix]]; that is, '''A''' is orthogonal if\n:<math>\\mathbf{A}^{\\operatorname{T}} = \\mathbf{A}^{-1}.</math>\n\nA square complex matrix whose transpose is equal to its conjugate inverse is called a [[unitary matrix]]; that is, '''A''' is unitary if\n:<math>\\mathbf{A}^{\\operatorname{T}} = \\overline{\\mathbf{A}^{-1}}.</math>\n\n==Products==\nIf '''A''' is an ''m'' × ''n'' matrix and '''A'''<sup>T</sup> is its transpose, then the result of [[matrix multiplication]] with these two matrices gives two square matrices: '''A A'''<sup>T</sup> is ''m'' × ''m'' and '''A'''<sup>T</sup> '''A''' is ''n'' × ''n''. Furthermore, these products are [[symmetric matrices]]. Indeed, the matrix product '''A A'''<sup>T</sup> has entries that are the [[inner product]] of a row of '''A''' with a column of '''A'''<sup>T</sup>. But the columns of '''A'''<sup>T</sup> are the rows of '''A''', so the entry corresponds to the inner product of two rows of '''A'''. If ''p''<sub>i j</sub> is the entry of the product, it is obtained from rows i and j in '''A'''. The entry ''p''<sub>j i</sub> is also obtained from these rows, thus ''p''<sub>i j</sub> = ''p''<sub>j i</sub>, and the product matrix (''p''<sub>i j</sub>) is symmetric. Similarly, the product '''A'''<sup>T</sup> '''A''' is a symmetric matrix.\n\nA quick proof of the symmetry of '''A A'''<sup>T</sup> results from the fact that it is its own transpose:\n:<math>\\left(\\mathbf{A} \\mathbf{A}^\\operatorname{T}\\right)^\\operatorname{T} = \\left(\\mathbf{A}^\\operatorname{T}\\right)^\\operatorname{T} \\mathbf{A}^\\operatorname{T}= \\mathbf{A} \\mathbf{A}^\\operatorname{T} .</math><ref>[[Gilbert Strang]] (2006) ''Linear Algebra and its Applications'' 4th edition, page 51, Thomson [[Brooks/Cole]] {{ISBN|0-03-010567-6}}</ref>\n\n== Transpose of a linear map ==\n{{see also|Transpose of a linear map}}\n\nThe transpose may be defined more generally:\n\nIf {{nowrap|1=''f'' : ''V'' → ''W''}} is a [[linear map]] between right ''R''-[[Module (mathematics)|module]]s ''V'' and ''W'' with respective [[dual module]]s ''V''<sup>∗</sup> and ''W''<sup>∗</sup>, the ''transpose'' of ''f'' is the linear map\n:<math>{}^\\operatorname{T} f : W^* \\to V^* : \\varphi \\mapsto \\varphi \\circ f .</math>\nEquivalently, the transpose <sup>t</sup>''f'' is defined by the relation\n:<math>\\left\\langle \\varphi , f ( v ) \\right\\rangle = \\left\\langle {}^\\operatorname{T} f ( \\varphi ) , v \\right\\rangle \\quad \\forall \\varphi \\in W^* , v \\in V ,</math>\nwhere {{nowrap|{{langle}}·,·{{rangle}}}} is the [[natural pairing]] of each dual space with its respective vector space. This definition also applies unchanged to left modules and to vector spaces.<ref>{{citation |author=Bourbaki |title=Algebra I |section=II §2.5 }}</ref>\n\nThe definition of the transpose may be seen to be independent of any bilinear form on the vector spaces, unlike the adjoint ([[#Adjoint of a bilinear map|below]]).\n\nIf the matrix ''A'' describes a linear map with respect to [[basis (linear algebra)|bases]] of ''V'' and ''W'', then the matrix ''A''<sup>T</sup> describes the transpose of that linear map with respect to the [[Dual basis |dual bases]].\n\n=== Transpose of a bilinear form ===\n{{main|Bilinear form}}\n\nEvery linear map to the dual space {{math|1=''f'' : ''V'' → ''V''<sup>∗</sup>}} defines a bilinear form {{math|1=''B'' : ''V'' × ''V'' → ''F''}}, with the relation {{math|1=''B''(''v'', ''w'') = ''f''(''v'')(''w'')}}. By defining the transpose of this bilinear form as the bilinear form <sup>t</sup>''B'' defined by the transpose {{math|1=<sup>t</sup>''f'' : ''V''<sup>∗∗</sup> → ''V''<sup>∗</sup>}} i.e. {{math|1=<sup>t</sup>''B''(''w'', ''v'') = <sup>t</sup>''f''(Ψ(''w''))(''v'')}}, we find that {{math|1=''B''(''v'', ''w'') = <sup>t</sup>''B''(''w'', ''v'')}}. Here, Ψ is the natural homomorphism {{math|''V'' → ''V''<sup>∗∗</sup>}} into the [[double dual]].\n\n=== Adjoint ===\n{{distinguish|Hermitian adjoint}}\n\nIf the vector spaces ''V'' and ''W'' have respectively [[nondegenerate form|nondegenerate]] [[bilinear form]]s ''B''<sub>''V''</sub> and ''B''<sub>''W''</sub>, a concept known as the ''adjoint,'' which is closely related to the transpose'','' may be defined:\n\nIf {{nowrap|1=''f'' : ''V'' → ''W''}} is a [[linear map]] between [[vector space]]s ''V'' and ''W'', we define ''g'' as the ''adjoint'' of ''f'' if {{nowrap|1=''g'' : ''W'' → ''V''}} satisfies\n:<math>B_V\\big(v, g(w)\\big) = B_W\\big(f(v), w\\big) \\quad \\forall v \\in V, w \\in W.</math>\n\nThese bilinear forms define an [[isomorphism]] between ''V'' and ''V''<sup>∗</sup>, and between ''W'' and ''W''<sup>∗</sup>, resulting in an isomorphism between the transpose and adjoint of ''f''. The matrix of the adjoint of a map is the transposed matrix only if the [[basis (linear algebra)|bases]] are orthonormal with respect to their bilinear forms. In this context, many authors use the term transpose to refer to the adjoint as defined here.\n\nThe adjoint allows us to consider whether {{nowrap|1=''g'' : ''W'' → ''V''}} is equal to {{nowrap|1=''f''<sup> −1</sup> : ''W'' → ''V''}}. In particular, this allows the [[orthogonal group]] over a vector space ''V'' with a quadratic form to be defined without reference to matrices (nor the components thereof) as the set of all linear maps {{nowrap|''V'' → ''V''}} for which the adjoint equals the inverse.\n\nOver a complex vector space, one often works with [[sesquilinear form]]s (conjugate-linear in one argument) instead of bilinear forms. The [[Hermitian adjoint]] of a map between such spaces is defined similarly, and the matrix of the Hermitian adjoint is given by the conjugate transpose matrix if the bases are orthonormal.\n\n== Implementation of matrix transposition on computers ==\n{{See also|In-place matrix transposition}}\n[[File:Row_and_column_major_order.svg|thumb|upright|Illustration of row- and column-major order]]\nOn a [[computer]], one can often avoid explicitly transposing a matrix in [[Random access memory|memory]] by simply accessing the same data in a different order. For example, [[software libraries]] for [[linear algebra]], such as [[BLAS]], typically provide options to specify that certain matrices are to be interpreted in transposed order to avoid the necessity of data movement.\n\nHowever, there remain a number of circumstances in which it is necessary or desirable to physically reorder a matrix in memory to its transposed ordering. For example, with a matrix stored in [[row-major order]], the rows of the matrix are contiguous in memory and the columns are discontiguous. If repeated operations need to be performed on the columns, for example in a [[fast Fourier transform]] algorithm, transposing the matrix in memory (to make the columns contiguous) may improve performance by increasing [[memory locality]].\n\nIdeally, one might hope to transpose a matrix with minimal additional storage. This leads to the problem of transposing an ''n''&nbsp;×&nbsp;''m'' matrix [[in-place]], with [[Big O notation|O(1)]] additional storage or at most storage much less than ''mn''. For ''n''&nbsp;≠&nbsp;''m'', this involves a complicated [[permutation]] of the data elements that is non-trivial to implement in-place. Therefore, efficient [[in-place matrix transposition]] has been the subject of numerous research publications in [[computer science]], starting in the late 1950s, and several algorithms have been developed.\n\n== See also ==\n* [[Conjugate transpose]]\n* [[Moore–Penrose pseudoinverse]]\n* [[Projection (linear algebra)]]\n\n== References ==\n{{reflist}}\n\n== Further reading ==\n* {{cite book |first=Jared M. |last=Maruskin |title=Essential Linear Algebra |location=San José |publisher=Solar Crest |year=2012 |isbn=978-0-9850627-3-6 |url={{Google books |plainurl=yes |id=aOF3-hx3u1kC |page=122 }} |pages=122–132 }}\n* {{cite book |first=Jacob T. |last=Schwartz |title=Introduction to Matrices and Vectors |location=Mineola |publisher=Dover |year=2001 |isbn=0-486-42000-0 |url={{Google books |plainurl=yes |id=fMG3y1z9Jw0C |page=126}} |pages=126–132 }}\n\n== External links ==\n* Gilbert Strang (Spring 2010) [https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/ Linear Algebra] from MIT Open Courseware\n* [http://mathworld.wolfram.com/Transpose.html Transpose], mathworld.wolfram.com\n\n{{linear algebra}}\n{{Tensors}}\n\n[[Category:Matrices]]\n[[Category:Abstract algebra]]\n[[Category:Linear algebra]]"
    },
    {
      "title": "Wheel theory",
      "url": "https://en.wikipedia.org/wiki/Wheel_theory",
      "text": "'''Wheels''' are a type of [[universal algebra|algebra]] where division is always defined.  In particular, [[division by zero]] is meaningful. The real numbers can be extended to a wheel, as can any [[commutative ring]].\n\nThe [[Riemann sphere]] can also be extended to a wheel by adjoining an element <math>\\bot</math>, where <math>0/0=\\bot</math>. The Riemann sphere is an extension of the [[complex plane]] by an element <math>\\infty</math>, where <math>z/0=\\infty</math> for any complex <math>z\\neq 0</math>. However, <math>0/0</math> is still undefined on the Riemann sphere, but is defined in its extension to a wheel.\n\nThe term ''wheel'' is inspired by the topological picture <math>\\odot</math> of the [[projective line]] together with an extra point <math>\\bot = 0/0</math>.{{sfn|Carlström|2004}}\n\n== Definition ==\nA wheel is an [[algebraic structure]] <math>( W, 0, 1, +, \\cdot, / )</math>, satisfying:\n\n* Addition and multiplication are [[commutative]] and [[associative]], with <math>0</math> and <math>1</math> as their respective identities.\n* <math>//x = x</math>\n* <math>/(xy) = /y/x</math>\n* <math>xz + yz = (x + y)z + 0z</math>\n* <math>(x + yz)/y = x/y + z + 0y</math>\n* <math>0\\cdot 0 = 0</math>\n* <math>(x+0y)z = xz + 0y</math>\n* <math>/(x+0y) = /x + 0y</math>\n* <math>0/0 + x = 0/0</math>\n\n== Algebra of wheels ==\nWheels replace the usual division as a [[binary operation|binary operator]] with multiplication, with a [[unary operation|unary operator]] applied to one argument <math>/x</math> similar (but not identical) to the [[multiplicative inverse]] <math>x^{-1}</math>, such that <math>a/b</math> becomes shorthand for <math>a \\cdot /b = /b \\cdot a</math>, and modifies the rules of [[algebra]] such that\n\n* <math>0x \\neq 0</math> in the general case\n* <math>x - x \\neq 0</math> in the general case\n* <math>x/x \\neq 1</math> in the general case, as <math>/x</math> is not the same as the [[multiplicative inverse]] of <math>x</math>.\n\nIf there is an element <math>a</math> such that <math>1 + a = 0</math>, then we may define negation by <math>-x = ax</math> and <math>x - y = x + (-y)</math>.\n\nOther identities that may be derived are\n* <math>0x + 0y = 0xy</math>\n* <math>x-x = 0x^2</math>\n* <math>x/x = 1 + 0x/x</math>\n\nAnd, for <math>x</math> with <math>0x = 0</math> and <math>0/x = 0</math>, we get the usual\n* <math>x-x = 0</math>\n* <math>x/x = 1</math>\n\nIf negation can be defined as above then the subset <math>\\{x\\mid 0x=0\\}</math> is a [[commutative ring]], and every commutative ring is such a subset of a wheel. If <math>x</math> is an invertible element of the commutative ring, then <math>x^{-1}=/x</math>. Thus, whenever <math>x^{-1}</math> makes sense, it is equal to <math>/x</math>, but the latter is always defined, even when <math>x=0</math>.\n\n== Wheel of fractions ==\nLet <math>A</math> be a commutative ring, and let <math>S</math> be a multiplicative [[monoid|submonoid]] of <math>A</math>.  Define the congruence relation <math>\\sim_S</math> on <math>A \\times A</math> via\n:<math>(x_1,x_2)\\sim_S(y_1,y_2)</math> means that there exist <math>s_x,s_y\\in S</math> such that <math>(s_x x_1,s_x x_2)=(s_y y_1,s_y y_2)</math>.\nDefine the wheel of fractions of <math>A</math> with respect to <math>S</math> as the quotient <math>A \\times A~/ \\sim_S</math> (and denoting the equivalence class containing <math>(x_1,x_2)</math> as <math>[x_1,x_2]</math>) with the operations\n:<math>0 = [0_A,1_A]</math> {{in5|10}}(additive identity)\n:<math>1 = [1_A,1_A]</math> {{in5|10}}(multiplicative identity)\n:<math>/[x_1,x_2] = [x_2,x_1]</math> {{in5|10}}(reciprocal operation)\n:<math>[x_1,x_2] + [y_1,y_2] = [x_1y_2 + x_2 y_1,x_2 y_2]</math> {{in5|10}}(addition operation)\n:<math>[x_1,x_2] \\cdot [y_1,y_2] = [x_1 y_1,x_2 y_2]</math> {{in5|10}}(multiplication operation)\n\n== Citations ==\n{{reflist}}\n\n== References ==\n*{{citation |year=1997 |last=Setzer|first=Anton |title=Wheels |url=http://www.cs.swan.ac.uk/~csetzer/articles/wheel.pdf }} (a draft)\n*{{citation |year=2004 |last=Carlström|first=Jesper |title=Wheels – On Division by Zero |journal=Mathematical Structures in Computer Science |doi=10.1017/S0960129503004110 |volume=14 |issue=1 |publisher=[[Cambridge University Press]] |pages=143–184 }} (also available online [http://www2.math.su.se/reports/2001/11/ here]).\n\n\n\n{{algebra-stub}}\n[[Category:Abstract algebra]]"
    },
    {
      "title": "Word problem (mathematics)",
      "url": "https://en.wikipedia.org/wiki/Word_problem_%28mathematics%29",
      "text": "{{About|algorithmic word problems in mathematics and computer science|other uses|Word problem (disambiguation){{!}}Word problem}}\n{{technical|date=December 2014}}\n\nIn [[mathematics]] and [[computer science]], a '''word problem''' for a set S with respect to a system of finite encodings of its elements is the [[decision problem|algorithmic problem of deciding]] whether two given representatives represent the same element of the set. The problem is commonly encountered in [[abstract algebra]], where given a presentation of an algebraic structure by [[generating set|generators]] and [[relator]]s, the problem is to determine if two expressions represent the same element; a prototypical example is the [[word problem for groups]]. Less formally, the word problem in an algebra is: given a set of identities ''E'', and two expressions ''x'' and ''y'', is it possible to transform ''x'' into ''y'' using the identities in ''E'' as [[rewriting]] rules in both directions? While answering this question may not seem hard, the remarkable (and [[deep result|deep]]) result that emerges, in many important cases, is that the [[undecidable problem|problem is undecidable]].\n\nMany, if not most all, undecidable problems in mathematics can be posed as word problems; see the [[list of undecidable problems]] for many examples.\n\n== Background and motivation ==\n\nMany occasions arise in mathematics where one wishes to use a finite amount of information to describe an element of a (typically infinite) set. This issue is particularly apparent in computational mathematics. Traditional models of computation (such as the [[Turing machine]]) have storage capacity which is unbounded, so it is in principle possible to perform computations with the elements of infinite sets. On the other hand, since the amount of storage space in use at any one time is finite, we need each element to have a finite  representation.\n\nFor various reasons, it is not always possible or desirable to use a system of ''unique'' encodings, that is, one in which every element has a single encoding. When using an encoding system without uniqueness, the question naturally arises of whether there is an algorithm which, given as input two encodings, decides whether they represent the same element. Such an algorithm is called a ''solution to the word problem'' for the encoding system.\n\n== The word problem in combinatorial calculus ==\n{{main | Combinatory logic#Undecidability of combinatorial calculus}}\nThe simplest example of an undecidable word problem occurs in [[combinatory logic]]: when are two strings of combinators equivalent? Because combinators encode all possible [[Turing machine]]s, and the equivalence of two Turing machines is undecidable, it follows that the equivalence of two strings of combinators is undecidable.\n\nLikewise, one has essentially the same problem in (untyped) [[lambda calculus]]: given two distinct lambda expressions, there is no algorithm which can discern whether they are equivalent or not; [[Lambda calculus#Undecidability of equivalence|equivalence is undecidable]]. For several typed variants of the lambda calculus, equivalence is decidable by comparison of normal forms.\n\n== The word problem in universal algebra ==\n\nIn [[algebra]], one often studies infinite algebras which are generated (under the [[finitary]] operations of the algebra) by finitely many elements. In this case, the elements of the algebra have a natural system of finite encoding as expressions in terms of the generators and operations. The word problem here is thus to determine, given two such expressions, whether they represent the same element of the algebra.\n\nRoughly speaking, the word problem in an algebra is: given a set ''E'' of identities (an [[equational theory]]), and two [[Term (logic)|terms]] ''s'' and ''t'', is it possible to transform ''s'' into ''t'' using the identities in ''E'' as [[rewriting]] rules in both directions?.<ref>[[Franz Baader]] and [[Tobias Nipkow]], ''[https://books.google.com/books?id=N7BvXVUCQk8C&printsec=frontcover#v=snippet&q=%22word%20problem%22&f=false Term Rewriting and All That]'', Cambridge University Press, 1998, p. 5</ref> \n<!--- wrong: The act of discovering such equivalences is known as [[unification (computer science)|unification]]. --->\nA proper extension of the ''word problem'' is known as the ''[[unification (computer science)|unification problem]]'' (a.k.a. the ''equation solving problem'').\nWhile the former asks whether two terms ''are'' equal, the latter asks whether they have ''instances'' that are equal.\nAs a common example, \"<math>2 + 3 \\stackrel{?}{=} 8 + (-3)</math>\" is a word problem in the [[Integer#Algebraic properties|integer group ℤ]],\nwhile \"<math>2 + x \\stackrel{?}{=} 8 + (-x)</math>\" is a unification problem in the same group; since the former terms happen to be equal in ℤ, the latter problem has the [[substitution (logic)|substitution]] <math>\\{x \\mapsto 3\\}</math> as a solution.\n\nSubstitutions may be ordered into a [[partial order]], thus, unification is the act of finding a [[join and meet|join]] on a [[lattice (order)|lattice]]. {{clarify|reason=In which lattice precisely? Unification maps two terms to a substitution, while term lattice meet maps two terms to a term, and substitution lattice meet maps two substitutions to a substitution.|date=July 2013}}\nIn this sense, the word problem on a lattice has a solution, namely, the set of all equivalent words is the [[free lattice]].{{clarify|reason=The solution of a word problem was explained above to be an algorithm, not a set.|date=June 2013}}\n\nOne of the most deeply studied cases of the word problem is in the theory of [[semigroup]]s and [[group (mathematics)|group]]s. \nThere are [[word problem for groups|many groups for which the word problem]] is not [[Decidability (logic)|decidable]], in that there is no Turing machine that can determine the equivalence of two ''arbitrary'' words in a finite time.\n\nThe word problem on [[ground term]]s is not decidable.<ref>Yuri Matijasevich, (1967) \"Simple examples of undecidable associative calculi\", ''Soviet Mathematics - Doklady'' '''8'''(2) pp 555–557.</ref> {{clarify|reason=In which algebra? It is certainly decidable in ℤ|date=June 2013}}\n\nThe word problem on free [[Heyting algebra]]s is difficult.<ref>Peter T. Johnstone, ''Stone Spaces'', (1982) Cambridge University Press, Cambridge, {{isbn|0-521-23893-5}}. ''(See chapter 1, paragraph 4.11)''</ref>  \nThe only known results are that the free Heyting algebra on one generator is infinite, and that the free [[complete Heyting algebra]] on one generator exists (and has one more element than the free Heyting algebra).\n\n==Example: A term rewriting system to decide the word problem in the free group==\n\nBläsius and Bürckert \n<ref>{{cite book| title=Deduktionsssysteme| year=1992| pages=291| publisher=Oldenbourg| editor=K. H. Bläsius and H.-J. Bürckert| accessdate=30 June 2013}}; here: p.126, 134</ref>\ndemonstrate the [[Knuth–Bendix algorithm]] on an axiom set for groups. \nThe algorithm yields a [[Confluence (abstract rewriting)|confluent]] and [[Abstract rewriting system#Termination and convergence|noetherian]] [[rewrite system#Term rewriting systems|term rewrite system]] that transforms every term into a unique [[Normal form (abstract rewriting)|normal form]].<ref>Apply rules in any order to a term, as long as possible; the result doesn't depend on the order; it is the term's normal form.</ref> \nThe rewrite rules are numbered incontiguous since some rules became redundant and were deleted during the algorithm run.\nThe equality of two terms follows from the axioms if and only if both terms are transformed into literally the same normal form term. For example, the terms\n:<math>((a^{-1} \\cdot a) \\cdot (b \\cdot b^{-1}))^{-1} \\stackrel{R2}{\\rightsquigarrow} (1 \\cdot (b \\cdot b^{-1}))^{-1} \\stackrel{R13}{\\rightsquigarrow} (1 \\cdot 1)^{-1} \\stackrel{R1}{\\rightsquigarrow} 1 ^{-1} \\stackrel{R8}{\\rightsquigarrow} 1</math>, and\n:<math>b \\cdot ((a \\cdot b)^{-1} \\cdot a) \\stackrel{R17}{\\rightsquigarrow} b \\cdot ((b^{-1} \\cdot a^{-1}) \\cdot a) \\stackrel{R3}{\\rightsquigarrow} b \\cdot (b^{-1} \\cdot (a^{-1} \\cdot a)) \\stackrel{R2}{\\rightsquigarrow} b \\cdot (b^{-1} \\cdot 1) \\stackrel{R11}{\\rightsquigarrow} b \\cdot b^{-1} \\stackrel{R13}{\\rightsquigarrow} 1</math>\nshare the same normal form, viz. <math>1</math>; therefore both terms are equal in every group.\nAs another example, the term <math>1 \\cdot (a \\cdot b)</math> and <math>b \\cdot (1 \\cdot a)</math> has the normal form <math>a \\cdot b</math> and <math>b \\cdot a</math>, respectively. Since the normal forms are literally different, the original terms cannot be equal in every group. In fact, they are usually different in [[abelian group|non-abelian groups]].\n\n{| style=\"border: 1px solid grey; float: left; margin: 1em 1em;\"\n|+ Group axioms used in Knuth–Bendix completion\n|- \n| '''A1''' || <math>1 \\cdot x</math> || <math>= x</math>\n|-\n| '''A2''' || <math>x^{-1} \\cdot x</math> || <math>= 1</math>\n|-\n| '''A3''' &nbsp; &nbsp; || <math>(x \\cdot y) \\cdot z</math> || <math>= x \\cdot (y \\cdot z)</math>\n|}\n{| style=\"border: 1px solid grey; float: left; margin: 1em 1em;\"\n|+ Term rewrite system obtained from Knuth–Bendix completion\n|-\n| '''R1''' || <math>1 \\cdot x </math> || <math>\\rightsquigarrow  x</math>\n|-\n| '''R2''' || <math>x^{-1} \\cdot x </math> || <math>\\rightsquigarrow  1</math>\n|-\n| '''R3''' || <math>(x \\cdot y) \\cdot z</math> || <math>\\rightsquigarrow x \\cdot (y \\cdot z)</math>\n|-\n| '''R4''' || <math>x^{-1} \\cdot (x \\cdot y) </math> || <math>\\rightsquigarrow  y</math>\n|-\n| '''R8''' || <math>1^{-1} </math> || <math>\\rightsquigarrow  1</math>\n|-\n| '''R11''' || <math>x \\cdot 1 </math> || <math>\\rightsquigarrow  x</math>\n|-\n| '''R12''' || <math>(x^{-1})^{-1} </math> || <math>\\rightsquigarrow  x</math>\n|-\n| '''R13''' || <math>x \\cdot x^{-1} </math> || <math>\\rightsquigarrow  1</math>\n|-\n| '''R14''' || <math>x \\cdot (x^{-1} \\cdot y) </math> || <math>\\rightsquigarrow  y</math>\n|-\n| '''R17'''  &nbsp; &nbsp; || <math>(x \\cdot y)^{-1} </math> || <math>\\rightsquigarrow  y^{-1} \\cdot x^{-1}</math>\n|}\n{{clear}}\n\n==See also==\n* [[Munn tree]]\n* [[Word problem for groups]]\n* [[Knuth–Bendix completion algorithm]]\n* [[Unification (computer science)]]\n\n==References==\n{{reflist}}\n\n[[Category:Abstract algebra]]\n[[Category:Combinatorics on words]]\n[[Category:Rewriting systems]]\n[[Category:Computational problems]]"
    },
    {
      "title": "Yoneda product",
      "url": "https://en.wikipedia.org/wiki/Yoneda_product",
      "text": "In algebra, the '''Yoneda product''' (named after [[Nobuo Yoneda]]) is the pairing between [[Ext group]]s of modules:\n:<math>\\operatorname{Ext}^n(M, N) \\otimes \\operatorname{Ext}^m(L, M) \\to \\operatorname{Ext}^{n+m}(L, N)</math>\ninduced by\n:<math>\\operatorname{Hom}(N, M) \\otimes \\operatorname{Hom}(M, L) \\to \\operatorname{Hom}(N, L),\\, f \\otimes g \\mapsto g \\circ f.</math>\n\nSpecifically, for an element <math>\\xi \\in \\operatorname{Ext}^n(M, N) </math>, thought of as an extension \n:<math>\\xi :  0 \\rightarrow N \\rightarrow E_0 \\rightarrow \\cdots \\rightarrow E_{n-1} \\rightarrow M \\rightarrow 0 </math>, \nand similarly \n:<math>\\rho : 0 \\rightarrow M \\rightarrow F_0\\rightarrow \\cdots \\rightarrow F_{m-1} \\rightarrow L \\rightarrow 0 \\in \\operatorname{Ext}^m(L, M)</math>, \nwe form the Yoneda (cup) product \n:<math>\\xi \\smile \\rho : 0 \\rightarrow N \\rightarrow E_0 \\rightarrow \\cdots \\rightarrow E_{n-1} \\rightarrow F_0 \\rightarrow \\cdots \\rightarrow F_{m-1} \\rightarrow L \\rightarrow 0 \\in \\operatorname{Ext}^{m + n}(L, N)</math>. \n\nNote that the middle map <math>E_{n-1} \\rightarrow F_0</math> factors through the given maps to <math>M</math>. \n\nWe extend this definition to include <math>m, n = 0</math> using the usual functoriality of the <math>\\operatorname{Ext}^*(\\_,\\_)</math> groups. \n\n== References ==\n*{{cite web|first=J. Peter|last=May|authorlink=J. Peter May|url=http://www.math.uchicago.edu/~may/MISC/TorExt.pdf |title=Notes on Tor and Ext}}\n\n== External links ==\n*http://mathoverflow.net/questions/121140/universality-of-ext-functor-using-yoneda-extensions\n\n{{algebra-stub}}\n\n[[Category:Abstract algebra]]"
    },
    {
      "title": "Zero divisor",
      "url": "https://en.wikipedia.org/wiki/Zero_divisor",
      "text": "{{Use American English|date = March 2019}}\n{{Short description|Ring element such that can be multiplied by a non-zero element to equal 0}}\nIn [[abstract algebra]], an [[element (mathematics)|element]] {{math|''a''}} of a [[ring (algebra)|ring]] {{math|''R''}} is called a '''left zero divisor''' if there exists a nonzero {{math|''x''}} such that {{math|1=''ax'' = 0}},<ref>{{citation |author= [[N. Bourbaki]] |title=Algebra I, Chapters 1–3 |page=98 |publisher=Springer-Verlag |year=1989}}</ref> or equivalently if the map from {{math|''R''}} to {{math|''R''}} that sends {{math|''x''}} to {{math|''ax''}} is not injective.{{efn|1=Since the map is not injective, we have {{math|1=''ax'' = ''ay''}}, in which {{math|''x''}} differs from {{math|''y''}}, and thus {{math|1=''a''(''x'' − ''y'') = 0}}.}}  Similarly, an [[element (mathematics)|element]] {{math|''a''}} of a ring is called a '''right zero divisor''' if there exists a nonzero {{math|''y''}} such that {{math|1=''ya'' = 0}}.  This is a partial case of [[divisibility (ring theory)|divisibility]] in rings.  An element that is a left or a right zero divisor is simply called a '''zero divisor'''.<ref>{{citation |author= Charles Lanski |year=2005 |title=Concepts in Abstract Algebra |publisher=American Mathematical Soc. |page=342 }}</ref> An element&nbsp;{{math|''a''}} that is both a left and a right zero divisor is called a '''two-sided zero divisor''' (the nonzero {{math|''x''}} such that {{math|1=''ax'' = 0}} may be different from the nonzero {{math|''y''}} such that {{math|1=''ya'' = 0}}). If the [[commutative ring|ring is commutative]], then the left and right zero divisors are the same.\n\nAn element of a ring that is not a zero divisor is called '''regular''', or a '''non-zero-divisor'''.  A zero divisor that is nonzero is called a '''nonzero zero divisor''' or a '''nontrivial zero divisor'''. If there are no nontrivial zero divisors in {{math|''R''}}, then {{math|''R''}} is a [[domain (ring theory)|domain]].\n\n== Examples ==\n<!-- It was valid, but nobody uses specifically Z×Z as a ring and hence, nobody cares about its zero divisors. In any way, generalized with the direct product example below. -->\n* In the [[modular arithmetic|ring <math>\\mathbb{Z}/4\\mathbb{Z}</math>]], the residue class <math>\\overline{2}</math> is a zero divisor since <math>\\overline{2} \\times \\overline{2}=\\overline{4}=\\overline{0}</math>.\n* The only zero divisor of the ring <math>\\mathbb{Z}</math> of [[integer]]s is <math>0</math>.\n* A [[nilpotent]] element of a nonzero ring is always a two-sided zero divisor.\n* An [[idempotent element (ring theory)|idempotent element]] <math>e\\ne 1</math> of a ring is always a two-sided zero divisor, since <math>e(1-e)=0=(1-e)e</math>.\n* Examples of zero divisors in the [[matrix ring|ring of <math>2\\times 2</math> matrices]] (over any [[zero ring|nonzero ring]]) are shown here:\n*:<math>\\begin{pmatrix}1&1\\\\2&2\\end{pmatrix}\\begin{pmatrix}1&1\\\\-1&-1\\end{pmatrix}=\\begin{pmatrix}-2&1\\\\-2&1\\end{pmatrix}\\begin{pmatrix}1&1\\\\2&2\\end{pmatrix}=\\begin{pmatrix}0&0\\\\0&0\\end{pmatrix} ,</math>\n*:<math>\\begin{pmatrix}1&0\\\\0&0\\end{pmatrix}\\begin{pmatrix}0&0\\\\0&1\\end{pmatrix}\n=\\begin{pmatrix}0&0\\\\0&1\\end{pmatrix}\\begin{pmatrix}1&0\\\\0&0\\end{pmatrix}\n=\\begin{pmatrix}0&0\\\\0&0\\end{pmatrix}</math>.\n*A [[product of rings|direct product]] of two or more [[zero ring|nonzero rings]] always has nonzero zero divisors.  For example, in <math>R_1 \\times R_2</math> with each <math>R_i</math> nonzero, <math>(1,0)(0,1) = (0,0)</math>, so <math>(1,0)</math> is a zero divisor.\n\n===One-sided zero-divisor===\n*Consider the ring of (formal) matrices <math>\\begin{pmatrix}x&y\\\\0&z\\end{pmatrix}</math> with <math>x,z\\in\\mathbb{Z}</math> and <math>y\\in\\mathbb{Z}/2\\mathbb{Z}</math>. Then <math>\\begin{pmatrix}x&y\\\\0&z\\end{pmatrix}\\begin{pmatrix}a&b\\\\0&c\\end{pmatrix}=\\begin{pmatrix}xa&xb+yc\\\\0&zc\\end{pmatrix}</math> and <math>\\begin{pmatrix}a&b\\\\0&c\\end{pmatrix}\\begin{pmatrix}x&y\\\\0&z\\end{pmatrix}=\\begin{pmatrix}xa&ya+zb\\\\0&zc\\end{pmatrix}</math>. If <math>x\\ne0\\ne z</math>, then <math>\\begin{pmatrix}x&y\\\\0&z\\end{pmatrix}</math> is a left zero divisor [[iff]] <math>x</math> is even, since <math>\\begin{pmatrix}x&y\\\\0&z\\end{pmatrix}\\begin{pmatrix}0&1\\\\0&0\\end{pmatrix}=\\begin{pmatrix}0&x\\\\0&0\\end{pmatrix}</math>, and it is a right zero divisor iff <math>z</math> is even for similar reasons. If either of <math>x,z</math> is <math>0</math>, then it is a two-sided zero-divisor.\n*Here is another example of a ring with an element that is a zero divisor on one side only.  Let <math>S</math> be the set of all [[sequence (mathematics)|sequences]] of integers <math>(a_1,a_2,a_3,...)</math>.  Take for the ring all [[additive map]]s from <math>S</math> to <math>S</math>, with [[pointwise]] addition and [[function composition|composition]] as the ring operations. (That is, our ring is <math>\\mathrm{End}(S)</math>, the '''[[endomorphism ring]]''' of the additive group <math>S</math>.) Three examples of elements of this ring are the '''right shift''' <math>R(a_1,a_2,a_3,...)=(0,a_1,a_2,...)</math>, the '''left shift''' <math>L(a_1,a_2,a_3,...)=(a_2,a_3,a_4,...)</math>, and the '''projection map''' onto the first factor <math>P(a_1,a_2,a_3,...)=(a_1,0,0,...)</math>.  All three of these [[additive map]]s are not zero, and the composites <math>LP</math> and <math>PR</math> are both zero, so <math>L</math> is a left zero divisor and <math>R</math> is a right zero divisor in the ring of additive maps from <math>S</math> to <math>S</math>.  However, <math>L</math> is not a right zero divisor and <math>R</math> is not a left zero divisor: the composite <math>LR</math> is the identity. Note also that <math>RL</math> is a two-sided zero-divisor since <math>RLP=0=PRL</math>, while <math>LR=1</math> is not in any direction.\n\n== Non-examples ==\n\n* The ring of integers [[modular arithmetic|modulo]] a [[prime number]] has no zero divisors other than 0.  Since every nonzero element is a [[unit (ring theory)|unit]], this ring is a [[finite field]].\n* More generally, a [[division ring]] has no zero divisors except 0.\n* A [[zero ring|nonzero]] [[commutative ring]] whose only zero divisor is 0 is called an [[integral domain]].\n\n== Properties ==\n\n* In the ring of {{mvar|n}}-by-{{mvar|n}} matrices over a [[field (mathematics)|field]], the left and right zero divisors coincide; they are precisely the [[singular matrix|singular matrices]]. In the ring of {{mvar|n}}-by-{{mvar|n}} matrices over an [[integral domain]], the zero divisors are precisely the matrices with [[determinant]] [[0 (number)|zero]].\n* Left or right zero divisors can never be [[unit (ring theory)|unit]]s, because if {{mvar|a}} is invertible and {{math|1=''ax'' = 0}}, then {{math|1=0 = ''a''<sup>−1</sup>0 = ''a''<sup>−1</sup>''ax'' = ''x''}}, whereas {{math|''x''}} must be nonzero.\n\n==Zero as a zero divisor==\n\nThere is no need for a separate convention regarding the case {{math|1=''a'' = 0}}, because the definition applies also in this case: \n* If {{math|''R''}} is a ring other than the [[zero ring]], then {{math|0}} is a (two-sided) zero divisor, because {{math|1=0 · a = 0 = a · 0}}, where {{math|a}} is a nonzero element of {{math|''R''}}.\n* If {{math|''R''}} is the [[zero ring]], in which {{math|1=0 = 1}}, then {{math|0}} is not a zero divisor, because there is no ''nonzero'' element that when multiplied by {{math|0}} yields {{math|0}}.\n\nSuch properties are needed in order to make the following general statements true:\n* In a nonzero commutative ring {{math|''R''}}, the set of non-zero-divisors is a [[multiplicative set]] in {{mvar|R}}.  (This, in turn, is important for the definition of the [[total quotient ring]].)  The same is true of the set of non-left-zero-divisors and the set of non-right-zero-divisors in an arbitrary ring, commutative or not.\n* In a commutative Noetherian ring {{math|''R''}}, the set of zero divisors is the union of the [[associated prime|associated prime ideals]] of {{math|''R''}}.\n\nSome references choose to exclude {{math|0}} as a zero divisor by convention, but then they must introduce exceptions in the two general statements just made.\n\n==Zero divisor on a module==\nLet {{mvar|R}} be a commutative ring, let {{mvar|M}} be an {{mvar|R}}-module, and let {{mvar|a}} be an element of {{mvar|R}}.  One says that {{mvar|a}} is '''{{mvar|M}}-regular''' if the multiplication by {{mvar|a}} map <math>M \\stackrel{a}\\to M</math> is injective, and that {{mvar|a}} is a '''zero divisor on {{mvar|M}}''' otherwise.<ref name=Matsumura-p12>{{citation |author=[[Hideyuki Matsumura]] |year=1980 |title=Commutative algebra, 2nd edition |publisher=The Benjamin/Cummings Publishing Company, Inc. |page=12}}</ref>  The set of {{mvar|M}}-regular elements is a [[multiplicative set]] in {{mvar|R}}.<ref name=Matsumura-p12/>\n\nSpecializing the definitions of \"{{mvar|M}}-regular\" and \"zero divisor on {{mvar|M}}\" to the case {{math|1=''M'' = ''R''}} recovers the definitions of \"regular\" and \"zero divisor\" given earlier in this article.\n\n== See also ==\n* [[Zero-product property]]\n* [[Glossary of commutative algebra]] (Exact zero divisor)\n* [[Zero-divisor graph]]\n\n== Notes ==\n{{notelist}}\n\n== References ==\n<references/>\n\n== Further reading ==\n* {{springer|title=Zero divisor|id=p/z099230}}\n* {{citation |year=2004 |title=Algebras, rings and modules |volume=Vol. 1 |publisher=Springer |isbn=1-4020-2690-0 |author1 = Michiel Hazewinkel|author2 = Nadiya Gubareni|author3=Nadezhda Mikhaĭlovna Gubareni |author4=Vladimir V. Kirichenko. |authorlink1=Michiel Hazewinkel }}\n* {{MathWorld |title=Zero Divisor |urlname=ZeroDivisor }}\n\n[[Category:Abstract algebra]]\n[[Category:Ring theory]]\n[[Category:0 (number)]]"
    },
    {
      "title": "Zero-product property",
      "url": "https://en.wikipedia.org/wiki/Zero-product_property",
      "text": "In [[algebra]], the '''zero-product property''' states that the product of two [[zero element|nonzero elements]] is nonzero.  In other words, it is the following assertion:\n<blockquote>\nIf <math>ab = 0</math>, then <math>a=0</math> or <math>b=0</math>.\n</blockquote>\nThe zero-product property is also known as the '''rule of zero product''', the '''null factor law''', the '''multiplication property of zero''' or the '''nonexistence of nontrivial [[zero divisor]]s.'''  All of the [[number system]]s studied in [[elementary mathematics]] &mdash; the [[integer]]s <math>\\mathbb{Z}</math>, the [[rational number]]s <math>\\mathbb{Q}</math>, the [[real number]]s <math>\\mathbb{R}</math>, and the [[complex number]]s <math>\\mathbb{C}</math> &mdash; satisfy the zero-product property.  In general, a [[Ring (mathematics)|ring]] which satisfies the zero-product property is called a [[Domain (ring theory)|domain]].\n\n==Algebraic context==\n\nSuppose <math>A</math> is an algebraic structure.  We might ask, does <math>A</math> have the zero-product property?  In order for this question to have meaning, <math>A</math> must have both additive structure and multiplicative structure.<ref group=note>There must be a notion of zero (the [[additive identity]]) and a notion of products, i.e., multiplication.</ref>  Usually one assumes that <math>A</math> is a [[Ring (mathematics)|ring]], though it could be something else, e.g. the set of nonnegative integers <math>\\{0,1,2,\\ldots\\}</math> with ordinary addition and multiplication, which is only a (commutative) [[semiring]].\n \nNote that if <math>A</math> satisfies the zero-product property, and if <math>B</math> is a subset of <math>A</math>, then <math>B</math> also satisfies the zero product property: if <math>a</math> and <math>b</math> are elements of <math>B</math> such that <math>ab=0</math>, then either <math>a=0</math> or <math>b=0</math> because <math>a</math> and <math>b</math> can also be considered as elements of <math>A</math>.\n\n==Examples==\n* A ring in which the zero-product property holds is called a [[Domain (ring theory)|domain]].  A [[Commutative ring|commutative]] domain with a [[Unit element|multiplicative identity]] element is called an [[integral domain]].  Any [[Field (abstract algebra)|field]] is an integral domain; in fact, any subring of a field is an integral domain (as long as it contains 1).  Similarly, any subring of a [[skew field]] is a domain.  Thus, the zero-product property holds for any subring of a skew field.\n* If <math>p</math> is a [[prime number]], then the ring of [[Modular arithmetic|integers modulo <math>p</math>]] has the zero-product property (in fact, it is a field).\n* The [[Gaussian integers]] are an [[integral domain]] because they are a subring of the complex numbers.\n* In the [[Skew field|strictly skew field]] of [[quaternions]], the zero-product property holds. This ring is not an integral domain, because the multiplication is not commutative.\n* The set of nonnegative integers <math>\\{0,1,2,\\ldots\\}</math> is not a ring (being instead a [[semiring]]), but it does satisfy the zero-product property.\n\n==Non-examples==\n\n* Let <math>\\mathbb{Z}_n</math> denote the ring of [[Modular arithmetic|integers modulo <math>n</math>]].  Then <math>\\mathbb{Z}_6</math> does not satisfy the zero product property: 2 and 3 are nonzero elements, yet <math>2 \\cdot 3 \\equiv 0 \\pmod{6}</math>.\n* In general, if <math>n</math> is a [[composite number]], then <math>\\mathbb{Z}_n</math> does not satisfy the zero-product property.  Namely, if <math>n = qm</math> where <math>0 < q,m < n</math>, then <math>m</math> and <math>q</math> are nonzero modulo <math>n</math>, yet <math>qm \\equiv 0 \\pmod{n}</math>.\n* The ring <math>\\mathbb{Z}^{2 \\times 2}</math> of 2 by 2 [[matrix (mathematics)|matrices]] with [[integer]] entries does not satisfy the zero-product property: if\n\n::<math>M = \\begin{pmatrix}1 & -1 \\\\ 0 & 0\\end{pmatrix}</math> and <math>N = \\begin{pmatrix}0 & 1 \\\\ 0 & 1\\end{pmatrix}</math>,\n\n:then\n\n::<math>MN = \\begin{pmatrix}1 & -1 \\\\ 0 & 0\\end{pmatrix} \\begin{pmatrix}0 & 1 \\\\ 0 & 1\\end{pmatrix} = \\begin{pmatrix}0 & 0 \\\\ 0 & 0\\end{pmatrix} = 0</math>,\n\n:yet neither <math>M</math> nor <math>N</math> is zero.\n\n* The ring of all [[function (mathematics)|function]]s <math>f: [0,1] \\to \\mathbb{R}</math>, from the [[unit interval]] to the [[real number]]s, has nontrivial zero divisors: there are pairs of functions which are not identically equal to zero yet whose product is the zero function.  In fact, it is not hard to construct, for any ''n'' &ge; 2, functions <math>f_1,\\ldots,f_n</math>, none of which is identically zero, such that <math>f_i \\, f_j</math> is identically zero whenever <math>i \\neq j</math>.\n* The same is true even if we consider only continuous functions, or only even infinitely smooth functions.\n\n==Application to finding roots of polynomials==\nSuppose <math>P</math> and <math>Q</math> are univariate polynomials with real coefficients, and <math>x</math> is a real number such that <math>P(x)Q(x) = 0</math>.  (Actually, we may allow the coefficients and <math>x</math> to come from any integral domain.)  By the zero-product property, it follows that either <math>P(x) = 0</math> or <math>Q(x) = 0</math>.  In other words, the roots of <math>PQ</math> are precisely the roots of <math>P</math> together with the roots of <math>Q</math>.\n\nThus, one can use [[factorization of polynomials|factorization]] to find the roots of a polynomial.  For example, the polynomial <math>x^3 - 2x^2 - 5x + 6</math> factorizes as <math>(x-3)(x-1)(x+2)</math>; hence, its roots are precisely 3, 1, and -2.\n\nIn general, suppose <math>R</math> is an integral domain and <math>f</math> is a [[Monic polynomial|monic]] univariate polynomial of degree <math>d \\geq 1</math> with coefficients in <math>R</math>.  Suppose also that <math>f</math> has <math>d</math> distinct roots <math>r_1,\\ldots,r_d \\in R</math>.  It follows (but we do not prove here) that <math>f</math> factorizes as <math>f(x) = (x-r_1) \\cdots (x-r_d)</math>.  By the zero-product property, it follows that <math>r_1,\\ldots,r_d</math> are the ''only'' roots of <math>f</math>: any root of <math>f</math> must be a root of <math>(x-r_i)</math> for some <math>i</math>.  In particular, <math>f</math> has at most <math>d</math> distinct roots.\n\nIf however <math>R</math> is not an integral domain, then the conclusion need not hold.  For example, the cubic polynomial <math>x^3 + 3x^2 + 2x</math> has six roots in <math>\\mathbb{Z}_6</math> (though it has only three roots in <math>\\mathbb{Z}</math>).\n\n==See also==\n* [[Fundamental theorem of algebra]]\n* [[Integral domain]] and [[domain (ring theory)|domain]]\n* [[Prime ideal]]\n* [[Zero divisor]]\n\n==Notes==\n{{Reflist|group=note}}\n\n==References==\n*David S. Dummit and Richard M. Foote, ''Abstract Algebra'' (3d ed.), Wiley, 2003, {{isbn|0-471-43334-9}}.\n\n==External links==\n* [http://planetmath.org/zeroruleofproduct PlanetMath: Zero rule of product]\n\n[[Category:Abstract algebra]]\n[[Category:Elementary algebra]]\n[[Category:Real analysis]]\n[[Category:Ring theory]]\n[[Category:0 (number)]]"
    },
    {
      "title": "Action algebra",
      "url": "https://en.wikipedia.org/wiki/Action_algebra",
      "text": "In [[algebraic logic]], an '''action algebra''' is an [[algebraic structure]] which is both a [[residuated semilattice]] and a [[Kleene algebra]].  It adds the star or reflexive transitive closure operation of the latter to the former, while adding the left and right residuation or implication operations of the former to the latter.  Unlike [[dynamic logic (modal logic)|dynamic logic]] and other modal logics of programs, for which programs and propositions form two distinct sorts, action algebra combines the two into a single sort.  It can be thought of as a variant of [[Heyting algebra|intuitionistic logic]] with star and with a noncommutative conjunction whose identity need not be the top element.  Unlike Kleene algebras, action algebras form a [[Algebraic variety|variety]], which furthermore is finitely axiomatizable, the crucial axiom being ''a''•(''a'' → ''a'')* ≤ ''a''.  Unlike models of the equational theory of Kleene algebras (the regular expression equations), the star operation of action algebras is reflexive transitive closure in every model of the equations.\n__TOC__\n\n==Definition==\n\nAn '''action algebra''' (''A'', ∨, 0, •, 1, ←, →, *) is an [[algebraic structure]] such that (''A'', ∨, •, 1, ←, →) forms a [[residuated semilattice]] while (''A'', ∨, 0, •, 1, *) forms a [[Kleene algebra]].<ref>{{citation\n| last = Kozen | first = Dexter\n| contribution = On Kleene algebras and closed semirings\n| contribution-url = http://boole.stanford.edu/pub/jelia.pdf\n| title = Mathematical Foundations of Computer Science (MFCS)\n| editor = B. Rovan\n| series = LNCS 452\n| pages = 26–47\n| publisher = Springer-Verlag\n| year = 1990}}</ref>  That is, it is any model of the joint theory of both classes of algebras.  Now Kleene algebras are axiomatized with quasiequations, that is, implications between two or more equations, whence so are action algebras when axiomatized directly in this way.  However, action algebras have the advantage that they also have an equivalent axiomatization that is purely equational.<ref>{{citation\n| last = Pratt | first = Vaughan\n| contribution = Action Logic and Pure Induction\n| contribution-url = http://boole.stanford.edu/pub/jelia.pdf\n| title = Logics in AI: European Workshop JELIA '90'' (ed. J. van Eijck)\n| series = LNCS 478\n| pages = 97–120\n| publisher = Springer-Verlag\n| year = 1990}}.</ref>  The language of action algebras extends in a natural way to that of [[action lattice]]s, namely by the inclusion of a meet operation.<ref>{{citation\n | last = Kozen | first = Dexter\n | contribution = On action algebras\n | contribution-url = http://www.cs.cornell.edu/~kozen/papers/act.pdf\n | mr = 1295061\n | pages = 78–88\n | publisher = MIT Press, Cambridge, MA\n | series = Found. Comput. Ser.\n | title = Logic and information flow\n | year = 1994}}.</ref>\n\nIn the following we write the inequality ''a'' ≤ ''b'' as an abbreviation for the equation ''a'' ∨ ''b'' = ''b''.  This allows us to axiomatize the theory using inequalities yet still have a purely equational axiomatization when the inequalities are expanded to equalities.\n\nThe equations axiomatizing action algebra are those for a residuated semilattice, together with the following equations for star.\n::: 1 ∨ ''a''*•''a''* ∨ ''a''  &nbsp; ≤ &nbsp; ''a''*\n::: ''a''* ≤ (''a'' ∨ ''b'')*\n::: (''a'' → ''a'')* &nbsp; ≤ &nbsp; ''a'' → ''a''\n\nThe first equation can be broken out into three equations, 1 ≤ ''a''*, ''a''*•''a''* ≤ ''a''*, and ''a'' ≤ ''a''*.  These force ''a''* to be reflexive, transitive,{{clarify|reason=These are properties of binary relations. Explain why a* can be considered as a binary relation.|date=June 2014}} and greater or equal to ''a'' respectively.  The second axiom asserts that star is monotone.  The third axiom can be written equivalently as ''a''•(''a'' → ''a'')* ≤ ''a'', a form which makes its role as induction more apparent.  These two axioms in conjunction with the axioms for a residuated semilattice force ''a''* to be the least reflexive transitive element of the semilattice greater or equal to ''a''.  Taking that as the definition of reflexive transitive closure of ''a'', we then have that for every element ''a'' of any action algebra, ''a''* is the reflexive transitive closure of ''a''.\n\nThe equational theory of the implication-free fragment of action algebras, those equations not containing → or ←, can be shown to coincide with the equational theory of Kleene algebras, also known as the [[regular expression]] equations.  In that sense the above axioms constitute a finite axiomatization of regular expressions.  Redko showed in 1967 that these equations had no finite axiomatization, for which [[John Horton Conway]] gave a shorter proof in 1971.  Salomaa gave an equation schema axiomatizing this theory which Kozen subsequently reformulated as a finite axiomatization using quasiequations, or implications between equations, the crucial quasiequations being those of induction: if ''x''•''a'' ≤ ''x'' then ''x''•''a''* ≤ ''x'', and if ''a''•''x'' ≤ ''x'' then ''a''*•''x'' ≤ ''x''.  Kozen defined a Kleene algebra to be any model of this finite axiomatization.\n\nConway showed that the equational theory of regular expressions admit models in which ''a''* was not the reflexive transitive closure of ''a'', by giving a four-element model 0 ≤ 1 ≤ ''a'' ≤ ''a''* in which ''a''•''a'' = ''a''.  In Conway's model, ''a'' is reflexive and transitive, whence its reflexive transitive closure should be ''a''.  However the regular expressions do not enforce this, allowing ''a''* to be strictly greater than ''a''.  Such anomalous behavior is not possible in an action algebra.\n\n==Examples==\nAny [[Heyting algebra]] (and hence any [[Boolean algebra (structure)|Boolean algebra]]) is made an action algebra by taking • to be ∧ and ''a''* = 1.  This is necessary and sufficient for star because the top element 1 of a Heyting algebra is its only reflexive element, and is transitive as well as greater or equal to every element of the algebra.\n\nThe set 2<sup>Σ*</sup> of all [[formal language]]s (sets of finite strings) over an alphabet Σ forms an action algebra with 0 as the empty set, 1 = {ε}, ∨ as union, • as concatenation, ''L'' ← ''M'' as the set of all strings ''x'' such that ''xM'' ⊆ ''L'' (and dually for ''M'' → ''L''), and ''L''* as the set of all strings of strings in ''L'' (Kleene closure).\n\nThe set 2<sup>''X''²</sup> of all binary relations on a set ''X'' forms an action algebra with 0 as the empty relation, 1 as the identity relation or equality, ∨ as union, • as relation composition, ''R'' ← ''S'' as the relation consisting of all pairs (''x,y'') such that for all ''z'' in ''X'', ''ySz'' implies ''xRz'' (and dually for ''S'' → ''R''), and ''R*'' as the reflexive transitive closure of ''R'', defined as the union over all relations ''R''<sup>''n''</sup> for integers ''n'' ≥ 0.\n\nThe two preceding examples are power sets, which are [[Boolean algebra (logic)|Boolean algebras]] under the usual set theoretic operations of union, intersection, and complement.  This justifies calling them '''Boolean action algebras'''.  The relational example constitutes a [[relation algebra]] equipped with an operation of reflexive transitive closure.  Note that every Boolean algebra is a Heyting algebra and therefore an action algebra by virtue of being an instance of the first example.\n\n==See also==\n* [[Kleene star]]\n* [[Regular expression]]\n\n==References==\n{{Reflist}}\n* {{cite book | first=J.H. | last=Conway | authorlink=John Horton Conway | title=Regular algebra and finite machines | publisher=Chapman and Hall | year=1971 | isbn=0-412-10620-5 | zbl=0231.94041 | location=London }}\n* V.N. Redko, On defining relations for the algebra of regular events (Russian), ''Ukrain. Mat. Z.'', 16:120–126, 1964.\n\n[[Category:Formal languages]]\n[[Category:Algebraic logic]]\n[[Category:Algebraic structures]]"
    },
    {
      "title": "Additive group",
      "url": "https://en.wikipedia.org/wiki/Additive_group",
      "text": "{{Wiktionary}}\nAn '''additive group''' is a [[group (mathematics)|group]] of which the group operation is to be thought of as ''addition'' in some sense.  It is usually [[abelian group|abelian]], and typically written using the symbol '''+''' for its binary operation.\n\nThis terminology is widely used with structures equipped with several operations for specifying the structure obtained by forgetting the other operations. Examples include the ''additive group''<ref>{{citation |first=N. |last=Bourbaki |title=Algebra I: Chapters 1–3 |chapter=§8.1 Rings |chapterurl=https://books.google.com/books?id=STS9aZ6F204C&pg=PA97 |year=1998 |publisher=Springer |isbn=978-3-540-64243-5 |page=97 |origyear=1970}}</ref> of the [[integers]], of a [[vector space]] and of a [[ring (mathematics)|ring]]. This is particularly useful with rings and [[field (mathematics)|fields]] to distinguish the additive underlying group from the [[multiplicative group]] of the [[unit (ring theory)|invertible element]]s.\n\n== References ==\n{{Reflist}}\n\n{{DEFAULTSORT:Additive group}}\n[[Category:Algebraic structures]]\n[[Category:Group theory]]"
    },
    {
      "title": "Affine monoid",
      "url": "https://en.wikipedia.org/wiki/Affine_monoid",
      "text": "In [[abstract algebra]], a branch of [[mathematics]], an '''affine monoid''' is a [[commutative]] [[monoid]] that is finitely generated, and is [[isomorphic]] to a submonoid of a [[free abelian group]]  ℤ<sup>''d''</sup>, ''d''&nbsp;≥&nbsp;0.<ref name=\"Gubeladze\">{{cite book |first=Winfried |last=Bruns |first2=Joseph |last2=Gubeladze |title=Polytopes, Rings, and K-Theory |publisher=Springer |series=Monographs in Mathematics |year=2009 |isbn=0-387-76356-2 |url=https://books.google.com/books?id=pbgg1pFxW8YC}}</ref>  Affine monoids are closely connected to [[convex polyhedra]], and their associated [[algebra (ring theory)|algebras]] are of much use in the algebraic study of these geometric objects.\n\n== Characterization ==\n* Affine monoids are '''finitely generated'''. This means for a monoid <math> M </math>, there exists <math> m_1, \\dots , m_n \\in M </math> such that  \n:<math> M = m_1\\mathbb{Z_+}+\\dots + m_n\\mathbb{Z_+} </math>.\n\n* Affine monoids are '''[[cancellation property|cancellative]]'''. In other words, \n:<math>x + y = x + z</math> implies that <math>y = z</math> for all <math>x,y,z \\in M</math>, where <math>+</math> denotes the [[binary operation]] on the affine monoid <math>M</math>.\n\n* Affine monoids are also '''[[Affine connection|torsion free]]'''. For an affine monoid <math>M</math>, <math>nx = ny</math> implies that <math>x = y</math> for <math> n \\in \\mathbb{N}</math>, and <math> x, y \\in M</math>.\n* A subset <math>N</math> of a monoid <math>M</math> that is itself a monoid with respect to the operation on <math>M</math> is a '''submonoid''' of <math>M</math>.\n\n=== Properties and examples ===\n* Every submonoid of <math>\\mathbb{Z}</math> is finitely generated. Hence, ''every'' submonoid of <math>\\mathbb{Z}</math> is affine.\n* The submonoid <math>\\{(x,y)\\in \\mathbb{Z} \\times \\mathbb{Z} \\mid y > 0\\} \\cup \\{(0,0)\\}</math> of <math>\\mathbb{Z} \\times \\mathbb{Z}</math> is ''not'' finitely generated, and therefore ''not'' affine.\n* The [[intersection (set theory)|intersection]] of two affine monoids is an affine monoid.\n\n== Affine monoids ==\n\n=== Group of differences ===\n:If <math>M</math> is an affine monoid, it can be [[embedding|embedded]] into a [[group (mathematics)|group]]. More specifically, there is a unique group <math>gp(M)</math>, called ''the group of differences'', in which <math>M</math> can be embedded.\n\n==== Definition ====\n*<math>gp(M)</math> can be viewed as the set of equivalences classes <math>x - y</math>, where <math>x - y = u - v</math> if and only if <math>x + v + z = u + y + z</math>, for <math>z \\in M</math>, and \n<math>(x-y) + (u-v) = (x+u) - (y+v)</math> defines the addition.<ref name=\"Gubeladze\" />\n\n*The '''rank''' of an affine monoid <math>M</math> is the [[rank of a group]] of <math>gp(M)</math>.<ref name=\"Gubeladze\" />\n*If an affine monoid <math>M</math> is given as a submonoid of <math>\\mathbb{Z}^r</math>, then <math>gp(M) \\cong \\mathbb{Z}M</math>, where <math>\\mathbb{Z}M</math> is the subgroup of <math>\\mathbb{Z}^r</math> <ref name=\"Gubeladze\" />\n\n==== Universal property ====\n\n*If <math>M</math> is an affine monoid, then the monoid [[homomorphism]] <math>\\iota : M \\to gp(M)</math> defined by <math>\\iota(x) = x + 0</math> satisfies the following [[universal property]]:\n\n:for any monoid homomorphism <math>\\varphi: M \\to G</math>, where <math>G</math> is a group, there is a unique group homomorphism <math>\\psi : gp(M) \\to G</math>, such that <math>\\varphi = \\psi \\circ \\iota</math>, and since affine monoids are cancellative, it follows that <math>\\iota</math> is an embedding. In other words, ''every'' affine monoid can be embedded into a group.\n\n=== Normal affine monoids ===\n\n==== Definition ====\n* If <math>M</math> is a submonoid of an affine monoid <math>N</math>, then the submonoid\n:<math> \\hat{M}_N = \\{x\\in N \\mid mx \\in M, m \\in \\mathbb{N}\\}</math>\nis the '''integral closure''' of <math>M</math> in <math>N</math>. If <math>M = \\hat{M_N}</math>, then <math>M</math> is '''integrally closed'''\n*The '''normalization''' of an affine monoid <math>M</math> is the integral closure of <math>M</math> in <math>gp(M)</math>. If the normalization of <math>M</math>, is <math>M</math> itself, then <math>M</math> is a '''normal''' affine monoid.<ref name=\"Gubeladze\" />\n* A monoid <math>M</math> is a normal affine monoid if and only if <math>\\mathbb{R}_+M</math> is finitely generated and <math>M = \\mathbb{Z}^r \\cap \\mathbb{R}_+M</math> .\n\n== Affine monoid rings ==\n: ''see also: [[Group ring]]''\n\n=== Definition ===\n\n* Let <math>M</math> be an affine monoid, and <math>R</math> a commutative [[ring (mathematics)|ring]]. Then one can form the '''affine monoid ring''' <math>R[M]</math>. This is an <math>R</math>-module with a free basis <math>M</math>, so if <math>f \\in R[M]</math>, then \n: <math>f = \\sum_{i=1}^{n}f_{i}x_i</math>, where <math>f_i \\in R, x_i \\in M</math>, and <math>n \\in \\mathbb{N}</math>. \n:In other words, <math>R[M]</math> is the set of finite sums of elements of <math>M</math> with coefficients in <math>R</math>.\n\n== Connection to [[convex geometry]] ==\n:Affine monoids arise naturally from convex polyhedra, [[convex cone]]s, and their associated discrete structures.\n\n* Let <math>C</math> be a rational [[convex cone]] in <math>\\mathbb{R}^n</math>, and let <math>L</math> be a [[lattice (group)|lattice]] in <math>\\mathbb{Q}^n</math>. Then <math>C \\cap L</math> is an affine monoid.<ref name=\"Gubeladze\" /> (Lemma 2.9, Gordan's lemma)\n* If <math>M</math> is a submonoid of <math>\\mathbb{R}^n</math>, then <math>\\mathbb{R}_+M</math> is a cone if and only if <math>M</math> is an affine monoid.\n* If <math>M</math> is a submonoid of <math>\\mathbb{R}^n</math>, and <math>C</math> is a cone generated by the elements of <math>gp(M)</math>, then <math>M \\cap C</math> is an affine monoid.\n* Let <math>P</math> in <math>\\mathbb{R}^n</math> be a rational polyhedron, <math>C</math> the [[recession cone]] of <math>P</math>, and <math>L</math> a lattice in <math>\\mathbb{Q}^n</math>. Then <math>P \\cap L</math> is a finitely generated [[module (mathematics)|module]] over the affine monoid <math>C \\cap L</math>.<ref name=\"Gubeladze\" /> (Theorem 2.12)\n\n== See also ==\n* [[Monoid]]\n* [[Convex cone]]\n* [[Convex polytope]]\n* [[Lattice (group)]]\n* [[K-theory]]\n\n== References ==\n<references />\n\n[[Category:Algebraic structures]]"
    },
    {
      "title": "Baer *-semigroup",
      "url": "https://en.wikipedia.org/wiki/Baer_%2A-semigroup",
      "text": "#REDIRECT [[Semigroup with involution#Baer *-semigroups]]\n\n{{DEFAULTSORT:Baer -semigroup}}\n[[Category:Semigroup theory]]\n[[Category:Algebraic structures]]"
    },
    {
      "title": "Band (mathematics)",
      "url": "https://en.wikipedia.org/wiki/Band_%28mathematics%29",
      "text": "{{Other uses|Band (disambiguation)}}\n\nIn [[mathematics]], a '''band''' (also called '''idempotent semigroup''') is a [[semigroup]] in which every element is [[idempotent]] (in other words equal to its own square). Bands were first studied and named by {{harvs|first=A. H.|last=Clifford|authorlink=Alfred H. Clifford|year=1954|txt}}; the [[lattice (order)|lattice]] of [[variety (universal algebra)|varieties]] of bands was described independently in the early 1970s by Biryukov, Fennemore and Gerhard.<ref name=\"bfg\"/> '''Semilattices''', '''left-zero bands''', '''right-zero bands''', '''rectangular bands''', '''normal bands''', '''left-regular bands''', '''right-regular bands''' and '''regular bands''', specific subclasses of bands which lie near the bottom of this lattice, are of particular interest and are briefly described below.\n\n==Varieties of bands==\nA class of bands forms a [[variety (universal algebra)|variety]] if it is closed under formation of subsemigroups, [[homomorphism|homomorphic images]] and [[direct product]]. Each variety of bands can be defined by a single  [[variety (universal algebra)|defining identity]].{{sfnp|Fennemore|1970}}\n\n===Semilattices===\n[[Semilattices]] are exactly [[commutative]] bands; that is, they are the bands satisfying the equation\n* ''xy'' = ''yx'' for all ''x'' and ''y''.\n\n===Zero bands===\nA '''left zero band''' is a band satisfying the equation\n* ''xy''&nbsp;=&nbsp;''x'',\nwhence its [[Cayley table]] has constant rows.\n\nSymmetrically, a '''right zero band''' is one satisfying\n* ''xy''&nbsp;=&nbsp;''y'',\nso that the Cayley table has constant columns.\n\n===Rectangular bands===\nA '''rectangular band''' is a band ''S'' that satisfies\n* ''xyx'' = ''x'' for all ''x'',&nbsp;''y''&nbsp;∈&nbsp;''S'',\nor equivalently,\n*''xyz''&nbsp;=&nbsp;''xz'' for all ''x'',&nbsp;''y'',&nbsp;''z''&nbsp;∈&nbsp;''S'',\nThe second characterization clearly implies the first, and conversely the first implies ''xyz = xy(zxz) = (x(yz)x)z = xz''.\n\nThere is a complete classification of rectangular bands.  Given arbitrary sets ''I'' and ''J'' one can define a semigroup operation on ''I''&nbsp;&times;&nbsp;''J'' by setting\n\n: <math>(i, j) \\cdot (k, \\ell) = (i, \\ell) \\, </math>\n\nThe resulting semigroup is a rectangular band because \n# for any pair (''i'',&nbsp;''j'') we have (''i'',&nbsp;''j'')&nbsp;·&nbsp;(''i'',&nbsp;''j'')&nbsp;=&nbsp;(''i'',&nbsp;''j'')\n# for any two pairs (''i''<sub>''x''</sub>,&nbsp;''j''<sub>''x''</sub>), (''i''<sub>''y''</sub>,&nbsp;''j''<sub>''y''</sub>) we have\n:: <math>  (i_x, j_x) \\cdot (i_y, j_y) \\cdot (i_x, j_x) = (i_x, j_x)</math>\n\nIn fact, any rectangular band is [[isomorphic]] to one of the above form.  Left zero and right zero bands are rectangular bands, and in fact every rectangular band is isomorphic to a direct product of a left zero band and a right zero band. All rectangular bands of prime order are zero bands, either left or right. A rectangular band is said to be purely rectangular if it is not a left or right zero band.<ref name=\"y71\">{{harvtxt|Yamada|1971}}.</ref>\n\nIn [[category theory|categorical]] language, one can say that the category of nonempty rectangular bands is [[equivalence of categories|equivalent]] to <math>\\mathrm{Set}_{\\ne \\emptyset} \\times \\mathrm{Set}_{\\ne \\emptyset}</math>, where <math>\\mathrm{Set}_{\\ne \\emptyset}</math> is the category with nonempty sets as objects and functions as morphisms.  This implies not only that every nonempty rectangular band is isomorphic to one coming from a pair of sets, but also these sets are uniquely determined up to a canonical isomorphism, and all homomorphisms between bands come from pairs of functions between sets.{{sfnp|Howie|1995}}  If the set ''I'' is empty in the above result, the rectangular band ''I''&nbsp;&times;&nbsp;''J'' is independent of ''J'', and vice versa.  This is why the above result only gives an equivalence between nonempty rectangular bands and pairs of nonempty sets.\n\n===Normal bands===\nA '''normal band''' is a band ''S'' satisfying\n* ''zxyz'' = ''zyxz'' for all ''x'', ''y'', and ''z''&nbsp;∈&nbsp;''S''.\nThis is the same equation used to define [[medial magma]]s, and so a normal band may also be called a medial band, and normal bands are examples of medial magmas.<ref name=\"y71\"/>\nWe can also say  a '''normal band''' is a band ''S'' satisfying\n* ''axyb''=''ayxb'' for all ''a'', ''b'', ''x'', and ''y''&nbsp;∈&nbsp;''S''.\n\n===Left-regular bands===\n\nA '''left-regular band''' is a band ''S'' satisfying\n\n* ''xyx = xy'' for all ''x'',&nbsp;''y'' &nbsp;∈&nbsp;''S''\n\nIf we take a semigroup and define ''a'' ≤ ''b''  if and only if ''ab = b'', we obtain a [[partial ordering]] if and only if this semigroup is a left-regular band.  Left-regular bands thus show up naturally in the study of [[Partially ordered sets|posets]].{{sfnp|Brown|2000}}\n\n===Right-regular bands===\n\nA '''right-regular band''' is a band ''S'' satisfying\n\n* ''xyx = yx'' for all ''x'',&nbsp;''y'' &nbsp;∈&nbsp;''S''\n\nAny right-regular band becomes a left-regular band using the opposite product.  Indeed, every variety of bands has an 'opposite' version; this gives rise to the reflection symmetry in the figure below.\n\n===Regular bands===\nA '''regular band''' is a band ''S'' satisfying\n* ''zxzyz'' = ''zxyz'' for all ''x'',&nbsp;''y'',&nbsp;''z''&nbsp;∈&nbsp;''S''\n\n===Lattice of varieties===\n[[File:Bands.svg|thumb|400px|Lattice of varieties of regular bands.]]\nWhen [[partially ordered set|partially ordered]] by inclusion, varieties of bands naturally form a [[Lattice (order)|lattice]], in which the meet of two varieties is their intersection and the join of two varieties is the smallest variety that contains both of them. The complete structure of this lattice is known; in particular, it is [[countability|countable]], [[complete lattice|complete]], and [[distributive lattice|distributive]].<ref name=\"bfg\">{{harvtxt|Biryukov|1970}}; {{harvtxt|Fennemore|1970}}; {{harvtxt|Gerhard|1970}}; {{harvtxt|Gerhard|Petrich|1989}}.</ref>  The sublattice consisting of the 13 varieties of regular bands is shown in the figure.  The varieties of left-zero bands, semilattices, and right-zero bands are the three atoms (non-trivial minimal elements) of this lattice.\n\nEach variety of bands shown in the figure is defined by just one identity.  This is not a coincidence: in fact, ''every'' variety of bands can be defined by a single identity.<ref name=\"bfg\"/>\n\n==See also==\n*[[Boolean ring]], a ring in which every element is (multiplicatively) idempotent\n*[[Nowhere commutative semigroup]]\n*[[Special classes of semigroups]]\n*[[Orthodox semigroup]]\n*[[Reversible cellular automaton#One-dimensional automata]]\n\n==Notes==\n{{reflist}}\n\n==References==\n*{{citation\n | last = Biryukov | first = A. P.\n | doi = 10.1007/BF02218673\n | issue = 3\n | journal = Algebra and Logic\n | pages = 153–164\n | title = Varieties of idempotent semigroups\n | volume = 9\n | year = 1970}}.\n*{{citation\n | last = Brown | first = Ken\n | journal = J. Theoret. Probab. \n | arxiv = math/0006145\n | pages = 871–938\n | title = Semigroups, rings, and Markov chains,\n | volume = 13\n | year = 2000| bibcode = 2000math......6145B}}.\n*{{citation\n | last = Clifford | first = Alfred Hoblitzelle | authorlink = Alfred H. Clifford\n | doi = 10.1090/S0002-9939-1954-0062119-9\n | mr = 0062119\n | journal = [[Proceedings of the American Mathematical Society]]\n | pages = 499–504\n | title = Bands of semigroups\n | volume = 5\n | year = 1954}}.\n*{{citation\n | last1 = Clifford | first1 = Alfred Hoblitzelle | authorlink = Alfred H. Clifford\n | last2 = Preston | first2 = Gordon Bamford\n | location = Moscow\n | publisher = Mir\n | title = The Algebraic Theory of Semigroups\n | year = 1972}}.\n*{{citation\n | last = Fennemore | first = Charles\n | doi = 10.1007/BF02573031\n | issue = 1\n | journal = [[Semigroup Forum]]\n | pages = 172–179\n | title = All varieties of bands\n | volume = 1\n | year = 1970}}.\n*{{citation\n | last = Gerhard | first = J. A.\n | doi = 10.1016/0021-8693(70)90073-6\n | issue = 2\n | journal = Journal of Algebra\n | pages = 195–224\n | title = The lattice of equational classes of idempotent semigroups\n | volume = 15\n | year = 1970}}.\n*{{citation\n | last1 = Gerhard | first1 = J. A. \n | last2 = Petrich | first2 = Mario\n | doi = 10.1112/plms/s3-58.2.323\n | journal = [[Proceedings of the London Mathematical Society]]\n | pages = 323–350\n | title = Varieties of bands revisited\n | volume = 3\n | year = 1989}}.\n*{{citation\n | last = Howie | first = John M. \n | isbn = 978-0-19-851194-6\n | title = Fundamentals of Semigroup Theory\n | publisher = Oxford U. Press\n | year = 1995}}.\n*{{citation\n | last = Nagy | first = Attila\n | isbn = 0-7923-6890-8\n | location = Dordrecht\n | publisher = [[Kluwer Academic Publishers]]\n | title = Special Classes of Semigroups\n | year = 2001}}.\n*{{citation\n | last = Yamada | first = Miyuki\n | doi = 10.1007/BF02572956\n | issue = 1\n | journal = [[Semigroup Forum]]\n | pages = 160–167\n | title = Note on exclusive semigroups\n | volume = 3\n | year = 1971}}.\n\n[[Category:Algebraic structures]]\n[[Category:Semigroup theory]]"
    },
    {
      "title": "Baumslag–Gersten group",
      "url": "https://en.wikipedia.org/wiki/Baumslag%E2%80%93Gersten_group",
      "text": "In the mathematical subject of [[geometric group theory]], the '''Baumslag–Gersten group''', also known as the '''Baumslag group''', is a particular [[one-relator group]] exhibiting some remarkable properties regarding its finite [[quotient group]]s, its [[Dehn function]] and the complexity of its [[word problem for groups|word problem]].\n\nThe group is given by the [[group presentation|presentation]]\n\n: <math> G=\\langle a,t \\mid a^{a^t}=a^2\\rangle =\\langle a, t \\mid (t^{-1}a^{-1}t) a (t^{-1} at)=a^2 \\rangle  </math>\n\nHere exponential notation for group elements denotes conjugation, that is, for <math>g, h\\in G, g^h=h^{-1}gh.</math>\n\n==History==\nThe Baumslag–Gersten group ''G'' was originally introduced in a 1969 paper of [[Gilbert Baumslag]],<ref name=B> Gilbert Baumslag, ''A non-cyclic one-relator group all of whose finite factor groups are cyclic'', [[Journal of the Australian Mathematical Society]] '''10''' (1969) 497–498. {{MR|0254127}} </ref> as an example of a non-[[residually finite]] [[one-relator group]] with an additional remarkable property that all finite [[quotient group]]s of this group are cyclic. Later, in 1992, Gersten<ref name=Ger>S. M. Gersten, ''Dehn functions and <math>l_1</math>-norms of finite presentations''. Algorithms and classification in combinatorial group theory (Berkeley, CA, 1989), 195–224, Math. Sci. Res. Inst. Publ., 23, Springer, New York, 1992. {{doi|10.1007/978-1-4613-9730-4_9}} {{MR|1230635}} </ref> showed that ''G'', despite being a one-relator group given by a rather simple presentation, has the [[Dehn function]] growing very quickly, namely faster than any fixed iterate of the exponential function. This example remains the fastest known growth of the Dehn function among one-relator groups. In 2011 Myasnikov, Ushakov and Won proved that ''G'' has the [[word problem for groups|word problem]] solvable in polynomial time.\n\n==Baumslag-Gersten group as an HNN extension==\n\nThe Baumslag–Gersten group ''G'' can also be realized as an [[HNN extension]] of the [[Baumslag–Solitar group]] <math>BS(1,2)=\\langle a, b\\mid a^b=a^2\\rangle </math> with stable letter ''t'' and two cyclic associated subgroups<math>\\langle a\\rangle, \\langle b\\rangle </math>:\n\n:<math>G=\\langle a,t \\mid a^{a^t}=a^2\\rangle=\\langle a,b,t \\mid a^b=a^2, a^t=b\\rangle.  </math>\n\n==Properties of the Baumslag–Gersten group ''G''==\n\n*Every finite [[quotient group]] of ''G'' is [[cyclic group|cyclic]]. In particular, the group ''G'' is not [[residually finite group|residually finite]].<ref name=B/>\n*An endomorphism of ''G'' is either an automorphism or its image is a cyclic subgroup of ''G''. In particular the group ''G'' is [[Hopfian group|Hopfian]] and [[co-Hopfian group|co-Hopfian]].<ref name=Br>\n{{cite journal\n| last       = Brunner\n| first      = Andrew\n| date       = 1980\n| title      = On a class of one-relator groups\n| jstor        = \n| journal    = [[Canadian Journal of Mathematics]] \n| mr         = 571934\n| volume     = 32\n| issue      = 2\n| pages      = 414–420\n| doi        = 10.4153/CJM-1980-032-8\n}}\n</ref>  \n*The [[outer automorphism group]] Out(''G'') of ''G'' is isomorphic to the additive group of dyadic rationals <math>\\mathbb Z\\left[\\frac{1}{2}\\right]</math> and in particular is not finitely generated.<ref name=Br/> \n*Gersten proved<ref name=Ger/> that the [[Dehn function]] ''f''(''n'') of ''G'' grows faster than any fixed iterate of the exponential. Subsequently Platonov<ref>A. N. Platonov, \n''An isoparametric function of the Baumslag–Gersten group.''{{icon ru}}\nVestnik Moskov. Univ. Ser. I Mat. Mekh. 2004, no. 3, 12--17; translation in \nMoscow Univ. Math. Bull. '''59''' (2004), no. 3, 12–17 (2005). {{MR|2127449}}  </ref> proved that ''f(n)'' is equivalent to \n\n:: <math> \\exp^{\\circ\\log n}(1)=(\\exp\\underbrace{\\circ \\cdots  \\circ}_{\\log n \\text{ times } } \\exp)(1) </math>\n\n*Myasnikov, Ushakov, and Won,<ref>Alexei Myasnikov, Alexander Ushakov, and Dong Wook Won, \n[https://ac.els-cdn.com/S0021869311004492/1-s2.0-S0021869311004492-main.pdf?_tid=4a113c4f-5d3d-4ac7-b462-729f04fa5156&acdnat=1537351986_783c671ad9564f8a6fa041fd2664324b  ''The word problem in the Baumslag group with a non-elementary Dehn function is polynomial time decidable''.] \n[[Journal of Algebra]] '''345''' (2011), 324–342. {{doi|10.1016/j.jalgebra.2011.07.024}} {{MR|2842068}} </ref> using compression methods of ``power circuits\" arithmetics, proved that the word problem in ''G'' is solvable in polynomial time. Thus the group ''G'' exhibits a large gap between the growth of its Dehn function and the complexity of its word problem.\n* The [[conjugacy problem]] in ''G'' is known to be decidable, but the only known worst-case upper bound estimate for the complexity of the conjugacy problem, due to Beese, is [[Elementary function arithmetic|elementary recursive]].<ref>J. Beese, ''Das Konjugations problem in der Baumslag–Gersten–Gruppe''. Diploma thesis, Fakultät Mathematik, Universität Stuttgart (2012) </ref> It is conjectured that this estimate is sharp, based on some reductions to power circuit division problems.<ref name=DMW>  Volker Diekert, Alexei G. Myasnikov, and Armin  Weiß, ''Conjugacy in Baumslag's group, generic case complexity, and division in power circuits''. Algorithmica '''76''' (2016), no. 4, 961–988. {{doi|10.1007/s00453-016-0117-z}} {{MR|3567623}} </ref> There is a [[generic-case complexity|strongly generically]] polynomial time solution of the conjugacy problem for ''G''.<ref name=DMW/>\n\n==Generalizations==\n\n*Brunner<ref name=Br/> considered one-relator groups of the form\n:: <math>\\langle a,t\\mid (a^p)^{(t^{-1}a^kt)}=a^m \\rangle, </math> where <math>p,k,m\\ne 0</math>\nand generalized many of Baumslag's original results in that context.\n*Mitra<ref> [[Mahan Mitra]],  ''Coarse extrinsic geometry: a survey''. The Epstein birthday \nschrift, 341–364, Geom. Topol. Monogr., '''1''', Geom. Topol. Publ., Coventry, 1998. {{doi|10.2140/gtm.1998.1.341}}{{MR|1668308}} </ref> considered a [[word-hyperbolic group|word-hyperbolic]] analog ''G'' of the Baumslag–Gersten group, where Mitra's group possesses a rank three free subgroup that is highly distorted in ''G'', namely where the subgroup distortion is higher than any fixed iterated power of the exponential.\n\n==See also==\n*[[Subgroup distortion]]\n\n==References==\n{{Reflist}}\n\n==External links==\n*[https://berstein.wordpress.com/2011/05/04/distortion-of-finitely-presented-subgroups-of-non-positively-curved-groups-i/ Distortion of finitely presented subgroups of non-positively curved groups], the blog of the Spring 2011 Berstein Seminar at Cornell, including van Kampen diagrams demonstrating subgroup distortion in the Baumslag–Gersten group and the discussion of Mitra-like examples\n\n[[Category: Geometric group theory]] \n[[Category:Algebraic structures]]"
    },
    {
      "title": "BCK algebra",
      "url": "https://en.wikipedia.org/wiki/BCK_algebra",
      "text": "In mathematics, '''BCI and BCK algebras''' are [[algebraic structure]]s, introduced by Y. Imai,  K. Iséki and S. Tanaka in 1966, that describe fragments of the propositional calculus involving implication known as BCI and [[BCK logic]]s.\n\n==Definition==\n\n===BCI algebra===\nAn algebra <math>\\left( X;\\ast\n,0\\right)</math> of type <math>\\left( 2,0\\right)</math> is called a ''BCI-algebra'' if, for any <math>x,y,z\\in X</math>, it satisfies the following conditions. (Informally, we may read <math>0</math> as \"truth\" and <math>x\\ast y</math> as \"<math>y</math> implies <math>x</math>\".)\n; BCI-1: <math>\\left( \\left( x\\ast y\\right) \\ast \\left( x\\ast z\\right)\n\\right) \\ast \\left( z\\ast y\\right) =0</math>\n; BCI-2: <math>\\left( x\\ast \\left( x\\ast y\\right) \\right) \\ast y=0</math>\n; BCI-3: <math>x\\ast x=0</math>\n; BCI-4: <math>x\\ast y=0 \\land y\\ast x=0\\implies x=y</math>\n; BCI-5: <math>x\\ast 0=0 \\implies x=0</math>\n\n===BCK algebra===\nA BCI-algebra <math>\\left( X;\\ast ,0\\right)</math> is called a ''BCK-algebra'' if it\nsatisfies the following condition:\n; BCK-1: <math>\\forall x\\in X: 0\\ast x=0. </math>\n\nA partial order can then be defined as ''x'' ≤ ''y'' iff x * y = 0.\n\nA BCK-algebra is said to be ''commutative'' if it satisfies:\n: <math>x\\ast (x\\ast y)= y\\ast (y\\ast x)</math>\nIn a commutative BCK-algebra ''x'' * (''x'' * ''y'') = ''x'' ∧ ''y'' is the [[greatest lower bound]] of ''x'' and ''y'' under the partial order ≤.\n\nA BCK-algebra is said to be bounded if it has a largest element, usually denoted by 1. In a bounded commutative BCK-algebra the least upper bound of two elements satisfies ''x'' ∨ ''y'' = 1 * ((1 * ''x'') ∧ (1 * ''y'')); that makes it a [[distributive lattice]].\n\n==Examples==\n\nEvery [[abelian group]] is a BCI-algebra, with * defined as group subtraction and 0 defined as the group identity.\n\nThe subsets of a set form a BCK-algebra, where A*B is the [[Complement (set theory)|difference]] A\\B (the elements in A but not in B), and 0 is the [[empty set]].\n\nA [[Boolean algebra (structure)|Boolean algebra]] is a BCK algebra if ''A''*''B'' is defined to be ''A''&and;&not;''B'' (''A'' does not imply ''B'').\n\nThe bounded commutative BCK-algebras are precisely the [[MV-algebra]]s.\n\n== References ==\n*{{citation|title=Review of several papers on BCI, BCK-Algebras\n|first= R. B. |last=Angell \n|journal=            The Journal of Symbolic Logic|volume= 35|issue= 3|year=1970|pages= 465–466 \n|jstor=2270728|issn=0022-4812 | doi = 10.2307/2270728}}\n*{{citation|url=http://projecteuclid.org/euclid.pja/1195522126\n|last=Arai|first= Yoshinari|last2= Iséki|first2= Kiyoshi|last3= Tanaka|first3= Shôtarô\n|title=Characterizations of BCI, BCK-algebras\n|journal=Proc. Japan Acad. |volume=42|year= 1966 |pages=105–107|doi=10.3792/pja/1195522126|mr=0202572|issue=2 }}\n*{{springer|id=B/b110170|title=BCH algebra|first=C.S.|last= Hoo}}\n*{{springer|id=B/b110180|title=BCI algebra|first=C.S.|last= Hoo}}\n*{{springer|id=B/b110190|title=BCK algebra|first=C.S.|last= Hoo}}\n*{{citation|first=K. |last=Iséki|first2=  S.|last2= Tanaka|title=An introduction to the theory of BCK-algebras  |journal=Math. Japon. |volume= 23  |year=1978|pages= 1–26}}\n* Y. Huang, ''BCI-algebra'', Science Press, Beijing, 2006.\n*{{citation|first=Y.|last= Imai|first2=   K|last2= Iséki|title=On axiom systems of propositional calculi, XIV  |journal=Proc. Japan Acad. Ser. A, Math. Sci. |volume= 42  |year=1966|pages= 19–22 |url=http://projecteuclid.org/euclid.pja/1195522169|doi=10.3792/pja/1195522169}}\n*{{citation|first=K. |last=Iséki|title=An algebra related with a propositional calculus|journal= Proc. Japan Acad. Ser. A, Math. Sci. |volume= 42  |year=1966|pages= 26–29|url=http://projecteuclid.org/euclid.pja/1195522171|doi=10.3792/pja/1195522171}}\n\n[[Category:Algebraic structures]]\n[[Category:Universal algebra]]"
    },
    {
      "title": "BF-algebra",
      "url": "https://en.wikipedia.org/wiki/BF-algebra",
      "text": "{{Multiple issues|\n{{more citations needed|date=November 2015}}\n{{no footnotes|date=November 2015}}\n{{Orphan|date=January 2019}}\n}}\n\nIn mathematics, '''BF algebras''' are a class of algebraic structures arising out of a symmetric \"Yin Yang\" concept for Bipolar Fuzzy logic, the name was introduced by Andrzej Walendziak in 2007.  The name covers discrete versions, but a canonical example arises in the BF space [-1,0]x[0,1] of pairs of (false-ness, truth-ness).\n\n==Definition==\nA '''BF-algebra''' is a non-empty [[subset]] <math>X</math> with a [[constant (mathematics)|constant]] <math>0</math> and a [[binary operation]] <math>*</math> satisfying the following:\n# <math>x*x=0</math>\n# <math>x*0=x</math>\n# <math>0*(x*y)=y*x</math>\n\n== Example ==\nLet <math>Z</math> be the [[Set (mathematics)|set]] of [[integer]]s and '<math>-</math>' be the binary operation '[[subtraction]]'. Then the [[algebra]]ic structure <math>(Z,-)</math> obeys the following [[Property (philosophy)|properties]]:\n# <math>x-x=0</math> \n# <math>x-0=x</math>\n# <math>0-(x-y)=y-x</math>\n\n==References==\n\n*{{citation|mr=2357811 \n|last=Walendziak|first= Andrzej\n|title=On BF-algebras\n|journal=Math. Slovaca |volume=57 |year=2007|issue= 2|pages= 119–128|doi=10.2478/s12175-007-0003-x}}\n\n[[Category:Algebraic structures]]"
    },
    {
      "title": "Biordered set",
      "url": "https://en.wikipedia.org/wiki/Biordered_set",
      "text": "{{Inappropriate tone|date=November 2012}}\nA '''biordered set''' (\"boset\") is a [[mathematical object]] that occurs in the description of the  [[structure]] of the set of [[idempotent]]s in a [[semigroup]]. The concept and the terminology were developed by [[K S S Nambooripad]] in the early 1970s.<ref>{{cite book|last=Nambooripad|first=K S S|title=Structure of regular semigroups|publisher=[[University of Kerala]], [[Thiruvananthapuram]], [[India]]|date=1973|isbn=0-8218-2224-1}}</ref><ref>{{cite journal|doi=10.1007/BF02194864|last=Nambooripad|first=K S S|date=1975|title=Structure of regular semigroups I . Fundamental regular semigroups|journal=[[Semigroup Forum]]|volume=9|issue=4|pages=354–363}}</ref><ref name=\"namb\">{{cite book|last=Nambooripad|first=K S S|title=Structure of regular semigroups – I|publisher=American Mathematical Society|date=1979|series=Memoirs of the American Mathematical Society|volume=224|isbn=978-0-8218-2224-1}}</ref>\nThe defining properties of a biordered set are expressed in terms of two [[quasiorder]]s defined on the set and hence the name biordered set. Patrick Jordan, while a master's student at University of   Sydney, introduced in 2002 the term '''boset'''  as an abbreviation of biordered set.<ref>Patrick K. Jordan. ''On biordered sets, including an alternative approach to fundamental regular semigroups''. Master’s thesis, University of Sydney, 2002.</ref>\n\nAccording to Mohan S. Putcha, \"The axioms defining a biordered set are quite complicated. However, considering the general nature of semigroups, it is rather surprising that such a finite axiomatization is even possible.\"<ref name=\"putcha\">{{cite book|last=Putcha|first=Mohan S|title=Linear algebraic monoids|publisher=Cambridge University Press|date=1988|series=London Mathematical Society Lecture Note Series|volume=133|pages=121–122|isbn=978-0-521-35809-5}}</ref>  Since the publication of the original definition of the biordered set by Nambooripad, several variations in the definition have been proposed. [[David Easdown]] simplified the definition and formulated the axioms in a special arrow notation invented by him.<ref>{{cite journal|last=Easdown|first=David|date=1984|title=Biordered sets are biordered subsets of idempotents of semigroups|journal=Journal of Australian Mathematical Society|volume=Series A, 32|issue=2|pages=258–268}}</ref>\n\nThe set of idempotents in a semigroup is a biordered set and every biordered set is the set of idempotents of some semigroup.<ref name=\"namb\"/><ref>{{cite journal|doi=10.1016/0021-8693(85)90028-6|last=Easdown|first=David|date=1985|title=Biordered sets come from semigroups|journal=Journal of Algebra|volume=96|pages=581–91}}</ref>\nA regular biordered set is a biordered set with an additional  property. The set of idempotents in a [[regular semigroup]]  is a regular biordered set, and  every  regular biordered set is the set of idempotents of some regular semigroup.<ref name=\"namb\"/>  \n \n== Definition ==\nThe formal definition of a biordered set given by Nambooripad<ref name=\"namb\"/> requires some preliminaries. \n\n=== Preliminaries ===\n\nIf ''X'' and ''Y'' be [[Set (mathematics)|sets]] and  ρ⊆ ''X'' × ''Y'', let ρ ( ''y'' ) = { ''x'' ∈ ''X'' : ''x'' ρ ''y'' }. \n  \nLet ''E''  be a [[Set (mathematics)|set]] in which  a [[Partial function|partial]] [[binary operation]], indicated by juxtaposition, is defined.  If ''D''<sub>''E''</sub> is the [[Domain (mathematics)|domain]] of the partial binary operation on ''E'' then ''D''<sub>''E''</sub> is a [[Relation (mathematics)|relation]] on ''E'' and (''e'',''f'') is in  ''D''<sub>''E''</sub>  if and only if the product ''ef'' exists in ''E''. The following relations can be defined in ''E'':\n\n:<math>\\omega^r = \\{(e,f) \\, :\\, fe = e\\}  </math>\n\n:<math>\\omega^l = \\{ (e,f)\\, :\\, ef = e \\}  </math>\n\n:<math> R = \\omega^r\\, \\cap \\, (\\omega^r)^{-1} </math>\n\n:<math> L = \\omega^l\\, \\cap \\, (\\omega^l)^{-1} </math>\n\n:<math> \\omega = \\omega^r \\, \\cap \\, \\omega^l </math>\n\nIf ''T'' is any [[Proposition (mathematics)|statement]] about ''E'' involving the partial binary operation and the above relations in ''E'', one can define the left-right [[Dual (mathematics)|dual]] of ''T'' denoted by ''T''*. If ''D''<sub>''E''</sub> is [[symmetric relation|symmetric]] then ''T''* is meaningful whenever ''T'' is. \n\n=== Formal definition ===\n\nThe set ''E''  is called a biordered set if the following [[axiom]]s and their duals hold for arbitrary elements ''e'', ''f'', ''g'', etc. in ''E''.\n\n:(B1)&#8195; ω<sup>''r'' </sup>   and ω<sup>''l''</sup>  are [[Reflexive relation|reflexive]] and [[Transitive relation|transitive]] relations on ''E'' and ''D''<sub>''E''</sub> = ( ω<sup>''r''</sup> ∪ ω <sup>''l''</sup> ) ∪ ( ω<sup>''r ''</sup> ∪ ω<sup>''l''</sup> )<sup>−1</sup>.\n\n:(B21)&#8194; If ''f'' is in ω<sup>''r''</sup>( ''e'' ) then ''f R fe '' ω ''e''.  \n\n:(B22)&#8194; If ''g'' ω<sup>''l''</sup> ''f'' and if ''f'' and ''g'' are in ω<sup>''r''</sup> ( ''e'' ) then ''ge'' ω<sup>''l''</sup> ''fe''.\n\n:(B31)&#8194; If ''g'' ω<sup>''r''</sup> ''f'' and ''f'' ω<sup>''r''</sup> ''e'' then ''gf'' = ( ''ge'' )''f''.\n\n:(B32)&#8194; If ''g'' ω<sup>''l''</sup> ''f'' and if ''f'' and ''g'' are in ω<sup>''r''</sup> ( ''e'' ) then ( ''fg'' )''e'' = ( ''fe'' )( ''ge'' ).\n\nIn ''M'' ( ''e'', ''f'' ) = ω<sup>''l''</sup> ( ''e'' ) ∩ ω<sup>''r''</sup> ( ''f'' ) (the '''''M''-set''' of ''e'' and ''f'' in that order), define a relation <math>\\prec</math> by \n\n:<math>g \\prec h\\quad \\Longleftrightarrow  \\quad eg \\,\\,\\omega^r\\,\\, eh\\,,\\,\\,\\,   gf\\,\\, \\omega^l \\,\\,hf</math>.  \n\nThen the set\n\n:<math> S(e,f) = \\{ h\\in M(e,f) : g\\prec h \\text{ for all } g\\in M(e,f) \\} </math>\n\nis called the '''sandwich set''' of ''e'' and ''f'' in that order. \n\n:(B4)&#8195; If ''f'' and ''g'' are in ω<sup>''r''</sup> ( ''e'' ) then ''S''( ''f'', ''g'' )''e'' = ''S'' ( ''fe'', ''ge'' ).\n\n=== ''M''-biordered sets and regular biordered sets ===\n\nWe say that a biordered set ''E'' is an '''''M''-biordered set''' if ''M'' ( ''e'', ''f'' ) ≠ ∅ for all ''e'' and ''f'' in ''E''.  \nAlso, ''E'' is called a  '''regular biordered set''' if ''S'' ( ''e'', ''f'' ) ≠ ∅ for all ''e'' and ''f'' in ''E''.\n\nIn 2012 Roman S. Gigoń gave a simple proof that ''M''-biordered sets arise from [[E-inversive semigroup|''E''-inversive semigroup]]s.<ref>Gigoń, Roman (2012). \"Some results on ''E''-inversive semigroups\". Quasigroups and Related Systems '''20''': 53-60.</ref>{{clarify|date=August 2014}}\n\n== Subobjects and morphisms ==\n=== Biordered subsets ===\nA subset ''F''  of a biordered set ''E''  is a biordered subset (subboset) of ''E'' if ''F'' is a biordered set under the partial binary operation inherited from ''E''. \n\nFor any ''e'' in ''E'' the sets ω<sup>''r''</sup> ( ''e'' ),  ω<sup>''l''</sup> ( ''e'' ) and ω  ( ''e'' ) are biordered subsets of ''E''.<ref name=\"namb\"/>\n\n=== Bimorphisms ===\n\nA mapping φ : ''E'' → ''F'' between two biordered sets ''E'' and ''F'' is a biordered set  homomorphism (also called a bimorphism) if for all ( ''e'', ''f'' ) in ''D''<sub>''E''</sub> we have ( ''e''φ ) ( ''f''φ ) = ( ''ef'' )φ.\n\n== Illustrative examples ==\n=== Vector space example ===\n\nLet ''V'' be a [[vector space]] and \n\n:''E'' =  { ( ''A'', ''B'' ) | ''V'' =  ''A''  ⊕  ''B'' }\n\nwhere ''V''  = ''A''  ⊕  ''B''  means that ''A'' and ''B'' are [[Linear subspace|subspaces]] of ''V'' and ''V'' is the  [[internal direct sum]] of ''A'' and ''B''. \nThe partial binary operation ⋆ on E defined by \n\n:( ''A'', ''B'' ) ⋆ ( ''C'', ''D'' ) = ( ''A'' + ( ''B'' ∩ ''C'' ), ( ''B'' + ''C'' ) ∩ ''D '') \n\nmakes ''E'' a biordered set. The quasiorders in ''E'' are characterised as follows: \n\n:( ''A'', ''B'' ) ω<sup>''r''</sup> ( ''C'', ''D'' ) ⇔ ''A'' ⊇ ''C''\n:( ''A'', ''B'' ) ω<sup>''l''</sup> ( ''C'', ''D'' ) ⇔ ''B'' ⊆ ''D''\n\n=== Biordered set of a semigroup ===\n\nThe set ''E'' of idempotents in a semigroup ''S'' becomes a biordered set if a partial binary operation is defined in ''E'' as follows: ''ef'' is defined in ''E'' if and only if ''ef'' = ''e'' or ''ef''= ''f'' or ''fe'' = ''e'' or ''fe'' = ''f'' holds in ''S''. If ''S'' is a regular semigroup then ''E'' is a regular biordered set.\n\nAs a concrete example, let ''S'' be the semigroup of all mappings of ''X'' = { 1, 2, 3 } into itself. Let the symbol (''abc'') denote the map for which 1 → ''a'',  2 → ''b'', and 3 → ''c''. The set ''E'' of idempotents in ''S'' contains the following elements:\n\n:(111), (222), (333)  (constant maps)\n\n:(122), (133), (121), (323), (113), (223)\n\n:(123) (identity map)\n\nThe following table (taking composition of mappings in the diagram order) describes the partial binary operation in ''E''. An '''X''' in a cell indicates that the corresponding multiplication is not defined. \n<center>\n{| class=\"wikitable\" border=\"2\"\n|-\n! style=\"background:#ADD8E6\"|∗\n!style=\"background:#ADD8E6\"|&nbsp;(111)&nbsp;\n!style=\"background:#ADD8E6\"|&nbsp;(222)&nbsp;\n!style=\"background:#ADD8E6\"|&nbsp;(333)&nbsp;\n!style=\"background:#ADD8E6\"|&nbsp;(122)&nbsp;\n!style=\"background:#ADD8E6\"|&nbsp;(133)&nbsp;\n!style=\"background:#ADD8E6\"|&nbsp;(121)&nbsp;\n!style=\"background:#ADD8E6\"|&nbsp;(323)&nbsp;\n!style=\"background:#ADD8E6\"|&nbsp;(113)&nbsp;\n!style=\"background:#ADD8E6\"|&nbsp;(223)&nbsp;\n!style=\"background:#ADD8E6\"|&nbsp;(123)&nbsp;\n|-style=\"background:#FFFDD0\"\n!style=\"background:#ADD8E6\"|&nbsp;(111)&nbsp;\n|&nbsp;(111)||&nbsp;(222)||&nbsp;(333)||&nbsp;(111)||&nbsp;(111)||&nbsp;(111)||&nbsp;(333)||&nbsp;(111)||&nbsp;(222)||&nbsp;(111)\n|-style=\"background:#FFFDD0\"\n!style=\"background:#ADD8E6\"|&nbsp;(222)&nbsp;\n|&nbsp;(111)||&nbsp;(222)||&nbsp;(333)||&nbsp;(222)||&nbsp;(333)||&nbsp;(222)||&nbsp;(222)||&nbsp;(111)||&nbsp;(222)||&nbsp;(222)\n|-style=\"background:#FFFDD0\"\n!style=\"background:#ADD8E6\"|&nbsp;(333)&nbsp;\n|&nbsp;(111)||&nbsp;(222)||&nbsp;(333)||&nbsp;(222)||&nbsp;(333)||&nbsp;(111)||&nbsp;(333)||&nbsp;(333)||&nbsp;(333)||&nbsp;(333)\n|-\n!style=\"background:#ADD8E6\"|&nbsp;(122)&nbsp;\n|style=\"background:#FFFDD0\"|&nbsp;(111)||style=\"background:#FFFDD0\"|&nbsp;(222)||style=\"background:#FFFDD0\"|&nbsp;(333)||&nbsp;(122)||&nbsp;(122)||&nbsp;(121)||style=\"background:silver; color:red\"|&nbsp;&nbsp;&nbsp;'''X'''||style=\"background:silver; color:red\"|&nbsp;&nbsp;&nbsp;'''X'''||style=\"background:silver; color:red\"|&nbsp;&nbsp;&nbsp;'''X'''||&nbsp;(122)\n|-\n!style=\"background:#ADD8E6\"|&nbsp;(133)&nbsp;\n|style=\"background:#FFFDD0\"|&nbsp;(111)||style=\"background:#FFFDD0\"|&nbsp;(222)||style=\"background:#FFFDD0\"|&nbsp;(333)||&nbsp;(122)||&nbsp;(133)||style=\"background:silver; color:red\"|&nbsp;&nbsp;&nbsp;'''X'''||style=\"background:silver; color:red\"|&nbsp;&nbsp;&nbsp;'''X'''||&nbsp;(133)||style=\"background:silver; color:red\"|&nbsp;&nbsp;&nbsp;'''X'''||&nbsp;(133)\n|-\n!style=\"background:#ADD8E6\"|&nbsp;(121)&nbsp;\n|style=\"background:#FFFDD0\"|&nbsp;(111)||style=\"background:#FFFDD0\"|&nbsp;(222)||style=\"background:#FFFDD0\"|&nbsp;(333)||&nbsp;(121) ||style=\"background:silver; color:red\"|&nbsp;&nbsp;&nbsp;'''X'''||&nbsp;(121)||&nbsp;(323)||style=\"background:silver; color:red\"|&nbsp;&nbsp;&nbsp;'''X'''||style=\"background:silver; color:red\"|&nbsp;&nbsp;&nbsp;'''X'''||&nbsp;(121)\n|-\n!style=\"background:#ADD8E6\"|&nbsp;(323)&nbsp;\n|style=\"background:#FFFDD0\"|&nbsp;(111)||style=\"background:#FFFDD0\"|&nbsp;(222)||style=\"background:#FFFDD0\"|&nbsp;(333)||style=\"background:silver; color:red\"|&nbsp;&nbsp;&nbsp;'''X'''||style=\"background:silver; color:red\"|&nbsp;&nbsp;&nbsp;'''X'''||&nbsp;(121)||&nbsp;&nbsp;(323)||style=\"background:silver; color:red\"|&nbsp;&nbsp;&nbsp;'''X'''||&nbsp;(323)||&nbsp;(323)\n|-\n!style=\"background:#ADD8E6\"|&nbsp;(113)&nbsp;\n|style=\"background:#FFFDD0\"|&nbsp;(111)||style=\"background:#FFFDD0\"|&nbsp;(222)||style=\"background:#FFFDD0\"|&nbsp;(333)||style=\"background:silver; color:red\"|&nbsp;&nbsp;&nbsp;'''X'''||&nbsp;(113)||style=\"background:silver; color:red\"|&nbsp;&nbsp;&nbsp;'''X'''||style=\"background:silver; color:red\"|&nbsp;&nbsp;&nbsp;'''X'''||&nbsp;(113)||&nbsp;(223)||&nbsp;(113)\n|-\n!style=\"background:#ADD8E6\"|&nbsp;(223)&nbsp;\n|style=\"background:#FFFDD0\"|&nbsp;(111)||style=\"background:#FFFDD0\"|&nbsp;(222)||style=\"background:#FFFDD0\"|&nbsp;(333)||style=\"background:silver; color:red\"|&nbsp;&nbsp;&nbsp;'''X'''||style=\"background:silver; color:red\"|&nbsp;&nbsp;&nbsp;'''X'''||style=\"background:silver; color:red\"|&nbsp;&nbsp;&nbsp;'''X'''||&nbsp;(233)||&nbsp;(113)||&nbsp;(223)||&nbsp;(223)\n|-\n!style=\"background:#ADD8E6\"|&nbsp;(123)&nbsp;\n|style=\"background:#FFFDD0\"|&nbsp;(111)||style=\"background:#FFFDD0\"|&nbsp;(222)||style=\"background:#FFFDD0\"|&nbsp;(333)||&nbsp;(122)||&nbsp;(133)||&nbsp;(121)||&nbsp;(323)||&nbsp;(113)||&nbsp;(223)||&nbsp;(123)\n|}\n</center>\n== References ==\n\n{{reflist}}\n\n[[Category:Semigroup theory]]\n[[Category:Algebraic structures]]\n[[Category:Mathematical structures]]"
    },
    {
      "title": "Biquandle",
      "url": "https://en.wikipedia.org/wiki/Biquandle",
      "text": "In [[mathematics]], '''biquandles''' and '''biracks''' are generalizations of [[racks and quandles|quandles and racks]]. Whereas the hinterland of quandles and racks is the theory of classical [[knot]]s, that of the bi-versions, is the theory of [[virtual knot]]s.\n\nBiquandles and biracks have two binary operations on a set <math> X </math> written <math> a^b </math> and <math> a_b </math>. These satisfy the following three axioms:\n\n1.  <math> a^{b c_b}= {a^c}^{b^c} </math>\n\n2.  <math> {a_b}_{c_b}= {a_c}_{b^c} </math>\n\n3.  <math> {a_b}^{c_b}= {a^c}_{b^c} </math>\n\nThese identities appeared in 1992 in reference [FRS] where the object was called a species.\n\nThe superscript and subscript notation is useful here because it dispenses with the need for brackets. For example,\nif we write <math> a*b </math> for <math> a_b </math> and <math> a**b </math> for <math> a^b </math> then the\nthree axioms above become\n\n1. <math> (a**b)**(c*b)=(a**c)**(b**c) </math>\n\n2. <math> (a*b)*(c*b)=(a*c)*(b**c) </math>\n\n3. <math> (a*b)**(c*b)=(a**c)*(b**c) </math>\n\nFor other notations see racks and quandles.\n\nIf in addition the two operations are [[invertible]], that is given <math> a, b  </math> in the set <math> X </math> there are unique <math> x, y  </math> in the set <math> X </math> such that <math> x^b=a </math> and <math> y_b=a </math> then the set <math> X </math> together with the two operations define a [[birack]].\n\nFor example, if <math> X </math>, with the operation <math> a^b </math>, is a [[quandle|rack]] then it is a birack if we define the other operation to be the [[Identity (mathematics)|identity]],  <math> a_b=a </math>.\n\nFor a birack the function <math> S:X^2 \\rightarrow X^2 </math> can be defined by\n\n: <math> S(a,b_a)=(b,a^b).\\,</math>\n\nThen\n\n1. <math> S </math> is a [[bijection]]\n\n2. <math> S_1S_2S_1=S_2S_1S_2 \\, </math>\n\nIn the second condition,  <math> S_1 </math> and <math> S_2 </math> are defined by <math> S_1(a,b,c)=(S(a,b),c)</math> and <math> S_2(a,b,c)=(a,S(b,c))</math>. This condition is sometimes known as the [[set-theoretic]] [[Yang-Baxter]] equation.\n\nTo see that 1. is true note that <math> S' </math> defined by\n\n: <math> S'(b,a^b)=(a,b_a)\\, </math>\n\nis the inverse to\n\n:<math> S \\,</math>\n\nTo see that 2. is true let us follow the progress of the triple <math> (c,b_c,a_{bc^b}) </math> under <math> S_1S_2S_1 </math>. So\n\n: <math> (c,b_c,a_{bc^b}) \\to (b,c^b,a_{bc^b}) \\to (b,a_b,c^{ba_b}) \\to (a, b^a, c^{ba_b}). </math>\n\nOn the other hand, <math> (c,b_c,a_{bc^b}) = (c, b_c, a_{cb_c}) </math>. Its progress under <math> S_2S_1S_2 </math> is\n\n: <math> (c, b_c, a_{cb_c}) \\to (c, a_c, {b_c}^{a_c}) \\to (a, c^a, {b_c}^{a_c}) = (a, c^a, {b^a}_{c^a}) \\to (a, b_a, c_{ab_a}) = (a, b^a, c^{ba_b}). </math>\n\nAny <math> S </math> satisfying 1. 2. is said to be a ''switch'' (precursor of biquandles and biracks).\n\nExamples of switches are the identity, the ''twist'' <math> T(a,b)=(b,a) </math> and <math> S(a,b)=(b,a^b) </math> where <math> a^b </math> is the operation of a rack.\n\nA switch will define a birack if the operations are invertible. Note that the identity switch does not do this.\n\n==Biquandles==\nA biquandle is a birack which satisfies some additional structure, as [https://arxiv.org/abs/0708.1951v1 described] by Nelson and Rische. The axioms of a biquandle are \"minimal\" in the sense that they are the weakest restrictions that can be placed on the two binary operations while making the biquandle of a virtual knot invariant under Reidemeister moves.\n\n==Linear biquandles==\n{{Empty section|date=November 2014}}\n\n==Application to virtual links and braids==\n{{Empty section|date=November 2014}}\n\n==Birack homology==\n{{Empty section|date=November 2014}}\n\n==Further reading==\n* [FJK] Roger Fenn, Mercedes Jordan-Santana, [[Louis Kauffman]] ''Biquandles and Virtual Links'', '''Topology and its Applications''', 145 (2004) 157–175\n* [FRS] Roger Fenn, [[Colin Rourke]], Brian Sanderson ''An Introduction to Species and the Rack Space'' in '''Topics in Knot Theory (1992)''', [[Kluwer]] 33–55\n* [K] L. H. Kauffman, ''Virtual Knot Theory'', '''European Journal of Combinatorics''' 20 (1999), 663–690.\n\n[[Category:Knot theory]]\n[[Category:Algebraic structures]]"
    },
    {
      "title": "Boolean algebra (structure)",
      "url": "https://en.wikipedia.org/wiki/Boolean_algebra_%28structure%29",
      "text": "{{short description|Algebraic structure modeling logical operations}}\n{{about||an introduction to the subject|Boolean algebra|an alternative presentation|Boolean algebras canonically defined}}\n{{insufficient inline citations|date=July 2013}}\nIn [[abstract algebra]], a '''Boolean algebra''' or '''Boolean lattice''' is a [[complemented lattice|complemented]] [[distributive lattice]]. This type of [[algebraic structure]] captures essential properties of both [[Set (mathematics)|set]] operations and [[logic]] operations. A Boolean algebra can be seen as a generalization of a [[power set]] algebra or a [[field of sets]], or its elements can be viewed as generalized [[truth value]]s. It is also a special case of a [[De Morgan algebra]] and a [[Kleene algebra (with involution)]].\n\nEvery Boolean algebra [[Boolean algebra (structure)#Boolean rings|gives rise]] to a [[Boolean ring]], and vice versa, with ring multiplication corresponding to [[logical conjunction|conjunction]] or [[meet (mathematics)|meet]] ∧, and ring addition to [[exclusive or|exclusive disjunction]] or [[symmetric difference]] (not [[logical disjunction|disjunction]] ∨). However, the theory of Boolean rings has an inherent asymmetry between the two operators, while the axioms and theorems of Boolean algebra express the symmetry of the theory described by the [[Duality principle (Boolean algebra)|duality principle]].<ref>Givant and [[Paul Halmos]], 2009, p.&nbsp;20</ref>\n[[Image:Hasse diagram of powerset of 3.svg|thumb|right|250px|Boolean lattice of subsets]]\n__TOC__\n\n== History ==  <!-- [[Boolean algebra (history)]] redirects here -->\n\nThe term \"Boolean algebra\" honors [[George Boole]] (1815–1864), a self-educated English mathematician. He introduced the [[algebraic system]] initially in a small pamphlet, ''The Mathematical Analysis of Logic'', published in 1847 in response to an ongoing public controversy between [[Augustus De Morgan]] and [[Sir William Hamilton, 9th Baronet|William Hamilton]], and later as a more substantial book, ''[[The Laws of Thought]]'', published in 1854. Boole's formulation differs from that described above in some important respects. For example, conjunction and disjunction in Boole were not a dual pair of operations. Boolean algebra emerged in the 1860s, in papers written by [[William Jevons]] and [[Charles Sanders Peirce]]. The first systematic presentation of Boolean algebra and [[distributive lattice]]s is owed to the 1890 ''Vorlesungen'' of [[Ernst Schröder]]. The first extensive treatment of Boolean algebra in English is [[A. N. Whitehead]]'s 1898 ''Universal Algebra''. Boolean algebra as an axiomatic algebraic structure in the modern axiomatic sense begins with a 1904 paper by [[Edward V. Huntington]]. Boolean algebra came of age as serious mathematics with the work of [[Marshall Stone]] in the 1930s, and with [[Garrett Birkhoff]]'s 1940 ''Lattice Theory''. In the 1960s, [[Paul Cohen (mathematician)|Paul Cohen]], [[Dana Scott]], and others found deep new results in [[mathematical logic]] and [[axiomatic set theory]] using offshoots of Boolean algebra, namely [[forcing (mathematics)|forcing]] and [[Boolean-valued model]]s.\n\n== Definition ==\n\nA '''Boolean algebra''' is a six-[[tuple]] consisting of a [[Set (mathematics)|set]] ''A'', equipped with two [[binary operation]]s ∧ (called \"meet\" or \"and\"), ∨ (called \"join\" or \"or\"), a [[unary operation]] ¬ (called \"complement\" or \"not\") and two elements 0 and 1 (called \"bottom\" and \"top\", or \"least\" and \"greatest\" element, also denoted by the symbols ⊥ and ⊤, respectively), such that for all elements ''a'', ''b'' and ''c'' of ''A'', the following [[axiom]]s hold:<ref>Davey, Priestley, 1990, p.109, 131, 144</ref>\n\n::{| cellpadding=5\n|''a'' ∨ (''b'' ∨ ''c'') = (''a'' ∨ ''b'') ∨ ''c''\n|''a'' ∧ (''b'' ∧ ''c'') = (''a'' ∧ ''b'') ∧ ''c''\n| [[associativity]]\n|-\n|''a'' ∨ ''b'' = ''b'' ∨ ''a''\n|''a'' ∧ ''b'' = ''b'' ∧ ''a''\n| [[commutativity]]\n|-\n|''a'' ∨ (''a'' ∧ ''b'') = ''a''\n|''a'' ∧ (''a'' ∨ ''b'') = ''a''\n| [[Absorption law|absorption]]\n|-\n|''a'' ∨ 0 = ''a''\n|''a'' ∧ 1 = ''a''\n| [[identity element|identity]]\n|-\n|''a'' ∨ (''b'' ∧ ''c'') = (''a'' ∨ ''b'') ∧ (''a'' ∨ ''c'')&nbsp;&nbsp;\n|''a'' ∧ (''b'' ∨ ''c'') = (''a'' ∧ ''b'') ∨ (''a'' ∧ ''c'')&nbsp;&nbsp;\n| [[distributivity]]\n|-\n|''a'' ∨ ¬''a'' = 1\n|''a'' ∧ ¬''a'' = 0\n| [[complemented lattice|complements]]\n|}\nNote, however, that the absorption law and even the associativity law can be excluded from the set of axioms as they can be derived from the other axioms (see [[Boolean algebra (structure)#Axiomatics|Proven properties]]).\n\nA Boolean algebra with only one element is called a '''trivial Boolean algebra''' or a '''degenerate Boolean algebra'''. (Some authors require 0 and 1 to be ''distinct'' elements in order to exclude this case.)\n\nIt follows from the last three pairs of axioms above (identity, distributivity and complements), or from the absorption axiom, that\n::''a'' = ''b'' ∧ ''a'' &nbsp;&nbsp;&nbsp; if and only if &nbsp;&nbsp;&nbsp; ''a'' ∨ ''b'' = ''b''.\nThe relation ≤ defined by ''a'' ≤ ''b'' if these equivalent conditions hold, is a [[partial order]] with least element 0 and greatest element 1. The meet ''a'' ∧ ''b'' and the join ''a'' ∨ ''b'' of two elements coincide with their [[infimum]] and [[supremum]], respectively, with respect to ≤.\n\nThe first four pairs of axioms constitute a definition of a [[bounded lattice]].\n\nIt follows from the first five pairs of axioms that any complement is unique.\n\nThe set of axioms is [[duality (order theory)|self-dual]] in the sense that if one exchanges ∨ with ∧ and 0 with 1 in an axiom, the result is again an axiom. Therefore, by applying this operation to a Boolean algebra (or Boolean lattice), one obtains another Boolean algebra with the same elements; it is called its '''dual'''.<ref>{{citation|title=Boolean Algebra|first=R. L.|last=Goodstein|publisher=Courier Dover Publications|year=2012|isbn=9780486154978|chapter=Chapter 2: The self-dual system of axioms|pages=21ff|chapter-url=https://books.google.com/books?id=0fxW2KiyxWwC&pg=PA21}}.</ref>\n\n== Examples ==\n\n* The simplest non-trivial Boolean algebra, the [[two-element Boolean algebra]], has only two elements, 0 and 1, and is defined by the rules:\n{|\n|-\n| width=\"70\" |\n|\n{| class=\"wikitable\" border=\"1\" cellpadding=\"4\" cellspacing=\"0\"\n|-\n! ∧ || 0 || 1\n|-\n! 0\n| 0 || 0\n|-\n! 1\n| 0 || 1\n|}\n|\n| width=\"30\" |\n|\n{| class=\"wikitable\" border=\"1\" cellpadding=\"4\" cellspacing=\"0\"\n|-\n! ∨ || 0 || 1\n|-\n! 0\n| 0 || 1\n|-\n! 1\n| 1 || 1\n|}\n|\n| width=\"40\" |\n|\n{| class=\"wikitable\" border=\"1\" cellpadding=\"4\" cellspacing=\"0\"\n|-\n! ''a'' || 0 || 1\n|-\n! ¬''a''\n| 1 || 0\n|}\n|}\n\n:* It has applications in [[logic]], interpreting 0 as ''false'', 1 as ''true'', ∧ as ''and'', ∨ as ''or'', and ¬ as ''not''. Expressions involving variables and the Boolean operations represent statement forms, and two such expressions can be shown to be equal using the above axioms if and only if the corresponding statement forms are [[logical equivalence|logically equivalent]].\n\n:* The two-element Boolean algebra is also used for circuit design in [[electrical engineering]];<ref>Strictly, electrical engineers tend to use additional states to represent other circuit conditions such as high impedance - see [[IEEE 1164]] or [[IEEE 1364]].</ref> here 0 and 1 represent the two different states of one [[bit]] in a [[digital circuit]], typically high and low [[voltage]]. Circuits are described by expressions containing variables, and two such expressions are equal for all values of the variables if and only if the corresponding circuits have the same input-output behavior. Furthermore, every possible input-output behavior can be modeled by a suitable Boolean expression.\n\n:* The two-element Boolean algebra is also important in the general theory of Boolean algebras, because an equation involving several variables is generally true in all Boolean algebras if and only if it is true in the two-element Boolean algebra (which can be checked by a trivial [[brute force search|brute force]] algorithm for small numbers of variables). This can for example be used to show that the following laws (''Consensus theorems'') are generally valid in all Boolean algebras:\n:** (''a'' ∨ ''b'') ∧ (¬''a'' ∨ ''c'') ∧ (''b'' ∨ ''c'') ≡ (''a'' ∨ ''b'') ∧ (¬''a'' ∨ ''c'')\n:** (''a'' ∧ ''b'') ∨ (¬''a'' ∧ ''c'') ∨ (''b'' ∧ ''c'') ≡ (''a'' ∧ ''b'') ∨ (¬''a'' ∧ ''c'')\n\n* The [[power set]] (set of all subsets) of any given nonempty set ''S'' forms a Boolean algebra, an [[algebra of sets]], with the two operations ∨ := ∪ (union) and ∧ := ∩ (intersection). The smallest element 0 is the [[empty set]] and the largest element 1 is the set ''S'' itself.\n\n:* After the two-element Boolean algebra, the simplest Boolean algebra is that defined by the [[power set]] of two atoms:\n{|\n|-\n| width=\"70\" |\n|\n{| class=\"wikitable\" border=\"1\" cellpadding=\"4\" cellspacing=\"0\"\n|-\n! ∧ || 0 || a || b || 1\n|-\n! 0\n| 0 || 0 || 0 || 0\n|-\n! a\n| 0 || a || 0 || a\n|-\n! b\n| 0 || 0 || b || b\n|-\n! 1\n| 0 || a || b || 1\n|}\n|\n| width=\"30\" |\n|\n{| class=\"wikitable\" border=\"1\" cellpadding=\"4\" cellspacing=\"0\"\n|-\n! ∨ || 0 || a || b || 1\n|-\n! 0\n| 0 || a || b || 1\n|-\n! a\n| a || a || 1 || 1\n|-\n! b\n| b || 1 || b || 1\n|-\n! 1\n| 1 || 1 || 1 || 1\n|}\n|\n| width=\"40\" |\n|\n{| class=\"wikitable\" border=\"1\" cellpadding=\"4\" cellspacing=\"0\"\n|-\n! ''x'' || 0 || a || b || 1\n|-\n! ¬''x''\n| 1 || b || a || 0\n|}\n|}\n\n* The set of all subsets of ''S'' that are either finite or [[cofinite]] is a Boolean algebra, an [[algebra of sets]].\n* Starting with the [[propositional calculus]] with κ sentence symbols, form the [[Lindenbaum–Tarski algebra|Lindenbaum algebra]] (that is, the set of sentences in the propositional calculus modulo [[logical equivalence]]).  This construction yields a Boolean algebra.  It is in fact the [[free Boolean algebra]] on κ generators.  A truth assignment in propositional calculus is then a Boolean algebra homomorphism from this algebra to the two-element Boolean algebra.\n* Given any [[linearly ordered]] set ''L'' with a least element, the interval algebra is the smallest algebra of subsets of ''L'' containing all of the half-open intervals [''a'', ''b'') such that ''a'' is in ''L'' and ''b'' is either in ''L'' or equal to ∞.  Interval algebras are useful in the study of [[Lindenbaum–Tarski algebra]]s; every countable Boolean algebra is isomorphic to an interval algebra.\n\n[[File:Lattice T 30.svg|thumb|x150px|[[Hasse diagram]] of the Boolean algebra of divisors of 30.]]\n* For any [[natural number]] ''n'', the set of all positive [[divisor]]s of ''n'', defining ''a''≤''b'' if ''a'' [[divides]] ''b'', forms a [[distributive lattice]]. This lattice is a Boolean algebra if and only if ''n'' is [[square-free integer|square-free]]. The bottom and the top element of this Boolean algebra is the natural number 1 and ''n'', respectively. The complement of ''a'' is given by ''n''/''a''. The meet and the join of ''a'' and ''b'' is given by the [[greatest common divisor]] (gcd) and the [[least common multiple]] (lcm) of ''a'' and ''b'', respectively. The ring addition ''a''+''b'' is given by lcm(''a'',''b'')/gcd(''a'',''b''). The picture shows an example for ''n'' = 30. As a counter-example, considering the non-square-free ''n''=60, the greatest common divisor of 30 and its complement 2 would be 2, while it should be the bottom element 1.\n* Other examples of Boolean algebras arise from [[topology|topological spaces]]: if ''X'' is a topological space, then the collection of all subsets of ''X'' which are [[Clopen set|both open and closed]] forms a Boolean algebra with the operations ∨ := ∪ (union) and ∧ := ∩ (intersection).\n* If ''R'' is an arbitrary [[mathematical ring|ring]] and we define the set of ''central idempotents'' by <br> ''A'' = { ''e'' ∈ ''R'' : ''e''<sup>2</sup> = ''e'', ''ex'' = ''xe'', ∀''x'' ∈ ''R'' } <br> then the set ''A'' becomes a Boolean algebra with the operations ''e'' ∨ ''f'' := ''e'' + ''f'' - ''ef'' and ''e'' ∧ ''f'' := ''ef''.\n\n== Homomorphisms and isomorphisms ==\n<!-- \"Boolean homomorphism\" redirects here -->\n\nA ''[[homomorphism]]'' between two Boolean algebras ''A'' and ''B'' is a [[function (mathematics)|function]] ''f'' : ''A'' → ''B'' such that for all ''a'', ''b'' in ''A'':\n\n: ''f''(''a'' ∨ ''b'') = ''f''(''a'') ∨ ''f''(''b''),\n: ''f''(''a'' ∧ ''b'') = ''f''(''a'') ∧ ''f''(''b''),\n: ''f''(0) = 0,\n: ''f''(1) = 1.\n\nIt then follows that ''f''(¬''a'') = ¬''f''(''a'') for all ''a'' in ''A''. The [[class (set theory)|class]] of all Boolean algebras, together with this notion of morphism, forms a [[full subcategory]] of the [[category theory|category]] of lattices.\n<!--The constant function with ''f''(''a'') = 1 for all ''a'' in ''A'' satisfies the first, second, and fourth conditions but not the third (unless ''B'' is the degenerate singleton Boolean algebra with 0 = 1), so it is not a Boolean algebra homomorphism.-->\n\nAn ''isomorphism'' between two Boolean algebras ''A'' and ''B'' is a homomorphism ''f'' : ''A'' → ''B'' with an inverse homomorphism, that is, a homomorphism ''g'' : ''B'' → ''A'' such that the [[function composition|composition]] ''g'' ◌ ''f'': ''A'' → ''A'' is the [[identity function]] on ''A'', and the composition ''f'' ◌ ''g'': ''B'' → ''B'' is the identity function on ''B''.  A homomorphism of Boolean algebras is an isomorphism if and only if it is [[bijection|bijective]].\n\n== Boolean rings ==\n{{Main|Boolean ring}}\nEvery Boolean algebra (A, ∧, ∨) gives rise to a [[ring (algebra)|ring]] (''A'', +, ·) by defining ''a'' + ''b'' := (''a'' ∧ ¬''b'') ∨ (''b'' ∧ ¬''a'') = (''a'' ∨ ''b'') ∧ ¬(''a'' ∧ ''b'') (this operation is called [[symmetric difference]] in the case of sets and [[Truth table#Exclusive disjunction|XOR]] in the case of logic) and ''a'' · ''b'' := ''a'' ∧ ''b''. The zero element of this ring coincides with the 0 of the Boolean algebra; the multiplicative identity element of the ring is the 1 of the Boolean algebra. This ring has the property that ''a'' · ''a'' = ''a'' for all ''a'' in ''A''; rings with this property are called [[Boolean ring]]s.\n\nConversely, if a Boolean ring ''A'' is given, we can turn it into a Boolean algebra by defining ''x'' ∨ ''y'' := ''x'' + ''y'' + (''x'' · ''y'') and ''x'' ∧ ''y'' := ''x'' · ''y''.\n<ref>Stone, 1936</ref><ref>Hsiang, 1985, p.260</ref>\nSince these two constructions are inverses of each other, we can say that every Boolean ring arises from a Boolean algebra, and vice versa. Furthermore, a map ''f'' : ''A'' → ''B'' is a homomorphism of Boolean algebras if and only if it is a homomorphism of Boolean rings. The [[category theory|categories]] of Boolean rings and Boolean algebras are equivalent.<ref>{{harvtxt|Cohn|2003}}, [https://books.google.com/books?id=VESm0MJOiDQC&pg=PA81 p.&nbsp;81].</ref>\n\nHsiang (1985) gave a [[Abstract rewriting system|rule-based algorithm]] to [[Word problem (mathematics)|check]] whether two arbitrary expressions denote the same value in every Boolean ring.\n<!---probably too much details(?):---\nHsiang (1985) gave a [[Confluence (abstract rewriting)|confluent]] and [[Abstract_rewriting_system#Termination_and_convergence|terminating]] [[Abstract rewriting system|rewrite system]] for Boolean rings, thus solving their [[Word problem (mathematics)|word problem]]: to check whether two arbitrary expressions ''s'' and ''t'' denote the same value in every Boolean ring, apply rewrite rules to ''s'' as long as possible, resulting in an expression ''s''<sub>n</sub>, obtain ''t''<sub>n</sub> from ''t'' in a similar way, and check whether ''s''<sub>n</sub> and ''t''<sub>n</sub> are literally identical, except for different parenthezation and order of operands of \"+\" or \"·\".\n--->\nMore generally, Boudet, [[Jean-Pierre Jouannaud|Jouannaud]], and Schmidt-Schauß (1989) gave an algorithm to [[Unification (computer science)#Particular background knowledge sets E|solve equations]] between arbitrary Boolean-ring expressions.\nEmploying the similarity of Boolean rings and Boolean algebras, both algorithms have applications in [[automated theorem proving]].\n\n== Ideals and filters ==\n{{Main|Ideal (order theory)|Filter (mathematics)}}\nAn ''ideal'' of the Boolean algebra ''A'' is a subset ''I'' such that for all ''x'', ''y'' in ''I'' we have ''x'' ∨ ''y'' in ''I'' and for all ''a'' in ''A'' we have ''a'' ∧ ''x'' in ''I''. This notion of ideal coincides with the notion of [[ring ideal]] in the Boolean ring ''A''. An ideal ''I'' of ''A'' is called ''prime'' if ''I'' ≠ ''A'' and if ''a'' ∧ ''b'' in ''I'' always implies ''a'' in ''I'' or ''b'' in ''I''. Furthermore, for every ''a'' ∈ ''A'' we have that ''a'' ∧ ''-a'' = 0 ∈ ''I'' and then ''a'' ∈ ''I'' or ''-a'' ∈ ''I'' for every ''a'' ∈ ''A'', if ''I'' is prime. An ideal ''I'' of ''A'' is called ''maximal'' if ''I'' ≠ ''A'' and if the only ideal properly containing ''I'' is ''A'' itself. For an ideal ''I'', if ''a'' ∉ ''I'' and ''-a'' ∉ ''I'', then ''I'' ∪ {''a''} or ''I'' ∪ {''-a''} is properly contained in another ideal ''J''. Hence, that an ''I'' is not maximal and therefore the notions of prime ideal and maximal ideal are equivalent in Boolean algebras. Moreover, these notions coincide with ring theoretic ones of [[prime ideal]] and [[maximal ideal]] in the Boolean ring ''A''.\n\nThe dual of an ''ideal'' is a ''filter''. A ''filter'' of the Boolean algebra ''A'' is a subset ''p'' such that for all ''x'', ''y'' in ''p'' we have ''x'' ∧ ''y'' in ''p'' and for all ''a'' in ''A'' we have ''a'' ∨ ''x'' in ''p''. The dual of a ''maximal'' (or ''prime'') ''ideal'' in a Boolean algebra is ''[[ultrafilter]]''. Ultrafilters can alternatively be described as [[2-valued morphism]]s from ''A'' to the two-element Boolean algebra. The statement ''every filter in a Boolean algebra can be extended to an ultrafilter'' is called the ''[[Boolean prime ideal theorem#The ultrafilter lemma|Ultrafilter Theorem]]'' and cannot be proven in [[Zermelo–Fraenkel set theory|ZF]], if [[Zermelo–Fraenkel set theory|ZF]] is [[consistent]]. Within ZF, it is strictly weaker than the [[axiom of choice]].\nThe Ultrafilter Theorem has many equivalent formulations: ''every Boolean algebra has an ultrafilter'', ''every ideal in a Boolean algebra can be extended to a prime ideal'', etc.\n\n== Representations ==\n\nIt can be shown that every ''finite'' Boolean algebra is isomorphic to the Boolean algebra of all subsets of a finite set.  Therefore, the number of elements of every finite Boolean algebra is a [[power of two]].\n\n[[Marshall H. Stone|Stone's]] celebrated ''[[Stone's representation theorem for Boolean algebras|representation theorem for Boolean algebras]]'' states that ''every'' Boolean algebra ''A'' is isomorphic to the Boolean algebra of all [[clopen set|clopen]] sets in some ([[compact space|compact]] [[totally disconnected]] [[Hausdorff space|Hausdorff]]) topological space.\n\n== Axiomatics ==\n\n{| align=\"right\" class=\"wikitable collapsible collapsed\" style=\"text-align:left\"\n! colspan=\"2\" | '''Proven properties'''\n|- valign=\"top\"\n|\n{| align=\"left\" class=\"collapsible collapsed\" style=\"text-align:left\"\n! '''UId<sub>1</sub>''' !!   !! colspan=\"2\" | If ''x'' ∨ ''o'' = ''x'' for all ''x'', then ''o'' = 0\n|-\n| Proof:      ||    || colspan=\"2\" | If ''x'' ∨ ''o'' = ''x'', then\n|-\n|             ||  || 0\n|-\n|             || = || 0 ∨ ''o'' || by assumption\n|-\n|             || = || ''o'' ∨ 0 || by '''Cmm<sub>1</sub>'''\n|-\n|             || = || ''o'' || by '''Idn<sub>1</sub>'''\n|}\n| '''UId<sub>2</sub>''' &nbsp; [dual] &nbsp; If ''x'' ∧ ''i'' = ''x'' for all ''x'', then ''i'' = 1\n|- valign=\"top\"\n|\n{| align=\"left\" class=\"collapsible collapsed\" style=\"text-align:left\"\n! '''Idm<sub>1</sub>''' !!   !! ''x'' ∨ ''x'' = ''x''\n|-\n| Proof:      ||   || ''x'' ∨ ''x''\n|-\n|             || = || (''x'' ∨ ''x'') ∧ 1 || by '''Idn<sub>2</sub>'''\n|-\n|             || = || (''x'' ∨ ''x'') ∧ (''x'' ∨ ¬''x'') || by '''Cpl<sub>1</sub>'''\n|-\n|             || = || ''x'' ∨ (''x'' ∧ ¬''x'') || by '''Dst<sub>1</sub>'''\n|-\n|             || = || ''x'' ∨ 0 || by '''Cpl<sub>2</sub>'''\n|-\n|             || = || ''x'' || by '''Idn<sub>1</sub>'''\n|}\n| '''Idm<sub>2</sub>''' &nbsp; [dual] &nbsp; ''x'' ∧ ''x'' = ''x''\n|- valign=\"top\"\n|\n{| align=\"left\" class=\"collapsible collapsed\" style=\"text-align:left\"\n! '''Bnd<sub>1</sub>''' !!   !! ''x'' ∨ 1 = 1\n|-\n| Proof:      ||   || ''x'' ∨ 1\n|-\n|             || = || (''x'' ∨ 1) ∧ 1 || by '''Idn<sub>2</sub>'''\n|-\n|             || = || 1 ∧ (''x'' ∨ 1) || by '''Cmm<sub>2</sub>'''\n|-\n|             || = || (''x'' ∨ ¬''x'') ∧ (''x'' ∨ 1) || by '''Cpl<sub>1</sub>'''\n|-\n|             || = || ''x'' ∨ (¬''x'' ∧ 1) || by '''Dst<sub>1</sub>'''\n|-\n|             || = || ''x'' ∨ ¬''x'' || by '''Idn<sub>2</sub>'''\n|-\n|             || = || 1 || by '''Cpl<sub>1</sub>'''\n|}\n| '''Bnd<sub>2</sub>''' &nbsp; [dual] &nbsp; ''x'' ∧ 0 = 0\n|- valign=\"top\"\n|\n{| align=\"left\" class=\"collapsible collapsed\" style=\"text-align:left\"\n! '''Abs<sub>1</sub>'''  !!   !! ''x'' ∨ (''x'' ∧ ''y'')  = ''x''\n|-\n| Proof:      ||   || ''x'' ∨ (''x'' ∧ ''y'')\n|-\n|             || = || (''x'' ∧ 1) ∨ (''x'' ∧ ''y'') || by '''Idn<sub>2</sub>'''\n|-\n|             || = || ''x'' ∧ (1 ∨ ''y'') || by '''Dst<sub>2</sub>'''\n|-\n|             || = || ''x'' ∧ (''y'' ∨ 1) || by '''Cmm<sub>1</sub>'''\n|-\n|             || = || ''x'' ∧ 1 || by '''Bnd<sub>1</sub>'''\n|-\n|             || = || ''x'' || by '''Idn<sub>2</sub>'''\n|}\n| '''Abs<sub>2</sub>''' &nbsp; [dual] &nbsp; ''x'' ∧ (''x'' ∨ ''y'') = ''x''\n|- valign=\"top\"\n| colspan=\"2\" |\n{| align=\"left\" class=\"collapsible collapsed\" style=\"text-align:left\"\n! '''UNg'''  !!   !! colspan=\"2\" | If ''x'' ∨ ''x''<sub>n</sub> = 1 and ''x'' ∧ ''x''<sub>n</sub> = 0, then ''x''<sub>n</sub> = ¬''x''\n|-\n| Proof:      ||   || colspan=\"2\" | If ''x'' ∨ ''x''<sub>n</sub> = 1 and ''x'' ∧ ''x''<sub>n</sub> = 0, then\n|-\n|             ||   ||''x''<sub>n</sub>\n|-\n|             || = || ''x''<sub>n</sub> ∧ 1 || by '''Idn<sub>2</sub>'''\n|-\n|             || = || ''x''<sub>n</sub> ∧ (''x'' ∨ ¬''x'') || by '''Cpl<sub>1</sub>'''\n|-\n|             || = || (''x''<sub>n</sub> ∧ ''x'') ∨ (''x''<sub>n</sub> ∧ ¬''x'') || by '''Dst<sub>2</sub>'''\n|-\n|             || = || (''x'' ∧ ''x''<sub>n</sub>) ∨ (¬''x'' ∧ ''x''<sub>n</sub>) || by '''Cmm<sub>2</sub>'''\n|-\n|             || = || 0 ∨ (¬''x'' ∧ ''x''<sub>n</sub>) || by assumption\n|-\n|             || = || (''x'' ∧ ¬''x'')  ∨ (¬''x'' ∧ ''x''<sub>n</sub>) || by '''Cpl<sub>2</sub>'''\n|-\n|             || = || (¬''x'' ∧ ''x'')  ∨ (¬''x'' ∧ ''x''<sub>n</sub>) || by '''Cmm<sub>2</sub>'''\n|-\n|             || = || ¬''x'' ∧ (''x'' ∨ ''x''<sub>n</sub>) || by '''Dst<sub>2</sub>'''\n|-\n|             || = || ¬''x'' ∧ 1 || by assumption\n|-\n|             || = || ¬''x'' || by '''Idn<sub>2</sub>'''\n|}\n|- valign=\"top\"\n| colspan=\"2\" |\n{| align=\"left\" class=\"collapsible collapsed\" style=\"text-align:left\"\n! '''DNg'''  !!   !! ¬¬''x'' = ''x''\n|-\n| Proof:      ||     || ¬''x'' ∨ ''x'' = ''x'' ∨ ¬''x'' = 1 || by '''Cmm<sub>1</sub>''', '''Cpl<sub>1</sub>'''\n|-\n|             || and || ¬''x'' ∧ ''x'' = ''x'' ∧ ¬''x'' = 0 || by '''Cmm<sub>2</sub>''', '''Cpl<sub>2</sub>'''\n|-\n|             || hence || ''x'' = ¬¬''x'' || by '''UNg'''\n|}\n|- valign=\"top\"\n|\n{| align=\"left\" class=\"collapsible collapsed\" style=\"text-align:left\"\n! '''A<sub>1</sub>'''  !!   !! ''x'' ∨ (¬''x'' ∨ ''y'') = 1\n|-\n| Proof:      ||   || ''x'' ∨ (¬''x'' ∨ ''y'')\n|-\n|             || = || (''x'' ∨ (¬''x'' ∨ ''y'')) ∧ 1 || by '''Idn<sub>2</sub>'''\n|-\n|             || = || 1 ∧ (''x'' ∨ (¬''x'' ∨ ''y'')) || by '''Cmm<sub>2</sub>'''\n|-\n|             || = || (''x'' ∨ ¬''x'') ∧ (''x'' ∨ (¬''x'' ∨ ''y'')) || by '''Cpl<sub>1</sub>'''\n|-\n|             || = || ''x'' ∨ (¬''x'' ∧ (¬''x'' ∨ ''y'')) || by '''Dst<sub>1</sub>'''\n|-\n|             || = || ''x'' ∨ ¬''x'' || by '''Abs<sub>2</sub>'''\n|-\n|             || = || 1 || by '''Cpl<sub>1</sub>'''\n|}\n| '''A<sub>2</sub>''' &nbsp; [dual] &nbsp; ''x'' ∧ (¬''x'' ∧ ''y'') = 0\n|- valign=\"top\"\n|\n{| align=\"left\" class=\"collapsible collapsed\" style=\"text-align:left\"\n! '''B<sub>1</sub>'''  !!   !! (''x'' ∨ ''y'') ∨ (¬''x'' ∧ ¬''y'') = 1\n|-\n| Proof:      ||   || (''x'' ∨ ''y'') ∨ (¬''x'' ∧ ¬''y'')\n|-\n|             || = || ((''x'' ∨ ''y'') ∨ ¬''x'') ∧ ((''x'' ∨ ''y'') ∨  ¬''y'') || by '''Dst<sub>1</sub>'''\n|-\n|             || = || (¬''x'' ∨ (''x'' ∨ ''y'')) ∧ (¬''y'' ∨ (''y'' ∨ ''x'')) || by '''Cmm<sub>1</sub>'''\n|-\n|             || = || (¬''x'' ∨ (¬¬''x'' ∨ ''y'')) ∧ (¬''y'' ∨ (¬¬''y'' ∨ ''x'')) || by '''DNg'''\n|-\n|             || = || 1 ∧ 1 || by '''A<sub>1</sub>'''\n|-\n|             || = || 1 || by '''Idn<sub>2</sub>'''\n|}\n| '''B<sub>2</sub>''' &nbsp; [dual] &nbsp; (''x'' ∧ ''y'') ∧ (¬''x'' ∨ ¬''y'') = 0\n|- valign=\"top\"\n|\n{| align=\"left\" class=\"collapsible collapsed\" style=\"text-align:left\"\n! '''C<sub>1</sub>'''  !!   !! (''x'' ∨ ''y'') ∧ (¬''x'' ∧ ¬''y'') = 0\n|-\n| Proof:      ||   || (''x'' ∨ ''y'') ∧ (¬''x'' ∧ ¬''y'')\n|-\n|             || = || (¬''x'' ∧ ¬''y'') ∧ (''x'' ∨ ''y'') || by '''Cmm<sub>2</sub>'''\n|-\n|             || = || ((¬''x'' ∧ ¬''y'') ∧ ''x'') ∨ ((¬''x'' ∧ ¬''y'') ∧ ''y'') || by '''Dst<sub>2</sub>'''\n|-\n|             || = || (''x'' ∧ (¬''x'' ∧ ¬''y'')) ∨ (''y'' ∧ (¬''y'' ∧ ¬''x'')) || by '''Cmm<sub>2</sub>'''\n|-\n|             || = || 0 ∨ 0 || by '''A<sub>2</sub>'''\n|-\n|             || = || 0 || by '''Idn<sub>1</sub>'''\n|}\n| '''C<sub>2</sub>''' &nbsp; [dual] &nbsp; (''x'' ∧ ''y'') ∨ (¬''x'' ∨ ¬''y'') = 1\n|- valign=\"top\"\n|\n{| align=\"left\" class=\"collapsible collapsed\" style=\"text-align:left\"\n! '''DMg<sub>1</sub>'''  !!   !! ¬(''x'' ∨ ''y'') = ¬''x'' ∧ ¬''y''\n|-\n| Proof:      ||   || by '''B<sub>1</sub>''', '''C<sub>1</sub>''', and '''UNg'''\n|}\n| '''DMg<sub>2</sub>''' &nbsp; [dual] &nbsp; ¬(''x'' ∧ ''y'') = ¬''x'' ∨ ¬''y''\n|- valign=\"top\"\n|\n{| align=\"left\" class=\"collapsible collapsed\" style=\"text-align:left\"\n! '''D<sub>1</sub>'''  !!   !! (''x''∨(''y''∨''z'')) ∨ ¬''x'' = 1\n|-\n| Proof:      ||   || (''x'' ∨ (''y'' ∨ ''z'')) ∨ ¬''x''\n|-\n|             || = || ¬''x'' ∨ (''x'' ∨ (''y'' ∨ ''z'')) || by '''Cmm<sub>1</sub>'''\n|-\n|             || = || ¬''x'' ∨ (¬¬''x'' ∨ (''y'' ∨ ''z'')) || by '''DNg'''\n|-\n|             || = || 1 || by '''A<sub>1</sub>'''\n|}\n| '''D<sub>2</sub>''' &nbsp; [dual] &nbsp; (''x''∧(''y''∧''z'')) ∧ ¬''x'' = 0\n|- valign=\"top\"\n|\n{| align=\"left\" class=\"collapsible collapsed\" style=\"text-align:left\"\n! '''E<sub>1</sub>'''  !!   !! ''y'' ∧ (''x''∨(''y''∨''z'')) = ''y''\n|-\n| Proof:      ||   || ''y'' ∧ (''x'' ∨ (''y'' ∨ ''z''))\n|-\n|             || = || (''y'' ∧ ''x'') ∨ (''y'' ∧ (''y'' ∨ ''z'')) || by '''Dst<sub>2</sub>'''\n|-\n|             || = || (''y'' ∧ ''x'') ∨ ''y'' || by '''Abs<sub>2</sub>'''\n|-\n|             || = || ''y'' ∨ (''y'' ∧ ''x'') || by '''Cmm<sub>1</sub>'''\n|-\n|             || = || ''y'' || by '''Abs<sub>1</sub>'''\n|}\n| '''E<sub>2</sub>''' &nbsp; [dual] &nbsp; ''y'' ∨ (''x''∧(''y''∧''z'')) = ''y''\n|- valign=\"top\"\n|\n{| align=\"left\" class=\"collapsible collapsed\" style=\"text-align:left\"\n! '''F<sub>1</sub>'''  !!   !! (''x''∨(''y''∨''z'')) ∨ ¬''y'' = 1\n|-\n| Proof:      ||   || (''x'' ∨ (''y'' ∨ ''z'')) ∨ ¬''y''\n|-\n|             || = || ¬''y'' ∨ (''x'' ∨ (''y'' ∨ ''z'')) || by '''Cmm<sub>1</sub>'''\n|-\n|             || = || (¬''y'' ∨ (''x'' ∨ (''y'' ∨ ''z''))) ∧ 1 || by '''Idn<sub>2</sub>'''\n|-\n|             || = || 1 ∧ (¬''y'' ∨ (''x'' ∨ (''y'' ∨ ''z''))) || by '''Cmm<sub>2</sub>'''\n|-\n|             || = || (''y'' ∨ ¬''y'') ∧ (¬''y'' ∨ (''x'' ∨ (''y'' ∨ ''z''))) || by '''Cpl<sub>1</sub>'''\n|-\n|             || = || (¬''y'' ∨ ''y'') ∧ (¬''y'' ∨ (''x'' ∨ (''y'' ∨ ''z''))) || by '''Cmm<sub>1</sub>'''\n|-\n|             || = || ¬''y'' ∨ (''y'' ∧ (''x'' ∨ (''y'' ∨ ''z''))) || by '''Dst<sub>1</sub>'''\n|-\n|             || = || ¬''y'' ∨ ''y'' || by '''E<sub>1</sub>'''\n|-\n|             || = || ''y'' ∨ ¬''y'' || by '''Cmm<sub>1</sub>'''\n|-\n|             || = || 1 || by '''Cpl<sub>1</sub>'''\n|}\n| '''F<sub>2</sub>''' &nbsp; [dual] &nbsp; (''x''∧(''y''∧''z'')) ∧ ¬''y'' = 0\n|- valign=\"top\"\n|\n{| align=\"left\" class=\"collapsible collapsed\" style=\"text-align:left\"\n! '''G<sub>1</sub>'''  !!   !! (''x''∨(''y''∨''z'')) ∨ ¬''z'' = 1\n|-\n| Proof:      ||   || (''x'' ∨ (''y'' ∨ ''z'')) ∨ ¬''z''\n|-\n|             || = || (''x'' ∨ (''z'' ∨ ''y'')) ∨ ¬''z'' || by '''Cmm<sub>1</sub>'''\n|-\n|             || = || 1 || by '''F<sub>1</sub>'''\n|}\n| '''G<sub>2</sub>''' &nbsp; [dual]  &nbsp;  (''x''∧(''y''∧''z'')) ∧ ¬''z'' = 0\n|- valign=\"top\"\n|\n{| align=\"left\" class=\"collapsible collapsed\" style=\"text-align:left\"\n! '''H<sub>1</sub>'''  !!   !! ¬((''x''∨''y'')∨''z'') ∧ ''x'' = 0\n|-\n| Proof:      ||   || ¬((''x'' ∨ ''y'') ∨ ''z'') ∧ ''x''\n|-\n|             || = || (¬(''x'' ∨ ''y'') ∧ ¬''z'') ∧ ''x'' || by '''DMg<sub>1</sub>'''\n|-\n|             || = || ((¬''x'' ∧ ¬''y'') ∧ ¬''z'') ∧ ''x'' || by '''DMg<sub>1</sub>'''\n|-\n|             || = || ''x'' ∧ ((¬''x'' ∧ ¬''y'') ∧ ¬''z'') || by '''Cmm<sub>2</sub>'''\n|-\n|             || = || (''x'' ∧ ((¬''x'' ∧ ¬''y'') ∧ ¬''z'')) ∨ 0 || by '''Idn<sub>1</sub>'''\n|-\n|             || = || 0 ∨ (''x'' ∧ ((¬''x'' ∧ ¬''y'') ∧ ¬''z'')) || by '''Cmm<sub>1</sub>'''\n|-\n|             || = || (''x'' ∧ ¬''x'') ∨ (''x'' ∧ ((¬''x'' ∧ ¬''y'') ∧ ¬''z'')) || by '''Cpl<sub>1</sub>'''\n|-\n|             || = || ''x'' ∧ (¬''x'' ∨ ((¬''x'' ∧ ¬''y'') ∧ ¬''z'')) || by '''Dst<sub>2</sub>'''\n|-\n|             || = || ''x'' ∧ (¬''x'' ∨ (¬''z'' ∧ (¬''x'' ∧ ¬''y''))) || by '''Cmm<sub>2</sub>'''\n|-\n|             || = || ''x'' ∧ ¬''x'' || by '''E<sub>2</sub>'''\n|-\n|             || = || 0 || by '''Cpl<sub>2</sub>'''\n|}\n| '''H<sub>2</sub>''' &nbsp; [dual]  &nbsp;  ¬((''x''∧''y'')∧''z'') ∨ ''x'' = 1\n|- valign=\"top\"\n|\n{| align=\"left\" class=\"collapsible collapsed\" style=\"text-align:left\"\n! '''I<sub>1</sub>'''  !!   !! ¬((''x''∨''y'')∨''z'') ∧ ''y'' = 0\n|-\n| Proof:      ||   || ¬((''x'' ∨ ''y'') ∨ ''z'') ∧ ''y''\n|-\n|             || = || ¬((''y'' ∨ ''x'') ∨ ''z'') ∧ ''y'' || by '''Cmm<sub>1</sub>'''\n|-\n|             || = || 0 || by '''H<sub>1</sub>'''\n|}\n| '''I<sub>2</sub>''' &nbsp; [dual]  &nbsp;  ¬((''x''∧''y'')∧''z'') ∨ ''y'' = 1\n|- valign=\"top\"\n|\n{| align=\"left\" class=\"collapsible collapsed\" style=\"text-align:left\"\n! '''J<sub>1</sub>'''  !!   !! ¬((''x''∨''y'')∨''z'') ∧ ''z'' = 0\n|-\n| Proof:      ||   || ¬((''x'' ∨ ''y'') ∨ ''z'') ∧ ''z''\n|-\n|             || = || (¬(''x'' ∨ ''y'') ∧ ¬''z'') ∧ ''z'' || by '''DMg<sub>1</sub>'''\n|-\n|             || = || ''z'' ∧ (¬(''x'' ∨ ''y'') ∧ ¬''z'') || by '''Cmm<sub>2</sub>'''\n|-\n|             || = || ''z'' ∧ ( ¬''z'' ∧ ¬(''x'' ∨ ''y'')) || by '''Cmm<sub>2</sub>'''\n|-\n|             || = || 0 || by '''A<sub>2</sub>'''\n|}\n| '''J<sub>2</sub>''' &nbsp; [dual]  &nbsp;  ¬((''x''∧''y'')∧''z'') ∨ ''z'' = 1\n|- valign=\"top\"\n|\n{| align=\"left\" class=\"collapsible collapsed\" style=\"text-align:left\"\n! '''K<sub>1</sub>'''  !!   !! (''x'' ∨ (''y'' ∨ ''z'')) ∨ ¬((''x'' ∨ ''y'') ∨ ''z'') = 1\n|-\n| Proof:      ||   || (''x''∨(''y''∨''z'')) ∨ ¬((''x'' ∨ ''y'') ∨ ''z'')\n|-\n|             || = || (''x''∨(''y''∨''z'')) ∨ (¬(''x'' ∨ ''y'') ∧ ¬''z'') || by '''DMg<sub>1</sub>'''\n|-\n|             || = || (''x''∨(''y''∨''z'')) ∨ ((¬''x'' ∧ ¬''y'') ∧ ¬''z'') || by '''DMg<sub>1</sub>'''\n|-\n|             || = || ((''x''∨(''y''∨''z'')) ∨ (¬''x'' ∧ ¬''y'')) ∧  ((''x''∨(''y''∨''z'')) ∨ ¬''z'')|| by '''Dst<sub>1</sub>'''\n|-\n|             || = || (((''x''∨(''y''∨''z'')) ∨ ¬''x'') ∧ ((''x''∨(''y''∨''z'')) ∨ ¬''y'')) ∧  ((''x''∨(''y''∨''z'')) ∨ ¬''z'')|| by '''Dst<sub>1</sub>'''\n|-\n|             || = || (1 ∧ 1) ∧ 1 || by '''D<sub>1</sub>''','''F<sub>1</sub>''','''G<sub>1</sub>'''\n|-\n|             || = || 1 || by '''Idn<sub>2</sub>'''\n|}\n| '''K<sub>2</sub>''' &nbsp; [dual]  &nbsp;  (''x'' ∧ (''y'' ∧ ''z'')) ∧ ¬((''x'' ∧ ''y'') ∧ ''z'') = 0\n|- valign=\"top\"\n|\n{| align=\"left\" class=\"collapsible collapsed\" style=\"text-align:left\"\n! '''L<sub>1</sub>'''  !!   !! (''x'' ∨ (''y'' ∨ ''z'')) ∧ ¬((''x'' ∨ ''y'') ∨ ''z'') = 0\n|-\n| Proof:      ||   || (''x'' ∨ (''y'' ∨ ''z'')) ∧ ¬((''x'' ∨ ''y'') ∨ ''z'')\n|-\n|             || = || ¬((''x''∨''y'')∨''z'') ∧ (''x'' ∨ (''y'' ∨ ''z'')) || by '''Cmm<sub>2</sub>'''\n|-\n|             || = || (¬((''x''∨''y'')∨''z'') ∧ ''x'') ∨ (¬((''x''∨''y'')∨''z'') ∧ (''y'' ∨ ''z'')) || by '''Dst<sub>2</sub>'''\n|-\n|             || = || (¬((''x''∨''y'')∨''z'') ∧ ''x'') ∨ ((¬((''x''∨''y'')∨''z'') ∧ ''y'') ∨ (¬((''x''∨''y'')∨''z'') ∧ ''z'')) || by '''Dst<sub>2</sub>'''\n|-\n|             || = || 0 ∨ (0 ∨ 0) || by '''H<sub>1</sub>''','''I<sub>1</sub>''','''J<sub>1</sub>'''\n|-\n|             || = || 0 || by '''Idn<sub>1</sub>'''\n|}\n| '''L<sub>2</sub>''' &nbsp; [dual]  &nbsp; (''x'' ∧ (''y'' ∧ ''z'')) ∨ ¬((''x'' ∧ ''y'') ∧ ''z'') = 1\n|- valign=\"top\"\n|\n{| align=\"left\" class=\"collapsible collapsed\" style=\"text-align:left\"\n! '''Ass<sub>1</sub>'''  !!   !! ''x'' ∨ (''y'' ∨ ''z'') = (''x'' ∨ ''y'') ∨ ''z''\n|-\n| Proof:      ||   || by '''K<sub>1</sub>''', '''L<sub>1</sub>''', '''UNg''', '''DNg'''\n|}\n| '''Ass<sub>2</sub>''' &nbsp; [dual]  &nbsp;  ''x'' ∧ (''y'' ∧ ''z'') = (''x'' ∧ ''y'') ∧ ''z''\n|-\n| colspan=\"2\" |\n{| align=\"left\" class=\"collapsible\" style=\"text-align:left\"\n|-\n! colspan=\"2\" | Abbreviations\n|-\n| '''UId''' ||  Unique Identity\n|-\n| '''Idm''' || [[Idempotence]]\n|-\n| '''Bnd''' || [[Bounded lattice|Boundaries]]\n|-\n| '''Abs''' || [[Absorption law]]\n|-\n| '''UNg''' ||  Unique Negation\n|-\n| '''DNg''' || [[Double negation]]\n|-\n| '''DMg''' || [[De Morgan's Law]]\n|-\n| '''Ass''' || [[Associativity]]\n|}\n|}\n{| align=\"right\" class=\"wikitable collapsible collapsed\" style=\"text-align:left\"\n! colspan=\"4\"| '''Huntington 1904 Boolean algebra axioms'''\n|- valign=\"top\"\n| '''Idn<sub>1</sub>'''  || ''x'' ∨ 0 = ''x''\n| '''Idn<sub>2</sub>'''  || ''x'' ∧ 1 = ''x''\n|- valign=\"top\"\n| '''Cmm<sub>1</sub>''' || ''x'' ∨ ''y'' = ''y'' ∨ ''x''\n| '''Cmm<sub>2</sub>''' || ''x'' ∧ ''y'' = ''y'' ∧ ''x''\n|- valign=\"top\"\n| '''Dst<sub>1</sub>'''  || ''x'' ∨ (''y''∧''z'') = (''x''∨''y'') ∧ (''x''∨''z'')\n| '''Dst<sub>2</sub>'''  || ''x'' ∧ (''y''∨''z'') = (''x''∧''y'') ∨ (''x''∧''z'')\n|- valign=\"top\"\n| '''Cpl<sub>1</sub>'''   || ''x'' ∨ ¬''x'' = 1\n| '''Cpl<sub>2</sub>'''   || ''x'' ∧ ¬''x'' = 0\n|-\n| colspan=\"4\" |\n{| align=\"left\" class=\"collapsible\" style=\"text-align:left\"\n|-\n! colspan=\"2\" | Abbreviations\n|-\n| '''Idn''' || [[Identity element|Identity]]\n|-\n| '''Cmm''' || [[Commutativity]]\n|-\n| '''Dst''' || [[Distributivity]]\n|-\n| '''Cpl''' || [[Complemented lattice|Complements]]\n|}\n|}\n\nThe first axiomatization of Boolean lattices/algebras in general was given by the English philosopher and mathematician [[Alfred North Whitehead]] in 1898.<ref name=\"PadmanabhanRudeanu2008\">Padmanabhan, [https://books.google.com/books?id=JlXSlpmlSv4C&pg=PA73#v=onepage&q&f=false p.&nbsp;73]</ref><ref>Whitehead, 1898, p.37</ref>\nIt included the [[#Definition|above axioms]] and additionally ''x''∨1=1 and ''x''∧0=0.\nIn 1904, the American mathematician [[Edward V. Huntington]] (1874–1952) gave probably the most parsimonious axiomatization based on ∧, ∨, ¬, even proving the associativity laws (see box).<ref>Huntington, 1904, p.292-293, (first of several axiomatizations by Huntington)</ref>\nHe also proved that these axioms are [[Independence (mathematical logic)|independent]] of each other.<ref>Huntington, 1904, p.296</ref>\nIn 1933, Huntington set out the following elegant axiomatization for Boolean algebra.  It requires just one binary operation + and a [[unary functional symbol]] ''n'', to be read as 'complement', which satisfy the following laws:\n\n# ''Commutativity'': ''x'' + ''y'' = ''y'' + ''x''.\n# ''Associativity'': (''x'' + ''y'') + ''z'' = ''x'' + (''y'' + ''z'').\n# ''Huntington equation'': ''n''(''n''(''x'') + ''y'') + ''n''(''n''(''x'') + ''n''(''y'')) = ''x''.\n\n[[Herbert Robbins]] immediately asked: If the Huntington equation is replaced with its dual, to wit:\n\n:4. ''Robbins Equation'': ''n''(''n''(''x'' + ''y'') + ''n''(''x'' + ''n''(''y''))) = ''x'',\n\ndo (1), (2), and (4) form a basis for Boolean algebra? Calling (1), (2), and (4) a ''Robbins algebra'', the question then becomes: Is every Robbins algebra a Boolean algebra? This question (which came to be known as the [[Robbins conjecture]]) remained open for decades, and became a favorite question of [[Alfred Tarski]] and his students. In 1996, [[William McCune]] at [[Argonne National Laboratory]], building on earlier work by Larry Wos, Steve Winker, and Bob Veroff, answered Robbins's question in the affirmative: Every Robbins algebra is a Boolean algebra. Crucial to McCune's proof was the [[automated reasoning program]] [[EQP]] he designed. For a simplification of McCune's proof, see Dahn (1998).\n\nFurther work has been done for reducing the number of axioms; see [[Minimal axioms for Boolean algebra]].\n\n{{clear}}\n\n== Generalizations ==\n\n{| style=\"float:right\"\n| {{Algebraic structures|Lattice}}\n|}\nRemoving the requirement of existence of a unit from the axioms of Boolean algebra yields \"generalized Boolean algebras\". Formally, a [[distributive lattice]] ''B'' is a generalized Boolean lattice, if it has a smallest element 0 and for any elements ''a'' and ''b'' in ''B'' such that ''a'' ≤ ''b'', there exists an element ''x'' such that a ∧ x = 0 and a ∨ x = b. Defining a ∖ b as the unique ''x'' such that (a ∧ b) ∨ x = a and (a ∧ b) ∧ x = 0, we say that the structure (B,∧,∨,∖,0) is a ''generalized Boolean algebra'', while (B,∨,0) is a ''generalized Boolean [[semilattice]]''. Generalized Boolean lattices are exactly the [[Ideal (order theory)|ideals]] of Boolean lattices.\n\nA structure that satisfies all axioms for Boolean algebras except the two distributivity axioms is called an [[orthocomplemented lattice]]. Orthocomplemented lattices arise naturally in [[quantum logic]] as lattices of closed subspaces for separable [[Hilbert space]]s.\n\n== See also ==\n{{div col|content=\n* [[List of Boolean algebra topics]]\n* [[Boolean domain]]\n* [[Boolean function]]\n* [[Boolean logic]]\n* [[Boolean ring]]\n* [[Boolean-valued function]]\n* [[Canonical form (Boolean algebra)]]\n* [[Complete Boolean algebra]]\n* [[De Morgan's laws]]\n* [[Finitary boolean function]]\n* [[Forcing (mathematics)]]\n* [[Free Boolean algebra]]\n* [[Heyting algebra]]\n* [[Hypercube graph]]\n* [[Karnaugh map]]\n* [[Laws of Form]]\n* [[Logic gate]]\n* [[Logical graph]]\n* [[Logical matrix]]\n* [[Propositional logic]]\n* [[Quine–McCluskey algorithm]]\n* [[Two-element Boolean algebra]]\n* [[Venn diagram]]\n* [[Conditional event algebra]]\n}}\n\n== Notes ==\n{{reflist}}\n\n== References ==\n*{{citation\n | last1 = Brown\n | first1 = Stephen\n | last2 = Vranesic\n | first2 = Zvonko\n | year = 2002\n | title = Fundamentals of Digital Logic with VHDL Design\n | edition = 2nd\n | publisher = [[McGraw-Hill|McGraw–Hill]]\n | isbn = 978-0-07-249938-4}}. See Section 2.5.\n*{{cite journal\n |author1=A. Boudet |author2=J.P. Jouannaud |author3=M. Schmidt-Schauß | title=Unification in Boolean Rings and Abelian Groups\n | journal=Journal of Symbolic Computation\n | year=1989\n | volume=8\n |issue=5 | pages=449–477\n | url=http://www.sciencedirect.com/science/article/pii/S0747717189800549/pdf?md5=713ed362e4b6f2db53923cc5ed47c818&pid=1-s2.0-S0747717189800549-main.pdf\n | doi=10.1016/s0747-7171(89)80054-9}}\n*{{citation|title=Basic Algebra: Groups, Rings, and Fields|first=Paul M.|last=Cohn|publisher=\tSpringer|year=2003|isbn=9781852335878|pages=51, 70–81}}\n*{{citation\n | last1 = Cori\n | first1 = Rene\n | last2 = Lascar\n | first2 = Daniel\n | year = 2000\n | title = Mathematical Logic: A Course with Exercises\n | publisher = [[Oxford University Press]]\n | isbn = 978-0-19-850048-3}}. See Chapter 2.\n*{{citation\n | last = Dahn\n | first = B. I.\n | year = 1998\n | title = Robbins Algebras are Boolean: A Revision of McCune's Computer-Generated Solution of the Robbins Problem\n | journal = Journal of Algebra\n | volume = 208\n | pages = 526–532\n | doi = 10.1006/jabr.1998.7467\n | issue = 2}}.\n*{{cite book\n |author1=B.A. Davey |author2=H.A. Priestley | title=Introduction to Lattices and Order\n | year=1990\n | publisher=Cambridge University Press\n | series=Cambridge Mathematical Textbooks}}\n*{{citation\n | last1 = Givant\n | first1 = Steven\n | first2 = Paul\n | last2 = Halmos\n | year = 2009\n | title = Introduction to Boolean Algebras\n | series = [[Undergraduate Texts in Mathematics]] | publisher = [[Springer Science+Business Media|Springer]]\n | isbn = 978-0-387-40293-2}}.\n*{{citation\n | authorlink = Paul Halmos\n | last = Halmos\n | first = Paul\n | year = 1963\n | title = Lectures on Boolean Algebras\n | publisher = Van Nostrand\n | isbn = 978-0-387-90094-0}}.\n*{{citation\n | authorlink1 = Paul Halmos\n | last1 = Halmos\n | first1 = Paul\n | first2 = Steven\n | last2 = Givant\n | year = 1998\n | title = Logic as Algebra\n | series = Dolciani Mathematical Expositions | volume = 21 | publisher = [[Mathematical Association of America]]\n | isbn = 978-0-88385-327-6}}.\n*{{cite journal\n | author=Hsiang, Jieh\n | title=Refutational Theorem Proving Using Term Rewriting Systems\n | journal=AI\n | year=1985\n | volume=25\n | issue=3\n | pages=255–300\n | url=https://www.researchgate.net/publication/223327412\n | doi=10.1016/0004-3702(85)90074-8| arxiv=cond-mat/0606434\n }}\n*{{cite journal\n | authorlink=Edward V. Huntington\n | author=Edward V. Huntington\n | title=Sets of Independent Postulates for the Algebra of Logic\n | journal=[[Transactions of the American Mathematical Society]]\n | year=1904\n | volume=5\n | issue=3\n | pages=288–309\n | jstor=1986459\n | doi=10.1090/s0002-9947-1904-1500675-4}}\n*{{citation\n | authorlink = Edward Vermilye Huntington\n | last = Huntington\n | first = E. V.\n | year = 1933\n | title = New sets of independent postulates for the algebra of logic\n | journal = [[Transactions of the American Mathematical Society]]\n | volume = 35\n | pages = 274–304\n | doi = 10.2307/1989325\n | pmid = 16577445\n | issue = 1\n | publisher = American Mathematical Society\n | url = http://www.ams.org/journals/tran/1933-035-01/S0002-9947-1933-1501684-X/S0002-9947-1933-1501684-X.pdf\n | jstor = 1989325| pmc = 1076183\n }}.\n*{{citation\n | authorlink = Edward Vermilye Huntington\n | last = Huntington\n | first = E. V.\n | year = 1933\n | title = Boolean algebra: A correction\n | journal = [[Transactions of the American Mathematical Society]]\n | volume = 35\n | pages = 557–558\n | doi = 10.2307/1989783\n | issue = 2\n | jstor = 1989783}}.\n*{{citation\n | last = Mendelson\n | first = Elliott\n | year = 1970\n | title = Boolean Algebra and Switching Circuits\n | series = Schaum's Outline Series in Mathematics | publisher = [[McGraw-Hill|McGraw–Hill]]\n | isbn = 978-0-07-041460-0}}.\n*{{citation\n | editor1-last = Monk\n | editor1-first = J. Donald\n | editor2-first = R.\n | editor2-last = Bonnet\n | year = 1989\n | title = Handbook of Boolean Algebras\n | publisher = [[Elsevier|North-Holland]]\n | isbn = 978-0-444-87291-3}}. In 3 volumes. (Vol.1:{{ISBN|978-0-444-70261-6}}, Vol.2:{{ISBN|978-0-444-87152-7}}, Vol.3:{{ISBN|978-0-444-87153-4}})\n*{{citation\n | last1 = Padmanabhan\n | first1 = Ranganathan\n | last2 = Rudeanu\n | first2 = Sergiu\n | year = 2008\n | title = Axioms for lattices and boolean algebras\n | publisher = World Scientific\n | isbn = 978-981-283-454-6}}.\n*{{citation\n | authorlink = Roman Sikorski\n | last = Sikorski\n | first = Roman\n | year = 1966\n | title = Boolean Algebras\n | series = Ergebnisse der Mathematik und ihrer Grenzgebiete\n | publisher = [[Springer Verlag]]\n | ref = Sikorski1966BooleanAlgebras\n }}.\n*{{citation\n | last = Stoll\n | first = R. R.\n | year = 1963\n | title = Set Theory and Logic\n | publisher = W. H. Freeman\n | isbn = 978-0-486-63829-4}}. Reprinted by [[Dover Publications]], 1979.\n*{{cite journal\n | authorlink=Marshall H. Stone\n | author=Marshall H. Stone\n | title=The Theory of Representations for Boolean Algebra\n | journal=[[Transactions of the American Mathematical Society]]\n | year=1936\n | volume=40\n | pages=37–111\n | doi=10.1090/s0002-9947-1936-1501865-8}}\n*{{cite book\n | authorlink=A.N. Whitehead\n | author=A.N. Whitehead\n | title=A Treatise on Universal Algebra\n | year=1898\n | publisher=Cambridge University Press\n | isbn=978-1-4297-0032-0\n | url=http://projecteuclid.org/euclid.chmm/1263316509}}\n\n== External links ==\n* {{springer|title=Boolean algebra|id=p/b016920}}\n* [[Stanford Encyclopedia of Philosophy]]: \"[http://plato.stanford.edu/entries/boolalg-math/ The Mathematics of Boolean Algebra,]\" by J. Donald Monk.\n* McCune W., 1997. ''[http://www.cs.unm.edu/~mccune/papers/robbins/ Robbins Algebras Are Boolean]'' JAR 19(3), 263—276\n* [http://demonstrations.wolfram.com/BooleanAlgebra/ \"Boolean Algebra\"] by [[Eric W. Weisstein]], [[Wolfram Demonstrations Project]], 2007.\n* Burris, Stanley N.; Sankappanavar, H. P., 1981. ''[http://www.thoralf.uwaterloo.ca/htdocs/ualg.html A Course in Universal Algebra.]''  Springer-Verlag. {{ISBN|3-540-90578-2}}.\n* {{MathWorld | urlname=BooleanAlgebra | title=Boolean Algebra}}\n\n{{DEFAULTSORT:Boolean Algebra (Structure)}}\n[[Category:Boolean algebra| ]]\n[[Category:Algebraic structures]]\n[[Category:Ockham algebras]]"
    },
    {
      "title": "Cancellative semigroup",
      "url": "https://en.wikipedia.org/wiki/Cancellative_semigroup",
      "text": "In [[mathematics]], a '''cancellative semigroup''' (also called a '''cancellation semigroup''') is a [[semigroup]] having the [[cancellation property]].<ref>{{harv|Clifford|Preston|1967|p=3}}</ref> In intuitive terms, the cancellation property asserts that from an [[equality (mathematics)|equality]] of the form ''a'' · ''b'' = ''a'' · ''c'', where · is a [[binary operation]], one can cancel the element ''a'' and deduce the equality ''b'' = ''c''.  In this case the element being canceled out is appearing as the left factors of {{nowrap|''a'' · ''b''}} and {{nowrap|''a'' · ''c''}} and hence it is a case of '''left cancellation property'''.  The '''right cancellation property''' can be defined analogously.  [[Prototype|Prototypical]] examples of cancellative semigroups are the [[group (mathematics)|group]]s and the [[semigroup]] of [[positive integer]]s under [[addition]] or [[multiplication]].  Cancellative semigroups are considered to be very close to being groups because cancellability is one of the necessary conditions for a semigroup to be embeddable in a group. Moreover, every finite cancellative semigroup is a group. One of the main problems associated with the study of cancellative semigroups is to determine the necessary and sufficient conditions for embedding a cancellative semigroup in a group.\n\nThe origins of the study of cancellative semigroups can be traced to the first substantial paper on semigroups, {{harv|Suschkewitsch|1928}}.<ref>{{cite web |url=http://www.gap-system.org/~history/Extras/Preston_semigroups.html |title=Personal reminiscences of the early history of semigroups |author=[[Gordon Preston|G. B. Preston]] |year=1990 |accessdate=2009-05-12 |archive-url=https://web.archive.org/web/20090109045100/http://www.gap-system.org/~history/Extras/Preston_semigroups.html# |archive-date=2009-01-09 |dead-url=yes |df= }}</ref>\n\n==Formal definitions==\n\nLet ''S'' be a semigroup.  An element ''a'' in ''S'' is '''left cancellative''' (or, is ''left cancellable'', or,  has the ''left cancellation property'') if {{nowrap|1=''ab'' = ''ac''}} implies {{nowrap|1=''b'' = ''c''}} for all ''b'' and ''c'' in ''S''.  If every element in ''S'' is left cancellative, then ''S'' is called a '''left cancellative semigroup'''.\n\nLet ''S'' be a semigroup.  An element ''a'' in ''S'' is '''right cancellative''' (or, is ''right cancellable'', or,  has the ''right cancellation property'') if {{nowrap|1=''ba'' = ''ca''}} implies {{nowrap|1=''b'' = ''c''}} for all ''b'' and ''c'' in ''S''.  If every element in ''S'' is right cancellative, then ''S'' is called a '''right cancellative semigroup'''.\n\nLet ''S'' be a semigroup.  If every element in ''S'' is both left cancellative and right cancellative, then ''S'' is called a '''cancellative semigroup'''.<ref>{{planetmath reference|id= 5926|title=Cancellative semigroup}}</ref>\n\n==Alternative definitions==\n\nIt is possible to restate the characteristic property of a cancellative element in terms of a property held by the corresponding left multiplication {{nowrap|''L''<sub>''a''</sub> : ''S'' → ''S''}} and right multiplication {{nowrap|''R''<sub>''a''</sub> : ''S'' → ''S''}} maps defined by {{nowrap|1=''L''<sub>''a''</sub>(''b'') = ''ab''}}, {{nowrap|1=''R''<sub>''a''</sub>(''b'') = ''ba''}}. An element ''a'' in ''S'' is '''left cancellative''' if and only if ''L''<sub>''a''</sub> is [[injective]]. An element ''a'' is '''right cancellative''' if and only if ''R''<sub>''a''</sub> is injective.\n\n==Examples==\n\n#Every [[group (mathematics)|group]] is a cancellative semigroup.\n#The set of [[natural number|positive integer]]s under addition is a cancellative semigroup.\n#The set of nonnegative integers under addition is a cancellative [[monoid]].\n#The set of positive integers under multiplication is a cancellative monoid.\n#A [[left zero semigroup]] is right cancellative but not left cancellative, unless it is trivial.\n#A [[right zero semigroup]] is left cancellative but not right cancellative, unless it is trivial.\n#A [[null semigroup]] with more than one element is neither left cancellative nor right cancellative. In such a semigroup there is no element which is either left cancellative or right cancellative.\n#Let ''S'' be the semigroup of real square [[matrix (mathematics)|matrices]] of order ''n'' under [[matrix multiplication]]. Let ''a'' be any element in ''S''. If ''a'' is [[nonsingular]] then ''a'' is both left cancellative and right cancellative. If ''a'' is singular then ''a'' is neither left cancellative nor right cancellative.\n\n==Finite cancellative semigroups==\n\nIt is an elementary result in [[group theory]] that a finite cancellative semigroup is a group. Let ''S'' be a finite cancellative semigroup. Cancellativity and finiteness taken together imply that {{nowrap|1=''Sa'' = ''aS'' = ''S''}} for all ''a'' in ''S''. So given an element ''a'' in ''S'', there is an element ''e''<sub>''a''</sub>, depending on ''a'',   in ''S'' such that {{nowrap|1=''ae''<sub>''a''</sub> = ''a''}}. Cancellativity now further implies that this ''e''<sub>''a''</sub> is independent of ''a'' and that {{nowrap|1=''xe''<sub>''a''</sub> = ''e''<sub>''a''</sub>''x'' = ''x''}} for all ''x'' in ''S''.  Thus ''e''<sub>''a''</sub> is the identity element of ''S'' which may from now on be denoted by ''e''.   Using the  property {{nowrap|1=''Sa'' = ''S''}}   one now sees that there is ''b'' in ''S'' such that {{nowrap|1=''ba'' = ''e''}}.  Cancellativity can be invoked to show that ''ab'' = ''e'' also, thereby establishing that every element ''a'' in ''S'' has an inverse in ''S''. Thus ''S'' must necessarily be a group.\n\nFurthermore, every cancellative [[epigroup]] is also a group.<ref>{{cite book|author=Peter M. Higgins|title=Techniques of semigroup theory|year=1992|publisher=Oxford University Press|isbn=978-0-19-853577-5|page=12}}</ref>\n\n==Embeddability in groups==\n\nA [[commutative]] semigroup can be embedded in a group (i.e., is isomorphic to a subset of a group) if and only if it is cancellative. The procedure for doing this is similar to that of embedding an integral domain in a field, {{harv|Clifford|Preston|1961|p=34}}. See also [[Grothendieck group]], the universal mapping from a commutative semigroup to [[abelian group]]s that is an embedding if the semigroup is cancellative.\n\nFor the embeddability of noncommutative semigroups in groups, cancellativity is obviously a necessary condition. However, it is not sufficient: there are (noncommutative and infinite) cancellative semigroups that cannot be embedded in a group.<ref>A. Malcev, ''On the Immersion of an Algebraic Ring into a Field'', \nMathematische Annalen\n1937, Volume 113, Issue 1, pp 686-691</ref>\nTo obtain a sufficient (but not necessary) condition, it may be observed that the proof of the result that a finite cancellative semigroup ''S'' is a group critically depended on the fact that ''Sa'' = ''S'' for all ''a'' in ''S''. The paper {{harv|Dubreil|1941}} generalized this idea and introduced the concept of a ''right reversible'' semigroup. A semigroup ''S'' is said to be ''right reversible'' if any two principal ideals of ''S'' intersect, that is, ''Sa'' ∩ ''Sb'' ≠  Ø  for all ''a'' and ''b'' in ''S''. The sufficient condition for the embeddability of semigroups in groups can now be stated as follows: ([[Ore condition|Ore's Theorem]]) Any right reversible cancellative semigroup can be embedded in a group, {{harv|Clifford|Preston|1961|p=35}}.\n\nThe first set of necessary and sufficient conditions for the embeddability of a semigroup in a group were given in {{harv|Malcev|1939}}.<ref>{{Citation |last=Paul M Cohn |title=Universal Algebra |publisher=[[Springer Science+Business Media|Springer]] |year=1981 |pages=268–269 |isbn=90-277-1254-9}}</ref> Though theoretically important, the conditions are countably infinite in number and no finite subset will suffice, as shown in {{harv|Malcev|1940}}.<ref>{{Citation |last=John Rhodes |date=April 1970 |title=Book Review of 'The Algebraic Theory of Semigroups Vol I & II' by A H Clifford & G B Preston |journal=Bulletin of the AMS |publisher=[[American Mathematical Society]] |postscript=.}} [http://www.ams.org/bull/1970-76-04/S0002-9904-1970-12504-6/S0002-9904-1970-12504-6.pdf] (Accessed on 11 May 2009)</ref> A different (but also countably infinite) set of necessary and sufficient conditions were given in {{harv|Lambek|1951}}, where it was shown that a semigroup can be embedded in a group if and only if it is cancellative and satisfies a so-called \"polyhedral condition\". The two embedding theorems by Malcev and Lambek were later compared in {{harv|Bush|1963}}.\n\n==See also==\n\n*[[Cancellation property]]\n*[[Special classes of semigroups]]\n\n==Notes==\n{{reflist}}\n\n==References==\n* {{Citation | last1=Bush | first1=George C. | title=The embedding theorems of Malcev and Lambek | doi=10.4153/CJM-1963-006-x | year=1963 | journal=Canadian Journal of Mathematics | volume=15 | pages=49–58 | url=https://cms.math.ca/10.4153/CJM-1963-006-x}}\n* {{Citation | last1=Clifford | first1=Alfred Hoblitzelle | author1-link=Alfred H. Clifford | last2=Preston | first2=Gordon Bamford | author2-link=Gordon Preston | title=The algebraic theory of semigroups. Vol. I | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=Mathematical Surveys, No. 7 | isbn=978-0-8218-0272-4 | mr=0132791 | year=1961}}\n* {{Citation | last1=Clifford | first1=Alfred Hoblitzelle | author1-link=Alfred H. Clifford | last2=Preston | first2=Gordon Bamford | author2-link=Gordon Preston | title=The algebraic theory of semigroups. Vol. II | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=Mathematical Surveys, No. 7 | mr=0218472 | year=1967}}\n* {{Citation | last1=Dubreil | first1=Paul | title=Contribution à la théorie des demi-groupes | mr=0016424 | year=1941 | journal=Mém. Acad. Sci. Inst. France (2) | volume=63 | issue=3 | pages=52}}\n* {{Citation | last1=Lambek | first1=J. | author1-link=Joachim Lambek | title=The immersibility of a semigroup into a group | doi=10.4153/CJM-1951-005-8 | year=1951 | journal=Canadian Journal of Mathematics | volume=3 | pages=34–43 | url=https://cms.math.ca/10.4153/CJM-1951-005-8}}\n* {{Citation | last1=Malcev | first1=A. I. | author1-link=Anatoly Maltsev | title=Über die Einbettung von assoziativen Systemen in Gruppen | mr=0002152 | year=1939 | journal=Rec. Math. (Mat. Sbornik) N.S. | volume=6 | pages=331–336}}\n* {{Citation | last1=Malcev | first1=A. I. | author1-link=Anatoly Maltsev | title=Über die Einbettung von assoziativen Systemen in Gruppen. II | mr=0002895 | year=1940 | journal=Rec. Math. (Mat. Sbornik) N.S. | volume=8 | pages=251–264}}\n* {{Citation | last1=Preston | first1=Gordon Bamford | author1-link=Gordon Preston | title=Monash Conference on Semigroup Theory (Melbourne, 1990) | url=http://www.gap-system.org/~history/Extras/Preston_semigroups.html | publisher=World Sci. Publ., River Edge, NJ | mr=1232669 | year=1991 | chapter=Personal reminiscences of the early history of semigroups | pages=16–30 | access-date=2009-05-12 | archive-url=https://web.archive.org/web/20090109045100/http://www.gap-system.org/~history/Extras/Preston_semigroups.html# | archive-date=2009-01-09 | dead-url=yes | df= }}\n* {{Citation | last1=Suschkewitsch | first1=Anton | title=Über die endlichen Gruppen ohne das Gesetz der eindeutigen Umkehrbarkeit | doi=10.1007/BF01459084 | mr=1512437 | year=1928 | journal=[[Mathematische Annalen]] | issn=0025-5831 | volume=99 | issue=1 | pages=30–50}}\n\n[[Category:Algebraic structures]]\n[[Category:Semigroup theory]]"
    },
    {
      "title": "Class of groups",
      "url": "https://en.wikipedia.org/wiki/Class_of_groups",
      "text": "{{Group theory sidebar |Basics}}\nA '''class of groups''' is a set theoretical collection of [[Group_(mathematics)|groups]] satisfying the property that if ''G'' is in the collection then every group isomorphic to ''G'' is also in the collection. This concept arose from the necessity to work with a bunch of groups satisfying certain special property (for example finiteness or commutativity).  Since [[set theory]] does not admit the \"set of all groups\", it is necessary to work with the more general concept of ''[[Class_(set_theory)|class]]''.\n\n== Definition ==\n\nA '''class of groups''' <math>\\mathfrak{X}~</math> is a collection of groups such that if <math>G\\in\\mathfrak{X}~</math> and <math>G\\cong H~</math> then <math>H\\in\\mathfrak{X}~</math>. Groups in the class <math>\\mathfrak{X}~</math> are referred to as <math>\\mathfrak{X}</math>-'''groups'''.\n\nFor a set of groups <math>\\mathfrak{I}~</math>, we denote by <math>(\\mathfrak{I})</math> the smallest class of groups containing <math>\\mathfrak{I}</math>. In particular for a group <math>G</math>, <math>(G)</math> denotes its isomorphism class.\n\n== Examples ==\n\nThe most common examples of classes of groups are:\n* <math>\\emptyset</math>: the empty class of groups\n* <math>\\mathfrak{C}</math>: the class of [[Cyclic_group|cyclic groups]].\n* <math>\\mathfrak{A}~</math>: the class of [[Abelian_group|abelian groups]].\n* <math>\\mathfrak{U}~</math>: the class of finite [[Supersolvable group|supersolvable groups]]\n* <math>\\mathfrak{N}~</math>: the class of [[nilpotent group|nilpotent groups]]\n* <math>\\mathfrak{S}~</math>: the class of finite [[Solvable_group|solvable groups]]\n* <math>\\mathfrak{I}~</math>: the class of finite [[Simple group|simple groups]]\n* <math>\\mathfrak{E}~</math>: the class of [[finite groups]]\n* <math>\\mathfrak{G}~</math>: the class of all groups\n\n== Product of classes of groups==\n\nGiven two classes of groups <math>\\mathfrak{X}~</math> and <math>\\mathfrak{Y}~</math> it is defined the '''product of classes'''\n\n<math>\\mathfrak{X}\\mathfrak{Y}~=(G|G \\text{ has a normal subgroup } N\\in\\mathfrak{X} \\text{ with } G/N\\in\\mathfrak{Y})</math>\n\nThis construction allows us to recursively define the '''power of a class''' by setting\n\n<math>\\mathfrak{X}^0=(1)</math> and <math>\\mathfrak{X}^n=\\mathfrak{X}^{n-1}\\mathfrak{X}</math>\n\nIt must be remarked that this [[binary operation]] on the class of classes of groups is neither [[associative]] nor [[commutative]]. For instance, consider the [[alternating group]] of degree 4 (and order 12); this group belongs to the class <math>(\\mathfrak{C}\\mathfrak{C})\\mathfrak{C}</math> because it has as a subgroup the group <math>V_4</math> which belongs to <math>\\mathfrak{C}\\mathfrak{C}</math> and furthermore <math>A_4/V_4\\cong C_3</math> which is in <math>\\mathfrak{C}</math>. However <math>A_4</math> has no non-trivial normal cyclic subgroup, so <math>A_4\\not\\in\\mathfrak{C}(\\mathfrak{C}\\mathfrak{C})</math>. Then <math>\\mathfrak{C}(\\mathfrak{C}\\mathfrak{C})\\not=(\\mathfrak{C}\\mathfrak{C})\\mathfrak{C}</math>.\n\nHowever it is straightforward from the definition that for any three classes of groups <math>\\mathfrak{X}</math>, <math>\\mathfrak{Y}</math>, and <math>\\mathfrak{Z}</math>,\n\n<math>\\mathfrak{X}(\\mathfrak{Y}\\mathfrak{Z})\\subseteq(\\mathfrak{X}\\mathfrak{Y})\\mathfrak{Z}</math>\n\n== Class maps and closure operations==\n\nA '''class map''' '''c''' is a map which assigns a class of groups <math>\\mathfrak{X}</math> to another class of groups <math>c\\mathfrak{X}</math>. A class map is said to be a closure operation if it satisfies the next properties:\n# '''c''' is expansive: <math>\\mathfrak{X}\\subseteq c\\mathfrak{X}</math>\n# '''c''' is [[idempotent]]: <math>c\\mathfrak{X}=c(c\\mathfrak{X})</math>\n# '''c''' is monotonic: If <math>\\mathfrak{X}\\subseteq\\mathfrak{Y}</math> then <math>c\\mathfrak{X}\\subseteq c\\mathfrak{Y}</math>\n\nSome of the most common examples of closure operations are:\n\n* <math>S\\mathfrak{X}=(G|G\\leq H, \\ H\\in\\mathfrak{X})</math>\n* <math>Q\\mathfrak{X}=(G|\\text{exists }H\\in\\mathfrak{X} \\text{ and an epimorphism from }H \\text{ to }G)</math>\n* <math>N_0\\mathfrak{X}=(G| \\text{ exists }K_i\\ (i=1,\\cdots,r)\\text{ subnormal in }G\\text{ with }K_i\\in\\mathfrak{X}\\text{ and }G=\\langle K_1,\\cdots,K_r\\rangle)</math>\n* <math>R_0\\mathfrak{X}=(G| \\text{ exists }N_i\\ (i=1,\\cdots,r)\\text{ normal in }G\\text{ with }G/N_i\\in\\mathfrak{X}\\text{ and }\\bigcap\\limits_{i=1}^rNi=1)</math>\n* <math>S_n\\mathfrak{X}=(G|G\\text{ is subnormal in }H\\text{ for some }H\\in\\mathfrak{X})</math>\n\n==References==\n{{reflist}}\n*{{Citation | last1=Ballester-Bolinches | first1=Adolfo | last2=Ezquerro | first2=Luis M. | title=Classes of finite groups | url=https://books.google.com/books?id=VoQ53SosWLIC | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Mathematics and Its Applications (Springer) | isbn=978-1-4020-4718-3 | mr=2241927  | year=2006 | volume=584}}\n*{{Citation | last1=Doerk | first1=Klaus | last2=Hawkes | first2=Trevor | title=Finite soluble groups | url=https://books.google.com/books?id=E7iL1eWB1TkC | publisher=Walter de Gruyter & Co. | location=Berlin | series=de Gruyter Expositions in Mathematics | isbn=978-3-11-012892-5 | mr=1169099  | year=1992 | volume=4}}\n\n==See also==\n[[Formation (group theory)|Formation]]\n[[Category:Properties of groups]]\n[[Category:Group theory]]\n[[Category:Algebraic structures]]"
    },
    {
      "title": "Clifford semigroup",
      "url": "https://en.wikipedia.org/wiki/Clifford_semigroup",
      "text": "A '''Clifford semigroup''' (sometimes also called \"inverse Clifford semigroup\") is a [[completely regular semigroup|completely regular]] [[inverse semigroup]].\nIt is an inverse semigroup with<ref>[http://www-circa.mcs.st-and.ac.uk/Theses/cacmsc.pdf Presentations of Semigroups and Inverse Semigroups] {{webarchive|url=https://web.archive.org/web/20061011032525/http://www-circa.mcs.st-and.ac.uk/Theses/cacmsc.pdf |date=2006-10-11 }} section 4.3 Some Results on Clifford Semigroups (accessed on 14 December 2014)</ref>\n<math>xx^{-1}=x^{-1}x</math>. Examples of Clifford semigroups are groups and commutative inverse semigroups.\n\nIn a Clifford semigroup,<ref>[https://gentzen.wordpress.com/2014/12/06/algebraic-characterizations-of-inverse-semigroups-and-strongly-regular-rings/ Algebraic characterizations of inverse semigroups and strongly regular rings] theorem 2 (accessed on 14 December 2014)</ref> <math> xy=yx \\leftrightarrow x^{-1}y=yx^{-1}</math>.\n\n==References==\n\n{{reflist}}\n\n[[Category:Algebraic structures]]\n[[Category:Semigroup theory]]\n\n\n{{abstract-algebra-stub}}"
    },
    {
      "title": "Commutative ring",
      "url": "https://en.wikipedia.org/wiki/Commutative_ring",
      "text": "In [[ring theory]], a branch of [[abstract algebra]], a '''commutative ring''' is a [[Ring (mathematics)|ring]] in which the multiplication operation is [[commutative]]. The study of commutative rings is called [[commutative algebra]]. Complementarily, [[noncommutative algebra]] is the study of [[noncommutative ring]]s where multiplication is not required to be commutative.\n\n{{Algebraic structures |Ring}}\n\n==Definition and first examples==\n\n===Definition===\n{{details|topic=the definition of rings|Ring (mathematics)}}\nA ''ring'' is a [[Set (mathematics)|set]] ''R'' equipped with two [[binary operation]]s, i.e. operations combining any two elements of the ring to a third. They are called ''addition'' and ''multiplication'' and commonly denoted by \"+\" and \"⋅\"; e.g. {{nowrap|''a'' + ''b''}} and {{nowrap|''a'' ⋅ ''b''}}. To form a ring these two operations have to satisfy a number of properties: the ring has to be an [[abelian group]] under addition as well as a [[monoid]] under multiplication, where multiplication [[distributive law|distributes]] over addition; i.e., {{nowrap|1=''a'' ⋅ (''b'' + ''c'') = (''a'' ⋅ ''b'') + (''a'' ⋅ ''c'')}}. The identity elements for addition and multiplication are denoted 0 and 1, respectively.\n\nIf the multiplication is commutative, i.e.\n:''a'' ⋅ ''b'' = ''b'' ⋅ ''a'',\nthen the ring ''R'' is called ''commutative''. In the remainder of this article, all rings will be commutative, unless explicitly stated otherwise.\n\n===First examples===\nAn important example, and in some sense crucial, is the [[integer|ring of integer]]s '''Z''' with the two operations of addition and multiplication. As the multiplication of integers is a commutative operation, this is a commutative ring. It is usually denoted '''Z''' as an abbreviation of the [[German language|German]] word ''Zahlen'' (numbers).\n\nA [[field (mathematics)|field]] is a commutative ring where <math> 0 \\not = 1 </math> and every [[0 (number)|non-zero]] element ''a'' is invertible; i.e., has a multiplicative inverse ''b'' such that ''a'' ⋅ ''b'' = 1. Therefore, by definition, any field is a commutative ring. The [[rational number|rational]], [[real number|real]] and [[complex number]]s form fields.\n\nIf ''R'' is a given commutative ring, then the set of all [[polynomial]]s in the variable ''X'' whose coefficients are in ''R'' forms the [[polynomial ring]], denoted ''R''[''X'']. The same holds true for several variables.\n\nIf ''V'' is some [[topological space]], for example a subset of some '''R'''<sup>''n''</sup>, real- or complex-valued [[continuous function]]s on ''V'' form a commutative ring. The same is true for [[differentiable function|differentiable]] or [[holomorphic function]]s, when the two concepts are defined, such as for ''V'' a [[complex manifold]].\n\n==Divisibility==\nIn contrast to fields, where every nonzero element is multiplicatively invertible, the concept of [[divisibility (ring theory)|divisibility for rings]] is richer. An element ''a'' of ring ''R'' is called a [[unit (algebra)|unit]] if it possesses a multiplicative inverse. Another particular type of element is the [[zero divisor]]s, i.e. an element ''a'' such that there exists a non-zero element ''b'' of the ring such that {{nowrap|1=''ab'' = 0}}. If ''R'' possesses no non-zero zero divisors, it is called an [[integral domain]] (or domain). An element ''a'' satisfying {{nowrap|1=''a''<sup>''n''</sup> = 0}} for some positive integer ''n'' is called [[nilpotent element|nilpotent]].\n\n===Localizations===\n{{Main|Localization of a ring}}\nThe ''localization'' of a ring is a process in which some elements are rendered invertible, i.e. multiplicative inverses are added to the ring. Concretely, if ''S'' is a [[multiplicatively closed subset]] of ''R'' (i.e. whenever {{nowrap|''s'', ''t'' ∈ ''S''}} then so is ''st'') then the ''localization'' of ''R'' at ''S'', or ''ring of fractions'' with denominators in ''S'', usually denoted ''S''<sup>−1</sup>''R'' consists of symbols\n:<math>\\frac{r}{s}</math> with ''r'' ∈ ''R'', ''s'' ∈ ''S''\nsubject to certain rules that mimic the cancellation familiar from rational numbers. Indeed, in this language '''Q''' is the localization of '''Z''' at all nonzero integers. This construction works for any integral domain ''R'' instead of '''Z'''. The localization (''R'' \\ {0})<sup>−1</sup>''R'' is a field, called the [[quotient field]] of ''R''.\n\n==Ideals and modules==\n{{hatnote|In the following, R denotes a commutative ring.}}\n\nMany of the following notions also exist for not necessarily commutative rings, but the definitions and properties are usually more complicated. For example, all ideals in a commutative ring are automatically [[two-sided ideal|two-sided]], which simplifies the situation considerably.\n\n===Modules and ideals===\n{{Main|Module (mathematics)|l1=Module}}\nFor a ring ''R'', an ''R''-''module'' ''M'' is like what a vector space is to a field. That is, elements in a module can be added; they can be multiplied by elements of ''R'' subject to the same axioms as for a vector space. The study of modules is significantly more involved than the one of vector spaces in [[linear algebra]], since several features of vector spaces fail for modules in general: modules need not be [[free module|free]], i.e., of the form\n:<math>M= \\bigoplus_{i \\in I} R.</math>\nEven for free modules, the [[rank of a free module]] (i.e. the analog of the dimension of vector spaces) may not be well-defined. Finally, submodules of finitely generated modules need not be finitely generated (unless ''R'' is Noetherian, see [[#submodules of f g modules|below]]).\n\n===Ideals===\n{{Main|Ideal (ring theory)|l1=Ideal|Factor ring}}\n''Ideals'' of a ring ''R'' are the [[submodule]]s of ''R'', i.e., the modules contained in ''R''. In more detail, an ideal ''I'' is a non-empty subset of ''R'' such that for all ''r'' in ''R'', ''i'' and ''j'' in ''I'', both ''ri'' and ''i'' + ''j'' are in ''I''. For various applications, understanding the ideals of a ring is of particular importance, but often one proceeds by studying modules in general.\n\nAny ring has two ideals, namely the [[0 (number)|zero ideal]] {0} and ''R'', the whole ring. These two ideals are the only ones precisely if ''R'' is a field. Given any subset ''F'' = {''f''<sub>''j''</sub>}<sub>''j'' ∈ ''J''</sub> of ''R'' (where ''J'' is some index set), the ideal ''generated by F'' is the smallest ideal that contains ''F''. Equivalently, it is given by finite [[linear combination]]s\n:''r''<sub>1</sub>''f''<sub>1</sub> + ''r''<sub>2</sub>''f''<sub>2</sub> + ... + ''r''<sub>''n''</sub>''f''<sub>''n''</sub>.\n\n====Principal ideal domains====\nIf ''F'' consists of a single element ''r'', the ideal generated by ''F'' consists of the multiples of ''r'', i.e., the elements of the form ''rs'' for arbitrary elements ''s''. Such an ideal is called a [[principal ideal]]. If every ideal is a principal ideal, ''R'' is called a [[principal ideal ring]]; two important cases are '''Z''' and ''k''[''X''], the polynomial ring over a field ''k''. These two are in addition domains, so they are called [[principal ideal domain]]s.\n\nUnlike for general rings, for a principal ideal domain, the properties of individual elements are strongly tied to the properties of the ring as a whole. For example, any principal ideal domain ''R'' is a [[unique factorization domain]] (UFD) which means that any element is a product of irreducible elements, in a (up to reordering of factors) unique way. Here, an element ''a'' in a domain is called [[irreducible element|irreducible]] if the only way of expressing it as a product\n:''a'' = ''bc'',\nis by either ''b'' or ''c'' being a unit. An example, important in [[Field (mathematics)|field theory]], are [[irreducible polynomial]]s, i.e., irreducible elements in ''k''[''X''], for a field ''k''. The fact that '''Z''' is a UFD can be stated more elementarily by saying that any natural number can be uniquely decomposed as product of powers of prime numbers. It is also known as the [[fundamental theorem of arithmetic]].\n\nAn element ''a'' is a [[prime element]] if whenever ''a'' divides a product ''bc'', ''a'' divides ''b'' or ''c''. In a domain, being prime implies being irreducible. The converse is true in a unique factorization domain, but false in general.\n\n====The factor ring====\nThe definition of ideals is such that \"dividing\" ''I'' \"out\" gives another ring, the ''factor ring'' ''R'' / ''I'': it is the set of [[coset]]s of ''I'' together with the operations\n:(''a'' + ''I'') + (''b'' + ''I'') = (''a'' + ''b'') + I and (''a'' + ''I'')(''b'' + ''I'') = ''ab'' + ''I''.\nFor example, the ring '''Z'''/''n'''''Z''' (also denoted '''Z'''<sub>''n''</sub>), where ''n'' is an integer, is the ring of integers modulo ''n''. It is the basis of [[modular arithmetic]].\n\nAn ideal is ''proper'' if it is strictly smaller than the whole ring. An ideal that is not strictly contained in any proper ideal is called [[maximal ideal|maximal]]. <cite id=characterisation_of_maximal_ideals></cite> An ideal ''m'' is maximal [[if and only if]] ''R'' / ''m'' is a field. <cite id=existence_of_maximal_ideals></cite> Except for the [[zero ring]], any ring (with identity) possesses at least one maximal ideal; this follows from [[Zorn's lemma]].\n\n===Noetherian rings===\n{{Main|Noetherian ring}}\nA ring is called ''Noetherian'' (in honor of [[Emmy Noether]], who developed this concept) if every [[ascending chain condition|ascending chain of ideals]]\n:0 ⊆ ''I''<sub>0</sub> ⊆ ''I''<sub>1</sub>  ... ⊆ ''I''<sub>''n''</sub> ⊆ ''I''<sub>''n'' + 1</sub> ⊆ ...\nbecomes stationary, i.e. becomes constant beyond some index ''n''. Equivalently, any ideal is generated by finitely many elements, or, yet equivalent, <cite id=submodules_of_f_g_modules>[[submodule]]s of finitely generated modules are finitely generated</cite>.\n\nBeing Noetherian is a highly important finiteness condition, and the condition is preserved under many operations that occur frequently in geometry. For example, if ''R'' is Noetherian, then so is the polynomial ring {{nowrap|''R''[''X''<sub>1</sub>, ''X''<sub>2</sub>, ..., ''X''<sub>''n''</sub>]}} (by [[Hilbert's basis theorem]]), any localization ''S''<sup>−1</sup>''R'', and also any factor ring ''R'' / ''I''.\n\nAny non-noetherian ring ''R'' is the [[union (set theory)|union]] of its Noetherian subrings. This fact, known as [[Noetherian approximation]], allows the extension of certain theorems to non-Noetherian rings.\n\n===Artinian rings===\nA ring is called [[Artinian ring|Artinian]] (after [[Emil Artin]]), if every descending chain of ideals\n:''R'' ⊇ ''I''<sub>0</sub> ⊇ ''I''<sub>1</sub>  ... ⊇ ''I''<sub>''n''</sub> ⊇ ''I''<sub>''n'' + 1</sub> ⊇ ...\nbecomes stationary eventually. Despite the two conditions appearing symmetric, Noetherian rings are much more general than Artinian rings. For example, '''Z''' is Noetherian, since every ideal can be generated by one element, but is not Artinian, as the chain\n:'''Z''' ⊋ 2'''Z''' ⊋ 4'''Z''' ⊋ 8'''Z''' ⊋ ...\nshows. In fact, by the [[Hopkins–Levitzki theorem]], every Artinian ring is Noetherian. More precisely, Artinian rings can be characterized as the Noetherian rings whose Krull dimension is zero.\n\n==The spectrum of a commutative ring==\n\n===Prime ideals===\n{{Main|Prime ideal}}\nAs was mentioned above, '''Z''' is a [[unique factorization domain]]. This is not true for more general rings, as algebraists realized in the 19th century. For example, in\n:<math>\\mathbf Z[\\sqrt{-5}],</math>\nthere are two genuinely distinct ways of writing 6 as a product:\n:<math>6 = 2 \\cdot 3 = (1 + \\sqrt{-5})(1 - \\sqrt{-5}).</math>\nPrime ideals, as opposed to prime elements, provide a way to circumvent this problem. A prime ideal is a proper (i.e., strictly contained in ''R'') ideal ''p'' such that, whenever the product ''ab'' of any two ring elements ''a'' and ''b'' is in ''p'', at least one of the two elements is already in ''p''. (The opposite conclusion holds for any ideal, by definition). Thus, if a prime ideal is principal, it is equivalently generated by a prime element. However, in rings such as <math>\\mathbf Z[\\sqrt{-5}]</math>, prime ideals need not be principal. This limits the usage of prime elements in ring theory. A cornerstone of algebraic number theory is, however, the fact that in any [[Dedekind ring]] (which includes <math>\\mathbf Z[\\sqrt{-5}]</math> and more generally the [[algebraic integers|ring of integers in a number field]]) any ideal (such as the one generated by 6) decomposes uniquely as a product of prime ideals.\n\nAny maximal ideal is a prime ideal or, more briefly, is prime. Moreover, an ideal ''I'' is prime if and only if the factor ring ''R'' / ''I'' is an integral domain. Proving that an ideal is prime, or equivalently that a ring has no zero-divisors can be very difficult. Yet another way of expressing the same is to say that the [[Complement (set theory)|complement]] ''R'' \\ ''p'' is multiplicatively closed. The localisation (''R'' \\ ''p'')<sup>−1</sup>''R'' is important enough to have its own notation: ''R''<sub>''p''</sub>. This ring has only one maximal ideal, namely ''pR''<sub>''p''</sub>. Such rings are called [[local ring|local]].\n\n===The spectrum===\n{{Main|Spectrum of a ring}}\n[[Image:Spec Z.png|right|400px|thumb|Spec ('''Z''') contains a point for the zero ideal. The closure of this point is the entire space. The remaining points are the ones corresponding to ideals (''p''), where ''p'' is a prime number. These points are closed.]]\nThe ''spectrum of a ring R'',<ref group=nb>This notion can be related to the [[Spectrum of an operator|spectrum]] of a linear operator, see [[Spectrum of a C*-algebra]] and [[Gelfand representation]].</ref>  denoted by Spec ''R'', is the set of all prime ideals of ''R''. It is equipped with a topology, the [[Zariski topology]], which reflects the algebraic properties of ''R'': a basis of open subsets is given by\n:''D''(''f'') = {''p'' ∈ ''Spec R'', ''f'' ∉ ''p''}, where ''f'' is any ring element.\nInterpreting ''f'' as a function that takes the value ''f'' mod ''p'' (i.e., the image of ''f'' in the residue field ''R''/''p''), this subset is the locus where ''f'' is non-zero. The spectrum also makes precise the intuition that localisation and factor rings are complementary: the natural maps ''R'' → ''R''<sub>''f''</sub> and ''R'' → ''R'' / ''fR'' correspond, after endowing the spectra of the rings in question with their Zariski topology, to complementary [[open immersion|open]] and [[closed immersion]]s respectively. Even for basic rings, such as illustrated for ''R'' = '''Z''' at the right, the Zariski topology is quite different from the one on the set of real numbers.\n\nThe spectrum contains the set of maximal ideals, which is occasionally denoted mSpec (''R''). For an [[algebraically closed field]] ''k'', mSpec (k[''T''<sub>1</sub>, ..., ''T''<sub>''n''</sub>] / (''f''<sub>1</sub>, ..., ''f''<sub>''m''</sub>)) is in bijection with the set\n:{''x'' =(''x''<sub>1</sub>, ..., ''x''<sub>''n''</sub>) ∊ ''k''<sup>''n''</sup> | ''f''<sub>1</sub>(''x'') = ... = ''f''<sub>''m''</sub>(''x'') = 0.}\nThus, maximal ideals reflect the geometric properties of solution sets of polynomials, which is an initial motivation for the study of commutative rings. However, the consideration of non-maximal ideals as part of the geometric properties of a ring is useful for several reasons. For example, the minimal prime ideals (i.e., the ones not strictly containing smaller ones) correspond to the [[irreducible component]]s of Spec ''R''. For a Noetherian ring ''R'', Spec ''R'' has only finitely many irreducible components. This is a geometric restatement of [[primary decomposition]], according to which any ideal can be decomposed as a product of finitely many [[primary ideal]]s. This fact is the ultimate generalization of the decomposition into prime ideals in Dedekind rings.\n\n===Affine schemes===\nThe notion of a spectrum is the common basis of commutative algebra and [[algebraic geometry]]. Algebraic geometry proceeds by endowing Spec ''R'' with a [[sheaf (mathematics)|sheaf]] <math>\\mathcal O</math> (an entity that collects functions defined locally, i.e. on varying open subsets). The datum of the space and the sheaf is called an [[affine scheme]]. Given an affine scheme, the underlying ring ''R'' can be recovered as the [[global section]]s of <math>\\mathcal O</math>. Moreover, this one-to-one correspondence between rings and affine schemes is also compatible with ring homomorphisms: any ''f'' : ''R'' → ''S'' gives rise to a [[continuous map]] in the opposite direction\n:Spec ''S'' → Spec ''R'', ''q'' ↦ ''f''<sup>−1</sup>(''q''), i.e. any prime ideal of ''S'' is mapped to its [[preimage]] under ''f'', which is a prime ideal of ''R''.\nThe resulting [[equivalence of categories|equivalence]] of the two said categories aptly reflects algebraic properties of rings in a geometrical manner.\n\nSimilar to the fact that [[manifold (mathematics)|manifolds]] are locally given by open subsets of '''R'''<sup>''n''</sup>, affine schemes are local models for [[scheme (mathematics)|schemes]], which are the object of study in algebraic geometry. Therefore, several notions concerning commutative rings stem from geometric intuition.\n\n===Dimension===\n{{Main|Krull dimension}}\nThe ''Krull dimension'' (or dimension) dim ''R'' of a ring ''R'' measures the \"size\" of a ring by, roughly speaking, counting independent elements in ''R''. The dimension of algebras over a field ''k'' can be axiomatized by four properties:\n* The dimension is a local property: dim ''R'' = sup<sub>p ∊ Spec ''R''</sub> dim ''R''<sub>''p''</sub>.\n* The dimension is independent of nilpotent elements: if ''I'' ⊆ ''R'' is nilpotent then dim ''R'' = dim ''R'' / ''I''.\n* The dimension remains constant under a finite extension: if ''S'' is an ''R''-algebra which is finitely generated as an ''R''-module, then dim ''S'' = dim ''R''.\n* The dimension is calibrated by dim ''k''[''X''<sub>1</sub>, ..., ''X''<sub>''n''</sub>] = ''n''. This axiom is motivated by regarding the polynomial ring in ''n'' variables as an algebraic analogue of [[affine space|''n''-dimensional space]].\n\nThe dimension is defined, for any ring ''R'', as the supremum of lengths ''n'' of chains of prime ideals\n:''p''<sub>0</sub> ⊊ ''p''<sub>1</sub> ⊊ ... ⊊ ''p''<sub>''n''</sub>.\nFor example, a field is zero-dimensional, since the only prime ideal is the zero ideal. The integers are one-dimensional, since chains are of the form (0) ⊊ (''p''), where ''p'' is a [[prime number]]. For non-Noetherian rings, and also non-local rings, the dimension may be infinite, but Noetherian local rings have finite dimension. Among the four axioms above, the first two are elementary consequences of the definition, whereas the remaining two hinge on important facts in [[commutative algebra]], the [[going-up theorem]] and [[Krull's principal ideal theorem]].\n\n==Ring homomorphisms==\n{{Main|Ring homomorphism}}\nA ''ring homomorphism'' or, more colloquially, simply a ''map'', is a map ''f'' : ''R'' → ''S'' such that\n:''f''(''a'' + ''b'') = ''f''(''a'') + ''f''(''b''), ''f''(''ab'') = ''f''(''a'')''f''(''b'') and ''f''(1) = 1.\nThese conditions ensure ''f''(0) = 0. Similarly as for other algebraic structures, a ring homomorphism is thus a map that is compatible with the structure of the algebraic objects in question. In such a situation ''S'' is also called an ''R''-algebra, by understanding that ''s'' in ''S'' may be multiplied by some ''r'' of ''R'', by setting\n:''r'' · ''s'' := ''f''(''r'') · ''s''.\n\nThe ''kernel'' and ''image'' of ''f'' are defined by ker (''f'') = {''r'' ∈ ''R'', ''f''(''r'') = 0} and im (''f'') = ''f''(''R'') = {''f''(''r''), ''r'' ∈ ''R''}. The kernel is an [[ring ideal|ideal]] of ''R'', and the image is a [[subring]] of ''S''.\n\nA ring homomorphism is called an isomorphism if it is bijective. An example of a ring isomorphism, known as the [[Chinese remainder theorem]], is\n:<math>\\mathbf Z/n = \\bigoplus_{i=0}^k \\mathbf Z/p_i</math>\nwhere ''n'' = ''p''<sub>1</sub>''p''<sub>2</sub>...''p''<sub>''k''</sub> is a product of pairwise distinct [[prime number]]s.\n\nCommutative rings, together with ring homomorphisms, form a [[category (mathematics)|category]]. The ring '''Z''' is the [[initial object]] in this category, which means that for any commutative ring ''R'', there is a unique ring homomorphism '''Z''' &rarr; ''R''. By means of this map, an integer ''n'' can be regarded as an element of ''R''. For example, the [[binomial formula]]\n:<math>(a+b)^n = \\sum_{k=0}^n \\binom n k a^k b^{n-k}</math>\nwhich is valid for any two elements ''a'' and ''b'' in any commutative ring ''R'' is understood in this sense by interpreting the binomial coefficients as elements of ''R'' using this map.\n\n[[File:Tensor product of algebras.png|thumb|The [[universal property]] of ''S'' ⊗<sub>''R''</sub> ''T''\n states that for any two maps ''S'' &rarr; ''W'' and ''T'' &rarr; ''W'' which make the outer quadrangle commute, there is a unique map ''S'' ⊗<sub>''R''</sub> ''T'' &rarr; ''W'' which makes the entire diagram commute.]]\nGiven two ''R''-algebras ''S'' and ''T'', their [[tensor product of algebras|tensor product]]\n:''S'' ⊗<sub>''R''</sub> ''T''\nis again a commutative ''R''-algebra. In some cases, the tensor product can serve to find a ''T''-algebra which relates to ''Z'' as ''S'' relates to ''R''. For example,\n:''R''[''X''] ⊗<sub>''R''</sub> ''T'' = ''T''[''X''].\n\n===Finite generation===\nAn ''R''-algebra ''S'' is called [[finitely generated algebra|finitely generated (as an algebra)]] if there are finitely many elements ''s''<sub>1</sub>, ..., ''s''<sub>''n''</sub> such that any element of ''s'' is expressible as a polynomial in the ''s''<sub>''i''</sub>. Equivalently, ''S'' is isomorphic to\n:''R''[''T''<sub>1</sub>, ..., ''T''<sub>''n''</sub>] / ''I''.\n\nA much stronger condition is that ''S'' is [[finitely generated module|finitely generated as an ''R''-module]], which means that any ''s'' can be expressed as a ''R''-linear combination of some finite set ''s''<sub>1</sub>, ..., ''s''<sub>''n''</sub>.\n\n==Local rings==\nA ring is called [[local ring|local]] if it has only a single maximal ideal, denoted by ''m''. For any (not necessarily local) ring ''R'', the localization\n:''R''<sub>''p''</sub>\nat a prime ideal ''p'' is local. This localization reflects the geometric properties of Spec ''R'' \"around ''p''\". Several notions and problems in commutative algebra can be reduced to the case when ''R'' is local, making local rings a particularly deeply studied class of rings. The [[residue field]] of ''R'' is defined as\n:''k'' = ''R'' / ''m''.\nAny ''R''-module ''M'' yields a ''k''-vector space given by ''M'' / ''mM''. [[Nakayama's lemma]] shows this passage is preserving important information: a finitely generated module ''M'' is zero if and only if ''M'' / ''mM'' is zero.\n\n===Regular local rings===\n[[File:Node_(algebraic_geometry).png|thumb|left|The [[cubic plane curve]] (red) defined by the equation ''y''<sup>2</sup> = ''x''<sup>2</sup>(''x'' + ''1'') is [[singularity (mathematics)|singular]] at the origin, i.e., the ring ''k''[''x'', ''y''] / ''y''<sup>2</sup> &minus; ''x''<sup>2</sup>(''x'' + ''1''), is not a regular ring. The tangent cone (blue) is a union of two lines, which also reflects the singularity.]]\nThe ''k''-vector space ''m''/''m''<sup>2</sup> is an algebraic incarnation of the [[cotangent space]]. Informally, the elements of ''m'' can be thought of as functions which vanish at the point ''p'', whereas ''m''<sup>2</sup> contains the ones which vanish with order at least 2. For any Noetherian local ring ''R'', the inequality\n:dim<sub>''k''</sub> ''m''/''m''<sup>2</sup> &ge; dim ''R''\nholds true, reflecting the idea that the cotangent (or equivalently the tangent) space has at least the dimension of the space Spec ''R''. If equality holds true in this estimate, ''R'' is called a [[regular local ring]]. A Noetherian local ring is regular if and only if the ring (which is the ring of functions on the [[tangent cone]])\n:<math>\\bigoplus_n m^n / m^{n+1}</math>\nis isomorphic to a polynomial ring over ''k''. Broadly speaking, regular local rings are somewhat similar to polynomial rings.<ref>{{harvtxt|Matsumura|loc=§7, Remarks, p. 143}}</ref> Regular local rings are UFD's.<ref>{{harvtxt|Matsumura|loc=§19, Theorem 48}}</ref>\n\n[[Discrete valuation ring]]s are equipped with a function which assign an integer to any element ''r''. This number, called the valuation of ''r'' can be informally thought of as a zero or pole order of ''r''. Discrete valuation rings are precisely the one-dimensional regular local rings. For example, the ring of germs of holomorphic functions on a [[Riemann surface]] is a discrete valuation ring.\n\n===Complete intersections===\n[[File:Twisted_cubic_curve.png|thumb|The [[twisted cubic]] (green) is a set-theoretic complete intersection, but not a complete intersection.]]\nBy [[Krull's principal ideal theorem]], a foundational result in the [[dimension theory (algebra)|dimension theory of rings]], the dimension of\n:''R'' = ''k''[''T''<sub>1</sub>, ..., ''T''<sub>''r''</sub>] / (''f''<sub>1</sub>, ..., ''f''<sub>''n''</sub>)\nis at least ''r'' &minus; ''n''. A ring ''R'' is called a [[complete intersection ring]] if it can be presented in a way that attains this minimal bound. This notion is also mostly studied for local rings. Any regular local ring is a complete intersection ring, but not conversely.\n\nA ring ''R'' is a ''set-theoretic'' complete intersection if the reduced ring associated to ''R'', i.e., the one obtained by dividing out all nilpotent elements, is a complete intersection. As of 2017, it is in general unknown, whether curves in three-dimensional space are set-theoretic complete intersections.<ref>{{harvtxt|Lyubeznik|1989}}</ref>\n\n===Cohen–Macaulay rings===\nThe [[depth (ring theory)|depth]] of a local ring ''R'' is the number of elements in some (or, as can be shown, any) maximal regular sequence, i.e., a sequence ''a''<sub>1</sub>, ..., \n''a''<sub>''n''</sub> &isin; ''m'' such that all ''a''<sub>''i''</sub> are non-zero divisors in\n:''R'' / (''a''<sub>1</sub>, ..., ''a''<sub>''i''&minus;1</sub>).\nFor any local Noetherian ring, the inequality\n:depth (''R'') &le; dim (''R'')\nholds. A local ring in which equality takes place is called a [[Cohen–Macaulay ring]]. Local complete intersection rings, and a fortiori, regular local rings are Cohen–Macaulay, but not conversely. Cohen–Macaulay combine desirable properties of regular rings (such as the property of being [[universally catenary ring]]s, which means that the (co)dimension of primes is well-behaved), but are also more robust under taking quotients than regular local rings.<ref>{{harvtxt|Eisenbud|1995|loc=Corollary 18.10, Proposition 18.13}}</ref>\n\n==Constructing commutative rings==\nThere are several ways to construct new rings out of given ones. The aim of such constructions is often to improve certain properties of the ring so as to make it more readily understandable. For example, an integral domain that is [[integral element#Equivalent definitions|integrally closed]] in its [[field of fractions]] is called [[normal ring|normal]]. This is a desirable property, for example any normal one-dimensional ring is necessarily [[Regular local ring|regular]]. Rendering{{clarify|date=March 2012}} a ring normal is known as ''normalization''.\n\n===Completions===\nIf ''I'' is an ideal in a commutative ring ''R'', the powers of ''I'' form [[neighborhood (topology)|topological neighborhoods]] of ''0'' which allow ''R'' to be viewed as a [[topological ring]]. This topology is called the [[I-adic topology|''I''-adic topology]]. ''R'' can then be completed with respect to this topology. Formally, the ''I''-adic completion is the [[inverse limit]] of the rings ''R''/''I<sup>n</sup>''. For example, if ''k'' is a field, ''k''<nowiki>[[</nowiki>''X''<nowiki>]]</nowiki>, the [[formal power series]] ring in one variable over ''k'', is the ''I''-adic completion of ''k''[''X''] where ''I'' is the principal ideal generated by ''X''. This ring serves as an algebraic analogue of the disk. Analogously, the ring of [[p-adic number|''p''-adic integers]] is the completion of '''Z'''  with respect to the principal ideal (''p''). Any ring that is isomorphic to its own completion, is called [[complete ring|complete]].\n\nComplete local rings satisfy [[Hensel's lemma]], which roughly speaking allows extending solutions (of various problems) over the residue field ''k'' to ''R''.\n\n==Homological notions==\nSeveral deeper aspects of commutative rings have been studied using methods from [[homological algebra]]. {{harvtxt|Hochster|2007}} lists some open questions in this area of active research.\n\n===Projective modules and Ext functors===\nProjective modules can be defined to be the [[direct summand]]s of free modules. If ''R'' is local, any finitely generated projective module is actually free, which gives content to an analogy between projective modules and [[vector bundle]]s.<ref>See also [[Serre–Swan theorem]].</ref> The [[Quillen–Suslin theorem]] asserts that any finitely generated projective module over ''k''[''T''<sub>1</sub>, ..., ''T''<sub>''n''</sub>] (''k'' a field) is free, but in general these two concepts differ. \nA local Noetherian ring is regular if and only if its [[global dimension]] is finite, say ''n'', which means that any finitely generated ''R''-module has a [[resolution (homological algebra)|resolution]] by projective modules of length at most ''n''.\n\nThe proof of this and other related statements relies on the usage of homological methods, such as the\n[[Ext functor]]. This functor is the [[derived functor]] of the functor\n:Hom<sub>''R''</sub>(''M'', &minus;).\nThe latter functor is exact if ''M'' is projective, but not otherwise: for a surjective map ''E'' &rarr; ''F'' of ''R''-modules, a map ''M'' &rarr; ''F'' need not extend to a map ''M'' &rarr; ''E''. The higher Ext functors measure the non-exactness of the Hom-functor. The importance of this standard construction in homological algebra stems can be seen from the fact that a local Noetherian ring ''R'' with residue field ''k'' is regular if and only if\n:Ext<sup>''n''</sup>(''k'', ''k'')\nvanishes for all large enough ''n''. Moreover, the dimensions of these Ext-groups, known as [[Betti number]]s, grow polynomially in ''n'' if and only if ''R'' is a [[local complete intersection]] ring.<ref>{{harvtxt|Christensen|Striuli|Veliche|2010}}</ref> A key argument in such considerations is the [[Koszul complex]], which provides an explicit free resolution of the residue field ''k'' of a local ring ''R'' in terms of a regular sequence.\n\n===Flatness===\nThe [[tensor product]] is another non-exact functor relevant in the context of commutative rings: for a general ''R''-module ''M'', the functor\n:''M'' ⊗<sub>''R''</sub> &minus;\nis only right exact. If it is exact, ''M'' is called [[flat module|flat]]. Despite being defined in terms of homological algebra, flatness has profound geometric implications. For example, if an ''R''-algebra ''S'' is flat, the dimensions of the fibers\n:''S'' / ''pS'' = ''S'' ⊗<sub>''R''</sub> ''R'' / ''p''\n(for prime ideals ''p'' in ''R'') have the \"expected\" dimension, namely dim ''S'' &minus; dim ''R'' + dim (''R'' / ''p'').\n\n==Properties==\nBy [[Wedderburn's little theorem|Wedderburn's theorem]], every finite [[division ring]] is commutative, and therefore a [[finite field]]. Another condition ensuring commutativity of a ring, due to [[Nathan Jacobson|Jacobson]], is the following: for every element ''r'' of ''R'' there exists an integer {{nowrap|''n'' > 1}} such that {{nowrap|1=''r''<sup>''n''</sup> = ''r''}}.<ref>{{Harvard citations|last = Jacobson|year = 1945|nb = yes}}</ref> If, ''r''<sup>2</sup> = ''r'' for every ''r'', the ring is called [[Boolean ring]]. More general conditions which guarantee commutativity of a ring are also known.<ref>{{Harvard citations|last = Pinter-Lucke|year = 2007|nb = yes}}</ref>\n\n==Generalizations==\n\n===Graded-commutative rings===\n[[File:Pair_of_pants.png|thumb|A [[pair of pants (mathematics)|pair of pants]] is a [[cobordism]] between a circle and two disjoint circles. Cobordism classes, with the [[cartesian product]] as multiplication and [[disjoint union]] as the sum, form the [[cobordism ring]].]]\nA [[graded ring]] ''R'' = ⨁<sub>''i''∊'''Z'''</sub> ''R''<sub>''i''</sub> is called [[graded-commutative ring|graded-commutative]] if\n:''ab'' = (&minus;1)<sup>deg ''a'' ⋅ deg ''b''.</sup>\nIf the ''R''<sub>''i''</sub> are connected by differentials ∂ such that an abstract form of the [[product rule]] holds, i.e.,\n:∂(''ab'') = ∂(''a'')''b'' + (&minus;1)<sup>deg ''a''</sup>∂(''b''),\n''R'' is called a [[differential graded algebra|commutative differential graded algebra]] (cdga). An example is the complex of [[differential form]]s on a [[manifold (mathematics)|manifold]], with the multiplication given by the [[exterior product]], is a cdga. The cohomology of a cdga is a graded-commutative ring, sometimes referred to as the [[cohomology ring]]. A broad range examples of graded rings arises in this way. For example, the [[Lazard's universal ring|Lazard ring]] is the ring of cobordism classes of complex manifolds.\n\nA graded-commutative ring with respect to a grading  by '''Z'''/2 (as opposed to '''Z''') is called a [[superalgebra]].\n\nA related notion is an [[almost commutative ring]], which means that ''R'' is [[filtration|filtered]] in such a way that the associated graded ring\n:gr ''R'' := ⨁ ''F''<sub>''i''</sub>''R'' / ⨁ ''F''<sub>''i''&minus;1</sub>''R''\nis commutative. An example is the [[Weyl algebra]] and more general rings of [[differential operator]]s.\n\n===Simplicial commutative rings===\nA [[simplicial commutative ring]] is a [[simplicial object]] in the category of commutative rings. They are building blocks for (connective) [[derived algebraic geometry]]. A closely related but more general notion is that of [[E-infinity ring|E<sub>∞</sub>-ring]].\n\n==Applications==\n\n===Simultaneous diagonalization of matrices===\nThe ring of [[Matrix (mathematics)|matrices]] is ''not'' commutative, since [[matrix multiplication]] fails to be commutative.<ref>For example,\n<math>\\begin{align}\n\\begin{bmatrix}\n 1 & 1\\\\\n 0 & 1\\\\\n\\end{bmatrix}\\cdot\n\\begin{bmatrix}\n 1 & 1\\\\\n 1 & 0\\\\\n\\end{bmatrix} &=\n\\begin{bmatrix}\n 2 & 1\\\\\n 1 & 0\\\\\n\\end{bmatrix}\\\\\n\\begin{bmatrix}\n 1 & 1\\\\\n 1 & 0\\\\\n\\end{bmatrix}\\cdot\n\\begin{bmatrix}\n 1 & 1\\\\\n 0 & 1\\\\\n\\end{bmatrix} &=\n\\begin{bmatrix}\n 1 & 2\\\\\n 1 & 1\\\\\n\\end{bmatrix}\n\\end{align}</math></ref>\nHowever, any two matrices ''A'' and ''B'' that do commute can be [[simultaneous diagonalization|simultaneously diagonalized]], i.e., there is an invertible matrix ''P'' such that both ''PAP''<sup>&minus;1</sup> and ''PBP''<sup>&minus;1</sup> are diagonal matrices. This fact makes representations of commutative [[Lie group]]s particularly simpler to understand than in general.\n\nAn example is the set of matrices of [[divided difference#Matrix form|divided differences]] with respect to a fixed set of nodes.\n\n==See also==\n* [[Almost ring]], a certain generalization of a commutative ring.\n* [[Divisibility (ring theory)]]: [[nilpotent element]], example: [[dual number]]s\n* Ideals and modules: [[Radical of an ideal]], [[Morita equivalence]]\n* Ring homomorphisms: [[integral element]]: [[Cayley–Hamilton theorem]], [[Integrally closed domain]], [[Krull ring]], [[Krull-Akizuki theorem]]<!-- [[Mori–Nagata theorem]], too specialized? -->\n* Primes: [[Prime avoidance lemma]], [[Jacobson radical]], [[Nilradical of a ring]], Spectrum: [[Compact space]], [[Connected ring]], [[Differential calculus over commutative algebras]], [[Banach–Stone theorem]]\n* Local rings: [[Gorenstein ring]]: [[Duality (mathematics)]], [[Eben Matlis]]; [[Dualizing module]], [[Popescu’s theorem]], [[Artin approximation theorem]].\n* \"Applications\" (commutative rings arising in mathematics): [[Holomorphic function]]s, [[Algebraic K-theory]], [[Topological K-theory]], [[Divided power structure]]s, [[Witt vector]]s, [[Hecke algebra acting on modular forms|Hecke algebra]], [[Fontaine's period rings]], [[Cluster algebra]], [[Convolution algebra]] (of a commutative group), see also [[Fréchet algebra]]\n\n==Notes==\n<references group=nb/>\n\n===Citations===\n<references />\n\n==References==\n* {{Citation | last1=Atiyah | first1=Michael | author1-link=Michael Atiyah | last2=Macdonald | first2=I. G. | author2-link=Ian G. Macdonald | title=Introduction to commutative algebra | publisher=Addison-Wesley Publishing Co. | year=1969 }}\n* {{Citation | last1=Balcerzyk | first1=Stanisław | last2=Józefiak | first2=Tadeusz | title=Commutative Noetherian and Krull rings | publisher=Ellis Horwood Ltd. | location=Chichester | series=Ellis Horwood Series: Mathematics and its Applications | isbn=978-0-13-155615-7 | year=1989}}\n* {{Citation | last1=Balcerzyk | first1=Stanisław | last2=Józefiak | first2=Tadeusz | title=Dimension, multiplicity and homological methods | publisher=Ellis Horwood Ltd. | location=Chichester | series=Ellis Horwood Series: Mathematics and its Applications.  | isbn=978-0-13-155623-2 | year=1989}}\n* {{Citation| last1=Christensen|first1=Lars Winther|first2=Janet|last2=Striuli|first3=Oana|last3=Veliche|title=Growth in the minimal injective resolution of a local ring|journal=Journal of the London Mathematical Society |series=Second Series|volume=81|issue=1|pages=24–44|year=2010|doi=10.1112/jlms/jdp058|url=https://arxiv.org/pdf/0812.4672|arxiv=0812.4672}}\n* {{Citation | last1=Eisenbud | first1=David | author1-link=David Eisenbud | title=Commutative algebra. With a view toward algebraic geometry. | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=[[Graduate Texts in Mathematics]] | isbn=978-0-387-94268-1 | mr=1322960 | year=1995 | volume=150}}\n* {{Citation|url=http://www.math.lsa.umich.edu/~hochster/homcj.pdf|title=Homological conjectures, old and new|first=Melvin|last=Hochster|journal=Illinois J. Math.|volume=51|issue=1|year=2007|pages=151–169}}\n* {{Citation | doi=10.2307/1969205 | last1=Jacobson | first1=Nathan | author1-link=Nathan Jacobson | title=Structure theory of algebraic algebras of bounded degree | year=1945 | journal=[[Annals of Mathematics]] | issn=0003-486X | volume=46 | issue=4 | pages=695–707 | jstor=1969205}}\n* {{Citation | last1=Kaplansky | first1=Irving | author1-link=Irving Kaplansky | title=Commutative rings | publisher=[[University of Chicago Press]] | edition=Revised | mr=0345945 | year=1974}}\n* {{Citation| last1=Lyubeznik|first1=Gennady|chapter=A survey of problems and results on the number of defining equations|title=Representations, resolutions and intertwining numbers|pages=375–390|year=1989|zbl=0753.14001}}\n* {{Citation | last1=Matsumura | first1=Hideyuki | title=Commutative Ring Theory | publisher=[[Cambridge University Press]] | edition=2nd | series=Cambridge Studies in Advanced Mathematics | isbn=978-0-521-36764-6 | year=1989}}\n* {{Citation | last1=Nagata | first1=Masayoshi | author1-link=Masayoshi Nagata | title=Local rings | publisher=Interscience Publishers  | series=Interscience Tracts in Pure and Applied Mathematics | isbn=978-0-88275-228-0 |year=1975 | mr=0155856 | origyear=1962 | volume=13 | pages=xiii+234}}\n* {{Citation | last1=Pinter-Lucke | first1=James | title=Commutativity conditions for rings: 1950–2005 | doi=10.1016/j.exmath.2006.07.001 | year=2007 | journal=Expositiones Mathematicae | issn=0723-0869 | volume=25 | issue=2 | pages=165–174}}\n* {{Citation | last1=Zariski | first1=Oscar | author1-link=Oscar Zariski | last2=Samuel | first2=Pierre | author2-link=Pierre Samuel | title=Commutative Algebra I, II | publisher= D. van Nostrand,  Inc. | location=Princeton, N.J. | series=University series in Higher Mathematics | year=1958–60}} ''(Reprinted 1975-76 by Springer as volumes 28-29 of Graduate Texts in Mathematics.)''\n\n{{Authority control}}\n\n[[Category:Commutative algebra]]\n[[Category:Ring theory]]\n[[Category:Algebraic structures]]"
    },
    {
      "title": "Complete Heyting algebra",
      "url": "https://en.wikipedia.org/wiki/Complete_Heyting_algebra",
      "text": "{{No footnotes|date=October 2009}}\n\nIn [[mathematics]], especially in [[order theory]], a '''complete Heyting algebra''' is a [[Heyting algebra]] that is [[completeness (order theory)|complete]] as a [[lattice (order)|lattice]]. Complete Heyting algebras are the [[object (category theory)|objects]] of three different [[category (category theory)|categories]]; the category '''CHey''', the category '''Loc''' of locales, and its [[opposite (category theory)|opposite]], the category '''Frm''' of frames. Although these three categories contain the same  objects, they differ in their [[morphism]]s, and thus get distinct names. Only the morphisms of '''CHey''' are [[homomorphism]]s of complete Heyting algebras.\n\nLocales and frames form the foundation of [[pointless topology]], which, instead of building on [[point-set topology]], recasts the ideas of [[general topology]] in categorical terms, as statements on frames and locales.\n\n== Definition ==\nConsider a [[partially ordered set]] (''P'', ≤) that is a [[complete lattice]]. Then ''P'' is a ''complete Heyting algebra'' if any of the following equivalent conditions hold:\n* ''P'' is a Heyting algebra, i.e. the operation {{nowrap|1=( ''x'' &and; &minus; )}} has a [[Adjoint functors|right adjoint]] (also called the lower adjoint of a (monotone) [[Galois connection]]), for each element ''x'' of ''P''.\n* For all elements ''x'' of ''P'' and all subsets ''S'' of ''P'', the following infinite [[distributivity (order theory)|distributivity]] law holds:\n: <math>x \\wedge \\bigvee_{s \\in S} s = \\bigvee_{s \\in S} (x \\wedge s).</math>\n* ''P'' is a distributive lattice, i.e., for all ''x'', ''y'' and ''z'' in ''P'', we have\n: <math>x \\wedge ( y \\vee z ) = ( x \\wedge y ) \\vee ( x \\wedge z )</math>\n\n: and the meet operations {{nowrap|1=( ''x'' &and; &minus; )}} are [[Scott continuous]] for all ''x'' in ''P'' (i.e., preserve the suprema of [[directed set]]s) .\n\n==Examples==\nThe system of all open sets of a given [[topological space]] ordered by inclusion is a complete Heyting algebra.\n\n== Frames and locales ==\nThe [[object (category theory)|objects]] of the category '''CHey''', the category '''Frm''' of frames and the category '''Loc''' of locales are the complete lattices satisfying the infinite distributive law. These categories differ in what constitutes a [[morphism]].\n\nThe morphisms of '''Frm''' are (necessarily [[monotonic|monotone]]) functions that [[limit preserving function (order theory)|preserve]] finite meets and arbitrary joins. Such functions are not homomorphisms of complete Heyting algebras.  The definition of Heyting algebras crucially involves the existence of right adjoints to the binary meet operation, which together define an additional [[implication operation]] ⇒. Thus, a ''homomorphism of complete Heyting algebras'' is a morphism of frames that in addition preserves implication. The morphisms of '''Loc''' are [[dual (category theory)|opposite]] to those of '''Frm''', and they are usually called maps (of locales).\n\nThe relation of locales and their maps to topological spaces and continuous functions may be seen as follows. Let\n:<math>f\\colon X\\to Y</math>\nbe any map. The [[power set]]s ''P''(''X'') and ''P''(''Y'') are [[complete Boolean algebra]]s, and the map\n: <math>f^{-1}\\colon P(Y)\\to P(X)</math>\nis a homomorphism of complete Boolean algebras. Suppose the spaces ''X'' and ''Y'' are [[topological space]]s, endowed with the topology ''O''(''X'') and ''O''(''Y'') of [[open set]]s on ''X'' and ''Y''. Note that ''O''(''X'') and ''O''(''Y'') are subframes of ''P''(''X'') and ''P''(''Y''). If ''ƒ'' is a continuous function, then\n: <math>f^{-1}\\colon O(Y)\\to O(X)</math>\npreserves finite meets and arbitrary joins of these subframes.  This shows that ''O'' is a [[functor]] from the category '''Top''' of topological spaces to the category '''Loc''' of locales, taking any continuous map\n: <math>f\\colon X\\to Y</math>\nto the map\n: <math>O(f)\\colon O(X)\\to O(Y)</math>\nin '''Loc''' that is defined in '''Frm''' to be the inverse image frame homomorphism\n: <math>f^{-1}\\colon O(Y)\\to O(X).</math>\nIt is common, given a map of locales\n: <math>f\\colon A\\to B</math>\nin '''Loc''', to write\n: <math>f^{*}\\colon B\\to A</math>\nfor the frame homomorphism that defines it in '''Frm'''. Hence, using this notation, ''O''(''ƒ'') is defined by the equation {{nowrap|1=''O''(''&fnof;'')<sup>*</sup> = ''&fnof;''<sup>&minus;1</sup>.}}\n\nConversely, any locale ''A'' has a topological space ''S''(''A'') that best approximates the locale, called its ''spectrum''. In addition, any map of locales\n: <math>f\\colon A\\to B</math>\ndetermines a continuous map\n: <math>S(A)\\to S(B),</math>\nand this assignment is functorial: letting ''P''(1) denote the locale that is obtained as the powerset of the terminal set {{nowrap|1=1 = { * },}} the points of ''S''(''A'') are the maps\n: <math>p\\colon P(1)\\to A</math>\nin '''Loc''', i.e., the frame homomorphisms\n: <math>p^*\\colon A\\to P(1).</math>\nFor each {{nowrap|1=''a'' &isin; ''A''}} we define the set {{nowrap|1=''U<sub>a</sub>'' &sube; ''S''(''A'')}} that consists of the points {{nowrap|1=''p'' &isin; ''S''(''A'')}} such that {{nowrap|1=''p''*(''a'') = { * }.}} It is easy to verify that this defines a frame homomorphism {{nowrap|1=''A'' &rarr; ''P''(''S''(''A'')),}} whose image is therefore a topology on ''S''(''A''). Then, if\n: <math>f\\colon A\\to B</math> is a map of locales,\nto each point {{nowrap|1=''p'' &isin; ''S''(''A'')}} we assign the point ''S''(''ƒ'')(''q'') defined by letting ''S''(''ƒ'')(p)* be the composition of ''p''* with ''ƒ''*, hence obtaining a continuous map\n: <math>S(f)\\colon S(A)\\to S(B).</math>\n\nThis defines a functor <math>S</math> from '''Loc''' to '''Top''', which is right adjoint to ''O''.\n\nAny locale that is isomorphic to the topology of its spectrum is called ''spatial'', and any topological space that is homeomorphic to the spectrum of its locale of open sets is called ''[[sober space|sober]]''. The adjunction between topological spaces and locales restricts to an [[equivalence of categories]] between sober spaces and spatial locales.\n\nAny function that preserves all joins (and hence any frame homomorphism) has a right adjoint, and, conversely, any function that preserves all meets has a left adjoint. Hence, the category '''Loc''' is isomorphic to the category whose objects are the frames and whose morphisms are the meet preserving functions whose left adjoints preserve finite meets. This is often regarded as a representation of '''Loc''', but it should not be confused with '''Loc''' itself, whose morphisms are formally the same as frame homomorphisms in the opposite direction.\n\n== Literature ==\n\n* [[P. T. Johnstone]], ''Stone Spaces'', Cambridge Studies in Advanced Mathematics 3, [[Cambridge University Press]], Cambridge, 1982. ({{ISBN|0-521-23893-5}})\n\n: ''Still a great resource on locales and complete Heyting algebras.''\n\n* G. Gierz, K. H. Hofmann, K. Keimel, J. D. Lawson, M. Mislove, and [[D. S. Scott]], ''Continuous Lattices and Domains'', In ''Encyclopedia of Mathematics and its Applications'', Vol. 93, Cambridge University Press, 2003. {{ISBN|0-521-80338-1}}\n\n: ''Includes the characterization in terms of meet continuity.''\n\n* Francis Borceux: ''Handbook of Categorical Algebra III'', volume 52 of ''Encyclopedia of Mathematics and its Applications''. Cambridge University Press, 1994.\n\n: ''Surprisingly extensive resource on locales and Heyting algebras. Takes a more categorical viewpoint.''\n\n* [[Steven Vickers]], ''Topology via logic'', Cambridge University Press, 1989, {{ISBN|0-521-36062-5}}.\n* {{cite book | zbl=1034.18001 | editor1-last=Pedicchio | editor1-first=Maria Cristina | editor2-last=Tholen | editor2-first=Walter | title=Categorical foundations. Special topics in order, topology, algebra, and sheaf theory | series=Encyclopedia of Mathematics and Its Applications | volume=97 | location=Cambridge | publisher=[[Cambridge University Press]] | year=2004 | isbn=0-521-83414-7 }}\n\n== External links ==\n* {{nlab|id=locale|title=Locale}}\n\n[[Category:Order theory]]\n[[Category:Algebraic structures]]"
    },
    {
      "title": "Completely regular semigroup",
      "url": "https://en.wikipedia.org/wiki/Completely_regular_semigroup",
      "text": "{{Use dmy dates|date=May 2013}}\nIn [[mathematics]], a '''completely regular semigroup''' is a [[semigroup]] in which every element is in some [[subgroup]] of the semigroup. The [[Class (set theory)|class]] of completely regular semigroups forms an important subclass of the [[Class (set theory)|class]] of [[regular semigroup]]s, the class of [[inverse semigroup]]s being another such subclass. [[Alfred H. Clifford]] was the first to publish  a major paper on completely regular semigroups though he used the terminology \"semigroups admitting relative inverses\" to refer to such semigroups.<ref>{{cite journal|doi=10.2307/1968781|author1=Clifford, A. H|date=1941|title=Semigroups admitting relative inverses|journal=Annals of Mathematics|publisher=American Mathematical Society|volume=42|issue=4|pages=1037–1049|jstor=1968781}}</ref> The name \"completely regular semigroup\" stems from Lyapin's book on semigroups.<ref>{{cite book|last=E S Lyapin|title=Semigroups|publisher=American Mathematical Society|date=1963}}</ref><ref>{{cite book|last=Mario Petrich|author2=Norman R Reilly|title=Completely regular semigroups  |publisher= Wiley-IEEE|date=1999|page=1|isbn=0-471-19571-5}}</ref> In the Russian literature, completely regular semigroups are often called \"Clifford semigroups\".<ref>{{cite book|last=Mario Petrich|author2=Norman R Reilly|title=Completely regular semigroups  |publisher= Wiley-IEEE|date=1999|page=63|isbn=0-471-19571-5}}</ref>\nIn the English literature, the name \"[[Clifford semigroup]]\" is used synonymously to \"inverse Clifford semigroup\", and refers to a completely regular [[inverse semigroup]].<ref>{{cite book|last=Mario Petrich|author2=Norman R Reilly|title=Completely regular semigroups  |publisher= Wiley-IEEE|date=1999|page=65|isbn=0-471-19571-5}}</ref>\nIn a completely regular semigroup, each [[Green's relations|Green]] ''H''-class is a [[group (mathematics)|group]] and the semigroup is the [[union (set theory)|union]] of these groups.<ref>{{cite book|last=John M Howie|title=Fundamentals of semigroup theory|publisher=Oxford University Press|date=1995|series=Oxford Science Publications|isbn=0-19-851194-9}} (Chap. 4)</ref> Hence completely regular semigroups are also referred to as \"unions of groups\". [[Epigroup]]s generalize this notion and their class includes all completely regular semigroups.\n\n==Examples==\n\"While there is an abundance of natural examples of inverse semigroups, for completely regular semigroups the examples (beyond completely simple semigroups) are mostly artificially constructed: the minimum ideal of a\nfinite semigroup is completely simple, and the various relatively free completely regular semigroups are the other more or less natural examples.\"<ref>{{Zbl|0967.20034}} (accessed on 5 May 2009)</ref>\n==See also==\n*[[Special classes of semigroups]]\n\n==References==\n\n{{reflist}}\n\n\n[[Category:Algebraic structures]]\n[[Category:Semigroup theory]]"
    }
  ]
}