{
  "pages": [
    {
      "title": "Trinomial",
      "url": "https://en.wikipedia.org/wiki/Trinomial",
      "text": "{{About|mathematics|the use in taxonomy|Trinomial name|the use identifying archaeological sites in the United States|Smithsonian trinomial}}\nIn [[elementary algebra]], a '''trinomial''' is a [[polynomial]] consisting of three terms or [[monomial]]s.<ref>{{cite web |url=https://www.mathsisfun.com/definitions/trinomial.html | title=Definition of Trinomial | work=Math Is Fun | accessdate=16 April 2016}}</ref>\n\n==Trinomial expressions==\n# <math>3x + 5y + 8z</math> with <math>x, y, z</math> variables\n# <math>3t + 9s^2 + 3y^3</math> with <math>t, s, y</math> variables\n# <math>3ts + 9t + 5s</math> with <math>t, s</math> variables \n# <math>A x^a y^b z^c + B t + C s</math> with <math>x, y, z, t, s</math>  variables, <math>a, b, c</math> nonnegative integers and <math>A, B, C</math> any constants.  \n# <math>Px^a + Qx^b + Rx^c</math> where <math>x</math> is variable and constants <math>a, b, c</math> are nonnegative integers and <math>P, Q, R</math> any constants.\n\n==Trinomial equation==\nA trinomial equation is a polynomial equation involving three terms. An example is the equation <math>x = q + x^m</math> studied by [[Johann Heinrich Lambert]] in the 18th century.<ref>{{cite journal |first=R. M. |last=Corless |first2=G. H. |last2=Gonnet |first3=D. E. G. |last3=Hare |first4=D. J. |last4=Jerey |first5=D. E. |last5=Knuth |year=1996 |url=http://www.cs.uwaterloo.ca/research/tr/1993/03/W.pdf |title=On the Lambert ''W'' Function |journal=Advances in Computational Mathematics |volume=5 |issue=1 |pages=329–359 |doi=10.1007/BF02124750 }}</ref>\n\n==See also==\n*[[Trinomial expansion]]\n*[[Monomial]]\n*[[Binomial (polynomial)|Binomial]]\n*[[Multinomial (disambiguation)|Multinomial]]\n*[[Simple expression]]\n*[[Compound expression]]\n\n==References==\n{{reflist}}\n\n{{polynomials}}\n{{algebra-stub}}\n\n[[Category:Elementary algebra]]\n[[Category:Polynomials]]"
    },
    {
      "title": "Tschirnhaus transformation",
      "url": "https://en.wikipedia.org/wiki/Tschirnhaus_transformation",
      "text": "{{no footnotes|date=January 2019}}\nIn [[mathematics]], a '''Tschirnhaus transformation''', also known as '''Tschirnhausen transformation''', is a type of mapping on [[polynomial]]s developed by [[Ehrenfried Walther von Tschirnhaus]] in 1683. It may be defined conveniently by means of [[field theory (mathematics)|field theory]], as the transformation on [[Minimal polynomial (field theory)|minimal polynomials]] implied by a different choice of [[primitive element (field theory)|primitive element]]. This is the most general transformation of an [[irreducible polynomial]] that takes a root to some [[rational function]] applied to that root.\n\nIn detail, let ''K'' be a field, and ''P''(''t'') a polynomial over ''K''. If ''P'' is irreducible, then the [[quotient ring]] of the [[polynomial ring]] ''K''[''t''] by the [[principal ideal]] generated by ''P'',\n\n:''K''[''t'']/(''P''(''t'')) = ''L'',\n\nis a [[field extension]] of ''K''. We have\n\n:''L'' = ''K''(&alpha;)\n\nwhere α is ''t'' modulo (''P''). That is, any element of ''L'' is a polynomial in α, which is thus a primitive element of ''L''. There will be other choices β of primitive element in ''L'': for any such choice of β we will have by definition:\n\n:&beta; = ''F''(&alpha;), &alpha; = ''G''(&beta;),\n\nwith polynomials ''F'' and ''G'' over ''K''. Now if ''Q'' is the minimal polynomial for β over ''K'', we can call ''Q'' a '''Tschirnhaus transformation''' of ''P''.\n\nTherefore the set of all Tschirnhaus transformations of an irreducible polynomial is to be described as running over all ways of changing ''P'', but leaving ''L'' the same. This concept is used in reducing quintics to [[Bring_radical#Bring–Jerrard_normal_form|Bring&ndash;Jerrard form]], for example. There is a connection with [[Galois theory]], when ''L'' is a [[Galois extension]] of ''K''. The [[Galois group]] may then be considered as all the Tschirnhaus transformations of ''P'' to itself.\n\n==See also==\n*[[Polynomial transformations]]\n\n==References==\n\n* {{MathWorld|urlname= TschirnhausenTransformation|title=Tschirnhausen Transformation}}\n* Tschirnhaus's 1683 paper [https://web.archive.org/web/20090226035640/http://www.sigsam.org/bulletin/articles/143/tschirnhaus.pdf \"A method for removing all intermediate terms from a given equation\"], translation by RF Green.\n\n[[Category:Polynomials]]\n[[Category:Field theory]]"
    },
    {
      "title": "Unimodular polynomial matrix",
      "url": "https://en.wikipedia.org/wiki/Unimodular_polynomial_matrix",
      "text": "In [[mathematics]], a '''unimodular polynomial matrix''' is a [[Square_matrix#Square_matrices_and_related_definitions|square]] [[polynomial matrix]] whose [[inverse matrix|inverse]] exists and is itself a polynomial matrix.  Equivalently, a polynomial matrix ''A'' is unimodular if its [[determinant]] det(''A'') is a nonzero [[Constant polynomial|constant]].\n\n== References ==\n* {{Citation | last1=Antsaklis\n | first1=Panos J.\n | last2=Michel\n | first2=Anthony N.\n | title=A Linear Systems Primer\n | year=2006\n | isbn=978-0-8176-4460-4\n | page=273 \n}}\n* {{Citation | last1=Horn | first1=Roger A. | last2=Johnson | first2=Charles R. | title=Matrix Analysis | publisher=[[Cambridge University Press]] | isbn=978-0-521-38632-6 | year=1985 | page=14}}.\n\n== External links ==\n*[http://www.polyx.com/glossary.htm] Polynomial matrix glossary at Polyx (A matlab toolbox)\n\n\n[[Category:Matrices]]\n[[Category:Polynomials]]\n\n\n{{Linear-algebra-stub}}"
    },
    {
      "title": "Vieta's formulas",
      "url": "https://en.wikipedia.org/wiki/Vieta%27s_formulas",
      "text": "{{short description|Relations between the coefficients and the roots of a polynomial}}\n{{For|a method for computing {{pi}}|Viète's formula}}\nIn [[mathematics]], '''Vieta's formulas''' are [[formula]]s that relate the [[coefficient]]s of a [[polynomial]] to sums and products of its [[Root of a function|roots]]. Named after [[François Viète]] (more commonly referred to by the Latinised form of his name, '''Franciscus Vieta'''), the formulas are used specifically in [[algebra]].\n\n==Basic formulas==\nAny general polynomial of degree ''n''\n:<math>P(x)=a_nx^n  + a_{n-1}x^{n-1} +\\cdots + a_1 x+ a_0 </math>\n\n(with the coefficients being real or complex numbers and ''a''<sub>''n''</sub>&nbsp;≠&nbsp;0) is known by the [[fundamental theorem of algebra]] to have ''n'' (not necessarily distinct) complex roots ''r''<sub>1</sub>,&nbsp;''r''<sub>2</sub>,&nbsp;...,&nbsp;''r''<sub>''n''</sub>. Vieta's formulas relate the polynomial's coefficients {&nbsp;''a''<sub>''k''</sub>&nbsp;} to signed sums of products of its roots {&nbsp;''r''<sub>''i''</sub>&nbsp;} as follows:\n\n:<math>\\begin{cases} r_1 + r_2 + \\dots + r_{n-1} + r_n = -\\dfrac{a_{n-1}}{a_{n}} \\\\ \n(r_1 r_2 + r_1 r_3+\\cdots + r_1 r_n) + (r_2r_3 + r_2r_4+\\cdots + r_2r_n)+\\cdots + r_{n-1}r_n = \\dfrac{a_{n-2}}{a_{n}} \\\\\n{} \\quad \\vdots \\\\ r_1 r_2 \\dots r_n = (-1)^n \\dfrac{a_0}{a_n}. \\end{cases}</math>\n\nEquivalently stated, the (''n''&nbsp;&minus;&nbsp;''k'')th coefficient ''a''<sub>''n''&minus;''k''</sub> is related to a signed sum of all possible products of ''k'' roots:\n\n: <math>\\sum_{1\\le i_1 < i_2 < \\cdots < i_k\\le n} r_{i_1}r_{i_2}\\cdots r_{i_k}=(-1)^k\\frac{a_{n-k}}{a_n}</math>\n\nfor ''k''&nbsp;=&nbsp;1,&nbsp;2,&nbsp;...,&nbsp;''n'' (the indices ''i''<sub>''k''</sub> are sorted in increasing order to ensure each product of ''k'' roots is used exactly once.\n\nThe left-hand sides of Vieta's formulas are the [[elementary symmetric polynomial|elementary symmetric function]]s of the roots.\n\n==Generalization to rings==\nVieta's formulas are frequently used with polynomials with coefficients in any [[integral domain]] ''R''. Then, the quotients <math>a_i/a_n</math> belong to the [[ring of fractions]] of ''R'' (and possibly are in ''R'' itself if <math>a_n</math> happens to be invertible in ''R'') and the roots <math>r_i</math> are taken in an [[algebraically closed field|algebraically closed extension]]. Typically, ''R'' is the ring of the [[integer]]s, the field of fractions is the field of the [[rational number]]s and the algebraically closed field is the field of the [[complex numbers]].\n\nVieta's formulas are then useful because they provide relations between the roots without having to compute them.\n\nFor polynomials over a commutative ring which is not an integral domain, Vieta's formulas are only valid when <math>a_n</math> is a non zero-divisor and <math>P(x)</math> factors as <math>a_n(x-r_1)(x-r_2)\\dots(x-r_n)</math>.  For example, in the ring of the integers [[Modular arithmetic|modulo]] 8, the polynomial <math>P(x)=x^2-1</math> has four roots: 1, 3, 5, and 7. Vieta's formulas are not true if, say, <math>r_1=1</math> and <math>r_2=3</math>, because <math>P(x)\\neq (x-1)(x-3)</math>.  However, <math>P(x)</math> does factor as <math> (x-1)(x-7)</math> and as <math>(x-3)(x-5)</math>, and Vieta's formulas hold if we set either <math>r_1=1</math> and <math>r_2=7</math> or <math>r_1=3</math> and <math>r_2=5</math>.\n\n==Example==\nVieta's formulas applied to quadratic and cubic polynomial:\n\nThe roots <math>r_1, r_2</math> of the [[quadratic polynomial]] <math>P(x) = ax^2 + bx + c</math> satisfy\n:<math> r_1 + r_2 = -\\frac{b}{a}, \\quad r_1 r_2 = \\frac{c}{a}.</math>\n\nThe first of these equations can be used to find the minimum (or maximum) of {{math|''P''}}; see {{slink|Quadratic equation|Vieta's formulas}}.\n\nThe roots <math>r_1, r_2, r_3</math> of the [[cubic polynomial]] <math>P(x) = ax^3 + bx^2 + cx + d</math> satisfy\n:<math> r_1 + r_2 + r_3 = -\\frac{b}{a}, \\quad r_1 r_2 + r_1 r_3 + r_2 r_3 = \\frac{c}{a}, \\quad r_1 r_2 r_3 = -\\frac{d}{a}.</math>\n\n==Proof==\nVieta's formulas can be proved by expanding the equality\n\n: <math>a_nx^n  + a_{n-1}x^{n-1} +\\cdots + a_1 x+ a_0 = a_n(x-r_1)(x-r_2)\\cdots (x-r_n)</math>\n\n(which is true since <math>r_1, r_2, \\dots, r_n</math> are all the roots of this  polynomial), multiplying the factors on the right-hand side, and identifying the coefficients of each power of <math>x.</math>\n\nFormally, if one expands <math>(x-r_1)(x-r_2)\\cdots(x-r_n),</math> the terms are precisely <math>(-1)^{n-k}r_1^{b_1}\\cdots r_n^{b_n} x^k,</math> where <math>b_i</math> is either 0 or 1, accordingly as whether <math>r_i</math> is included in the product or not, and ''k'' is the number of <math>r_i</math> that are excluded, so the total number of factors in the product is ''n'' (counting ''<math>x^k</math>'' with multiplicity ''k'') – as there are ''n'' binary choices (include <math>r_i</math> or ''x''), there are <math>2^n</math> terms – geometrically, these can be understood as the vertices of a hypercube. Grouping these terms by degree yields the elementary symmetric polynomials in <math>r_i</math> – for ''x<sup>k</sup>,'' all distinct ''k''-fold products of <math>r_i.</math>\n\n== History ==\nAs reflected in the name, the formulas were discovered by the 16th century French mathematician [[François Viète]], for the case of positive roots.\n\nIn the opinion of the 18th century British mathematician [[Charles Hutton]], as quoted by Funkhouser,<ref>{{Harv|Funkhouser|1930}}</ref> the general principle (not only for positive real roots) was first understood by the 17th century French mathematician [[Albert Girard]]: \n<blockquote>...[Girard was] the first person who understood the general doctrine of the formation of the coefficients of the powers from the sum of the roots and their products. He was the first who discovered the rules for summing the powers of the roots of any equation.</blockquote>\n\n==See also==\n\n* [[Newton's identities]]\n* [[Elementary symmetric polynomial]]\n* [[Symmetric polynomial]]\n* [[Content (algebra)]]\n* [[Properties of polynomial roots]]\n* [[Gauss–Lucas theorem]]\n* [[Rational root theorem]]\n* [[Vieta jumping]]\n\n== References ==\n{{reflist}}\n* {{springer|title=Viète theorem|id=p/v096630}}\n* {{Citation| first= H. Gray | last=Funkhouser | authorlink = Howard G. Funkhouser | title=A short account of the history of symmetric functions of roots of equations | journal=American Mathematical Monthly | year=1930 | volume= 37 | issue=7 | pages=357–365 | doi=10.2307/2299273| jstor= 2299273| publisher= Mathematical Association of America }}\n*{{Citation\n | last       = Vinberg\n | first      = E. B.\n | authorlink= Ernest Vinberg\n | title      = A course in algebra\n | publisher  = American Mathematical Society, Providence, R.I\n | year       = 2003\n | pages      = \n | isbn       = 0-8218-3413-4\n}}\n\n*{{Citation\n | last       = Djukić\n | first      = Dušan| title      = The IMO compendium: a collection of problems suggested for the International Mathematical Olympiads, 1959–2004\n | publisher  = Springer, New York, NY\n | year       = 2006\n | pages      = \n | isbn       = 0-387-24299-6\n|display-authors=etal}}\n\n{{DEFAULTSORT:Viete's Formulas}}\n[[Category:Articles containing proofs]]\n[[Category:Polynomials]]\n[[Category:Elementary algebra]]"
    },
    {
      "title": "Wall polynomial",
      "url": "https://en.wikipedia.org/wiki/Wall_polynomial",
      "text": "{{for|the unrelated family of orthogonal polynomials sometimes called Wall polynomials|little q-Laguerre polynomials}}\n\nIn mathematics, a '''Wall polynomial''' is a polynomial studied by {{harvtxt|Wall|1963}} in his work on [[conjugacy class]]es in [[classical group]]s, and named by {{harvtxt|Andrews|1984}}.\n\n==References==\n\n*{{Citation | last1=Andrews | first1=George E. | title=On the Wall polynomials and the L-M-W conjectures | url=http://journals.cambridge.org/production/action/cjoGetFulltext?fulltextid=4905516 | doi=10.1017/S1446788700021716 | mr=742241 | year=1984 | journal=Australian Mathematical Society. Journal. Series A. Pure Mathematics and Statistics | issn=0263-6115 | volume=37 | issue=1 | pages=17–26}}\n*{{Citation | last1=Wall | first1=G. E. | title=On the conjugacy classes in the unitary, symplectic and orthogonal groups | doi=10.1017/S1446788700027622 | mr=0150210 | year=1963 | journal=Australian Mathematical Society. Journal. Series A. Pure Mathematics and Statistics | issn=0263-6115 | volume=3 | pages=1–62}}\n\n[[Category:Polynomials]]"
    },
    {
      "title": "Wu's method of characteristic set",
      "url": "https://en.wikipedia.org/wiki/Wu%27s_method_of_characteristic_set",
      "text": "{{More footnotes|date=November 2012}}\n'''[[Wu Wenjun|Wenjun Wu]]'s method''' is an algorithm for solving [[systems of polynomial equations|multivariate polynomial equations]] introduced in the late 1970s by the Chinese mathematician [[Wu Wenjun|Wen-Tsun Wu]]. This method is based on the mathematical concept of '''characteristic set''' introduced in the late 1940s by [[Joseph Ritt|J.F. Ritt]]. It is fully independent of the [[Gröbner basis]] method, introduced by [[Bruno Buchberger]] (1965), even if Gröbner bases may be used to compute characteristic sets.<ref>{{cite book|editor-last=Corrochano|editor-first=Eduardo Bayro|editor2-first=Garret |editor2-last=Sobczyk|title=Geometric algebra with applications in science and engineering|page=110 |year=2001|publisher=Birkhäuser|location=Boston, Mass|isbn=9780817641993}}</ref><ref>P. Aubry, D. Lazard, M. Moreno Maza (1999). [http://www.sciencedirect.com/science/article/pii/S0747717199902699 On the theories of triangular sets]. Journal of Symbolic Computation, 28(1–2):105–124</ref>\n\nWu's method is powerful for [[automatic theorem proving|mechanical theorem proving]] in [[elementary geometry]], and provides a complete decision process for certain classes of problem. It has been used in research in his laboratory (KLMM, Key Laboratory of Mathematics Mechanization in Chinese Academy of Science) and around the world.  The main trends of research on Wu's method concern [[systems of polynomial equations]] of positive dimension and [[differential algebra]] where [[Joseph Ritt|Ritt]]'s results have been made effective.<ref>Hubert, E. ''Factorisation Free Decomposition Algorithms in Differential Algebra.'' Journal of Symbolic Computation, (May 2000): 641–662.</ref><ref>[[Maple (software)]] package '''diffalg'''.</ref> Wu's method has been applied in various scientific fields, like biology, computer vision, robot kinematics and especially automatic proofs in geometry.<ref>Chou, Shang-Ching; Gao, Xiao Shan; Zhang, Jing Zhong. ''Machine proofs in geometry''. World Scientific, 1994.</ref>\n\n==Informal description==\n'''Wu's method''' uses [[polynomial]] division to solve problems of the form:\n\n: <math> \\forall x, y, z, \\dots I(x, y, z, \\dots) \\implies f(x, y, z, \\dots) \\, </math>\n\nwhere ''f'' is a [[polynomial equation]] and ''I'' is a [[Logical conjunction|conjunction]] of [[polynomial equation]]s. The algorithm is complete for such problems over the [[complex domain]].\n\nThe core idea of the algorithm is that you can divide one polynomial by another to give a remainder. Repeated division results in either the remainder vanishing (in which case the ''I'' implies ''f'' statement is true), or an irreducible remainder is left behind (in which case the statement is false).\n\nMore specifically, for an [[ideal (ring theory)|ideal]] ''I'' in the [[ring (mathematics)|ring]] ''k''[''x''<sub>1</sub>,&nbsp;...,&nbsp;''x''<sub>''n''</sub>] over a field ''k'', a (Ritt) characteristic set ''C'' of ''I'' is composed of a set of polynomials in ''I'', which is in triangular shape: polynomials in ''C'' have distinct main variables (see the formal definition below). Given a characteristic set ''C'' of ''I'', one can decide if a polynomial ''f'' is zero modulo ''I''. That is, the membership test is checkable for ''I'', provided a characteristic set of ''I''.\n\n==Ritt characteristic set==\n\nA Ritt characteristic set is a finite set of polynomials in triangular form of an ideal. This triangular set satisfies\ncertain minimal condition with respect to the Ritt ordering, and it preserves many interesting geometrical properties\nof the ideal. However it may not be its system of generators.\n\n===Notation===\n\nLet R be the multivariate polynomial ring ''k''[''x''<sub>1</sub>, ..., ''x''<sub>''n''</sub>] over a field ''k''.\nThe variables are ordered linearly according to their subscript: ''x''<sub>1</sub> < ... < ''x''<sub>''n''</sub>.\nFor a non-constant polynomial ''p'' in R, the greatest variable effectively presenting in ''p'', called '''main variable''' or '''class''', plays a particular role:\n''p'' can be naturally regarded as a univariate polynomial in its main variable ''x''<sub>''k''</sub> with coefficients in ''k''[''x''<sub>1</sub>, ..., ''x''<sub>''k''−1</sub>].\nThe degree of p as a univariate polynomial in its main variable is also called its main degree.\n\n===Triangular set===\n\nA set ''T'' of non-constant polynomials is called a '''triangular set''' if all polynomials in ''T'' have distinct main variables.  This generalizes triangular [[systems of linear equations]] in a natural way.\n\n===Ritt ordering===\n\nFor two non-constant polynomials ''p'' and ''q'', we say ''p'' is smaller than ''q'' with respect to '''Ritt ordering''' and written as ''p''&nbsp;<<sub>''r''</sub>&nbsp;''q'', if one of the following assertions holds:\n:(1)  the main variable of ''p'' is smaller than the main variable of ''q'', that is, mvar(''p'')&nbsp;<&nbsp;mvar(''q''),\n:(2) ''p'' and ''q'' have the same main variable, and the main degree of ''p'' is less than the '''main degree''' of&nbsp;''q'', that is, mvar(''p'')&nbsp;=&nbsp;mvar(''q'') and mdeg(''p'')&nbsp;<&nbsp;mdeg(''q'').\n\nIn this way, (''k''[''x''<sub>1</sub>,&nbsp;...,&nbsp;''x''<sub>''n''</sub>],<<sub>''r''</sub>) forms a [[well partial order]]. However, the Ritt ordering is not a [[total order]]:\nthere exist polynomials p and q such that neither ''p''&nbsp;<<sub>''r''</sub>&nbsp;''q'' nor ''p''&nbsp;&gt;<sub>''r''</sub>&nbsp;''q''. In this case, we say that ''p'' and ''q'' are not comparable.\nNote that the Ritt ordering is comparing the '''rank''' of ''p'' and ''q''. The rank, denoted by rank(''p''), of a non-constant polynomial ''p'' is defined to be a power of\nits main variable: mvar(''p'')<sup>mdeg(''p'')</sup> and ranks are compared by comparing first the variables and then, in case of equality of the variables, the degrees.\n\n===Ritt ordering on triangular sets===\n\nA crucial generalization on Ritt ordering is to compare triangular sets.\nLet ''T''&nbsp;=&nbsp;{&nbsp;''t''<sub>1</sub>,&nbsp;...,&nbsp;''t''<sub>''u''</sub>} and ''S''&nbsp;=&nbsp;{&nbsp;''s''<sub>1</sub>,&nbsp;...,&nbsp;''s''<sub>''v''</sub>} be two triangular sets\nsuch that polynomials in ''T'' and ''S'' are sorted increasingly according to their main variables.\nWe say ''T'' is smaller than S w.r.t. Ritt ordering if one of the following assertions holds\n\n:(1) there exists ''k''&nbsp;≤&nbsp;min(''u'',&nbsp;''v'') such that rank(''t''<sub>''i''</sub>)&nbsp;=&nbsp;rank(''s''<sub>''i''</sub>) for 1&nbsp;≤&nbsp;''i''&nbsp;<&nbsp;''k'' and ''t''<sub>''k''</sub>&nbsp;<<sub>''r''</sub>&nbsp;''s''<sub>''k''</sub>,\n:(2) ''u''&nbsp;>&nbsp;''v'' and rank(''t''<sub>''i''</sub>)&nbsp;=&nbsp;rank(''s''<sub>''i''</sub>) for 1&nbsp;≤&nbsp;''i''&nbsp;≤&nbsp;''v''.\n\nAlso, there exists incomparable triangular sets w.r.t Ritt ordering.\n\n===Ritt characteristic set===\n\nLet I be a non-zero ideal of k[x<sub>1</sub>, ..., x<sub>n</sub>]. A subset T of I is a '''Ritt characteristic set''' of I if one of the following conditions holds:\n:(1) T consists of a single nonzero constant of k,\n:(2) T is a triangular set and T is minimal w.r.t Ritt ordering in the set of all triangular sets contained in I.\n\nA polynomial ideal may possess (infinitely) many characteristic sets, since Ritt ordering is a partial order.\n\n==Wu characteristic set==\n\nThe Ritt–Wu process, first devised by Ritt, subsequently modified by Wu, computes not a Ritt characteristic but an extended one, called Wu characteristic set or ascending chain.\n\nA non-empty subset T of the ideal <F> generated by F is a '''Wu characteristic set''' of F if one of the following condition holds\n\n:(1) T = {a} with a being a nonzero constant,\n:(2) T is a triangular set and there exists a subset G of <F> such that <F> = <G> and every polynomial in G is [[regular chain|pseudo-reduced]] to zero with respect to T.\n\nNote that Wu characteristic set is defined to the set F of polynomials, rather to the ideal <F> generated by F. Also it can be shown that a Ritt characteristic set T of <F> is a Wu characteristic set of F. Wu characteristic sets can be computed by Wu's algorithm CHRST-REM, which only requires pseudo-remainder computations and no factorizations are needed.\n\nWu's characteristic set method has exponential complexity; improvements in computing efficiency by weak chains, [[regular chain]]s, saturated chain were introduced<ref>Chou S C, Gao X S; Ritt–Wu's decomposition algorithm and geometry theorem proving. Proc of CADE, 10 LNCS, #449, Berlin, Springer Verlag, 1990 207–220.</ref>\n\n==Decomposing algebraic varieties==\n\nAn application is an algorithm for solving systems of algebraic equations by means of characteristic sets. More precisely, given a finite subset F of polynomials, there is an algorithm to compute characteristic sets T<sub>1</sub>, ...,\nT<sub>e</sub> such that:\n\n:<math>V(F) = W(T_1)\\cup \\cdots \\cup W(T_e), \\, </math>\n\nwhere W(T<sub>i</sub>) is the difference of V(T<sub>i</sub>) and V(h<sub>i</sub>), here h<sub>i</sub> is the product of initials of the polynomials in T<sub>i</sub>.\n\n==See also==\n*[[Regular chain]]\n*[[Mathematics-Mechanization Platform]]\n\n==References==\n<references/>\n\n*P. Aubry, M. Moreno Maza (1999) Triangular Sets for Solving Polynomial Systems: a Comparative Implementation of Four Methods. J. Symb. Comput. 28(1–2): 125–154\n*David A. Cox, John B. Little, Donal O'Shea.  Ideals, Varieties, and Algorithms.  2007.\n*{{cite web|last=Hua-Shan|first=Liu |title=WuRittSolva: Implementation of Wu-Ritt Characteristic Set Method |url=http://library.wolfram.com/infocenter/MathSource/5716/ |work=Wolfram Library Archive |publisher=Wolfram |date=24 August 2005 |accessdate=17 November 2012}}\n*{{cite book|last=Heck|first=André|title=Introduction to Maple|year=2003|publisher=Springer|location=New York|isbn=9780387002309|pages=105, 508|edition=3.}}\n*Ritt, J. (1966). Differential Algebra. New York, Dover Publications.\n*Dongming Wang (1998). Elimination Methods. Springer-Verlag, Wien, Springer-Verlag\n*Dongming Wang (2004). Elimination Practice, Imperial College Press, London {{isbn|1-86094-438-8}}\n*Wu, W. T. (1984). Basic principles of mechanical theorem proving in elementary geometries. J. Syst. Sci. Math. Sci., 4, 207–35\n*Wu, W. T. (1987). A zero structure theorem for polynomial equations solving. MM Research Preprints, 1, 2–12\n*{{cite journal|last=Xiaoshan|first=Gao|author2=Chunming, Yuan |author3=Guilin, Zhang |title=Ritt-Wu's characteristic set method for ordinary difference polynomial systems with arbitrary ordering|journal=Acta Mathematica Scientia|year=2009|volume=29|issue=4|pages=1063–1080|doi=10.1016/S0252-9602(09)60086-2|citeseerx=10.1.1.556.9549}}\n\n==External links==\n*[http://www.mmrc.iss.ac.cn/~dwang/wsolve.htm  wsolve Maple package]\n*[http://www.mmrc.iss.ac.cn/~lzhi/Research/wuritt.html The Characteristic Set Method]\n\n[[Category:Computer algebra]]\n[[Category:Algebraic geometry]]\n[[Category:Commutative algebra]]\n[[Category:Polynomials]]"
    },
    {
      "title": "Cubic form",
      "url": "https://en.wikipedia.org/wiki/Cubic_form",
      "text": "In [[mathematics]], a '''cubic form''' is a [[homogeneous polynomial]] of degree 3, and a '''cubic hypersurface''' is the [[zero set]] of a cubic form.  In the case of a cubic form in three variables, the zero set is a [[cubic plane curve]].\n\nIn {{harv|Delone|Faddeev|1964}}, [[Boris Delone]] and [[Dmitry Faddeev]] showed that binary cubic forms with integer coefficients can be used to parametrize [[order (ring theory)|orders]] in [[cubic field]]s. Their work was generalized in {{harv|Gan|Gross|Savin|2002|loc=§4}} to include all cubic rings,<ref>A '''cubic ring''' is a [[ring (mathematics)|ring]] that is isomorphic to '''Z'''<sup>3</sup> as a [[Module (mathematics)|'''Z'''-module]].</ref><ref>In fact, [[Pierre Deligne]] pointed out that the correspondence works over an arbitrary [[Scheme (mathematics)|scheme]].</ref> giving a [[discriminant]]-preserving [[bijection]] between [[Orbit (group theory)|orbits]] of a GL(2,&nbsp;'''Z''')-[[Group action (mathematics)|action]] on the space of integral binary cubic forms and cubic rings up to [[isomorphism]].\n\nThe classification of real cubic forms <math>a x^3 + 3 b x^2 y + 3 c x y^2 + d y^3</math> is linked to the classification of [[umbilical point]]s of surfaces. The [[equivalence class]]es of such cubics form a three-dimensional [[real projective space]] and the subset of [[parabolic form]]s define a surface – the [[umbilic torus]].<ref name=port>{{Citation|first=Ian R.|last=Porteous|title=Geometric Differentiation, For the Intelligence of Curves and Surfaces|isbn=978-0-521-00264-6|pp=350\n|date=2001|publisher=Cambridge University Press| edition=2nd}}</ref>\n\n==Examples==\n*[[Cubic plane curve]]\n*[[Elliptic curve]]\n*[[Fermat cubic]]\n*[[Cubic 3-fold]]\n*[[Koras–Russell cubic threefold]]\n*[[Klein cubic threefold]]\n*[[Segre cubic]]\n\n==Notes==\n{{reflist}}\n\n==References==\n*{{Citation\n| last=Delone\n| first=Boris\n| author-link=Boris Delone\n| last2=Faddeev\n| first2=Dmitriĭ\n| title=The theory of irrationalities of the third degree\n| publisher=American Mathematical Society\n| series=Translations of Mathematical Monographs\n| volume=10\n| year=1964\n| origyear=1940, Translated from the Russian by Emma Lehmer and Sue Ann Walker\n| mr=0160744\n}}\n*{{Citation\n| last1=Gan\n| first1=Wee-Teck\n| last2=Gross\n| first2=Benedict\n| author2-link=Benedict Gross\n| last3=Savin\n| first3=Gordan\n| title=Fourier coefficients of modular forms on ''G''<sub>2</sub>\n| year=2002\n| journal=Duke Mathematical Journal\n| volume=115\n| number=1\n| pages=105–169\n| doi=10.1215/S0012-7094-02-11514-2\n| mr=1932327\n| citeseerx=10.1.1.207.3266\n}}\n*{{eom|id=c/c027260|first=V.A.|last= Iskovskikh|first2=V.L.|last2= Popov|author2-link=Vladimir L. Popov|title=Cubic form}}\n*{{eom|id=c/c027270|first=V.A.|last= Iskovskikh|first2=V.L.|last2= Popov|author2-link=Vladimir L. Popov|title=Cubic hypersurface}}\n*{{Citation | last1=Manin | first1=Yuri Ivanovich | author1-link=Yuri Ivanovich Manin | title=Cubic forms | origyear=1972 | url=https://books.google.com/books?id=W03vAAAAMAAJ | publisher=North-Holland | location=Amsterdam | edition=2nd | series=North-Holland Mathematical Library | isbn=978-0-444-87823-6 | mr=833513 | year=1986 | volume=4}}\n\n[[Category:Homogeneous polynomials| ]]\n[[Category:Multilinear algebra]]\n[[Category:Algebraic geometry]]\n[[Category:Algebraic varieties]]\n\n\n{{algebraic-geometry-stub}}"
    },
    {
      "title": "Homogeneous polynomial",
      "url": "https://en.wikipedia.org/wiki/Homogeneous_polynomial",
      "text": "\n{{More footnotes|date=July 2018}}\n\nIn [[mathematics]], a '''homogeneous polynomial''' is a [[polynomial]] whose nonzero terms all have the same [[Degree of a polynomial|degree]].<ref>D. Cox, J. Little, D. O'Shea: ''Using Algebraic Geometry'', 2nd ed., page 2. Springer-Verlag, 2005.</ref> For example, <math>x^5 + 2 x^3 y^2 + 9 x y^4</math> is a homogeneous polynomial of degree 5, in two variables; the sum of the exponents in each term is always 5. The polynomial <math>x^3 + 3 x^2 y + z^7</math> is not homogeneous, because the sum of exponents does not match from term to term. A polynomial is homogeneous if and only if it defines a [[homogeneous function]].  An '''algebraic form''', or simply '''form''', is a [[function (mathematics)|function]] defined by a homogeneous polynomial.<ref>However, as some authors do not make a clear distinction between a polynomial and its associated function, the terms ''homogeneous polynomial'' and ''form'' are sometimes considered as synonymous.</ref> A '''binary form''' is a form in two variables. A ''form'' is also a function defined on a [[vector space]], which may be expressed as a homogeneous function of the coordinates over any [[basis (linear algebra)|basis]].\n\nA polynomial of degree 0 is always homogeneous; it is simply an element of the [[field (mathematics)|field]] or [[ring (mathematics)|ring]] of the coefficients, usually called a constant or a scalar. A form of degree 1 is a linear form.<ref>''Linear forms'' are defined only for finite-dimensional vector space, and have thus to be distinguished from ''[[linear functional]]s'', which are defined for every vector space. \"Linear functional\" is rarely used for finite-dimensional vector spaces.</ref> A form of degree 2 is a [[quadratic form]]. In [[geometry]], the [[Euclidean distance]] is the [[square root]] of a quadratic form.\n\nHomogeneous polynomials are ubiquitous in mathematics and physics.<ref>Homogeneous polynomials in physics often appear as a consequence of [[dimensional analysis]], where measured quantities must match in real-world problems.</ref> They play a fundamental role in algebraic geometry, as a [[projective algebraic variety]] is defined as the set of  the common zeros of a set of homogeneous polynomials.\n\n==Properties==\nA homogeneous polynomial defines a [[homogeneous function]]. This means that, if a [[multivariate polynomial]] ''P'' is homogeneous of degree ''d'', then\n:<math>P(\\lambda x_1, \\ldots, \\lambda x_n)=\\lambda^d\\,P(x_1,\\ldots,x_n)\\,,</math>\nfor every <math>\\lambda</math> in any [[field (mathematics)|field]] containing the [[coefficient]]s of ''P''. Conversely, if the above relation is true for infinitely many  <math>\\lambda</math> then the polynomial is homogeneous of degree ''d''.\n\nIn particular, if ''P'' is homogeneous then \n:<math>P(x_1,\\ldots,x_n)=0 \\quad\\Rightarrow\\quad P(\\lambda x_1, \\ldots, \\lambda x_n)=0,</math>\nfor every <math>\\lambda.</math> This property is fundamental in the definition of a [[projective variety]].\n\nAny nonzero polynomial may be decomposed, in a unique way, as a sum of homogeneous polynomials of different degrees, which are called the '''homogeneous components''' of the polynomial. \n\nGiven a [[polynomial ring]] <math>R=K[x_1, \\ldots,x_n]</math> over a [[field (mathematics)|field]] (or, more generally, a [[ring (mathematics)|ring]]) ''K'', the homogeneous polynomials of degree ''d'' form\na [[vector space]] (or a [[module (mathematics)|module]]), commonly denoted <math>R_d.</math> The above unique decomposition means that <math>R</math> is the [[direct sum]] of the <math>R_d</math> (sum over all [[nonnegative integer]]s).\n\nThe dimension of the vector space (or [[free module]]) <math>R_d</math> is the number of different monomials of degree ''d'' in ''n'' variables (that is the maximal number of nonzero terms in a homogeneous polynomial of degree ''d'' in ''n'' variables). It is equal to the [[binomial coefficient]]\n\n:<math>\\binom{d+n-1}{n-1}=\\binom{d+n-1}{d}=\\frac{(d+n-1)!}{d!(n-1)!}.</math>\n\n{{anchor|Euler's identity for homogeneous polynomials}}\nHomogeneous polynomial satisfy  [[Euler's homogeneous function theorem|Euler's identity for homogeneous functions]]. That is, if {{math|''P''}} is a homogeneous polynomial of degree {{math|''d''}} in the indeterminates <math>x_1, \\ldots, x_n,</math> one has, whichever is the [[commutative ring]] of the coefficients,\n:<math>dP=\\sum_{i=1}^n x_i\\frac{\\partial P}{\\partial x_i},</math>\nwhere <math>\\textstyle \\frac{\\partial P}{\\partial x_i}</math> denotes the [[formal derivative|formal partial derivative]] of {{math|''P''}} with respect to <math>x_i.</math>\n\n==Homogenization==\nA non-homogeneous polynomial ''P''(''x''<sub>''1''</sub>,...,''x''<sub>''n''</sub>) can be homogenized by introducing an additional variable ''x''<sub>0</sub> and defining the homogeneous polynomial sometimes denoted <sup>''h''</sup>''P'':<ref>D. Cox, J. Little, D. O'Shea: ''Using Algebraic Geometry'', 2nd ed., page 35. Springer-Verlag, 2005.</ref>\n:<math>{^h\\!P}(x_0,x_1,\\dots, x_n) = x_0^d P \\left (\\frac{x_1}{x_0},\\dots, \\frac{x_n}{x_0} \\right ),</math>\nwhere ''d'' is the [[Degree of a polynomial|degree]] of ''P''. For example, if\n:<math>P=x_3^3 + x_1 x_2+7,</math>\nthen\n:<math>^h\\!P=x_3^3 + x_0 x_1x_2 + 7 x_0^3.</math>\n\nA homogenized polynomial can be dehomogenized by setting the additional variable ''x''<sub>0</sub> = 1. That is\n:<math>P(x_1,\\dots, x_n)={^h\\!P}(1,x_1,\\dots, x_n).</math>\n\n==See also==\n<!-- the two first items must remain at the beginning, because, they are more related to the subject than the others -->\n*[[Multi-homogeneous polynomial]]\n*[[Quasi-homogeneous polynomial]]\n*[[Diagonal form]]\n*[[Graded algebra]]\n*[[Hilbert series and Hilbert polynomial]]\n*[[Multilinear form]]\n*[[Multilinear map]]\n*[[Polarization of an algebraic form]]\n*[[Schur polynomial]]\n*[[Symbol of a differential operator]]\n\n==References==\n{{reflist}}\n\n==External links==\n* {{mathworld|urlname=HomogeneousPolynomial|title = Homogeneous Polynomial}}\n\n{{DEFAULTSORT:Homogeneous Polynomial}}\n[[Category:Homogeneous polynomials| ]]\n[[Category:Multilinear algebra]]\n[[Category:Algebraic geometry]]"
    },
    {
      "title": "Diagonal form",
      "url": "https://en.wikipedia.org/wiki/Diagonal_form",
      "text": "{{Unreferenced|date=December 2009}}\nIn [[mathematics]], a '''diagonal form''' is an algebraic form ([[homogeneous polynomial]]) without cross-terms involving different [[indeterminate (variable)|indeterminates]]. That is, it is\n\n:<math>\\Sigma a_i {x_i}^m\\ </math>\n\nfor some given degree ''m'', summed for 1 ≤ ''i'' ≤ ''n''.\n\nSuch forms ''F'', and the [[hypersurface]]s ''F'' = 0 they define in [[projective space]], are very special in geometric terms, with many symmetries. They also include famous cases like the [[Fermat curve]]s, and other examples well known in the theory of [[Diophantine equation]]s.\n\nA great deal has been worked out about their theory: [[algebraic geometry]], [[local zeta-function]]s via [[Jacobi sum]]s, [[Hardy-Littlewood circle method]].\n\n==Examples==\n:<math>X^2+Y^2-Z^2 = 0</math> is the [[unit circle]] in ''P''<sup>2</sup>\n:<math>X^2-Y^2-Z^2 = 0</math> is the [[unit hyperbola]] in ''P''<sup>2</sup>.\n:<math>x_0^3+x_1^3+x_2^3+x_3^3=0</math> gives the Fermat [[cubic surface]] in ''P''<sup>3</sup> with 27 lines. The 27 lines in this example are easy to describe explicitly: they are the 9 lines of the form (''x'' : ''ax'' : ''y'' : ''by'') where ''a'' and ''b'' are fixed numbers with cube &minus;1,  and their 18 conjugates under permutations of coordinates.\n\n:<math>x_0^4+x_1^4+x_2^4+x_3^4=0</math> gives a [[K3 surface]] in ''P''<sup>3</sup>.\n\n{{DEFAULTSORT:Diagonal Form}}\n[[Category:Homogeneous polynomials]]\n[[Category:Algebraic varieties]]"
    },
    {
      "title": "Norm form",
      "url": "https://en.wikipedia.org/wiki/Norm_form",
      "text": "In [[mathematics]], a '''norm form''' is a [[homogeneous form]] in ''n'' variables constructed from the [[field norm]] of a [[field extension]] ''L''/''K'' of degree ''n''.<ref>{{citation\n | last = Lekkerkerker | first = Cornelis Gerrit\n | authorlink = Gerrit Lekkerkerker\n | location = Amsterdam\n | mr = 0271032\n | page = 29\n | publisher = North-Holland Publishing Co.\n | series = Bibliotheca Mathematica\n | title = Geometry of numbers\n | url = https://books.google.com/books?id=XZ7iBQAAQBAJ&pg=PA29\n | volume = 8\n | year = 1969}}.</ref> That is, writing ''N'' for the norm mapping to ''K'', and selecting a basis\n\n:''e''<sub>1</sub>, ..., ''e''<sub>''n''</sub>\n\nfor ''L'' as a vector space over ''K'', the form is given by\n\n:''N''(''x''<sub>1</sub>''e''<sub>1</sub> + ... + ''x''<sub>''n''</sub>''e''<sub>''n''</sub>)\n\nin variables \n\n:''x''<sub>1</sub>, ..., ''x''<sub>''n''</sub>.\n\nIn [[number theory]] norm forms are studied as [[Diophantine equation]]s, where they generalize, for example, the [[Pell equation]].<ref>{{citation\n | last1 = Bombieri | first1 = Enrico\n | authorlink1 = Enrico Bombieri\n | last2 = Gubler | first2 = Walter\n | doi = 10.1017/CBO9780511542879\n | isbn = 978-0-521-84615-8\n | mr = 2216774\n | pages = 190–191\n | publisher = Cambridge University Press, Cambridge\n | series = New Mathematical Monographs\n | title = Heights in Diophantine geometry\n | url = https://books.google.com/books?id=3ATnwmGegvsC&pg=PA190\n | volume = 4\n | year = 2006}}.</ref> For this application the field ''K'' is usually the rational number field, the field ''L'' is an [[algebraic number field]], and the basis is taken of some [[order (ring theory)|order]] in the [[ring of integers]] ''O''<sub>''L''</sub> of ''L''.\n\n==References==\n{{reflist}}\n\n{{DEFAULTSORT:Norm Form}}\n[[Category:Field theory]]\n[[Category:Diophantine equations]]\n[[Category:Homogeneous polynomials]]"
    },
    {
      "title": "Polynomial SOS",
      "url": "https://en.wikipedia.org/wiki/Polynomial_SOS",
      "text": "{{about|representing a [[positive polynomial|non-negative polynomial]] as sum of squares of polynomials|representing polynomial as a sum of squares of rational functions|Hilbert's seventeenth problem|the sum of squares of consecutive integers|square pyramidal number|representing an integer as a sum of squares of integers|Lagrange's four-square theorem}}\n\nIn [[mathematics]], a [[Homogeneous polynomial|form]] (i.e. a homogeneous polynomial) ''h''(''x'') of degree 2''m'' in the real ''n''-dimensional vector ''x'' is sum of squares of forms (SOS) if and only if there exist forms <math>g_1(x),\\ldots,g_k(x)</math> of degree ''m'' such that\n\n:<math>\nh(x)=\\sum_{i=1}^k g_i(x)^2 .\n</math>\n\nEvery form that is SOS is also a [[positive polynomial]], and although the converse is not always true, Hilbert proved that for ''n'' = 2, ''m'' = 1 or ''n'' = 3 and 2''m'' = 4 a form is SOS if and only if it is positive.<ref>{{cite journal|last1=Hilbert|first1=David|title=Ueber die Darstellung definiter Formen als Summe von Formenquadraten|journal=Mathematische Annalen|date=September 1888|volume=32|issue=3|pages=342–350|doi=10.1007/bf01443605}}</ref> The same is also valid for the analog problem on positive ''symmetric'' forms.<ref>{{cite journal|last1=Choi|first1=M. D.|last2=Lam|first2=T. Y.|title=An old question of Hilbert|journal=Queen's Papers in Pure and Applied Mathematics|date=1977|volume=46|pages=385–405}}</ref><ref>{{cite journal|last1=Goel|first1=Charu|last2=Kuhlmann|first2=Salma|last3=[[Bruce Reznick|Reznick]]|first3=Bruce|title=On the Choi–Lam analogue of Hilbert's 1888 theorem for symmetric forms|journal=Linear Algebra and its Applications|date=May 2016|volume=496|pages=114–120|doi=10.1016/j.laa.2016.01.024|arxiv=1505.08145}}</ref>\n\nAlthough not every form can be represented as SOS, explicit sufficient conditions for a form to be SOS have been found.<ref>{{cite journal|last1=Lasserre|first1=Jean B.|title=Sufficient conditions for a real polynomial to be a sum of squares|journal=Archiv der Mathematik|volume=89|issue=5|pages=390–398|doi=10.1007/s00013-007-2251-y|url=http://www.optimization-online.org/DB_HTML/2007/02/1587.html|arxiv=math/0612358|year=2007|citeseerx=10.1.1.240.4438}}</ref><ref>{{cite journal|last1=Powers|first1=Victoria|last2=Wörmann|first2=Thorsten|title=An algorithm for sums of squares of real polynomials|journal=Journal of Pure and Applied Algebra|date=1998|volume=127|issue=1|pages=99–104|doi=10.1016/S0022-4049(97)83827-3|url=http://www.mathcs.emory.edu/~vicki/pub/sos.pdf}}</ref> Moreover, every real nonnegative form can be approximated as closely as desired (in the <math>l_1</math>-norm of its coefficient vector) by a sequence of forms <math>\\{f_\\epsilon\\}</math> that are SOS.<ref>{{cite journal|last1=Lasserre|first1=Jean B.|title=A Sum of Squares Approximation of Nonnegative Polynomials|journal=SIAM Review|date=2007|volume=49|issue=4|pages=651–669|doi=10.1137/070693709|arxiv=math/0412398}}</ref> \n\n== Square matricial representation (SMR) ==\nTo establish whether a form ''h''(''x'') is SOS amounts to solving a [[convex optimization]] problem. Indeed, any ''h''(''x'') can be written as\n\n:<math>\nh(x)=x^{\\{m\\}'}\\left(H+L(\\alpha)\\right)x^{\\{m\\}}\n</math>\n\nwhere <math>x^{\\{m\\}}</math> is a vector containing a base for the forms of degree ''m'' in ''x'' (such as all monomials of degree ''m'' in ''x''), the prime &prime; denotes the [[transpose]], ''H'' is any symmetric matrix satisfying\n\n:<math>\nh(x)=x^{\\left\\{m\\right\\}'}Hx^{\\{m\\}}\n</math>\n\nand <math>L(\\alpha)</math> is a linear parameterization of the [[vector space|linear space]]\n\n:<math>\n\\mathcal{L}=\\left\\{L=L':~x^{\\{m\\}'} L x^{\\{m\\}}=0\\right\\}.\n</math>\n\nThe dimension of the vector <math>x^{\\{m\\}}</math> is given by\n\n:<math>\n\\sigma(n,m)=\\binom{n+m-1}{m}\n</math>\n\nwhereas the dimension of the vector <math>\\alpha</math> is given by\n\n:<math>\n\\omega(n,2m)=\\frac{1}{2}\\sigma(n,m)\\left(1+\\sigma(n,m)\\right)-\\sigma(n,2m).\n</math>\n\nThen, ''h''(''x'') is SOS if and only if there exists a vector <math>\\alpha</math> such that\n\n:<math>\nH + L(\\alpha) \\ge 0,\n</math>\n\nmeaning that the matrix <math>H + L(\\alpha)</math> is [[positive-semidefinite matrix|positive-semidefinite]]. This is a [[linear matrix inequality]] (LMI) feasibility test, which is a convex optimization problem. The expression <math>h(x)=x^{\\{m\\}'}\\left(H+L(\\alpha)\\right)x^{\\{m\\}}</math> was introduced in <ref>{{cite conference |url=http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7099515 |title=On convexification of some minimum distance problems |last1=Chesi |first1=G. |last2=Tesi |first2=A. |last3=Vicino |first3=A. |last4=Genesio |first4=R.  |date=1999 |publisher=IEEE |book-title=Proceedings of the 5th European Control Conference |pages=1446–1451 |location=Karlsruhe, Germany}}</ref>  with the name square matricial representation (SMR) in order to establish whether a form is SOS via an LMI. This representation is also known as Gram matrix.<ref>{{cite conference |url= |title=Sums of squares of real polynomials |last1=Choi |first1=M. |last2=Lam |first2=T. |last3=Reznick |first3=B. |date=1995 |book-title=Proceedings of Symposia in Pure Mathematics |pages=103–125 |location=}}</ref>\n\n=== Examples ===\n\n*Consider the form of degree 4 in two variables <math>h(x)=x_1^4-x_1^2x_2^2+x_2^4</math>. We have\n:<math>\nm=2,~x^{\\{m\\}}=\\left(\\begin{array}{c}x_1^2\\\\x_1x_2\\\\x_2^2\\end{array}\\right),\n~H+L(\\alpha)=\\left(\\begin{array}{ccc}\n1&0&-\\alpha_1\\\\0&-1+2\\alpha_1&0\\\\-\\alpha_1&0&1\n\\end{array}\\right).\n</math>\n:Since there exists &alpha; such that <math>H+L(\\alpha)\\ge 0</math>, namely <math>\\alpha=1</math>, it follows that ''h''(''x'') is SOS.\n\n*Consider the form of degree 4 in three variables <math>h(x)=2x_1^4-2.5x_1^3x_2+x_1^2x_2x_3-2x_1x_3^3+5x_2^4+x_3^4</math>. We have\n:<math>\nm=2,~x^{\\{m\\}}=\\left(\\begin{array}{c}x_1^2\\\\x_1x_2\\\\x_1x_3\\\\x_2^2\\\\x_2x_3\\\\x_3^2\\end{array}\\right),\n~H+L(\\alpha)=\\left(\\begin{array}{cccccc}\n2&-1.25&0&-\\alpha_1&-\\alpha_2&-\\alpha_3\\\\\n-1.25&2\\alpha_1&0.5+\\alpha_2&0&-\\alpha_4&-\\alpha_5\\\\\n0&0.5+\\alpha_2&2\\alpha_3&\\alpha_4&\\alpha_5&-1\\\\\n-\\alpha_1&0&\\alpha_4&5&0&-\\alpha_6\\\\\n-\\alpha_2&-\\alpha_4&\\alpha_5&0&2\\alpha_6&0\\\\\n-\\alpha_3&-\\alpha_5&-1&-\\alpha_6&0&1\n\\end{array}\\right).\n</math>\n:Since <math>H+L(\\alpha)\\ge 0</math> for <math>\\alpha=(1.18,-0.43,0.73,1.13,-0.37,0.57)</math>, it follows that ''h''(''x'') is SOS.\n\n== Generalizations ==\n\n=== Matrix SOS ===\nA matrix form ''F''(''x'') (i.e., a matrix whose entries are forms) of dimension ''r'' and degree ''2m'' in the real ''n''-dimensional vector ''x'' is SOS if and only if there exist matrix forms <math>G_1(x),\\ldots,G_k(x)</math> of degree ''m'' such that\n\n:<math>\nF(x)=\\sum_{i=1}^k G_i(x)'G_i(x) .\n</math>\n\n==== Matrix SMR ====\nTo establish whether a matrix form ''F''(''x'') is SOS amounts to solving a convex optimization problem. Indeed, similarly to the scalar case any ''F''(''x'') can be written according to the SMR as\n\n:<math>\nF(x)=\\left(x^{\\{m\\}}\\otimes I_r\\right)'\\left(H+L(\\alpha)\\right)\\left(x^{\\{m\\}}\\otimes I_r\\right)\n</math>\n\nwhere <math>\\otimes</math> is the [[Kronecker product]] of matrices, ''H'' is any symmetric matrix satisfying\n\n:<math>\nF(x)=\\left(x^{\\{m\\}}\\otimes I_r\\right)'H\\left(x^{\\{m\\}}\\otimes I_r\\right)\n</math>\n\nand <math>L(\\alpha)</math> is a linear parameterization of the linear space\n\n:<math>\n\\mathcal{L}=\\left\\{L=L':~\\left(x^{\\{m\\}}\\otimes I_r\\right)'L\\left(x^{\\{m\\}}\\otimes I_r\\right)=0\\right\\}.\n</math>\n\nThe dimension of the vector <math>\\alpha</math> is given by\n\n:<math>\n\\omega(n,2m,r)=\\frac{1}{2}r\\left(\\sigma(n,m)\\left(r\\sigma(n,m)+1\\right)-(r+1)\\sigma(n,2m)\\right).\n</math>\n\nThen, ''F''(''x'') is SOS if and only if there exists a vector <math>\\alpha</math> such that the following LMI holds:\n\n:<math>\nH+L(\\alpha) \\ge 0.\n</math>\n\nThe expression <math>F(x)=\\left(x^{\\{m\\}}\\otimes I_r\\right)'\\left(H+L(\\alpha)\\right)\\left(x^{\\{m\\}}\\otimes I_r\\right)</math> was introduced in <ref>{{cite conference |url=http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1272307 |title=Robust stability for polytopic systems via polynomially parameter-dependent Lyapunov functions |last1=Chesi |first1=G. |last2=Garulli |first2=A. |last3=Tesi |first3=A. |last4=Vicino |first4=A. |date=2003 |publisher=IEEE |book-title=Proceedings of the 42nd IEEE Conference on Decision and Control |pages=4670–4675 |location=Maui, Hawaii}}</ref> in order to establish whether a matrix form is SOS via an LMI.\n\n=== Noncommutative polynomial SOS ===\n\nConsider the [[free algebra]] ''R''⟨''X''⟩ generated by the ''n'' noncommuting letters ''X'' = (''X''<sub>1</sub>,...,''X''<sub>''n''</sub>) and equipped with the involution <sup>T</sup>, such that <sup>T</sup> fixes ''R'' and ''X''<sub>1</sub>,...,''X''<sub>''n''</sub> and reverse words formed by ''X''<sub>1</sub>,...,''X''<sub>''n''</sub>.\nBy analogy with the commutative case, the noncommutative symmetric polynomials ''f'' are the noncommutative polynomials of the form ''f''&nbsp;=&nbsp;''f''<sup>''T''</sup>. When any real matrix of any dimension ''r x r'' is evaluated at a symmetric noncommutative polynomial ''f''  results in a [[positive semi-definite]] matrix, ''f'' is said to be matrix-positive.\n\nA noncommutative polynomial is SOS if there exists noncommutative polynomials <math>h_1,\\ldots,h_k</math> such that\n\n: <math> f(X) = \\sum_{i=1}^{k} h_i(X)^T h_i(X).</math>\n\nSurprisingly, in the noncommutative scenario a noncommutative polynomial is SoS if and only if it is matrix-positive.<ref>{{cite journal|last1=Helton|first1=J. William|title=\"Positive\" Noncommutative Polynomials Are Sums of Squares|journal=The Annals of Mathematics|date=September 2002|volume=156|issue=2|pages=675–694|doi=10.2307/3597203|jstor=3597203}}</ref> Moreover, there exist algorithms available to decompose matrix-positive polynomials in sum of squares of noncommutative polynomials.<ref>{{cite journal|last1=Burgdorf|first1=Sabine|last2=Cafuta|first2=Kristijan|last3=Klep|first3=Igor|last4=Povh|first4=Janez|title=Algorithmic aspects of sums of Hermitian squares of noncommutative polynomials|journal=Computational Optimization and Applications|date=25 October 2012|volume=55|issue=1|pages=137–153|doi=10.1007/s10589-012-9513-8|citeseerx=10.1.1.416.543}}</ref>\n\n== References ==\n{{Reflist}}\n\n== See also ==\n* [[Sum-of-squares optimization]]\n* [[Positive polynomial]]\n* [[Hilbert's seventeenth problem]]\n\n[[Category:Homogeneous polynomials]]\n[[Category:Real algebraic geometry]]"
    },
    {
      "title": "Octic function",
      "url": "https://en.wikipedia.org/wiki/Octic_function",
      "text": "#REDIRECT [[Octic equation]]\n\n[[Category:Polynomial functions]]"
    },
    {
      "title": "Ring of polynomial functions",
      "url": "https://en.wikipedia.org/wiki/Ring_of_polynomial_functions",
      "text": "In [[mathematics]], the '''ring of polynomial functions''' on a [[vector space]] ''V'' over a field ''k'' gives a coordinate-free analog of a [[polynomial ring]]. It is denoted by ''k''[''V'']. If ''V'' has finite dimension and is viewed as an [[algebraic variety]], then ''k''[''V''] is precisely the [[coordinate ring]] of ''V''.\n\nThe explicit definition of the ring can be given as follows. If <math>k[t_1, \\dots, t_n]</math> is a polynomial ring, then we can view <math>t_i</math> as coordinate functions on <math>k^n</math>; i.e., <math>t_i(x) = x_i</math> when <math>x = (x_1, \\dots, x_n).</math> This suggests the following: given a vector space ''V'', let ''k''[''V''] be the ring generated by the [[dual space]] <math>V^*</math>, which is a subring of the ring of all functions <math>V \\to k</math>. If we fix a basis for ''V'' and write <math>t_i</math> for its dual basis, then ''k''[''V''] consists of polynomials in <math>t_i</math>.\n\nIf ''k'' is infinite, then ''k''[''V''] is the [[symmetric algebra]] of the dual space <math>V^*</math>.\n\nIn applications, one also defines ''k''[''V''] when ''V'' is defined over some subfield of ''k'' (e.g., ''k'' is the complex field and ''V'' is a real vector space.) The same definition still applies.\n\nThroughout the article, for simplicity, the base field ''k'' is assumed to be infinite.\n\n== Relation with polynomial ring ==\nLet <math>A=K[x]</math> be the set of all polynomials over a field ''K'' and ''B'' be the set of all polynomial functions in one variable over ''K''.  Both ''A'' and ''B'' are algebras over ''K'' given by the standard multiplication and addition of polynomials and functions. We can map each <math>f</math> in ''A'' to <math>\\hat{f}</math> in ''B'' by the rule <math>\\hat{f}(t) = f(t)</math>.  A routine check shows that the mapping <math>f \\mapsto \\hat{f}</math> is a homomorphism of the algebras ''A'' and ''B''. This homomorphism is an isomorphism if and only if ''K'' is an infinite field. For example, if ''K'' is a finite field then let <math>p(x) = \\prod\\limits_{t \\in K} (x-t)</math>. ''p'' is a nonzero polynomial in ''K''[''x''], however <math>p(t) = 0</math> for all ''t'' in ''K'', so <math>\\hat{p} = 0</math> is the zero function and our homomorphism is not an isomorphism (and, actually, the algebras are not isomorphic, since the algebra of polynomials is infinite while that of polynomial functions is finite).\n\nIf ''K'' is infinite then choose a polynomial ''f'' such that <math>\\hat{f} = 0</math>.  We want to show this implies that <math>f = 0</math>.  Let <math>\\deg f = n</math> and let <math>t_0,t_1,\\dots,t_n</math> be ''n''&nbsp;+&nbsp;1 distinct elements of ''K''.  Then <math>f(t_i) = 0</math> for <math>0 \\le i \\le n</math> and by [[Lagrange interpolation]] we have <math>f = 0</math>.  Hence the mapping <math>f \\mapsto \\hat{f}</math> is injective. Since this mapping is clearly surjective, it is bijective and thus an [[algebra isomorphism]] of ''A'' and ''B''.\n\n== Symmetric multilinear maps ==\nLet ''k'' be an infinite field of characteristic zero (or at least very large) and ''V'' a finite-dimensional vector space.\n\nLet <math>S^q(V)</math> denote the vector space of multilinear functionals <math>\\textstyle \\lambda: \\prod_1^q V \\to k</math> that are symmetric; <math>\\lambda(v_1, \\dots, v_q)</math> is the same for all permutations of <math>v_i</math>'s.\n\nAny λ in <math>S^q(V)</math> gives rise to a homogeneous polynomial function ''f'' of degree ''q'': we just let <math>f(v) = \\lambda(v, \\dots, v).</math> To see that ''f'' is a polynomial function, choose a basis <math>e_i, \\, 1 \\le i \\le n</math> of ''V'' and <math>t_i</math> its dual. Then\n:<math>\\lambda(v_1, \\dots, v_q) = \\sum_{i_1, \\dots, i_q = 1}^n \\lambda(e_{i_1}, \\dots, e_{i_q}) t_{i_1}(v_1) \\cdots t_{i_q}(v_q)</math>,\nwhich implies ''f'' is a polynomial in ''t''<sub>''i''</sub>'s.\n\nThus, there is a well-defined linear map:\n:<math>\\phi: S^q(V) \\to k[V]_q, \\, \\phi(\\lambda)(v) = \\lambda(v, \\cdots, v).</math>\nWe show it is an isomorphism. Choosing a basis as before, any homogeneous polynomial function ''f'' of degree ''q'' can be written as:\n:<math>f = \\sum_{i_1, \\dots, i_q = 1}^n a_{i_1 \\cdots i_q} t_{i_1} \\cdots t_{i_q}</math>\nwhere <math>a_{i_1 \\cdots i_q}</math> are symmetric in <math>i_1, \\dots, i_q</math>. Let\n:<math>\\psi(f)(v_1, \\dots, v_q) = \\sum_{i_1, \\cdots, i_q = 1}^n a_{i_1 \\cdots i_q} t_{i_1}(v_1) \\cdots t_{i_q}(v_q).</math>\nClearly, <math>\\phi\\circ\\psi</math> is the identity; in particular, φ is surjective. To see φ is injective, suppose φ(λ) = 0. Consider\n:<math>\\phi(\\lambda)(t_1 v_1 + \\cdots + t_q v_q) = \\lambda(t_1 v_1 + \\cdots + t_q v_q, ..., t_1 v_1 + \\cdots +\nt_q v_q)</math>,\nwhich is zero. The coefficient of ''t''<sub>1</sub>''t''<sub>2</sub> … ''t''<sub>''q''</sub> in the above expression is q! times λ(''v''<sub>1</sub>, …, ''v''<sub>''q''</sub>); it follows that λ = 0.\n\nNote: φ is independent of a choice of basis; so the above proof shows that ψ is also independent of a basis, the fact not a priori obvious.\n\nExample: A bilinear functional gives rise to a quadratic form in a unique way and any quadratic form arises in this way.\n\n== Taylor series expansion ==\n{{main|Taylor series}}\nGiven a smooth function, locally, one can get a partial derivative of the function from its Taylor series expansion and, conversely, one can recover the function from the series expansion. This fact continues to hold for polynomials functions on a vector space. If ''f'' is in ''k''[''V''], then we write: for ''x'', ''y'' in ''V'',\n:<math>f(x + y) = \\sum_{n=0}^{\\infty} g_n(x, y)</math>\nwhere ''g''<sub>''n''</sub>(x, y) are homogeneous of degree ''n'' in ''y'' and only finitely many of them are nonzero. We then let\n:<math>(P_y f)(x) = g_1(x, y),</math>\nresulting in the linear endomorphism ''P''<sub>''y''</sub> of ''k''[''V'']. It is called the polarization operator. We then have, as promised:\n{{math_theorem|math_statement=For each ''f'' in ''k''[V] and ''x'', ''y'' in ''V'',\n:<math>f(x + y) = \\sum_{n=0}^{\\infty} {1 \\over n!} P_y^n f(x)</math>.}}\nProof: We first note that (''P''<sub>''y''</sub> ''f'') (''x'') is the coefficient of ''t'' in ''f''(''x'' + ''t'' ''y''); in other words, since ''g''<sub>0</sub>(''x'', ''y'') = ''g''<sub>0</sub>(''x'', 0) = ''f''(''x''),\n:<math>P_y f (x) = \\left . {d \\over dt} \\right |_{t=0} f(x + ty)</math>\nwhere the right-hand side is, by definition,\n:<math>\\left . {f(x+ty) - f(x) \\over t} \\right |_{t=0}.</math>\nThe theorem follows from this. For example, for ''n'' = 2, we have:\n:<math>P_y^2 f (x) = \\left . {\\partial \\over \\partial t_1} \\right |_{t_1=0} P_y f(x + t_1 y) = \\left . {\\partial \\over \\partial t_1} \\right |_{t_1=0} \\left . {\\partial \\over \\partial t_2} \\right |_{t_2=0} f(x + (t_1 + t_2) y) = 2! g_2(x, y).</math>\nThe general case is similar. <math>\\square</math>\n\n==Operator product algebra==\nWhen the polynomials are valued not over a field ''k'', but instead are valued over some algebra, then one may define additional structure.  Thus, for example, one may consider the ring of functions over ''[[general linear group|GL(n,m)]]'', instead of for ''k = GL(1,m)''.{{clarify|GL(1\", m) is k^* not k|date=September 2015}} In this case, one may impose an additional axiom.\n\nThe '''operator product algebra''' is an [[associative algebra]] of the form\n\n:<math>A^i(x)B^j(y) = \\sum_k f^{ij}_k (x,y,z) C^k(z)</math>\n\nThe [[structure constant]]s <math>f^{ij}_k (x,y,z)</math> are required to be single-valued functions, rather than sections of some [[vector bundle]].  The fields (or operators) <math>A^i(x)</math> are required to span the [[ring of functions]]. In practical calculations, it is usually required that the sums be analytic within some [[radius of convergence]]; typically with a radius of convergence of <math>|x-y|</math>. Thus, the ring of functions can be taken to be the ring of polynomial functions.\n\nThe above can be considered to be an additional requirement imposed on the ring; it is sometimes called the ''bootstrap''.  In [[physics]], a special case of the operator product algebra is known as the [[operator product expansion]].\n\n== See also ==\n* [[Algebraic geometry of projective spaces]]\n* [[Polynomial ring]]\n* [[Symmetric algebra]]\n* [[Zariski tangent space]]\n\n== Notes ==\n{{reflist}}\n\n== References ==\n* {{citation|last1=Kobayashi|first1=S.|last2=Nomizu|first2=K.|title=[[Foundations of Differential Geometry]], Vol. 2|publisher=Wiley-Interscience|year=1963|publication-date= 2004|edition=new}}.\n\n[[Category:Polynomial functions]]\n[[Category:Ring theory]]"
    },
    {
      "title": "Septic function",
      "url": "https://en.wikipedia.org/wiki/Septic_function",
      "text": "#REDIRECT [[Septic equation]]\n\n[[Category:Polynomial functions]]"
    },
    {
      "title": "Sextic function",
      "url": "https://en.wikipedia.org/wiki/Sextic_function",
      "text": "#REDIRECT [[Sextic equation]]\n\n[[Category:Polynomial functions]]"
    },
    {
      "title": "Zero function",
      "url": "https://en.wikipedia.org/wiki/Zero_function",
      "text": "#REDIRECT [[0#Related mathematical terms]]\n[[Category:Polynomial functions]]"
    },
    {
      "title": "Rational function",
      "url": "https://en.wikipedia.org/wiki/Rational_function",
      "text": "{{About||the use in automata theory|Finite-state transducer|the use in monoid theory|Rational function (monoid)}}\n{{Use American English|date = January 2019}}\n{{Short description|Ratio of polynomial functions}}\n{{More footnotes|date=September 2015}}\nIn [[mathematics]], a '''rational function''' is any [[function (mathematics)|function]] which can be defined by a '''rational fraction''', ''i.e.'' an [[algebraic fraction]] such that both the numerator and the denominator are [[polynomial]]s. The [[coefficient]]s of the polynomials need not be [[rational number]]s; they may be taken in any [[field (mathematics)|field]] ''K''. In this case, one speaks of a rational function and a rational fraction ''over K''. The values of the [[variable (mathematics)|variable]]s may be taken in any field ''L'' containing ''K''. Then the [[domain (function)|domain]] of the function is the set of the values of the variables for which the denominator is not zero and the [[codomain]] is ''L''.\n\nThe set of rational functions over a field ''K'' is a field, the [[field of fractions]] of the [[ring (mathematics)|ring]] of the [[polynomial function]]s over ''K''.\n\n==Definitions==\nA function <math>f(x)</math> is called a rational function if and only if it can be written in the form\n\n:<math> f(x) = \\frac{P(x)}{Q(x)} </math>\n\nwhere <math>P\\,</math> and <math>Q\\,</math> are [[polynomial function]]s of <math>x\\,</math> and <math>Q\\,</math> is not the [[zero function]].  The [[domain of a function|domain]] of <math>f\\,</math> is the set of all values of  <math>x\\,</math> for which the denominator <math>Q(x)\\,</math> is not zero.\n\nHowever, if <math>\\textstyle P</math> and <math>\\textstyle Q</math> have a non-constant [[polynomial greatest common divisor]] <math>\\textstyle R</math>, then setting <math>\\textstyle P=P_1R</math> and <math>\\textstyle Q=Q_1R</math> produces a rational function\n\n:<math> f_1(x) = \\frac{P_1(x)}{Q_1(x)}, </math>\n\nwhich may have a larger domain than <math> f(x)</math>, and is equal to <math> f(x)</math> on the domain of <math> f(x).</math> It is a common usage to identify <math> f(x)</math> and <math> f_1(x)</math>, that is to extend \"by continuity\" the domain of <math> f(x)</math> to that of <math> f_1(x).</math> Indeed, one can define a rational fraction as an [[equivalence class]] of fractions of polynomials, where two fractions <math>\\frac{A(x)}{B(x)}</math> and <math>\\frac{C(x)}{D(x)}</math> are considered equivalent if <math>A(x)D(x)=B(x)C(x)</math>. In this case <math>\\frac{P(x)}{Q(x)}</math> is equivalent to <math>\\frac{P_1(x)}{Q_1(x)}</math>.\n\nA '''proper rational function''' is a rational function in which the [[Degree of a polynomial|degree]] of <math>P(x)</math> is no greater than the degree of <math>Q(x)</math> and both are [[real polynomial]]s.<ref>{{multiref|Martin J. Corless, Art Frazho, ''Linear Systems and Control'', p. 163, CRC Press, 2003 {{isbn|0203911377}}.|Malcolm W. Pownall, ''Functions and Graphs: Calculus Preparatory Mathematics'', p. 203, Prentice-Hall, 1983 {{isbn|0133323048}}.}}</ref>\n\n==Examples==\n{{multiple image\n | header = Examples of rational functions\n | align = right\n | direction = vertical\n | width = 300\n | image1 = RationalDegree3.svg\n | alt1 = Rational function of degree 3\n | caption1 = Rational function of degree 3: <math>y = \\frac{x^3-2x}{2(x^2-5)}</math>\n | image2 = RationalDegree2byXedi.svg\n | alt2 = Rational function of degree 2\n | caption2 = Rational function of degree 2: <math>y = \\frac{x^2-3x-2}{x^2-4}</math>\n}}\n\nThe rational function\n\n:<math>f(x) = \\frac{x^3-2x}{2(x^2-5)}</math>\n\nis not defined at\n\n:<math>x^2=5 \\Leftrightarrow x=\\pm \\sqrt{5}.</math>\n\nIt is asymptotic to <math>\\tfrac{x}{2}</math> as <math>x\\to \\infty.</math>\n\nThe rational function\n\n:<math>f(x) = \\frac{x^2 + 2}{x^2 + 1}</math>\n\nis defined for all [[real number]]s, but not for all [[complex number]]s, since if ''x'' were a square root of <math>-1</math> (i.e. the [[imaginary unit]] or its negative), then formal evaluation would lead to division by zero:\n\n:<math>f(i) = \\frac{i^2 + 2}{i^2 + 1} = \\frac{-1 + 2}{-1 + 1} = \\frac{1}{0},</math>\n\nwhich is undefined.\n\nA [[constant function]] such as ''f''(''x'') = π is a rational function since constants are polynomials.  Note that the function itself is rational, even though the [[value (mathematics)|value]] of ''f''(''x'') is irrational for all ''x''.\n\nEvery [[polynomial function]] <math>f(x) = P(x)</math> is a rational function with <math>Q(x) = 1.</math> A function that cannot be written in this form, such as <math>f(x) = \\sin(x),</math> is not a rational function. The adjective \"irrational\" is not generally used for functions.\n\nThe rational function <math>f(x) = \\tfrac{x}{x}</math> is equal to 1 for all ''x'' except 0, where there is a [[removable singularity]]. The sum, product, or quotient (excepting division by the zero polynomial) of two rational functions is itself a rational function. However, the process of reduction to standard form may inadvertently result in the removal of such singularities unless care is taken. Using the definition of rational functions as equivalence classes gets around this, since ''x''/''x'' is equivalent to 1/1.\n\n==Taylor series==\nThe coefficients of a [[Taylor series]] of any rational function satisfy a [[Recurrence relation|linear recurrence relation]], which can be found by equating the rational function to a Taylor series with indeterminate coefficients, and collecting [[like terms]] after clearing the denominator.\n\nFor example,\n\n:<math>\\frac{1}{x^2 - x + 2} = \\sum_{k=0}^{\\infty} a_k x^k.</math>\n\nMultiplying through by the denominator and distributing,\n\n:<math>1 = (x^2 - x + 2) \\sum_{k=0}^{\\infty} a_k x^k</math>\n\n:<math>1 = \\sum_{k=0}^{\\infty} a_k x^{k+2} - \\sum_{k=0}^{\\infty} a_k x^{k+1} + 2\\sum_{k=0}^{\\infty} a_k x^k.</math>\n\nAfter adjusting the indices of the sums to get the same powers of ''x'', we get\n\n:<math>1 = \\sum_{k=2}^{\\infty} a_{k-2} x^k - \\sum_{k=1}^{\\infty} a_{k-1} x^k + 2\\sum_{k=0}^{\\infty} a_k x^k.</math>\n\nCombining like terms gives\n\n:<math>1 = 2a_0 + (2a_1 - a_0)x + \\sum_{k=2}^{\\infty} (a_{k-2} - a_{k-1} + 2a_k) x^k.</math>\n\nSince this holds true for all ''x'' in the radius of convergence of the original Taylor series, we can compute as follows.  Since the [[constant term]] on the left must equal the constant term on the right it follows that\n\n:<math>a_0 = \\frac{1}{2}.</math>\n\nThen, since there are no powers of ''x'' on the left, all of the [[coefficient]]s on the right must be zero, from which it follows that\n\n:<math>a_1 = \\frac{1}{4}</math>\n\n:<math>a_{k} = \\frac{1}{2} (a_{k-1} - a_{k-2})\\quad for\\ k \\ge 2.</math>\n\nConversely, any sequence that satisfies a linear recurrence determines a rational function when used as the coefficients of a Taylor series. This is useful in solving such recurrences, since by using [[partial fraction|partial fraction decomposition]] we can write any proper rational function as a sum of factors of the form ''1 / (ax + b)'' and expand these as [[geometric series]], giving an explicit formula for the Taylor coefficients; this is the method of [[generating functions]].\n\n==Abstract algebra and geometric notion== <!-- Rational expression redirects here -->\nIn [[abstract algebra]] the concept of a polynomial is extended to include formal expressions in which the coefficients of the polynomial can be taken from any [[field (mathematics)|field]].  In this setting given a field ''F'' and some indeterminate ''X'', a '''rational expression''' is any element of the [[field of fractions]] of the [[polynomial ring]] ''F''[''X''].  Any rational expression can be written as the quotient of two polynomials ''P''/''Q'' with ''Q'' ≠ 0, although this representation isn't unique.  ''P''/''Q'' is equivalent to ''R''/''S'', for polynomials ''P'', ''Q'', ''R'', and ''S'', when ''PS'' = ''QR''.  However, since ''F''[''X''] is a [[unique factorization domain]], there is a [[irreducible fraction|unique representation]] for any rational expression ''P''/''Q'' with ''P'' and ''Q'' polynomials of lowest degree and ''Q'' chosen to be [[monic polynomial|monic]].  This is similar to how a [[Fraction (mathematics)|fraction]] of integers can always be written uniquely in lowest terms by canceling out common factors.\n\nThe field of rational expressions is denoted ''F''(''X''). This field is said to be generated (as a field) over ''F'' by (a [[transcendental element]]) ''X'', because ''F''(''X'') does not contain any proper subfield containing both ''F'' and the element ''X''.\n\n===Complex rational functions===\nIn [[complex analysis]], a rational function\n\n:<math>f(z) = \\frac{P(z)}{Q(z)}</math>\n\nis the ratio of two polynomials with complex coefficients, where ''Q'' is not the zero polynomial and ''P'' and ''Q'' have no common factor (this avoids ''f'' taking the indeterminate value 0/0). The domain and range of ''f'' are usually taken to be the [[Riemann sphere]], which avoids any need for special treatment at the [[pole (complex analysis)|poles]] of the function (where ''Q''(''z'') is 0).\n\nThe ''degree'' of a rational function is the maximum of the [[degree of a polynomial|degrees]] of its constituent polynomials ''P'' and ''Q''. If the degree of ''f'' is ''d'', then the equation\n\n:<math>f(z) = w \\,</math>\n\nhas ''d'' distinct solutions in ''z'' except for certain values of ''w'', called ''critical values'', where two or more solutions coincide. The function ''f'' can therefore be thought of as a ''d''-fold [[covering map|covering]] of the ''w''-sphere by the ''z''-sphere.\n\nRational functions with degree 1 are called ''[[Möbius transformation]]s'' and form the [[automorphism]]s [[group (mathematics)|group]] of the [[Riemann sphere]]. Rational functions are representative examples of [[meromorphic function]]s.\n\n===Notion of a rational function on an algebraic variety=== \n{{Main|Function field of an algebraic variety}}\n\nLike [[Polynomial ring#The polynomial ring in several variables|polynomials]], rational expressions can also be generalized to ''n'' indeterminates ''X''<sub>1</sub>,..., ''X''<sub>''n''</sub>, by taking the field of fractions of ''F''[''X''<sub>1</sub>,..., ''X''<sub>''n''</sub>], which is denoted by ''F''(''X''<sub>1</sub>,..., ''X''<sub>''n''</sub>).\n\nAn extended version of the abstract idea of rational function is used in algebraic geometry. There the [[function field of an algebraic variety]] ''V'' is formed as the field of fractions of the [[coordinate ring]] of ''V'' (more accurately said, of a Zariski-dense affine open set in ''V''). Its elements ''f'' are considered as regular functions in the sense of algebraic geometry on non-empty open sets ''U'', and also may be seen as morphisms to the [[projective line]].\n\n==Applications==\nThese objects are first encountered in [[elementary algebra|school algebra]].  In more advanced mathematics they play an important role in [[ring theory]], especially in the construction of [[field extension]]s. They also provide an example of a ''nonarchimedean field'' (see [[Archimedean property]]).\n\nRational functions are used in [[numerical analysis]] for [[interpolation]] and [[approximation]] of functions, for example the [[Padé approximation]]s introduced by [[Henri Padé]]. Approximations in terms of rational functions are well suited for [[computer algebra system]]s and other numerical [[software]]. Like polynomials, they can be evaluated straightforwardly, and at the same time they express more diverse behavior than polynomials. <!-- Care must be taken, however, since small errors in denominators close to zero can cause large errors in evaluation. -->\n\nRational functions are used to approximate or model more complex equations in science and engineering including fields and forces in physics, spectroscopy in analytical chemistry, enzyme kinetics in biochemistry, electronic circuitry, aerodynamics, medicine concentrations in vivo, wave functions for atoms and molecules, optics and photography to improve image resolution, and acoustics and sound{{Citation needed|date=April 2017}}.\n\nIn [[signal processing]], the [[Laplace transform]] (for continuous systems) or the [[z-transform]] (for discrete-time systems) of the [[impulse response]] of commonly-used [[linear time-invariant system]]s (filters) with [[infinite impulse response]] are rational functions over complex numbers.\n\n==See also==\n* [[Field of fractions]]\n* [[Partial fraction decomposition]]\n* [[Partial fractions in integration]]\n* [[Function field of an algebraic variety]]\n* [[Algebraic fraction]]s{{snd}}a generalization of rational functions that allows taking integer roots\n\n==References==\n{{Reflist}}\n*{{springer|id=Rational_function&oldid=17805|title=Rational function}}\n*{{Citation |last1=Press|first1=W.H.|last2=Teukolsky|first2=S.A.|last3=Vetterling|first3=W.T.|last4=Flannery|first4=B.P.|year=2007|title=Numerical Recipes: The Art of Scientific Computing|edition=3rd|publisher=Cambridge University Press| publication-place=New York|isbn=978-0-521-88068-8|chapter=Section 3.4. Rational Function Interpolation and Extrapolation|chapter-url=http://apps.nrbook.com/empanel/index.html?pg=124}}\n\n==External links==\n* [http://jsxgraph.uni-bayreuth.de/wiki/index.php/Rational_functions Dynamic visualization of rational functions with JSXGraph]\n\n[[Category:Algebraic varieties]]\n[[Category:Morphisms of schemes]]\n[[Category:Meromorphic functions]]\n[[Category:Rational functions| ]]"
    },
    {
      "title": "Chebyshev rational functions",
      "url": "https://en.wikipedia.org/wiki/Chebyshev_rational_functions",
      "text": "{{for|the Chebyshev rational functions used in the design of [[elliptic filter]]s|Elliptic rational functions}}\n\n[[Image:ChebychevRational1.png|thumb|300px|Plot of the Chebyshev rational functions for {{math|''n'' {{=}} 0, 1, 2, 3, 4}} for {{math|0.01 ≤ ''x'' ≤ 100}}, log scale.]]\nIn [[mathematics]], the '''Chebyshev rational functions''' are a sequence of functions which are both [[rational functions|rational]] and [[orthogonal functions|orthogonal]]. They are named after [[Pafnuty Chebyshev]]. A rational Chebyshev function of degree {{math|''n''}} is defined as:\n\n:<math>R_n(x)\\ \\stackrel{\\mathrm{def}}{=}\\  T_n\\left(\\frac{x-1}{x+1}\\right)</math>\n\nwhere {{math|''T<sub>n</sub>''(''x'')}} is a [[Chebyshev polynomial]] of the first kind.\n\n== Properties==\n\nMany properties can be derived from the properties of the Chebyshev polynomials of the first kind. Other properties are unique to the functions themselves.\n\n=== Recursion ===\n\n:<math>R_{n+1}(x)=2\\,\\frac{x-1}{x+1}R_n(x)-R_{n-1}(x) \\quad \\text{for } n\\ge 1</math>\n\n=== Differential equations ===\n\n:<math>(x+1)^2R_n(x)=\\frac{1}{n+1}\\frac{\\mathrm{d}}{\\mathrm{d}x}R_{n+1}(x)-\\frac{1}{n-1}\\frac{\\mathrm{d}}{\\mathrm{d}x}R_{n-1}(x) \\quad \\text{for } n\\ge 2</math>\n\n:<math>(x+1)^2x\\frac{\\mathrm{d}^2}{\\mathrm{d}x^2}R_n(x)+\\frac{(3x+1)(x+1)}{2}\\frac{\\mathrm{d}}{\\mathrm{d}x}R_n(x)+n^2R_{n}(x) = 0</math>\n\n=== Orthogonality ===\n[[Image:ChebychevRational2.png|thumb|300px|Plot of the absolute value of the seventh-order ({{math|''n'' {{=}} 7}}) Chebyshev rational function for {{math|0.01 ≤ ''x'' ≤ 100}}. Note that there are {{math|''n''}} zeroes arranged symmetrically about {{math|''x'' {{=}} 1}} and if {{math|''x''<sub>0</sub>}} is a zero, then {{math|{{sfrac|1|''x''<sub>0</sub>}}}} is a zero as well. The maximum value between the zeros is unity. These properties hold for all orders.]]\n\nDefining:\n\n:<math>\\omega(x) \\ \\stackrel{\\mathrm{def}}{=}\\  \\frac{1}{(x+1)\\sqrt{x}}</math>\n\nThe orthogonality of the Chebyshev rational functions may be written:\n\n:<math>\\int_{0}^\\infty R_m(x)\\,R_n(x)\\,\\omega(x)\\,\\mathrm{d}x=\\frac{\\pi c_n}{2}\\delta_{nm}</math>\n\nwhere {{math|''c<sub>n</sub>'' {{=}} 2}} for {{math|''n'' {{=}} 0}} and {{math|''c<sub>n</sub>'' {{=}} 1}} for {{math|''n'' ≥ 1}}; {{math|''δ<sub>nm</sub>''}} is the [[Kronecker delta]] function.\n\n=== Expansion of an arbitrary function ===\nFor an arbitrary function {{math|''f''(''x'') ∈ ''L''{{su|b=''ω''|p=2|lh=0.8em}}}} the orthogonality relationship can be used to expand {{math|''f''(''x'')}}:\n\n:<math>f(x)=\\sum_{n=0}^\\infty F_n R_n(x)</math>\n\nwhere\n\n:<math>F_n=\\frac{2}{c_n\\pi}\\int_{0}^\\infty f(x)R_n(x)\\omega(x)\\,\\mathrm{d}x.</math>\n\n== Particular values ==\n\n:<math>\\begin{align}\nR_0(x)&=1\\\\\nR_1(x)&=\\frac{x-1}{x+1}\\\\\nR_2(x)&=\\frac{x^2-6x+1}{(x+1)^2}\\\\\nR_3(x)&=\\frac{x^3-15x^2+15x-1}{(x+1)^3}\\\\\nR_4(x)&=\\frac{x^4-28x^3+70x^2-28x+1}{(x+1)^4}\\\\\nR_n(x)&=(x+1)^{-n}\\sum_{m=0}^{n} (-1)^m\\binom{2n}{2m}x^{n-m}\n\\end{align}</math>\n\n== Partial fraction expansion ==\n\n:<math>R_n(x)=\\sum_{m=0}^{n} \\frac{(m!)^2}{(2m)!}\\binom{n+m-1}{m}\\binom{n}{m}\\frac{(-4)^m}{(x+1)^m} </math>\n\n== References ==\n*{{cite journal\n | first= Ben-Yu\n | last= Guo\n | authorlink = |first2=Jie |last2=Shen |first3=Zhong-Qing |last3=Wang\n | year = 2002\n | title = Chebyshev rational spectral and pseudospectral methods on a semi-infinite interval\n | journal = Int. J. Numer. Meth. Engng\n | volume = 53\n | issue = \n | pages = 65–84\n | doi = 10.1002/nme.392\n | id = \n | url = http://www.math.purdue.edu/~shen/pub/GSW_IJNME02.pdf\n | accessdate = 2006-07-25\n | citeseerx= 10.1.1.121.6069\n }}\n\n[[Category:Rational functions]]"
    },
    {
      "title": "Legendre rational functions",
      "url": "https://en.wikipedia.org/wiki/Legendre_rational_functions",
      "text": "[[Image:LegendreRational1.png|thumb|300px|Plot of the Legendre rational functions for n=0,1,2 and 3 for ''x'' between 0.01 and 100.]]\nIn [[mathematics]] the '''Legendre rational functions''' are a sequence of [[orthogonal functions]] on [0, ∞). They are obtained by composing the [[Cayley transform]] with [[Legendre polynomials]].\n\nA rational Legendre function of degree ''n'' is defined as:\n\n:<math>R_n(x) = \\frac{\\sqrt{2}}{x+1}\\,P_n\\left(\\frac{x-1}{x+1}\\right)</math>\n\nwhere <math>P_n(x)</math> is a Legendre polynomial. These functions are [[eigenfunction]]s of the singular [[Sturm-Liouville problem]]:\n\n:<math>(x+1)\\partial_x(x\\partial_x((x+1)v(x)))+\\lambda v(x)=0</math>\n\nwith eigenvalues\n\n:<math>\\lambda_n=n(n+1)\\,</math>\n\n== Properties==\n\nMany properties can be derived from the properties of the Legendre polynomials of the first kind. Other properties are unique to the functions themselves.\n\n=== Recursion ===\n\n:<math>R_{n+1}(x)=\\frac{2n+1}{n+1}\\,\\frac{x-1}{x+1}\\,R_n(x)-\\frac{n}{n+1}\\,R_{n-1}(x)\\quad\\mathrm{for\\,n\\ge 1}</math>\n\nand\n\n:<math>2(2n+1)R_n(x)=(x+1)^2(\\partial_x R_{n+1}(x)-\\partial_x R_{n-1}(x))+(x+1)(R_{n+1}(x)-R_{n-1}(x))</math>\n\n=== Limiting behavior ===\n[[Image:LegendreRational2.png|thumb|300px|Plot of the seventh order (''n=7'') Legendre rational function multiplied by ''1+x'' for ''x'' between 0.01 and 100. Note that there are ''n'' zeroes arranged symmetrically about ''x=1'' and if ''x''<sub>0</sub> is a zero, then ''1/x''<sub>0</sub> is a zero as well. These properties hold for all orders.]]\nIt can be shown that\n\n:<math>\\lim_{x\\rightarrow \\infty}(x+1)R_n(x)=\\sqrt{2}</math>\n\nand\n\n:<math>\\lim_{x\\rightarrow \\infty}x\\partial_x((x+1)R_n(x))=0</math>\n\n=== Orthogonality ===\n\n:<math>\\int_{0}^\\infty R_m(x)\\,R_n(x)\\,dx=\\frac{2}{2n+1}\\delta_{nm}</math>\n\nwhere <math>\\delta_{nm}</math> is the [[Kronecker delta]] function.\n\n== Particular values ==\n\n:<math>R_0(x)=1\\,</math>\n:<math>R_1(x)=\\frac{x-1}{x+1}\\,</math>\n:<math>R_2(x)=\\frac{x^2-4x+1}{(x+1)^2}\\,</math>\n:<math>R_3(x)=\\frac{x^3-9x^2+9x-1}{(x+1)^3}\\,</math>\n:<math>R_4(x)=\\frac{x^4-16x^3+36x^2-16x+1}{(x+1)^4}\\,</math>\n\n== References ==\n\n{{cite journal\n | last = Zhong-Qing\n | first = Wang\n | authorlink = \n |author2=Ben-Yu, Guo\n  | year = 2005\n | title = A mixed spectral method for incompressible viscous fluid flow in an infinite strip\n | journal = Mat. apl. comput.\n | volume = 24\n | issue = 3\n | pages = \n | doi = 10.1590/S0101-82052005000300002\n | id = \n | url = http://www.scielo.br/scielo.php?script=sci_arttext&pid=S0101-82052005000300002&lng=en&nrm=iso\n | format = PDF\n | accessdate = 2006-08-08\n }}\n\n[[Category:Rational functions]]"
    },
    {
      "title": "Linear fractional transformation",
      "url": "https://en.wikipedia.org/wiki/Linear_fractional_transformation",
      "text": "{{Use American English|date = January 2019}}\n{{Short description|Möbius transformation generalized to rings other than the complex numbers}}\n{{technical|date=March 2019}}\n\nIn [[mathematics]], a '''linear fractional transformation''' is, roughly speaking,  a transformation of the form\n:<math>z \\mapsto \\frac{az + b} {cz + d},</math> \nwhich has an [[inverse function|inverse]]. The precise definition depends on the nature of {{math|''a'', ''b'', ''c'', ''d''}}, and {{mvar|z}}. In other words, a linear fractional transformation is a ''[[transformation (function)|transformation]]'' that is represented by a ''fraction'' whose numerator and denominator are ''[[linear polynomial|linear]]''.\n\nIn the most basic setting, {{math|''a'', ''b'', ''c'', ''d''}}, and {{mvar|z}} are [[complex number]]s (in which case the transformation is also called a [[Möbius transformation]]), or more generally to a [[field (mathematics)|field]]. The invertibility condition is then {{math|''ad'' – ''bc'' ≠ 0}}. Over a field, a linear fractional transformation is the [[restriction (mathematics)|restriction]] to the field of a [[projective transformation]] or [[homography]] of the [[projective line]]. \n\nWhen {{math|''a'', ''b'', ''c'', ''d''}} are [[integer]] (or, more generally, belong to an [[integral domain]]), {{mvar|z}} is supposed to be a [[rational number]] (or to belong to the [[field of fractions]] of the integral domain. In this case, the invertibility condition is that {{math|''ad'' – ''bc''}} must be a [[unit (ring theory)|unit]] of the domain (that is {{val|1}} or {{val|-1}} in the case of integers).<ref>N. J. Young (1984) [http://www.sciencedirect.com/science/article/pii/0024379584901319 \"Linear fractional transformations in rings and modules\"], [[Linear Algebra and its Applications]] 56:251–90</ref>\n\nIn the most general setting, the {{math|''a'', ''b'', ''c'', ''d''}} and {{mvar|z}} are [[square matrices]], or, more generally, elements of a [[ring (mathematics)|ring]]. An example of such linear fractional transformation is the [[Cayley transform]], which was originally defined on the 3 x 3 real [[matrix ring]]. \n\nLinear fractional transformations are widely used in various areas of mathematics and its applications to engineering , such as classical [[geometry]], [[number theory]] (they are used, for example, in [[Wiles's proof of Fermat's Last Theorem]]), [[group theory]], [[control theory]].\n\n==General definition==\nIn general, a linear fractional transformation is a [[homography#Over a ring|homography]] of P(''A''), the [[projective line over a ring]] ''A''. When ''A'' is a [[commutative ring]], then a linear fractional transformation has the familiar form\n:<math>z \\mapsto \\frac{az + b} {cz + d},</math>\nwhere {{math|''a'', ''b'', ''c'', ''d''}} are elements of ''A'' such that {{math|''ad'' – ''bc''}} is a [[unit (ring theory)|unit]] of ''a'' (that is {{math|''ad'' – ''bc''}} has a [[multiplicative inverse]] in ''A'')\n\nIn a non-commutative ring ''A'', with (''z,t'') in ''A''<sup>2</sup>, the units ''u'' determine an [[equivalence relation]] <math>(z,t) \\sim (uz,ut) .</math> An [[equivalence class]] in the projective line over ''A'' is written U(''z,t''). Then linear fractional transformations act on the right of an element of P(''A''):\n:<math>U(z,t) \\begin{pmatrix}a & c \\\\ b & d \\end{pmatrix} = U(za + tb, zc + td) \\sim U((zc + td)^{-1}(za + tb), 1).</math>\nThe ring is embedded in its projective line by ''z'' → U(''z'',1), so ''t'' = 1 recovers the usual expression. This linear fractional transformation is well-defined since U(''za'' + ''tb'', ''zc'' + ''td'') does not depend on which element is selected from its equivalence class for the operation.\n\nThe linear fractional transformations form a [[group (mathematics)|group]], denoted <math>\\operatorname{PGL}_1(A). </math>\n\nThe group <math>\\operatorname{PGL}_1(\\Z)</math> of the linear fractional transformations is called the [[modular group]]. It has been widely studied because its numerous applications to [[number theory]], which include, in particular, [[Wiles's proof of Fermat's Last Theorem]].\n\n== Use in higher mathematics ==\nIn mathematics, the most basic setting for linear fractional transforms is the [[Möbius transformation]], which commonly appears in the theory of [[continued fraction]]s, and in [[analytic number theory]] of [[elliptic curve]]s and [[modular form]]s, as it describes the automorphisms of the [[upper half-plane]] under the action of the [[modular group]]. It also provides a canonical example of [[Hopf fibration]], where the [[geodesic flow]] induced by the linear fractional transformation decomposes complex projective space into [[stable manifold|stable and unstable manifolds]], with the [[horocycle]]s appearing perpendicular to the geodesics. See [[Anosov flow]] for a worked example of the fibration: in this example, the geodesics are given by the fractional linear transform\n\n:<math>\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} \\cdot i\\exp(t) = \\frac{ai\\exp(t)+b}{ci\\exp(t)+d} </math>\n\nwith ''a'', ''b'', ''c'' and ''d'' real, with <math>ad-bc=1</math>.  Roughly speaking, the [[center manifold]] is generated by the [[parabolic transformation]]s, the unstable manifold by the hyperbolic transformations, and the stable manifold by the elliptic transformations. \n\n== Use in control theory == \nLinear fractional transformations are widely used in [[control theory]] to solve plant-controller relationship problems in [[mechanical engineering|mechanical]] and [[electrical engineering]].<ref>John Doyle, Andy Packard, Kemin Zhou, \"Review of LFTs, LMIs, and mu\", (1991) ''Proceedings of the 30th Conference on Decision and Control'' [http://www.cds.caltech.edu/~doyle/wiki/images/7/70/CDC1991.pdf]</ref><ref>Juan C. Cockburn, \"Multidimensional Realizations of Systems with Parametric Uncertainty\" [https://www.math.ucsd.edu/~helton/MTNSHISTORY/CONTENTS/2000PERPIGNAN/CDROM/articles/SI20A_4.pdf]</ref> The general procedure of combining linear fractional transformations with the [[Redheffer star product]] allows them to be applied to the [[scattering theory]] of general differential equations, including the [[S-matrix]] approach in quantum mechanics and quantum field theory, the scattering of acoustic waves in media (e.g. thermoclines and submarines in oceans, etc.) and the general analysis of scattering and bound states in differential equations. Here, the 3x3 matrix components refer to the incoming, bound and outgoing states. Perhaps the simplest example application of linear fractional transformations occurs in the analysis of the [[damped harmonic oscillator]]. Another elementary application is obtaining the [[Frobenius normal form]], i.e. the [[companion matrix]] of a polynomial.\n\n==Conformal property==\nThe commutative rings of [[split-complex number]]s and [[dual number]]s join the ordinary [[complex number]]s as rings that express angle. In each case the [[exponential map (Lie theory)|exponential map]] applied to the imaginary axis produces an [[group isomorphism|isomorphism]] between [[one-parameter group]]s in (''A'', + ) and in the [[group of units]] (''U'', × ):\n:<math>\\exp(y j) = \\cosh y + j \\sinh y, \\quad j^2 = +1 ,</math>\n:<math>\\exp(y \\epsilon) = 1 + y \\epsilon, \\quad \\epsilon^2 = 0 ,</math>\n:<math>\\exp(y i) = \\cos y + i \\sin y, \\quad i^2 = -1 .</math>\nThe \"angle\" ''y'' is [[hyperbolic angle]], [[slope]], or [[angle|circular angle]] according to the host ring.\n\nA linear fractional transformation can be [[generator (mathematics)|generated]] by [[multiplicative inverse|multiplicative inversion]] ''z'' → 1/''z'' and [[affine transformation]]s  ''z'' → ''a z'' + ''b''. Conformality can be confirmed by showing the generators are all conformal. The translation ''z'' → ''z'' + ''b'' is a change of origin and makes no difference to angle. To see that ''z'' → ''az'' is conformal, consider the [[polar decomposition#Alternative planar decompositions|polar decomposition]] of ''a'' and ''z''. In each case the angle of ''a'' is added to that of ''z'' giving a [[conformal map]]. Finally, inversion is conformal since ''z'' → 1/''z'' sends\n<math>\\exp(y b) \\mapsto \\exp(-y b), \\quad b^2 = 1, 0, -1 .</math>\n\n== See also==\n* [[Linear-fractional programming]]\n* [[H-infinity methods in control theory]]\n\n==References==\n{{Reflist}}\n* B.A. Dubrovin, A.T. Fomenko, S.P. Novikov (1984) ''Modern Geometry — Methods and Applications'', volume 1, chapter 2, §15 Conformal transformations of Euclidean and Pseudo-Euclidean spaces of several dimensions,  [[Springer-Verlag]] {{isbn|0-387-90872-2}}.\n* Geoffry Fox (1949) ''Elementary Theory of a hypercomplex variable and the theory of conformal mapping in the hyperbolic plane'', Master’s thesis, [[University of British Columbia]].\n* P.G. Gormley (1947) \"Stereographic projection and the linear fractional group of transformations of quaternions\", [[Proceedings of the Royal Irish Academy]], Section A 51:67–85.\n* A.E. Motter & M.A.F. Rosa (1998) \"Hyperbolic calculus\", [[Advances in Applied Clifford Algebras]] 8(1):109 to 28, §4 Conformal transformations, page 119.\n* Tsurusaburo Takasu (1941) [http://projecteuclid.org/euclid.pja/1195578674 Gemeinsame Behandlungsweise der elliptischen konformen, hyperbolischen konformen und parabolischen konformen Differentialgeometrie, 2], [[Japan Academy|Proceedings of the Imperial Academy]] 17(8): 330–8, link from [[Project Euclid]], {{mr|id=14282}}\n* [[Isaak Yaglom]] (1968) ''Complex Numbers in Geometry'', page 130 & 157, [[Academic Press]]\n\n[[Category:Rational functions]]\n[[Category:Conformal mapping]]\n[[Category:Projective geometry]]"
    },
    {
      "title": "Heaviside cover-up method",
      "url": "https://en.wikipedia.org/wiki/Heaviside_cover-up_method",
      "text": "The '''Heaviside cover-up method''', named after [[Oliver Heaviside]], is one possible approach in determining the coefficients when performing the [[partial fraction expansion|partial-fraction expansion]] of a [[rational function]].<ref>Calculus and Analytic Geometry 7th Edition, Thomas/Finney, 1988, pp. 482-489</ref>\n\n== Method ==\n\nSeparation of a fractional algebraic expression into partial fractions is the reverse of the process of combining fractions by converting each fraction to the lowest common denominator (LCM) and adding the numerators.  This separation can be accomplished by the Heaviside cover-up method, another method for determining the coefficients of a partial fraction. Case one has fractional expressions where factors in the denominator are unique. Case two has fractional expressions where some factors may repeat as powers of a binomial.\n\nIn integral calculus we would want to write a fractional algebraic expression as the sum of its partial fractions in order to take the integral of each simple fraction separately.  Once the original denominator, D<sub>0</sub>, has been factored we '''set up a fraction for each factor in the denominator'''.  We may use a subscripted D to represent the denominator of the respective partial fractions which are the factors in D<sub>0</sub>.  Letters A, B, C, D, E, and so on will represent the '''numerators''' of the respective partial fractions.  When a partial fraction term has a single (i.e. unrepeated) binomial in the denominator, the numerator is a [[residue (complex analysis)|residue]] of the function defined by the input fraction.\n\nWe calculate each respective numerator by (1) taking the root of the denominator (i.e. the value of ''x'' that makes the denominator zero) and (2) then substituting this root into the original expression but ignoring the corresponding factor in the denominator.  Each root for the variable is the value which would give an undefined value to the expression since we do not divide by zero.\n\n'''General formula for a cubic denominator with three distinct roots''':\n\n: <math>\\frac{\\ell x^2 + mx + n}{(x-a)(x-b)(x-c)} = \\frac{A}{(x-a)} + \\frac{B}{(x-b)} + \\frac{C}{(x-c)}</math>\n\nWhere\n\n: <math>A =\\frac{\\ell a^2 + ma + n}{(a-b)(a-c)};</math>\n\nand where\n\n: <math>B = \\frac{\\ell b^2 + mb + n}{(b-c)(b-a)};</math>\n\nand where\n\n: <math>C = \\frac{\\ell c^2 + mc + n}{(c-a)(c-b)}.</math>\n\n=== Case one ===\n\nFactorize the expression in the denominator. Set up a partial fraction for each factor in the denominator. Apply the cover-up rule to solve for the new numerator of each partial fraction.\n\n==== Example ====\n\n: <math>\\frac{3x^2 + 12x + 11}{(x+1)(x+2)(x+3)} = \\frac{A}{x+1} + \\frac{B}{x+2} + \\frac{C}{x+3}</math>\n\nSet up a partial fraction for each factor in the denominator. With this framework we apply the cover-up rule to solve for ''A'', ''B'', and ''C''.\n\n1.  ''D''<sub>1</sub> is ''x'' + 1; set it equal to zero.  This gives the residue for ''A'' when ''x'' = &minus;1.\n\n2.  Next, substitute this value of x into the fractional expression, but without ''D''<sub>1</sub>.\n\n3. Put this value down as the value of ''A''.\n\nProceed similarly for ''B'' and ''C''.\n\n''D''<sub>2</sub> is ''x'' + 2; For the residue ''B'' use ''x'' = &minus;2.\n\n''D''<sub>3</sub> is ''x'' + 3; For residue ''C'' use ''x'' = &minus;3.\n\nThus, to solve for ''A'', use ''x'' = &minus;1 in the expression but without ''D''<sub>1</sub>:\n\n: <math>\\frac{3x^2 + 12x + 11}{(x+2)(x+3)} = \\frac{3 -12 +11}{(1)(2)} = \\frac{2}{2} = 1 = A.</math>\n\nThus, to solve for ''B'', use ''x'' = &minus;2 in the expression but without ''D''<sub>2</sub>:\n\n: <math>\\frac{3x^2 + 12x + 11}{(x+1)(x+3)} = \\frac{12 -24 +11}{(-1)(1)} = \\frac{-1}{(-1)} = +1 = B.</math>\n\nThus, to solve for ''C'', use ''x'' = &minus;3 in the expression but without ''D''<sub>3</sub>:\n\n: <math>\\frac{3x^2 + 12x + 11}{(x+1)(x+2)} = \\frac{27 -36 +11}{(-2)(-1)} = \\frac{2}{(+2)} = +1 = C.</math>\n\nThus,\n\n: <math>\\frac{3x^2 + 12x + 11}{(x+1)(x+2)(x+3)} = \\frac{1}{x+1} + \\frac{1}{x+2} + \\frac{1}{x+3}</math>\n\n=== Case two ===\n\nWhen factors of the denominator include powers of one expression we\n\n# Set up a partial fraction for each unique factor and each lower power of D;\n# Set up an equation showing the '''relation of the numerators''' if all were converted to the LCD.\n\nFrom the equation of numerators we solve for each numerator, A, B, C, D, and so on.\nThis equation of the numerators is an absolute identity, true for all values of x.  So, we may select any value of x and solve for the numerator.\n\n==== Example ====\n\n: <math>\\frac{3x + 5}{(1-2x)^2} = \\frac{A}{(1-2x)^2} + \\frac{B}{1-2x}</math>\n\nHere, we set up a partial fraction for each descending power of the denominator. Then we solve for the numerators, A and B. As <math>(1-2x)</math> is a repeated factor, we now need to find two numbers, as so we need an additional relation in order to solve for both.\nTo write '''the relation of numerators''' the second fraction needs another factor of <math>(1-2x)</math> to convert it to the LCD, giving us <math>3x+ 5 = A + B(1-2x) </math>.  In general, if a binomial factor is raised to the power of <math>n</math>, then <math>n</math> constants <math>A_k</math> will be needed, each appearing divided by successive powers, <math>(1-2x)^k</math>, where <math>k</math> runs from 1 to <math>n</math>.  The cover-up rule can be used to find <math>A_n</math>, but is still <math>A_1</math> that is called the '''residue'''.  Here, <math>n = 2</math>, <math>A = A_2</math>, and <math>B = A_1</math>\n\n'''To solve for <math>A</math>&nbsp;:'''\n\n<math>A</math> can be solved by setting the denominator of the first fraction to zero, <math>1-2x = 0</math>.\n\nSolving for <math>x</math> gives the cover-up value for <math>A</math>: when <math>x=1/2</math>.\n\nWhen we substitute this value, <math>x=1/2</math>, we get:\n\n: <math>3\\left(\\frac{1}{2}\\right)+5=A+B(0)</math>\n\n: <math>A=\\frac{3}{2}+5=\\frac{13}{2}</math>\n\n'''To solve for <math>B</math>&nbsp;:'''\n\nSince the equation of the numerators, here, <math>3x+5=A+B(1-2x)</math>, is true for '''all values of <math>x</math>''', pick a value for <math>x</math> and use it to solve for <math>B</math>.\n\nAs we have solved for the value of <math>A</math> above, <math>A=13/2</math>, we may use that value to solve for <math>B</math>.\n\nWe may pick <math>x=0</math>&nbsp;, use <math>A=13/2</math>&nbsp;, and then solve for <math>B</math>&nbsp;:\n\n: <math>\n\\begin{align}\n3x + 5 &= A+B(1-2x) \\\\\n0 + 5 &= \\frac{13}{2} + B(1 + 0) \\\\\n\\frac{10}{2} &= \\frac{13}{2} + B \\\\\n-\\frac{3}{2} &= B \\\\\n\\end{align}\n</math>\n\nWe may pick <math>x=1</math>&nbsp;, Then solve for <math>B</math>&nbsp;:\n\n: <math>\n\\begin{align}\n3x + 5 &=A+B(1-2x) \\\\\n3 + 5 &= \\frac{13}{2} + B(1 - 2) \\\\\n8 &= \\frac{13}{2} + B(-1) \\\\\n\\frac{16}{2} &= \\frac{13}{2} - B \\\\\nB &= -\\frac{3}{2}\n\\end{align}\n</math>\n\nWe may pick <math>x=-1</math>&nbsp;. Solve for <math>B</math>&nbsp;:\n\n: <math>\n\\begin{align}\n3x + 5 &=A+B(1-2x) \\\\\n-3 + 5 &= \\frac{13}{2} + B(1 + 2) \\\\\n\\frac{4}{2}   &= \\frac{13}{2}  + 3B \\\\\n-\\frac{9}{2}  &= 3B \\\\\n-\\frac{3}{2}  &= B\n\\end{align}\n</math>\n\nHence,\n\n: <math>\\frac{3x + 5}{(1-2x)^2} = \\frac{13/2}{(1-2x)^2} + \\frac{-3/2}{(1-2x)},</math>\n\nor\n\n: <math>\\frac{3x + 5}{(1-2x)^2} = \\frac{13}{2(1-2x)^2} - \\frac{3}{2(1-2x)}</math>\n\n==References==\n{{reflist}}\n\n==External links==\n* http://www.math-cs.gordon.edu/courses/ma225/handouts/heavyside.pdf\n* [http://math.mit.edu/~jorloff/suppnotes/suppnotes03/ MIT 18.03 Notes] on [http://math.mit.edu/~jorloff/suppnotes/suppnotes03/h.pdf Heaviside’s Cover-up Method] by Prof. [[Arthur Mattuck]].\n\n\n[[Category:Partial fractions]]"
    },
    {
      "title": "Partial fractions in complex analysis",
      "url": "https://en.wikipedia.org/wiki/Partial_fractions_in_complex_analysis",
      "text": "In [[complex analysis]], a '''partial fraction expansion''' is a way of writing a [[meromorphic function]]  ''f(z)'' as an infinite sum of [[rational functions]] and [[polynomials]]. When ''f(z)'' is a rational function, this reduces to the usual [[partial fractions|method of partial fractions]].\n\n==Motivation==\n\nBy using [[polynomial long division]] and the partial fraction technique from algebra, any rational function can be written as a sum of terms of the form ''1 / (az + b)<sup>k</sup>'' + ''p(z)'', where ''a'' and ''b'' are complex, ''k'' is an integer, and ''p(z)'' is a polynomial. Just as [[polynomial factorization]] can be generalized to the [[Weierstrass factorization theorem]], there is an analogy to partial fraction expansions for certain meromorphic functions.\n\nA proper rational function, i.e. one for which the [[degree of a polynomial|degree]] of the denominator is greater than the degree of the numerator, has a partial fraction expansion with no polynomial terms. Similarly, a meromorphic function ''f(z)'' for which |''f(z)''| goes to 0 as ''z'' goes to infinity at least as quickly as |''1/z''|, has an expansion with no polynomial terms.\n\n==Calculation==\n\nLet ''f(z)'' be a function meromorphic in the finite complex plane with [[pole (complex analysis)|poles]] at ''&lambda;<sub>1</sub>'', ''&lambda;<sub>2</sub>'', ..., and let (''&Gamma;<sub>1</sub>'', ''&Gamma;<sub>2</sub>'', ...) be a sequence of simple closed curves such that:\n\n* The origin lies inside each curve ''&Gamma;<sub>k</sub>''\n* No curve passes through a pole of ''f''\n* ''&Gamma;<sub>k</sub>'' lies inside ''&Gamma;<sub>k+1</sub>'' for all ''k''\n* <math>\\lim_{k\\rightarrow \\infty} d(\\Gamma_k) = \\infty</math>, where ''d(&Gamma;<sub>k</sub>)'' gives the distance from the curve to the origin\n\nSuppose also that there exists an integer ''p'' such that\n\n:<math>\\lim_{k\\rightarrow \\infty} \\oint_{\\Gamma_k} \\left|\\frac{f(z)}{z^{p+1}}\\right| |dz| < \\infty</math>\n\nWriting PP(''f(z)''; ''z = &lambda;<sub>k</sub>'') for the [[principal part]] of the [[Laurent expansion]] of ''f'' about the point ''&lambda;<sub>k</sub>'', we have\n\n:<math>f(z) = \\sum_{k=0}^{\\infty} \\operatorname{PP}(f(z); z = \\lambda_k),</math>\n\nif ''p = -1'', and if ''p > -1'',\n\n:<math>f(z) = \\sum_{k=0}^{\\infty} (\\operatorname{PP}(f(z); z = \\lambda_k) + c_{0,k} + c_{1,k}z + \\cdots + c_{p,k}z^p),</math>\n\nwhere the coefficients ''c<sub>j,k</sub>'' are given by\n\n:<math>c_{j,k} = \\operatorname{Res}_{z=\\lambda_k} \\frac{f(z)}{z^{j+1}}</math>\n\n&lambda;<sub>0</sub> should be set to 0, because even if ''f(z)'' itself does not have a pole at 0, the [[residue (complex analysis)|residue]]s of ''f(z)/z<sup>j+1</sup>'' at ''z'' = 0 must still be included in the sum.\n\nNote that in the case of &lambda;<sub>0</sub> = 0, we can use the Laurent expansion of ''f(z)'' about the origin to get\n\n:<math>f(z) = \\frac{a_{-m}}{z^m} + \\frac{a_{-m+1}}{z^{m-1}} + \\cdots + a_0 + a_1 z + \\cdots</math>\n:<math>c_{j,k} = \\operatorname{Res}_{z=0} \\left(\\frac{a_{-m}}{z^{m+j+1}} + \\frac{a_{-m+1}}{z^{m+j}} + \\cdots + \\frac{a_j}{z} + \\cdots\\right) = a_j,</math>\n:<math>\\sum_{j=0}^p c_{j,k}z^j = a_0 + a_1 z + \\cdots + a_p z^p</math>\n\nso that the polynomial terms contributed are exactly the [[regular part]] of the Laurent series up to ''z<sup>p</sup>''.\n\nFor the other poles ''&lambda;<sub>k</sub>'' where ''k'' &ge; 1, ''1/z<sup>j+1</sup>'' can be pulled out of the [[residue (complex analysis)|residue]] calculations:\n\n:<math>c_{j,k} = \\frac{1}{\\lambda_k^{j+1}} \\operatorname{Res}_{z=\\lambda_k} f(z)</math>\n:<math>\\sum_{j=0}^p c_{j,k}z^j = [\\operatorname{Res}_{z=\\lambda_k} f(z)] \\sum_{j=0}^p \\frac{1}{\\lambda_k^{j+1}} z^j</math>\n\nTo avoid issues with convergence, the poles should be ordered so that if &lambda;<sub>k</sub> is inside &Gamma;<sub>n</sub>, then &lambda;<sub>j</sub> is also inside &Gamma;<sub>n</sub> for all ''j'' < ''k''.\n\n==Example==\n\nThe simplest examples of meromorphic functions with an infinite number of poles are the non-entire trigonometric functions, so take the function tan(''z''). tan(''z'') is meromorphic with poles at ''(n + 1/2)&pi;'', ''n'' = 0, &plusmn;1, &plusmn;2, ... The contours ''&Gamma;<sub>k</sub>'' will be squares with vertices at ''&plusmn;&pi;k &plusmn; &pi;ki'' traversed counterclockwise, ''k'' > 1, which are easily seen to satisfy the necessary conditions.\n\nOn the horizontal sides of ''&Gamma;<sub>k</sub>'',\n\n:<math>z = t \\pm \\pi k i,\\ \\ t \\in [-\\pi k, \\pi k],</math>\n\nso\n\n:<math>|\\tan(z)|^2 = \\left|\\frac{\\sin(t)\\cosh(\\pi k) \\pm i\\cos(t)\\sinh(\\pi k)}{\\cos(t)\\cosh(\\pi k) \\pm i\\sin(t)\\sinh(\\pi k)}\\right|^2</math>\n\n:<math>|\\tan(z)|^2 = \\frac{\\sin^2(t)\\cosh^2(\\pi k) + \\cos^2(t)\\sinh^2(\\pi k)}{\\cos^2(t)\\cosh^2(\\pi k) + \\sin^2(t)\\sinh^2(\\pi k)}</math>\n\nsinh(''x'') < cosh(''x'') for all real ''x'', which yields\n\n:<math>|\\tan(z)|^2 < \\frac{\\cosh^2(\\pi k)(\\sin^2(t) + \\cos^2(t))}{\\sinh^2(\\pi k)(\\cos^2(t) + \\sin^2(t))} = \\coth^2(\\pi k)</math>\n\nFor ''x'' > 0, coth(''x'') is continuous, decreasing, and bounded below by 1, so it follows that on the horizontal sides of ''&Gamma;<sub>k</sub>'', |tan(''z'')| < coth(''&pi;''). Similarly, it can be shown that |tan(''z'')| < 1 on the vertical sides of ''&Gamma;<sub>k</sub>''.\n\nWith this bound on |tan(''z'')| we can see that\n\n:<math>\\oint_{\\Gamma_k} \\left|\\frac{\\tan(z)}{z}\\right| dz \\le \\operatorname{length}(\\Gamma_k) \\max_{z\\in \\Gamma_k} \\left|\\frac{\\tan(z)}{z}\\right| < 8k \\pi \\frac{\\coth(\\pi)}{k\\pi} = 8\\coth(\\pi) < \\infty.</math>\n\n(The maximum of |1/''z''| on ''&Gamma;<sub>k</sub>'' occurs at the minimum of |''z''|, which is ''k&pi;'').\n\nTherefore ''p'' = 0, and the partial fraction expansion of tan(''z'') looks like\n\n:<math>\\tan(z) = \\sum_{k=0}^{\\infty} (\\operatorname{PP}(\\tan(z); z = \\lambda_k) + \\operatorname{Res}_{z=\\lambda_k} \\frac{\\tan(z)}{z}).</math>\n\nThe principal parts and [[residue (complex analysis)|residue]]s are easy enough to calculate, as all the poles of tan(''z'') are simple and have residue -1:\n\n:<math>\\operatorname{PP}(\\tan(z); z = (n + \\frac{1}{2})\\pi) = \\frac{-1}{z - (n + \\frac{1}{2})\\pi}</math>\n:<math>\\operatorname{Res}_{z=(n + \\frac{1}{2})\\pi} \\frac{\\tan(z)}{z} = \\frac{-1}{(n + \\frac{1}{2})\\pi}</math>\n\nWe can ignore ''&lambda;<sub>0</sub>'' = 0, since both tan(''z'') and tan(''z'')/''z'' are analytic at 0, so there is no contribution to the sum, and ordering the poles ''&lambda;<sub>k</sub>'' so that ''&lambda;<sub>1</sub>'' = ''&pi;''/2, ''&lambda;<sub>2</sub>'' = -''&pi;''/2, ''&lambda;<sub>3</sub>'' = 3''&pi;''/2, etc., gives\n\n:<math>\\tan(z) = \\sum_{k=0}^{\\infty} \\left[\\left(\\frac{-1}{z - (k + \\frac{1}{2})\\pi} - \\frac{1}{(k + \\frac{1}{2})\\pi}\\right) + \\left(\\frac{-1}{z + (k + \\frac{1}{2})\\pi} + \\frac{1}{(k + \\frac{1}{2})\\pi}\\right)\\right]</math>\n:<math>\\tan(z) = \\sum_{k=0}^{\\infty} \\frac{-2z}{z^2 - (k + \\frac{1}{2})^2\\pi^2}</math>\n\n==Applications==\n\n===Infinite products===\n\nBecause the partial fraction expansion often yields sums of ''1/(a+bz)'', it can be useful in finding a way to write a function as an [[infinite product]]; integrating both sides gives a sum of logarithms, and exponentiating gives the desired product:\n\n:<math>\\tan(z) = -\\sum_{k=0}^{\\infty} \\left(\\frac{1}{z - (k + \\frac{1}{2})\\pi} + \\frac{1}{z + (k + \\frac{1}{2})\\pi}\\right)</math>\n:<math>\\int_0^z \\tan(w) dw = \\log \\sec z</math>\n:<math>\\int_0^z \\frac{1}{w \\pm (k + \\frac{1}{2})\\pi} dw = \\log\\left(1 \\pm \\frac{z}{(k + \\frac{1}{2})\\pi}\\right)</math>\n\nApplying some logarithm rules,\n\n:<math>\\log \\sec z = -\\sum_{k=0}^{\\infty} \\left(\\log\\left(1 - \\frac{z}{(k + \\frac{1}{2})\\pi}\\right) + \\log\\left(1 + \\frac{z}{(k + \\frac{1}{2})\\pi}\\right)\\right)</math>\n:<math>\\log \\cos z = \\sum_{k=0}^{\\infty} \\log\\left(1 - \\frac{z^2}{(k + \\frac{1}{2})^2\\pi^2}\\right),</math>\n\nwhich finally gives\n\n:<math>\\cos z = \\prod_{k=0}^{\\infty} \\left(1 - \\frac{z^2}{(k + \\frac{1}{2})^2\\pi^2}\\right).</math>\n\n===Laurent series===\nThe partial fraction expansion for a function can also be used to find a Laurent series for it by simply replacing the rational functions in the sum with their Laurent series, which are often not difficult to write in closed form. This can also lead to interesting identities if a Laurent series is already known.\n\nRecall that\n\n:<math>\\tan(z) = \\sum_{k=0}^{\\infty} \\frac{-2z}{z^2 - (k + \\frac{1}{2})^2\\pi^2} = \\sum_{k=0}^{\\infty} \\frac{-8z}{4z^2 - (2k + 1)^2\\pi^2}.</math>\n\nWe can expand the summand using a geometric series:\n\n:<math>\\frac{-8z}{4z^2 - (2k + 1)^2\\pi^2} = \\frac{8z}{(2k + 1)^2\\pi^2} \\frac{1}{1 - (\\frac{2z}{(2k + 1)\\pi})^2} = \\frac{8}{(2k + 1)^2\\pi^2}\\sum_{n=0}^{\\infty} \\frac{2^{2n}}{(2k + 1)^{2n}\\pi^{2n}} z^{2n + 1}.</math>\n\nSubstituting back,\n:<math>\\tan(z) = 2\\sum_{k=0}^{\\infty} \\sum_{n=0}^{\\infty} \\frac{2^{2n+2}}{(2k + 1)^{2n+2}\\pi^{2n+2}} z^{2n + 1},</math>\n\nwhich shows that the coefficients ''a<sub>n</sub>'' in the Laurent (Taylor) series of ''tan(z)'' about ''z'' = 0 are\n\n:<math>a_{2n+1} = \\frac{T_{2n+1}}{(2n+1)!} = \\frac{2^{2n+3}}{\\pi^{2n+2}} \\sum_{k=0}^{\\infty} \\frac{1}{(2k + 1)^{2n+2}}</math>\n:<math>a_{2n} = \\frac{T_{2n}}{(2n)!} = 0,</math>\n\nwhere ''T<sub>n</sub>'' are the [[tangent numbers]].\n\nConversely, we can compare this formula to the Taylor expansion for tan(''z'') about z = 0 to calculate the infinite sums:\n\n:<math>\\tan(z) = z + \\frac{1}{3}z^3 + \\frac{2}{15}z^5 + \\cdots</math>\n:<math>\\sum_{k=0}^{\\infty} \\frac{1}{(2k + 1)^2} = \\frac{\\pi^2}{2^3} = \\frac{\\pi^2}{8}</math>\n:<math>\\sum_{k=0}^{\\infty} \\frac{1}{(2k + 1)^4} = \\frac{1}{3} \\frac{\\pi^4}{2^5} = \\frac{\\pi^4}{96}.</math>\n\n==See also==\n* [[Partial fraction]]\n* [[Line integral]]\n* [[Residue (complex analysis)]]\n* [[Residue theorem]]\n\n==References==\n* Markushevich, A.I. ''Theory of functions of a complex variable''. Trans. Richard A. Silverman. Vol. 2. Englewood Cliffs, N.J.: Prentice-Hall, 1965.\n\n[[Category:Complex analysis]]\n[[Category:Partial fractions]]"
    },
    {
      "title": "Laurent series",
      "url": "https://en.wikipedia.org/wiki/Laurent_series",
      "text": "{{Short description|Power series generalized to allow negative powers}}\n{{about|doubly infinite power series|power series with finitely many negative exponents|Formal Laurent series}}\n\n[[Image:Laurent series.svg|frame|right|A Laurent series is defined with respect to a particular point ''c'' and a path of integration γ. The path of integration must lie in an annulus, indicated here by the red color, inside which ''f''(''z'') is [[holomorphic function|holomorphic]] ([[Holomorphic functions are analytic|analytic]]).]]\n\nIn [[mathematics]], the '''Laurent series''' of a complex function ''f''(''z'') is a representation of that function as a [[power series]] which includes terms of negative degree. It may be used to express complex functions in cases where a [[Taylor series]] expansion cannot be applied. The Laurent series was named after and first published by [[Pierre Alphonse Laurent]] in 1843.  [[Karl Weierstrass]] may have discovered it first in a paper written in 1841, but it was not published until after his death.<ref>{{citation|title=Complex Analysis: In the Spirit of Lipman Bers|volume=245|series=Graduate Texts in Mathematics|first1=Rubi|last1=Rodriguez|first2=Irwin|last2=Kra|first3=Jane P.|last3=Gilman|author3-link=Jane Piore Gilman|publisher=Springer|year=2012|isbn=9781441973238|page=12|url=https://books.google.com/books?id=fZbf629lTy0C&pg=PA12}}.</ref>\n\nThe Laurent series for a complex function ''f''(''z'') about a point ''c'' is given by:\n\n:<math>f(z)=\\sum_{n=-\\infty}^\\infty a_n(z-c)^n</math>\n\nwhere ''a<sub>n</sub> and c'' are constants, with ''a<sub>n</sub>'' defined by a [[line integral]] that generalizes [[Cauchy's integral formula]]:\n\n:<math>a_n=\\frac{1}{2\\pi i} \\oint_\\gamma \\frac{f(z)}{(z-c)^{n+1}}\\,dz.</math>\n\nThe path of integration <math>\\gamma</math> is counterclockwise around a [[Jordan curve]] enclosing ''c'' and lying in an [[annulus (mathematics)|annulus]] ''A'' in which <math>f(z)</math> is [[holomorphic function|holomorphic]] (analytic). The expansion for <math>f(z)</math> will then be valid anywhere inside the annulus. The annulus is shown in red in the figure on the right, along with an example of a suitable path of integration labeled <math>\\gamma</math>. If we take <math>\\gamma</math> to be a circle <math> |z-c| = \\varrho</math>, where <math>r < \\varrho < R</math>, this just amounts\nto computing the complex [[Fourier coefficients]] of the restriction of <math>f</math> to <math>\\gamma</math>. The fact that these\nintegrals are unchanged by a deformation of the contour <math>\\gamma</math> is an immediate consequence of [[Green's theorem]]. \n\nIn practice, the above integral  formula may not offer the most practical method for computing the coefficients\n<math>a_n</math> for a given function <math>f(z)</math>; instead, one often pieces together the Laurent\nseries by combining known Taylor expansions.\nBecause the Laurent expansion of a function is [[Unique (mathematics)|unique]] whenever\nit exists, any  expression of this form that actually equals the given function \n<math>f(z)</math> in some annulus must actually be the \nLaurent expansion of <math>f(z)</math>.\n\n== Convergent Laurent series ==\n\nLaurent series with complex coefficients are an important tool in [[complex analysis]], especially to investigate the behavior of functions near [[mathematical singularity|singularities]].\n[[Image:Expinvsqlau SVG.svg|right|thumb|''e''<sup>&minus;(1/''x''<sup>2</sup>)</sup> and Laurent approximations: see text for key. As the negative degree of the Laurent series rises, it approaches the correct function.]]\n[[Image:Expinvsqlau GIF.gif|right|thumb|''e''<sup>&minus;(1/''x''<sup>2</sup>)</sup> and its Laurent approximations with the negative degree rising. The neighborhood around the zero singularity can never be approximated.]]\n\nConsider for instance the function <math>f(x) = e^{-1/x^2}</math> with <math>f(0) = 0</math>. As a real function, it is infinitely differentiable everywhere; as a complex function however it is not differentiable at ''x'' = 0. By replacing ''x'' with &minus;1/''x''<sup>2</sup> in the [[power series]] for the [[exponential function]], we obtain its Laurent series which converges and is equal to ''f''(''x'') for all complex numbers ''x'' except at the singularity ''x'' = 0. The graph opposite shows ''e''<sup>&minus;1/''x''<sup>2</sup></sup> in black and its Laurent approximations\n\n:<math>\\sum_{n=0}^N(-1)^n\\,{x^{-2n}\\over n!}</math>\n\nfor ''N'' = <span style=\"color:#b30000;\">1</span>, <span style=\"color:#00b300;\">2</span>, <span style=\"color:#0000b3;\">3</span>, <span style=\"color:#b3b300;\">4</span>, <span style=\"color:#00b3b3;\">5</span>, <span style=\"color:#b300b3;\">6</span>, <span style=\"color:#b3b3b3;\">7</span> and <span style=\"color:#33b300;\">50</span>. As ''N'' → ∞, the approximation becomes exact for all (complex) numbers ''x'' except at the singularity ''x'' = 0.\n\nMore generally, Laurent series can be used to express [[holomorphic function]]s defined on an [[Annulus (mathematics)|annulus]], much as [[power series]] are used to express holomorphic functions defined on a [[Disk (mathematics)|disc]].\n\nSuppose \n\n:<math>\\sum_{n=-\\infty}^\\infty a_n ( z - c )^n</math>\n\nis a given Laurent series with complex coefficients ''a''<sub>''n''</sub> and a complex center ''c''. Then there exists a [[unique (mathematics)|unique]] inner radius <var>r</var> and outer radius ''R'' such that:\n* The Laurent series converges on the open annulus ''A''&nbsp;≡ {''z''&nbsp;: ''r''&nbsp;<&nbsp;|''z''&nbsp;&minus;&nbsp;''c''|&nbsp;<&nbsp;''R''}. To say that the Laurent series converges, we mean that both the positive degree power series and the negative degree power series converge. Furthermore, this convergence will be [[uniform convergence|uniform]] on [[compact set]]s. Finally, the convergent series defines a [[holomorphic function]] ''f''(''z'') on the open annulus.\n* Outside the annulus, the Laurent series diverges. That is, at each point of the [[exterior (topology)|exterior]] of ''A'', the positive degree power series or the negative degree power series diverges.\n* On the [[boundary (topology)|boundary]] of the annulus, one cannot make a general statement, except to say that there is at least one point on the inner boundary and one point on the outer boundary such that ''f''(''z'') cannot be holomorphically continued to those points.\n\nIt is possible that ''r'' may be zero or ''R'' may be infinite; at the other extreme, it's not necessarily true that ''r'' is less than ''R''.\nThese radii can be computed as follows:\n\n:<math>\\begin{align}\n            r &= \\limsup_{n\\rightarrow\\infty} |a_{-n}|^\\frac{1}{n} \\\\\n  {1 \\over R} &= \\limsup_{n\\rightarrow\\infty} |a_n|^\\frac{1}{n}.\n\\end{align}</math>\n\nWe take ''R'' to be infinite when this latter [[limit superior|lim sup]] is zero.\n\nConversely, if we start with an annulus of the form ''A''&nbsp;≡ {''z''&nbsp;: ''r''&nbsp;< |''z''&nbsp;&minus;&nbsp;''c''|&nbsp;<&nbsp;''R''} and a holomorphic function ''f''(''z'') defined on ''A'', then there always exists a unique Laurent series with center ''c'' which converges (at least) on ''A'' and represents the function ''f''(''z'').\n\nAs an example, consider the following rational function, along with its [[partial fraction]] expansion:\n\n:<math>f(z) \\ =\\ \\frac{1}{(z{-}1)(z{-}2i)} \n\\ =\\ \\frac{1+2i}{5}\\left(\\frac{1}{z{-}1} - \\frac{1}{z{-}2i}\\right)\n.</math>\n\nThis function has singularities at ''z''&nbsp;=&nbsp;1 and ''z''&nbsp;=&nbsp;2''i'', where the denominator of the expression is zero and the expression is therefore undefined.\nA [[Taylor series]] about ''z''&nbsp;=&nbsp;0 (which yields a power series) will only converge in a disc of [[radius]] 1, since it \"hits\" the singularity at 1.\n\nHowever, there are three possible Laurent expansions about 0, depending on the radius of ''z'':\n* One series is defined on the inner disc where |''z''|&nbsp;<&nbsp;1; it is the same as the Taylor series,\n:<math>f(z) = \\frac{1 + 2i}{5} \\sum_{n=0}^\\infty \\left(\\frac{1}{(2i)^{n + 1}} - 1\\right)z^n.</math>\nThis follows from the partial fraction form of the function, along with the formula for the sum  of a [[geometric series]], <math>\\textstyle \\frac{1}{z-a} = - \\frac{1}{a} \\sum_{n=0}^\\infty \\left( \\tfrac{z}{a} \\right)^n </math> for <math>|z| < |a| </math>.\n* The second series is defined on the middle annulus where 1&nbsp;< |''z''|&nbsp;<&nbsp;2, caught between the two singularities,\n:<math>f(z) = \\frac{1 + 2i}{5} \\left(\\sum_{n=1}^\\infty z^{-n} + \\sum_{n=0}^\\infty \\frac{1}{(2i)^{n + 1}}z^n\\right).</math>\nHere, we use the alternative form of the geometric series summation, <math>\\textstyle \\frac{1}{z-a} = \\frac{1}{z}\\sum_{n=0}^\\infty \\left(\\tfrac{a}{z}\\right)^n </math>  for <math>|a| < |z| </math>.\n* The third series is defined on the infinite outer annulus where 2&nbsp;< |''z''|&nbsp;<&nbsp;∞,\n:<math>f(z) = \\frac{1 + 2i}{5} \\sum_{n=1}^\\infty (1 - (2i)^{n - 1}) z^{-n}.</math>\nThis series can be derived using geometric series as before, or by performing polynomial long division of 1 by (''x''&minus;1)(''x''&minus;2i), not stopping with a remainder but continuing into ''x''<sup>&minus;''n''</sup> terms; indeed, the \"outer\" Laurent series of a rational function is analogous to the decimal form of a fraction. (The \"inner\" Taylor series expansion can be obtained similarly, just reversing the [[monomial order|term order]] in the division algorithm.)\n\nThe case ''r''&nbsp;=&nbsp;0; i.e., a holomorphic function ''f''(''z'') which may be undefined at a single point ''c'', is especially important.\n\nThe coefficient ''a''<sub>−1</sub> of the Laurent expansion of such a function is called the [[residue (complex analysis)|residue]] of ''f''(''z'') at the singularity ''c''; it plays a prominent role in the [[residue theorem]].\n\nFor an example of this, consider\n\n:<math>f(z) = {e^z \\over z} + e^\\frac{1}{z}.</math>\n\nThis function is holomorphic everywhere except at ''z''&nbsp;=&nbsp;0.\nTo determine the Laurent expansion about ''c''&nbsp;=&nbsp;0, we use our knowledge of the Taylor series of the [[exponential function]]:\n\n:<math>f(z) = \\cdots + \\left( {1 \\over 3!} \\right) z^{-3} + \\left( {1 \\over 2!} \\right) z^{-2} + 2z^{-1} + 2 + \\left( {1 \\over 2!} \\right) z + \\left( {1 \\over 3!} \\right) z^2 + \\left( {1 \\over 4!} \\right) z^3 + \\cdots</math>\n\nand we find that the residue is&nbsp;2.\n\n== Uniqueness ==\n\nSuppose a function ''f''(''z'') holomorphic on the annulus  ''r'' < |''z'' − ''c''| < ''R'' has two Laurent series:\n\n: <math>f(z)=\\sum_{n=-\\infty}^{\\infty}a_{n}\\left(z-c\\right)^{n}=\\sum_{n=-\\infty}^{\\infty}b_{n}\\left(z-c\\right)^{n}.</math>\n\nMultiply both sides with <math>\\left(z-c\\right)^{-k-1}</math>, where k is an arbitrary integer, and integrate on a path γ inside the annulus,\n\n: <math>\\oint_{\\gamma}\\sum_{n=-\\infty}^{\\infty}a_{n}\\left(z-c\\right)^{n-k-1}\\mathrm{d}z=\\oint_{\\gamma}\\sum_{n=-\\infty}^{\\infty}b_{n}\\left(z-c\\right)^{n-k-1}\\mathrm{d}z.</math>\n\nThe series converges uniformly on <math>r+\\epsilon\\leq|z-c|\\leq R-\\epsilon</math>, where ε is a positive number small enough for γ to be contained in the constricted closed annulus, so the integration and summation can be interchanged. Substituting the identity\n\n: <math>\\oint_{\\gamma}(z-c)^{n-k-1}dz=2\\pi i\\delta_{nk}</math>\n\ninto the summation yields\n\n: <math>a_k=b_k</math>\n\nHence the Laurent series is unique.\n\n== Laurent polynomials ==\n{{main|Laurent polynomial}}\n\nA '''Laurent polynomial''' is a Laurent series in which only finitely many coefficients are non-zero. Laurent polynomials differ from ordinary [[polynomial]]s in that they may have terms of negative degree.\n\n== Principal part ==\nThe '''principal part''' of a Laurent series is the series of terms with negative degree, that is\n: <math>\\sum_{k=-\\infty}^{-1} a_k (z-c)^k.</math>\n\nIf the principal part of ''f'' is a finite sum, then ''f'' has a [[pole (complex analysis)|pole]] at ''c'' of order equal to (negative) the degree of the highest term; on the other hand, if ''f'' has an [[essential singularity]] at ''c'', the principal part is an infinite sum (meaning it has infinitely many non-zero terms).\n\nIf the inner radius of convergence of the Laurent series for ''f'' is 0, then ''f'' has an essential singularity at ''c'' if and only if the principal part is an infinite sum, and has a pole otherwise.\n\nIf the inner radius of convergence is positive, ''f'' may have infinitely many negative terms but still be regular at ''c'', as in the example above, in which case it is represented by a ''different'' Laurent series in a disk about&nbsp;''c''.\n\nLaurent series with only finitely many negative terms are well-behaved—they are a power series divided by <math>z^k</math>, and can be analyzed similarly—while Laurent series with infinitely many negative terms have complicated behavior on the inner circle of convergence.\n\n=== Multiplication ===\nLaurent series cannot in general be multiplied.\nAlgebraically, the expression for the terms of the product may involve infinite sums which need not converge (one cannot take the [[convolution]] of integer sequences).\nGeometrically, the two Laurent series may have non-overlapping annuli of convergence.\n\nTwo Laurent series with only ''finitely'' many negative terms can be multiplied: algebraically, the sums are all finite; geometrically, these have poles at ''c'', and inner radius of convergence 0, so they both converge on an overlapping annulus.\n\nThus when defining [[Formal power series#Formal Laurent series|formal Laurent series]], one requires Laurent series with only finitely many negative terms.\n\nSimilarly, the sum of two convergent Laurent series need not converge, though it is always defined formally, but the sum of two bounded below Laurent series (or any Laurent series on a punctured disk) has a non-empty annulus of convergence.\n\n== See also ==\n\n* [[Puiseux series]]\n* [[Mittag-Leffler's theorem]]\n* [[Formal Laurent series]] &mdash; Laurent series considered ''formally'', with coefficients from an arbitrary [[commutative ring]], without regard for convergence, and with only ''finitely'' many negative terms, so that multiplication is always defined.\n* [[Z-transform]] &mdash; the special case where the Laurent series is taken about zero has much use in time series analysis.\n* [[Fourier series]] &mdash; the substitution <math>z=e^{\\pi i w}</math> transforms a Laurent series into a Fourier series, or conversely. This is used in the ''q''-series expansion of the [[j-invariant|''j''-invariant]].\n* [[Padé approximant]] &mdash; Another technique used when a [[Taylor series]] is not viable\n\n==References==\n{{reflist}}\n\n== External links ==\n* {{springer|title=Laurent series|id=p/l057690}}\n* {{MacTutor Biography|id=Laurent_Pierre}}\n* {{MathWorld | urlname=LaurentSeries | title=Laurent Series }}\n*[http://www.mrob.com/pub/muency/laurentseries.html Laurent Series and Mandelbrot set by Robert Munafo]\n\n{{DEFAULTSORT:Laurent Series}}\n[[Category:Complex analysis]]\n[[Category:Series expansions]]"
    },
    {
      "title": "Maclaurin series",
      "url": "https://en.wikipedia.org/wiki/Maclaurin_series",
      "text": "#REDIRECT [[Taylor series]]\n\n[[Category:Series expansions]]"
    },
    {
      "title": "Madhava series",
      "url": "https://en.wikipedia.org/wiki/Madhava_series",
      "text": "In [[mathematics]], a  '''Madhava series''' or '''Leibniz series''' is any one of the series in a collection of [[infinite series]] expressions all of which are believed to have been discovered by [[Madhava of Sangamagrama]] (c.&nbsp;1350 – c.&nbsp;1425), the founder of the [[Kerala school of astronomy and mathematics]] and later by [[Gottfried Wilhelm Leibniz]], among others. These expressions are the [[Maclaurin series]] expansions of the trigonometric [[sine]], [[cosine]] and [[arctangent]] [[Function (mathematics)|functions]], and the special case of the power series expansion of the arctangent function yielding a formula for computing π. The power series expansions of sine and cosine functions are respectively called ''Madhava's sine series'' and ''Madhava's cosine series''.  The power series expansion of the arctangent function is sometimes called ''Madhava–Gregory series''<ref>Reference to Gregory–Madhava series: {{Cite web|url=http://www.luigigobbi.com/EarliestKnownUsesOfSomeOfTheWordsOfMathematics/G-K.htm|title=Earliest Known Uses of Some of the Words of Mathematics|accessdate=11 February 2010}}</ref><ref>Reference to Gregory–Madhava series: {{Cite web|url=http://www.mat.uc.pt/~jaimecs/pessoal/hpm.html|title=History of Mathematics in the classroom|last=Jaime Carvalho e Silva|date = July 1994|accessdate=15 February 2010}}</ref> or ''Gregory–Madhava series''. These power series are also collectively called  ''Taylor–Madhava series''.<ref>{{Cite journal|title=Topic entry on complex analysis : Introduction|publisher=PlanetMath.org|url=http://planetmath.org/encyclopedia/ComplexAnalytic2.html|accessdate=10 February 2010}}</ref> The formula for π is  referred to as ''Madhava–[[Isaac Newton|Newton]] series'' or ''Madhava–[[Gottfried Leibniz|Leibniz]] series'' or [[Leibniz formula for pi]] or Leibnitz–Gregory–Madhava series.<ref>{{Cite journal|last=Pascal Sebah|author2=Xavier Gourdon|year=2004|title=Collection of series for pi|url=http://math.bu.edu/people/tkohl/teaching/spring2008/piSeries.pdf|accessdate=10 February 2010}}</ref>  These further names for the various series are reflective of the names of the [[Western world|Western]] discoverers or popularizers of the respective series.\n\nThe derivations use many calculus related concepts such as summation, rate of change, and interpolation, which suggests that Indian mathematicians had a solid understanding of the concept of limit and the basics of calculus long before they were developed in Europe. Other evidence from Indian mathematics up to this point such as interest in infinite series and the use of a base ten decimal system also suggest that it was possible for calculus to have developed in India almost 300 years before its recognized birth in Europe.<ref name=Webb_4.pdf>{{cite journal|last1=Webb|first1=Phoebe|title=The Development of Calculus in the Kerala School|journal=TME |volume=11 |issue=3|page=495|url=http://www.math.umt.edu/tmme/vol11no3/Webb_4.pdf}}</ref>\n\nNo surviving works of Madhava contain explicit statements regarding the expressions which are now referred to as Madhava series. However, in the writing of later members of the [[Kerala school of astronomy and mathematics]] like [[Nilakantha Somayaji]] and [[Jyeshthadeva]] one can find unambiguous attributions of these series to Madhava. It is also in the works of these later astronomers and mathematicians one can trace the Indian proofs of these series expansions. These proofs provide enough indications about the approach Madhava had adopted to arrive at his series expansions.\n\nUnlike most previous cultures, which had been rather nervous about the concept of infinity, Madhava was more than happy to play around with infinity, particularly infinite series. He showed how, although the number 1 can be approximated by adding a half plus a quarter plus an eighth plus a sixteenth, etc., (as even the ancient Egyptians and Greeks had known), the exact total of 1 can only be achieved by adding up infinitely many fractions. But Madhava went further and linked the idea of an infinite series with geometry and trigonometry. He realized that, by successively adding and subtracting different odd number fractions to infinity, he could home in on an exact formula for π (this was two centuries before Leibniz was to come to the same conclusion in Europe).<ref>{{cite book |title=How Mechanics Shaped the Modern World |edition=illustrated |first1=David |last1=Allen |publisher=Springer Science & Business Media |year=2013 |isbn=978-3-319-01701-3 |page=156 |url=https://books.google.com/books?id=wRm4BAAAQBAJ}} [https://books.google.com/books?id=wRm4BAAAQBAJ&pg=PA156 Extract of page 156]</ref>\n\n==Madhava's series in modern notations==\nIn the writings of the mathematicians and astronomers of the [[Kerala school of astronomy and mathematics|Kerala school]], Madhava's series are described couched in the terminology and concepts fashionable at that time. When we translate these ideas into the notations and concepts of modern-day mathematics, we obtain the current equivalents of Madhava's series. These present-day counterparts of the infinite series expressions discovered by Madhava are the following:\n\n<center>\n{| class=\"wikitable\"\n|-\n! No.\n! Series\n! Name\n! Western discoverers of the series<br>and approximate dates of discovery<ref>{{Cite book|last=Charles Henry Edwards|title=The historical development of the calculus|publisher=Springer|year=1994|edition=3|series=Springer Study Edition Series|page=205|isbn=978-0-387-94313-8}}</ref>\n|-\n| style=\"text-align:center;\"|1\n| style=\"padding:0 1ex;\"|sin ''x'' = ''x'' &minus; {{sfrac|''x''<sup>3</sup>|3!}} + {{sfrac|''x''<sup>5</sup>|5!}} &minus; {{sfrac|''x''<sup>7</sup>|7!}} + ...\n| style=\"padding:0 1ex;\"|Madhava's sine series\n| style=\"padding:0 1ex;\"|Isaac Newton (1670) and Wilhelm Leibniz (1676)\n|-\n| style=\"text-align:center;\"|2\n| style=\"padding:0 1ex;\"|cos ''x'' = 1 &minus; {{sfrac|''x''<sup>2</sup>|2!}} + {{sfrac|''x''<sup>4</sup>|4!}} &minus; {{sfrac|''x''<sup>6</sup>|6!}} + ...\n| style=\"padding:0 1ex;\"|Madhava's cosine series\n| style=\"padding:0 1ex;\"|Isaac Newton (1670) and Wilhelm Leibniz (1676)\n|-\n| style=\"text-align:center;\"|3\n| style=\"padding:0 1ex;\"|arctan ''x'' = ''x'' &minus; {{sfrac|''x''<sup>3</sup>|3}} + {{sfrac|''x''<sup>5</sup>|5}} &minus; {{sfrac|''x''<sup>7</sup>|7}} + ...\n| style=\"padding:0 1ex;\"|Madhava's series for arctangent\n| style=\"padding:0 1ex;\"|James Gregory (1671) and Wilhelm Leibniz (1676)\n|-\n| style=\"text-align:center;\"|4\n| style=\"padding:0 1ex;\"|[[Leibniz formula for π|{{sfrac|{{pi}}|4}} = 1 &minus; {{sfrac|1|3}} + {{sfrac|1|5}} &minus; {{sfrac|1|7}} + ...]]\n| style=\"padding:0 1ex;\"|Madhava's formula for {{pi}}\n| style=\"padding:0 1ex;\"|James Gregory (1671) and Wilhelm Leibniz (1676)\n|}\n</center>\n\n==Madhava series in \"Madhava's own words\"==\nNone of Madhava's works, containing any of the series expressions attributed to him, have survived. These series expressions are found in the writings of the followers of Madhava in the [[Kerala school of astronomy and mathematics|Kerala school]]. At many places these authors have clearly stated that these are \"as told by Madhava\". Thus the enunciations of the various series found in [[Tantrasamgraha]] and its commentaries can be safely assumed to be in \"Madhava's own words\". The translations of the relevant verses as given in the ''Yuktidipika'' commentary of [[Tantrasamgraha]]  (also known as ''Tantrasamgraha-vyakhya'') by [[Sankara Variar]] (circa. 1500 - 1560 CE)  are reproduced below. These are then rendered in current mathematical notations.<ref>{{Cite journal|last=A.K. Bag|year=1975|title=Madhava's sine and cosine series|journal=Indian Journal of History of Science|volume=11|issue=1|pages=54–57|url=http://www.new.dli.ernet.in/rawdataupload/upload/insa/INSA_1/20005af4_54.pdf|accessdate=11 February 2010|deadurl=yes|archiveurl=https://web.archive.org/web/20100214195826/http://www.new.dli.ernet.in/rawdataupload/upload/insa/INSA_1/20005af4_54.pdf|archivedate=14 February 2010|df=dmy-all}}</ref><ref>{{Cite book|last=C.K. Raju|title=Cultural Foundations of Mathematics : Nature of Mathematical Proof and the Transsmission of the Calculus from India to Europe in the 16 c. CE|publisher=Centre for Studies in Civilistaion|location=New Delhi|year=2007|series=History of Science, Philosophy and Culture in Indian Civilisation|volume=X Part 4|pages=114–120|isbn=81-317-0871-3}}</ref>\n\n==Madhava's sine series==\n\n===In Madhava's own words===\n\nMadhava's sine series is stated in verses 2.440 and 2.441 in ''Yukti-dipika'' commentary (''Tantrasamgraha-vyakhya'') by [[Sankara Variar]]. A translation of the verses follows.\n\n''Multiply the arc by the square of the arc, and take the result of repeating that (any number of times). Divide (each of the above numerators)  by the squares of the successive even numbers increased by that number and multiplied by the square of the radius.  Place the arc and the successive results so obtained one below the other, and subtract each from the one above. These together give the jiva, as collected together in the verse beginning with \"vidvan\" etc. ''\n\n===Rendering in modern notations===\n\nLet ''r'' denote the radius of the circle and ''s'' the arc-length.\n\n*The following numerators are formed first:\n:: <math>s \\cdot s^2 ,\\qquad s \\cdot s^2 \\cdot s^2 , \\qquad s \\cdot s^2 \\cdot s^2 \\cdot s^2, \\qquad \\cdots</math>\n*These are then divided by quantities specified in the verse.\n:: <math>s\\cdot \\frac{s^2}{(2^2+2)r^2}, \\qquad s\\cdot \\frac{s^2}{(2^2+2)r^2}\\cdot \\frac{s^2}{(4^2+4)r^2},\\qquad s\\cdot \\frac{s^2}{(2^2+2)r^2}\\cdot \\frac{s^2}{(4^2+4)r^2}\\cdot \\frac{s^2}{(6^2+6)r^2}, \\qquad \\cdots </math>\n*Place the arc and the successive results so obtained one below the other, and subtract each from the one above to get ''jiva'':\n::<math> \\text{jiva}= s - \\left [ s\\cdot \\frac{s^2}{(2^2+2)r^2} - \\left [ s\\cdot \\frac{s^2}{(2^2+2)r^2}\\cdot \\frac{s^2}{(4^2+4)r^2} -\\left [ s\\cdot \\frac{s^2}{(2^2+2)r^2}\\cdot \\frac{s^2}{(4^2+4)r^2}\\cdot \\frac{s^2}{(6^2+6)r^2}-\\cdots\\right]\\right]\\right] </math>\n\n===Transformation to current notation===\n\nLet θ be the angle subtended by the arc ''s'' at the centre of the circle. Then ''s'' = ''r θ'' and ''jiva'' = ''r'' sin ''θ''. Substituting these in the last expression and simplifying we get\n:<math>\\sin \\theta = \\theta - \\frac{\\theta^3}{3!} + \\frac{\\theta^5}{5!} - \\frac{\\theta^7}{7!} + \\quad \\cdots </math>\nwhich is the infinite power series expansion of the sine function.\n\n===Madhava's reformulation for numerical computation===\nThe last line in the verse &prime;''as collected together in the verse beginning with \"vidvan\" etc.''&prime; is a reference to a reformulation of the series introduced  by Madhava himself to make it convenient for easy computations for specified values of the arc and the radius.\nFor such a reformulation, Madhava considers a circle one quarter of which measures 5400 minutes (say ''C'' minutes) and develops a scheme for the easy computations of the ''jiva''&prime;s of the various arcs of such a circle. Let ''R'' be the radius of a circle one quarter of which measures C.\nMadhava had already computed the value of π using his series formula for π.<ref name=\"Raju\">{{Cite book|last=C.K. Raju|title=Cultural foundations of mathematics: The nature of mathematical proof and the transmission of calculus from India to Europe in the 16 thc. CE|publisher=Centre for Studies in Civilizations|location=Delhi|year=2007|series=History of Philosophy, Science and Culture in Indian Civilization|volume=X Part 4|page=119}}</ref> Using this value of π, namely  3.1415926535922, the radius ''R'' is computed as follows:\nThen\n\n:''R'' = 2 &times; 5400 / &pi; = 3437.74677078493925 = 3437 [[arcminute]]s 44 [[arcsecond]]s  48 sixtieths of an [[arcsecond]] = 3437&prime;  44&prime;&prime;  48&prime;&prime;&prime;.\n\nMadhava's  expression for ''jiva'' corresponding to any arc ''s'' of a circle of radius ''R'' is equivalent to the following:\n\n:<math>\\begin{align}\n\\text{jiva } & = s - \\frac{s^3}{R^2(2^2+2)} + \\frac{s^5}{R^4(2^2+2)(4^2+4)}- \\cdots \\\\[6pt]\n& = s  - \\left(\\frac{s}{C}\\right)^3 \\left [   \\frac{R \\left(\\frac{\\pi}{2}\\right)^3}{3!} \n- \\left(\\frac{s}{C}\\right)^2 \\left [  \\frac{R \\left(\\frac{\\pi}{2}\\right)^5}{5!}\n - \\left(\\frac{s}{C}\\right)^2 \\left [  \\frac{R \\left(\\frac{\\pi}{2}\\right)^7}{7!} - \\cdots  \\right ]\\right]\\right].\n\\end{align}</math>\n\nMadhava now  computes the following values:\n\n<center>\n{| class=\"wikitable\"\n|-\n! No.\n! Expression\n! Value\n! Value in [[Katapayadi system]]\n|-\n| &nbsp;&nbsp; 1 &nbsp;&nbsp;\n| &nbsp;&nbsp; R &times; (π / 2)<sup>3</sup> / 3! &nbsp;&nbsp;\n|  &nbsp;&nbsp; 2220&prime; &nbsp; 39&prime;&prime; &nbsp;  40&prime;&prime;&prime;  &nbsp;&nbsp;\n|  &nbsp;&nbsp; ni-rvi-ddhā-nga-na-rē-ndra-rung &nbsp;&nbsp;\n|-\n| &nbsp;&nbsp;  2 &nbsp;&nbsp;\n|  &nbsp;&nbsp; R &times; (π / 2)<sup>5</sup> / 5!  &nbsp;&nbsp;\n|  &nbsp;&nbsp; 273&prime; &nbsp; 57&prime;&prime; &nbsp; 47&prime;&prime;&prime;  &nbsp;&nbsp;\n| &nbsp;&nbsp;  sa-rvā-rtha-śī-la-sthi-ro &nbsp;&nbsp;\n|-\n|  &nbsp;&nbsp; 3 &nbsp;&nbsp;\n|  &nbsp;&nbsp; R &times; (π / 2)<sup>7</sup> / 7!  &nbsp;&nbsp;\n|  &nbsp;&nbsp; 16&prime; &nbsp; 05&prime;&prime; &nbsp;  41&prime;&prime;&prime; &nbsp;&nbsp;\n|  &nbsp;&nbsp; ka-vī-śa-ni-ca-ya &nbsp;&nbsp;\n|-\n|  &nbsp;&nbsp; 4 &nbsp;&nbsp;\n|  &nbsp;&nbsp; R &times; (π / 2)<sup>9</sup> / 9!  &nbsp;&nbsp;\n|  &nbsp;&nbsp; 33&prime;&prime; &nbsp; 06&prime;&prime;&prime; &nbsp;&nbsp;\n|  &nbsp;&nbsp; tu-nna-ba-la &nbsp;&nbsp;\n|-\n|  &nbsp;&nbsp; 5 &nbsp;&nbsp;\n|  &nbsp;&nbsp; R &times; (π / 2)<sup>11</sup> / 11!  &nbsp;&nbsp;\n|  &nbsp;&nbsp; 44&prime;&prime;&prime; &nbsp;&nbsp;\n|  &nbsp;&nbsp; vi-dvān &nbsp;&nbsp;\n|-\n|}\n</center>\n\nThe ''jiva'' can now be computed using the following scheme:\n\n:''jiva'' = ''s'' &minus; (''s'' / ''C'')<sup>3</sup> [ (2220&prime; 39&prime;&prime;   40&prime;&prime;&prime;) &minus;  (''s'' / ''C'')<sup>2</sup> [ (273&prime;  57&prime;&prime;  47&prime;&prime;&prime;) &minus; (''s'' / ''C'')<sup>2</sup> [ (16&prime; 05&prime;&prime;  41&prime;&prime;&prime;) &minus; (''s'' / ''C'')<sup>2</sup>[ (33&prime;&prime; 06&prime;&prime;&prime;) &minus; (''s'' / ''C'')<sup>2</sup>  (44&prime;&prime;&prime; ) ] ] ] ].\n\nThis gives an approximation of ''jiva'' by its Taylor polynomial of the 11'th order. It involves one division, six multiplications and five subtractions only. Madhava prescribes this numerically efficient computational scheme in the following words (translation of verse 2.437 in ''Yukti-dipika''):\n\n''vi-dvān, tu-nna-ba-la, ka-vī-śa-ni-ca-ya, sa-rvā-rtha-śī-la-sthi-ro, ni-rvi-ddhā-nga-na-rē-ndra-rung . Successively multiply these five numbers in order by the square of the arc divided by the quarter of the circumference (5400&prime;), and subtract from the next number. (Continue this process with the result so obtained and the next number.) Multiply the final result by the cube of the arc divided by quarter of the circumference and subtract from the arc.''\n\n==Madhava's cosine series==\n\n===In Madhava's own words===\n\nMadhava's cosine series is stated in verses 2.442 and 2.443 in ''Yukti-dipika'' commentary (''Tantrasamgraha-vyakhya'') by [[Sankara Variar]]. A translation of the verses follows.\n\n''Multiply the square of the arc by the unit (i.e. the radius) and take the result of repeating that (any number of times). Divide (each of the above numerators) by the square of the successive even numbers decreased by that number and multiplied by the square of the radius. But the first term is (now)(the one which is) divided by twice the radius. Place the successive results so obtained one below the other and subtract each from the one above. These together give the śara as collected together in the verse beginning with stena, stri, etc. ''\n\n===Rendering in modern notations===\n\nLet ''r'' denote the radius of the circle and ''s'' the arc-length.\n\n*The following numerators are formed first:\n:: <math>r \\cdot s^2 ,\\qquad r \\cdot s^2 \\cdot s^2 , \\qquad r \\cdot s^2 \\cdot s^2 \\cdot s^2 , \\qquad \\cdots </math>\n*These are then divided by quantities specified in the verse.\n:: <math>r\\cdot \\frac{s^2}{(2^2 - 2)r^2}, \\qquad r\\cdot \\frac{s^2}{(2^2 - 2)r^2}\\cdot \\frac{s^2}{(4^2-4)r^2},\\qquad r\\cdot \\frac{s^2}{(2^2-2)r^2}\\cdot \\frac{s^2}{(4^2-4)r^2}\\cdot \\frac{s^2}{(6^2-6)r^2}, \\qquad \\cdots </math>\n*Place the arc and the successive results so obtained one below the other, and subtract each from the one above to get ''śara'':\n:: <math> \\text{sara}=   r\\cdot \\frac{s^2}{(2^2 - 2)r^2} - \\left [ r\\cdot \\frac{ s^2}{(2^2-2)r^2}\\cdot \\frac{s^2}{(4^2-4)r^2} -\\left [ r\\cdot \\frac{ s^2}{(2^2-2)r^2}\\cdot \\frac{s^2}{(4^2-4)r^2}\\cdot \\frac{s^2}{(6^2-6)r^2}-\\cdots\\right]\\right] </math>\n\n===Transformation to current notation===\n\nLet ''θ'' be the angle subtended by the arc ''s'' at the centre of the circle. Then ''s'' = ''rθ'' and ''śara'' = ''r''(1 − cos ''θ''). Substituting these in the last expression and simplifying we get\n:<math>1 - \\cos \\theta = \\frac{\\theta^2}{2!}  -  \\frac{\\theta^4}{4!} + \\frac{\\theta^6}{6!} + \\quad \\cdots </math>\nwhich gives  the infinite power series expansion of the cosine function.\n\n===Madhava's reformulation for numerical computation===\n\nThe last line in the verse &prime;''as collected together in the verse beginning with stena, stri, etc.''&prime; is a reference to a reformulation introduced  by Madhava himself to make the series convenient for easy computations for specified values of the arc and the radius.\nAs in the case of the sine series, Madhava considers a circle one quarter of which measures 5400 minutes (say ''C'' minutes) and develops a scheme for the easy computations of the ''śara''&prime;s of the various arcs of such a circle. Let ''R'' be the radius of a circle one quarter of which measures C. Then, as in the case of the sine series, Madhava gets\n''R'' = 3437&prime;  44&prime;&prime;  48&prime;&prime;&prime;.\n\nMadhava's  expression for ''śara'' corresponding to any arc ''s'' of a circle of radius ''R'' is equivalent to the following:\n\n:<math>\n\\begin{align}\n\\text{jiva } & = R\\cdot \\frac{s^2}{R^2(2^2-2)} - R\\cdot \\frac{s^4}{R^4(2^2-2)(4^2-4)}- \\cdots \\\\\n& = \\left(\\frac{s}{C}\\right)^2 \\left[   \\frac{R \\left(\\frac{\\pi}{2}\\right)^2}{2!} \n- \\left(\\frac{s}{C}\\right)^2 \\left[  \\frac{R \\left(\\frac{\\pi}{2}\\right)^4}{4!}\n - \\left(\\frac{s}{C}\\right)^2 \\left[  \\frac{R \\left(\\frac{\\pi}{2}\\right)^6}{6!} - \\cdots  \\right]\\right]\\right]\n\\end{align}\n</math>\n\nMadhava now  computes the following values:\n\n<center>\n{| class=\"wikitable\"\n|-\n! No.\n! Expression\n! Value\n! Value in [[Katapayadi system]]\n|-\n| &nbsp;&nbsp; 1 &nbsp;&nbsp;\n| &nbsp;&nbsp; R &times; (π / 2)<sup>2</sup> / 2! &nbsp;&nbsp;\n|  &nbsp;&nbsp; 4241&prime; &nbsp; 09&prime;&prime; &nbsp;  00&prime;&prime;&prime;  &nbsp;&nbsp;\n|  &nbsp;&nbsp; u-na-dha-na-krt-bhu-re-va &nbsp;&nbsp;\n|-\n| &nbsp;&nbsp;  2 &nbsp;&nbsp;\n|  &nbsp;&nbsp; R &times; (π / 2)<sup>4</sup> / 4!  &nbsp;&nbsp;\n|  &nbsp;&nbsp; 872&prime; &nbsp; 03&prime;&prime; &nbsp; 05 &prime;&prime;&prime;  &nbsp;&nbsp;\n| &nbsp;&nbsp; mī-nā-ngo-na-ra-sim-ha &nbsp;&nbsp;\n|-\n|  &nbsp;&nbsp; 3 &nbsp;&nbsp;\n|  &nbsp;&nbsp; R &times; (π / 2)<sup>6</sup> / 6!  &nbsp;&nbsp;\n|  &nbsp;&nbsp; 071&prime; &nbsp; 43&prime;&prime; &nbsp;  24&prime;&prime;&prime; &nbsp;&nbsp;\n|  &nbsp;&nbsp; bha-drā-nga-bha-vyā-sa-na &nbsp;&nbsp;\n|-\n|  &nbsp;&nbsp; 4 &nbsp;&nbsp;\n|  &nbsp;&nbsp; R &times; (π / 2)<sup>8</sup> / 8!  &nbsp;&nbsp;\n|  &nbsp;&nbsp; 03&prime; &nbsp; 09&prime;&prime; &nbsp; 37&prime;&prime;&prime; &nbsp;&nbsp;\n|  &nbsp;&nbsp; su-ga-ndhi-na-ga-nud  &nbsp;&nbsp;\n|-\n|  &nbsp;&nbsp; 5 &nbsp;&nbsp;\n|  &nbsp;&nbsp; R &times; (π / 2)<sup>10</sup> / 10!  &nbsp;&nbsp;\n|  &nbsp;&nbsp; 05&prime;&prime; &nbsp; 12&prime;&prime;&prime; &nbsp;&nbsp;\n|  &nbsp;&nbsp; strī-pi-śu-na  &nbsp;&nbsp;\n|-\n|  &nbsp;&nbsp; 6 &nbsp;&nbsp;\n|  &nbsp;&nbsp; R &times; (π / 2)<sup>12</sup> / 12!  &nbsp;&nbsp;\n|  &nbsp;&nbsp; 06&prime;&prime;&prime; &nbsp;&nbsp;\n|  &nbsp;&nbsp; ste-na &nbsp;&nbsp;\n|-\n|}\n</center>\n\nThe ''śara'' can now be computed using the following scheme:\n\n:''śara'' = (''s'' / ''C'')<sup>2</sup> [ (4241&prime;  09&prime;&prime;   00&prime;&prime;&prime;) &minus;  (''s'' / ''C'')<sup>2</sup> [ (872&prime; 03&prime;&prime; 05 &prime;&prime;&prime;) &minus; (''s'' / ''C'')<sup>2</sup> [  (071&prime; 43&prime;&prime; 24&prime;&prime;&prime;) &minus; (''s'' / ''C'')<sup>2</sup>[ (03&prime; 09&prime;&prime; 37&prime;&prime;&prime;) &minus; (''s'' / ''C'')<sup>2</sup>  [(05&prime;&prime; 12&prime;&prime;&prime;) − (s / C)<sup>2</sup> (06&prime;&prime;&prime;) ] ] ] ] ]\n\nThis gives an approximation of ''śara'' by its Taylor polynomial of the 12'th order. This also involves one division, six multiplications and five subtractions only. Madhava prescribes this numerically efficient computational scheme in the following words (translation of verse 2.438 in ''Yukti-dipika''):\n\n''The six stena, strīpiśuna, sugandhinaganud, bhadrāngabhavyāsana, mīnāngonarasimha, unadhanakrtbhureva. Multiply by the square of the arc  divided by the quarter of the circumference and subtract from the next number. (Continue with the result and the next number.) Final result will be [[utkrama-jya]] (R versed sign).''\n\n==Madhava's arctangent series==\n\n===In Madhava's own words===\n\nMadhava's arctangent series is stated in verses 2.206 &ndash; 2.209 in ''Yukti-dipika'' commentary (''Tantrasamgraha-vyakhya'') by [[Sankara Variar]]. A translation of the verses is given below.<ref>{{Cite book|last=C.K. Raju|title=Cultural Foundations of Mathematics : Nature of Mathematical Proof and the Transsmission of the Calculus from India to Europe in the 16 c. CE|publisher=Centre for Studies in Civilistaion|location=New Delhi|year=2007|series=History of Science, Philosophy and Culture in Indian Civilisation|volume=X Part 4|page=231|isbn=81-317-0871-3}}</ref>\n[[Jyesthadeva]] has also given a description of this series in [[Yuktibhasa]].<ref>{{Cite web|url=http://www-gap.dcs.st-and.ac.uk/~history/Biographies/Madhava.html|archive-url=https://web.archive.org/web/20060514012903/http://www-gap.dcs.st-and.ac.uk/~history/Biographies/Madhava.html|dead-url=yes|archive-date=2006-05-14|title=Madhava of Sangamagramma|author1=J J O'Connor|author2=E F Robertson|lastauthoramp=yes|date=November 2000|publisher=School of Mathematics and Statistics University of St Andrews, Scotland|accessdate=14 February 2010}}</ref>\n<ref>R.C. Gupta, The Madhava-Gregory series, Math. Education 7 (1973), B67-B70.</ref>\n<ref>[[K.V. Sarma]], A History of the Kerala School of Hindu Astronomy (Hoshiarpur, 1972).</ref>\n\n''Now, by just the same argument, the determination of the arc of a desired sine can be (made). That is as follows: The first result is the product of the desired sine and the radius divided by the cosine of the arc. When one has made the square of the sine the multiplier and the square of the cosine the divisor, now a group of results is to be determined from the (previous) results beginning from the first.  When these are divided in order by the odd numbers 1, 3, and so forth, and when one has subtracted the sum of the even(-numbered) results from the sum of the odd (ones), that should be the arc. Here the smaller of the sine and cosine is required to be considered as the desired (sine). Otherwise, there would be no termination of results even if repeatedly (computed).''\n\n''By means of the same argument, the circumference can be computed in another way too. That is as (follows): The first result should by the square root of the square of the diameter multiplied by twelve. From then on, the result should be divided by three (in) each successive (case). When these are divided in order by the odd numbers, beginning with 1, and when one has subtracted the (even) results from the sum of the odd, (that) should be the circumference.''\n\n===Rendering in modern notations===\n\nLet ''s'' be the arc of the desired sine (''[[jya]]'' or ''jiva'') ''y''. Let ''r'' be the radius  and ''x'' be the cosine (''[[kojya|kotijya]]'').\n\n*The first result is <math>\\tfrac{y \\cdot r}{x}</math>.\n*Form the multiplier and divisor <math>\\tfrac{y^2}{x^2}</math>.\n*Form the group of results: \n::<math>\\frac{y \\cdot r}{x}\\cdot\\frac{y^2}{x^2}, \\qquad \\frac{y \\cdot r}{x}\\cdot\\frac{y^2}{x^2}\\cdot\\frac{y^2}{x^2}, \\qquad \\cdots</math>\n*These are divided in order by the numbers 1, 3, and so forth:\n:: <math> \\frac{1}{1}\\frac{y \\cdot r}{x}, \\qquad \\frac{1}{3}\\frac{y \\cdot r}{x}\\cdot\\frac{y^2}{x^2}, \\qquad \\frac{1}{5}\\frac{y \\cdot r}{x}\\cdot\\frac{y^2}{x^2}\\cdot\\frac{y^2}{x^2}, \\qquad \\cdots</math>\n*Sum of odd-numbered  results: \n::<math>\\frac{1}{1}\\frac{y \\cdot r}{x} + \\frac{1}{5}\\frac{y \\cdot r}{x}\\cdot\\frac{y^2}{x^2}\\cdot\\frac{y^2}{x^2}+\\cdots</math>\n*Sum of even-numbered results:  \n::<math>\\frac{1}{3}\\frac{y \\cdot r}{x}\\cdot\\frac{y^2}{x^2} + \\frac{1}{7}\\frac{y \\cdot r}{x}\\cdot\\frac{y^2}{x^2}\\cdot\\frac{y^2}{x^2}\\cdot\\frac{y^2}{x^2}+\\cdots</math>\n*The arc is now given by\n::<math>s = \\left(\\frac{1}{1}\\frac{y \\cdot r}{x} + \\frac{1}{5}\\frac{y \\cdot r}{x}\\cdot\\frac{y^2}{x^2}\\cdot\\frac{y^2}{x^2}+\\cdots\\right) - \\left(\\frac{1}{3}\\frac{y \\cdot r}{x}\\cdot\\frac{y^2}{x^2} + \\frac{1}{7}\\frac{y \\cdot r}{x}\\cdot\\frac{y^2}{x^2}\\cdot\\frac{y^2}{x^2}\\cdot\\frac{y^2}{x^2}+\\cdots\\right)</math>\n\n===Transformation to current notation===\n\nLet θ be the angle subtended by the arc ''s'' at the centre of the circle. Then ''s'' = ''r''&theta;,  ''x'' = ''[[kotijya]]'' = ''r'' cos &theta; and ''y'' = ''[[jya]]'' = ''r'' sin θ.\nThen ''y'' / ''x'' = tan θ. Substituting these in the last expression and simplifying we get\n*<math>\\theta = \\tan \\theta - \\frac{\\tan^3 \\theta}{3} + \\frac{\\tan^5\\theta}{5} - \\frac{\\tan^7 \\theta}{7} + \\quad \\cdots </math>.\nLetting tan θ = ''q'' we finally have\n\n*<math> \\tan^{-1} q = q - \\frac{q^3}{3} + \\frac{q^5}{5} - \\frac{q^7}{7} +  \\quad \\cdots </math>\n\n===Another formula for the circumference of a circle===\n\nThe second part of the quoted text specifies another formula for the computation of the circumference ''c'' of a circle having diameter ''d''. This is as follows.\n\n:<math> \nc= \\sqrt{12 d^2} - \\frac{\\sqrt{12 d^2}}{3\\cdot 3} + \\frac{\\sqrt{12 d^2}}{3^2 \\cdot 5} - \\frac{\\sqrt{12 d^2}}{3^3 \\cdot 7}+ \\quad \\cdots\n</math>\n\nSince  ''c'' =  &pi; ''d'' this can be reformulated as a formula to compute π as follows.\n\n:<math> \n\\pi = \\sqrt{12}\\left( 1 - \\frac{1}{3\\cdot3}+\\frac{1}{3^2\\cdot 5} -\\frac{1}{3^3\\cdot 7} +\\quad \\cdots\\right) \n</math>\n\nThis is obtained by substituting ''q'' = <math>1/\\sqrt{3}</math> (therefore ''θ'' = &pi; / 6) in the power series expansion for tan<sup>−1</sup> ''q'' above.\n\n{{comparison_pi_infinite_series.svg|400px{{!}}none|two Madhava series (the one with {{radic|12}} in dark blue) and}}\n\n==See also==\n*[[Madhava of Sangamagrama]]\n*[[Madhava's sine table]]\n*[[Padé approximant]]\n*[[Taylor Series]]\n*[[Laurent series]]\n*[[Puiseux series]]\n\n==References==\n{{Reflist|colwidth=30em}}\n\n==Further reading==\n{{Refbegin|colwidth=60em}}\n*{{Cite book|last=Joseph|first=George Gheverghese|origyear=1991|date=October 2010|title=The Crest of the Peacock: Non-European Roots of Mathematics|edition=3rd|publisher=[[Princeton University Press]]|isbn=978-0-691-13526-7|url=http://press.princeton.edu/titles/9308.html}}\n*[[K. V. Sarma]], A History of the Kerala School of Hindu Astronomy (Hoshiarpur, 1972).\n*A. K. Bag, Madhava's sine and cosine series, Indian J. History Sci. 11 (1) (1976), 54–57.\n*D. Gold and D Pingree, A hitherto unknown Sanskrit work concerning Madhava's derivation of the power series for sine and cosine, Historia Sci. No. 42 (1991), 49–65.\n*R. C. Gupta, Madhava's and other medieval Indian values of pi, Math. Education 9 (3) (1975), B45–B48.\n*R. C. Gupta, Madhava's power series computation of the sine, Ganita 27 (1–2) (1976), 19–24.\n*R. C. Gupta, On the remainder term in the Madhava–Leibniz's series, Ganita Bharati 14 (1–4) (1992), 68–71.\n*R. C. Gupta, The Madhava–Gregory series, Math. Education 7 (1973), B67–B70.\n*T. Hayashi, T. Kusuba and M. Yano, The correction of the Madhava series for the circumference of a circle, Centaurus 33 (2–3) (1990), 149–174.\n*R. C. Gupta, The Madhava–Gregory series for tan<sup>&minus;1</sup>''x'', Indian Journal of Mathematics Education, 11(3), 107–110, 1991.\n*{{Cite book|last=[[Kim Plofker]]|title=Mathematics in India  |publisher=Princeton University Press|location=Princeton|year=2009|pages=217–254|isbn=978-0-691-12067-6}}\n*\"The discovery of the series formula for &pi; by Leibniz, Gregory, and Nilakantha\" by Ranjan Roy in : &nbsp;{{Cite book|title=Sherlock Holmes in Babylon and other tales of mathematical history|editor1=Marlow Anderson |editor2=Victor Katz |editor3=Robin Wilson |publisher=[[The Mathematical Association of America]]|year=2004|pages=111–121|isbn=0-88385-546-1}}\n*\"Ideas of calculus in Islam and India\" by Victor J Katz in : &nbsp;{{Cite book|title=Sherlock Holmes in Babylon and other tales of mathematical history|editor1=Marlow Anderson |editor2=Victor Katz |editor3=Robin Wilson |publisher=The Mathematical Association of America|year=2004|pages=122–130|isbn=0-88385-546-1}}\n*\"Was calculus invented in India?\" by David Bressoud in : &nbsp;{{Cite book|title=Sherlock Holmes in Babylon and other tales of mathematical history|editor1=Marlow Anderson |editor2=Victor Katz |editor3=Robin Wilson |publisher=The Mathematical Association of America|year=2004|pages=131–137|isbn=0-88385-546-1}}\n*{{Cite book|title=The mathematics of Egypt, Mesopotemia, China, India and Islam: A source book|editor=Victor J Katz|publisher=Princeton University Press|location=Princeton|year=2007|pages=480–495|chapter=Chapter 4 : Mathematics in India IV. Kerala School|isbn=978-0-691-11485-9}}\n*{{Cite book|last=Glen Van Brummelen|title=The mathematics of the heavens and the earth : the early history of trigonometry|publisher=Princeton University Press |location=Princeton|year=2009|pages=113–120|isbn=978-0-691-12973-0}}\n*D. Pouvreau, Trigonométrie et \"développements en séries\" en Inde médiévale, I.R.E.M. de l'Université de Toulouse III (2003), 162 pages. {{oclc|758823300}}\n*D. Pouvreau, \"Sur l'accélération de la convergence de la série de Madhava-Leibniz\", Quadrature, n°97 (2015), pp.&nbsp;17–25. {{ISBN|978-2-7598-0528-0}}\n{{Refend}}\n{{Use dmy dates|date=September 2010}}\n\n{{DEFAULTSORT:Madhava Series}}\n[[Category:Mathematical series]]\n[[Category:History of mathematics]]\n[[Category:Kerala school]]\n[[Category:Series expansions]]\n[[Category:Indian mathematics]]"
    },
    {
      "title": "Schlömilch's Series",
      "url": "https://en.wikipedia.org/wiki/Schl%C3%B6milch%27s_Series",
      "text": "'''Schlömilch's Series''' is a [[Fourier series]] type expansion of twice continuously differentiable function in the interval <math>(0,\\pi)</math> in terms of the [[Bessel function of the first kind]], named after the German mathematician [[Oscar Schlömilch]], who derived the series in 1857<ref>Schlomilch, G. (1857). On Bessel's function. Zeitschrift fur Math, and Pkys., 2, 155-158.</ref><ref>Whittaker, E. T., & Watson, G. N. (1996). A course of modern analysis. Cambridge university press.</ref><ref>Rayleigh, L. (1911). LXII. On a physical interpretation of Schlömilch's theorem in Bessel's functions. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 21(124), 567-571.</ref><ref>Watson, G. N. (1995). A treatise on the theory of Bessel functions. Cambridge university press.</ref><ref>Chapman, S. (1911). On the general theory of summability, with application to Fourier's and other series. Quarterly Journal, 43, 1-52.</ref>. The real-valued function <math>f(x)</math> has the following expansion,\n\n:<math>f(x) = a_0 + \\sum_{n=1}^\\infty a_n J_0(nx)</math>\n\nwhere\n\n:<math>\\begin{align}\na_0 &= f(0) + \\frac{1}{\\pi} \\int_0^\\pi \\int_0^{\\pi/2} u f'(u\\sin\\theta)\\ d\\theta\\ du, \\\\\na_n &= \\frac{2}{\\pi} \\int_0^\\pi \\int_0^{\\pi/2} u\\cos nu \\ f'(u\\sin\\theta)\\ d\\theta\\ du.\n\\end{align}</math>\n\n==Examples and Properties==\nSome examples and properties of Schlömilch's Series are following\n*Null functions in the interval <math>(0,\\pi)</math> can be expressed by Schlömilch's Series, <math>0 = \\frac{1}{2}+\\sum_{n=1}^\\infty (-1)^n J_0(nx)</math>, which cannot be obtained by [[Fourier Series]].\n*<math>x = \\frac{\\pi^2}{4}-2\\sum_{n=1,3,...}^\\infty \\frac{J_0(nx)}{n^2}, \\quad 0<x<\\pi.</math>\n*<math>x^2 = \\frac{2\\pi^2}{3} + 8 \\sum_{n=1}^\\infty \\frac{(-1)^n}{n^2}J_0(nx), \\quad -\\pi<x<\\pi.</math>\n*<math>\\frac{1}{x} + \\sum_{m=1}^k\\frac{2}{\\sqrt{x^2-4m^2\\pi^2}} = \\frac{1}{2} + \\sum_{n=1}^\\infty J_0(nx), \\quad 2k\\pi<x<2(k+1)\\pi.</math>\n* If <math>(r,z)</math> are the cylindrical polar coordinates, then the series <math>1+\\sum_{n=1}^\\infty e^{-nz}J_0(nr)</math> is a solution of [[Laplace equation]] for <math>z>0</math>.\n\n==References==\n{{reflist|30em}}\n\n[[Category:Series expansions]]"
    },
    {
      "title": "Taylor series",
      "url": "https://en.wikipedia.org/wiki/Taylor_series",
      "text": "{{for|other notions of series expansion|Series (mathematics)}}\n{{good article}}\n[[File:sintay_SVG.svg|thumb|300px|As the degree of the Taylor polynomial rises, it approaches the correct function. This image shows {{math|sin ''x''}} and its Taylor approximations, polynomials of degree <span style=\"color:red;\">'''1'''</span>, <span style=\"color:orange;\">'''3'''</span>, <span style=\"color:yellow;\">'''5'''</span>, <span style=\"color:lime;\">'''7'''</span>, <span style=\"color:blue;\">'''9'''</span>, <span style=\"color:indigo;\">'''11'''</span> and <span style=\"color:violet;\">'''13'''</span>.]]\n{{Calculus |Series}}\n\nIn [[mathematics]], a '''Taylor series''' is a representation of a [[function (mathematics)|function]] as an [[Series (mathematics)|infinite sum]] of terms that are calculated from the values of the function's [[derivative]]s at a single point.<ref name=\"MAT 314\">{{cite web| publisher=Canisius College| work=MAT 314| url=http://www.pas.rochester.edu/~rajeev/papers/canisiustalks.pdf| title=Neither Newton nor Leibniz – The Pre-History of Calculus and Celestial Mechanics in Medieval Kerala| accessdate=2006-07-09| deadurl=no| archiveurl=https://web.archive.org/web/20150223113517/http://www.pas.rochester.edu/~rajeev/papers/canisiustalks.pdf| archivedate=2015-02-23| df=}}</ref><ref name=\"dani\">{{cite journal| title=Ancient Indian Mathematics – A Conspectus| author= S. G. Dani|journal= Resonance |volume=17|issue=3|year=2012|pages=236–246|doi=10.1007/s12045-012-0022-y}}</ref>\n<ref>{{Ranjan Roy, The Discovery of the Series Formula for π by Leibniz, Gregory and Nilakantha, Mathematics Magazine\nVol. 63, No. 5 (Dec., 1990), pp. 291-306.}}</ref>\n\nIn the West, the subject was formulated by the Scottish mathematician [[James Gregory (mathematician)|James Gregory]] and formally introduced by the English mathematician [[Brook Taylor]] in 1715. If the Taylor series is centered at zero, then that series is also called a '''Maclaurin series''', after the Scottish mathematician [[Colin Maclaurin]], who made extensive use of this special case of Taylor series in the 18th century.\n\nA function can be approximated by using a finite number of terms of its Taylor series. [[Taylor's theorem]] gives quantitative estimates on the error introduced by the use of such an approximation.  The polynomial formed by taking some initial terms of the Taylor series is called a Taylor polynomial. The Taylor series of a function is the [[limit of a sequence|limit]] of that function's Taylor polynomials as the degree increases, provided that the limit exists. A function may not be equal to its Taylor series, even if its Taylor series [[Convergence (mathematics)|converges]] at every point. A function that is equal to its Taylor series in an [[open interval]] (or a [[Disk (mathematics)|disc]] in the [[complex plane]]) is known as an [[analytic function]] in that interval.\n\n==Definition==\nThe Taylor series of a [[real-valued function|real]] or [[complex-valued function]] {{math|''f''&thinsp;(''x'')}} that is [[infinitely differentiable function|infinitely differentiable]] at a [[real number|real]] or [[complex number]] {{math|''a''}} is the [[power series]]\n<!--\n\nAny changes to the following formula, without first obtaining consensus on the discussion page will be reverted.  In particular, *DO NOT* add f(x)= here.\n\n-->:<math>f(a)+\\frac {f'(a)}{1!} (x-a)+ \\frac{f''(a)}{2!} (x-a)^2+\\frac{f'''(a)}{3!}(x-a)^3+ \\cdots, </math>\n\nwhere {{math|''n''!}} denotes the [[factorial]] of {{mvar|n}} and {{math|''f''{{isup|(''n'')}}(''a'')}} denotes the {{mvar|n}}th [[derivative]] of {{mvar|f}} evaluated at the point {{mvar|a}}.  In the more compact [[Summation#Capital-sigma notation|sigma notation]], this can be written as\n<!--\n\nAny changes to the following formula, without first obtaining consensus on the discussion page will be reverted.  In particular, *DO NOT* add f(x)= here.\n\n-->:<math> \\sum_{n=0} ^ {\\infty} \\frac {f^{(n)}(a)}{n!} (x-a)^{n}.</math>\n\nThe derivative of order zero of {{mvar|f}} is defined to be {{mvar|f}} itself and {{math|(''x'' − ''a'')<sup>0</sup>}} and {{math|0!}} are both defined to be&nbsp;1. When {{math|''a'' {{=}} 0}}, the series is also called a [[Colin Maclaurin#Contributions to mathematics|Maclaurin series]].<ref>{{harvnb|Thomas|Finney|1996|loc=§8.9}}</ref>\n\n==Examples==\nThe Taylor series for any [[polynomial]] is the polynomial itself.\n\nThe Maclaurin series for {{math|{{sfrac|1|1 − ''x''}}}} is the [[geometric series]]\n\n:<math>1+x+x^2+x^3+\\cdots</math>\n\nso the Taylor series for {{math|{{sfrac|1|''x''}}}} at {{math|''a'' {{=}} 1}} is\n\n:<math>1-(x-1)+(x-1)^2-(x-1)^3+\\cdots.</math>\n\nBy integrating the above Maclaurin series, we find the Maclaurin series for {{math|[[Natural logarithm|log]](1  − ''x'')}}, where log denotes the [[natural logarithm]]:\n\n:<math>-x-\\tfrac{1}{2}x^2-\\tfrac{1}{3}x^3-\\tfrac{1}{4}x^4-\\cdots</math>\n\nand the corresponding Taylor series for {{math|log ''x''}} at {{math|''a'' {{=}} 1}} is\n\n:<math>(x-1)-\\tfrac{1}{2}(x-1)^2+\\tfrac{1}{3}(x-1)^3-\\tfrac{1}{4}(x-1)^4+\\cdots,</math>\n\nand more generally, the corresponding Taylor series for {{math|log ''x''}} at some {{math|''a'' {{=}} ''x''<sub>0</sub>}} is:\n\n:<math>\\log x_0 + \\frac{1}{x_0} ( x - x_0 ) - \\frac{1}{x_0^2}\\frac{\\left( x - x_0 \\right)^2}{2} + \\cdots.</math>\n\nThe Taylor series for the [[exponential function]] {{math|''e''<sup>''x''</sup>}} at {{math|''a'' {{=}} 0}} is\n\n:<math>\\begin{align}\n   \\sum_{n=0}^\\infty \\frac{x^n}{n!} &= \\frac{x^0}{0!} + \\frac{x^1}{1!} + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\frac{x^4}{4!} + \\frac{x^5}{5!}+ \\cdots \\\\\n   &= 1 + x + \\frac{x^2}{2} + \\frac{x^3}{6} + \\frac{x^4}{24} + \\frac{x^5}{120} + \\cdots.\n \\end{align}</math>\n\nThe above expansion holds because the derivative of {{math|''e''<sup>''x''</sup>}} with respect to {{mvar|x}} is also {{math|''e''<sup>''x''</sup>}} and {{math|''e''<sup>0</sup>}} equals&nbsp;1. This leaves the terms {{math|(''x'' − 0)<sup>''n''</sup>}} in the numerator and {{math|''n''!}} in the denominator for each term in the infinite sum.\n\n==History==\nThe Greek philosopher [[Zeno of Elea|Zeno]] considered the problem of summing an infinite series to achieve a finite result, but rejected it as an impossibility<ref>{{cite book|last1=Lindberg|first1=David|title=The Beginnings of Western Science|date=2007|publisher=University of Chicago Press|isbn=978-0-226-48205-7|page=33|edition=2nd}}</ref>; the result was [[Zeno's paradox]]. Later, [[Aristotle]] proposed a philosophical resolution of the paradox, but the mathematical content was apparently unresolved until taken up by [[Archimedes]], as it had been prior to Aristotle by the Presocratic Atomist [[Democritus]]. It was through Archimedes's [[method of exhaustion]] that an infinite number of progressive subdivisions could be performed to achieve a finite result.<ref>{{cite book |last=Kline |first=M. |year=1990 |title=Mathematical Thought from Ancient to Modern Times |location=New York |publisher=Oxford University Press |pages=35–37 |isbn=0-19-506135-7 }}</ref> [[Liu Hui]] independently employed a similar method a few centuries later.<ref>{{cite book |last=Boyer |first=C. |last2=Merzbach |first2=U.|author2-link= Uta Merzbach |year=1991 |title=A History of Mathematics |location= |publisher=John Wiley and Sons |edition=Second revised |pages=202–203 |isbn=0-471-09763-2 }}<!--I'm sure there are better refs than this. Hui gave fairly \"rigorous\" bounds on the convergence, if I recall. But it isn't addressed here.--></ref>\n\nIn the 14th century, the earliest examples of the use of Taylor series and closely related methods were given by [[Madhava of Sangamagrama]].<ref name=\"MAT 314\">{{cite web| publisher=Canisius College| work=MAT 314| url=http://www.pas.rochester.edu/~rajeev/papers/canisiustalks.pdf| title=Neither Newton nor Leibniz – The Pre-History of Calculus and Celestial Mechanics in Medieval Kerala| accessdate=2006-07-09| deadurl=no| archiveurl=https://web.archive.org/web/20150223113517/http://www.pas.rochester.edu/~rajeev/papers/canisiustalks.pdf| archivedate=2015-02-23| df=}}</ref><ref name=\"dani\">{{cite journal| title=Ancient Indian Mathematics – A Conspectus| author= S. G. Dani|journal= Resonance |volume=17|issue=3|year=2012|pages=236–246|doi=10.1007/s12045-012-0022-y}}</ref> Though no record of his work survives, writings of later [[Indian mathematics|Indian mathematicians]] suggest that he found a number of special cases of the Taylor series, including those for the [[trigonometric function]]s of [[sine]], [[cosine]], [[tangent (trigonometric function)|tangent]], and [[arctangent]]. The [[Kerala School of Astronomy and Mathematics]] further expanded his works with various series expansions and rational approximations until the 16th century.\n\nIn the 17th century, [[James Gregory (astronomer and mathematician)|James Gregory]] also worked in this area and published several Maclaurin series. It was not until 1715 however that a general method for constructing these series for all functions for which they exist was finally provided by [[Brook Taylor]],<ref>{{cite book|language=Latin|last=Taylor |first=Brook |title=Methodus Incrementorum Directa et Inversa |trans-title=Direct and Reverse Methods of Incrementation |location=London |date=1715 |at=p. 21–23 (Prop. VII, Thm. 3, Cor. 2)}} Translated into English in {{cite book|first=D. J. |last=Struik|title=A Source Book in Mathematics 1200–1800 |location=Cambridge, Massachusetts |publisher=Harvard University Press |date=1969 |pages= 329–332}}</ref> after whom the series are now named.\n\nThe Maclaurin series was named after [[Colin Maclaurin]], a professor in Edinburgh, who published the special case of the Taylor result in the 18th century.\n\n==Analytic functions==\n[[Image:Exp neg inverse square.svg|300px|thumb|right|The function {{math|1=<strong style=\"color:#803300\">''e''<sup>(−1/''x''<sup>2</sup>)</sup></strong>}} is not analytic at {{math|1=''x'' {{=}} 0}}: the Taylor series is identically 0, although the function is not.]]\n{{main article|analytic function}}\nIf {{math|''f''&thinsp;(''x'')}} is given by a convergent power series in an open disc (or interval in the real line) centred at {{mvar|b}} in the complex plane, it is said to be [[analytic function|analytic]] in this disc.  Thus for {{mvar|x}} in this disc, {{mvar|f}} is given by a convergent power series\n:<math>f(x) = \\sum_{n=0}^\\infty a_n(x-b)^n.</math>\nDifferentiating by {{mvar|x}} the above formula {{mvar|n}} times, then setting {{math|''x'' {{=}} ''b''}} gives:\n:<math>\\frac{f^{(n)}(b)}{n!} = a_n</math>\nand so the power series expansion agrees with the Taylor series.  Thus a function is analytic in an open disc centred at {{mvar|b}} if and only if its Taylor series converges to the value of the function at each point of the disc.\n\nIf {{math|''f''&thinsp;(''x'')}} is equal to its Taylor series for all {{mvar|x}} in the complex plane, it is called [[entire function|entire]]. The polynomials, [[exponential function]] {{math|''e''<sup>''x''</sup>}}, and the [[trigonometric function]]s sine and cosine, are examples of entire functions. Examples of functions that are not entire include the [[square root]], the [[logarithm]], the [[trigonometric function]] tangent, and its inverse, [[arctan]]. For these functions the Taylor series do not [[convergent series|converge]] if {{mvar|x}} is far from {{mvar|b}}. That is, the Taylor series [[divergent series|diverges]] at {{mvar|x}} if the distance between {{mvar|x}} and {{mvar|b}} is larger than the [[radius of convergence]]. The Taylor series can be used to calculate the value of an entire function at every point, if the value of the function, and of all of its derivatives, are known at a single point.\n\nUses of the Taylor series for analytic functions include:\n# The partial sums (the [[Taylor polynomial]]s) of the series can be used as approximations of the function. These approximations are good if sufficiently many terms are included.\n#Differentiation and integration of power series can be performed term by term and is hence particularly easy.\n#An [[analytic function]] is uniquely extended to a [[holomorphic function]] on an [[open disk]] in the [[complex number|complex plane]]. This makes the machinery of [[complex analysis]] available.\n#The (truncated) series can be used to compute function values numerically, (often by recasting the polynomial into the [[Chebyshev form]] and evaluating it with the [[Clenshaw algorithm]]).\n#Algebraic operations can be done readily on the power series representation; for instance, [[Euler's formula]] follows from Taylor series expansions for trigonometric and exponential functions. This result is of fundamental importance in such fields as [[harmonic analysis]].\n#Approximations using the first few terms of a Taylor series can make otherwise unsolvable problems possible for a restricted domain; this approach is often used in physics.\n\n==Approximation error and convergence==\n{{main|Taylor's theorem}}\n[[Image:Taylorsine.svg|300px|thumb|right|The sine function (blue) is closely approximated by its Taylor polynomial of degree 7 (pink) for a full period centered at the origin.]]\n[[Image:LogTay.svg|300px|thumb|right|The Taylor polynomials for {{math|log(1 + ''x'')}} only provide accurate approximations in the range {{math|−1 < ''x'' ≤ 1}}. For {{math|''x'' > 1}}, Taylor polynomials of higher degree provide worse approximations.]]\n[[Image:Logarithm GIF.gif|300px|thumb|right|The Taylor approximations for {{math|log(1 + ''x'')}} (black). For {{math|''x'' > 1}}, the approximations diverge.]]\n\nPictured on the right is an accurate approximation of {{math|sin ''x''}} around the point {{math|''x'' {{=}} 0}}. The pink curve is a polynomial of degree seven:\n\n:<math>\\sin\\left( x \\right) \\approx x - \\frac{x^3}{3!} + \\frac{x^5}{5!} - \\frac{x^7}{7!}.\\!</math>\n\nThe error in this approximation is no more than {{math|{{sfrac|{{abs|''x''}}<sup>9</sup>|9!}}}}. In particular, for {{math|−1 < ''x'' < 1}}, the error is less than&nbsp;0.000003.\n\nIn contrast, also shown is a picture of the natural logarithm function {{math|[[Natural logarithm|log]](1 + ''x'')}} and some of its [[Taylor polynomial]]s around {{math|''a'' {{=}} 0}}. These approximations converge to the function only in the region {{math|−1 < ''x'' ≤ 1}}; outside of this region the higher-degree Taylor polynomials are ''worse'' approximations for the function. This is similar to [[Runge's phenomenon]].{{cn|date=November 2017}}\n\nThe ''error'' incurred in approximating a function by its {{mvar|n}}th-degree Taylor polynomial is called the ''remainder'' or ''[[Residual (numerical analysis)|residual]]'' and is denoted by the function {{math|''R''<sub>''n''</sub>(''x'')}}. [[Taylor's theorem]] can be used to obtain a bound on the size of the remainder.\n\nIn general, Taylor series need not be [[convergent series|convergent]] at all. And in fact the set of functions with a convergent Taylor series is a [[meager set]] in the [[Fréchet space]] of [[smooth functions]]. And even if the Taylor series of a function {{mvar|f}} does converge, its limit need not in general be equal to the value of the function {{math|''f''&thinsp;(''x'')}}. For example, the function\n:<math>\nf(x) = \\begin{cases}\ne^{-\\frac{1}{x^2}}&\\text{if } x\\neq0\\\\\n0&\\text{if } x=0\n\\end{cases}\n</math>\nis [[infinitely differentiable]] at {{math|''x'' {{=}} 0}}, and has all derivatives zero there. Consequently, the Taylor series of {{math|''f''&thinsp;(''x'')}} about {{math|''x'' {{=}} 0}} is identically zero. However, {{math|''f''&thinsp;(''x'')}} is not the zero function, so does not equal its Taylor series around the origin.  Thus, {{math|''f''&thinsp;(''x'')}} is an example of a [[non-analytic smooth function]].\n\nIn [[real analysis]], this example shows that there are [[infinitely differentiable function]]s {{math|''f''&thinsp;(''x'')}} whose Taylor series are ''not'' equal to {{math|''f''&thinsp;(''x'')}} even if they converge. By contrast, the [[holomorphic function]]s studied in [[complex analysis]] always possess a convergent Taylor series, and even the Taylor  series of [[meromorphic function]]s, which might have singularities, never converge to a value different from the function itself. The complex function {{math|''e''<sup>−1/''z''<sup>2</sup></sup>}}, however, does not approach 0 when {{mvar|z}} approaches 0 along the imaginary axis, so it is not [[Continuous function|continuous]] in the complex plane and its Taylor series is undefined at 0.\n\nMore generally, every sequence of real or complex numbers can appear as [[coefficient]]s in the Taylor series of an infinitely differentiable function defined on the real line, a consequence of [[Borel's lemma]].  As a result, the [[radius of convergence]] of a Taylor series can be zero. There are even infinitely differentiable functions defined on the real line whose Taylor series have a radius of convergence 0 everywhere.<ref>{{Citation | last = Rudin | first = Walter | author-link = Walter Rudin| title = Real and Complex Analysis | place = New Dehli | publisher = McGraw-Hill | year = 1980 | page = 418, Exercise 13 | isbn = 0-07-099557-5 | postscript = <!--none-->}}</ref>\n\nA function cannot be written as a Taylor series centred at a [[singularity (mathematics)|singularity]]; in these cases, one can often still achieve a series expansion if one allows also negative powers of the variable {{mvar|x}}; see [[Laurent series]]. For example, {{math|''f''&thinsp;(''x'') {{=}} ''e''<sup>−1/''x''<sup>2</sup></sup>}} can be written as a Laurent series.\n\n===Generalization===\nThere is, however, a generalization<ref>{{citation|first=William|last=Feller|authorlink=William Feller|title=An introduction to probability theory and its applications, Volume 2|edition=3rd|publisher=Wiley|year=1971|pages=230–232}}.</ref><ref>{{citation|first1=Einar|last1=Hille|authorlink1=Einar Hille|first2=Ralph S.|last2=Phillips|authorlink2=Ralph S. Phillips|title=Functional analysis and semi-groups|publisher=American Mathematical Society|series=AMS Colloquium Publications|volume=31|year=1957|pages=300–327}}.</ref> of the Taylor series that does converge to the value of the function itself for any [[bounded function|bounded]] [[continuous function]] on {{math|(0,∞)}}, using the calculus of [[finite differences]].  Specifically, one has the following theorem, due to [[Einar Hille]], that for any {{math|''t'' > 0}},\n:<math>\\lim_{h\\to 0^+}\\sum_{n=0}^\\infty \\frac{t^n}{n!}\\frac{\\Delta_h^nf(a)}{h^n} = f(a+t).</math>\nHere {{math|Δ{{su|p=''n''|b=''h''}}}} is the {{mvar|n}}th finite difference operator with step size {{mvar|h}}. The series is precisely the Taylor series, except that divided differences appear in place of differentiation: the series is formally similar to the [[Newton series]]. When the function {{mvar|f}} is analytic at {{mvar|a}}, the terms in the series converge to the terms of the Taylor series, and in this sense generalizes the usual Taylor series.\n\nIn general, for any infinite sequence {{math|''a''<sub>''i''</sub>}}, the following power series identity holds:\n:<math>\\sum_{n=0}^\\infty\\frac{u^n}{n!}\\Delta^na_i = e^{-u}\\sum_{j=0}^\\infty\\frac{u^j}{j!}a_{i+j}.</math>\nSo in particular,\n:<math>f(a+t) = \\lim_{h\\to 0^+} e^{-\\frac{t}{h}}\\sum_{j=0}^\\infty f(a+jh) \\frac{\\left(\\frac{t}{h}\\right)^j}{j!}.</math>\nThe series on the right is the [[expectation value]] of {{math|''f''&thinsp;(''a'' + ''X'')}}, where {{mvar|X}} is a [[Poisson distribution|Poisson-distributed]] [[random variable]] that takes the value {{math|''jh''}} with probability {{math|''e''<sup>−''t''/''h''</sup>·{{sfrac|(''t''/''h''){{isup|''j''}}|''j''!}}}}. Hence,\n:<math>f(a+t) = \\lim_{h\\to 0^+} \\int_{-\\infty}^\\infty f(a+x)dP_{\\frac{t}{h},h}(x).</math>\nThe [[law of large numbers]] implies that the identity holds.<ref>{{cite book|authorlink=William Feller|first=William|last=Feller|title=An introduction to probability theory and its applications|volume=2|edition=3|page=231|year=1970}}</ref>\n\n==List of Maclaurin series of some common functions==\n{{see also|List of mathematical series}}\nSeveral important Maclaurin series expansions follow.<ref>Most of these can be found in {{harv|Abramowitz|Stegun|1970}}.</ref> All these expansions are valid for complex arguments {{mvar|x}}.\n\n=== Exponential function ===\n[[File:Exp series.gif|right|thumb|The [[exponential function]] {{math|''e''<sup>''x''</sup>}} (in blue), and the sum of the first {{math|''n'' + 1}} terms of its Taylor series at 0 (in red).]]\nThe [[exponential function]] <math>e^x</math> (with base [[e (mathematics)|{{mvar|e}}]]) has Maclaurin series\n:<math>e^{x} = \\sum^{\\infty}_{n=0} \\frac{x^n}{n!} = 1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\cdots </math>.\nIt converges for all {{mvar|x}}.\n\n=== Natural logarithm ===\nThe [[natural logarithm]] (with base [[e (mathematics)|{{mvar|e}}]]) has Maclaurin series\n:<math>\\begin{align}\n\\log(1-x) &= - \\sum^{\\infty}_{n=1} \\frac{x^n}n = -x - \\frac{x^2}2 - \\frac{x^3}3 - \\cdots , \\\\\n\\log(1+x) &= \\sum^\\infty_{n=1} (-1)^{n+1}\\frac{x^n}n = x - \\frac{x^2}2 + \\frac{x^3}3 - \\cdots .\n\\end{align}</math>\nThey converge for <math>|x| < 1</math>. Also log(1-x) converges for x=-1 and log(1+x) converges for x=1.\n\n=== Geometric series ===\n\nThe [[geometric series]] and its derivatives have Maclaurin series\n\n:<math>\\begin{align}\n\\frac{1}{1-x} &= \\sum^\\infty_{n=0} x^n \\\\\n\\frac{1}{(1-x)^2} &= \\sum^\\infty_{n=1} nx^{n-1}\\\\\n\\frac{1}{(1-x)^3} &= \\sum^\\infty_{n=2} \\frac{(n-1)n}{2} x^{n-2}.\n\\end{align}</math>\nAll are convergent for <math>|x| < 1</math>.  These are special cases of the [[#Binomial series|binomial series]] given in the next section.\n\n=== Binomial series ===\n\nThe [[binomial series]] is the power series\n\n:<math>(1+x)^\\alpha = \\sum_{n=0}^\\infty \\binom{\\alpha}{n} x^n</math>\n\nwhose coefficients are the generalized [[binomial coefficient]]s\n\n: <math>\\binom{\\alpha}{n} = \\prod_{k=1}^n \\frac{\\alpha-k+1}k = \\frac{\\alpha(\\alpha-1)\\cdots(\\alpha-n+1)}{n!}.</math>\n\n(If {{math| ''n'' {{=}} 0}}, this product is an [[empty product]] and has value 1.)  It converges for <math>|x| < 1</math> for any real or complex number {{mvar|α}}.\n\nWhen {{math|''α'' {{=}} −1}}, this is essentially the infinite geometric series mentioned in the previous section.  The special cases {{math|''α'' {{=}} {{sfrac|1|2}}}} and {{math|''α'' {{=}} −{{sfrac|1|2}}}} give the [[square root]] function and its [[multiplicative inverse|inverse]]:\n\n:<math>\\begin{align} \n(1+x)^\\frac12 &= 1 + \\tfrac{1}{2}x - \\tfrac{1}{8}x^2 + \\tfrac{1}{16}x^3 - \\tfrac{5}{128}x^4 + \\tfrac{7}{256}x^5 - \\ldots, \\\\\n(1+x)^{-\\frac12} &= 1 -\\tfrac{1}{2}x + \\tfrac{3}{8}x^2 - \\tfrac{5}{16}x^3 + \\tfrac{35}{128}x^4 - \\tfrac{63}{256}x^5 + \\ldots.\n\\end{align}</math>\n\nWhen only the [[linear approximation|linear term]] is retained, this simplifies to the [[binomial approximation]].\n\n=== Trigonometric functions ===\nThe usual [[trigonometric function]]s and their inverses have the following Maclaurin series:\n:<math>\\begin{align}\n\\sin x &= \\sum^{\\infty}_{n=0} \\frac{(-1)^n}{(2n+1)!} x^{2n+1} &&= x - \\frac{x^3}{3!} + \\frac{x^5}{5!} - \\cdots && \\text{for all } x\\\\[6pt]\n\\cos x &= \\sum^{\\infty}_{n=0} \\frac{(-1)^n}{(2n)!} x^{2n} &&= 1 - \\frac{x^2}{2!} + \\frac{x^4}{4!} - \\cdots && \\text{for all } x\\\\[6pt]\n\\tan x &= \\sum^{\\infty}_{n=1} \\frac{B_{2n} (-4)^n \\left(1-4^n\\right)}{(2n)!} x^{2n-1} &&= x + \\frac{x^3}{3} + \\frac{2 x^5}{15} + \\cdots && \\text{for }|x| < \\frac{\\pi}{2}\\\\[6pt]\n\\sec x &= \\sum^{\\infty}_{n=0} \\frac{(-1)^n E_{2n}}{(2n)!} x^{2n} &&=1+\\frac{x^2}{2}+\\frac{5x^4}{24}+\\cdots && \\text{for }|x| < \\frac{\\pi}{2}\\\\[6pt]\n\\arcsin x &= \\sum^{\\infty}_{n=0} \\frac{(2n)!}{4^n (n!)^2 (2n+1)} x^{2n+1} &&=x+\\frac{x^3}{6}+\\frac{3x^5}{40}+\\cdots && \\text{for }|x| \\le 1\\\\[6pt]\n\\arccos x &=\\frac{\\pi}{2}-\\arcsin x\\\\&=\\frac{\\pi}{2}- \\sum^{\\infty}_{n=0} \\frac{(2n)!}{4^n (n!)^2 (2n+1)} x^{2n+1}&&=\\frac{\\pi}{2}-x-\\frac{x^3}{6}-\\frac{3x^5}{40}-\\cdots&& \\text{for }|x| \\le 1\\\\[6pt]\n\\arctan x &= \\sum^{\\infty}_{n=0} \\frac{(-1)^n}{2n+1} x^{2n+1} &&=x-\\frac{x^3}{3} + \\frac{x^5}{5}-\\cdots && \\text{for }|x| \\le 1,\\ x\\neq\\pm i\n\\end{align}</math>\n\nAll angles are expressed in [[radian]]s. The numbers {{math|''B<sub>k</sub>''}} appearing in the expansions of {{math|tan ''x''}} are the [[Bernoulli numbers]]. The {{math|''E''<sub>''k''</sub>}} in the expansion of {{math|sec ''x''}} are [[Euler number]]s.\n\n=== Hyperbolic functions ===\nThe [[hyperbolic function]]s have Maclaurin series closely related to the series for the corresponding trigonometric functions:\n:<math>\\begin{align}\n\\sinh x &= \\sum^{\\infty}_{n=0} \\frac{x^{2n+1}}{(2n+1)!} &&= x + \\frac{x^3}{3!} + \\frac{x^5}{5!} + \\cdots && \\text{for all } x\\\\[6pt]\n\\cosh x &= \\sum^{\\infty}_{n=0} \\frac{x^{2n}}{(2n)!} &&= 1 + \\frac{x^2}{2!} + \\frac{x^4}{4!} + \\cdots && \\text{for all } x\\\\[6pt]\n\\tanh x &= \\sum^{\\infty}_{n=1} \\frac{B_{2n} 4^n \\left(4^n-1\\right)}{(2n)!} x^{2n-1} &&= x-\\frac{x^3}{3}+\\frac{2x^5}{15}-\\frac{17x^7}{315}+\\cdots && \\text{for }|x| < \\frac{\\pi}{2}\\\\[6pt]\n\\operatorname{arsinh} x &= \\sum^{\\infty}_{n=0} \\frac{(-1)^n (2n)!}{4^n (n!)^2 (2n+1)} x^{2n+1} && && \\text{for }|x| \\le 1\\\\[6pt]\n\\operatorname{artanh} x &= \\sum^{\\infty}_{n=0} \\frac{x^{2n+1}}{2n+1} && && \\text{for }|x| \\le 1,\\ x\\neq\\pm 1\n\\end{align}</math>\n\nThe numbers {{math|''B<sub>k</sub>''}} appearing in the series for {{math|tanh ''x''}} are the [[Bernoulli numbers]].\n\n==Calculation of Taylor series==\nSeveral methods exist for the calculation of Taylor series of a large number of functions. One can attempt to use the definition of the Taylor series, though this often requires generalizing the form of the coefficients according to a readily apparent pattern. Alternatively, one can use manipulations such as substitution, multiplication or division, addition or subtraction of standard Taylor series to construct the Taylor series of a function, by virtue of Taylor series being power series. In some cases, one can also derive the Taylor series by repeatedly applying [[integration by parts]]. Particularly convenient is the use of [[computer algebra system]]s to calculate Taylor series.\n\n===First example===\nIn order to compute the 7th degree Maclaurin polynomial for the function\n:<math>f(x)=\\log(\\cos x),\\quad x\\in\\left(-\\frac{\\pi}{2},\\frac{\\pi}{2}\\right)</math> ,\none may first rewrite the function as\n:<math>f(x)=\\log\\bigl(1+(\\cos x-1)\\bigr)\\!</math>.\nThe Taylor series for the natural logarithm is (using the [[big O notation]])\n:<math>\\log(1+x) = x - \\frac{x^2}2 + \\frac{x^3}3 + {O}\\left(x^4\\right)\\!</math>\nand for the cosine function\n:<math>\\cos x - 1 = -\\frac{x^2}2 + \\frac{x^4}{24} - \\frac{x^6}{720} + {O}\\left(x^8\\right)\\!</math>.\nThe latter series expansion has a zero [[constant term]], which enables us to substitute the second series into the first one and to easily omit terms of higher order than the 7th degree by using the big {{mvar|O}} notation:\n\n:<math>\\begin{align}f(x)&=\\log\\bigl(1+(\\cos x-1)\\bigr)\\\\\n&=(\\cos x-1) - \\tfrac12(\\cos x-1)^2 + \\tfrac13(\\cos x-1)^3+ {O}\\left((\\cos x-1)^4\\right)\\\\\n&=\\left(-\\frac{x^2}2 + \\frac{x^4}{24} - \\frac{x^6}{720} +{O}\\left(x^8\\right)\\right)-\\frac12\\left(-\\frac{x^2}2+\\frac{x^4}{24}+{O}\\left(x^6\\right)\\right)^2+\\frac13\\left(-\\frac{x^2}2+O\\left(x^4\\right)\\right)^3 + {O}\\left(x^8\\right)\\\\ & =-\\frac{x^2}2 + \\frac{x^4}{24}-\\frac{x^6}{720} - \\frac{x^4}8 + \\frac{x^6}{48} - \\frac{x^6}{24} +O\\left(x^8\\right)\\\\\n& =- \\frac{x^2}2 - \\frac{x^4}{12} - \\frac{x^6}{45}+O\\left(x^8\\right). \\end{align}\\!</math>\nSince the cosine is an [[even function]], the coefficients for all the odd powers {{math|''x'', ''x''<sup>3</sup>, ''x''<sup>5</sup>, ''x''<sup>7</sup>, ...}} have to be zero.\n\n===Second example===\nSuppose we want the Taylor series at 0 of the function\n: <math>g(x)=\\frac{e^x}{\\cos x}.\\!</math>\nWe have for the exponential function\n: <math>e^x = \\sum^\\infty_{n=0} \\frac{x^n}{n!} =1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\frac{x^4}{4!}+\\cdots\\!</math>\nand, as in the first example,\n: <math>\\cos x = 1 - \\frac{x^2}{2!} + \\frac{x^4}{4!} - \\cdots\\!</math>\nAssume the power series is\n: <math>\\frac{e^x}{\\cos x} = c_0 + c_1 x + c_2 x^2 + c_3 x^3 + \\cdots\\!</math>\nThen multiplication with the denominator and substitution of the series of the cosine yields\n: <math>\\begin{align} e^x &= \\left(c_0 + c_1 x + c_2 x^2 + c_3 x^3 + \\cdots\\right)\\cos x\\\\\n&=\\left(c_0 + c_1 x + c_2 x^2 + c_3 x^3 + c_4x^4 + \\cdots\\right)\\left(1 - \\frac{x^2}{2!} + \\frac{x^4}{4!} - \\cdots\\right)\\\\&=c_0 - \\frac{c_0}{2}x^2 + \\frac{c_0}{4!}x^4 + c_1x - \\frac{c_1}{2}x^3 + \\frac{c_1}{4!}x^5 + c_2x^2 - \\frac{c_2}{2}x^4 + \\frac{c_2}{4!}x^6 + c_3x^3 - \\frac{c_3}{2}x^5 + \\frac{c_3}{4!}x^7 + c_4x^4 +\\cdots \\end{align}\\!</math>\nCollecting the terms up to fourth order yields\n: <math>e^x =c_0 + c_1x + \\left(c_2 - \\frac{c_0}{2}\\right)x^2 + \\left(c_3 - \\frac{c_1}{2}\\right)x^3+\\left(c_4-\\frac{c_2}{2}+\\frac{c_0}{4!}\\right)x^4 + \\cdots\\!</math>\nThe values of <math>c_i</math> can be found by comparison of coefficients with the top expression for <math>e^x</math>, yielding:\n: <math>\\frac{e^x}{\\cos x}=1 + x + x^2 + \\frac{2x^3}{3} + \\frac{x^4}{2} + \\cdots.\\!</math>\n\n===Third example===\nHere we employ a method called \"indirect expansion\" to expand the given function. This method uses the known Taylor expansion of the exponential function. In order to expand {{math|(1 + ''x'')''e<sup>x</sup>''}} as a Taylor series in {{mvar|x}}, we use the known Taylor series of function {{math|''e''<sup>''x''</sup>}}:\n: <math>e^x = \\sum^\\infty_{n=0} \\frac{x^n}{n!} =1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\frac{x^4}{4!}+\\cdots.</math>\nThus,\n: <math>\\begin{align}(1+x)e^x &= e^x + xe^x = \\sum^\\infty_{n=0} \\frac{x^n}{n!} + \\sum^\\infty_{n=0} \\frac{x^{n+1}}{n!} = 1 + \\sum^\\infty_{n=1} \\frac{x^n}{n!} + \\sum^\\infty_{n=0} \\frac{x^{n+1}}{n!} \\\\ &= 1 + \\sum^\\infty_{n=1} \\frac{x^n}{n!} + \\sum^\\infty_{n=1} \\frac{x^n}{(n-1)!} =1 + \\sum^\\infty_{n=1}\\left(\\frac{1}{n!} + \\frac{1}{(n-1)!}\\right)x^n \\\\ &= 1 + \\sum^\\infty_{n=1}\\frac{n+1}{n!}x^n\\\\ &= \\sum^\\infty_{n=0}\\frac{n+1}{n!}x^n.\\end{align}</math>\n\n==Taylor series as definitions==\nClassically, [[algebraic function]]s are defined by an algebraic equation, and [[transcendental function]]s (including those discussed above) are defined by some property that holds for them, such as a [[differential equation]]. For example, the [[exponential function]] is the function which is equal to its own derivative everywhere, and assumes the value 1 at the origin. However, one may equally well define an [[analytic function]] by its Taylor series.\n\nTaylor series are used to define functions and \"[[operator (mathematics)|operator]]s\" in diverse areas of mathematics. In particular, this is true in areas where the classical definitions of functions break down. For example, using Taylor series, one may extend analytic functions to sets of matrices and operators, such as the [[matrix exponential]] or [[matrix logarithm]].\n\nIn other areas, such as formal analysis, it is more convenient to work directly with the [[power series]] themselves. Thus one may define a solution of a differential equation ''as'' a power series which, one hopes to prove, is the Taylor series of the desired solution.\n\n==Taylor series in several variables==\n\nThe Taylor series may also be generalized to functions of more than one variable with<ref>{{citation|author=[[Lars Hörmander]]|title=The analysis of partial differential operators, volume 1|year=1990|publisher=Springer|at=Eqq. 1.1.7 and 1.1.7′}}</ref><ref>{{citation|author1=Duistermaat|author2=Kolk|title=Distributions: Theory and applications|publisher=Birkhauser|year=2010|at=ch. 6}}</ref>\n\n:<math>\\begin{align}\nT(x_1,\\ldots,x_d) &= \\sum_{n_1=0}^\\infty \\cdots \\sum_{n_d = 0}^\\infty  \\frac{(x_1-a_1)^{n_1}\\cdots (x_d-a_d)^{n_d}}{n_1!\\cdots n_d!}\\,\\left(\\frac{\\partial^{n_1 + \\cdots + n_d}f}{\\partial x_1^{n_1}\\cdots \\partial x_d^{n_d}}\\right)(a_1,\\ldots,a_d) \\\\\n&= f(a_1, \\ldots,a_d) + \\sum_{j=1}^d \\frac{\\partial f(a_1, \\ldots,a_d)}{\\partial x_j} (x_j - a_j) + \\frac{1}{2!} \\sum_{j=1}^d \\sum_{k=1}^d \\frac{\\partial^2 f(a_1, \\ldots,a_d)}{\\partial x_j \\partial x_k} (x_j - a_j)(x_k - a_k) + \\\\ \n& \\qquad \\qquad + \\frac{1}{3!} \\sum_{j=1}^d\\sum_{k=1}^d\\sum_{l=1}^d \\frac{\\partial^3 f(a_1, \\ldots,a_d)}{\\partial x_j \\partial x_k \\partial x_l} (x_j - a_j)(x_k - a_k)(x_l - a_l) + \\cdots\n\\end{align}</math>\n\nFor example, for a function <math>f(x,y)</math> that depends on two variables, {{mvar|x}} and {{mvar|y}}, the Taylor series to second order about the point {{math|(''a'', ''b'')}} is\n\n:<math>f(a,b) +(x-a) f_x(a,b) +(y-b) f_y(a,b) + \\frac{1}{2!}\\Big( (x-a)^2 f_{xx}(a,b) + 2(x-a)(y-b) f_{xy}(a,b) +(y-b)^2 f_{yy}(a,b) \\Big)</math>\n\nwhere the subscripts denote the respective [[partial derivative]]s.\n\nA second-order Taylor series expansion of a scalar-valued function of more than one variable can be written compactly as\n\n:<math>T(\\mathbf{x}) = f(\\mathbf{a}) + (\\mathbf{x} - \\mathbf{a})^\\mathsf{T} D f(\\mathbf{a}) + \\frac{1}{2!} (\\mathbf{x} - \\mathbf{a})^\\mathsf{T} \\left \\{D^2 f(\\mathbf{a}) \\right \\} (\\mathbf{x} - \\mathbf{a}) + \\cdots,</math>\n\nwhere {{math|''D'' ''f''&thinsp;('''a''')}} is the [[gradient]] of {{mvar|f}} evaluated at {{math|'''x''' {{=}} '''a'''}} and {{math|''D''<sup>2</sup> ''f''&thinsp;('''a''')}} is the [[Hessian matrix]]. Applying the [[multi-index notation]] the Taylor series for several variables becomes\n\n:<math>T(\\mathbf{x}) = \\sum_{|\\alpha| \\geq 0}\\frac{(\\mathbf{x}-\\mathbf{a})^\\alpha}{\\alpha !} \\left({\\mathrm{\\partial}^{\\alpha}}f\\right)(\\mathbf{a}),</math>\n\nwhich is to be understood as a still more abbreviated [[multi-index]] version of the first equation of this paragraph, with a full analogy to the single variable case. \n\n=== Example ===\n[[Image:Second Order Taylor.svg|200px|thumb|right|Second-order Taylor series approximation (in orange) of a function {{math|''f''&thinsp;(''x'',''y'') {{=}} ''e<sup>x</sup>'' log(1 + ''y'')}} around the origin.]]\nIn order to compute a second-order Taylor series expansion around point {{math|(''a'', ''b'') {{=}} (0, 0)}} of the function\n:<math>f(x,y)=e^x\\log(1+y),</math>\n\none first computes all the necessary partial derivatives:\n\n:<math>\\begin{align}\nf_x &= e^x\\log(1+y) \\\\[6pt]\nf_y &= \\frac{e^x}{1+y} \\\\[6pt]\nf_{xx} &= e^x\\log(1+y) \\\\[6pt]\nf_{yy}  &= - \\frac{e^x}{(1+y)^2}  \\\\[6pt]\nf_{xy} &=f_{yx} =  \\frac{e^x}{1+y} .\n\\end{align}</math>\n\nEvaluating these derivatives at the origin gives the Taylor coefficients\n\n:<math>\\begin{align}\nf_x(0,0) &= 0 \\\\\nf_y(0,0) &=1 \\\\\nf_{xx}(0,0) &=0 \\\\\nf_{yy}(0,0) &=-1 \\\\\nf_{xy}(0,0) &=f_{yx}(0,0)=1.\n\\end{align}</math>\n\nSubstituting these values in to the general formula\n\n:<math>T(x,y) = f(a,b) +(x-a) f_x(a,b) +(y-b) f_y(a,b) +\\frac{1}{2!}\\Big( (x-a)^2f_{xx}(a,b) + 2(x-a)(y-b)f_{xy}(a,b) +(y-b)^2 f_{yy}(a,b) \\Big)+ \\cdots</math>\n\nproduces\n\n:<math>\\begin{align}\nT(x,y) &= 0 + 0(x-0) + 1(y-0) + \\frac{1}{2}\\Big( 0(x-0)^2 + 2(x-0)(y-0) + (-1)(y-0)^2 \\Big) + \\cdots \\\\\n&= y + xy - \\frac{y^2}{2} + \\cdots \n\\end{align}</math>\n\nSince {{math|log(1 + ''y'')}} is analytic in {{math|{{abs|''y''}} < 1}}, we have\n:<math>e^x\\log(1+y)= y + xy - \\frac{y^2}{2} + \\cdots, \\qquad |y| < 1.</math>\n\n== Comparison with Fourier series ==\n{{main|Fourier series}}\nThe trigonometric [[Fourier series]] enables one to express a [[periodic function]] (or a function defined on a closed interval {{math|[''a'',''b'']}}) as an infinite sum of [[trigonometric function]]s ([[sine]]s and [[cosine]]s). In this sense, the Fourier series is analogous to Taylor series, since the latter allows one to express a function as an infinite sum of [[power function|powers]]. Nevertheless, the two series differ from each other in several relevant issues:\n* The finite truncations of the Taylor series of {{math|''f''&thinsp;(''x'')}} about the point {{math|''x'' {{=}} ''a''}} are all exactly equal to {{math|''f''}} at {{math|''a''}}. In contrast, the Fourier series is computed by integrating over an entire interval, so there is generally no such point where all the finite truncations of the series are exact.\n* The computation of Taylor series requires the knowledge of the function on an arbitrary small [[Neighbourhood (mathematics)|neighbourhood]] of a point, whereas the computation of the Fourier series requires knowing the function on its whole domain [[interval (mathematics)|interval]]. In a certain sense one could say that the Taylor series is \"local\" and the Fourier series is \"global\".\n* The Taylor series is defined for a function which has infinitely many derivatives at a single point, whereas the Fourier series is defined for any [[integrable function|integrable]] function.  In particular, the function could be nowhere differentiable. (For example, {{math|''f''&thinsp;(''x'')}} could be a [[Weierstrass function]].)\n* The convergence of both series has very different properties. Even if the Taylor series has positive convergence radius, the resulting series may not coincide with the function; but if the function is analytic then the series converges [[pointwise convergence|pointwise]] to the function, and [[uniform convergence|uniformly]] on every compact subset of the convergence interval. Concerning the Fourier series, if the function is [[Square-integrable function|square-integrable]] then the series converges in [[Convergence in quadratic mean|quadratic mean]], but additional requirements are needed to ensure the pointwise or uniform convergence (for instance, if the function is periodic and of class C<sup>1</sup> then the convergence is uniform).\n* Finally, in practice one wants to approximate the function with a finite number of terms, say with a Taylor polynomial or a partial sum of the trigonometric series, respectively. In the case of the Taylor series the error is very small in a neighbourhood of the point where it is computed, while it may be very large at a distant point. In the case of the Fourier series the error is distributed along the domain of the function.\n\n==See also==\n* [[Asymptotic expansion]]\n* [[Generating function]]\n* [[Laurent series]]\n* [[Madhava series]]\n* [[Newton polynomial|Newton's divided difference interpolation]]\n* [[Padé approximant]]\n* [[Puiseux series]]\n* [[Shift operator]]\n\n==Notes==\n{{reflist|30em}}\n\n==References==\n*{{Citation| last1=Abramowitz| first1=Milton| author1-link=Milton Abramowitz| last2=Stegun| first2=Irene A.| author2-link=Irene Stegun| title=[[Abramowitz and Stegun|Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables]]| publisher=[[Dover Publications]]| location=New York| id=Ninth printing| year=1970| postscript=<!--none-->}}\n*{{citation| last1=Thomas|first1=George B., Jr.|last2=Finney|first2=Ross L.| title=Calculus and Analytic Geometry |edition=9th | publisher=Addison Wesley| year=1996| isbn=0-201-53174-7| postscript=<!--none-->}}\n*{{citation| last=Greenberg|first=Michael| title=Advanced Engineering Mathematics |edition=2nd | publisher=Prentice Hall| year=1998| isbn=0-13-321431-1| postscript=<!--none-->}}\n\n==External links==\n{{Sister project links\n|wikt=Taylor series\n|commons=Category:Taylor series\n|b=Calculus/Taylor series\n|v=Taylor's series\n|n=no |q=no |s=no |species=no\n|d=Q131187}}\n* {{springer|title=Taylor series|id=p/t092320}}\n* {{MathWorld| urlname= TaylorSeries| title= Taylor Series}}\n* [http://blog.ivank.net/taylor-polynomial-clarified.html Taylor polynomial] - practical introduction\n* [http://www-groups.dcs.st-and.ac.uk/~history/Projects/Pearce/Chapters/Ch9_3.html Madhava of Sangamagramma ]\n* \"[http://csma31.csm.jmu.edu/physics/rudmin/ParkerSochacki.htm Discussion of the Parker-Sochacki Method]\"\n* [https://web.archive.org/web/20070605020930/http://stud3.tuwien.ac.at/~e0004876/taylor/Taylor_en.html Another Taylor visualisation] &mdash; where you can choose the point of the approximation and the number of derivatives\n* [http://numericalmethods.eng.usf.edu/topics/taylor_series.html Taylor series revisited for numerical methods] at [http://numericalmethods.eng.usf.edu Numerical Methods for the STEM Undergraduate]\n* [http://cinderella.de/files/HTMLDemos/2C02_Taylor.html  Cinderella 2: Taylor expansion]\n* [http://www.sosmath.com/calculus/tayser/tayser01/tayser01.html Taylor series]\n* [http://www.efunda.com/math/taylor_series/inverse_trig.cfm Inverse trigonometric functions Taylor series]\n* {{cite web |title=Essence of Calculus: Taylor series |url=https://www.youtube.com/watch?v=3d6DsjIBzJ4&index=11&list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr |via=[[YouTube]] }}\n\n[[Category:Real analysis]]\n[[Category:Complex analysis]]\n[[Category:Series expansions]]"
    },
    {
      "title": "Root-finding algorithm",
      "url": "https://en.wikipedia.org/wiki/Root-finding_algorithm",
      "text": "In [[mathematics]] and [[computing]], a '''root-finding algorithm''' is an [[algorithm]] for finding roots of [[continuous function]]s.   A [[root of a function|root]] of a [[function (mathematics)|function]] {{math|''f''}}, from the [[real number]]s to real numbers or from the [[complex number]]s to the complex numbers, is a number {{math|''x''}} such that {{math|1=''f''(''x'') = 0}}. As, generally, the roots of a function cannot be computed exactly, nor expressed in [[closed form expression|closed form]], root-finding algorithms provide approximations to roots, expressed either as [[floating point]] numbers or as small isolating [[interval (mathematics)|intervals]],  or [[disk (mathematics)|disks]] for complex roots (an interval or disk output being equivalent to an approximate output together with an error bound).\n\n[[equation solving|Solving an equation]] {{math|1=''f''(''x'') = ''g''(''x'')}} is the same as finding the roots of the function {{math|1=''h''(''x'') = ''f''(''x'') – ''g''(''x'')}}. Thus root-finding algorithms allow solving any [[equation (mathematics)|equation]] defined by continuous functions. However, most root-finding algorithms do not guarantee that they will find all the roots; in particular, if such an algorithm does not find any root, that does not mean that no root exists. \n\nMost numerical root-finding methods use [[iteration]], producing a [[sequence]] of numbers that hopefully converge towards the root as a  [[Limit of a sequence|limit]]. They require one or more ''initial guesses'' of the root as starting values, then each iteration of the algorithm produces a successively more accurate approximation to the root.   Since the iteration must be stopped at some point these methods produce an approximation to the root, not an exact solution.    Many methods compute subsequent values by evaluating an auxiliary function on the preceding values. The limit is thus a [[Fixed point (mathematics)|fixed point]] of the auxiliary function, which is chosen for having the roots of the original equation as fixed points, and for converging rapidly to these fixed points.\n\nThe behaviour of general root-finding algorithms is studied in [[numerical analysis]]. However, for polynomials, root-finding study belongs generally to [[computer algebra]], since algebraic properties of polynomials are fundamental for the most efficient algorithms. The efficiency of an algorithm may depend dramatically on the characteristics of the given functions. For example, many algorithms use the [[derivative]] of the input function, while others work on every [[continuous function]]. In general, numerical algorithms are not guaranteed to find all the roots of a function, so failing to find a root does not prove that there is no root. However, for [[polynomial]]s, there are specific algorithms that use algebraic properties for certifying that no root is missed, and locating the roots in separate intervals (or [[disk (mathematics)|disks]] for complex roots) that are small enough to ensure the convergence of numerical methods (typically [[Newton's method]]) to the unique root so located.\n\n== Bracketing methods ==\n\nBracketing methods determine successively smaller intervals (brackets) that contain a root. When the interval is small enough, then a root has been found. They generally use the [[intermediate value theorem]], which asserts that if a continuous function has values of opposite signs at the end points of an interval, then the function has at least one root in the interval. Therefore, they require to start with an interval such that the function takes opposite signs at the end points of the interval. However, in the case of [[polynomial]]s there are other methods ([[Descartes' rule of signs]], [[Budan's theorem]] and [[Sturm's theorem]]) for getting information on the number of roots in an interval. They lead to efficient algorithms for [[real-root isolation]] of polynomials, which ensure finding all real roots with a guaranteed accuracy.\n\n=== Bisection method ===\nThe simplest root-finding algorithm is the [[bisection method]]. Let {{math|''f''}} be a [[continuous function]], for which one knows an interval {{math|[''a'', ''b'']}} such that  {{math|''f''(''a'')}} and {{math|''f''(''b'')}} have opposite signs (a bracket). Let {{math|1=''c'' = (''a'' +''b'')/2}} be the middle of the interval (the midpoint or the point that bisects the interval). Then either {{math|''f''(''a'')}} and {{math|''f''(''c'')}}, or {{math|''f''(''c'')}} and {{math|''f''(''b'')}} have opposite signs, and one has divided by two the size of the interval. Although the bisection method is robust, it gains one and only one [[bit]] of accuracy with each iteration. Other methods, under appropriate conditions, can gain accuracy faster.\n\n=== False position (''regula falsi'') ===\nThe [[false position method]], also called the ''regula falsi'' method, is similar to the bisection method, but instead of using bisection search's middle of the interval it uses the [[x-intercept|{{math|''x''}}-intercept]] of the line that connects the plotted function values at the endpoints of the interval, that is \n:<math>c= \\frac{af(b)-bf(a)}{f(b)-f(a)}.</math>\n\nFalse position is similar to the [[secant method]], except that, instead of retaining the last two points, it makes sure to keep one point on either side of the root.  The false position method can be faster than the bisection method and will never diverge like the secant method; however, it may fail to converge in some naive implementations due to roundoff errors that may lead to a wrong sign for {{math|''f''(''c'')}}; typically, this may occur if the [[rate of variation]] of {{mvar|f}} is large in the neighborhood of the root.\n\n[[Ridders' method]] is a variant of the false position method that uses the value of function at the midpoint of the interval, for getting a function with the same root, to which the false position method is applied. This gives a faster convergence with a similar robustness.\n\n== Interpolation ==\n\nMany root-finding processes work by [[interpolation]]. This consists in using the last computed approximate values of the root for approximating the function by a [[polynomial]] of low degree, which takes the same values at these approximate roots. Then the root of the polynomial is computed and used as a new approximate value of the root of the function, and the process is iterated.\n\nTwo values allow interpolating a function by a polynomial of degree one (that is approximating the graph of the function by a line). This is the basis of the [[secant method]]. Three values define a [[quadratic function]], which approximates the graph of the function by a [[parabola]]. This is [[Muller's method]].\n\n''Regula falsi'' is also an interpolation method, which differs secant method by using, for interpolating by a line, two points that are not necessarily the last two computed points.\n\n== Iterative methods ==\nAlthough all root-finding algorithms proceed by [[iteration]], an [[iterative method|iterative]] root-finding method generally use a specific type of iteration, consisting of defining an auxiliary function, which is applied to the last computed approximations of a root for getting a new approximation. The iteration stops when a [[fixed-point iteration|fixed point]] ([[up to]] the desired precision) of the auxiliary function is reached, that is when the new computed value is sufficiently close to the preceding ones.\n\n=== Newton's method (and similar derivative-based methods) ===\n[[Newton's method]] assumes the function ''f'' to have a continuous [[derivative]]. Newton's method may not converge if started too far away from a root. However, when it does converge, it is faster than the bisection method, and is usually quadratic. Newton's method is also important because it readily generalizes to higher-dimensional problems. Newton-like methods with higher orders of convergence are the [[Householder's method]]s. The first one after Newton's method is [[Halley's method]] with cubic order of convergence.\n\n=== Secant method ===\nReplacing the derivative in Newton's method with a [[finite difference]], we get the [[secant method]]. This method does not require the computation (nor the existence) of a derivative, but the price is slower convergence (the order is approximately 1.6). A generalization of the secant method in higher dimensions is [[Broyden's method]].\n\n=== Steffensen's method ===\nIf we use a polynomial fit to remove the quadratic part of the finite difference used in the Secant method, so that it better approximates the derivative, we obtain [[Steffensen's method]], which has quadratic convergence, and whose behavior (both good and bad) is essentially the same Newton's method, but does not require a derivative.\n\n=== Inverse interpolation ===\nThe appearance of complex values in interpolation methods can be avoided by interpolating the [[inverse function|inverse]] of ''f'', resulting in the [[inverse quadratic interpolation]] method. Again, convergence is asymptotically faster than the secant method, but inverse quadratic interpolation often behaves poorly when the iterates are not close to the root.\n\n==Combinations of methods==\n\n=== Brent's method ===\n[[Brent's method]] is a combination of the bisection method, the secant method and [[inverse quadratic interpolation]]. At every iteration, Brent's method decides which method out of these three is likely to do best, and proceeds by doing a step according to that method. This gives a robust and fast method, which therefore enjoys considerable popularity.\n\n== Roots of polynomials ==\n\nFinding roots of [[polynomial]] is a long-standing problem that has been the object of many research along centuries. A witness of this is that, until the 19th century, [[algebra]] meant essentially [[theory of equations|theory of polynomial equations]].\n\nFinding the root of a [[linear polynomial]] (degree one) is easy and needs only one division. For [[quadratic polynomial]]s (degree two), the [[quadratic formula]] produces a solution, but its numerical evaluation may require some care for ensuring [[numerical stability]]. For degrees three and four, there are closed-form solutions in terms of [[radical expression|radicals]], which are generally not convenient for numerical evaluation, as being too complicated and involving the computation of several [[nth root|{{mvar|n}}th roots]] whose computation is not easier than the direct computation of the roots of the polynomial (for example the expression of the real roots of a [[cubic polynomial]] may involve non-real [[cube root]]s). For polynomials of degree five or higher [[Abel–Ruffini theorem]] asserts that there is, in general, no radical expression of the roots. \n\nSo, except for very low degrees, root finding of polynomials consists of finding approximations of the roots. By the [[fundamental theorem of algebra]], one knows that a polynomial of degree {{mvar|n}} has at most {{mvar|n}} real or complex roots, and this number is reached for almost all polynomials. \n\nIt follows that the problem of root finding for polynomials may be split in three different subproblems;\n* Finding one root\n* Finding all roots\n* Finding roots in a specific region of the [[complex plane]], typically the real roots or the real roots in a given interval (for example, when roots represents a physical quantity, only the real positive ones are interesting).\n\nFor finding one root, [[Newton's method]] and other general [[iterative method]]s work generally well.\n\nFor finding all the roots, the oldest method is, when a root {{mvar|r}} has been found, to divide the polynomial by {{math|''x'' – ''r''}}, and restart iteratively the search of a root of the quotient polynomial. However, except for low degrees, this does not work well because of the [[numerical instability]]: [[Wilkinson's polynomial]] shows that a very small modification of one coefficient may change dramatically not only the value of the roots, but also their nature (real or complex). Also, even with a good approximation, when one evaluates a polynomial at an approximate root, one may get a result that is far to be close to zero. For example, if a polynomial of degree 20 (the degree of Wilkinson's polynomial) has a root close to 10, the derivative of the polynomial at the root may be of the order of <math>10^{20};</math> this implies that an error of <math>10^{-10}</math> on the value of the root may produce a value of the polynomial at the approximate root that is of the order of <math>10^{10}.</math>\n\nFor avoiding these problems, methods have been elaborated, which compute all roots simultaneously, to any desired accuracy. Presently the most efficient method is [[Aberth method]]. A [[free software|free]] implementation is available under the name of [[MPSolve]]. This is a reference implementation, which can find routinely the roots of polynomials of degree larger than 1,000, with more than 1,000 significant decimal digits.\n\nThe methods for computing all roots may be used for computing real roots. However, it may be difficult to decide whether a root with a small imaginary part is real or not. Moreover, as the number of the real roots is, in the average, the logarithm of the degree, it a waste of computer resources, to compute the non-real roots when one is interested in real roots. \n\nThe oldest method for computing the number of real roots, and the number of roots in an interval results from [[Sturm's theorem]], but the methods based on [[Descartes' rule of signs]] and its extensions—[[Budan's theorem|Budan's]] and [[Vincent's theorem]]s—are generally more efficient. For root finding, all proceed by reducing the size of the intervals in which roots are searched until getting intervals containing zero or one root. Then the intervals containing one root may be further reduced for getting a quadratic convergence of [[Newton's method]] to the isolated roots. The main [[computer algebra system]]s ([[Maple (software)|Maple]], [[Mathematica]], [[SageMath]]) have each a variant of this method as the default algorithm for the real roots of a polynomial.  \n\n===Finding one root===\n\nThe most widely used method for computing a root is [[Newton's method]], which consists of the iterations of the computation of \n:<math>x_{n+1}=x_n-\\frac{f(x_n)}{f'(x_n)},</math>\nby starting from a well-chosen value <math>x_0.</math>\nIf {{mvar|f}} is a polynomial, the computation is faster when using [[Horner rule]] for computing the polynomial and its derivative. \n\nThe convergence is generally [[quadratic convergence|quadratic]], it may converge much slowly or even not converge at all. In particular, if the polynomial has no real root, and <math>x_0</math> is real, then Newton's method cannot converge. However, if the polynomial has a real root, which is larger than the larger real root of its derivative, then Newton's method converges quadratically to this largest root if <math>x_0</math> is larger that this larger root (there are easy ways for computing an upper bound of the roots, see [[Properties of polynomial roots]]). This is the starting point of [[Horner method]] for computing the roots.\n\nWhen one root {{mvar|r}} has been found, one may use [[Euclidean division of polynomials|Euclidean division]] for removing the factor {{math|''x'' – ''r''}} from the polynomial. Computing a root of the resulting quotient, and repeating the process provides, in principle, a way for computing all roots. However, this iterative scheme is numerically unstable; the approximation errors accumulate during the successive factorizations, so that the last roots are determined with a polynomial that deviates widely from a factor of the original polynomial. To reduce this error, one may, for each root that is found, restart Newton's method with the original polynomial, and this approximate root as starting value. \n\nHowever, there is no warranty that this will allow finding all roots. In fact, the problem of finding the roots of a polynomial from its coefficients is in general highly [[ill-conditioned]]. This is illustrated by \n[[Wilkinson's polynomial]]: the roots of this polynomial of degree 20 are the 20 first positive integers; changing the last bit of the 32-bit representation of one of its coefficient (equal to –210) produces a polynomial with only 10 real roots and 10 complex roots with imaginary parts larger than 0.6. \n\nClosely related to Newton's method are [[Halley's method]] and [[Laguerre's method]]. Both use the polynomial and its two first derivations for an iterative process that has a [[cubic convergence]]. Combining two consecutive steps of these methods into a single test, one gets a [[rate of convergence]] of 9, at the cost of 6 polynomial evaluations (with Horner rule). On the other hand, combining three steps of Newtons method gives a rate of convergence of 8 at the cost of the same number of polynomial evaluation. This gives a slight advantage to these methods (less clear for Laguerre's method, as a square root has to be computed at each step).\n\nWhen applying these methods to polynomials with real coefficients and real starting points, Newton's and Halley's method stay inside the real number line. One has to choose complex starting points to find complex roots. In contrast, the Laguerre method with a square root in its evaluation will leave the real axis of its own accord.\n\nAnother class of methods is based on converting the problem of finding polynomial roots to the problem of finding eigenvalues of the [[companion matrix]] of the polynomial. In principle, one can use any [[eigenvalue algorithm]] to find the roots of the polynomial. However, for efficiency reasons one prefers methods that employ the structure of the matrix, that is, can be implemented in matrix-free form. Among these methods are the [[power method]], whose application to the transpose of the companion matrix is the classical [[Bernoulli's method]] to find the root of greatest modulus. The [[inverse power method]] with shifts, which finds some smallest root first, is what drives the complex (''cpoly'') variant of the [[Jenkins–Traub algorithm]] and gives it its numerical stability. Additionally, it is insensitive to multiple roots and has fast convergence with order <math>1+\\varphi\\approx 2.6</math> (where <math>\\varphi</math> is the [[golden ratio]]) even in the presence of clustered roots. This fast convergence comes with a cost of three polynomial evaluations per step, resulting in a residual of {{math|''O''({{!}}''f''(''x''){{!}}<sup>2+3''φ''</sup>)}}, that is a slower convergence than with three steps of Newton's method.\n\n===Finding roots in pairs===\nIf the given polynomial only has real coefficients, one may wish to avoid computations with complex numbers. To that effect, one has to find quadratic factors for pairs of conjugate complex roots. The application of the [[multidimensional Newton's method]] to this task results in [[Bairstow's method]].\n\nThe real variant of [[Jenkins–Traub algorithm]] is an improvement of this method.\n\n===Finding all roots at once===\nThe simple [[Durand–Kerner method|Durand–Kerner]] and the slightly more complicated [[Aberth method]] simultaneously find all of the roots using only simple [[complex number]] arithmetic. Accelerated algorithms for multi-point evaluation and interpolation similar to the [[fast Fourier transform]] can help speed them up for large degrees of the polynomial. It is advisable to choose an asymmetric, but evenly distributed set of initial points. The implementation of this method in the [[free software]] [[MPSolve]] is a reference for its efficiency and its accuracy.\n\nAnother method with this style is the [[Graeffe's method|Dandelin–Gräffe method]] (sometimes also ascribed to [[Nikolai Ivanovich Lobachevsky|Lobachevsky]]), which uses [[polynomial transformations]] to repeatedly and implicitly square the roots. This greatly magnifies variances in the roots. Applying [[Vieta's formulas|Viète's formulas]], one obtains easy approximations for the modulus of the roots, and with some more effort, for the roots themselves.\n\n===Exclusion and enclosure methods===\nSeveral fast tests exist that tell if a segment of the real line or a region of the complex plane contains no roots. By bounding the modulus of the roots and recursively subdividing the initial region indicated by these bounds, one can isolate small regions that may contain roots and then apply other methods to locate them exactly.\n\nAll these methods involve finding the coefficients of shifted and scaled versions of the polynomial. For large degrees, [[fast Fourier transform|FFT]]-based accelerated methods become viable.\n\nFor real roots, see next sections.\n\nThe [[Lehmer–Schur algorithm]] uses the [[Lehmer–Schur algorithm#Schur–Cohn test|Schur–Cohn test]] for circles; a variant, [[Lehmer–Schur algorithm#Wilf's global bisection algorithm|Wilf's global bisection algorithm]] uses a winding number computation for rectangular regions in the complex plane.\n\nThe [[splitting circle method]] uses FFT-based polynomial transformations to find large-degree factors corresponding to clusters of roots. The precision of the factorization is maximized using a Newton-type iteration. This method is useful for finding the roots of polynomials of high degree to arbitrary precision; it has almost optimal complexity in this setting.{{cn|date=November 2018}}\n\n===Real-root isolation===\n{{main|Real-root isolation}}\n\nFinding the real roots of a polynomial with real coefficients is a problem that has received much attention since the beginning of 19th century, and is still an active domain of research. Most root-finding algorithms can find some real roots, but cannot certify having found all the roots. Methods for finding all complex roots, such as [[Aberth method]] can provide the real roots. However, because of the numerical instability of polynomials (see [[Wilkinson's polynomial]]), they may need [[arbitrary-precision arithmetic]] for deciding which roots are real. Moreover, they compute all complex roots when only few are real.\n\nIt follows that the standard way of computing real roots is to compute first disjoint intervals, called ''isolating intervals'', such that each one contains exactly one real root, and together they contain all the roots. This computation is called ''real-root isolation''. Having isolating interval, one may use fast numerical methods, such as [[Newton's method]] for improving the precision of the result.\n\nThe oldest complete algorithm for real-root isolation results from [[Sturm's theorem]]. However, it appears to be much less efficient than the methods based on [[Descartes' rule of signs]] and [[Vincent's theorem]]. These methods divide into two main classes, one using [[continued fraction]]s and the other using bisection. Both method have been dramatically improved since the beginning of 21st century. With these improvements they reach a [[computational complexity]] that is similar to that of the best algorithms for computing all  the roots (even when all roots are real). \n\nThese algorithms have been implemented and are available in [[Mathematica]] (continued fraction method) and [[Maple (software)|Maple]] (bisection method). Both implementations can routinely find the real roots of polynomials of degree higher than 1,000.\n\n===Finding multiple roots of polynomials===\n{{main article|Square-free factorization}}\nMost root-finding algorithms behave badly when there are [[Multiplicity (mathematics)|multiple roots]] or very close roots. However, for polynomials whose coefficients are exactly given as [[integer]]s or [[rational number]]s, there is an efficient method to factorize them into factors that have only simple roots and whose coefficients are also exactly given. This method, called '''[[square-free factorization]]''', is based on the multiple roots of a polynomial being the roots of the [[polynomial greatest common divisor|greatest common divisor]] of the polynomial and its derivative.\n\nThe square-free factorization of a polynomial ''p'' is a factorization <math>p=p_1p_2^2\\cdots p_k^k </math> where each <math>p_i</math> is either 1 or a polynomial without multiple roots, and two different <math>p_i</math> do not have any common root.\n\nAn efficient method to compute this factorization is [[Square-free factorization#Yun's algorithm|Yun's algorithm]].\n\n==See also==\n{{div col|colwidth=27em}}\n* [[Broyden's method]]\n* [[Cryptographically secure pseudorandom number generator]] — a class of functions designed specifically to be unsolvable by root-finding algorithms.\n* [[GNU Scientific Library]]\n* [[Graeffe's method]]\n* [[Lill's method]]\n* [[List of root finding algorithms]]\n* [[MPSolve]]\n* [[Multiplicity (mathematics)]]\n* [[Nth root algorithm|''n''th root algorithm]]\n* [[Polynomial greatest common divisor]]\n* [[System of polynomial equations]] — root-finding algorithms in the multivariate case\n* [[Steffensen's method]]\n* [[Kantorovich theorem]]\n{{div col end}}\n\n==References==\n'''Notes'''\n{{Reflist|30em}}\n'''Sources'''\n{{refbegin}}\n*{{Cite book |last1=Press |first1=W. H. |last2=Teukolsky |first2=S. A. |last3=Vetterling |first3=W. T. |last4=Flannery |first4=B. P. |year=2007 |title=Numerical Recipes: The Art of Scientific Computing |edition=3rd |publisher=Cambridge University Press |publication-place=New York |isbn=978-0-521-88068-8 |chapter=Chapter 9. Root Finding and Nonlinear Sets of Equations |chapter-url=http://apps.nrbook.com/empanel/index.html#pg=442}}\n{{refend}}\n\n==External links==\n*[http://gams.nist.gov/serve.cgi/Class/F1a1/ GAMS: Roots of polynomials with real coefficients]\n*[http://www.hvks.com/Numerical/websolver.php Free online polynomial root finder for both real and complex coefficients]\n* Kehagias, Spyros: RealRoots, a free App for iPhone, iPod Touch and iPad to compare Sturm's method and VAS https://itunes.apple.com/gr/app/realroots/id483609988?mt=8\n\n{{Root-finding algorithms}}\n\n[[Category:Root-finding algorithms|*]]"
    },
    {
      "title": "Aberth method",
      "url": "https://en.wikipedia.org/wiki/Aberth_method",
      "text": "The '''Aberth method''', or '''Aberth–Ehrlich method''', named after Oliver Aberth<ref name=\":0\">{{cite journal|last = Aberth|first = Oliver|year = 1973|title = Iteration methods for finding all zeros of a polynomial simultaneously|journal = Math. Comp.|publisher = Mathematics of Computation, Vol. 27, No. 122|volume = 27|issue = 122|pages = 339–344|doi = 10.2307/2005621|jstor = 2005621|quote = Because of the obvious analogy from electrostatics, this field may be called the field of a unit plus charge ... To avoid this, we assign a unit minus charge at each sampling point. The idea here is that when a sampling point z, is near a simple zero, the field from the minus charge at z, should counteract that from the plus charge at the zero, preventing a second sampling point from converging to this zero.}}</ref> and Louis W. Ehrlich,<ref name=\":2\">{{cite journal| last=Ehrlich| first=Louis W.| title=A modified Newton method for polynomials| journal=Comm. ACM| volume=10 | issue = 2 | year=1967|pages=107–108|doi=10.1145/363067.363115}}</ref> is a [[root-finding algorithm]] developed in 1967 for simultaneous approximation of all the roots of a [[univariate polynomial]].\n\nThis method converges cubically, an improvement over the [[Durand–Kerner method]], another algorithm for approximating all roots at once, which converges quadratically.<ref name=\":0\" /><ref name=\":2\" />  (However, both algorithms converge linearly at [[Multiplicity (mathematics)|multiple zeros]].<ref name=\":1\" />)\n\nThis method is used in [[MPSolve]], which is the reference software for approximating all roots of a polynomial to an arbitrary precision.\n\n==Description==\n\nLet <math>  p(x)=p_nx^n+p_{n-1}x^{n-1}+\\cdots+p_1x+p_0  </math> be a [[univariate]] polynomial of degree ''n'' with real or complex coefficients. Then there exist complex numbers <math>z^*_1,\\,z^*_2,\\dots,z^*_n</math>, the roots of ''p(x)'', that give the [[factorisation]]:\n\n:<math>p(x)=p_n\\cdot(x-z^*_1)\\cdot(x-z^*_2)\\cdots(x-z^*_n).</math>\n\nAlthough those numbers are unknown, [[Properties of polynomial roots|upper and lower bounds]] for their absolute values are computable from the coefficients of the polynomial. Now one can pick ''n'' distinct numbers in the complex plane—randomly or evenly distributed—such that their absolute values are within the same bounds.  (Also, if the zeros are symmetrical, the starting points must not be exactly symmetrical along the same axis, as this can prevent convergence.)<ref name=\":0\" />  A set of such numbers is called an initial approximation of the set of roots of ''p(x)''. This approximation can be iteratively improved using the following procedure.\n\nLet <math>z_1,\\dots,z_n\\in\\mathbb C</math> be the current approximations of the zeros of ''p(x)''. Then offset numbers <math>w_1,\\dots,w_n\\in\\mathbb C</math> are computed as\n\n:<math>w_k=\\frac{\\frac{p(z_k)}{p'(z_k)}}{1-\\frac{p(z_k)}{p'(z_k)}\\cdot \\sum_{j\\ne k}\\frac1{z_k-z_j}},</math>\n\nwhere ''p'(z)'' is the polynomial derivative of ''p'' evaluated in the point ''z''.\n\nThe next set of approximations of roots of ''p(x)'' is then <math>  z_1-w_1,\\dots,z_n-w_n  </math>. One can measure the quality of the current approximation by the values of the polynomial or by the size of the offsets.\n\nConceptually, this method uses an [[electrostatic]] analogy, modeling the approximated zeros as movable negative [[Point particle|point charges]], which converge toward the true zeros, represented by fixed positive point charges.<ref name=\":0\" />  A direct application of Newton's method to each approximated zero will often cause multiple starting points to incorrectly converge to the same root.  The Aberth method avoids this by also modeling the repulsive effect the movable charges have on each other.  In this way, when a movable charge has converged on a zero, their charges will cancel out, so that other movable charges are no longer attracted to that location, encouraging them to converge to other \"unoccupied\" zeros.  ([[Thomas Joannes Stieltjes|Stieltjes]] also modeled the positions of zeros of polynomials as solutions to electrostatic problems.)\n\nInside the formula of the Aberth method one can find elements of [[Newton's method]] and the [[Durand–Kerner method]]. Details for an efficient implementation, esp. on the choice of good initial approximations, can be found in Bini (1996).<ref name=\":1\">{{cite journal| last=Bini| first=Dario Andrea| title=Numerical computation of polynomial zeros by means of Aberth's method| journal=Numerical Algorithms| volume=13| year=1996| pages=179–200| url=http://www.springerlink.com/content/b35647833p354348| doi=10.1007/BF02207694| issue=2| bibcode=1996NuAlg..13..179B}}</ref>\n\nThe updates of the roots may be executed as a simultaneous [[Jacobi method|Jacobi]]-like iteration where first all new approximations are computed from the old approximations or as a sequential [[Gauss–Seidel method|Gauss–Seidel]]-like iteration that uses each new approximation from the time it is computed.\n\nA very similar method is the Newton-Maehly method. It computes the zeros one after another, but instead of an explicit deflation it divides by the already acquired linear factors on the fly. The Aberth method is like the Newton-Maehly method for computing the last root while pretending you have already found the other ones.<ref>{{cite journal| last=Bauer| first=F.L.| last2=Stoer| first2=J. | title=Algorithm 105: Newton Maehly | journal=Comm. ACM| volume=5 | issue = 7 | year=1962 | pages= 387–388 |doi=10.1145/368273.368423}}</ref>\n\n==Derivation from Newton's method==\n\nThe iteration formula is the univariate Newton iteration for the function\n\n:<math>F(x)=\\frac{p(x)}{\\prod_{j=0;\\,j\\ne k}^n(x-z_j)}</math>\n\nIf the values <math>z_1,\\dots,z_n</math> are already close to the roots of ''p''(''x''), then the rational function ''F''(''x'') is almost linear with a dominant root close to <math>z_k</math> and poles at <math>z_1,\\dots,z_{k-1},z_{k+1},\\dots,z_n</math> that direct the Newton iteration away from the roots of ''p(x)'' that are close to them. That is, the corresponding basins of attraction get rather small, while the root close to <math>z_k</math> has a wide region of attraction.\n\nThe Newton step <math>\\tfrac{F(x)}{F'(x)}</math> in the univariate case is the reciprocal value to the logarithmic derivative \n:<math>\\begin{align}\n       \\frac{F'(x)}{F(x)}\n    &= \\frac{d}{dx}\\ln|F(x)|\\\\\n    &= \\frac{d}{dx}\\big(\\ln|p(x)|-\\sum_{j=0;\\,j\\ne k}^n\\ln|x-z_j|\\big)\\\\\n    &= \\frac{p'(x)}{p(x)}-\\sum_{j=0;\\,j\\ne k}^n\\frac1{x-z_j}\n\\end{align}\n</math>\n\nThus, the new approximation is computed as\n:<math>z_k'=z_k-\\frac{F(z_k)}{F'(z_k)}=z_k-\\frac1{\\frac{p'(z_k)}{p(z_k)}-\\sum_{j=0;\\,j\\ne k}^n\\frac1{z_k-z_j}}\\,,</math>\nwhich is the update formula of the Aberth&ndash;Ehrlich method.\n\n==Literature==\n<references />\n\n==See also==\n* [[MPSolve]] A package for numerical computation of polynomial roots. Free usage for scientific purpose.\n\n{{DEFAULTSORT:Aberth Method}}\n[[Category:Root-finding algorithms]]"
    },
    {
      "title": "Bailey's method (root finding)",
      "url": "https://en.wikipedia.org/wiki/Bailey%27s_method_%28root_finding%29",
      "text": "#REDIRECT [[Halley%27s_method#Method]]\n[[Category:Root-finding algorithms]]"
    },
    {
      "title": "Bairstow's method",
      "url": "https://en.wikipedia.org/wiki/Bairstow%27s_method",
      "text": "{{primary sources|date=November 2018}}\nIn [[numerical analysis]], '''Bairstow's method''' is an efficient [[algorithm]] for finding the [[Root of a function|root]]s of a real [[polynomial]] of arbitrary degree.  The algorithm first appeared in the appendix of the 1920 book ''Applied Aerodynamics'' by [[Leonard Bairstow]].{{incomplete citation|date=November 2018}} The algorithm finds the roots in [[complex conjugate]] pairs using only real arithmetic.\n\nSee [[root-finding algorithm]] for other algorithms.\n\n==Description of the method==\n\nBairstow's approach is to use [[Newton's method]] to adjust the coefficients ''u'' and ''v'' in the [[quadratic function|quadratic]] <math>x^2 + ux + v</math> until its roots are also roots of the polynomial being solved.  The roots of the quadratic may then be determined, and the polynomial may be divided by the quadratic to eliminate those roots.  This process is then iterated until the polynomial becomes quadratic or linear, and all the roots have been determined.\n\n[[Polynomial long division|Long division]] of the polynomial to be solved\n:<math>P(x)=\\sum_{i=0}^n a_i x^i</math>  \nby <math>x^2 + ux + v</math> yields a quotient <math>Q(x)=\\sum_{i=0}^{n-2} b_i x^i</math> and a remainder <math> cx+d </math> such that\n:<math>  P(x)=(x^2+ux+v)\\left(\\sum_{i=0}^{n-2} b_i x^i\\right) + (cx+d). </math>\nA second division of <math>Q(x)</math> by <math>x^2 + ux + v</math> is performed to yield a quotient <math>R(x)=\\sum_{i=0}^{n-4} f_i x^i</math> and remainder <math>gx+h</math> with\n:<math>  Q(x)=(x^2+ux+v)\\left(\\sum_{i=0}^{n-4} f_i x^i\\right) + (gx+h). </math>\n\nThe variables <math>c,\\,d,\\,g,\\,h</math>, and the <math> \\{b_i\\},\\;\\{f_i\\} </math> are functions of <math>u</math> and <math>v</math>. They can be found recursively as follows.\n:<math>\\begin{align}\nb_n &= b_{n-1} = 0,& f_n &= f_{n-1} = 0,\\\\ \nb_i &= a_{i+2}-ub_{i+1}-vb_{i+2}&f_i &= b_{i+2}-uf_{i+1}-vf_{i+2}\n  \\qquad (i=n-2,\\ldots,0),\\\\\nc &= a_1-ub_0-vb_1,& g &= b_1-uf_0-vf_1,\\\\\nd & =a_0-vb_0,& h & =b_0-vf_0.\n\\end{align}</math>\nThe quadratic evenly divides the polynomial when\n:<math>c(u,v)=d(u,v)=0. \\,</math>\nValues of <math>u</math> and <math>v</math> for which this occurs can be discovered by picking starting values and iterating Newton's method in two dimensions\n:<math>\n\\begin{bmatrix}u\\\\ v\\end{bmatrix}  \n:=\n\\begin{bmatrix}u\\\\ v\\end{bmatrix} \n- \\begin{bmatrix}\n    \\frac{\\partial c}{\\partial u}&\\frac{\\partial c}{\\partial v}\\\\[3pt]\n    \\frac{\\partial d}{\\partial u} &\\frac{\\partial d}{\\partial v}\n  \\end{bmatrix}^{-1} \n  \\begin{bmatrix}c\\\\ d\\end{bmatrix}\n:=\n\\begin{bmatrix}u\\\\ v\\end{bmatrix} \n- \\frac{1}{vg^2+h(h-ug)} \n  \\begin{bmatrix}\n    -h & g\\\\[3pt]\n    -gv & gu-h\n  \\end{bmatrix}\n  \\begin{bmatrix}c\\\\ d\\end{bmatrix}\n</math>\nuntil convergence occurs. This method to find the zeroes of polynomials can thus be easily implemented with a programming language or even a spreadsheet.\n\n==Example==\nThe task is to determine a pair of roots of the polynomial\n:<math> f(x) = 6 \\, x^5 + 11 \\, x^4 - 33 \\, x^3 - 33 \\, x^2 + 11 \\, x + 6.</math>\nAs first quadratic polynomial one may choose the normalized polynomial formed from the leading three coefficients of ''f''(''x''), \n:<math>\n  u = \\frac{a_{n-1}}{a_n} = \\frac{11}{6} ; \n    \\quad \n  v = \\frac{a_{n-2}}{a_n} = - \\frac{33}{6}.\\,</math>\n\nThe iteration then produces the table\n\n{| class=\"wikitable\" border=\"1\"\n|+ Iteration steps of Bairstow's method\n! Nr           \n! u                \n! v             \n! step length       \n! roots\n|-\n| 0\n| 1.833333333333  \n| −5.500000000000   \n| 5.579008780071         \n| −0.916666666667±2.517990821623  \n|-  \n| 1      \n| 2.979026068546  \n| −0.039896784438   \n| 2.048558558641         \n| −1.489513034273±1.502845921479  \n|- \n| 2      \n| 3.635306053091  \n| 1.900693009946    \n| 1.799922838287         \n| −1.817653026545±1.184554563945  \n|-\n| 3      \n| 3.064938039761  \n| 0.193530875538    \n| 1.256481376254         \n| −1.532469019881±1.467968126819  \n|-\n| 4     \n| 3.461834191232  \n| 1.385679731101    \n| 0.428931413521         \n| −1.730917095616±1.269013105052  \n|----   \n| 5      \n| 3.326244386565  \n| 0.978742927192    \n| 0.022431883898         \n| −1.663122193282±1.336874153612  \n|----   \n| 6      \n| 3.333340909351  \n| 1.000022701147    \n| 0.000023931927         \n| −1.666670454676±1.333329555414  \n|----\n| 7      \n| 3.333333333340  \n| 1.000000000020    \n| 0.000000000021         \n| −1.666666666670±1.333333333330  \n|----\n| 8      \n| 3.333333333333  \n| 1.000000000000    \n| 0.000000000000         \n| −1.666666666667±1.333333333333  \n|}\n\nAfter eight iterations the method produced a quadratic factor that contains the roots −1/3 and −3 within the represented precision. The step length from the fourth iteration on demonstrates the superlinear speed of convergence.\n\n==Performance==\nBairstow's algorithm inherits the local quadratic convergence of Newton's method, except in the case of quadratic factors of multiplicity higher than 1, when convergence to that factor is linear. A particular kind of instability is observed when the polynomial has odd degree and only one real root. Quadratic factors that have a small value at this real root tend to diverge to infinity.\n\n{|\n|-\n|[[File:Bairstow-fractal_1_0_0_0_0_m1_scale_03.png|240px]]\n|[[File:Bairstow-fractal_1_0_0_0_0_m1_0_scale_3.png |240px]]\n|[[File:Bairstow-fractal_6_11_m33_m33_11_6_scale_03.png|240px]]\n|-\n|<math>f(x)=x^5-1</math>\n|<math>f(x)=x^6-x</math>\n|<math>\\begin{align}f(x)=&6x^5+11x^4-33x^3\\\\&-33x^2+11x+6\\end{align}</math>\n|-\n|}\n\nThe images represent pairs <math>(s,t)\\in[-3,3]^2</math>. Points in the upper half plane ''t''&nbsp;>&nbsp;0 correspond to a linear factor with roots <math>s\\pm it</math>, that is <math>x^2+ux+v=(x-s)^2+t^2</math>. Points in the lower half plane ''t''&nbsp;<&nbsp;0 correspond to quadratic factors with roots <math>s\\pm t</math>, that is, <math>x^2+ux+v=(x-s)^2-t^2</math>, so in general <math>(u,\\,v)=(-2s,\\, s^2+t\\,|t|)</math>. Points are colored according to the final point of the Bairstow iteration, black points indicate divergent behavior.\n\nThe first image is a demonstration of the single real root case. The second indicates that one can remedy the divergent behavior by introducing an additional real root, at the cost of slowing down the speed of convergence. One can also in the case of odd degree polynomials first find a real root using Newton's method and/or an interval shrinking method, so that after deflation a better-behaved even-degree polynomial remains. The third image corresponds to the example above.\n\n==External links==\n* [http://mathworld.wolfram.com/BairstowsMethod.html Bairstow's Algorithm on Mathworld]\n* [http://library.lanl.gov/numerical/bookfpdf.html Numerical Recipes in Fortran 77 Online]\n* [http://www.polarhome.com:793/~amate2/php/polysolver_en.php Example polynomial root solver (deg(''P'')&nbsp;≤&nbsp;10) using Bairstow's Method]\n* [http://www.vtk.org/doc/nightly/html/classvtkPolynomialSolversUnivariate.html LinBairstowSolve, an open-source C++ implementation of the Lin-Bairstow method available as a method of the VTK library]\n*[http://catc.ac.ir/mazlumi/jscodes/bairstow.php Online root finding of a polynomial – Bairstow's method] by Farhad Mazlumi\n{{Root-finding algorithms}}\n\n[[Category:Root-finding algorithms]]"
    },
    {
      "title": "Bisection method",
      "url": "https://en.wikipedia.org/wiki/Bisection_method",
      "text": "{{short description|Algorithm for finding a zero of a function}}\n{{about|searching zeros of continuous functions|searching a finite sorted array|binary search algorithm}}\n\n[[Image:Bisection method.svg|250px|thumb|A few steps of the bisection method applied over the starting range [a<sub>1</sub>;b<sub>1</sub>]. The bigger red dot is the root of the function.]]\n\nIn [[mathematics]], the '''bisection method''' is a [[Root-finding algorithm|root-finding method]] that applies to any [[continuous function]]s for which one knows two values with opposite signs. The method consists of repeatedly [[Bisection|bisecting]] the [[Interval (mathematics)|interval]] defined by these values and then selecting the  subinterval in which the function changes sign, and therefore must contain a [[Root of a function|root]]. It is a very simple and robust method, but it is also relatively slow. Because of this, it is often used to obtain a rough approximation to a solution which is then used as a starting point for more rapidly converging methods.<ref>{{Harvnb|Burden|Faires|1985|p=31}}</ref> The method is also called the '''interval halving''' method,<ref>{{cite web |url=http://siber.cankaya.edu.tr/NumericalComputations/ceng375/node32.html |title=Archived copy |accessdate=2013-11-07 |deadurl=yes |archiveurl=https://web.archive.org/web/20130519092250/http://siber.cankaya.edu.tr/NumericalComputations/ceng375/node32.html |archivedate=2013-05-19 |df= }}</ref> the '''[[Binary_search_algorithm|binary search method]]''',<ref>{{Harvnb|Burden|Faires|1985|p=28}}</ref> or the '''dichotomy method'''.<ref>{{Cite web|title = Dichotomy method - Encyclopedia of Mathematics|url = https://www.encyclopediaofmath.org/index.php/Dichotomy_method|website = www.encyclopediaofmath.org|accessdate = 2015-12-21}}</ref>\n\nFor [[polynomial]]s, more elaborated methods exist for testing the existence of a root in an interval ([[Descartes' rule of signs]], [[Sturm's theorem]], [[Budan's theorem]]). They allow extending bisection method into efficient algorithms for finding all real roots of a polynomial; see [[Real-root isolation]].\n\n== The method ==\nThe method is applicable for numerically solving the equation ''f''(''x'')&nbsp;=&nbsp;0 for the [[Real number|real]] variable ''x'', where ''f'' is a [[continuous function]] defined on an interval [''a'',&nbsp;''b''] and where ''f''(''a'') and ''f''(''b'') have opposite signs. In this case ''a'' and ''b'' are said to bracket a root since, by the [[intermediate value theorem]], the continuous function ''f'' must have at least one root in the interval (''a'', ''b'').\n\nAt each step the method divides the interval in two by computing the midpoint ''c'' = (''a''+''b'') / 2 of the interval and the value of the function ''f''(''c'') at that point. Unless ''c'' is itself a root (which is very unlikely, but possible) there are now only two possibilities: either ''f''(''a'') and ''f''(''c'') have opposite signs and bracket a root, or ''f''(''c'') and ''f''(''b'') have opposite signs and bracket a root.<ref>If the function has the same sign at the endpoints of an interval, the endpoints may or may not bracket roots of the function.</ref> The method selects the subinterval that is guaranteed to be a bracket as the new interval to be used in the next step. In this way an interval that contains a zero of ''f'' is reduced in width by 50% at each step. The process is continued until the interval is sufficiently small.\n\nExplicitly, if ''f''(''a'') and ''f''(''c'') have opposite signs, then the method sets ''c'' as the new value for ''b'', and if ''f''(''b'') and ''f''(''c'') have opposite signs then the method sets ''c'' as the new ''a''. (If ''f''(''c'')=0 then ''c'' may be taken as the solution and the process stops.) In both cases, the new ''f''(''a'') and ''f''(''b'') have opposite signs, so the method is applicable to this smaller interval.<ref>{{Harvnb|Burden|Faires|1985|p=28}} for section</ref>\n\n=== Iteration tasks ===\nThe input for the method is a continuous function ''f'', an interval [''a'', ''b''], and the function values ''f''(''a'') and ''f''(''b''). The function values are of opposite sign (there is at least one zero crossing within the interval). Each iteration performs these steps:\n# Calculate ''c'', the midpoint of the interval, ''c'' = {{sfrac|''a ''+ ''b''|2}}.\n# Calculate the function value at the midpoint, ''f''(''c'').\n# If convergence is satisfactory (that is, ''c'' - ''a'' is sufficiently small, or |''f''(''c'')| is sufficiently small), return ''c'' and stop iterating.\n# Examine the sign of ''f''(''c'') and replace either (''a'', ''f''(''a'')) or (''b'', ''f''(''b'')) with (''c'', ''f''(''c'')) so that there is a zero crossing within the new interval.\n\nWhen implementing the method on a computer, there can be problems with finite precision, so there are often additional convergence tests or limits to the number of iterations.  Although ''f'' is continuous, finite precision may preclude a function value ever being zero. For example, consider {{math|''f''(''x'') {{=}} ''x'' &minus; &pi;}}; there will never be a finite representation of ''x'' that gives zero. Additionally, the difference between ''a'' and ''b'' is limited by the floating point precision; i.e., as the difference between ''a'' and ''b'' decreases, at some point the midpoint of [''a'',&nbsp;''b''] will be numerically identical to (within floating point precision of) either ''a'' or ''b''.\n\n===Algorithm===\nThe method may be written in [[pseudocode]] as follows:<ref>{{Harvnb|Burden|Faires|1985|p=29}}. This version recomputes the function values at each iteration rather than carrying them to the next iterations.</ref>\n '''INPUT:''' Function ''f'', \n        endpoint values ''a'', ''b'', \n        tolerance ''TOL'', \n        maximum iterations ''NMAX''\n '''CONDITIONS:''' ''a'' < ''b'', \n             either ''f''(''a'') < 0 and ''f''(''b'') > 0 or ''f''(''a'') > 0 and ''f''(''b'') < 0\n '''OUTPUT:''' value which differs from a root of ''f''(''x'') = 0 by less than ''TOL''\n  \n ''N'' ← 1\n '''While''' ''N'' ≤ ''NMAX'' ''# limit iterations to prevent infinite loop''\n   ''c'' ← (''a'' + ''b'')/2 ''# new midpoint''\n   '''If''' ''f''(''c'') = 0 or (''b'' – ''a'')/2 < ''TOL'' '''then''' ''# solution found''\n     Output(''c'')\n     '''Stop'''\n   '''EndIf'''\n   ''N'' ← ''N'' + 1 ''# increment step counter''\n   '''If''' sign(''f''(''c'')) = sign(''f''(''a'')) '''then''' ''a'' ← ''c'' '''else''' ''b'' ← ''c'' ''# new interval''\n '''EndWhile'''\n Output(\"Method failed.\") ''# max number of steps exceeded''\n\n== Example: Finding the root of a polynomial ==\nSuppose that the  bisection method is used to find a root of the polynomial \n:<math> f(x) = x^3 - x - 2 \\,.</math>\nFirst, two numbers <math> a </math> and <math> b </math> have to be found such that <math>f(a)</math> and <math>f(b)</math> have opposite signs. For the above function, <math> a = 1 </math> and <math> b = 2 </math> satisfy this criterion, as \n:<math> f(1) = (1)^3 - (1) - 2 = -2  </math>\nand\n:<math> f(2) = (2)^3 - (2) - 2 = +4  \\,.</math>\nBecause the function is continuous, there must be a root within the interval [1, 2].\n\nIn the first iteration, the end points of the interval which brackets the root are  <math> a_1 = 1 </math> and <math> b_1 = 2 </math>, so the midpoint is\n:<math> c_1 = \\frac{2+1}{2} = 1.5 </math>\nThe function value at the midpoint is <math> f(c_1) = (1.5)^3 - (1.5) - 2 = -0.125 </math>. Because <math> f(c_1) </math> is negative,  <math> a = 1 </math> is replaced with <math> a = 1.5 </math> for the next iteration to ensure that <math> f(a) </math> and <math> f(b) </math> have opposite signs.  As this continues, the interval between <math> a </math> and <math> b </math> will become increasingly smaller, converging on the root of the function.  See this happen in the table below.\n\n{| class=\"wikitable\"\n! Iteration !! <math>a_n</math> !! <math>b_n</math> !! <math>c_n</math> !! <math>f(c_n)</math>\n|- style=\"text-align: left;\"\n| style=\"text-align: right;\" |1|| 1 || 2 || 1.5 || −0.125\n|- style=\"text-align: left;\"\n| style=\"text-align: right;\" |2|| 1.5|| 2|| 1.75|| style=\"text-align: right;\" |  1.6093750\n|- style=\"text-align: left;\"\n| style=\"text-align: right;\" |3|| 1.5|| 1.75|| 1.625|| style=\"text-align: right;\" |  0.6660156\n|- style=\"text-align: left;\"\n| style=\"text-align: right;\" |4|| 1.5|| 1.625|| 1.5625||  style=\"text-align: right;\" | 0.2521973\n|- style=\"text-align: left;\"\n| style=\"text-align: right;\" |5|| 1.5|| 1.5625|| 1.5312500|| style=\"text-align: right;\" |  0.0591125\n|- style=\"text-align: left;\"\n| style=\"text-align: right;\" |6|| 1.5|| 1.5312500|| 1.5156250|| −0.0340538\n|- style=\"text-align: left;\"\n| style=\"text-align: right;\" |7|| 1.5156250|| 1.5312500|| 1.5234375|| style=\"text-align: right;\" |  0.0122504\n|- style=\"text-align: left;\"\n| style=\"text-align: right;\" |8|| 1.5156250|| 1.5234375|| 1.5195313|| −0.0109712\n|- style=\"text-align: left;\"\n| style=\"text-align: right;\" |9|| 1.5195313|| 1.5234375|| 1.5214844|| style=\"text-align: right;\" |  0.0006222\n|- style=\"text-align: left;\"\n| style=\"text-align: right;\" |10|| 1.5195313|| 1.5214844|| 1.5205078|| −0.0051789\n|- style=\"text-align: left;\"\n| style=\"text-align: right;\" |11|| 1.5205078|| 1.5214844|| 1.5209961|| −0.0022794\n|- style=\"text-align: left;\"\n| style=\"text-align: right;\" |12|| 1.5209961|| 1.5214844|| 1.5212402|| −0.0008289\n|- style=\"text-align: left;\"\n| style=\"text-align: right;\" |13|| 1.5212402|| 1.5214844|| 1.5213623|| −0.0001034\n|- style=\"text-align: left;\"\n| style=\"text-align: right;\" |14|| 1.5213623|| 1.5214844|| 1.5214233|| style=\"text-align: right;\" |  0.0002594\n|-style=\"text-align: left;\"\n| style=\"text-align: right;\" |15|| 1.5213623|| 1.5214233|| 1.5213928|| style=\"text-align: right;\" |  0.0000780\n|}\n\nAfter 13 iterations, it becomes apparent that there is a convergence to about 1.521: a root for the polynomial.\n\n== Analysis ==\nThe method is guaranteed to converge to a root of ''f'' if ''f'' is a [[continuous function]] on the interval [''a'', ''b''] and ''f''(''a'') and ''f''(''b'') have opposite signs.  The [[approximation error|absolute error]] is halved at each step so the method [[Rate of convergence|converges linearly]], which is comparatively slow.\n\nSpecifically, if ''c''<sub>1</sub> = {{sfrac|''a''+''b''|2}} is the midpoint of the initial interval, and ''c''<sub>''n''</sub> is the midpoint of the interval in the ''n''th step, then the difference between ''c''<sub>''n''</sub> and a solution ''c'' is bounded by<ref>{{Harvnb|Burden|Faires|1985|p=31}}, Theorem 2.1</ref>\n:<math>|c_n-c|\\le\\frac{|b-a|}{2^n}.</math>\nThis formula can be used to determine in advance the number of iterations that the bisection method would need to converge to a root to within a certain tolerance.\nThe number of iterations needed, ''n'', to achieve a given error (or tolerance), ε, is given by:\n<math>n = \\log_2\\left(\\frac{\\epsilon_0}{\\epsilon}\\right)=\\frac{\\log\\epsilon_0-\\log\\epsilon}{\\log2} , </math>\n\nwhere <math>\\epsilon_0 = \\text{initial bracket size} = b-a .</math>\n\nTherefore, the linear convergence is expressed by <math>\\epsilon_{n+1} = \\text{constant} \\times \\epsilon_n^m, \\ m=1 .</math>\n\n== See also ==\n*[[Binary search algorithm]]\n*[[Lehmer–Schur algorithm]], generalization of the bisection method in the complex plane\n*[[Nested intervals]]\n\n== References ==\n{{reflist|30em}}\n* {{Citation| last1=Burden | first1=Richard L. | last2=Faires | first2=J. Douglas | title=Numerical Analysis | publisher=PWS Publishers | edition=3rd | isbn=0-87150-857-5 | year=1985 | chapter=2.1 The Bisection Algorithm}}\n\n==Further reading==\n* {{Citation | last1=Corliss | first1=George | title=Which root does the bisection algorithm find? | year=1977 | journal=SIAM Review | issn=1095-7200 | volume=19 | issue=2 | pages=325–327 | doi=10.1137/1019044 |ref=none}}\n* {{Citation | last1=Kaw | first1=Autar | last2=Kalu | first2=Egwu | year=2008 | title=Numerical Methods with Applications | edition=1st | publisher= | url=http://numericalmethods.eng.usf.edu/topics/textbook_index.html | isbn= | doi= | ref=none | deadurl=yes | archiveurl=https://web.archive.org/web/20090413123941/http://numericalmethods.eng.usf.edu/topics/textbook_index.html | archivedate=2009-04-13 | df= }}<!-- isbn for 2nd abridged edition: 978-0578057651. Why isn't the website the 2nd edition? -->\n\n== External links ==\n{{wikiversity|The bisection method}}\n{{wikibooks|Numerical Methods|Equation Solving}}\n*{{MathWorld|title=Bisection|urlname=Bisection}}\n* [https://web.archive.org/web/20060901073129/http://numericalmethods.eng.usf.edu/topics/bisection_method.html Bisection Method] Notes, PPT, Mathcad, Maple, Matlab, Mathematica from [https://web.archive.org/web/20060906070428/http://numericalmethods.eng.usf.edu/ Holistic Numerical Methods Institute]\n\n{{Root-finding algorithms}}\n\n[[Category:Root-finding algorithms]]\n[[Category:Articles with example pseudocode]]"
    },
    {
      "title": "Brent's method",
      "url": "https://en.wikipedia.org/wiki/Brent%27s_method",
      "text": "{{dablink|For Brent's cycle-detection algorithm, see [[Cycle_detection#Brent's_algorithm]].}}\n\nIn [[numerical analysis]], '''Brent's method''' is a [[root-finding algorithm]] combining the [[bisection method]], the [[secant method]] and [[inverse quadratic interpolation]]. It has the reliability of bisection but it can be as quick as some of the less-reliable methods. The algorithm tries to use the potentially fast-converging secant method or inverse quadratic interpolation if possible, but it falls back to the more robust bisection method if necessary. Brent's method is due to [[Richard Brent (scientist)|Richard Brent]]<ref>{{harvnb|Brent|1973}}</ref> and builds on an earlier algorithm by [[Theodorus Dekker]].<ref>{{harvnb|Dekker|1969}}</ref> Consequently, the method is also known as the '''Brent–Dekker method'''.\n\n[[Chandrupatla's method]] is a variant which is simpler and converges faster for functions that are flat around their roots (which means they have multiple roots or closely-located roots).<ref>https://doi.org/10.1016/S0965-9978(96)00051-8</ref><ref>https://www.embeddedrelated.com/showarticle/855.php</ref>\n\n== Dekker's method ==\n\nThe idea to combine the bisection method with the secant method goes back to {{Harvtxt|Dekker|1969}}.\n\nSuppose that we want to solve the equation ''f''(''x'') = 0. As with the bisection method, we need to initialize Dekker's method with two points, say ''a''<sub>0</sub> and ''b''<sub>0</sub>, such that ''f''(''a''<sub>0</sub>) and ''f''(''b''<sub>0</sub>) have opposite signs. If ''f'' is continuous on [''a''<sub>0</sub>, ''b''<sub>0</sub>], the [[intermediate value theorem]] guarantees the existence of a solution between ''a''<sub>0</sub> and ''b''<sub>0</sub>.\n\nThree points are involved in every iteration:\n* ''b''<sub>''k''</sub> is the current iterate, i.e., the current guess for the root of ''f''.\n* ''a''<sub>''k''</sub> is the \"contrapoint,\" i.e., a point such that ''f''(''a''<sub>''k''</sub>) and ''f''(''b''<sub>''k''</sub>) have opposite signs, so the interval [''a''<sub>''k''</sub>, ''b''<sub>''k''</sub>] contains the solution. Furthermore, |''f''(''b''<sub>''k''</sub>)| should be less than or equal to |''f''(''a''<sub>''k''</sub>)|, so that ''b''<sub>''k''</sub> is a better guess for the unknown solution than ''a''<sub>''k''</sub>.\n* ''b''<sub>''k''&minus;1</sub> is the previous iterate (for the first iteration, we set ''b''<sub>''k''&minus;1</sub> = ''a''<sub>0</sub>).\n\nTwo provisional values for the next iterate are computed. The first one is given by linear interpolation, also known as the secant method:\n::<math> s = \\begin{cases}  b_k - \\frac{b_k-b_{k-1}}{f(b_k)-f(b_{k-1})} f(b_k), & \\mbox{if }  f(b_k)\\neq f(b_{k-1})  \\\\ m & \\mbox{otherwise } \\end{cases} </math>\n\nand the second one is given by the bisection method\n::<math> m = \\frac{a_k+b_k}{2}. </math>\nIf the result of the secant method, ''s'', lies strictly between ''b''<sub>''k''</sub> and ''m'', then it becomes the next iterate (''b''<sub>''k''+1</sub> = ''s''), otherwise the midpoint is used (''b''<sub>''k''+1</sub> = ''m'').\n\nThen, the value of the new contrapoint is chosen such that ''f''(''a''<sub>''k''+1</sub>) and ''f''(''b''<sub>''k''+1</sub>) have opposite signs. If ''f''(''a''<sub>''k''</sub>) and ''f''(''b''<sub>''k''+1</sub>) have opposite signs, then the contrapoint remains the same: ''a''<sub>''k''+1</sub> = ''a''<sub>''k''</sub>. Otherwise, ''f''(''b''<sub>''k''+1</sub>) and ''f''(''b''<sub>''k''</sub>) have opposite signs, so the new contrapoint becomes ''a''<sub>''k''+1</sub> = ''b''<sub>''k''</sub>.\n\nFinally, if |''f''(''a''<sub>''k''+1</sub>)| < |''f''(''b''<sub>''k''+1</sub>)|, then ''a''<sub>''k''+1</sub> is probably a better guess for the solution than ''b''<sub>''k''+1</sub>, and hence the values of ''a''<sub>''k''+1</sub> and ''b''<sub>''k''+1</sub> are exchanged.\n\nThis ends the description of a single iteration of Dekker's method.\n\nDekker's method performs well if the function ''f'' is reasonably well-behaved. However, there are circumstances in which every iteration employs the secant method, but the iterates ''b''<sub>''k''</sub> converge very slowly (in particular, |''b''<sub>''k''</sub> &minus; ''b''<sub>''k''&minus;1</sub>| may be arbitrarily small). Dekker's method requires far more iterations than the bisection method in this case.\n\n== Brent's method ==\n{{Harvtxt|Brent|1973}} proposed a small modification to avoid this problem. He inserted an additional test which must be satisfied before the result of the secant method is accepted as the next iterate. Two inequalities must be simultaneously satisfied:\nGiven a specific numerical tolerance <math>\\delta</math>, if the previous step used the bisection method, the inequality\n::<math> |\\delta| < |b_k - b_{k-1}| </math>\nmust hold to perform interpolation, otherwise the bisection method is performed and its result used for the next iteration.\n\nIf the previous step performed interpolation, then the inequality\n::<math> |\\delta| < |b_{k-1} - b_{k-2}| </math>\nis used instead to perform the next  action (to choose) interpolation (when inequality is true) or bisection method (when inequality is not true).\n\nAlso, if the previous step used the bisection method, the inequality\n::<math>|s-b_k| < \\begin{matrix} \\frac12 \\end{matrix} |b_k - b_{k-1}|</math>\nmust hold, otherwise the bisection method is performed and its result used for the next iteration. If the previous step performed interpolation, then the  inequality\n:: <math>|s-b_k| < \\begin{matrix} \\frac12 \\end{matrix} |b_{k-1} - b_{k-2}|</math>\nis used instead.\n\nThis modification ensures that at the kth iteration, a bisection step will be performed in at most <math>2\\log_2(|b_{k-1}-b_{k-2}|/\\delta)</math> additional iterations, because the above conditions force consecutive interpolation step sizes to halve every two iterations, and after at most <math>2\\log_2(|b_{k-1}-b_{k-2}|/\\delta)</math> iterations, the step size will be smaller than <math>\\delta</math>, which invokes a bisection step. Brent proved that his method requires at most ''N''<sup>2</sup> iterations, where ''N'' denotes the number of iterations for the bisection method. If the function ''f'' is well-behaved, then Brent's method will usually proceed by either inverse quadratic or linear interpolation, in which case it will converge [[rate of convergence|superlinearly]].\n\nFurthermore, Brent's method uses [[inverse quadratic interpolation]] instead of [[linear interpolation]] (as used by the secant method). If ''f''(''b''<sub>''k''</sub>), ''f''(''a''<sub>''k''</sub>) and ''f''(''b''<sub>''k''&minus;1</sub>) are distinct, it slightly increases the efficiency. As a consequence, the condition for accepting ''s'' (the value proposed by either linear interpolation or inverse quadratic interpolation) has to be changed: ''s'' has to lie between (3''a''<sub>''k''</sub> + ''b''<sub>''k''</sub>) / 4 and ''b''<sub>''k''</sub>.\n\n==Algorithm==\n{{original research|section|date=February 2019}}\n '''input''' ''a'', ''b'', and (a pointer to) a function for ''f''\n calculate ''f''(''a'')\n calculate ''f''(''b'')\n '''if''' ''f''(''a'')''f''(''b'') &ge; 0 '''then''' \n   exit function because the root is not bracketed.\n '''end if'''\n '''if''' |''f''(''a'')| < |''f''(''b'')| '''then'''\n   swap (''a'',''b'')\n '''end if'''\n ''c'' := ''a''\n '''set''' mflag\n '''repeat until''' ''f''(''b'' or ''s'') = 0 '''or''' |''b'' &minus; ''a''| '''is''' small enough ''(convergence)''\n   '''if''' ''f''(''a'') ≠ ''f''(''c'') '''and''' ''f''(''b'') ≠ ''f''(''c'') '''then'''\n     {{nowrap|1=<math> s := \\frac{af(b)f(c)}{(f(a)-f(b))(f(a)-f(c))} + \\frac{bf(a)f(c)}{(f(b)-f(a))(f(b)-f(c))} + \\frac{cf(a)f(b)}{(f(c)-f(a))(f(c)-f(b))} </math>}} ''([[inverse quadratic interpolation]])''\n   '''else'''\n     {{nowrap|<math> s := b - f(b) \\frac{b-a}{f(b)-f(a)} </math>}} ''([[secant method]])''\n   '''end if'''\n   '''if''' ''(condition 1)'' ''s'' '''is not''' {{nowrap|between <math>(3a+b)/4</math> and {{var|b}}}} '''or'''\n      ''(condition 2)'' (mflag '''is''' set '''and''' <span class=\"nowrap\">|''s''&minus;''b''| ≥ |''b''&minus;''c''|/2)</span> '''or'''\n      ''(condition 3)'' (mflag '''is''' cleared '''and'''  <span class=\"nowrap\">|''s''&minus;''b''| ≥ |''c''&minus;''d''|/2)</span> '''or'''\n      ''(condition 4)'' (mflag '''is''' set '''and'''  <span class=\"nowrap\">|''b''&minus;''c''| < |{{mvar|&delta;}}|)</span> '''or'''\n      ''(condition 5)'' (mflag '''is''' cleared '''and'''  <span class=\"nowrap\">|''c''&minus;''d''| < |{{mvar|&delta;}}|)</span> '''then'''\n     <math> s := \\frac{a+b}{2} </math> ''([[bisection method]])''\n     '''set''' mflag\n   '''else'''\n     '''clear''' mflag\n   '''end if'''\n   calculate ''f''(''s'')\n   ''d'' := ''c''  ''(d is assigned for the first time here; it won't be used above on the first iteration because mflag is set)''\n   ''c'' := ''b''\n   '''if''' ''f''(''a'')''f''(''s'') < 0 '''then'''\n     ''b'' := ''s'' \n   '''else'''\n     ''a'' := ''s'' \n   '''end if'''\n   '''if''' |''f''(''a'')| < |''f''(''b'')| '''then'''\n     swap (''a'',''b'') \n   '''end if'''\n '''end repeat'''\n '''output''' ''b'' ''or s (return the root)''\n\n==Example==\n\nSuppose that we are seeking a zero of the function defined by '''''f''(''x'') = (''x'' + 3)(''x'' &minus; 1)<sup>2</sup>'''.\n\nWe take '''[''a''<sub>0</sub>, ''b''<sub>0</sub>] = [&minus;4, 4/3]''' as our initial interval.\n\nWe have ''f''(''a''<sub>0</sub>) = &minus;25 and ''f''(''b''<sub>0</sub>) = 0.48148 (all numbers in this section are rounded), so the conditions ''f''(''a''<sub>0</sub>) ''f''(''b''<sub>0</sub>) < 0 and |''f''(''b''<sub>0</sub>)| ≤ |''f''(''a''<sub>0</sub>)| are satisfied.\n\n[[Image:Brent method example.svg|thumb|Graph of ''f''(''x'') = (''x'' + 3)(''x'' &minus; 1)<sup>2</sup>]]\n# In the first iteration, we use linear interpolation between (''b''<sub>&minus;1</sub>, ''f''(''b''<sub>&minus;1</sub>)) = (''a''<sub>0</sub>, ''f''(''a''<sub>0</sub>)) = (&minus;4, &minus;25) and (''b''<sub>0</sub>, ''f''(''b''<sub>0</sub>)) = (1.33333, 0.48148), which yields ''s'' = 1.23256. This lies between (3''a''<sub>0</sub> + ''b''<sub>0</sub>) / 4 and ''b''<sub>0</sub>, so this value is accepted. Furthermore, ''f''(1.23256) = 0.22891, so we set ''a''<sub>1</sub> = ''a''<sub>0</sub> and ''b''<sub>1</sub> = ''s'' = 1.23256.\n# In the second iteration, we use inverse quadratic interpolation between (''a''<sub>1</sub>, ''f''(''a''<sub>1</sub>)) = (&minus;4, &minus;25) and (''b''<sub>0</sub>, ''f''(''b''<sub>0</sub>)) = (1.33333, 0.48148) and (''b''<sub>1</sub>, ''f''(''b''<sub>1</sub>)) = (1.23256, 0.22891). This yields 1.14205, which lies between (3''a''<sub>1</sub> + ''b''<sub>1</sub>) / 4 and ''b''<sub>1</sub>. Furthermore, the inequality |1.14205 &minus; ''b''<sub>1</sub>| ≤ |''b''<sub>0</sub> &minus; ''b''<sub>&minus;1</sub>| / 2 is satisfied, so this value is accepted. Furthermore, ''f''(1.14205) = 0.083582, so we set ''a''<sub>2</sub> = ''a''<sub>1</sub> and ''b''<sub>2</sub> = 1.14205.\n# In the third iteration, we use inverse quadratic interpolation between (''a''<sub>2</sub>, ''f''(''a''<sub>2</sub>)) = (&minus;4, &minus;25) and (''b''<sub>1</sub>, ''f''(''b''<sub>1</sub>)) = (1.23256, 0.22891) and (''b''<sub>2</sub>, ''f''(''b''<sub>2</sub>)) = (1.14205, 0.083582). This yields 1.09032, which lies between (3''a''<sub>2</sub> + ''b''<sub>2</sub>) / 4 and ''b''<sub>2</sub>. But here Brent's additional condition kicks in: the inequality |1.09032 &minus; ''b''<sub>2</sub>| ≤ |''b''<sub>1</sub> &minus; ''b''<sub>0</sub>| / 2 is not satisfied, so this value is rejected. Instead, the midpoint ''m'' = &minus;1.42897 of the interval [''a''<sub>2</sub>, ''b''<sub>2</sub>] is computed. We have ''f''(''m'') = 9.26891, so we set ''a''<sub>3</sub> = ''a''<sub>2</sub> and ''b''<sub>3</sub> = &minus;1.42897.\n# In the fourth iteration, we use inverse quadratic interpolation between (''a''<sub>3</sub>, ''f''(''a''<sub>3</sub>)) = (&minus;4, &minus;25) and (''b''<sub>2</sub>, ''f''(''b''<sub>2</sub>)) = (1.14205, 0.083582) and (''b''<sub>3</sub>, ''f''(''b''<sub>3</sub>)) = (&minus;1.42897, 9.26891). This yields 1.15448, which is not in the interval between (3''a''<sub>3</sub> + ''b''<sub>3</sub>) / 4  and ''b''<sub>3</sub>). Hence, it is replaced by the midpoint ''m'' = &minus;2.71449. We have ''f''(''m'') = 3.93934, so we set ''a''<sub>4</sub> = ''a''<sub>3</sub> and ''b''<sub>4</sub> = &minus;2.71449.\n# In the fifth iteration, inverse quadratic interpolation yields &minus;3.45500, which lies in the required interval. However, the previous iteration was a bisection step, so the inequality |&minus;3.45500 &minus; ''b''<sub>4</sub>| ≤ |''b''<sub>4</sub> &minus; ''b''<sub>3</sub>| / 2 need to be satisfied. This inequality is false, so we use the midpoint ''m'' = &minus;3.35724. We have ''f''(''m'') = &minus;6.78239, so ''m'' becomes the new contrapoint (''a''<sub>5</sub> = &minus;3.35724) and the iterate remains the same (''b''<sub>5</sub> = ''b''<sub>4</sub>). \n# In the sixth iteration, we cannot use inverse quadratic interpolation because ''b''<sub>5</sub> = ''b''<sub>4</sub>. Hence, we use linear interpolation between (''a''<sub>5</sub>, ''f''(''a''<sub>5</sub>)) = (&minus;3.35724, &minus;6.78239) and (''b''<sub>5</sub>, ''f''(''b''<sub>5</sub>)) = (&minus;2.71449, 3.93934). The result is ''s'' = &minus;2.95064, which satisfies all the conditions. But since the iterate did not change in the previous step, we reject this result and fall back to bisection. We update ''s'' = -3.03587, and ''f''(''s'') = -0.58418.\n# In the seventh iteration, we can again use inverse quadratic interpolation. The result is ''s'' = &minus;3.00219, which satisfies all the conditions. Now, ''f''(''s'') = &minus;0.03515, so we set ''a''<sub>7</sub> = ''b''<sub>6</sub> and ''b''<sub>7</sub> = &minus;3.00219 (''a''<sub>7</sub> and ''b''<sub>7</sub> are exchanged so that the condition |''f''(''b''<sub>7</sub>)| ≤ |''f''(''a''<sub>7</sub>)| is satisfied). (''Correct'' : linear interpolation {{tmath|1=s = -2.99436, f(s) = 0.089961}})\n# In the eighth iteration, we cannot use inverse quadratic interpolation because ''a''<sub>7</sub> = ''b''<sub>6</sub>. Linear interpolation yields ''s'' = &minus;2.99994, which is accepted. (''Correct'' : {{tmath|1=s = -2.9999, f(s) = 0.0016}})\n# In the following iterations, the root ''x'' = &minus;3 is approached rapidly: ''b''<sub>9</sub> = &minus;3 + 6·10<sup>&minus;8</sup> and ''b''<sub>10</sub> = &minus;3 &minus; 3·10<sup>&minus;15</sup>. (''Correct'' : Iter 9 : ''f''(''s'') = -1.4E-07, Iter 10 : ''f''(''s'') = 6.96E-12)\n\n==Implementations==\n\n* {{Harvtxt|Brent|1973}} published an [[Algol 60]] implementation. \n* [[Netlib]] contains a Fortran translation of this implementation with slight modifications. \n* The [[PARI/GP]] method <tt>solve</tt> implements the method. \n* Other implementations of the algorithm (in C++, C, and Fortran) can be found in the [[Numerical Recipes]] books. \n* The [[Apache Commons]] Math library implements the algorithm in [[Java (programming language)|Java]].\n* The [[SciPy]] optimize module implements the algorithm in [[Python (programming language)]]\n* The Modelica Standard Library implements the algorithm in [[Modelica]].\n* The <tt>uniroot</tt> function implements the algorithm in [[R (software)]].\n* The [[Boost (C++ libraries)]] implements two algorithms based on Brent's method in [[C++]] in the Math toolkit:\n# Function minimization at [https://www.boost.org/doc/libs/release/boost/math/tools/minima.hpp minima.hpp] with an example [https://www.boost.org/doc/libs/release/libs/math/doc/html/math_toolkit/brent_minima.html locating function minima].\n# Root finding implements the newer TOMS748, a more modern and efficient algorithm than Brent's original, at [https://www.boost.org/doc/libs/release/boost/math/tools/toms748_solve.hpp TOMS748], and [https://www.boost.org/doc/libs/release/libs/math/doc/html/root_finding.html Boost.Math rooting finding] that [https://www.boost.org/doc/libs/release/libs/math/doc/html/math_toolkit/roots_noderiv/bracket_solve.html uses TOMS748 internally] with [https://www.boost.org/doc/libs/release/libs/math/example/root_finding_example.cpp examples].\n* The [https://github.com/JuliaNLSolvers/Optim.jl Optim.jl] package implements the algorithm in [[Julia (programming language)]]\n\n==References==\n{{Reflist}}\n\n*{{Citation |first=R. P. |last=Brent |authorlink=Richard Brent (scientist) |year=1973 |title=Algorithms for Minimization without Derivatives |chapter=Chapter 4: An Algorithm with Guaranteed Convergence for Finding a Zero of a Function|publisher=Prentice-Hall |location=Englewood Cliffs, NJ |isbn=0-13-022335-2}}\n*{{Citation |first=T. J. |last=Dekker |authorlink=Theodorus Dekker|year=1969 |contribution=Finding a zero by means of successive linear interpolation |editor1-first=B. |editor1-last=Dejon |editor2-first=P. |editor2-last=Henrici |title=Constructive Aspects of the Fundamental Theorem of Algebra |publisher=Wiley-Interscience |location=London |isbn=978-0-471-20300-1}}\n\n==Further reading==\n*{{Cite book |last=Atkinson |first=Kendall E. |year=1989 |title=An Introduction to Numerical Analysis |edition=2nd |chapter=Section 2.8. |publisher=John Wiley and Sons |isbn=0-471-50023-2}}\n*{{Cite book |last1=Press |first1=W. H. |last2=Teukolsky |first2=S. A. |last3=Vetterling |first3=W. T. |last4=Flannery |first4=B. P. |year=2007 |title=Numerical Recipes: The Art of Scientific Computing |edition=3rd |publisher=Cambridge University Press |publication-place=New York |isbn=978-0-521-88068-8 |chapter=Section 9.3. Van Wijngaarden–Dekker–Brent Method |chapter-url=http://apps.nrbook.com/empanel/index.html#pg=454}}\n* {{Cite journal |first1=G. E. |last1=Alefeld |first2=F. A. |last2=Potra |first3=Yixun |last3=Shi |title=Algorithm 748: Enclosing Zeros of Continuous Functions |journal=Transactions on Mathematical Software |volume=21 |issue=3 |date=September 1995 |pages=327-344 |doi=10.1145/210089.210111}}\n\n==External links==\n* [http://www.netlib.org/go/zeroin.f zeroin.f] at [[Netlib]].\n* [http://people.sc.fsu.edu/~jburkardt/cpp_src/brent/brent.html module brent in C++ (also C, Fortran, Matlab)] by John Burkardt \n* [https://www.gnu.org/software/gsl/ GSL] implementation.\n* [https://www.boost.org/doc/libs/1_67_0/libs/math/doc/html/root_finding.html Boost C++] implementation.\n* [https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.brentq.html#scipy.optimize.brentq Python (Scipy)] implementation\n\n{{Root-finding algorithms}}\n\n[[Category:Root-finding algorithms]]"
    },
    {
      "title": "Broyden's method",
      "url": "https://en.wikipedia.org/wiki/Broyden%27s_method",
      "text": "In numerical analysis, '''Broyden's method''' is a [[quasi-Newton method]] for [[root-finding algorithm|finding roots]] in {{math|''k''}} variables. It was originally described by [[Charles George Broyden|C. G. Broyden]] in 1965.<ref>{{cite journal\n  | last = Broyden\n  | first = C. G.\n  | title = A Class of Methods for Solving Nonlinear Simultaneous Equations\n  | journal = Mathematics of Computation\n  | volume = 19\n  | issue = 92\n  | pages = 577–593\n  | publisher = American Mathematical Society\n  |date=October 1965\n  | doi = 10.1090/S0025-5718-1965-0198670-6\n  | jstor = 2003941 }}</ref>\n\n[[Newton's method]] for solving {{math|'''f'''('''x''') {{=}} '''0'''}} uses the [[Jacobian matrix and determinant|Jacobian matrix]], {{math|'''J'''}}, at every iteration. However, computing this Jacobian is a difficult and expensive operation. The idea behind Broyden's method is to compute the whole Jacobian only at the first iteration and to do rank-one updates at other iterations.\n\nIn 1979 Gay proved that when Broyden's method is applied to a linear system of size {{math|''n'' × ''n''}}, it\nterminates in {{math|2 ''n''}} steps,<ref>{{cite journal\n  | last = Gay\n  | first = D. M.\n  | title = Some convergence properties of Broyden's method\n  | journal = SIAM Journal on Numerical Analysis\n  | volume = 16\n  | issue = 4\n  | pages = 623–630\n  | publisher = SIAM\n  |date=August 1979\n  | doi = 10.1137/0716047\n}}</ref> although like all quasi-Newton methods, it may not converge for nonlinear systems.\n\n== Description of the method ==\n\n=== Solving single-variable equation ===\n\nIn the secant method, we replace the first derivative {{math|''f''′}} at {{math|''x''<sub>''n''</sub>}} with the [[finite-difference]] approximation:\n\n:<math>f'(x_n) \\simeq \\frac{f(x_n) - f(x_{n - 1})}{x_n - x_{n - 1}},</math>\n\nand proceed similar to [[Newton's method]]:\n\n:<math>x_{n + 1} = x_n - \\frac{f(x_n)}{f'(x_n)},</math>\n\nwhere {{math|''n''}} is the iteration index.\n\n=== Solving a system of nonlinear equations ===\n\nConsider a system of {{math|''k''}} nonlinear equations\n:<math>\\mathbf f(\\mathbf x) = \\mathbf 0 ,</math>\n\nwhere {{math|'''f'''}} is a vector-valued function of vector {{math|'''x'''}}:\n\n:<math>\\mathbf x = (x_1, x_2, x_3, \\dotsc, x_k),</math>\n:<math>\\mathbf f(\\mathbf x) = \\big(f_1(x_1, x_2, \\dotsc, x_k), f_2(x_1, x_2, \\dotsc, x_k), \\dotsc, f_k(x_1, x_2, \\dotsc, x_k)\\big).</math>\n\nFor such problems, Broyden gives a generalization of the one-dimensional Newton's method, replacing the derivative with the [[Jacobian matrix and determinant|Jacobian]] {{math|'''J'''}}. The Jacobian matrix is determined iteratively, based on the '''secant equation''' in the finite-difference approximation:\n\n:<math>\\mathbf J_n (\\mathbf x_n - \\mathbf x_{n - 1}) \\simeq \\mathbf f(\\mathbf x_n) - \\mathbf f(\\mathbf x_{n - 1}),</math>\n\nwhere {{math|''n''}} is the iteration index.  For clarity, let us define:\n\n:<math>\\mathbf f_n = \\mathbf f(\\mathbf x_n),</math>\n:<math>\\Delta \\mathbf x_n = \\mathbf x_n - \\mathbf x_{n - 1},</math>\n:<math>\\Delta \\mathbf f_n = \\mathbf f_n - \\mathbf f_{n - 1},</math>\n\nso the above may be rewritten as\n\n:<math>\\mathbf J_n \\Delta \\mathbf x_n \\simeq \\Delta \\mathbf f_n.</math>\n\nThe above equation is [[Underdetermined system|underdetermined]] when {{math|''k''}} is greater than one. Broyden suggests using the current estimate of the Jacobian matrix {{math|'''J'''<sub>''n''−1</sub>}} and improving upon it by taking the solution to the secant equation that is a minimal modification to {{math|'''J'''<sub>''n''−1</sub>}}:\n\n:<math>\\mathbf J_n = \\mathbf J_{n - 1} + \\frac{\\Delta \\mathbf f_n - \\mathbf J_{n - 1} \\Delta \\mathbf x_n}{\\|\\Delta \\mathbf x_n\\|^2} \\Delta \\mathbf x_n^{\\mathrm T}.</math>\n\nThis minimizes the following [[Matrix norm#Frobenius norm|Frobenius norm]]:\n\n:<math>\\|\\mathbf J_n - \\mathbf J_{n - 1}\\|_{\\rm F} .</math>\n\nWe may then proceed in the Newton direction:\n\n:<math>\\mathbf x_{n + 1} = \\mathbf x_n - \\mathbf J_n^{-1} \\mathbf f(\\mathbf x_n) .</math>\n\nBroyden also suggested using the [[Sherman–Morrison formula]] to update directly the inverse of the Jacobian matrix:\n\n:<math>\\mathbf J_n^{-1} = \\mathbf J_{n - 1}^{-1} + \\frac{\\Delta \\mathbf x_n - \\mathbf J^{-1}_{n - 1} \\Delta \\mathbf f_n}{\\Delta \\mathbf x_n^{\\mathrm T} \\mathbf J^{-1}_{n - 1} \\Delta \\mathbf f_n} \\Delta \\mathbf x_n^{\\mathrm T} \\mathbf J^{-1}_{n - 1}.</math>\n\nThis first method is commonly known as the \"good Broyden's method\".\n\nA similar technique can be derived by using a slightly different modification to {{math|'''J'''<sub>''n''−1</sub>}}.  This yields a second method, the so-called \"bad Broyden's method\" (but see<ref>{{cite journal\n  | last = Kvaalen\n  | first = Eric\n  | title = A faster Broyden method\n  | journal = BIT Numerical Mathematics\n  | volume = 31\n  | issue = 2\n  | pages = 369–372\n  | publisher = SIAM\n  | date = November 1991\n  | doi = 10.1007/BF01931297\n}}</ref>):\n:<math>\\mathbf J_n^{-1} = \\mathbf J_{n - 1}^{-1} + \\frac{\\Delta \\mathbf x_n - \\mathbf J^{-1}_{n - 1} \\Delta \\mathbf f_n}{\\|\\Delta \\mathbf f_n\\|^2} \\Delta \\mathbf f_n^{\\mathrm T}.</math>\n\nThis minimizes a different Frobenius norm:\n\n:<math>\\|\\mathbf J_n^{-1} - \\mathbf J_{n - 1}^{-1}\\|_{\\rm F}.</math>\n\nMany other quasi-Newton schemes have been suggested in [[Optimization (mathematics)|optimization]], where one seeks a maximum or minimum by finding the root of the first derivative ([[gradient]] in multiple dimensions). The Jacobian of the gradient is called [[Hessian matrix|Hessian]] and is symmetric, adding further constraints to its update.\n\n== Other members of the Broyden class ==\nBroyden has defined not only two methods, but a whole class of methods. Other members of this class have been added by other authors.\n* The [[Davidon–Fletcher–Powell formula|Davidon–Fletcher–Powell update]] is the only member of this class being published before the two members defined by Broyden.<ref>{{cite journal\n  | last = Broyden\n  | first = C. G.\n  | title = A Class of Methods for Solving Nonlinear Simultaneous Equations\n  | journal = Mathematics of Computation\n  | volume = 19\n  | issue = 92\n  | pages = 577–593\n  | publisher = American Mathematical Society\n  |date=October 1965\n  | doi = 10.1090/S0025-5718-1965-0198670-6\n  | jstor = 2003941 }}</ref>\n* Schubert's or sparse Broyden algorithm – a modification for sparse Jacobian matrices.<ref>{{Cite journal|title = Modification of a quasi-Newton method for nonlinear equations with a sparse Jacobian|url = http://www.ams.org/mcom/1970-24-109/S0025-5718-1970-0258276-9/|journal = Mathematics of Computation|date = 1970-01-01|issn = 0025-5718|pages = 27–30|volume = 24|issue = 109|doi = 10.1090/S0025-5718-1970-0258276-9|first = L. K.|last = Schubert}}</ref>\n* Klement (2014) – uses fewer iterations to solve many equation systems.<ref>{{Cite journal|title = On Using Quasi-Newton Algorithms of the Broyden Class for Model-to-Test Correlation|url = http://www.jatm.com.br/ojs/index.php/jatm/article/view/373|journal = Journal of Aerospace Technology and Management|date = 2014-11-23|issn = 2175-9146|pages = 407–414|volume = 6|issue = 4|doi = 10.5028/jatm.v6i4.373|language = en|first = Jan|last = Klement}}</ref><ref>{{Cite web|title = Broyden class methods – File Exchange – MATLAB Central|url = http://www.mathworks.com/matlabcentral/fileexchange/55251-broyden-class-methods|website = www.mathworks.com|access-date = 2016-02-04}}</ref>\n\n== See also ==\n* [[Secant method]]\n* [[Newton's method]]\n* [[Quasi-Newton method]]\n* [[Newton's method in optimization]]\n* [[Davidon-Fletcher-Powell formula]]\n* [[BFGS method|Broyden-Fletcher-Goldfarb-Shanno (BFGS) method]]\n\n== References ==\n{{Reflist}}\n\n== Further reading ==\n* {{cite book |first=J. E.|last=Dennis |authorlink=John E. Dennis |first2=Robert B. |last2=Schnabel |authorlink2=Robert B. Schnabel |title=Numerical Methods for Unconstrained Optimization and Nonlinear Equations |location=Englewood Cliffs |publisher=Prentice Hall |year=1983 |isbn=0-13-627216-9 |pages=168–193 }}\n* {{cite book |first=R. |last=Fletcher |title=Practical Methods of Optimization |location=New York |publisher=John Wiley & Sons |edition=Second |year=1987 |pages=44–79 |isbn=0-471-91547-5 }}\n\n== External links ==\n* [https://exchange.esa.int/thermal-workshop/attachments/workshop2014/parts/quasiNewton.pdf Simple basic explanation: The story of the blind archer]\n\n{{Optimization algorithms|convex}}\n\n[[Category:Root-finding algorithms]]"
    },
    {
      "title": "Budan's theorem",
      "url": "https://en.wikipedia.org/wiki/Budan%27s_theorem",
      "text": "In mathematics, '''Budan's theorem''' is a theorem for bounding the number of real roots of a polynomial in an interval, and computing the [[parity (mathematics)|parity]] of this number. It was published in 1807 by [[François Budan de Boislaurent]]. \n\nA similar theorem was published independently by [[Joseph Fourier]] in 1820. Each of these theorems are a corollary of the other. Fourier's statement appears more often in the literature of 19th century and has been referred to as '''Fourier's''', '''Budan–Fourier''', '''Fourier–Budan''', and even Budan's theorem  \n\nBudan's original formulation is used in fast modern algorithms for [[real-root isolation]] of polynomials.\n\n==Sign variation==\nLet <math>c_0, c_1, c_2, \\ldots c_k</math> be a finite sequence of real numbers. A ''sign variation'' or ''sign change'' in the sequence is a pair of indices {{math|''i'' < ''j''}} such that <math>c_ic_j < 0,</math> and either {{math|1=''j'' = ''i'' + 1}} or <math> c_k = 0 </math> for all {{mvar|k}} such that {{math|''i'' < ''k'' < ''j''}}.\n\nIn other words, a sign variation occurs in the sequence at each place where the signs change, when ignoring zeros. \n\nFor studying the real roots of a polynomial, the number of sign variations of several sequences may used. For Budan's theorem, it is the sequence of the coefficients. For the [[Budan–Fourier theorem]], it is the sequence of values of the successive derivatives at a point. For [[Sturm's theorem]] it is the sequence of values at a point of the [[Sturm sequence]].\n\n==Descartes' rule of signs==\n{{main|Descartes' rule of signs}}\n\nAll results described in this articles are based on Descartes' rule of signs.\n\nIf {{math|''p''(''x'')}} is a [[univariate polynomial]] with real coefficients, let us denote by {{math|#<sub>+</sub>(''p'')}} the number of its positive real roots, counted with their multiplicity,<ref name=mult>This means that a root of multiplicity {{mvar|m}} is counted as {{mvar|m}} roots.</ref> and by {{math|''v''(''p'')}} the number of sign variations in the sequence of its coefficients. [[Descartes]]'s rule of signs asserts that\n:{{math|''v''(''p'') – #<sub>+</sub>(''p'')}} is a nonnegative even integer.\n\nIn particular, if {{math|''v''(''p'') ≤ 1}}, then one has {{math|1=#<sub>+</sub>(''p'') = ''v''(''p'')}}.\n\n==Budan's statement ==\nGiven a [[univariate polynomial]] {{math|''p''(''x'')}} with real coefficients, let us denote by {{math|#<sub>(''ℓ'',''r'']</sub>(''p'')}} the number of real roots, counted with their multiplicities,<ref name=mult /> of {{mvar|p}} in a [[half-open interval]] {{math|(''ℓ'', ''r'']}} (with {{math|''ℓ'' < ''r''}} real numbers). Let us denote also by {{math|''v''<sub>''h''</sub>(''p'')}} the number of sign variations in the sequence of the coefficients of the polynomial {{math|1=''p''<sub>''h''</sub>(''x'') = ''p''(''x'' + ''h'')}}. In particular, one has {{math|1=''v''(''p'') = ''v''<sub>0</sub>(''p'')}} with the notation of the preceding section.\n\nBudan's theorem is the following: \n:<math>v_\\ell(p)-v_r(p)-\\#_{(\\ell,r]}</math> is a nonnegative even integer.\n\nAs <math>\\#_{(\\ell,r]}</math> is non negative, this implies <math>v_\\ell(p)\\ge v_r(p).</math>\n\nThis is a generalization of Descartes' rule of signs, as, if one chooses {{mvar|r}} sufficiently large, it is larger than all real roots of {{mvar|p}}, and all the coefficients of <math>p_r(x)</math> are positive, that is <math>v_r(p)=0.</math> Thus <math>v_0(p)= v_0(p)- v_r(p),</math> and <math>\\#_+ = \\#_{(0,r)},</math> which makes Descartes' rule of signs a special case of Budan's theorem.\n\nAs for Descartes' rule of signs, if <math>v_\\ell(p)-v_r(p)\\le 1,</math> one has <math>\\#_{(\\ell,r]}=v_\\ell(p)-v_r(p).</math> This means that, if <math>v_\\ell(p)-v_r(p)\\le 1</math> one has a \"zero-root test\" and a \"one-root test\".\n\n=== Examples ===\n1. Given the polynomial  <math>p(x)=x^3 -7x + 7,</math> and the open interval <math>(0,2)</math>, one has\n\n: <math>\\begin{align}p(x+0)&=p(x)=x^3 -7x + 7\\\\\n  p(x+2)&=(x+2)^3 -7(x+2) + 7=x^3+6x^2+5x+1\n\\end{align}.</math>\n\nThus, <math>v_0(p)-v_2(p)= 2-0=2,</math> and Budan's theorem asserts that the polynomial <math>p(x)</math> has either two or zero real roots in the open interval <math>(0,2).</math>\n\n2. With the same polynomial <math>p(x)=x^3 -7x + 7 </math> one has\n: <math>p(x+1)=(x+1)^3 -7(x+1) + 7=x^3+3x^2-4x+1.</math>\nThus, <math>v_0(p)-v_1(p)= 2-2=0,</math> and Budan's theorem asserts that the polynomial <math>p(x)</math> has no real root in the open interval <math>(0,1).</math> This is an example of the use of Budan's theorem as a zero-root test.\n\n==Fourier's statement {{anchor|Fourier's theorem}}==<!-- This anchor is linked from five redirects (in bold in the text, and with - instead of –. -->\nThe '''Fourier's theorem on polynomial real roots''', also called  '''Fourier–Budan theorem''' or '''Budan–Fourier theorem''' (sometimes just '''Budan's theorem''') is exactly the same as Budan's theorem, except that, for {{math|1=''h'' = ''l''}} and {{mvar|r}}, the sequence of the coefficients of {{math|''p''(''x'' + ''h'')}} is replaced by the sequence of the derivatives of {{mvar|p}} at {{mvar|h}}.\n\nEach theorem is a corollary of the other. This results from the [[Taylor expansion]] \n:<math>p(x)=\\sum_{i=0}^{\\deg p} \\frac {p^{(i)}(h)}{i!} (x-h)^i</math>\nof the polynomial {{mvar|p}} at {{mvar|h}}, which implies that the coefficient of {{math|''x''<sup>''i''</sup>}} in {{math|''p''(''x'' + ''h'')}} is the quotient of <math>p^{(i)}(h)</math> by {{math|''i''!}}, a positive number. Thus the sequences considered in Sturm's theorem and in Budan's theorem have the same number of sign variations.\n\nThis strong relationship between the two theorems may explain the priority controversy that occurred in 19th century, and the use of several names for the same theorem. In modern usage, for computer computation, Budan's theorem is generally preferred since the sequences have much larger coefficients in Fourier's theorem than in Budan's, because of the factorial factor.\n\n==Proof==\nAs each theorem is a corollary of the other, it suffices to prove Fourier's theorem.\n\nThus, consider a polynomial {{math|''p''(''x'')}}, and an interval {{math|(''l'',''r'']}}. When the value of {{mvar|x}} increases from {{mvar|l}} to {{mvar|r}}, the number of sign variations in the sequence of the derivatives of {{mvar|p}} may change only when the value of {{mvar|x}} pass through a root of {{mvar|p}} or one of its derivatives. \n\nLet us denote by {{mvar|f}} either the polynomial {{mvar|p}} or any of its derivatives. For any root {{mvar|h}} of multiplicity {{mvar|m}} of {{mvar|f}}, this polynomial is well approximated near {{mvar|h}} by <math>a(x-h)^m</math> for some constant {{mvar|a}}. Moreover, for {{math|1=''i'' = 1, ..., ''m''}}, its {{mvar|i}}th derivative is approximated by <math>\\textstyle a(x-h)^{m-i}\\prod_{j=1}^i (m-j+1).</math> It follows that, in the sequence formed by {{mvar|f}} and its {{mvar|m}} first derivatives, there are {{mvar|m}} sign variations for {{math|''x'' < ''h''}} and zero for {{math|''x'' ≥ ''h''}}.\n\nThis shows that, when {{mvar|x}} increases and pass through a root of {{mvar|p}} of multiplicity {{mvar|m}}, then the number of sign variations in the sequence of the derivative decreases of {{mvar|m}}.\n\nNow, for {{math|''i'' > 0}}, let {{mvar|h}} be a root of the {{mvar|i}}th derivative <math>f=p^{(i)}</math> of {{mvar|p}}, which is not a root of <math>p^{(i-1)}.</math> There are two cases to be considered. If the multiplicity {{mvar|m}} of the root {{mvar|h}} is even, then <math>f=p^{(i)}</math> and <math>p^{(i-1)}</math> keep a constant sign when {{mvar|x}} pass through {{mvar|h}}. This implies that the number of sign of variation in the sequence of derivatives decrease by the even number {{mvar|m}}. On the other hand, if {{mvar|m}} is odd, <math>f=p^{(i)}</math> changes of sign at {{mvar|h}}, while <math>p^{(i-1)}</math> does not. There are thus {{math|''m'' + 1}} sign variations. Thus, when {{mvar|x}} pass through {{mvar|h}}, the number of sign variation decrease either of {{math|''m''}} or {{math|''m'' + 1}}, which are nonnegative even numbers in each case.\n\n==History==\nThe problem of counting and locating the real roots of a polynomial started to be systematically studied only in\nthe beginning of the 19th. \n\nIn 1807, [[François Budan de Boislaurent]] discovered a method for extending [[Descartes' rule of signs]]—valid for the interval {{math|(0, +∞)}}—to any interval.<ref name=nmethode>{{cite book|last=Budan|first=François D.|title=Nouvelle méthode pour la résolution des équations numériques|year=1807|publisher=Courcier|location=Paris|url=https://books.google.com/books?id=VyMOAAAAQAAJ&redir_esc=y}}</ref> \n\n[[Joseph Fourier]] published a similar theorem in 1820,<ref name=Fourier>{{cite journal|last=Fourier|first=Jean Baptiste Joseph|title=Sur l'usage du théorème de Descartes dans la recherche des limites des racines|year=1820|journal=Bulletin des Sciences, par la Société Philomatique de Paris|pages=156–165|url=https://archive.org/details/bulletindesscien20soci}}</ref> on which he worked for more than twenty years.<ref name=arago>{{citation|last=Arago|first=François|title=Biographies of distinguished scientific men|year=1859|publisher=Ticknor and Fields (English Translation)|page=383|location=Boston|url=https://books.google.com/books?id=xGgSAAAAIAAJ&redir_esc=y}}</ref>\n\nBecause of the similarity between the two theorems, there was a priority controversy,<ref name=BF>{{cite journal|last=Akritas|first=Alkiviadis G.|title=On the Budan–Fourier Controversy|url=http://dl.acm.org/citation.cfm?id=1089243|journal=ACM SIGSAM Bulletin|year=1981|volume=15|number=1|pages=8–10|doi=10.1145/1089242.1089243}}</ref><ref name=Reflections>{{cite journal|last=Akritas|first=Alkiviadis G.|title=Reflections on a Pair of Theorems by Budan and Fourier|jstor=2690097|year=1982|journal=Mathematics Magazine|volume=55|number=5|pages=292–298|doi=10.2307/2690097}}</ref> despite the fact that the two theorems were discovered independently.<ref name=arago /> It was generally Fourier's formulation and proof that were used, during the 19th century, in textbooks on the [[theory of equations]].\n\n=== Use in 19th century ===\n\nBudan's and Sturm theorems were soon considered of a great importance, although they do not solve completely the problem of counting the number of real roots of a polynomial in an interval. This problem was completely solved \nin 1827 by [[Jacques Charles François Sturm|Sturm]]. \n\nAlthough Sturm's theorem is not based on [[Descartes' rule of signs]], Sturm's and Fourier's theorems are related not only by the use of the number of sign variations of a sequence of numbers, but also by a similar approach of the problem. Sturm himself acknowledged having been inspired by Fourier's methods:<ref name=Sturm>{{cite journal|last=Hourya|first=Benis-Sinaceur|title=Deux moments dans l'histoire du Théorème d'algèbre de Ch. F. Sturm|journal= Revue d'histoire des sciences |year=1988|volume=41|number=2|page=108| url=http://www.persee.fr/web/revues/home/prescript/article/rhs_0151-4105_1988_num_41_2_4093}}</ref> ''« C'est en m'appuyant sur les principes qu'il a posés, et en imitant ses démonstrations, que j'ai trouvé les nouveaux théorèmes que je vais énoncer. »'' which translates into ''« It is by relying upon the principles he has laid out and by imitating his proofs that I have found the new theorems which I am about to present. »''\n\nBecause of this, during the 19th century, Fourier's and Sturm's theorems appeared together in almost all books on the theory of equations. \n\nFourier and Budan left open the problem of reducing the size of the intervals in which roots are searched in a way that, eventually, the difference between the numbers of sign variations is at most one, allowing certifying that the final intervals contains at most one root each. This problem was solved in 1834 by Alexandre Joseph Hidulph Vincent.<ref name=paper_1834>{{cite journal|last=Vincent|first=Alexandre Joseph Hidulph\n|title=Mémoire sur la résolution des équations numériques |url=http://gallica.bnf.fr/ark:/12148/bpt6k57787134/f4.image.r=Agence%20Rol.langEN\n|journal=Mémoires de la Société Royale des Sciences, de l' Agriculture et des Arts, de Lille|year=1834|pages=1–34}}</ref> Roughly speaking, [[Vincent's theorem]] consists of using [[continued fraction]]s for replacing Budan's linear transformations of the variable by [[Möbius transformation]]s.\n\nBudan's, Fourier's and Vincent theorem sank into oblivion at the end of 19th century. The last author mentioning these theorems before the second half of 20th century [[Joseph Alfred Serret]].<ref name=Serret>{{cite book|last=Serret|first=Joseph A.|title=Cours d'algèbre supérieure. Tome I|year=1877|publisher=Gauthier-Villars|pages=363–368|url=https://archive.org/details/coursdalgbresu01serruoft}}</ref> There were introduced again in 1976 by Collins and Akritas, for providing, in [[computer algebra]], an efficient algorithm for real roots isolation on computers.<ref>Collins, G. E., & Akritas, A. G. (1976, August). Polynomial real root isolation using Descarte's rule of signs. In ''Proceedings of the third ACM symposium on Symbolic and algebraic computation'' (pp. 272-275). ACM.</ref>\n\n==See also==\n*[[Properties of polynomial roots]]\n*[[Root-finding algorithm]]\n\n==References==\n{{reflist}}\n\n==External links==\n{{MacTutor|title=Budan de Boislaurent}}\n\n[[Category:Mathematical theorems]]\n[[Category:Root-finding algorithms]]\n[[Category:Real algebraic geometry]]"
    },
    {
      "title": "Durand–Kerner method",
      "url": "https://en.wikipedia.org/wiki/Durand%E2%80%93Kerner_method",
      "text": "In [[numerical analysis]], the '''Durand–Kerner method''', discovered by [[Karl Weierstrass]] in 1891 and rediscovered independently by Durand in 1960 and Kerner in 1966, is a [[root-finding algorithm]] for solving [[polynomial]] [[equation (mathematics)|equations]].<ref name=\"Petković\">{{cite book|last1=Petković|first1=Miodrag|title=Iterative methods for simultaneous inclusion of polynomial zeros|date=1989|publisher=Springer|location=Berlin [u.a.]|isbn=978-3-540-51485-5|pages=31–32}}</ref> In other words, the method can be used to solve numerically the equation\n\n: ƒ(''x'') = 0\n\nwhere ƒ is a given polynomial, which can be taken to be scaled so that the leading coefficient is&nbsp;1.\n\n==Explanation==\nThe explanation is for equations of [[Degree of a polynomial|degree]] four. It is easily generalized to other degrees.\n\nLet the polynomial &fnof; be defined by\n\n:&fnof;(''x'') = ''x''<sup>4</sup> + ''ax''<sup>3</sup> + ''bx''<sup>2</sup> + ''cx'' + ''d''\n\nfor all ''x''.\n\nThe known numbers ''a, b, c, d'' are the [[coefficient]]s.\n\nLet the (complex) numbers ''P,Q,R,S'' be the roots of this polynomial &fnof;.\n\nThen\n\n:&fnof;(''x'') = (''x'' &minus; ''P'')(''x'' &minus; ''Q'')(''x'' &minus; ''R'')(''x'' &minus; ''S'')\n\nfor all ''x''.  One can isolate the value ''P'' from this equation,\n\n:<math>P=x-\\frac{f(x)}{(x-Q)(x-R)(x-S)}.</math>\n\nSo if used as a [[fixed point (mathematics)|fixed point]] [[iteration]]\n:<math>x_1:=x_0-\\frac{f(x_0)}{(x_0-Q)(x_0-R)(x_0-S)},</math>\nit is strongly stable in that every initial point ''x<sub>0</sub>'' ≠ ''Q,R,S''\ndelivers after one iteration the root ''P=x<sub>1</sub>''.\n\nFurthermore, if one replaces the zeros ''Q'', ''R'' and ''S''\nby approximations ''q'' ≈ ''Q'', ''r'' ≈ ''R'',  ''s'' ≈ ''S'',\nsuch that ''q,r,s'' are not equal to ''P'', then ''P''\nis still a fixed point of the perturbed fixed point iteration\n\n:<math>x_{k+1}:=x_k-\\frac{f(x_k)}{(x_k-q)(x_k-r)(x_k-s)},</math>\nsince\n\n:<math>P-\\frac{f(P)}{(P-q)(P-r)(P-s)} = P - 0 = P.</math>\n\nNote that the denominator is still different from zero.\nThis fixed point iteration is a [[contraction mapping]]\nfor ''x'' around ''P''.\n\nThe clue to the method now is to combine\nthe fixed point iteration for ''P'' with similar iterations\nfor ''Q,R,S'' into a simultaneous iteration for all roots.\n\nInitialize ''p, q, r, s'':\n\n:''p''<sub>0</sub> := (0.4 + 0.9&nbsp;i)<sup>0</sup> ;\n:''q''<sub>0</sub> := (0.4 + 0.9&nbsp;i)<sup>1</sup> ;\n:''r''<sub>0</sub> := (0.4 + 0.9&nbsp;i)<sup>2</sup> ;\n:''s''<sub>0</sub> := (0.4 + 0.9&nbsp;i)<sup>3</sup> ;\n\nThere is nothing special about choosing 0.4&nbsp;+&nbsp;0.9&nbsp;i except that it is neither a [[real number]] nor a [[root of unity]].\n\nMake the substitutions for ''n'' = 1,2,3,&middot;&middot;&middot; \n:{|\n|-\n|<math> p_n = p_{n-1} - \\frac{f(p_{n-1})}{ (p_{n-1}-q_{n-1})(p_{n-1}-r_{n-1})(p_{n-1}-s_{n-1}) }; </math>\n|-\n|<math> q_n = q_{n-1} - \\frac{f(q_{n-1})}{ (q_{n-1}-p_n)(q_{n-1}-r_{n-1})(q_{n-1}-s_{n-1}) }; </math>\n|-\n|<math> r_n = r_{n-1} - \\frac{f(r_{n-1})}{ (r_{n-1}-p_n)(r_{n-1}-q_n)(r_{n-1}-s_{n-1}) }; </math>\n|-\n|<math> s_n = s_{n-1} - \\frac{f(s_{n-1})}{ (s_{n-1}-p_n)(s_{n-1}-q_n)(s_{n-1}-r_n) }. </math>\n|}\n\nRe-iterate until the numbers ''p, q, r, s''\nstop essentially changing in relative to the desired precision.\nThen they have the values ''P, Q, R, S'' in some order\nand in the chosen precision. So the problem is solved.\n\nNote that you must use [[complex number]] arithmetic,\nand that the roots are found simultaneously rather than one at a time.\n\n== Variations ==\nThis iteration procedure, like the [[Gauss–Seidel method]] for linear equations,\ncomputes one number at a time based on the already computed numbers.\nA variant of this procedure, like the [[Jacobi method]],\ncomputes a vector of root approximations at a time.\nBoth variant are effective root-finding algorithms.\n\nOne could also choose the initial values for ''p,q,r,s''\nby some other procedure, even randomly, but in a way that \n*they are inside some not-too-large circle containing also the roots of &fnof;(''x''), e.g. the circle around the origin with radius <math>1+\\max(|a|,|b|,|c|,|d|)</math>, (where 1,''a,b,c,d'' are the coefficients of &fnof;(''x'')) \nand that\n*they are not too close to each other,\nwhich may increasingly become a concern\nas the degree of the polynomial increases.\n\nIf the coefficients are real and the polynomial has odd degree, then it must have at least one real root.  To find this, use a real value of ''p''<sub>0</sub> as the initial guess and make ''q''<sub>0</sub> and ''r''<sub>0</sub>, etc, [[complex conjugate]] pairs.  Then the iteration will preserve these properties; that is, ''p''<sub>''n''</sub> will always be real, and ''q''<sub>''n''</sub> and ''r''<sub>''n''</sub>, etc, will always be conjugate.  In this way, the ''p''<sub>''n''</sub> will converge to a real root ''P''.  Alternatively, make all of the initial guesses real; they will remain so.\n\n== Example ==\nThis example is from the reference 1992. The equation solved is {{nowrap|1=''x''<sup>3</sup> − 3''x''<sup>2</sup> + 3''x'' − 5 = 0}}. The first 4 iterations move ''p'', ''q'', ''r'' seemingly chaotically, but then the roots are located to 1 decimal. After iteration number 5 we have 4 correct decimals, and the subsequent iteration number 6 confirms that the computed roots are fixed. This general behaviour is characteristic for the method.\n\n::{|class=\"wikitable\"\n|----\n!it.-no.\n!p\n!q\n!r\n|----\n!0\n| +1.0000 + 0.0000i\n| +0.4000 + 0.9000i\n| &minus;0.6500 + 0.7200i\n|----\n!1\n| +1.3608 + 2.0222i\n| &minus;0.3658 + 2.4838i\n| &minus;2.3858 &minus; 0.0284i\n|----\n!2\n| +2.6597 + 2.7137i\n| +0.5977 + 0.8225i\n| &minus;0.6320&minus;1.6716i\n|----\n! 3\n| +2.2704 + 0.3880i\n| +0.1312 + 1.3128i\n| +0.2821 &minus; 1.5015i\n|----\n! 4  \n| +2.5428 &minus; 0.0153i\n| +0.2044 + 1.3716i\n| +0.2056 &minus; 1.3721i\n|----\n! 5  \n| +2.5874 + 0.0000i\n| +0.2063 + 1.3747i\n| +0.2063 &minus; 1.3747i\n|----\n! 6  \n| +2.5874 + 0.0000i\n| +0.2063 + 1.3747i\n| +0.2063 &minus; 1.3747i\n|----\n|}\nNote that the equation has one real root and one pair of complex conjugate roots, and that the sum of the roots is&nbsp;3.\n\n==Derivation of the method via Newton's method==\n\nFor every ''n''-tuple of complex numbers, there is exactly one monic polynomial of degree ''n'' that has them as its zeros (keeping multiplicities). This polynomial is given by multiplying all the corresponding linear factors, that is\n\n:<math>\n   g_{\\vec z}(X)=(X-z_1)\\cdots(X-z_n).\n</math>\n\nThis polynomial has coefficients that depend on the prescribed zeros,\n\n:<math>g_{\\vec z}(X)=X^n+g_{n-1}(\\vec z)X^{n-1}+\\cdots+g_0(\\vec z).</math>\n\nThose coefficients are, up to a sign, the [[elementary symmetric polynomial]]s <math>\\alpha_1(\\vec z),\\dots,\\alpha_n(\\vec z)</math> of degrees ''1,...,n''.\n\nTo find all the roots of a given polynomial <math>f(X)=X^n+c_{n-1}X^{n-1}+\\cdots+c_0</math> with coefficient vector <math>(c_{n-1},\\dots,c_0)</math> simultaneously is now the same as to find a solution vector to the system\n\n:<math>\\begin{matrix}\nc_0&=&g_0(\\vec z)&=&(-1)^n\\alpha_n(\\vec z)&=&(-1)^nz_1\\cdots z_n\\\\\nc_1&=&g_1(\\vec z)&=&(-1)^{n-1}\\alpha_{n-1}(\\vec z)\\\\\n&\\vdots&\\\\\nc_{n-1}&=&g_{n-1}(\\vec z)&=&-\\alpha_1(\\vec z)&=&-(z_1+z_2+\\cdots+z_n).\n\\end{matrix}\n</math>\n\nThe Durand–Kerner method is obtained as the multidimensional [[Newton's method]] applied to this system. It is algebraically more comfortable to treat those identities of coefficients as the identity of the corresponding polynomials, <math>g_{\\vec z}(X)=f(X)</math>. In the Newton's method one looks, given some initial vector <math>\\vec z</math>, for an increment vector <math>\\vec w</math> such that <math>g_{\\vec z+\\vec w}(X)=f(X)</math> is satisfied up to second and higher order terms in the increment. For this one solves the identity\n\n:<math>f(X)-g_{\\vec z}(X)=\\sum_{k=1}^n\\frac{\\partial g_{\\vec z}(X)}{\\partial z_k}w_k=-\\sum_{k=1}^n w_k\\prod_{j\\ne k}(X-z_j).</math>\n\nIf the numbers <math>z_1,\\dots,z_n</math> are pairwise different, then the polynomials in the terms of the right hand side form a basis of the ''n''-dimensional space <math>\\mathbb C[X]_{n-1}</math> of polynomials with maximal degree ''n''&nbsp;&minus;&nbsp;1. Thus a solution <math>\\vec w</math> to the increment equation exists in this case. The coordinates of the increment <math>\\vec w</math> are simply obtained by evaluating the increment equation\n\n:<math>-\\sum_{k=1}^n w_k\\prod_{j\\ne k}(X-z_j)=f(X)-\\prod_{j=1}^n(X-z_j)</math>\n\nat the points <math>X=z_k</math>, which results in\n\n:<math>\n-w_k\\prod_{j\\ne k}(z_k-z_j)=-w_kg_{\\vec z}'(z_k)=f(z_k)\n</math>, that is <math>\nw_k=-\\frac{f(z_k)}{\\prod_{j\\ne k}(z_k-z_j)}.\n</math>\n\n==Root inclusion via Gerschgorin's circles==\n\nIn the [[quotient ring]] (algebra) of [[residue class]]es modulo &fnof;(''X''), the multiplication by ''X'' defines an [[endomorphism]] that has the zeros of &fnof;(''X'') as [[eigenvalue]]s with the corresponding multiplicities. Choosing a basis, the multiplication operator is represented by its coefficient matrix ''A'', the [[companion matrix]] of &fnof;(''X'') for this basis.\n\nSince every polynomial can be reduced modulo &fnof;(''X'') to a polynomial of degree ''n''&nbsp;&minus;&nbsp;1 or lower, the space of residue classes can be identified with the space of polynomials of degree bounded by ''n''&nbsp;&minus;&nbsp;1. \nA problem specific basis can be taken from [[Lagrange interpolation]] as the set of ''n'' polynomials\n\n:<math>b_k(X)=\\prod_{1\\le j\\le n,\\;j\\ne k}(X-z_j),\\quad k=1,\\dots,n,</math>\n\nwhere <math>z_1,\\dots,z_n\\in\\mathbb C</math> are pairwise different complex numbers. Note that the kernel functions for the Lagrange interpolation are <math>L_k(X)=\\frac{b_k(X)}{b_k(z_k)}</math>.\n\nFor the multiplication operator applied to the basis polynomials one obtains from the Lagrange interpolation\n{|\n|-\n|<math>X\\cdot b_k(X)\\mod f(X)=X\\cdot b_k(X)-f(X)</math>\n|<math>=\\sum_{j=1}^n\\Big(z_j\\cdot b_k(z_j)-f(z_j)\\Big)\\cdot \\frac{b_j(X)}{b_j(z_j)}</math>\n|-\n|\n|<math>=z_k\\cdot b_k(X)+\\sum_{j=1}^n w_j\\cdot b_j(X)</math>,\n|}\nwhere <math>w_j=-\\frac{f(z_j)}{b_j(z_j)}</math> are again the Weierstrass updates.\n\nThe companion matrix of &fnof;(''X'') is therefore\n: <math> A = \\mathrm{diag}(z_1,\\dots,z_n)\n  +\\begin{pmatrix}1\\\\\\vdots\\\\1\\end{pmatrix}\\cdot\\left(w_1,\\dots,w_n\\right).\n</math>\n\nFrom the transposed matrix case of the [[Gershgorin circle theorem]] it follows that all eigenvalues of ''A'', that is, all roots of &fnof;(''X''), are contained in the union of the disks <math>D(a_{k,k},r_k)</math> with a radius <math>r_k=\\sum_{j\\ne k}\\big|a_{j,k}\\big|</math>.\n\nHere one has <math>a_{k,k}=z_k+w_k</math>, so the centers are the next iterates of the Weierstrass iteration, and radii <math>r_k=(n-1)\\left|w_k\\right|</math> that are multiples of the Weierstrass updates. If the roots of &fnof;(''X'') are all well isolated (relative to the computational precision) and the points <math>z_1,\\dots,z_n\\in\\mathbb C</math> are sufficidently close approximations to these roots, then all the disks will become disjoint, so each one contains exactly one zero. The midpoints of the circles will be better approximations of the zeros.\n\nEvery conjugate matrix <math>TAT^{-1}</math> of ''A'' is as well a companion matrix of &fnof;(''X''). Choosing ''T'' as diagonal matrix leaves the structure of ''A'' invariant. The root close to <math>z_k</math> is contained in any isolated circle with center <math>z_k</math> regardless of ''T''. Choosing the optimal diagonal matrix ''T'' for every index results in better estimates (see ref. Petkovic et al. 1995).\n\n==Convergence results==\n\nThe connection between the Taylor series expansion and Newton's method suggests that the distance from <math>z_k+w_k</math> to the corresponding root is of the order <math>O(|w_k|^2)</math>, if the root is well isolated from nearby roots and the approximation is sufficiently close to the root. So after the approximation is close, Newton's method converges ''quadratically''; that is: the error is squared with every step (which will greatly reduce the error once it is less than 1). In the case of the Durand–Kerner method, convergence is quadratic if the vector <math>\\vec z=(z_1,\\dots,z_n)</math> is close to some permutation of the vector of the roots of &fnof;.\n\nFor the conclusion of linear convergence there is a more specific result (see ref. Petkovic et al. 1995). If the initial vector <math>\\vec z</math> and its vector of Weierstrass updates <math>\\vec w=(w_1,\\dots,w_n)</math> satisfies the inequality\n\n:<math>\\max_{1\\le k\\le n}\\big|w_k\\big| \\le \\frac1{5n} \\min_{1\\le j<k\\le n}\\big|z_k-z_j\\big|,</math>\n\nthen this inequality also holds for all iterates, all inclusion disks <math>\\textstyle D\\left(z_k+w_k,(n-1)|w_k|\\right)</math>\nare disjoint and linear convergence with a contraction factor of ''1/2'' holds. Further, the inclusion disks can in this case be chosen as\n\n:<math>\\textstyle D\\left(z_k+w_k,\\frac14 |w_k|\\right)\\qquad k = 1,\\dots, n,</math>\n\neach containing exactly one zero of &fnof;.\n\n==References==\n{{Reflist}}\n\n* {{cite conference|last=Weierstraß|first= Karl|authorlink=Karl Weierstraß|title=Neuer Beweis des Satzes, dass jede ganze rationale Function einer Veränderlichen dargestellt werden kann als ein Product aus linearen Functionen derselben Veränderlichen|booktitle=Sitzungsberichte der königlich preussischen Akademie der Wissenschaften zu Berlin|year=1891|url=http://bibliothek.bbaw.de/bibliothek-digital/digitalequellen/schriften/anzeige?band=10-sitz/1891-2&seite:int=00000565}}\n* {{cite conference|last=Durand|first=E.|booktitle=Solutions Numériques des Equations Algébriques |volume=1|editor=Masson |display-editors=etal |title=Equations du type ''F''(''x'')&nbsp;=&nbsp;0: Racines d'un polynome|year= 1960}}\n* {{cite journal|last= Kerner|first= Immo O.|title=Ein Gesamtschrittverfahren zur Berechnung der Nullstellen von Polynomen|journal=Numerische Mathematik|volume=8|year= 1966|pages= 290–294|url= http://www.springerlink.com/content/q5p055l61pm63206|doi=10.1007/BF02162564 }}\n* {{cite journal|author=Prešić, Marica|title=A convergence theorem for a method for simultaneous determination of all zeros of a polynomial|journal=Publications de l'institut mathematique (Beograd) (N.S.)|volume=28|pages=158–168 |year=1980 | issue=42}}\n* {{cite journal|author=Petkovic, M.S., Carstensen, C. and Trajkovic, M.|title=Weierstrass formula and zero-finding methods|journal=Numerische Mathematik|volume=69|year=1995|pages=353–372|url=|citeseerx=10.1.1.53.7516}}\n*  Bo Jacoby, ''Nulpunkter for polynomier'', CAE-nyt (a periodical for Dansk CAE Gruppe [Danish CAE Group]), 1988.\n*  Agnethe Knudsen, ''Numeriske Metoder'' (lecture notes), Københavns Teknikum.\n*  Bo Jacoby, ''Numerisk løsning af ligninger'', Bygningsstatiske meddelelser (Published by Danish Society for Structural Science and Engineering) volume 63 no. 3-4, 1992, pp.&nbsp;83–105.\n* {{cite book|last=Gourdon|first=Xavier|title=Combinatoire, Algorithmique et Geometrie des Polynomes|publisher=Ecole Polytechnique|location= Paris|year=1996|url=http://algo.inria.fr/gourdon/thesis.html}}\n* [[Victor Pan]] (May 2002): [http://www.cs.gc.cuny.edu/tr/techreport.php?id=26 ''Univariate Polynomial Root-Finding with Lower Computational Precision and Higher Convergence Rates'']. Tech-Report, City University of New York\n* {{cite journal|first= Arnold|last= Neumaier|title= Enclosing clusters of zeros of polynomials|journal= Journal of Computational and Applied Mathematics|volume= 156 |year=2003|url=http://www.mat.univie.ac.at/~neum/papers.html#polzer|doi= 10.1016/S0377-0427(03)00380-7|pages= 389}}\n* Jan Verschelde, ''[http://www2.math.uic.edu/~jan/mcs471f03/Project_Two/proj2/node2.html The method of Weierstrass (also known as the Durand–Kerner method)]'', 2003.\n\n==External links==\n* ''[http://home.roadrunner.com/~jbmatthews/misc/groots.html Ada Generic_Roots using the Durand–Kerner Method]'' &mdash; an [[open-source]] implementation in [[Ada programming language|Ada]]\n* ''[http://sites.google.com/site/drjohnbmatthews/polyroots Polynomial Roots]'' &mdash; an [[Open-Source|open-source]] implementation in [[Java programming language|Java]]\n* ''[http://www.cpc.wmin.ac.uk/~spiesf/Solve/solve.html Roots Extraction from Polynomials : The Durand–Kerner Method]'' &mdash; contains a [[Java applet]] demonstration\n\n{{DEFAULTSORT:Durand-Kerner method}}\n[[Category:Root-finding algorithms]]"
    },
    {
      "title": "False position method",
      "url": "https://en.wikipedia.org/wiki/False_position_method",
      "text": "In [[mathematics]], the '''false position method''' or '''''regula falsi''''' is a very old method for solving an equation in one unknown, that, in modified form, is still in use. In simple terms, the method is the [[trial and error]] technique of using test (\"false\") values for the variable and then adjusting the test value according to the outcome. This is sometimes also referred to as \"guess and check\". Versions of the method predate the advent of [[algebra]] and the use of [[equation]]s.\n\nAs an example, consider problem 26 in the [[Rhind papyrus]], which asks for a solution of (written in modern notation) the equation {{math|1=''x'' + ''x''/4 = 15}}. This is solved by false position,<ref name=Katz>{{citation|first=Victor J.|last=Katz|title=A History of Mathematics|edition=2nd|year=1998|publisher=Addison Wesley Longman|isbn=978-0-321-01618-8|page=15}}</ref> using a technique that predates formally written equations. First, guess that {{math|1=''x'' = 4}} to obtain, on the left, {{nobreak|1=4 + 4/4 = 5}}. This guess is a good choice since it produces an integer value. However, 4 is not the solution of the original equation, as it gives a value which is three times too small. To compensate, multiply {{mvar|x}} (currently set to 4) by 3 and substitute again to get {{nobreak|1=12 + 12/4 = 15}}, verifying that the solution is {{math|1=''x'' = 12}}.\n\nModern versions of the technique employ systematic ways of choosing new test values and are concerned with the questions of whether or not an approximation to a solution can be obtained, and if it can, how fast can the approximation be found.\n\n==Two historical types==\n\nTwo basic types of false position method can be distinguished historically, ''simple false position'' and ''double false position''.\n\n''Simple false position'' is aimed at solving problems involving direct proportion. Such problems can be written algebraically in the form: determine {{math|''x''}} such that\n:<math>ax = b ,</math>\nif {{math|''a''}} and {{math|''b''}} are known. The method begins by using a test input value {{math|''x''′}}, and finding the corresponding output value {{math|''b''′}} by multiplication:  {{math|1=''ax''′  = ''b''′}}. The correct answer is then found by proportional adjustment, {{math|1=''x''  =  {{sfrac|''b'' | ''b''′}} ''x''′ }}.\n\n''Double false position'' is aimed at solving more difficult problems that can be written algebraically in the form: determine {{math|''x''}} such that\n:<math>f(x) = ax + c = 0 ,</math>\nif it is known that\n:<math>f(x_1) = b_1, \\qquad f(x_2) = b_2 .</math>\nDouble false position is mathematically equivalent to [[linear interpolation]]. By using a pair of test inputs and the corresponding pair of outputs, the result of this [[algorithm]] given by,<ref name=Smith>{{citation|first=D. E.|last=Smith|title=History of Mathematics|volume=II|year=1958|origyear=1925|publisher=Dover|isbn=978-0-486-20430-7|pages=437−441}}</ref>\n:<math> x = \\frac{b_1 x_2 - b_2 x_1}{b_1 - b_2},</math>\nwould be memorized and carried out by rote. Indeed, the rule as given by [[Robert Recorde]] in his ''Ground of Artes'' (c. 1542) is:<ref name=Smith />\n\n<poem>\n:::Gesse at this woorke as happe doth leade.\n:::By chaunce to truthe you may procede.\n:::And firste woorke by the question,\n:::Although no truthe therein be don.\n:::Suche falsehode is so good a grounde,\n:::That truth by it will soone be founde.\n::::  From many bate to many mo,\n:::From to fewe take to fewe also.\n:::With to much ioyne to fewe againe,\n:::To to fewe adde to manye plaine.\n:::In crossewaies multiplye contrary kinde,\n:::All truthe by falsehode for to fynde. </poem>\n\nFor an affine [[linear function]],\n:<math>f(x) = ax + c ,</math>\ndouble false position provides the exact solution, while for a [[nonlinear system|nonlinear]] function {{math|''f''}} it provides an [[approximation]] that can be successively improved by [[iterative method|iteration]].\n\n==History==\n\nThe simple false position technique is found in [[cuneiform]] tablets from ancient [[Babylonian mathematics]], and in [[papyrus|papyri]] from ancient [[Egyptian mathematics]].<ref>Jean-Luc Chabert, ed., ''A History of Algorithms: From the Pebble to the Microchip'' (Berlin: Springer, 1999), pp. 86-91.</ref><ref name=Katz />\n\nDouble false position arose in late antiquity as a purely arithmetical algorithm.  In the ancient [[Chinese mathematics|Chinese mathematical]] text called ''[[The Nine Chapters on the Mathematical Art]]'' (九章算術),<ref name=\"Needham1959\">{{cite book|author=Joseph Needham|title=Science and Civilisation in China: Volume 3, Mathematics and the Sciences of the Heavens and the Earth|url=https://books.google.com/books?id=jfQ9E0u4pLAC&pg=PA147#v=onepage&q&f=false|date=1 January 1959|publisher=Cambridge University Press|isbn=978-0-521-05801-8|pages=147–}}</ref> dated from 200 BC to AD 100, most of Chapter 7 was devoted to the algorithm. There, the procedure was justified by concrete arithmetical arguments, then applied creatively to a wide variety of story problems, including one involving what we would call [[secant line]]s on a [[conic section]]. A more typical example is this \"joint purchase\" problem:<ref>{{Cite web|url=http://www-groups.dcs.st-and.ac.uk/~history/HistTopics/Nine_chapters.html|title=Nine chapters|website=www-groups.dcs.st-and.ac.uk|access-date=2019-02-16}}</ref>\n<blockquote>\nNow an item is purchased jointly; everyone contributes 8 [coins], the excess is 3; everyone contributes 7, the deficit is 4. Tell: The number of people, the item price, what is each? Answer: 7 people, item price 53.<ref>Shen Kangshen, John N. Crossley and Anthony W.-C. Lun, 1999. ''The Nine Chapters on the Mathematical Art: Companion and Commentary''. Oxford: Oxford University Press, p. 358.</ref>\n</blockquote>\n\nBetween the 9th and 10th centuries, the [[Egyptians|Egyptian]] mathematician [[Abu Kamil]] wrote a now-lost treatise on the use of double false position, known as the ''Book of the Two Errors'' (''Kitāb al-khaṭāʾayn''). The oldest surviving writing on double false position from the [[Middle East]] is that of [[Qusta ibn Luqa]] (10th century), an  [[Arab]] mathematician from [[Baalbek]], [[Lebanon]]. He justified the technique by a formal, [[Euclidean geometry|Euclidean-style geometric proof]]. Within the tradition of [[Mathematics in medieval Islam|medieval Muslim mathematics]], double false position was known as ''hisāb al-khaṭāʾayn'' (\"reckoning by two errors\"). It was used for centuries to solve practical problems such as commercial and juridical questions (estate partitions according to rules of [[Islamic inheritance jurisprudence|Quranic inheritance]]), as well as purely recreational problems. The algorithm was often memorized with the aid of [[mnemonics]], such as a verse attributed to [[Ibn al-Yasamin]] and balance-scale diagrams explained by [[al-Hassar]] and [[Ibn al-Banna]], all three being mathematicians of [[Moroccan people|Moroccan]] origin.<ref name=\"Schwartz\">{{Cite conference |conference=Eighth North African Meeting on the History of Arab Mathematics |last=Schwartz |first=R. K. |title=Issues in the Origin and Development of Hisab al-Khata’ayn (Calculation by Double False Position) |location=Radès, Tunisia |year=2004}} Available online at:  http://facstaff.uindy.edu/~oaks/Biblio/COMHISMA8paper.doc and {{cite web |url=http://www.ub.edu/islamsci/Schwartz.pdf |title=Archived copy |accessdate=2012-06-08 |deadurl=yes |archiveurl=https://web.archive.org/web/20140516012137/http://www.ub.edu/islamsci/Schwartz.pdf |archivedate=2014-05-16 |df= }}</ref>\n\nLeonardo of Pisa ([[Fibonacci]]) devoted Chapter 13 of his book ''[[Liber Abaci]]'' (AD 1202) to explaining and demonstrating the uses of double false position, terming the method ''regulis elchatayn'' after the ''al-khaṭāʾayn'' method that he had learned from [[Arab]] sources.<ref name=\"Schwartz\"/> In 1494, [[Luca Pacioli|Pacioli]] used the term ''el cataym'' in his book ''[[Summa de arithmetica]]'', probably taking the term from Fibonacci. Other European writers would follow Pacioli and sometimes provided a translation into Latin or the vernacular. For instance, [[Niccolò Fontana Tartaglia|Tartaglia]] translates the Latinized version of Pacioli's term into the vernacular \"false positions\" in 1556.<ref>{{citation|title=General Trattato|volume=I|year=1556|place=Venice|page=fol. 238, v|quote=Regola Helcataym (vocabulo Arabo) che in nostra lingua vuol dire delle false Positioni}}</ref> Pacioli's term nearly disappeared in the 16th century European works and the technique went by various names such as \"Rule of False\", \"Rule of Position\" and \"Rule of False Position\". ''Regula Falsi'' appears as the Latinized version of Rule of False as early as 1690.<ref name=Smith />\n\nSeveral 16th century European authors felt the need to apologize for the name of the method in a science that seeks to find the truth. For instance, in 1568 Humphrey Baker says:<ref name=Smith />\n{{quote|The Rule of falsehoode is so named not for that it teacheth anye deceyte or falsehoode, but that by fayned numbers taken at all aduentures, it teacheth to finde out the true number that is demaunded, and this of all the vulgar Rules which are in practise) is y<sup>e</sup> most excellence.}}\n\n==Numerical analysis==\n\nThe method of false position provides an exact solution for linear functions, but more direct algebraic techniques have supplanted its use for these functions. However, in [[numerical analysis]], double false position became a [[root-finding algorithm]] used in iterative numerical approximation techniques.\n\nMany equations, including most of the more complicated ones, can be solved only by iterative numerical approximation. This consists of trial and error, in which various values of the unknown quantity are tried. That trial-and-error may be guided by calculating, at each step of the procedure, a new estimate for the solution. There are many ways to arrive at a calculated-estimate and ''regula falsi'' provides one of these.\n\nGiven an equation, move all of its terms to one side so that it has the form, {{math|1=''f''&thinsp;(''x'') = 0}}, where {{mvar|f}} is some function of the unknown variable {{mvar|x}}. A value {{mvar|c}} that satisfies this equation, that is, {{math|1=''f''&thinsp;(''c'') = 0}}, is called a ''root'' or ''zero'' of the function {{mvar|f}} and is a solution of the original equation. If {{mvar|f}} is a [[continuous function]] and there exist two points {{math|''a''<sub>0</sub>}} and {{math|''b''<sub>0</sub>}} such that {{math|''f''&thinsp;(''a''<sub>0</sub>)}} and {{math|''f''&thinsp;(''b''<sub>0</sub>)}} are of opposite signs, then, by the [[intermediate value theorem]], the function {{math|''f''}} has a root in the interval {{math|(''a''<sub>0</sub>, ''b''<sub>0</sub>)}}.\n\nThere are many [[root-finding algorithm]]s that can be used to obtain approximations to such a root. One of the most common is [[Newton's method]], but it can fail to find a root under certain circumstances and it may be computationally costly since it requires a computation of the function's [[derivative]]. Other methods are needed and one general class of methods are the ''two-point bracketing methods''. These methods proceed by producing a sequence of shrinking intervals {{math|[''a''<sub>''k''</sub>, ''b''<sub>''k''</sub>]}}, at the {{mvar|k}}th step, such that {{math|(''a''<sub>''k''</sub>, ''b''<sub>''k''</sub>)}} contains a root of {{math|''f''}}.\n\n===Two-point bracketing methods===\n\nThese methods start with two {{mvar|x}}-values, initially found by trial-and-error, at which {{math|''f''&thinsp;(''x'')}} has opposite signs. Under the continuity assumption, a root of {{mvar|f}} is guaranteed to lie between these two values, that is to say, these values \"bracket\" the root. A point strictly between these two values is then selected and used to create a smaller interval that still brackets a root. If {{mvar|c}} is the point selected, then the smaller interval goes from {{mvar|c}} to the endpoint where {{math|''f''&thinsp;(''x'')}} has the sign opposite that of {{math|''f''&thinsp;(''c'')}}. In the improbable case that {{math|1=''f''&thinsp;(''c'') = 0}}, a root has been found and the algorithm stops. Otherwise, the procedure is repeated as often as necessary to obtain an approximation to the root to any desired accuracy.\n\nThe point selected in any current interval can be thought of as an estimate of the solution. The different variations of this method involve different ways of calculating this solution estimate.\n\nPreserving the bracketing and ensuring that the solution estimates lie in the interior of the bracketing intervals guarantees that the solution estimates will converge toward the solution, a guarantee not available with other root finding methods such as [[Newton's method]] or the [[secant method]].\n\nThe simplest variation, called the [[bisection method]], calculates the solution estimate as the [[midpoint]] of the bracketing interval. That is, if at step {{mvar|k}}, the current bracketing interval is {{math|[''a''<sub>''k''</sub>, ''b''<sub>''k''</sub>]}}, then the new solution estimate {{mvar|c<sub>k</sub>}} is obtained by, \n:<math>c_k=\\frac{a_k+b_k}{2}.</math>\n\nThis ensures that {{mvar|c<sub>k</sub>}} is between {{mvar|a<sub>k</sub>}} and {{mvar|b<sub>k</sub>}}, thereby guaranteeing convergence toward the solution.\n\nSince the bracketing interval's length is halved at each step, the bisection method's error is, on average, halved with each iteration. The method gains roughly a decimal place of accuracy, for each 3 iterations.{{citation needed|date=October 2018}}\n\n===The ''regula falsi'' (false position) method===\n[[Image:False position method.svg|right|351px|thumb|The first two iterations of the false position method. The red curve shows the function {{mvar|f}} and the blue lines are the secants.]]\nThe convergence rate of the bisection method could possibly be improved by using a different solution estimate.\n\nThe ''regula falsi'' method calculates the new solution estimate as the [[X-intercept|{{mvar|x}}-intercept]] of the [[line segment]] joining the endpoints of the function on the current bracketing interval. Essentially, the root is being approximated by replacing the actual function by a line segment on the bracketing interval and then using the classical double false position formula on that line segment.<ref>{{citation|page=40|first=S. D.|last=Conte|title=Elementary Numerical Analysis / an algorithmic approach|year=1965|publisher=McGraw-Hill}}</ref>\n\nMore precisely, suppose that in the {{mvar|k}}-th iteration the bracketing interval is {{math|(''a''<sub>''k''</sub>,  ''b''<sub>''k''</sub>)}}. Construct the line through the points {{math|(''a''<sub>''k''</sub>, ''f''&thinsp;(''a''<sub>''k''</sub>))}} and {{math|(''b''<sub>''k''</sub>, ''f''&thinsp;(''b''<sub>''k''</sub>))}}, as illustrated. This line is a [[secant method|secant]] or chord of the graph of the function {{math|''f''}}. In [[slope|point-slope form]], its equation is given by\n\n:<math> y - f(b_k) = \\frac{f(b_k)-f(a_k)}{b_k-a_k} (x-b_k). </math>\n\nNow choose {{math|''c''<sub>''k''</sub>}} to be the {{mvar|x}}-intercept of this line, that is, the value of {{mvar|x}} for which {{math|1=''y'' = 0}}, and substitute these values to obtain\n\n:<math> f(b_k) + \\frac{f(b_k)-f(a_k)}{b_k-a_k} (c_k-b_k) = 0. </math>\n\nSolving this equation for ''c''<sub>''k''</sub> gives:\n\n:<math>\n  c_k = b_k - f(b_k) \\frac{b_k-a_k}{f(b_k)-f(a_k)} = \\frac{a_k f(b_k) - b_k f(a_k)}{f(b_k) - f(a_k)}.</math>\n\nThis last symmetrical form has a computational advantage:\n\nAs a solution is approached, {{mvar|a<sub>k</sub>}} and {{mvar|b<sub>k</sub>}} will be very close together, and nearly always of the same sign. Such a subtraction can lose significant digits. Because {{math|''f''&thinsp;(''b''<sub>''k''</sub>)}} and {{math|''f''&thinsp;(''a''<sub>''k''</sub>)}} are always of opposite sign the “subtraction” in the numerator of the improved formula is effectively an addition (as is the subtraction in the denominator too).\n\nAt iteration number {{math|''k''}}, the number {{mvar|c<sub>k</sub>}} is calculated as above and then, if {{math|''f''&thinsp;(''a''<sub>''k''</sub>)}} and {{math|''f''&thinsp;(''c''<sub>''k''</sub>)}} have the same sign, set {{math|1=''a''<sub>''k'' + 1</sub> = ''c''<sub>''k''</sub>}} and {{math|1=''b''<sub>''k'' + 1</sub> = ''b''<sub>''k''</sub>}}, otherwise set {{math|1=''a''<sub>''k'' + 1</sub> = ''a''<sub>''k''</sub>}} and {{math|1=''b''<sub>''k'' + 1</sub> = ''c''<sub>''k''</sub>}}. This process is repeated until the root is approximated sufficiently well.\n\nThe above formula is also used in the secant method, but the secant method always retains the last two computed points, and so, while it is slightly faster, it does not preserve bracketing and may not converge.\n\nThe fact that ''regula falsi'' always converges, and has versions that do well at avoiding slowdowns, makes it a good choice when speed is needed. However, its rate of convergence can drop below that of the bisection method.\n\n==Analysis==\n\nSince the initial end-points\n{{math|''a''<sub>0</sub>}} and {{math|''b''<sub>0</sub>}} are chosen such that {{math|''f''&thinsp;(''a''<sub>0</sub>)}} and {{math|''f''&thinsp;(''b''<sub>0</sub>)}} are of opposite signs, at each step, one of the end-points will get closer to a root of {{math|''f''}}.\nIf the second derivative of {{math|''f''}} is of constant sign (so there is no [[inflection point]]) in the interval,\nthen one endpoint (the one where {{math|''f''}} also has the same sign) will remain fixed for all subsequent\niterations while the converging endpoint becomes updated.  As a result,\nunlike the [[bisection method]], the width of the bracket does not tend to\nzero (unless the zero is at an inflection point around which {{math|1=sign(''f''&thinsp;) = −sign(''f''&thinsp;\")}}).  As a consequence, the linear approximation to {{math|''f''&thinsp;(''x'')}}, which is used to pick the false position,\ndoes not improve as rapidly as possible.\n\nOne example of this phenomenon is the function \n:<math> f(x) = 2x^3-4x^2+3x </math>\non the initial bracket\n[&minus;1,1].  The left end, &minus;1, is never replaced (it does not change at first and after the first three iterations, {{math|''f''&thinsp;\"}} is negative on the interval) and thus the width\nof the bracket never falls below 1.  Hence, the right endpoint approaches 0 at\na linear rate (the number of accurate digits grows linearly, with a [[rate of convergence]] of 2/3).{{citation needed|date=October 2018}}\n\nFor discontinuous functions, this method can only be expected to find a point where the function changes sign (for example at {{math|1=''x'' = 0}} for [[multiplicative inverse|{{math|1/''x''}}]] or the [[sign function]]).  In addition to sign changes, it is also possible for the method to converge to a point where the limit of the function is zero, even if the function is undefined (or has another value) at that point (for example at {{math|1=''x'' = 0}} for the function given by {{math|1=''f''&thinsp;(''x'') = abs(''x'') − ''x''<sup>2</sup>}} when {{math|''x'' ≠ 0}} and by {{math|1=''f''&thinsp;(0) = 5}}, starting with the interval [-0.5, 3.0]).\nIt is mathematically possible with discontinuous functions for the method to fail to converge to a zero limit or sign change, but this is not a problem in practice since it would require an infinite sequence of coincidences for both endpoints to get stuck converging to discontinuities where the sign does not change, for example at {{math|1=''x'' = ±1}} in \n:<math>f(x) = \\frac{1}{(x-1)^2} + \\frac{1}{(x+1)^2}.</math>\nThe [[method of bisection]] avoids this hypothetical convergence problem.\n\n==Improvements in ''regula falsi''==\nThough ''regula falsi'' always converges, usually considerably faster than bisection, there are situations that can slow its convergence – sometimes to a prohibitive degree. That problem isn't unique to ''regula falsi'': Other than bisection, ''all'' of the numerical equation-solving methods can have a slow-convergence or no-convergence problem under some conditions. Sometimes, Newton's method and the secant method ''diverge'' instead of converging – and often do so under the same conditions that slow ''regula falsi's'' convergence.\n\nBut, though ''regula falsi'' is one of the best methods, and even in its original un-improved version would often be the best choice; for example, when Newton's isn't used because the derivative is prohibitively time-consuming to evaluate, or when Newton's and ''Successive-Substitutions'' have failed to converge.\n\n''Regula falsi's'' failure mode is easy to detect: The same end-point is retained twice in a row. The problem is easily remedied by picking instead a modified false position, chosen to avoid slowdowns due to those relatively unusual unfavorable situations. A number of such improvements to ''regula falsi'' have been proposed; two of them, the Illinois algorithm and the Anderson−Björk algorithm, are described below.\n\n{{anchor|anc_Illinois_algorithm}}\n===The Illinois algorithm===\nThe Illinois algorithm halves the {{mvar|y}}-value of the retained end point in the next estimate computation when the new {{mvar|y}}-value (that is, {{math|''f''&thinsp;(''c''<sub>''k''</sub>}})) has the same sign as the previous one ({{math|''f''&thinsp;(''c''<sub>''k'' − 1</sub>}})), meaning that the end point of the previous step will be retained. Hence:\n\n:<math> c_k = \\frac{\\frac{1}{2}f(b_k) a_k - f(a_k) b_k}{\\frac{1}{2}f(b_k) - f(a_k)}</math>\nor\n:<math> c_k = \\frac{f(b_k) a_k - \\frac{1}{2}f(a_k) b_k}{f(b_k) - \\frac{1}{2}f(a_k)},</math>\n\ndown-weighting one of the endpoint values to force the next {{math|''c''<sub>''k''</sub>}} to occur on that side of the function.<ref name=Dahlquist/> The factor&nbsp;½ used above looks arbitrary, but it guarantees superlinear convergence (asymptotically, the algorithm will perform two regular steps after any modified step, and has order of convergence 1.442). There are other ways to pick the rescaling which give even better superlinear convergence rates.<ref name=\"Ford\">{{Citation |first=J. A. |last=Ford |year=1995 |title=Improved Algorithms of Illinois-type for the Numerical Solution of Nonlinear Equations |series=Technical Report |id=CSM-257 |publisher=University of Essex Press |citeseerx=10.1.1.53.8676 }}</ref>\n\nThe above adjustment to ''regula falsi'' is called the '''Illinois algorithm''' by some scholars.<ref name=Dahlquist>{{cite book |title=Numerical Methods |first1=Germund |last1=Dahlquist |authorlink1=Germund Dahlquist |first2=Åke |last2=Björck |pages=231–232 |url=https://books.google.com/books?id=armfeHpJIwAC&pg=PA232 |origyear=1974 |year=2003 |publisher=Dover |isbn=978-0486428079}}</ref><ref>{{cite journal |last1=Dowell |first1=M. |last2=Jarratt |first2=P. |doi=10.1007/BF01934364 |title=A modified regula falsi method for computing the root of an equation |journal=BIT |volume=11 |issue=2 |pages=168–174 |year=1971}}</ref> Ford (1995) summarizes and analyzes this and other similar superlinear variants of the method of false position.<ref name=\"Ford\"/>\n\n===Anderson − Björk algorithm===\nSuppose that in the {{mvar|k}}-th iteration the bracketing interval is {{math|[''a''<sub>''k''</sub>,  ''b''<sub>''k''</sub>]}} and that the functional value of the new calculated estimate {{math|''c''<sub>''k''</sub>}} has the same sign as {{math|''f''&thinsp;(''b''<sub>''k''</sub>)}}. In this case, the new bracketing interval {{math|1=[''a''<sub>''k'' + 1</sub>, ''b''<sub>''k'' + 1</sub>] = [''a''<sub>''k''</sub>, ''c''<sub>''k''</sub>]}} and the left-hand endpoint has been retained.\n(So far, that's the same as ordinary Regula Falsi and the Illinois algorithm.)\n\nBut, whereas the Illinois algorithm would multiply {{math|''f''&thinsp;(''a''<sub>''k''</sub>)}} by {{sfrac|1|2}}, Anderson − Björk algorithm multiplies it by {{math|''m''}}, where {{math|''m''}} has one of the two following values:\n\n:<math>m = 1 - \\frac{f(c_k)}{f(b_k)}</math>    if that value of {{math|''m''}} is positive,\n\notherwise, let  <math>m = \\frac{1}{2}</math>.\n\nFor simple roots, Anderson-Björk was the clear winner in Galdino's numerical tests.<ref>{{cite journal|last1=Galdino|first1=Sérgio|title=A family of regula falsi root-finding methods|journal=Proceedings of 2011 World Congress on Engineering and Technology|date=2011|volume=1|url=http://cstm.cnki.net/stmt/TitleBrowse/KnowledgeNet/IEEE201110004133?db=STMI8515|accessdate=9 September 2016}}</ref><ref>{{cite paper|last1=Galdino|first1=Sérgio|title=A family of regula falsi root-finding methods|date=2011|url=http://sergiogaldino.pbworks.com/w/file/fetch/66011429/0130-1943543|accessdate=11 July 2017}}</ref>\n\nFor multiple roots, no method was much faster than bisection. In fact, the only methods that were as fast as bisection were three new methods introduced by Galdino.\nBut even they were only a little faster than bisection.\n\n==Practical considerations==\n\nWhen solving one equation, or just a few, using a computer, the bisection method is an adequate choice. Although bisection isn't as fast as the other methods—when they're at their best and don't have a problem—bisection nevertheless is guaranteed to converge at a useful rate, roughly halving the error with each iteration &ndash; gaining roughly a decimal place of accuracy with every 3&nbsp;iterations.\n\nFor manual calculation, by calculator, one tends to want to use faster methods, and they usually, but not always, converge faster than bisection. But a computer, even using bisection, will solve an equation, to the desired accuracy, so rapidly that there's no need to try to save time by using a less reliable method—and every method is less reliable than bisection.\n\nAn exception would be if the computer program had to solve equations very many times during its run. Then the time saved by the faster methods could be significant.\n\nThen, a program could start with Newton's method, and, if Newton's isn't converging, switch to ''regula falsi'', maybe in one of its improved versions, such as the Illinois or Anderson-Bjőrk versions. Or, if even that isn't converging as well as bisection would, switch to bisection, which always converges at a useful, if not spectacular, rate.\n\nWhen the change in {{math|''y''}} has become very small, and {{math|''x''}} is also changing very little, then Newton's method most likely will not run into trouble, and will converge. So, under those favorable conditions, one could switch to Newton's method if one wanted the error to be very small and wanted very fast convergence.\n\n==Example code==\n\nThis example program, written in the [[C (programming language)|C programming language]], includes the '''Illinois Algorithm.'''\nTo find the positive number {{math|''x''}} where {{math|1=cos(''x'') = ''x''<sup>3</sup>}}, the equation is transformed into a root-finding form {{math|1=''f''&thinsp;(''x'') = cos(''x'') - ''x''<sup>3</sup> = 0}}.\n\n<syntaxhighlight lang=\"c\">\n#include <stdio.h>\n#include <math.h>\n\ndouble f(double x)\n{\n   return cos(x) - x*x*x;\n}\n/* s,t: endpoints of an interval where we search\n   e: half of upper bound for relative error\n   m: maximal number of iterations */\ndouble FalsiMethod(double s, double t, double e, int m)\n{\n   double r,fr;\n   int n, side=0;\n   /* starting values at endpoints of interval */\n   double fs = f(s);\n   double ft = f(t);\n\n   for (n = 0; n < m; n++)\n   {\n\n       r = (fs*t - ft*s) / (fs - ft);\n       if (fabs(t-s) < e*fabs(t+s)) break;\n       fr = f(r);\n\n       if (fr * ft > 0)\n       {\n         /* fr and ft have same sign, copy r to t */\n         t = r; ft = fr;\n         if (side==-1) fs /= 2;\n         side = -1;\n       }\n       else if (fs * fr > 0)\n       {\n         /* fr and fs have same sign, copy r to s */\n         s = r;  fs = fr;\n         if (side==+1) ft /= 2;\n         side = +1;\n       }\n       else\n       {\n         /* fr * f_ very small (looks like zero) */\n         break;\n       } \n    }\n    return r;\n}\n\nint main(void)\n{\n    printf(\"%0.15f\\n\", FalsiMethod(0, 1, 5E-15, 100));\n    return 0;\n}\n</syntaxhighlight>\n\nAfter running this code, the final answer is approximately\n0.865474033101614\n\n==See also==\n* [[Ridders' method]], another root-finding method based on the false position method\n* [[Brent's method]]\n\n==References==\n{{reflist|30em}}\n\n==Further reading==\n* Richard L. Burden, J. Douglas Faires (2000). ''Numerical Analysis'', 7th ed. Brooks/Cole. {{ISBN|0-534-38216-9}}.\n* L.E. Sigler (2002). ''Fibonacci's Liber Abaci, Leonardo Pisano's Book of Calculation''. Springer-Verlag, New York. {{ISBN|0-387-40737-5}}.\n\n{{Root-finding algorithms}}\n\n[[Category:Root-finding algorithms]]\n[[Category:Articles with example C code]]"
    },
    {
      "title": "Fast inverse square root",
      "url": "https://en.wikipedia.org/wiki/Fast_inverse_square_root",
      "text": "[[File:OpenArena-Rocket.jpg|thumb|right|Lighting and reflection calculations (shown here in the [[first-person shooter]] ''[[OpenArena]]'') use the fast inverse square root code to compute [[angle of incidence (optics)|angles of incidence]] and reflection.]]\n\n'''Fast inverse square root''', sometimes referred to as '''Fast InvSqrt()''' or by the [[hexadecimal]] constant '''0x5F3759DF''', is an algorithm that estimates {{math|{{frac|1|{{sqrt|''x''}}}}}}, the [[Multiplicative inverse|reciprocal]] (or multiplicative inverse) of the [[square root]] of a 32-bit [[floating-point]] number {{math|''x''}} in [[Single-precision floating-point format#IEEE 754 single-precision binary floating-point format: binary32|IEEE 754 floating-point format]]. This operation is used in [[digital signal processing]] to [[Unit vector|normalize]] a vector, i.e., scale it to length 1. For example, [[computer graphics]] programs use inverse square roots to compute [[angle of incidence (optics)|angles of incidence]] and [[Reflection (computer graphics)|reflection]] for [[lighting]] and [[shading]].  The algorithm is best known for its implementation in 1999 in the source code of ''[[Quake III Arena]]'', a [[first-person shooter]] video game that made heavy use of [[3D graphics]]. The algorithm only started appearing on public forums such as [[Usenet]] in 2002 or 2003.<ref name=\"Beyond3D\" />{{Refn|group=note|There was a discussion on the Chinese developer forum CSDN back in 2000.<ref name=\"csdn\" />}} At the time, it was generally [[computationally expensive]] to compute the reciprocal of a floating-point number, especially on a large scale; the fast inverse square root bypassed this step.\n\nThe algorithm accepts a 32-bit floating-point number as the input and stores a halved value for later use. Then, treating the bits representing the floating-point number as a 32-bit integer, a [[logical shift]] right by one bit is performed and the result subtracted from the [[magic number (programming)|magic number]] [[Hexadecimal#Representing hexadecimal|0x]]5F3759DF, which is a floating point representation of an approximation of {{sqrt|2<sup>127</sup>}}.<ref name=\"mrob\"/> This results in the first approximation of the inverse square root of the input. Treating the bits again as a floating-point number, it runs one iteration of [[Newton's method]], yielding a more precise approximation.\n\nThe algorithm was originally attributed to [[John D. Carmack|John Carmack]], but an investigation showed that the code had deeper roots in both the hardware and software side of computer graphics. Adjustments and alterations passed through both [[Silicon Graphics]] and [[3dfx Interactive]], with Gary Tarolli's implementation for the [[SGI Indigo]] as the earliest known use. It is not known how the constant was originally derived, though investigation has shed some light on possible methods.\n\n==Motivation==\n[[File:Surface normal.png|thumb|right|[[Surface normal]]s are used extensively in lighting and shading calculations, requiring the calculation of norms for vectors.  A field of vectors normal to a surface is shown here.]]\n[[File:Reflection for Semicircular Mirror.svg|thumb|right|upright=0.7|A two-dimensional example of using the normal {{math|''C''}} to find the angle of reflection from the angle of incidence; in this case, on light reflecting from a curved mirror. The fast inverse square root is used to generalize this calculation to three-dimensional space.]]\nThe inverse square root of a floating point number is used in calculating a [[Unit vector|normalized vector]].{{Sfn|Blinn|2003|p=130}} Programs can use normalized vectors to determine angles of incidence and [[Lambert's cosine law|reflection]]. [[3D graphics]] programs must perform millions of these calculations every second to simulate lighting. When the code was developed in the early 1990s, most floating-point processing power lagged behind the speed of integer processing.<ref name=\"Beyond3D\" /> This was troublesome for 3D graphics programs before the advent of specialized hardware to handle [[Transform, clipping, and lighting|transform and lighting]].\n\nThe length of the vector is determined by calculating its [[Euclidean norm]]: the square root of the sum of squares of the [[vector components]]. When each component of the vector is divided by that length, the new vector will be a [[unit vector]] pointing in the same direction. In a 3D graphics program, all vectors are in three-[[Dimension (vector space)|dimensional]] space, so {{math|'''''v'''''}} would be a vector {{math|(''v''<sub>1</sub>, ''v''<sub>2</sub>, ''v''<sub>3</sub>)}}.\n\n:<math>\\|\\boldsymbol{v}\\| = \\sqrt{v_1^2+v_2^2+v_3^2}</math>\nis the Euclidean norm of the vector.\n\n:<math>\\boldsymbol{\\hat{v}} = \\frac{\\boldsymbol{v}}{\\left\\|\\boldsymbol{v}\\right\\|}</math>\nis the normalized (unit) vector, using {{math|{{norm|'''''v'''''}}<sup>2</sup>}} to represent {{math|''v''{{su|b=1|p=2}} + ''v''{{su|b=2|p=2}} + ''v''{{su|b=3|p=2}}}}.\n\n:<math>\\boldsymbol{\\hat{v}} = \\frac{\\boldsymbol{v}}{\\sqrt{\\left\\|\\boldsymbol{v}\\right\\|^2}}</math>\nwhich relates the unit vector to the inverse square root of the distance components. The inverse square root can be used to compute {{math|'''''v̂'''''}} because this equation is equivalent to\n:<math>\\boldsymbol{\\hat{v}} = \\boldsymbol{v}\\, \\frac{1}{\\sqrt{\\left\\|\\boldsymbol{v}\\right\\|^2}}</math>\nwhere the fraction term is the inverse square root of {{math|{{norm|'''''v'''''}}<sup>2</sup>}}.\n\nAt the time, floating-point division was generally expensive compared to multiplication; the fast inverse square root algorithm bypassed the division step, giving it its performance advantage. ''Quake III Arena'', a [[first-person shooter]] video game, used the fast inverse square root algorithm to accelerate graphics computation, but the algorithm has since been implemented in some dedicated hardware [[vertex shader]]s using [[field-programmable gate array]]s (FPGA).{{Sfn|Middendorf|2007|pp=155–164}}\n\n==Overview of the code==\nThe following code is the fast inverse square root implementation from ''[[Quake III Arena]]'', stripped of [[C preprocessor]] directives, but including the exact original comment text:<ref name=\"quakesrc\" /><!--\n\nThe original source code includes \"fuck\" in the comments. This is a direct quote. See [[WP:CENSOR]].\n\n-->\n<source lang=\"c\">\nfloat Q_rsqrt( float number )\n{\n\tlong i;\n\tfloat x2, y;\n\tconst float threehalfs = 1.5F;\n\n\tx2 = number * 0.5F;\n\ty  = number;\n\ti  = * ( long * ) &y;                       // evil floating point bit level hacking\n\ti  = 0x5f3759df - ( i >> 1 );               // what the fuck? \n\ty  = * ( float * ) &i;\n\ty  = y * ( threehalfs - ( x2 * y * y ) );   // 1st iteration\n//\ty  = y * ( threehalfs - ( x2 * y * y ) );   // 2nd iteration, this can be removed\n\n\treturn y;\n}\n</source>\n\nAt the time, the general method to compute the inverse square root was to calculate an approximation for {{math|{{frac|1|{{sqrt|''x''}}}}}}, then revise that approximation via another method until it came within an acceptable error range of the actual result. [[Methods of computing square roots|Common software methods]] in the early 1990s drew approximations from a [[lookup table]].{{Sfn|Eberly|2001|p=504}} The key of the fast inverse square root was to directly compute an approximation by utilizing the structure of floating-point numbers, proving faster than table lookups. The algorithm was approximately four times faster than computing the square root with another method and calculating the reciprocal via floating-point division.{{Sfn|Lomont|2003|p=1}} The algorithm was designed with the [[IEEE 754-1985]] 32-bit floating-point specification in mind, but investigation from Chris Lomont showed that it could be implemented in other floating-point specifications.{{Sfn|Lomont|2003}}\n\nThe advantages in speed offered by the fast inverse square root [[Hack (computer science)|kludge]] came from treating the longword<ref group=note>Use of the type <code>long</code> reduces the portability of this code on modern systems. For the code to execute properly, <code>sizeof(long)</code> must be 4 bytes, otherwise negative outputs may result. Under many modern 64-bit systems, <code>sizeof(long)</code> is 8 bytes.</ref> containing the floating point number as an integer then subtracting it from a specific constant, '''0x5F3759DF'''.  The purpose of the constant is not immediately clear to someone viewing the code, so, like other such constants found in code, it is often called a [[Magic number (programming)#Unnamed numerical constants|magic number]].<ref name=\"Beyond3D\" />{{Sfn|Lomont|2003|p=3}}{{Sfn|McEniry|2007|p=2, 16}}{{Sfn|Eberly|2001|p=2}} This integer subtraction and bit shift results in a longword which when treated as a floating point number is a rough approximation for the inverse square root of the input number.  One iteration of Newton's method is performed to gain some accuracy, and the code is finished.  The algorithm generates reasonably accurate results using a unique first approximation for [[Newton's method]]; however, it is much slower and less accurate than using the [[Streaming SIMD Extensions|SSE]] instruction <code>rsqrtss</code> on x86 processors also released in 1999.<ref name=\"ruskin\" /><ref name=\"agner\" />\n\nIn terms of C standards, reinterpreting a floating point value as an integer by dereferencing a casted pointer to it is considered [[undefined behavior]]. The proper way to accomplish the fast inverse square root, in a more standard conforming way, is to type-pun floating point values and integers through a [[union type]]. Type-punning with a union type is considered undefined behavior in [[C++]] standards.\n\n<source lang=\"c\">\nfloat Q_rsqrt( float number )\n{\t\n\tconst float x2 = number * 0.5F;\n\tconst float threehalfs = 1.5F;\n\n\tunion {\n\t\tfloat f;\n\t\tuint32_t i;\n\t} conv = {number}; // member 'f' set to value of 'number'.\n\tconv.i  = 0x5f3759df - ( conv.i >> 1 );\n\tconv.f  *= ( threehalfs - ( x2 * conv.f * conv.f ) );\n\treturn conv.f;\n}\n</source>\n\n===A worked example===\nAs an example, the number {{math|''x'' {{=}} 0.15625}} can be used to calculate {{math|{{frac|1|{{sqrt|''x''}}}} ≈ 2.52982}}. The first steps of the algorithm are illustrated below:\n\n 0011_1110_0010_0000_0000_0000_0000_0000  Bit pattern of both x and i\n 0001_1111_0001_0000_0000_0000_0000_0000  Shift right one position: (i &gt;&gt; 1)\n 0101_1111_0011_0111_0101_1001_1101_1111  The magic number 0x5F3759DF\n 0100_0000_0010_0111_0101_1001_1101_1111  The result of 0x5F3759DF - (i &gt;&gt; 1)\n\nUsing IEEE 32-bit representation:\n\n 0_01111100_01000000000000000000000  1.25 × 2<sup>−3</sup>\n 0_00111110_00100000000000000000000  1.125 × 2<sup>−65</sup>\n 0_10111110_01101110101100111011111  1.432430... × 2<sup>63</sup>\n 0_10000000_01001110101100111011111  1.307430... × 2<sup>1</sup>\n\nReinterpreting this last bit pattern as a floating point number gives the approximation {{math|''y'' {{=}} 2.61486}}, which has an error of about 3.4%. After one single iteration of [[Newton's method]], the final result is {{math|''y'' {{=}} 2.52549}}, an error of only 0.17%.\n\n==Algorithm==\nThe algorithm computes {{math|{{frac|1|{{sqrt|''x''}}}}}} by performing the following steps:\n\n# Alias the argument {{math|''x''}} to an integer as a way to compute an approximation of [[binary logarithm|{{math|log<sub>2</sub>(''x'')}}]]\n# Use this approximation to compute an approximation of {{math|log<sub>2</sub>({{frac|1|{{sqrt|''x''}}}}){{=}} −1/2 log<sub>2</sub>(''x'')}}\n# Alias back to a float, as a way to compute an approximation of the base-2 exponential\n# Refine the approximation using a single iteration of the Newton's method.\n\n===Floating-point representation===\n{{Main|Single-precision floating-point format}}\n\nSince this algorithm relies heavily on the bit-level representation of single-precision floating-point numbers, a short overview of this representation is provided here. In order to encode a non-zero real number {{math|''x''}} as a single precision float, the first step is to write {{math|''x''}} as a [[Normalized number|normalized]] binary number:{{Sfn|Goldberg|1991|p=7}}\n\n:<math>\\begin{align}\nx &= \\pm 1.b_1b_2b_3\\ldots \\times 2^{e_x}\\\\\n  &= \\pm 2^{e_x} (1 + m_x)\n\\end{align}</math>\n\nwhere the exponent {{math|''e<sub>x</sub>''}} is an integer, {{math|''m<sub>x</sub>'' ∈ [0, 1)}}, and {{math|1.''b''<sub>1</sub>''b''<sub>2</sub>''b''<sub>3</sub>...}} is the binary representation of the \"significand\" {{math|(1 + ''m<sub>x</sub>'')}}. Since the single bit before the point in the significand is always 1, it need not be stored. From this form, three unsigned integers are computed:{{Sfn|Goldberg|1991|pp=15&ndash;20}}\n\n* {{math|''S<sub>x</sub>''}}, the \"sign bit\", is 0 if {{math|''x'' > 0}}, and 1 if {{math|''x'' < 0}} (1 bit)\n* {{math|''E<sub>x</sub>'' {{=}} ''e<sub>x</sub>'' + ''B''}} is the \"biased exponent\", where {{math|''B'' {{=}} 127}} is the \"[[exponent bias]]\"<ref group=note>{{math|''E<sub>x</sub>''}} should be in the range {{math|[1, 254]}} for {{math|''x''}} to be representable as a [[Normal number (computing)|normal number]].</ref> (8 bits)\n* {{math|''M<sub>x</sub>'' {{=}} ''m<sub>x</sub>'' × ''L''}}, where {{math|''L'' {{=}} 2<sup>23</sup>}}<ref group=note>The only real numbers that can be represented ''exactly'' as floating point are those for which {{math|''M<sub>x</sub>''}} is an integer. Other numbers can only be represented approximately by rounding them to the nearest exactly representable number.</ref> (23 bits)\n\nThese fields are then packed, left to right, into a 32-bit container.{{Sfn|Goldberg|1991|p=16}}\n\nAs an example, consider again the number {{math|''x'' {{=}} 0.15625 {{=}} 0.00101<sub>2</sub>}}. Normalizing {{math|''x''}} yields:\n\n:{{math|''x'' {{=}} +2<sup>−3</sup>(1 + 0.25)}}\n\nand thus, the three unsigned integer fields are:\n\n* {{math|''S'' {{=}} 0}}\n* {{math|''E'' {{=}} −3 + 127 {{=}} 124 {{=}} {{gaps|0111|1100}}<sub>2</sub>}}\n* {{math|''M'' {{=}} 0.25 × 2<sup>23</sup> {{=}} {{val|2097152}} {{=}} {{gaps|010|0000|0000|0000|0000|0000}}<sub>2</sub>}}\n\nthese fields are packed as shown in the figure below:\n\n[[File:Float w significand 2.svg|center]]\n\n===Aliasing to an integer as an approximate logarithm===\n{{see also|Floating point#Piecewise linear approximation to exponential and logarithm}}\nIf {{math|{{frac|1|{{sqrt|''x''}}}}}} was to be calculated without a computer or a calculator, a [[table of logarithms]] would be useful, together with the identity {{math|log<sub>''b''</sub>({{frac|1|{{sqrt|''x''}}}}) {{=}} −{{sfrac|1|2}} log<sub>''b''</sub>(''x'')}}, which is valid for every base {{math|''b''}}. The fast inverse square root is based on this identity, and on the fact that aliasing a float32 to an integer gives a rough approximation of its logarithm. Here is how:\n\nIf {{math|''x''}} is a positive [[Normal number (computing)|normal number]]:\n\n:<math>x = 2^{e_x} (1 + m_x)</math>\n\nthen\n\n:<math>\\log_2(x) = e_x + \\log_2(1 + m_x)</math>\n\nand since {{math|''m<sub>x</sub>'' ∈ [0, 1)}}, the logarithm on the right hand side can be approximated by{{Sfn|McEniry|2007|p=3}}\n\n:<math>\\log_2(1 + m_x) \\approx m_x + \\sigma</math>\n\nwhere {{math|''σ''}} is a free parameter used to tune the approximation. For example, {{math|''σ'' {{=}} 0}} yields exact results at both ends of the interval, while {{math|''σ'' ≈ 0.0430357}} yields the [[Approximation theory#Optimal polynomials|optimal approximation]] (the best in the sense of the [[uniform norm]] of the error).\n\n[[File:Log_by_aliasing_to_int.svg|thumb|right|The integer aliased to a floating point number (in blue), compared to a scaled and shifted logarithm (in gray).]]\n\nThus there is the approximation\n\n:<math>\\log_2(x) \\approx e_x + m_x + \\sigma.</math>\n\nInterpreting the floating-point bit-pattern of {{math|''x''}} as an integer {{math|''I<sub>x</sub>''}} yields<ref group=note>{{math|''S<sub>x</sub>'' {{=}} 0}} since {{math|''x'' > 0}}.</ref>\n\n:<math>\\begin{align}\nI_x &= E_x L + M_x\\\\\n    &= L (e_x + B + m_x)\\\\\n    &= L (e_x + m_x + \\sigma + B - \\sigma)\\\\\n    &\\approx L \\log_2(x) + L (B - \\sigma).\n\\end{align}</math>\n\nIt then appears that {{math|''I<sub>x</sub>''}} is a scaled and shifted piecewise-linear approximation of {{math|log<sub>2</sub>(''x'')}}, as illustrated in the figure on the right. In other words, {{math|log<sub>2</sub>(''x'')}} is approximated by\n\n:<math>\\log_2(x) \\approx \\frac{I_x}{L} - (B - \\sigma).</math>\n\n=== First approximation of the result ===\nThe calculation of {{math|''y'' {{=}} {{frac|1|{{sqrt|''x''}}}}}} is based on the identity\n\n:<math>\\log_2(y) = - \\tfrac{1}{2}\\log_2(x)</math>\n\nUsing the approximation of the logarithm above, applied to both {{math|''x''}} and {{math|''y''}}, the above equation gives:\n\n:<math>\\frac{I_y}{L} - (B - \\sigma) \\approx - \\frac{1}{2}\\left(\\frac{I_x}{L} - (B - \\sigma)\\right)</math>\nThus, an approximation of {{math|''I<sub>y</sub>''}} is:\n:<math>I_y \\approx \\tfrac{3}{2} L (B - \\sigma) - \\tfrac{1}{2} I_x</math>\n\nwhich is written in the code as\n\n:<source lang=\"c\">\ni  = 0x5f3759df - ( i >> 1 );\n</source>\n\nThe first term above is the magic number\n\n:<math>\\tfrac{3}{2} L (B - \\sigma) = \\text{0x5F3759DF}</math>\n\nfrom which it can be inferred that {{math|''σ'' ≈ 0.0450466}}. The second term, {{math|{{sfrac|1|2}}''I<sub>x</sub>''}}, is calculated by shifting the bits of {{math|''I<sub>x</sub>''}} one position to the right.{{sfn|Hennessey|Patterson|1998|p=305}}\n\n===Newton's method===\n{{Main|Newton's method}}\n[[File:3rd-iter.png|alt=|thumb|Relative error between fast inverse square root carrying out 3 iterations of Newton's root-finding method, and direct calculation. Note that double precision is adopted.]]\n[[File:4th-iter.png|thumb|Smallest representable difference between two double precision numbers is reached after carrying out 4 iterations.|alt=]]\nWith {{math|''y''}} as the inverse square root, {{math|''f&thinsp;''(''y'') {{=}} {{sfrac|1|''y''<sup>2</sup>}} − ''x'' {{=}} 0}}. The approximation yielded by the earlier steps can be refined by using a [[root-finding method]], a method that finds the [[zero of a function]]. The algorithm uses [[Newton's method]]: if there is an approximation, {{math|''y''<sub>''n''</sub>}} for {{math|''y''}}, then a better approximation can be calculated {{math|''y''<sub>''n''+1</sub>}} by taking {{math|''y''<sub>''n''</sub> − {{sfrac|''f&thinsp;''(''y<sub>n</sub>'')|''f&thinsp;′''(''y<sub>n</sub>'')}}}}, where {{math|''f&thinsp;′''(''y<sub>n</sub>'')}} is the [[derivative]] of {{math|''f&thinsp;''(''y'')}} at {{math|''y''<sub>''n''</sub>}}.{{Sfn|Hardy|1908|p=323}} For the above {{math|''f&thinsp;''(''y'')}},\n:<math>y_{n+1} = \\frac{y_{n}\\left(3-xy_n^2\\right)}{2}</math>\nwhere {{math|''f&thinsp;''(''y'') {{=}} {{sfrac|1|''y''<sup>2</sup>}} − ''x''}} and {{math|''f&nbsp;′''(''y'') {{=}} −{{sfrac|2|''y''<sup>3</sup>}}}}.\n\nTreating {{math|''y''}} as a floating-point number, <code>y = y*(threehalfs - x2*y*y);</code> is equivalent to\n:<math>y_{n+1} = y_{n}\\left (\\frac32-\\frac{x}{2}y_n^2\\right ) = \\frac{y_{n}\\left(3-xy_n^2\\right)}{2}.</math>\nBy repeating this step, using the output of the function ({{math|''y''<sub>''n''+1</sub>}}) as the input of the next iteration, the algorithm causes {{math|''y''}} to [[Rate of convergence|converge]] to the inverse square root.{{Sfn|McEniry|2007|p=6}} For the purposes of the [[id Tech 3|''Quake III'' engine]], only one iteration was used.  A second iteration remained in the code but was [[Comment out|commented out]].{{Sfn|Eberly|2001|p=2}}\n\n===Accuracy===\n[[File:Invsqrt0-10000.svg|thumb|right|A graph showing the difference between the heuristic fast inverse square root and the direct inversion of square root supplied by [[C++ Standard Library|libstdc]].{{citation needed|date=April 2012}} (Note the log scale on both axes.)]]\nAs noted above, the approximation is surprisingly accurate. The graph on the right plots the error of the function (that is, the error of the approximation after it has been improved by running one iteration of Newton's method), for inputs starting at 0.01, where the standard library gives 10.0 as a result, while InvSqrt() gives 9.982522, making the difference 0.017479, or 0.175% of the true value, 10. The absolute error only drops from then on, while the relative error stays within the same bounds across all orders of magnitude.\n\n==History and investigation==\n[[File:John Carmack E3 2006.jpg|thumb|[[John Carmack]], co-founder of id Software, is commonly associated with the code, though he actually did not write it.]]\nThe source code for ''Quake III'' was not released until [[QuakeCon 2005]], but copies of the fast inverse square root code appeared on [[Usenet]] and other forums as early as 2002 or 2003.<ref name=\"Beyond3D\" /> Initial speculation pointed to John Carmack as the probable author of the code, but he demurred and suggested it was written by Terje Mathisen, an accomplished assembly programmer who had previously helped id Software with ''Quake'' optimization.  Mathisen had written an implementation of a similar bit of code in the late 1990s, but the original authors proved to be much further back in the history of 3D computer graphics with Gary Tarolli's implementation for the [[SGI Indigo]] as a possible earliest known use. Rys Sommefeldt concluded that the original algorithm was devised by Greg Walsh at [[Ardent Computer]] in consultation with [[Cleve Moler]], the creator of [[MATLAB]].<ref name=\"Beyond3Dp2\" /> [[Cleve Moler]] learned about this trick from code written by [[William Kahan]] and K.C. Ng at Berkeley around 1986<ref name=\"MolerResp\" />  [[Jim Blinn]] also demonstrated a simple approximation of the inverse square root in a 1997 column for ''[[List of Institute of Electrical and Electronics Engineers publications|IEEE Computer Graphics and Applications]]''.{{Sfn|Blinn|1997|pp=80–84}}<ref name=\"fdlibm\" />  Paul Kinney implemented a fast method for the FPS T Series computer<ref>{{Cite journal|last=Fazzari|first=Rod|last2=Miles|first2=Doug|last3=Carlile|first3=Brad|last4=Groshong|first4=Judson|date=1988|title=A New Generation of Hardware and Software for the FPS T Series|url=|journal=Proceedings of the 1988 Array Conference|volume=|pages=75–89|via=}}</ref> around 1986.  The system included vector floating-point hardware which was not rich in integer operations. The floating-point values were floated to allow manipulation to create the initial approximation.\n\nIt is not known precisely how the exact value for the magic number was determined.  Chris Lomont developed a function to minimize [[approximation error]] by choosing the magic number {{math|''R''}} over a range. He first computed the optimal constant for the linear approximation step as '''0x5F37642F''', close to '''0x5F3759DF''', but this new constant gave slightly less accuracy after one iteration of Newton's method.{{Sfn|Lomont|2003|p=10}} Lomont then searched for a constant optimal even after one and two Newton iterations and found '''0x5F375A86''', which is more accurate than the original at every iteration stage.{{Sfn|Lomont|2003|p=10}} He concluded by asking whether the exact value of the original constant was chosen through derivation or [[trial and error]].{{Sfn|Lomont|2003|pp=10–11}}  Lomont said that the magic number for 64-bit IEEE754 size type double is '''0x5FE6EC85E7DE30DA''', but it was later shown by Matthew Robertson to be exactly '''0x5FE6EB50C7B537A9'''.<ref name=\"robertson\" />\n\nA complete mathematical analysis for determining the magic number is now available for single-precision floating-point numbers.{{Sfn|Moroz|2018}}\n\n==See also==\n* {{format link|Methods of computing square roots#Approximations that depend on the floating point representation}}\n* [[Magic number (programming)|Magic number]]\n\n==Notes==\n<references group=note/>\n\n==References==\n{{reflist|refs=\n<ref name=\"fdlibm\">{{cite web|url=http://www.netlib.org/fdlibm/e_sqrt.c|title=sqrt implementation in fdlibm}}</ref>\n<ref name=\"MolerResp\">{{cite web|last1=Moler|first1=Cleve|title=Symplectic Spacewar|url=http://blogs.mathworks.com/cleve/2012/06/19/symplectic-spacewar/#comment-13|website=MATLAB Central - Cleve's Corner|publisher=MATLAB|accessdate=2014-07-21}}</ref>\n<ref name=\"Beyond3D\">{{cite web|url=http://www.beyond3d.com/content/articles/8/|title=Origin of Quake3's Fast InvSqrt()|last=Sommefeldt|first=Rys|date=2006-11-29|work=Beyond3D|accessdate=2009-02-12}}</ref>\n<ref name=\"quakesrc\">{{cite web|url=https://github.com/id-Software/Quake-III-Arena/blob/master/code/game/q_math.c#L552|title=quake3-1.32b/code/game/q_math.c|work=[[Quake III Arena]]|publisher=[[id Software]]|accessdate=2017-01-21}}</ref>\n<ref name=\"Beyond3Dp2\">{{cite web|url=http://www.beyond3d.com/content/articles/15/|title=Origin of Quake3's Fast InvSqrt() - Part Two|accessdate=2008-04-19|author=Sommefeldt, Rys|date=2006-12-19|publisher=Beyond3D}}</ref>\n<ref name=\"ruskin\">{{cite web|url=http://assemblyrequired.crashworks.org/timing-square-root/ |title=Timing square root |work=Some Assembly Required |first=Elan |last=Ruskin |date=2009-10-16 |accessdate=2015-05-07}}</ref>\n<ref name=\"agner\">{{cite web|url=http://www.agner.org/optimize/instruction_tables.pdf |title=Lists of instruction latencies, throughputs and micro-operation breakdowns for Intel, AMD and VIA CPUs |accessdate=2017-09-08 |first=Agner |last=Fog}}</ref>\n<ref name=\"robertson\">{{cite web|url=https://cs.uwaterloo.ca/~m32rober/rsqrt.pdf|title=A Brief History of InvSqrt|author=Matthew Robertson|date=2012-04-24|publisher=UNBSJ}}</ref>\n<ref name=\"csdn\">{{cite web|url=http://bbs.csdn.net/topics/41888 |title=Discussion on CSDN |archive-url=https://web.archive.org/web/20150702180504/http://bbs.csdn.net/topics/41888 |archive-date=2015-07-02 |deadurl=yes |df= }}</ref>\n<ref name=\"mrob\">{{cite web|url=https://mrob.com/pub/math/numbers-16.html#le009_16|title=Notable Properties of Specific Numbers|last=Munafo|first=Robert|website=mrob.com|archive-url=https://web.archive.org/web/20181116074733/https://mrob.com/pub/math/numbers-16.html#le009_16|archive-date=16 November 2018|dead-url=no}}</ref>\n}}\n\n=== Bibliography ===\n{{Refbegin|30em}}\n*{{cite journal\n|ref=harv\n|last=Blinn\n|first= Jim\n|title=Floating Point Tricks\n|journal=IEEE Computer Graphics & Applications\n|date=July 1997\n|volume=17\n|issue=4\n|doi=10.1109/38.595279\n|page=80}}\n*{{cite book\n|last=Blinn\n|ref=harv\n|first=Jim\n|title=Jim Blinn's Corner: Notation, notation notation\n|publisher=Morgan Kaufmann\n|year=2003\n|isbn=1-55860-860-5}}\n*{{cite book\n|ref=harv\n|last=Eberly\n|first=David\n|title=3D Game Engine Design\n|publisher=Morgan Kaufmann\n|year=2001\n|isbn=978-1-55860-593-0}}\n*{{cite journal\n|last1=Goldberg\n|first1=David\n|title=What every computer scientist should know about floating-point arithmetic\n|journal=ACM Computing Surveys\n|date=1991\n|volume=23\n|issue=1\n|pages=5–48\n|doi=10.1145/103162.103163\n|ref=harv}}\n*{{cite book|last1=Hardy|first1=Godfrey|title=A Course of Pure Mathematics|date=1908|publisher=[[Cambridge University Press]]|location=[[Cambridge|Cambridge, UK]]|isbn=0-521-72055-9|url=http://www.gutenberg.org/files/38769|ref=harv}}\n*{{cite book\n|ref={{harvid|Hennessey|Patterson|1998}}\n|last=Hennessey\n|first=John\n|author2=Patterson, David A.\n |title=Computer Organization and Design\n|publisher=Morgan Kaufmann Publishers\n|location=San Francisco, CA\n|year=1998\n|edition=2nd\n|isbn=978-1-55860-491-9}}\n*{{cite web\n|ref=harv\n|url=http://www.lomont.org/Math/Papers/2003/InvSqrt.pdf\n|title=Fast Inverse Square Root \n|last=Lomont\n|first=Chris\n|date=February 2003\n|accessdate=2009-02-13}}\n*{{cite web\n|ref=harv\n|url=http://www.daxia.com/bibis/upload/406Fast_Inverse_Square_Root.pdf\n|title=The Mathematics Behind the Fast Inverse Square Root Function Code \n|last=McEniry \n|first=Charles\n|date=August 2007\n|archive-url=https://web.archive.org/web/20150511044204/http://www.daxia.com/bibis/upload/406Fast_Inverse_Square_Root.pdf\n|archive-date=2015-05-11}}\n*{{cite conference\n | ref = {{harvid|Middendorf|2007}}\n | last1 =Middendorf | first1 =Lars\n | last2 = Mühlbauer |first2=Felix\n | last3 = Umlauf |first3=George\n | last4 = Bodba |first4=Christophe\n | date = June 1, 2007\n | title = Embedded Vertex Shader in FPGA\n | conference = [[International Federation for Information Processing|IFIP]] TC10 Working Conference:International Embedded Systems Symposium (IESS)\n | booktitle = Embedded System Design: Topics, Techniques and Trends\n | editor = Rettberg, Achin\n | others = et al.\n | publisher = Springer\n | location = [[Irvine, California]]\n | isbn = 978-0-387-72257-3\n}}\n<!-- no reference exists in article-->\n*{{cite web\n|archiveurl=https://web.archive.org/web/20090215020337/http://www.hackszine.com/blog/archive/2008/12/quakes_fast_inverse_square_roo.html\n|url=http://www.hackszine.com/blog/archive/2008/12/quakes_fast_inverse_square_roo.html\n|title=Quake's fast inverse square root\n|last=Striegel\n|first=Jason\n|date=2008-12-04\n|work=Hackszine\n|publisher=[[O'Reilly Media]]\n|archivedate=2009-02-15\n|accessdate=2013-01-07}}\n*{{Citation\n|title=754-1985 - IEEE Standard for Binary Floating-Point Arithmetic\n|author=[[IEEE Computer Society]]\n|publisher=[[Institute of Electrical and Electronics Engineers]]\n|date=1985\n|url=https://standards.ieee.org/findstds/standard/754-1985.html\n|ref=ieee754\n}}\n*{{cite journal\n|ref=harv\n|last=Moroz\n|first=Leonid V.\n|author2=Walczyk, Cezary J.\n|author3=Hrynchyshyn, Andriy\n|author4=Holimath, Vijay\n|author5=Cieslinski, Jan L.\n|title=Fast calculation of inverse square root with the use of magic constant analytical approach\n|journal=Applied Mathematics and Computation \n|publisher=Elsevier Science Inc.\n|date=January 2018\n|volume=316\n|issue=C\n|doi=10.1016/j.amc.2017.08.025\n|pages=245–255|arxiv=1603.04483\n}}\n{{Refend}}\n\n== Further reading ==\n<!-- no reference exists in article-->\n*{{cite journal\n|last=Kushner\n|first=David\n|date=August 2002\n|title=The wizardry of Id\n|journal=IEEE Spectrum\n|volume=39\n|issue=8\n|pages=42–47\n|doi=10.1109/MSPEC.2002.1021943}}\n\n==External links==\n*[http://blog.quenta.org/2012/09/0x5f3759df.html 0x5f3759df], further investigations into accuracy and generalizability of the algorithm by Christian Plesner Hansen\n*[https://www.beyond3d.com/content/articles/8/ Origin of Quake3's Fast InvSqrt()]\n*[https://github.com/id-Software/Quake-III-Arena Quake III Arena source code]\n*[https://www.desmos.com/calculator/yoz6n1wlvu Implementation of InvSqrt] in [http://www.desmos.com DESMOS]\n\n{{Quake}}\n{{Id Software}}\n\n{{good article}}\n\n[[Category:Quake (series)]]\n[[Category:Source code]]\n[[Category:Root-finding algorithms]]\n[[Category:Articles with example C code]]"
    },
    {
      "title": "Graeffe's method",
      "url": "https://en.wikipedia.org/wiki/Graeffe%27s_method",
      "text": "In [[mathematics]], '''Graeffe's method''' or '''Dandelin&ndash;Lobachesky&ndash;Graeffe method''' is an [[Root-finding algorithm|algorithm for finding all of the roots of a polynomial]].  It was developed independently by [[Germinal Pierre Dandelin]] in 1826 and [[Lobachevsky]] in 1834. In 1837 [[Karl Heinrich Gräffe]] also discovered the principal idea of the method.<ref>{{cite journal |journal=The American Mathematical Monthly |title=Dandelin, Lobačevskiǐ, or Graeffe |first=Alston Scott |last=Householder |author-link=Alston Scott Householder |volume=66 |year=1959 |page=464&ndash;466 |jstor=2310626 |doi=10.2307/2310626}}</ref>  The method separates the roots of a polynomial by squaring them repeatedly. This squaring of the roots is done implicitly, that is, only working on the coefficients of the polynomial. Finally, [[Viète's formulas]] are used in order to approximate the roots.\n\n==Dandelin&ndash;Graeffe iteration==\nLet {{math|''p''(''x'')}} be a polynomial of degree {{mvar|n}}\n\n:<math>p(x) = (x-x_1)\\cdots(x-x_n).</math>\n\nThen \n\n:<math>p(-x) = (-1)^n (x+x_1)\\cdots(x+x_n).</math>\n\nLet {{math|''q''(''x'')}} be the polynomial which has the squares <math>x_1^2, \\cdots, x_n^2</math> as its roots,\n\n:<math>q(x)= \\left (x-x_1^2 \\right )\\cdots \\left (x-x_n^2 \\right ). </math>\n\nThen we can write:\n\n:<math>\\begin{align}\nq(x^2) & = \\left (x^2-x_1^2 \\right )\\cdots \\left (x^2-x_n^2 \\right ) \\\\\n& = (x-x_1)(x+x_1) \\cdots (x-x_n) (x+x_n) \\\\\n& = \\left \\{(x - x_1) \\cdots (x - x_n) \\right \\} \\times \\left \\{(x + x_1) \\cdots (x + x_n) \\right \\} \\\\\n& = p(x) \\times \\left \\{(-1)^n (-x - x_1) \\cdots (-x - x_n) \\right \\} \\\\\n& = p(x) \\times \\left \\{(-1)^n p(-x) \\right \\} \\\\\n& = (-1)^n p(x) p(-x)\n\\end{align}</math>\n\n{{math|''q''(''x'')}} can now be computed by algebraic operations on the coefficients of the polynomial {{math|''p''(''x'')}} alone. Let: \n\n:<math>\\begin{align}\np(x) &= x^n+a_1x^{n-1}+\\cdots+a_{n-1}x+a_n \\\\\nq(x) &= x^n+b_1x^{n-1}+\\cdots+b_{n-1}x+b_n\n\\end{align}</math>\n\nthen the coefficients are related by\n\n:<math>b_k=(-1)^k a_k^2 + 2\\sum_{j=0}^{k-1}(-1)^j\\,a_ja_{2k-j}, \\qquad a_0=b_0=1. </math>\n\nGraeffe observed that if one separates {{math|''p''(''x'')}} into its odd and even parts:\n\n:<math>p(x)=p_e \\left (x^2 \\right )+x p_o\\left (x^2 \\right ),</math>\n\nthen one obtains a simplified algebraic expression for {{math|''q''(''x'')}}:\n\n:<math>q(x)=(-1)^n \\left (p_e(x)^2-x p_o(x)^2 \\right ).</math>\n\nThis expression involves the squaring of two polynomials of only half the degree, and is therefore used in most implementations of the method. \n\nIterating this procedure several times separates the roots with respect to their magnitudes. Repeating ''k'' times gives a polynomial of degree {{mvar|n}}:\n\n:<math>q^k(y) = y^n + {a^k}_1\\,y^{n-1} + \\cdots + {a^k}_{n-1}\\,y + {a^k}_n \\, </math>\n\nwith roots \n\n:<math>y_1=x_1^{2^k},\\,y_2=x_2^{2^k},\\,\\dots,\\,y_n=x_n^{2^k}.</math> \n\nIf the magnitudes of the roots of the original polynomial were separated by some factor <math>\\rho>1</math>, that is, <math>|x_k|\\ge\\rho |x_{k+1}|</math>, then the roots of the ''k''-th iterate are separated by a fast growing factor \n\n:<math>\\rho^{2^k}\\ge 1+2^k(\\rho-1)</math>.\n\n==Classical Graeffe's method==\nNext the [[Vieta relations]] are used\n:<math>\\begin{align}\na^k_{\\;1} &= -(y_1+y_2+\\cdots+y_n)\\\\\na^k_{\\;2} &= y_1 y_2 + y_1 y_3+\\cdots+y_{n-1} y_n\\\\\n        &\\;\\vdots\\\\\na^k_{\\;n} &= (-1)^n(y_1 y_2 \\cdots y_n).\n\\end{align}</math>\n\nIf the roots <math>x_1,\\dots,x_n</math> are sufficiently separated, say by a factor <math>\\rho>1</math>, <math>|x_m|\\ge \\rho|x_{m+1}|</math>, then the iterated powers <math>y_1,y_2,...,y_n</math> of the roots are separated by the factor <math>\\rho^{2^k}</math>, which quickly becomes very big.\n\nThe coefficients of the iterated polynomial can then be approximated by their leading term,\n:<math>a^k_{\\;1} \\approx -y_1</math>\n:<math>a^k_{\\;2} \\approx y_1 y_2</math> and so on,\nimplying\n:<math>\n  y_1\\approx -a^k_{\\;1},\\; \n  y_2\\approx -a^k_{\\;2}/a^k_{\\;1},\n    \\;\\dots\\;\n  y_n\\approx -a^k_{\\;n}/a^k_{\\;n-1}.\n</math>\n\nFinally, logarithms are used in order to find the absolute values of the roots of the original polynomial. These magnitudes alone are already useful to generate meaningful starting points for other root-finding methods.\n\nTo also obtain the angle of these roots, a multitude of methods has been proposed, the most simple one being to successively compute the square root of a (possibly complex) root of <math>q^m(y)</math>, ''m'' ranging from ''k'' to 1, and testing which of the two sign variants is a root of <math>q^{m-1}(x)</math>. Before continuing to the roots of <math>q^{m-2}(x)</math>, it might be necessary to numerically improve the accuracy of the root approximations for <math>q^{m-1}(x)</math>, for instance by [[Newton's method]].\n\nGraeffe's method works best for polynomials with simple real roots, though it can be adapted for polynomials with complex roots and coefficients, and roots with higher multiplicity. For instance, it has been observed<ref>{{cite journal |journal=The American Mathematical Monthly |title=Notes on the Graeffe Method of Root Squaring |first=G.C. |last=Best |volume=56 |number=2 |year=1949 |page=91&ndash;94 |jstor=2306166 |doi=10.2307/2306166}}</ref>  that for a root <math>x_{\\ell+1}=x_{\\ell+2}=\\dots=x_{\\ell+d}</math> with multiplicity ''d'', \nthe fractions \n:<math>\\left|\\frac{(a^{m-1}_{\\;\\ell+i})^2}{a^{m}_{\\;\\ell+i}}\\right|</math> tend to <math>\\binom{d}{i}</math>\nfor <math>i=0,1,\\dots,d</math>. This allows to estimate the multiplicity structure of the set of roots.\n\nFrom a numerical point of view, this method is problematic since the coefficients of the iterated polynomials span very quickly many orders of magnitude, which implies serious numerical errors. One second, but minor concern is that many different polynomials lead to the same Graeffe iterates.\n\n== Tangential Graeffe method ==\nThis method replaces the numbers by truncated [[power series]] of degree 1, also known as [[dual numbers]]. Symbolically, this is achieved by introducing an \"algebraic infinitesimal\" <math>\\varepsilon</math> with the defining property <math>\\varepsilon^2=0</math>. Then the polynomial\n<math>p(x+\\varepsilon)=p(x)+\\varepsilon\\,p'(x)</math> has roots <math>x_m-\\varepsilon</math>, with powers\n:::<math>(x_m-\\varepsilon)^{2^k}=x_m^{2^k}-\\varepsilon\\,{2^k}\\,x_m^{2^k-1}=y_m+\\varepsilon\\,\\dot y_m.</math>\nThus the value of <math>x_m</math> is easily obtained as fraction <math>x_m=-\\tfrac{2^k\\,y_m}{\\dot y_m}.</math>\n\nThis kind of computation with infinitesimals is easy to implement analogous to the computation with complex numbers. If one assumes complex coordinates or an initial shift by some randomly chosen complex number, then all roots of the polynomial will be distinct and consequently recoverable with the iteration.\n\n== Renormalization ==\nEvery polynomial can be scaled in domain and range such that in the resulting polynomial the first and the last coefficient have size one. If the size of the inner coefficients is bounded by ''M'', then the size of the inner coefficients after one stage of the Graeffe iteration is bounded by <math>nM^2</math>. After ''k'' stages one gets the bound <math>n^{2^k-1}M^{2^k}</math> for the inner coefficients.\n\nTo overcome the limit posed by the growth of the powers, Malajovich–Zubelli propose to represent coefficients and intermediate results in the ''k''th stage of the algorithm by a scaled polar form\n:::<math>c=\\alpha\\,e^{-2^k\\,r},</math>\nwhere <math>\\alpha=\\frac{c}{|c|}</math> is a complex number of unit length and <math>r=-2^{-k}\\log|c|</math> is a positive real. Splitting off the power <math>2^k</math> in the exponent reduces the absolute value of ''c'' to the corresponding dyadic root. Since this preserves the magnitude of the (representation of the) initial coefficients, this process was named renormalization.\n\nMultiplication of two numbers of this type is straightforward, whereas addition is performed following the factorization <math>c_3=c_1+c_2=|c_1|\\cdot\\left(\\alpha_1+\\alpha_2\\tfrac{|c_2|}{|c_1|}\\right)</math>, where <math>c_1</math> is chosen as the larger of both numbers, that is, <math>r_1<r_2</math>. Thus\n:::<math>\\alpha_3=\\tfrac{s}{|s|}</math> and <math>r_3=r_1+2^{-k}\\,\\log{|s|}</math> with <math>s=\\alpha_1+\\alpha_2\\,e^{2^k(r_1-r_2)}.</math>\n\nThe coefficients <math>a_0,a_1,\\dots,a_n</math> of the final stage ''k'' of the Graeffe iteration, for some reasonably large value of ''k'', are represented by pairs <math>(\\alpha_m,r_m)</math>, <math>m=0,\\dots,n</math>. By identifying the corners of the convex envelope of the point set <math>\\{(m,r_m):\\;m=0,\\dots,n\\}</math> one can determine the multiplicities of the roots of the polynomial. Combining this renormalization with the tangent iteration one can extract directly from the coefficients at the corners of the envelope the roots of the original polynomial.\n\n== See also ==\n* [[Root-finding algorithm]]\n\n== References ==\n<references />\n* {{MathWorld|urlname=GraeffesMethod|title=Graeffe's Method}}\n* {{cite journal |first1=Gregorio |last1=Malajovich |first2=Jorge P. |last2=Zubelli |title=Tangent Graeffe iteration |journal=Numerische Mathematik |volume=89 |pages=749–782 |year=2001 |issue=4 |doi=10.1007/s002110100278 |citeseerx=10.1.1.44.3611}}\n\n{{DEFAULTSORT:Graeffe's Method}}\n[[Category:Root-finding algorithms]]"
    },
    {
      "title": "Halley's method",
      "url": "https://en.wikipedia.org/wiki/Halley%27s_method",
      "text": "{{Use dmy dates|date=July 2013}}\nIn [[numerical analysis]], '''Halley's method''' is a [[root-finding algorithm]] used for functions of one real variable with a continuous second derivative. It is named after its inventor [[Edmond Halley]].\n\nThe algorithm is second in the class of [[Householder's method]]s, after [[Newton's method]]. Like the latter, it produces iteratively a sequence of approximations to the root; their [[rate of convergence]] to the root is cubic. Multidimensional versions of this method exist.\n\nHalley's method exactly finds the roots of a linear-over-linear [[Padé approximant|Padé approximation]] to the function, in contrast to [[Newton's method]] or the [[Secant method]] which approximate the function linearly, or [[Muller's method]] which approximates the function quadratically.<ref>{{Cite journal | last1 = Boyd | first1 = J. P. | title = Finding the Zeros of a Univariate Equation: Proxy Rootfinders, Chebyshev Interpolation, and the Companion Matrix | doi = 10.1137/110838297 | journal = SIAM Review | volume = 55 | issue = 2 | pages = 375–396 | year = 2013 | pmid =  | pmc = }}</ref>\n\n==Method==\nEdmond Halley was an English mathematician who introduced the method now called by his name. Halley's method is a numerical algorithm for solving the nonlinear equation ''f''(''x'') = 0. In this case, the function ''f'' has to be a function of one real variable. The method consists of a sequence of iterations:\n\n:<math>x_{n+1} = x_n - \\frac {2 f(x_n) f'(x_n)} {2 {[f'(x_n)]}^2 - f(x_n) f''(x_n)} </math>\n\nbeginning with an initial guess ''x''<sub>0</sub>.<ref>{{Cite journal|last=Scavo|first=T. R.|last2=Thoo|first2=J. B.|year=1995|title=On the geometry of Halley's method|journal=American Mathematical Monthly|volume=102|issue=5|pages=417–426|doi=10.2307/2975033|jstor=2975033}}</ref>\n\nIf ''f'' is a three times continuously differentiable function and ''a'' is a zero of ''f'' but not of its derivative, then, in a neighborhood of ''a'', the iterates ''x<sub>n</sub>'' satisfy:\n\n:<math>| x_{n+1} - a | \\le K \\cdot {| x_n - a |}^3,\\text{ for some }K > 0.</math>\n\nThis means that the iterates converge to the zero if the initial guess is sufficiently close, and that the convergence is cubic.<ref>{{Cite journal|last=Alefeld|first=G.|year=1981|title=On the convergence of Halley's method|jstor=2321760|journal=American Mathematical Monthly|volume=88|issue=7|pages=530–536|doi=10.2307/2321760}}</ref>\n\nThe following alternative formulation shows the similarity between Halley's method and Newton's method. The expression <math>f(x_n)/f'(x_n)</math> is computed only once, and it is particularly useful when <math>f''(x_n)/f'(x_n)</math> can be simplified:\n\n:<math>x_{n+1} = x_n -\\frac{f(x_n)}{f'(x_n) - \\frac{f(x_n)}{f'(x_n)}\\frac{f''(x_n)}{2}}= x_n - \\frac {f(x_n)} {f'(x_n)} \\left[1 - \\frac {f(x_n)}{f'(x_n)} \\cdot \\frac {f''(x_n)} {2 f'(x_n)} \\right]^{-1}.</math>\n\nWhen the [[second derivative]] is very close to zero, the Halley's method iteration is almost the same as the Newton's method iteration.\n\n==Derivation==\nConsider the function\n\n:<math>g(x) = \\frac {f(x)} {\\sqrt{|f'(x)|}}.</math>\n\nAny root of ''f'' which is ''not'' a root of its derivative is a root of ''g''; and any root ''r'' of ''g'' must be a root of ''f'' provided the derivative of ''f'' at ''r'' is not infinite. Applying [[Newton's method]] to ''g'' gives\n\n:<math>x_{n+1} = x_n - \\frac{g(x_n)}{g'(x_n)}</math>\n\nwith\n\n:<math>g'(x) = \\frac{2[f'(x)]^2 - f(x) f''(x)}{2 f'(x) \\sqrt{|f'(x)|}}, </math>\n\nand the result follows. Notice that if ''f''′(''c'') = 0, then one cannot apply this at ''c'' because ''g''(''c'') would be undefined.\n\n==Cubic convergence==\nSuppose ''a'' is a root of ''f'' but not of its derivative. And suppose that the third derivative of ''f'' exists and is continuous in a neighborhood of ''a'' and ''x<sub>n</sub>'' is in that neighborhood. Then [[Taylor's theorem]] implies:\n\n:<math>0 = f(a) = f(x_n) + f'(x_n) (a - x_n) + \\frac{f''(x_n)}{2} (a - x_n)^2 + \\frac{f'''(\\xi)}{6} (a - x_n)^3</math>\n\nand also\n\n:<math>0 = f(a) = f(x_n) + f'(x_n) (a - x_n) + \\frac{f''(\\eta)}{2} (a - x_n)^2,</math>\n\nwhere ξ and η are numbers lying between ''a'' and ''x<sub>n</sub>''. Multiply the first equation by <math>2f'(x_n)</math> and subtract from it the second equation times <math>f''(x_n)(a - x_n)</math> to give:\n\n:<math>\\begin{align}\n0 &= 2 f(x_n) f'(x_n) + 2 [f'(x_n)]^2 (a - x_n) + f'(x_n) f''(x_n) (a - x_n)^2 + \\frac{f'(x_n) f'''(\\xi)}{3} (a - x_n)^3 \\\\\n&\\qquad- f(x_n) f''(x_n) (a - x_n) - f'(x_n) f''(x_n) (a - x_n)^2 - \\frac{f''(x_n) f''(\\eta)}{2} (a - x_n)^3.\n\\end{align}</math>\n\nCanceling <math>f'(x_n) f''(x_n) (a - x_n)^2</math> and re-organizing terms yields:\n\n:<math>0 = 2 f(x_n) f'(x_n) + \\left(2 [f'(x_n)]^2 - f(x_n) f''(x_n) \\right) (a - x_n) + \\left(\\frac{f'(x_n) f'''(\\xi)}{3} - \\frac{f''(x_n) f''(\\eta)}{2} \\right) (a - x_n)^3.</math>\n\nPut the second term on the left side and divide through by\n\n:<math> 2 [f'(x_n)]^2 - f(x_n) f''(x_n) </math>\n\nto get:\n\n:<math>a - x_n = \\frac{-2f(x_n) f'(x_n)}{2[f'(x_n)]^2 - f(x_n) f''(x_n)} - \\frac{2f'(x_n) f'''(\\xi) - 3 f''(x_n) f''(\\eta)}{6(2 [f'(x_n)]^2 - f(x_n) f''(x_n))} (a - x_n)^3.</math>\n\nThus:\n\n:<math>a - x_{n+1} = - \\frac{2 f'(x_n) f'''(\\xi) - 3 f''(x_n) f''(\\eta)}{12[f'(x_n)]^2 - 6 f(x_n) f''(x_n)} (a - x_n)^3.</math>\n\nThe limit of the coefficient on the right side as {{math|''x<sub>n</sub>'' → ''a''}} is:\n\n:<math>-\\frac{2 f'(a) f'''(a) - 3 f''(a) f''(a)}{12 [f'(a)]^2}.</math>\n\nIf we take ''K'' to be a little larger than the absolute value of this, we can take absolute values of both sides of the formula and replace the absolute value of coefficient by its upper bound near ''a'' to get:\n\n:<math>|a - x_{n+1}| \\leq K |a - x_n|^3</math>\n\nwhich is what was to be proved.\n\nTo summarize,\n\n:<math>\\Delta x_{i+1} =\\frac{3(f'')^2 - 2f' f'''}{12(f')^2} (\\Delta x_i)^3 + O[\\Delta x_i]^4, \\qquad \\Delta x_i \\triangleq x_i - a.</math><ref>{{Cite journal|last=Proinov|first=Petko D.|last2=Ivanov|first2=Stoil I.|year=2015|title=On the convergence of Halley's method for simultaneous computation of polynomial zeros|journal=J. Numer. Math.|volume=23|issue=4|pages=379–394|doi=10.1515/jnma-2015-0026}}</ref>\n\n==References==\n<references/>\n[https://link.springer.com/article/10.1007/s00009-014-0400-7 Petko D. Proinov, Stoil I. Ivanov, On the Convergence of Halley’s Method for Multiple Polynomial Zeros, Mediterranean J. Math. 12, 555 - 572 (2015)]\n\n==External links==\n* {{MathWorld|urlname=HalleysMethod|title=Halley’s method}}\n* ''[http://numbers.computation.free.fr/Constants/Algorithms/newton.html Newton's method and high order iterations]'', Pascal Sebah and Xavier Gourdon, 2001 (the site has a link to a Postscript version for better formula display)\n\n{{DEFAULTSORT:Halley's Method}}\n[[Category:Root-finding algorithms]]"
    },
    {
      "title": "Householder's method",
      "url": "https://en.wikipedia.org/wiki/Householder%27s_method",
      "text": "{{refimprove|date=November 2013}}\nIn [[mathematics]], and more specifically in [[numerical analysis]], '''Householder's methods''' are a class of [[root-finding algorithm]]s that are used for functions of one real variable with continuous derivatives up to some order {{math|''d'' + 1}}. Each of these methods is characterized by the number {{math|''d''}}, which is known as the ''order'' of the method. The algorithm is iterative and has a [[order of convergence|rate of convergence]] of {{math|''d'' + 1}}.\n\nThese methods are named after the American mathematician [[Alston Scott Householder]].\n\n== Method ==\n\nHouseholder's method is a numerical algorithm for solving the nonlinear equation {{math|1=''f''(''x'') = 0}}. In this case, the function {{math|''f''}} has to be a function of one real variable. The method consists of a sequence of iterations\n\n:<math>x_{n+1} = x_n + d\\; \\frac {\\left(1/f\\right)^{(d-1)}(x_n)} {\\left(1/f\\right)^{(d)}(x_n)} </math>\n\nbeginning with an initial guess {{math|''x''<sub>0</sub>}}.<ref>{{cite book|last=Householder|first=Alston Scott|year=1970|title=The Numerical Treatment of a Single Nonlinear Equation|publisher=McGraw-Hill|isbn=0-07-030465-3|page=169}}</ref>\n\nIf {{math|''f''}} is a {{math|''d'' + 1}} times continuously differentiable function and {{math|''a''}} is a zero of {{math|''f''}} but not of its derivative, then, in a neighborhood of {{math|''a''}}, the iterates {{math|''x''<sub>''n''</sub>}} satisfy:{{cn|date=May 2018}}\n\n:<math>| x_{n+1} - a | \\le K \\cdot {| x_n - a |}^{d+1} </math>, for some <math>K > 0.\\!</math>\n\nThis means that the iterates converge to the zero if the initial guess is sufficiently close, and that the convergence has order {{math|''d'' + 1}}.\n\nDespite their order of convergence, these methods are not widely used because the gain in precision is not commensurate with the rise in effort for large {{math|''d''}}. The ''Ostrowski index'' expresses the error reduction in the number of function evaluations instead of the iteration count.<ref>{{Cite book |first=A. M.|last=Ostrowski |title=Solution of Equations and Systems of Equations |series=Pure and Applied Mathematics |edition=Second |volume=9  |publisher=Academic Press |location=New York |year=1966}}</ref>\n* For polynomials, the evaluation of the first {{math|''d''}} derivatives of {{math|''f''}} at {{math|''x''<sub>''n''</sub>}} using the [[Horner method]] has an effort of {{math|''d'' + 1}} polynomial evaluations. Since {{math|''n''(''d'' + 1)}} evaluations over {{math|''n''}} iterations give an error exponent of {{math|(''d'' + 1)<sup>''n''</sup>}}, the exponent for one function evaluation is <math>\\sqrt[d+1]{d+1}</math>, numerically {{math|1.4142}}, {{math|1.4422}}, {{math|1.4142}}, {{math|1.3797}} for {{math|1=''d'' = 1, 2, 3, 4}}, and falling after that.\n* For general functions the derivative evaluation using the Taylor arithmetic of [[automatic differentiation]] requires the equivalent of {{math|(''d'' + 1)(''d'' + 2)/2}} function evaluations. One function evaluation thus reduces the error by an exponent of <math>\\sqrt[3]{2}\\approx 1.2599</math> for Newton's method, <math>\\sqrt[6]{3}\\approx 1.2009</math> for Halley's method and falling towards 1 or linear convergence for the higher order methods.\n\n== Motivation ==\n=== First approach ===\nAn approximate idea of Householder's method derives from the [[geometric series]]. Let the [[real number|real]]-valued, [[continuous function|continuously]] [[derivative|differentiable]] function {{math|''f(x)''}} have a simple zero at {{math|1=''x'' = ''a''}}, that is a root {{math|1=''f''(''a'') = 0}} of multiplicity one, which is equivalent to <math>f'(a)\\ne0</math>. Then {{math|1/''f''(''x'')}} has a singularity at {{math|''a''}}, specifically a simple pole (also of multiplicity one), and close to {{math|''a''}} the behavior of {{math|1/''f''(''x'')}} is dominated by {{math|1/(''x'' − ''a'')}}. Approximately, one gets\n:<math>\\frac1{f(x)}=\\frac1{f(x)-f(a)}=\\frac{x-a}{f(x)-f(a)}\\cdot\\frac{-1}{a(1-x/a)}\n  \\approx\\frac{-1}{af'(a)}\\cdot\\sum_{k=0}^\\infty\\left(\\frac{x}{a}\\right)^k.\n</math>\nHere <math>f'(a)\\ne0</math> because {{math|''a''}} is a simple zero of {{math|''f''(''x'')}}. The coefficient of degree {{math|''d''}} has the value <math>C\\,a^{-d}</math>. Thus, one can now reconstruct the zero {{math|''a''}} by dividing the coefficient of degree {{math|''d'' − 1}} by the coefficient of degree {{math|''d''}}. Since this geometric series is an approximation to the [[Taylor expansion]] of {{math|1/''f''(''x'')}}, one can get estimates of the zero of {{math|''f''(''x'')}} – now without prior knowledge of the location of this zero – by dividing the corresponding coefficients of the Taylor expansion of {{math|1/''f''(''x'')}} or, more generally, {{math|1/''f''(''b''+''x'')}}. From that one gets, for any integer {{math|''d''}}, and if the corresponding derivatives exist,\n:<math>\n  a \\approx b+\\frac{(1/f)^{(d-1)}(b)}{(d-1)!}\\;\\frac{d!}{(1/f)^{(d)}(b)} = \n            b+d\\;\\frac{(1/f)^{(d-1)}(b)}{(1/f)^{(d)}(b)}.</math>\n\n=== Second approach ===\nSuppose {{math|1=''x'' = ''a''}} is a simple root. Then near {{math|1=''x'' = ''a''}}, {{math|(1/''f'')(''x'')}} is a [[meromorphic function]]. Suppose we have the [[Taylor expansion]]:\n:<math>\n(1/f)(x) = \\sum_{d=0}^{\\infty} \\frac{(1/f)^{(d)}(b)}{d!} (x-b)^d.\n</math>\nBy [[König's theorem (complex analysis)|König's theorem]], we have:\n:<math>\na-b = \\lim_{d\\rightarrow \\infty} \\frac{\\frac{(1/f)^{(d-1)}(b)}{(d-1)!}}{\\frac{(1/f)^{(d)}(b)}{d!}} = \\lim_{d\\rightarrow \\infty} d\\frac{(1/f)^{(d-1)}(b)}{(1/f)^{(d)}(b)}.\n</math>\nThis suggests that Householder's iteration might be a good convergent iteration. The actual proof of the convergence is also based on this idea.\n\n== The methods of lower order ==\n\nHouseholder's method of order 1 is just [[Newton's method]], since:\n:<math>\\begin{array}{rl}\nx_{n+1} =& x_n + 1\\,\\frac {\\left(1/f\\right)(x_n)} {\\left(1/f\\right)^{(1)}(x_n)}\\\\[.7em]\n=& x_n + \\frac{1}{f(x_n)}\\cdot\\left(\\frac{-f'(x_n)}{f(x_n)^2}\\right)^{-1}\\\\[.7em]\n=& x_n - \\frac{f(x_n)}{f'(x_n)}.\n\\end{array}\n</math>\n\nFor Householder's method of order 2 one gets [[Halley's method]], since the identities\n:<math>\\textstyle \n  (1/f)'(x)=-\\frac{f'(x)}{f(x)^2}\\ \n</math>\nand\n:<math>\\textstyle\\ \n  (1/f)''(x)=-\\frac{f''(x)}{f(x)^2}+2\\frac{f'(x)^2}{f(x)^3}</math>\nresult in\n:<math>\\begin{array}{rl}\nx_{n+1} =& x_n + 2\\,\\frac {\\left(1/f\\right)'(x_n)} {\\left(1/f\\right)''(x_n)}\\\\[1em]\n=& x_n + \\frac{-2f(x_n)\\,f'(x_n)}{-f(x_n)f''(x_n)+2f'(x_n)^2}\\\\[1em]\n=& x_n - \\frac{f(x_n)f'(x_n)}{f'(x_n)^2-\\tfrac12f(x_n)f''(x_n)}\\\\[1em]\n=& x_n + h_n\\;\\frac{1}{1+\\frac12(f''/f')(x_n)\\,h_n}.\n\\end{array}\n</math>\nIn the last line, <math>h_n=-\\tfrac{f(x_n)}{f'(x_n)}</math> is the update of the Newton iteration at the point <math>x_n</math>. This line was added to demonstrate where the difference to the simple Newton's method lies.\n\nThe third order method is obtained from the identity of the third order derivative of {{math|1/''f''}}\n:<math>\\textstyle\n  (1/f)'''(x)=-\\frac{f'''(x)}{f(x)^2}+6\\frac{f'(x)\\,f''(x)}{f(x)^3}-6\\frac{f'(x)^3}{f(x)^4}\n</math>\nand has the formula\n:<math>\\begin{array}{rl}\nx_{n+1} =& x_n + 3\\,\\frac {\\left(1/f\\right)''(x_n)} {\\left(1/f\\right)'''(x_n)}\\\\[1em]\n=& x_n - \\frac{6f(x_n)\\,f'(x_n)^2-3f(x_n)^2f''(x_n)}{6f'(x_n)^3-6f(x_n)f'(x_n)\\,f''(x_n)+f(x_n)^2\\,f'''(x_n)}\\\\[1em]\n=& x_n + h_n\\frac{1+\\frac12(f''/f')(x_n)\\,h_n}{1+(f''/f')(x_n)\\,h_n+\\frac16(f'''/f')(x_n)\\,h_n^2}\n\\end{array}\n</math>\nand so on.\n\n== Example ==\nThe first problem solved by Newton with the Newton-Raphson-Simpson method was the polynomial equation <math>y^3-2y-5=0</math>. He observed that there should be a solution close to 2. Replacing {{math|1=''y'' = ''x'' + 2}} transforms the equation into\n:<math>0=f(x)=-1 + 10 x + 6 x^2 + x^3</math>.\nThe Taylor series of the reciprocal function starts with\n:<math>\\begin{array}{rl}\n1/f(x)=& - 1 - 10\\,x - 106 \\,x^2 - 1121 \\,x^3 - 11856 \\,x^4 - 125392 \\,x^5\\\\\n       & - 1326177 \\,x^6 - 14025978 \\,x^7 - 148342234 \\,x^8 - 1568904385 \\,x^9\\\\\n       & - 16593123232 \\,x^{10} +O(x^{11})\n\\end{array}</math>\nThe result of applying Householder's methods of various orders at {{math|1=''x'' = 0}} is also obtained by dividing neighboring coefficients of the latter power series. For the first orders one gets the following values after just one iteration step: For an example, in the case of the 3rd order,\n<math> x_1 = 0.0 + 106/1121 = 0.09455842997324</math>.\n\n{|class=\"wikitable\"\n!d\n!x<sub>1</sub>\n|-\n|1\n|    '''0.1'''00000000000000000000000000000000\n|-\n|2\n|    '''0.094'''339622641509433962264150943396\n|-\n|3\n|    '''0.09455'''8429973238180196253345227475\n|-\n|4\n|    '''0.094551'''282051282051282051282051281\n|-\n|5\n|    '''0.09455148'''6538216154140615031261961\n|-\n|6\n|    '''0.094551481'''438752142436492263099118\n|-\n|7\n|    '''0.09455148154'''3746895938379484125813\n|-\n|8\n|    '''0.0945514815423'''36756233561913325371\n|-\n|9\n|    '''0.09455148154232'''4837086869382419375\n|-\n|10\n|    '''0.094551481542326'''678478801765822984\n|}\nAs one can see, there are a little bit more than {{math|''d''}} correct decimal places for each order d.  The first one hundred digits of the correct solution are {{gaps|0.09455|1481|5423|2659|1482|3865|4057|9302|9638|5730|6105|62823|91803|04128|52904|53121|89983|48366|71462|67281|77715|77578}}.\n\nLet's calculate the <math>x_2, x_3, x_4</math> values for some lowest order,\n\n:<math> f = -1 + 10x + 6x^2 + x^3 </math>\n:<math> f^\\prime = 10 + 12x + 3x^2 </math>\n:<math> f^{\\prime\\prime} = 12 + 6x </math>\n:<math> f^{\\prime\\prime\\prime} = 6 </math>\n\nAnd using following relations,\n\n: 1st order; <math> x_{i+1} = x_{i} -f(x_i)/f^{\\prime}(x_i) </math>\n: 2nd order; <math> x_{i+1} = x_{i} + (-2ff^{\\prime} ) / (2{f^{\\prime}}^2 - ff^{\\prime\\prime}) </math>\n: 3rd order; <math> x_{i+1} = x_{i} - \\frac {6f {f^{\\prime}}^2 - 3f^2 f^{\\prime\\prime} }\n           {6{f^{\\prime}}^3 -6 f f^{\\prime}f^{\\prime\\prime} + f^2f^{\\prime\\prime\\prime}} </math>\n\n{|class=\"wikitable\"\n!x\n!1st (Newton)\n!2nd (Halley)\n!3rd order\n!4th order\n|-\n| x<sub>1</sub>\n|    '''0.'''100000000000000000000000000000000\n|    '''0.094'''339622641509433962264150943395\n|    '''0.09455'''8429973238180196253345227475\n|    '''0.094551'''28205128\n|-\n| x<sub>2</sub>\n|'''0.0945'''68121104185218165627782724844\n|'''0.09455148154'''0164214717107966227500\n|'''0.094551481542326591482'''567319958483\n|\n|-\n| x<sub>3</sub>\n|'''0.094551481'''698199302883823703544266\n|'''0.094551481542326591482386540579303'''\n|'''0.094551481542326591482386540579303'''\n|\n|-\n| x<sub>4</sub>\n|'''0.0945514815423265914'''96064847153714\n|'''0.094551481542326591482386540579303'''\n|'''0.094551481542326591482386540579303'''\n|\n|-\n| x<sub>5</sub>\n|'''0.094551481542326591482386540579303'''\n|\n|\n|\n|-\n| x<sub>6</sub>\n|'''0.094551481542326591482386540579303'''\n|\n|\n|\n|}\n\n<!---gauge00 inserted\nI calculated this table using\nx=0.09455148169819930297; y=x-(-1+10x+6x^2+x^3)/(10+12x+3x^2)\n\nx=0.09455148154232659148238654 ; y0=(-1+10x+6x^2+x^3);y1=(10+12x+3x^2);y2=12+6x; x-2*y0*y1/(2*y1*y1-y0*y2)\n\nx=xx;  y0=(-1+10x+6x^2+x^3);y1=(10+12x+3x^2);y2=12+6x; y3=6; x-(6*y0*y1^2-3*y0*y0*y2)/(6*y1^3-6*y0*y1*y2+y0^2*y3)\n\nin web2.0cal.com\n---->\n\n== Derivation ==\n\nAn exact derivation of Householder's methods starts from the [[Padé approximation]] of order {{math|''d'' + 1}} of the function, where the approximant with linear [[numerator]] is chosen. Once this has been achieved, the update for the next approximation results from computing the unique zero of the numerator.\n\nThe Padé approximation has the form\n:<math>f(x+h)=\\frac{a_0+h}{b_0+b_1h+\\cdots+b_{d-1}h^{d-1}}+O(h^{d+1}).</math>\nThe rational function has a zero at <math>h=-a_0</math>.\n\nJust as the Taylor polynomial of degree {{math|''d''}} has {{math|''d'' + 1}} coefficients that depend on the function {{math|''f''}}, the Padé approximation also has {{math|''d'' + 1}} coefficients dependent on {{math|''f''}} and its derivatives. More precisely, in any Padé approximant, the degrees of the numerator and denominator polynomials have to add to the order of the approximant. Therefore, <math>b_d=0</math> has to hold.\n\nOne could determine the Padé approximant starting from the Taylor polynomial of {{math|''f''}} using [[Euclid's algorithm]]. However, starting from the Taylor polynomial of {{math|1/''f''}} is shorter and leads directly to the given formula. Since\n:<math>\n  (1/f)(x+h) = \n     (1/f)(x)+(1/f)'(x)h+\\cdots+(1/f)^{(d-1)}(x)\\frac{h^{d-1}}{(d-1)!}+(1/f)^{(d)}(x)\\frac{h^d}{d!}+O(h^{d+1})\n</math>\nhas to be equal to the inverse of the desired rational function, we get after multiplying with <math>a_0+h</math> in the power <math>h^d</math> the equation\n:<math>0=b_d=a_0(1/f)^{(d)}(x)\\frac1{d!}+(1/f)^{(d-1)}(x)\\frac1{(d-1)!}</math>.\n\nNow, solving the last equation for the zero <math>h=-a_0</math> of the numerator results in\n:<math>\\begin{array}{rl}\n  h=&-a_0=\n    \\frac{\\frac1{(d-1)!}(1/f)^{(d-1)}(x)}{\\frac1{d!}(1/f)^{(d)}(x)}\\\\[1em]\n  =&d\\,\\frac{(1/f)^{(d-1)}(x)}{(1/f)^{(d)}(x)}\n\\end{array}</math>.\n\nThis implies the iteration formula\n:<math>x_{n+1} = x_n + d\\; \\frac { \\left(1/f\\right)^{(d-1)} (x_n) } { \\left(1/f\\right)^{(d)} (x_n) } </math>.\n\n== Relation to Newton's method ==\nHouseholder's method applied to the real-valued function {{math|''f''(''x'')}} is the same as Newton's method applied to the function {{math|''g''(''x'')}}:\n:<math>x_{n+1} = x_n - \\frac{g(x_n)}{g'(x_n)}</math>\nwith\n:<math>g(x) = \\left|(1/f)^{(d-1)}\\right|^{-1/d}\\,.</math>\nIn particular, {{math|1=''d'' = 1}} gives Newton's method unmodified and {{math|1=''d'' = 2}} gives Halley's method.\n\n== References ==\n{{reflist}}\n\n== External links ==\n* {{cite web|title=Newton's method and high order iteration|url=http://numbers.computation.free.fr/Constants/Algorithms/newton.html|author=Pascal Sebah and Xavier Gourdon|year=2001}} ''Note'': Use the [[PostScript]] version of this link; the website version is not compiled correctly.\n\n[[Category:Root-finding algorithms]]"
    },
    {
      "title": "Illinois algorithm",
      "url": "https://en.wikipedia.org/wiki/Illinois_algorithm",
      "text": "#REDIRECT [[False position method#anc_Illinois_algorithm]]\n\n[[Category:Root-finding algorithms]]"
    },
    {
      "title": "Integer square root",
      "url": "https://en.wikipedia.org/wiki/Integer_square_root",
      "text": "In [[number theory]], the '''integer square root''' (isqrt) of a [[positive integer]] ''n'' is the positive integer ''m'' which is the [[floor and ceiling functions|greatest integer less than or equal]] to the [[square root]] of ''n'',\n\n: <math>\\mbox{isqrt}( n ) = \\lfloor \\sqrt n \\rfloor.</math>\n\nFor example, <math>\\mbox{isqrt}(27) = 5</math> because <math>5\\cdot 5=25 \\le 27</math> and <math>6\\cdot 6=36 > 27</math>.\n\n==Algorithm using Newton's method==\nOne way of calculating <math>\\sqrt{n}</math> and <math>\\mbox{isqrt}( n )</math> is to use [[Newton's method]] to find a solution for the equation <math>x^{2} - n = 0</math>, giving the iterative formula\n\n: <math>{x}_{k+1} = \\frac{1}{2}\\left(x_k + \\frac{ n }{x_k}\\right), \\quad k \\ge 0, \\quad x_0 > 0.</math>\n\nThe [[sequence]] <math>\\{ x_k \\}</math> [[Limit (mathematics)|converges]] [[Rate of convergence|quadratically]] to <math>\\sqrt{n}</math> as <math>k\\to \\infty</math>. It can be proven that if <math>x_{0} = n</math> is chosen as the initial guess, one can stop as soon as\n:<math>| x_{k+1}-x_{k}| < 1</math>\nto ensure that <math>\\lfloor x_{k+1} \\rfloor=\\lfloor \\sqrt n \\rfloor.</math>\n\n===Using only integer division===\nFor computing <math>\\lfloor \\sqrt n \\rfloor</math> for very large integers ''n'', one can use the quotient of [[Euclidean division]] for both of the division operations.  This has the advantage of only using integers for each intermediate value, thus making the use of [[floating point]] representations of large numbers unnecessary.  It is equivalent to using the iterative formula\n\n: <math>{x}_{k+1} = \\left\\lfloor \\frac{1}{2}\\left(x_k + \\left\\lfloor \\frac{ n }{x_k} \\right\\rfloor \\right) \\right\\rfloor, \\quad k \\ge 0, \\quad x_0 > 0, \\quad x_0 \\in \\mathbb{Z}.</math>\n\nBy using the fact that\n\n: <math>\\left\\lfloor \\frac{1}{2}\\left(x_k + \\left\\lfloor \\frac{ n }{x_k} \\right\\rfloor \\right) \\right\\rfloor = \\left\\lfloor \\frac{1}{2}\\left(x_k + \\frac{ n }{x_k} \\right) \\right\\rfloor,</math>\n\none can show that this will reach <math>\\lfloor \\sqrt n \\rfloor</math> within a finite number of iterations.\n\nHowever, <math>\\lfloor \\sqrt n \\rfloor</math> is not necessarily a [[Fixed point (mathematics)|fixed point]] of the above iterative formula. Indeed, it can be shown that <math>\\lfloor \\sqrt n \\rfloor</math> is a fixed point if and only if <math>n + 1</math> is not a perfect square. If <math>n + 1</math> is a perfect square, the sequence ends up in a period-two cycle between <math>\\lfloor \\sqrt n \\rfloor</math> and <math>\\lfloor \\sqrt n \\rfloor + 1</math> instead of converging.  For termination, it suffices to check that either the number has converged or it has increased by exactly one from the previous step, in which case the new result is discarded.\n\n===Domain of computation===\nAlthough <math>\\sqrt{n}</math> is [[irrational number|irrational]] for many <math>n</math>, the sequence <math>\\{ x_k \\}</math> contains only [[rational number|rational]] terms when <math> x_0 </math> is rational. Thus, with this method it is unnecessary to exit the [[field (mathematics)|field]] of rational numbers in order to calculate <math>\\mbox{isqrt}( n )</math>, a fact which has some theoretical advantages.\n\n===Stopping criterion===\nOne can prove that <math>c=1</math> is the largest possible number for which the stopping criterion \n:<math>|x_{k+1} - x_{k}| < c</math>\nensures <math>\\lfloor x_{k+1} \\rfloor=\\lfloor \\sqrt n \\rfloor</math>\nin the algorithm above.\n\nIn implementations which use number formats that cannot represent all rational numbers exactly (for example, floating point), a stopping constant less than one should be used to protect against roundoff errors.\n\n==Digit-by-digit algorithm==\n\nThe traditional [[Methods_of_computing_square_roots#Digit-by-digit_calculation|pen-and-paper algorithm]] for computing the square root <math>\\sqrt{n}</math> is based on working from higher digit places to lower, and as each new digit pick the largest that will still yield a square <math>\\leq n</math>. If stopping after the one's place, the result computed will be the integer square root.\n\n===Using [[bitwise operation]]s===\n\nIf working in [[base 2]], the choice of digit is simplified to that between 0 (the \"small candidate\") and 1 (the \"large candidate\"), and digit manipulations can be expressed in terms of binary shift operations. With <code>*</code> being multiplication, <code>&lt;&lt;</code> being left shift, and <code>&gt;&gt;</code> being logical right shift, a [[Recursion (computer science)|recursive]] algorithm to find the integer square root of any [[natural number]] is:\n\n function integerSqrt(n):\n     if n &lt; 0:\n         error \"integerSqrt works for only nonnegative inputs\"\n     else if n &lt; 2:\n         return n\n     else:\n         # Recursive call:\n         smallCandidate = integerSqrt(n &gt;&gt; 2) &lt;&lt; 1\n         largeCandidate = smallCandidate + 1\n         if largeCandidate*largeCandidate &gt; n:\n             return smallCandidate\n         else:\n             return largeCandidate\n\nOr, iteratively instead of recursively:\n\n function integerSqrt(n):\n     if n &lt; 0:\n         error \"integerSqrt works for only nonnegative inputs\"\n     \n     # Find greatest shift.\n     shift = 2\n     nShifted = n &gt;&gt; shift\n     # We check for nShifted being n, since some implementations\n     # perform shift operations modulo the word size.\n     while nShifted ≠ 0 and nShifted ≠ n:\n         shift = shift + 2\n         nShifted = n &gt;&gt; shift\n     shift = shift - 2\n     \n     # Find digits of result.\n     result = 0\n     while shift ≥ 0:\n         result = result &lt;&lt; 1\n         candidateResult = result + 1\n         if candidateResult*candidateResult ≤ n &gt;&gt; shift:\n             result = candidateResult\n         shift = shift - 2\n    \n     return result\n\nTraditional pen-and-paper presentations of the digit-by-digit algorithm include various optimisations not present in the code above, in particular the trick of presubtracting the square of the previous digits which makes a general multiplication step unnecessary.\n\n== See also ==\n* [[Methods of computing square roots]]\n\n==External links==\n*[http://mathcentral.uregina.ca/RR/database/RR.09.95/grzesina1.html A geometric view of the square root algorithm]\n\n{{number theoretic algorithms}}\n\n[[Category:Number theoretic algorithms]]\n[[Category:Number theory]]\n[[Category:Root-finding algorithms]]"
    },
    {
      "title": "Inverse quadratic interpolation",
      "url": "https://en.wikipedia.org/wiki/Inverse_quadratic_interpolation",
      "text": "In [[numerical analysis]], '''inverse quadratic interpolation''' is a [[root-finding algorithm]], meaning that it is an algorithm for solving equations of the form ''f''(''x'') = 0. The idea is to use [[polynomial interpolation|quadratic interpolation]] to approximate the [[inverse function|inverse]] of ''f''. This algorithm is rarely used on its own, but it is important because it forms part of the popular [[Brent's method]].\n\n==The method==\n\nThe inverse quadratic interpolation algorithm is defined by the [[recurrence relation]]\n\n:<math> x_{n+1} = \\frac{f_{n-1}f_n}{(f_{n-2}-f_{n-1})(f_{n-2}-f_n)} x_{n-2} + \\frac{f_{n-2}f_n}{(f_{n-1}-f_{n-2})(f_{n-1}-f_n)} x_{n-1} </math>\n\n:::::<math> {} + \\frac{f_{n-2}f_{n-1}}{(f_n-f_{n-2})(f_n-f_{n-1})} x_n, </math>\n\nwhere ''f''<sub>''k''</sub> = ''f''(''x''<sub>''k''</sub>). As can be seen from the recurrence relation, this method requires three initial values, ''x''<sub>0</sub>, ''x''<sub>1</sub> and ''x''<sub>2</sub>.\n\n==Explanation of the method==\n\nWe use the three preceding iterates, ''x''<sub>''n''&minus;2</sub>, ''x''<sub>''n''&minus;1</sub> and ''x''<sub>''n''</sub>, with their function values, ''f''<sub>''n''&minus;2</sub>, ''f''<sub>''n''&minus;1</sub> and ''f''<sub>''n''</sub>. Applying the [[Lagrange polynomial|Lagrange interpolation formula]] to do quadratic interpolation on the inverse of ''f'' yields\n\n:<math> f^{-1}(y) = \\frac{(y-f_{n-1})(y-f_n)}{(f_{n-2}-f_{n-1})(f_{n-2}-f_n)} x_{n-2} + \\frac{(y-f_{n-2})(y-f_n)}{(f_{n-1}-f_{n-2})(f_{n-1}-f_n)} x_{n-1} </math>\n\n:::::<math> {} + \\frac{(y-f_{n-2})(y-f_{n-1})}{(f_n-f_{n-2})(f_n-f_{n-1})} x_n. </math>\n\nWe are looking for a root of ''f'', so we substitute ''y'' = ''f''(''x'') = 0 in the above equation and this results in the above recursion formula.\n\n==Behaviour==\n\nThe asymptotic behaviour is very good: generally, the iterates ''x''<sub>''n''</sub> converge fast to the root once they get close. However, performance is often quite poor if you do not start very close to the actual root. For instance, if by any chance two of the function values ''f''<sub>''n''&minus;2</sub>, ''f''<sub>''n''&minus;1</sub> and ''f''<sub>''n''</sub> coincide, the algorithm fails completely. Thus, inverse quadratic interpolation is seldom used as a stand-alone algorithm.\n\nThe order of this convergence is approximately 1.8 as can be proved by [[Secant Method]] analysis.\n\n==Comparison with other root-finding methods==\n\nAs noted in the introduction, inverse quadratic interpolation is used in [[Brent's method]]. \n\nInverse quadratic interpolation is also closely related to some other root-finding methods.\nUsing [[linear interpolation]] instead of quadratic interpolation gives the [[secant method]]. Interpolating ''f'' instead of the inverse of ''f'' gives [[Muller's method]].\n\n==See also==\n* [[Successive parabolic interpolation]] is a related method that uses parabolas to find extrema rather than roots.\n\n==References==\n\n*[[James F. Epperson]], [https://books.google.com/books?id=Mp8-z5mHptcC&lpg=PP1&pg=PA182#v=onepage&q&f=false An introduction to numerical methods and analysis], pages 182-185, Wiley-Interscience, 2007. {{isbn|978-0-470-04963-1}} \n\n[[Category:Root-finding algorithms]]"
    },
    {
      "title": "Laguerre's method",
      "url": "https://en.wikipedia.org/wiki/Laguerre%27s_method",
      "text": "{{short description|Polynomial root-finding algorithm}}\nIn [[numerical analysis]], '''Laguerre's method''' is a [[root-finding algorithm]] tailored to [[polynomial]]s. In other words, Laguerre's method can be used to numerically solve the equation {{math|''p''(''x'') {{=}} 0}} for a given polynomial {{math|''p''(''x'')}}.  One of the most useful properties of this method is that it is, from extensive empirical study, very close to being a \"sure-fire\" method, meaning that it is almost guaranteed to always converge to ''some'' root of the polynomial, no matter what initial guess is chosen. However, for [[computer]] computation, more efficient methods are known, with which it is guaranteed to find all roots (see {{slink|Root-finding algorithm|Roots of polynomials}}) or all real roots (see [[Real-root isolation]]).\n\nThis method is named in honour of [[Edmond Laguerre]], a French mathematician.\n\n==Definition==\nThe algorithm of the Laguerre method to find one root of a polynomial {{math|''p''(''x'')}} of degree {{mvar|n}} is:\n* Choose an initial guess {{math|''x''<sub>0</sub>}}\n* For {{math|''k'' {{=}} 0, 1, 2, &hellip;}}\n** If <math>p(x_k)</math> is very small, exit the loop\n** Calculate <math> G = \\frac{p'(x_k)}{p(x_k)}</math>\n** Calculate <math> H = G^2 - \\frac{p''(x_k)}{p(x_k)}</math>\n** Calculate <math> a = \\frac{n}{G \\plusmn \\sqrt{(n-1)(nH - G^2)}} </math>, where the sign is chosen to give the denominator with the larger absolute value, to avoid [[loss of significance]] as iteration proceeds.\n** Set <math> x_{k+1} = x_k - a </math>\n* Repeat until ''a'' is small enough or if the maximum number of iterations has been reached.\n\nIf a root has been found, the corresponding linear factor can be removed from ''p''. This deflation step reduces the degree of the polynomial by one, so that eventually, approximations for all roots of ''p'' can be found. Note however that deflation can lead to approximate factors that differ significantly from the corresponding exact factors. This error is least if the roots are found in the order of increasing magnitude.\n\n==Derivation==\nThe [[fundamental theorem of algebra]] states that every ''n''th degree polynomial <math>p</math> can be written in the form\n\n:<math>p(x) = C(x - x_1)(x - x_2)\\cdots(x - x_n), </math>\n\nsuch that <math>x_k</math> where <math>(k=1, 2,..., n)</math> are the roots of the polynomial. If we take the [[natural logarithm]] of both sides, we find that\n\n:<math>\\ln |p(x)| = \\ln |C| + \\ln |x - x_1| + \\ln |x - x_2| + \\cdots + \\ln |x - x_n|. </math>\n\nDenote the derivative by\n\n:<math>G = \\frac{d}{dx} \\ln |p(x)| = \\frac{1}{x - x_1} + \\frac{1}{x - x_2} + \\cdots + \\frac{1}{x - x_n}, </math>\n\nand the negated second derivative by\n\n:<math> H = -\\frac{d^2}{dx^2} \\ln |p(x)|= \\frac{1}{(x - x_1)^2} + \\frac{1}{(x - x_2)^2} + \\cdots + \\frac{1}{(x - x_n)^2}. </math>\n\nWe then make what Acton calls a 'drastic set of assumptions', that the root we are looking for, say, <math>x_1</math> is a certain distance away from our guess <math>x</math>, and all the other roots are clustered together some distance away. If we denote these distances by\n:<math>\na = x - x_1 \\,\n</math>\nand\n:<math>\nb = x - x_i,\\quad i = 2, 3,\\ldots, n\n</math>\nthen our equation for <math>G</math> may be written as\n:<math>\nG = \\frac{1}{a} + \\frac{n - 1}{b}\n</math>\nand the expression for <math>H</math> becomes\n:<math>\nH = \\frac{1}{a^2} + \\frac{n-1}{b^2}.\n</math>\nSolving these equations for <math>a</math>, we find that\n:<math>\na = \\frac{n}{G \\plusmn \\sqrt{(n-1)(nH - G^2)}}\n</math>,\nwhere the square root of a complex number is chosen to produce larger absolute value of the denominator, or equivalently, to satisfy:\n:<math>\\operatorname{Re}\\,(\\overline{G}  \\sqrt{(n-1)(nH - G^2)}\\,)>0</math>,\nwhere {{math|Re}} denotes real part of a complex number, and {{overline|{{mvar|G}}}} is the complex conjugate of {{mvar|G}}; or\n:<math>\na = \\frac{p(x)}{p'(x)}\\cdot\n      \\left(\n         \\frac1n+\\frac{n-1}n\\,\\sqrt{1-\\frac{n}{n-1}\\,\\frac{p(x)p''(x)}{p'(x)^2}}\n      \\right)^{-1}\n</math>,\nwhere the square root of a complex number is chosen to have a non-negative real part.\n\nFor small values of {{math|''p''(''x'')}} this formula differs from the offset of the third order [[Halley's method]] by an error of {{math|''O''(''p''(''x'')<sup>3</sup>)}}, so convergence close to a root will be cubic as well.\n\nNote that, even if the 'drastic set of assumptions' does not work for some particular polynomial {{math|''p''(''x'')}}, {{math|''p''(''x'')}} can be transformed into a related polynomial {{mvar|r}} for which the assumptions are correct, e.g. by shifting the origin towards a suitable complex number {{mvar|w}}, giving {{math|''q''(''x'') {{=}} ''p''(''x'' − ''w'')}}, to give distinct roots distinct magnitudes if necessary (which it will be if some roots are complex conjugates), and then getting {{mvar|r}} from {{math|''q''(''x'')}} by repeatedly applying the root squaring transformation used in [[Graeffe's method]] enough times to make the smaller roots significantly smaller than the largest root (and so, clustered in comparison); the approximate root from Graeffe's method can then be used to start the new iteration for Laguerre's method on {{mvar|r}}. An approximate root for {{math|''p''(''x'')}} may then be obtained straightforwardly from that for {{mvar|r}}.\n\nIf we make the stronger assumption that the terms in {{mvar|G}} corresponding to the roots {{math|''x<sub>i</sub>'', ''i'' {{=}} 2, 3, &hellip;, ''n''}} are negligibly small in comparison to the term corresponding to the root {{math|''x''<sub>1</sub>}}, this leads to [[Newton's method]].\n\n==Properties==\n[[Image:Attraction zones of Laguerre's.png|200px|right|thumb|Attraction zones of Laguerre's method for polynomial x^4 + 2*x^3 + 3*x^2 + 4*x + 1]]\n\nIf {{mvar|x}} is a simple root of the polynomial {{math|''p''(''x'')}}, then Laguerre's method converges [[rate of convergence|cubically]] whenever the initial guess {{math|''x''<sub>0</sub>}} is close enough to the root {{mvar|x}}. On the other hand, if {{mvar|x}} is a [[multiple root]] then the convergence is only linear.  This is obtained with the penalty of calculating values for the polynomial and its first and second derivatives at each stage of the iteration.\n\nA major advantage of Laguerre's method is that it is almost guaranteed to converge to ''some'' root of the polynomial ''no matter where the initial approximation is chosen''.  This is in contrast to other methods such as the [[Newton's method|Newton–Raphson method]] which may fail to converge for poorly chosen initial guesses.  It may even converge to a complex root of the polynomial, because of the square root being taken in the calculation of {{mvar|a}} above may be of a negative number.  This may be considered an advantage or a liability depending on the application to which the method is being used.  Empirical evidence has shown that convergence failure is extremely rare, making this a good candidate for a general purpose polynomial root finding algorithm.  However, given the fairly limited theoretical understanding of the algorithm, many numerical analysts are hesitant to use it as such, and prefer better understood methods such as the [[Jenkins–Traub algorithm]], for which more solid theory has been developed.  Nevertheless, the algorithm is fairly simple to use compared to these other \"sure-fire\" methods, easy enough to be used by hand or with the aid of a pocket calculator when an automatic computer is unavailable.  The speed at which the method converges means that one is only very rarely required to compute more than a few iterations to get high accuracy.\n\n==References==\n*{{cite book |authorlink=Forman S. Acton |first=Forman S. |last=Acton |title=Numerical Methods that Work |publisher=Harper & Row |year=1970 |isbn=0-88385-450-3 }}\n*{{cite journal |first=S. |last=Goedecker |title=Remark on Algorithms to Find Roots of Polynomials |journal=SIAM J. Sci. Comput. |volume=15 |issue=5 |pages=1059–1063 |year=1994 |doi=10.1137/0915064 }}\n*{{cite paper |first=Wankere R. |last=Mekwi |year=2001 |url=http://eprints.maths.ox.ac.uk/archive/00000016/ |title=Iterative Methods for Roots of Polynomials |work=Master's thesis, University of Oxford }}\n*{{cite journal |first=V. Y. |last=Pan |title=Solving a Polynomial Equation: Some History and Recent Progress |journal=SIAM Rev. |volume=39 |issue=2 |pages=187–220 |year=1997 |doi=10.1137/S0036144595288554 }}\n*{{Cite book | last1=Press | first1=WH | last2=Teukolsky | first2=SA | last3=Vetterling | first3=WT | last4=Flannery | first4=BP | year=2007 | title=Numerical Recipes: The Art of Scientific Computing | edition=3rd | publisher=Cambridge University Press |  publication-place=New York | isbn=978-0-521-88068-8 | chapter=Section 9.5.3. Laguerre's Method | chapter-url=http://apps.nrbook.com/empanel/index.html#pg=466}}\n*{{cite book |first=Anthony |last=Ralston |first2=Philip |last2=Rabinowitz |title=A First Course in Numerical Analysis |publisher=McGraw-Hill |year=1978 |isbn=0-07-051158-6 }}\n\n{{Root-finding algorithms}}\n[[Category:Root-finding algorithms]]"
    },
    {
      "title": "Lehmer–Schur algorithm",
      "url": "https://en.wikipedia.org/wiki/Lehmer%E2%80%93Schur_algorithm",
      "text": "In [[mathematics]], the '''Lehmer–Schur algorithm''' (named after [[Derrick Henry Lehmer]] and [[Issai Schur]]) is a [[root-finding algorithm]] for [[complex polynomial]]s, extending the idea of enclosing roots like in the one-dimensional [[bisection method]] to the complex plane. It uses the Schur-Cohn test to test increasingly smaller disks for the presence or absence of roots.\n\n==Schur-Cohn algorithm==\nThis [[algorithm]] allows to find the distribution of the roots of a complex polynomial with respect to the [[unit circle]] in the complex plane.\n<ref>{{cite journal |last1=Cohn |first1=A |title=Uber die Anzahl der Wurzeln einer algebraischen Gleichung in einem Kreise. |journal=Math. Z. |date=1922 |volume=14 |pages=110–148 |doi=10.1007/BF01215894 |url=http://gdz.sub.uni-goettingen.de/dms/load/img/?PPN=PPN266833020_0014&DMDID=dmdlog10}}</ref>\n<ref name=\"Henrici\">{{cite book |last1=Henrici |first1=Peter |title=Applied and computational complex analysis. Volume I: Power series- integration-conformal mapping-location of zeros. |date=1988 |publisher=New York etc.: John Wiley |isbn=0-471-60841-6 |pages=xv + 682 |edition=  Repr. of the orig., publ. 1974 by John Wiley \\& Sons Ltd., Paperback}}</ref>\n<ref>{{cite book |last1=Marden |first1=Morris |title=The geometry of the zeros of a polynomial in a complex variable. |date=1949 |publisher=Mathematical Surveys. No. 3. New York: American Mathematical Society (AMS). |page=148 }}</ref> \nIt is based on two auxiliary polynomials, introduced by Schur.<ref>{{cite journal |last1=Schur |first1=I |title=Über Potenzreihen, die im Innern des Einheitskreises beschränkt sind. |journal=Journal für die reine und angewandte Mathematik |date=1917 |volume=1917 |issue=147 |pages=205–232 |doi=10.1515/crll.1917.147.205 |url=http://gdz.sub.uni-goettingen.de/dms/load/img/?PID=GDZPPN00216860X}}</ref>\nFor a complex polynomial <math>p</math> of [[degree of a polynomial|degree]] <math>n</math> its ''reciprocal adjoint polynomial'' <math>p^{*}</math> is defined by <math>p^{*}(z) = z^{n}\\overline{p(\\bar{z}^{-1})} </math> and its ''Schurtransform'' <math>Tp</math> by\n\n:<math>\nTp =\\overline{p(0)}p-\\overline{p^*(0)}p^*,\n</math>\nwhere a bar denotes [[complex conjugate|complex conjugation]].\n\nSo, if <math> p(z) = a_{n}z^{n}+\\cdots+a_{1}z +a_{0} </math> with <math>a_n\\neq 0</math>, then\n<math> p^{*}(z) = \\bar{a}_{0}z^{n}+  \\bar{a}_{1}z^{n-1}+\\cdots+\\bar{a}_{n}</math>,\nwith [[polynomial|leading zero-terms]], if any, removed. The [[polynomial|coefficients]] of <math>Tp</math> can therefore be directly expressed in those of <math>p</math> and, since one or more leading coefficients cancel, <math>Tp</math> has lower degree than <math>p</math>. The roots of <math>p</math>, <math>p^{*}</math>, and <math>Tp</math> are related as follows.\n\n;Lemma\nLet <math>p</math> be a complex polynomial and <math>\\delta = (Tp)(0)</math>. \n*The roots of <math>p^{*}</math>, including their [[Multiplicity (mathematics)|multiplicities]], are the images under [[Circle inversion|inversion]] in the unit circle of the non-zero roots of <math>p</math>.\n* If <math>\\delta \\neq 0</math>, then <math>p,\\,p^*</math>, and <math>Tp</math> share roots on the unit circle, including their multiplicities.\n* If <math>\\delta>0</math>, then <math>p</math> and <math>Tp</math> have the same number of roots inside the  unit circle.\n* If <math>\\delta<0</math>, then <math>p^*</math> and <math>Tp</math> have the same number of roots inside the  unit circle.\n\n;Proof\nFor <math>z\\neq 0</math> we have <math>p^*(z)=z^n \\overline{p(z/|z|^2)}</math> and, in particular,\n<math>|p^*(z)| = |p(z)|</math> for <math>|z|=1</math>.\nAlso <math>\\delta \\neq 0</math> implies <math>|p(0)| \\neq |p^*(0)|</math>. From this and the definitions above the first two statements follow.\nThe other two statements are a consequence of [[Rouché's theorem]]  applied on the unit circle to the functions <math>\\overline{p(0)}p(z)/r(z)</math> and  <math>-\\overline{p^*(0)}p^*(z)/r(z)</math> , where <math>r</math> is a polynomial that has as its roots the roots of <math>p</math> on the unit circle, with the same multiplicities. □\n\nFor a more accessible representation of the lemma,\nlet <math>n^-_p,n^0_p</math>, and <math>n^+_p</math> denote the number of roots of <math>p</math> inside, on, and outside the unit circle respectively and similarly for <math>Tp</math>.\nMoreover let <math>d</math> be the difference in degree of <math>p</math> and <math>Tp</math>. Then the lemma implies that\n<math> (n^-_p,\\;n^0_p,\\;n^+_p)=(n^-_{Tp},\\;n^0_{Tp},\\;n^+_{Tp}+d)</math> if <math>\\delta>0</math> and\n<math> (n^-_p,\\;n^0_p,\\;n^+_p)=(n^+_{Tp}+d,\\;n^0_{Tp},\\;n^-_{Tp})</math> if <math>\\delta<0</math>\n(note the interchange of <math>^+</math> and <math>^-</math>).\n\nNow consider the sequence of polynomials <math>T^{k}p</math> <math>(k=0,1,\\ldots)</math>, where <math>T^0 p=p</math> and <math>T^{k+1}p=T(T^{k}p)</math>. \nApplication of the foregoing to each pair of consecutive members of this sequence gives the following result.\n\n;Theorem[Schur-Cohn test]\nLet <math>p</math> be a complex polynomial with <math>Tp\\neq0</math> and let <math>K</math> be the smallest number such that <math>T^{K+1}p=0</math>. Moreover let <math>\\delta_k=(T^{k}p)(0)</math> for <math>k=1,\\ldots,K</math> and <math>d_{k}= \\deg T^{k}p</math> for <math>k=0,\\ldots,K</math>.\n* All roots of <math>p</math> lie inside the unit circle if and only if\n<math>\\delta_1<0</math>,  <math>\\delta_k>0</math> for <math>k=2,\\ldots,K</math>, and  <math>d_{K}=0</math>.\n* All roots of <math>p</math> lie outside the unit circle if and only if \n<math>\\delta_k>0</math> for <math>k=1,\\ldots,K</math> and <math>d_{K}=0</math>.\n* If <math>d_{K}=0</math> and if <math>\\delta_k < 0 </math> for <math>k=k_0,k_1 \\ldots k_m</math> (in increasing order) and <math>\\delta_k > 0 </math> otherwise, then <math>p</math> has no roots on the unit circle and the number of roots of <math>p</math> inside the unit circle  is\n:<math>\n\\sum_{i=0}^m (-1)^i d_{k_i-1}\n</math>.\n\nMore generally, the distribution of the roots of a polynomial <math>p</math> with respect to an arbitrary circle in the complex plane, say one with centre <math>c</math> and radius <math>\\rho</math>, can be found by application of the Schur-Cohn test to the 'shifted and scaled' polynomial <math>q</math> defined by <math>q(z)=p(c+\\rho\\,z)</math>.\n\nNot every scaling factor is allowed, however, for the Schur-Cohn test can be applied to the polynomial <math>q</math> only if none of the following equalities occur: <math>T^{k}q(0)=0</math> for some <math>k=1,\\ldots K</math> or <math>T^{K+1}q=0</math> while <math>d_K>0</math>. Now, the coefficients of the polynomials <math>T^{k}q</math> are polynomials in <math>\\rho</math> and the said equalities result in polynomial equations for <math>\\rho</math>, which therefore hold for only finitely many values of <math>\\rho</math>. So a suitable scaling factor can always be found, even arbitrarily close to <math>1</math>.\n\n==Lehmer's method==\n\nLehmers method is as follows.\n<ref>{{cite journal |last1=Lehmer |first1=D.H. |title=A machine method for solving polynomial equations. |journal=J. Assoc. Comput. Mach. |date=1961 |volume=8 |pages=151–162 |doi=10.1145/321062.321064}}</ref>\nFor a given complex polynomial <math>p</math>, with the Schur-Cohn test a circular disk can be found large enough to contain all roots of <math>p</math>. Next this disk can be covered with a set of overlapping smaller disks, one of them placed concentrically and the remaining ones evenly spread over the annulus yet to be covered. From this set, using the test again,  disks containing no root of <math>p</math> can be removed. With each of the remaining disks this procedure of covering and removal can be repeated and so any number of times, resulting in a set of arbitrarily small disks that together contain all roots of <math>p</math>.\n\nThe merits of the method are that it consists of repetition of a single procedure and that all roots are found simultaneously, whether they are real or complex, single, multiple or clustered. Also deflation, i.e. removal of roots already found, is not needed and every test starts with the full-precision, original polynomial. And, remarkably, this polynomial has never to be evaluated.\n\nHowever, the smaller the disks become, the more the coefficients of the corresponding 'scaled' polynomials will differ in relative magnitude. This may cause overflow or underflow of computer computations, thus limiting the radii of the disks from below and thereby the precision of the computed roots.\n<ref name=\"Henrici\" />\n.<ref>{{cite journal |last1=Stewart |first1=G.W.III |title=On Lehmer's method for finding the zeros of a polynomial. |journal=Math. Comput. |date=1969 |volume=23 |pages=829–835 |doi=10.2307/2004970}}</ref>\nTo avoid extreme scaling, or just for the sake of efficiency, one may start with testing a number of concentric disks for the number of included roots and thus reduce the region  where roots occur to a number of narrow , concentric annuli. Repeating this procedure with another centre and combining the results, the said region becomes the union of intersections of such annuli.\n<ref>{{cite journal |last1=Loewenthal |first1=Dan |title=Improvement on the Lehmer-Schur root detection method. |journal=J. Comput. Phys. |date=1993 |volume=109 |issue=2 |pages=164–168 |doi=10.1006/jcph.1993.1209}}</ref>\nFinally, when a small disk is found that contains a single root, that root may be further approximated using other methods, e.g. [[Newton's method]].\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Lehmer-Schur Algorithm}}\n[[Category:Root-finding algorithms]]"
    },
    {
      "title": "Muller's method",
      "url": "https://en.wikipedia.org/wiki/Muller%27s_method",
      "text": "'''Muller's method''' is a [[root-finding algorithm]], a [[numerical analysis|numerical]] method for solving equations of the form ''f''(''x'') = 0. It was first presented by [[David E. Muller]] in 1956.\n\nMuller's method is based on the [[secant method]], which constructs at every iteration a line through two points on the graph of ''f''. Instead, Muller's method uses three points, constructs the [[parabola]] through these three points, and takes the intersection of the [[x-axis|''x''-axis]] with the parabola to be the next approximation.\n\n==Recurrence relation==\n\nMuller's method is a recursive method which generates an approximation of the [[Zero of a function|root]] ξ of ''f'' at each iteration. Starting with  the three initial values ''x''<sub>0</sub>, ''x''<sub>−1</sub> and ''x''<sub>−2</sub>, the first iteration calculates the first approximation ''x''<sub>1</sub>, the second iteration calculates the second approximation ''x''<sub>2</sub>, the third iteration calculates the third approximation ''x''<sub>3</sub>, etc. Hence the ''k''<sup>''th''</sup> iteration generates approximation ''x''<sub>''k''</sub>. Each iteration takes as input the last three generated approximations and the value of ''f'' at these approximations. Hence the ''k''<sup>''th''</sup> iteration takes as input the values ''x''<sub>''k''-1</sub>, ''x''<sub>''k''-2</sub> and ''x''<sub>''k''-3</sub> and the function values ''f''(''x''<sub>''k''-1</sub>), ''f''(''x''<sub>''k''-2</sub>) and ''f''(''x''<sub>''k''-3</sub>). The approximation ''x''<sub>''k''</sub> is calculated as follows.\n\nA parabola ''y''<sub>''k''</sub>(''x'') is constructed which goes through the three points (''x''<sub>''k''-1</sub>,&nbsp;''f''(''x''<sub>''k''-1</sub>)), (''x''<sub>''k''-2</sub>,&nbsp;''f''(''x''<sub>''k''-2</sub>)) and (''x''<sub>''k''-3</sub>,&nbsp;''f''(''x''<sub>''k''-3</sub>)). When written in the [[Newton polynomial|Newton form]], ''y''<sub>''k''</sub>(''x'') is\n:<math> y_k(x) = f(x_{k-1}) + (x-x_{k-1}) f[x_{k-1}, x_{k-2}] + (x-x_{k-1}) (x-x_{k-2}) f[x_{k-1}, x_{k-2}, x_{k-3}], \\, </math>\nwhere ''f''[''x''<sub>''k''-1</sub>, ''x''<sub>''k''-2</sub>] and ''f''[''x''<sub>''k''-1</sub>, ''x''<sub>''k''-2</sub>, ''x''<sub>''k''-3</sub>] denote [[divided differences]]. This can be rewritten as\n:<math> y_k(x) = f(x_{k-1}) + w(x-x_{k-1}) + f[x_{k-1}, x_{k-2}, x_{k-3}] \\, (x-x_{k-1})^2 \\, </math>\nwhere\n:<math> w = f[x_{k-1},x_{k-2}] + f[x_{k-1},x_{k-3}] - f[x_{k-2},x_{k-3}]. \\, </math>\nThe next iterate ''x''<sub>''k''</sub> is now given as the solution closest to ''x''<sub>''k''-1</sub> of the quadratic equation ''y''<sub>''k''</sub>(''x'') = 0. This yields the [[recurrence relation]]\n:<math> x_{k} = x_{k-1} - \\frac{2f(x_{k-1})}{w \\pm \\sqrt{w^2 - 4f(x_{k-1})f[x_{k-1}, x_{k-2}, x_{k-3}]}}. </math>\nIn this formula, the sign should be chosen such that the denominator is as large as possible in magnitude. We do not use the standard formula for solving [[quadratic equation]]s because that may lead to [[loss of significance]].\n\nNote that ''x''<sub>''k''</sub> can be complex, even if the previous iterates were all real. This is in contrast with other root-finding algorithms like the [[secant method]], [[Sidi's generalized secant method]] or [[Newton's method]], whose iterates will remain real if one starts with real numbers. Having complex iterates can be an advantage (if one is looking for complex roots) or a disadvantage (if it is known that all roots are real), depending on the problem.\n\n==Speed of convergence==\n\nThe [[rate of convergence|order of convergence]] of Muller's method is approximately 1.84. This can be compared with 1.62 for the [[secant method]] and 2 for [[Newton's method]]. So, the secant method makes less progress per iteration than Muller's method and Newton's method makes more progress.\n\nMore precisely, if ξ denotes a single root of ''f'' (so ''f''(ξ) = 0 and ''f''<nowiki>'</nowiki>(ξ) ≠ 0), ''f'' is three times continuously differentiable, and the initial guesses ''x''<sub>0</sub>, ''x''<sub>1</sub>, and ''x''<sub>2</sub> are taken sufficiently close to ξ, then the iterates satisfy\n:<math> \\lim_{k\\to\\infty} \\frac{|x_k-\\xi|}{|x_{k-1}-\\xi|^\\mu} = \\left| \\frac{f'''(\\xi)}{6f'(\\xi)} \\right|^{(\\mu-1)/2}, </math>\nwhere μ ≈ 1.84 is the positive solution of <math> x^3 - x^2 - x - 1 = 0 </math>.\n\n==Generalizations and related methods==\n\nMuller's method fits a parabola, i.e. a second-order [[polynomial]], to the last three obtained points ''f''(''x''<sub>''k''-1</sub>), ''f''(''x''<sub>''k''-2</sub>) and ''f''(''x''<sub>''k''-3</sub>) in each iteration. One can generalize this and fit a polynomial ''p''<sub>''k,m''</sub>(''x'') of [[Degree of a polynomial|degree]] ''m'' to the last ''m''+1 points in the ''k''<sup>''th''</sup> iteration. Our parabola ''y''<sub>''k''</sub> is written as ''p''<sub>''k'',2</sub> in this notation. The degree ''m'' must be 1 or larger. The next approximation ''x''<sub>''k''</sub> is now one of the roots of the ''p''<sub>''k,m''</sub>, i.e. one of the solutions of ''p''<sub>''k,m''</sub>(''x'')=0. Taking ''m''=1 we obtain the secant method whereas ''m''=2 gives Muller's method.\n\nMuller calculated that the sequence {''x''<sub>''k''</sub>} generated this way converges to the root ξ with an order μ<sub>''m''</sub> where μ<sub>''m''</sub> is the positive solution of <math> x^{m+1} - x^m - x^{m-1} - \\dots - x - 1 = 0 </math>.\n\nThe method is much more difficult though for ''m''>2 than it is for ''m''=1 or ''m''=2 because it is much harder to determine the roots of a polynomial of degree 3 or higher. Another problem is that there seems no prescription of which of the roots of ''p''<sub>''k,m''</sub> to pick as the next approximation ''x''<sub>''k''</sub> for ''m''>2.\n\nThese difficulties are overcome by [[Sidi's generalized secant method]] which also employs the polynomial ''p''<sub>''k,m''</sub>. Instead of trying to solve ''p''<sub>''k,m''</sub>(''x'')=0, the next approximation ''x''<sub>''k''</sub> is calculated with the aid of the derivative of ''p''<sub>''k,m''</sub> at ''x''<sub>''k''-1</sub> in this method.\n\n==References==\n\n* Muller, David E., \"A Method for Solving Algebraic Equations Using an Automatic Computer,\" ''Mathematical Tables and Other Aids to Computation'', 10 (1956), 208-215. {{JSTOR|2001916}}\n* Atkinson, Kendall E. (1989). ''An Introduction to Numerical Analysis'', 2nd edition, Section 2.4. John Wiley & Sons, New York. {{ISBN|0-471-50023-2}}.\n* Burden, R. L. and Faires, J. D. ''Numerical Analysis'', 4th edition, pages 77ff.\n*{{Cite book | last1=Press | first1=WH | last2=Teukolsky | first2=SA | last3=Vetterling | first3=WT | last4=Flannery | first4=BP | year=2007 | title=Numerical Recipes: The Art of Scientific Computing | edition=3rd | publisher=Cambridge University Press |  publication-place=New York | isbn=978-0-521-88068-8 | chapter=Section 9.5.2. Muller's Method | chapter-url=http://apps.nrbook.com/empanel/index.html#pg=466}}\n\n{{DEFAULTSORT:Mullers method}}\n[[Category:Root-finding algorithms]]"
    },
    {
      "title": "Nth root algorithm",
      "url": "https://en.wikipedia.org/wiki/Nth_root_algorithm",
      "text": "{{DISPLAYTITLE:''n''th root algorithm}}\n\nThe [[Principal branch|principal]] [[nth root|''n''th root]] <math>\\sqrt[n]{A}</math> of a [[negative and positive numbers|positive]] [[real number]] ''A'', is the positive real solution of the equation <math>x^n = A</math>.\n\n(For a positive integer ''n'' there are ''n'' distinct [[complex number|complex]] solutions to this equation if <math>A > 0</math>, but only one is positive and real).\n\nThere is a very fast-[[Limit of a sequence|converging]] ''' ''n''th root algorithm''' for finding <math>\\sqrt[n]{A}</math>:\n#Make an initial guess <math>x_0</math>\n#Set <math>x_{k+1} = \\frac{1}{n} \\left[{(n-1)x_k +\\frac{A}{x_k^{n-1}}}\\right]</math>. In practice we do <math>\\Delta x_k = \\frac{1}{n} \\left[{\\frac{A}{x_k^{n-1}}} - x_k\\right]; x_{k+1} = x_{k} + \\Delta x_k </math>.\n#Repeat step 2 until the desired precision is reached, i.e. <math> | \\Delta x_k | < \\epsilon</math> .\n\nA special case is the familiar [[Methods of computing square roots#Babylonian method|square-root algorithm]]. By setting ''n'' = 2, the ''iteration rule'' in step 2 becomes the square root iteration rule:\n:<math>x_{k+1} = \\frac{1}{2}\\left(x_k + \\frac{A}{x_k}\\right)</math>\n\nSeveral different derivations of this algorithm are possible. One derivation shows it is a special case of [[Newton's method]] (also called the Newton-Raphson method) for finding zeros of a function <math>f(x)</math> beginning with an initial guess. Although Newton's method is iterative, meaning it approaches the solution through a series of increasingly accurate guesses, it converges very quickly. The rate of convergence is quadratic, meaning roughly that the number of bits of accuracy doubles on each iteration (so improving a guess from 1 bit to 64 bits of precision requires only 6 iterations). For this reason, this algorithm is often used in computers as a very fast method to calculate square roots.\n\nFor large ''n'', the ''n''<sup>th</sup> root algorithm is somewhat less efficient since it requires the computation of <math>x_k^{n-1}</math> at each step, but can be efficiently implemented with a good [[exponentiation]] algorithm.\n\n== Derivation from Newton's method ==\n\n[[Newton's method]] is a method for finding a zero of a function ''f(x)''. The general iteration scheme is:\n\n#Make an initial guess <math>x_0</math>\n#Set <math>x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}</math>\n#Repeat step 2 until the desired precision is reached.\n\nThe ''n''<sup>th</sup> root problem can be viewed as searching for a zero of the function\n\n:<math>f(x) = x^n - A</math>\n\nSo the derivative is\n\n:<math>f^\\prime(x) = n x^{n-1}</math>\n\nand the iteration rule is\n\n:<math>x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}</math>\n:<math> = x_k - \\frac{x_k^n - A}{n x_k^{n-1}}</math>\n:<math> = x_k - \\frac{x_k}{n}+\\frac{A}{n x_k^{n-1}}</math>\n:<math> = \\frac{1}{n} \\left[{(n-1)x_k +\\frac{A}{x_k^{n-1}}}\\right]</math>\n\nleading to the general ''n''<sup>th</sup> root algorithm.\n\n==See also==\n\n*[[Recurrence relation]]\n*[[Shifting nth root algorithm|Shifting ''n''th root algorithm]]\n\n==References==\n*{{Citation |first=Kendall E. |last=Atkinson |title=An introduction to numerical analysis |location=New York |publisher=Wiley |year=1989 |edition=2nd |isbn=0-471-62489-6 }}.\n\n[[Category:Root-finding algorithms]]"
    },
    {
      "title": "Ridders' method",
      "url": "https://en.wikipedia.org/wiki/Ridders%27_method",
      "text": "In [[numerical analysis]], '''Ridders' method''' is a [[root-finding algorithm]] based on the [[false position method]] and the use of an [[exponential function]] to successively approximate a root of a continuous function<math>f(x)</math> . The method is due to C. Ridders.<ref>{{Cite journal | last1 = Ridders | first1 = C. | doi = 10.1109/TCS.1979.1084580 | title = A new algorithm for computing a single root of a real continuous function | journal = IEEE Transactions on Circuits and Systems | volume = 26 | pages = 979–980| year = 1979 | pmid =  | pmc = }}</ref><ref>{{cite book|title=Numerical Methods in Engineering with Python|first=Jaan |last=Kiusalaas| publisher=Cambridge University Press| year=2010| isbn=978-0-521-19132-6 | edition=2nd| pages=146–150| url=https://books.google.com/books?id=9SG1r8EJawIC&pg=PT156}}</ref>\n\nRidders' method is simpler than [[Muller's method]] or [[Brent's method]] but with similar performance.<ref>{{cite book | last1=Press | first1=WH | last2=Teukolsky | first2=SA | last3=Vetterling | first3=WT | last4=Flannery | first4=BP | year=2007 | title=[[Numerical Recipes]]: The Art of Scientific Computing | edition=3rd | publisher=Cambridge University Press |  publication-place=New York | isbn=978-0-521-88068-8 | chapter=Section 9.2.1. Ridders' Method | chapter-url=http://apps.nrbook.com/empanel/index.html#pg=452}}</ref>  The formula below converges quadratically when the function is well-behaved, which implies that the number of additional significant digits found at each step approximately doubles; but the function has to be evaluated twice for each step, so the overall [[order of convergence]] of the method is <math>\\sqrt{2}</math> . If the function is not well-behaved, the root remains bracketed and the length of the bracketing interval at least halves on each iteration, so convergence is guaranteed. \n\n==Method==\nGiven two values of the independent variable, <math>x_0</math> and <math>x_2</math>, which are on two different sides of the root being sought, i.e.,<math>f(x_0)f(x_2) < 0</math>, the method begins by evaluating the function at the midpoint  <math>x_1 = (x_0 +x_2)/2 </math>.  One then finds the unique exponential function <math>e^{ax}</math> such that function <math>h(x)=f(x)e^{ax}</math> satisfies <math>h(x_1)=(h(x_0) +h(x_2))/2 </math>.  Specifically, parameter <math>a</math> is determined by\n:<math>e^{a(x_1 - x_0)} = \\frac{f(x_1)-\\operatorname{sign}[f(x_0)]\\sqrt{f(x_1)^2 - f(x_0)f(x_2)}}{f(x_2)} .</math>\n\nThe false position method is then applied to the points <math>(x_0,h(x_0))</math> and <math>(x_2,h(x_2))</math>, leading to a new value <math>x_3  </math> between <math>x_0 </math> and <math>x_2 </math>, \n:<math>x_3 = x_1 + (x_1 - x_0)\\frac{\\operatorname{sign}[f(x_0)]f(x_1)}{\\sqrt{f(x_1)^2 - f(x_0)f(x_2)}},</math>\nwhich will be used as one of the two bracketing values in the next step of the iteration. \n\nThe other bracketing value is taken to be <math>x_1 </math> if <math>f(x_1)f(x_3) <0</math> (well-behaved case), or otherwise whichever of <math>x_0 </math> and <math>x_2 </math> has function value of opposite sign to <math>f(x_3)</math>.  The procedure can be terminated when a given accuracy is obtained. \n\n\n\n==References==\n{{reflist}}\n\n[[Category:Root-finding algorithms]]\n\n\n{{mathapplied-stub}}"
    },
    {
      "title": "Root of a function",
      "url": "https://en.wikipedia.org/wiki/Root_of_a_function",
      "text": "#REDIRECT [[Zero of a function]]\n\n{{R from move}}\n{{R from synonym}}\n\n[[Category:Root-finding algorithms]]"
    },
    {
      "title": "Secant method",
      "url": "https://en.wikipedia.org/wiki/Secant_method",
      "text": "{{Use American English|date = March 2019}}\n{{Short description|Root-finding method}}\n[[Image:Secant method.svg|thumb|300px|The first two iterations of the secant method. The red curve shows the function ''f'', and the blue lines are the secants. For this particular case, the secant method will not converge to the visible root.]]\n\nIn [[numerical analysis]], the '''secant method''' is a [[root-finding algorithm]] that uses a succession of [[Root of a function|roots]] of [[secant line]]s to better approximate a root of a [[Function (mathematics)|function]] ''f''.  The secant method can be thought of as a [[finite-difference]] approximation of [[Newton's method]]. However, the method was developed independently of Newton's method and predates it by over 3000 years.<ref>{{Citation |last=Papakonstantinou |first=J. |title=The Historical Development of the Secant Method in 1-D |url=http://citation.allacademic.com/meta/p_mla_apa_research_citation/2/0/0/0/4/p200044_index.html | accessdate = 2011-06-29}}</ref>\n\n==The method==\nThe secant method is defined by the [[recurrence relation]]\n:<math>\nx_n\n = x_{n-1} - f(x_{n-1}) \\frac{x_{n-1} - x_{n-2}}{f(x_{n-1}) - f(x_{n-2})}\n = \\frac{x_{n-2} f(x_{n-1}) - x_{n-1} f(x_{n-2})}{f(x_{n-1}) - f(x_{n-2})}.\n</math>\n\nAs can be seen from the recurrence relation, the secant method requires two initial values, ''x''<sub>0</sub> and ''x''<sub>1</sub>, which should ideally be chosen to lie close to the root.\n\n==Derivation of the method==\nStarting with initial values {{math|''x''<sub>0</sub>}} and {{math|''x''<sub>1</sub>}}, we construct a line through the points {{math|(''x''<sub>0</sub>, ''f''(''x''<sub>0</sub>))}} and {{math|(''x''<sub>1</sub>, ''f''(''x''<sub>1</sub>))}}, as shown in the picture above. In slope–intercept form, the equation of this line is\n\n:<math>y = \\frac{f(x_1) - f(x_0)}{x_1 - x_0}(x - x_1) + f(x_1).</math>\n    \nThe root of this linear function, that is the value of {{mvar|x}} such that {{math|''y'' {{=}} 0}} is\n\n:<math>x = x_1 - f(x_1) \\frac{x_1 - x_0}{f(x_1) - f(x_0)}.</math>\n\nWe then use this new value of {{mvar|x}} as {{math|''x''<sub>2</sub>}} and repeat the process, using {{math|''x''<sub>1</sub>}} and {{math|''x''<sub>2</sub>}} instead of {{math|''x''<sub>0</sub>}} and {{math|''x''<sub>1</sub>}}. We continue this process, solving for {{math|''x''<sub>3</sub>}}, {{math|''x''<sub>4</sub>}}, etc., until we reach a sufficiently high level of precision (a sufficiently small difference between {{math|''x''<sub>''n''</sub>}} and {{math|''x''<sub>''n''−1</sub>}}):\n\n:<math>\n\\begin{align}\nx_2 & = x_1 - f(x_1) \\frac{x_1 - x_0}{f(x_1) - f(x_0)}, \\\\[6pt]\nx_3 & = x_2 - f(x_2) \\frac{x_2 - x_1}{f(x_2) - f(x_1)}, \\\\[6pt]\n& \\,\\,\\,\\vdots \\\\[6pt]\nx_n & = x_{n-1} - f(x_{n-1}) \\frac{x_{n-1} - x_{n-2}}{f(x_{n-1}) - f(x_{n-2})}.\n\\end{align}\n</math>\n\n==Convergence==\n\nThe iterates <math>x_n</math> of the secant method converge to a root of <math>f</math>, if the initial values <math>x_0</math> and <math>x_1</math> are sufficiently close to the root. The [[order of convergence]] is ''φ'', where\n:<math>\\varphi = \\frac{1+\\sqrt{5}}{2} \\approx 1.618</math>\nis the [[golden ratio]]. In particular, the convergence is superlinear, but not quite [[quadratic convergence|quadratic]].\n\nThis result only holds under some technical conditions, namely that <math>f</math> be twice continuously differentiable and the root in question be simple (i.e., with multiplicity 1).\n\nIf the initial values are not close enough to the root, then there is no guarantee that the secant method converges. There is no general definition of \"close enough\", but the criterion has to do with how \"wiggly\" the function is on the interval <math>[x_0, x_1]</math>. For example, if <math>f</math> is differentiable on that interval and there is a point where <math>f' = 0</math> on the interval, then the algorithm may not converge.\n\n==Comparison with other root-finding methods==\n\nThe secant method does not require that the root remain bracketed, like the [[bisection method]] does, and hence it does not always converge. The [[false position method]] (or {{lang|la|regula falsi}}) uses the same formula as the secant method. However, it does not apply the formula on <math>x_{n-1}</math> and <math>x_{n-2}</math>, like the secant method, but on <math>x_{n-1}</math> and on the last iterate <math>x_k</math> such that <math>f(x_k)</math> and <math>f(x_{n-1})</math> have a different sign. This means that the [[false position method]] always converges.\n\nThe recurrence formula of the secant method can be derived from the formula for [[Newton's method]]\n:<math>x_n = x_{n-1} - \\frac{f(x_{n-1})}{f'(x_{n-1})}</math>\nby using the [[finite-difference]] approximation\n:<math>f'(x_{n-1}) \\approx \\frac{f(x_{n-1}) - f(x_{n-2})}{x_{n-1} - x_{n-2}}.</math>\nThe secant method can be interpreted as a method in which the derivative is replaced by an approximation and is thus a [[quasi-Newton method]].\n\nIf we compare Newton's method with the secant method, we see that Newton's method converges faster (order 2 against ''φ''&nbsp;≈&nbsp;1.6). However, Newton's method requires the evaluation of both <math>f</math> and its derivative <math>f'</math> at every step, while the secant method only requires the evaluation of <math>f</math>. Therefore, the secant method may occasionally be faster in practice. For instance, if we assume that evaluating <math>f</math> takes as much time as evaluating its derivative and we neglect all other costs, we can do two steps of the secant method (decreasing the logarithm of the error by a factor ''φ''<sup>2</sup>&nbsp;≈&nbsp;2.6) for the same cost as one step of Newton's method (decreasing the logarithm of the error by a factor&nbsp;2), so the secant method is faster. If, however, we consider parallel processing for the evaluation of the derivative, Newton's method proves its worth, being faster in time, though still spending more steps.\n\n==Generalizations==\n\n[[Broyden's method]] is a generalization of the secant method to more than one dimension.\n\nThe following graph shows the function ''f'' in red and the last secant line in bold blue.  In the graph, the ''x'' intercept of the secant line seems to be a good approximation of the root of ''f''.\n\n[[Image:Secant method example code result.svg|center]]\n\n==A computational example==\nThe secant method is applied to find a root of the function {{math|''f''(''x'') {{=}} ''x''<sup>2</sup> − 612}}. Here is an implementation in the [[MATLAB]] language (from calculation, we expect that the iteration converges at {{math|''x'' {{=}} 24.7386}}):\n\n<source lang=Matlab>\nf=@(x) x^2 - 612;\nx(1)=10;\nx(2)=30;\nfor i=3:7\n    x(i) = x(i-1) - (f(x(i-1)))*((x(i-1) - x(i-2))/(f(x(i-1)) - f(x(i-2))));\nend\nroot=x(7)\n</source>\n\n==Notes==\n{{reflist}}\n\n== See also ==\n* [[False position method]]\n\n==References==\n* {{citation | last1=Kaw | first1=Autar | last2=Kalu | first2=Egwu | year=2008 | title=Numerical Methods with Applications | edition=1st | url=http://www.autarkaw.com/books/numericalmethods/index.html }}.\n* {{cite book| title=Numerical analysis for applied science|first1=Myron B. |last1=Allen |first2=Eli L. |last2=Isaacson | pages=188–195| isbn=978-0-471-55266-6| year=1998| publisher=[[John Wiley & Sons]]| url=https://books.google.com/books?id=PpB9cjOxQAQC}}\n\n==External links==\n* [http://numericalmethods.eng.usf.edu/topics/secant_method.html Secant Method] Notes, PPT, Mathcad, Maple, Mathematica, Matlab at [http://numericalmethods.eng.usf.edu Holistic Numerical Methods Institute]\n* {{MathWorld|urlname=SecantMethod|title=Secant Method}}\n\n{{Root-finding algorithms}}\n\n[[Category:Root-finding algorithms]]"
    },
    {
      "title": "Sidi's generalized secant method",
      "url": "https://en.wikipedia.org/wiki/Sidi%27s_generalized_secant_method",
      "text": "'''Sidi's generalized secant method''' is a [[root-finding algorithm]], that is, a [[numerical method]] for solving [[equations]] of the form <math>f(x)=0</math> . The method was published\nby Avram Sidi.<ref>\nSidi, Avram, \"Generalization Of The Secant Method For Nonlinear Equations\", Applied Mathematics E-notes '''8''' (2008), 115–123, http://www.math.nthu.edu.tw/~amen/2008/070227-1.pdf\n</ref>\n\nThe method is a generalization of the [[secant method]]. Like the secant method, it is an [[iterative method]] which requires one evaluation of <math>f</math> in each iteration and no [[derivative]]s of <math>f</math>. The method can converge much faster though, with an [[Rate of convergence|order]] which approaches 2 provided that <math>f</math> satisfies the regularity conditions described below.\n\n== Algorithm ==\n\nWe call <math>\\alpha</math> the root of <math>f</math>, that is, <math>f(\\alpha)=0</math>. Sidi's method is an iterative method which generates a [[sequence]] <math>\\{ x_i \\}</math> of approximations of <math>\\alpha</math>. Starting with ''k'' + 1 initial approximations <math>x_1 , \\dots , x_{k+1}</math>, the approximation <math>x_{k+2}</math> is calculated in the first iteration, the approximation <math>x_{k+3}</math> is calculated in the second iteration, etc. Each iteration takes as input the last ''k'' + 1 approximations and the value of <math>f</math> at those approximations. Hence the ''n''th iteration takes as input the approximations <math>x_n , \\dots , x_{n+k}</math> and the values <math>f(x_n) , \\dots , f(x_{n+k})</math>.\n\nThe number ''k'' must be 1 or larger: ''k'' = 1, 2, 3, ....  It remains fixed during the execution of the algorithm. In order to obtain the starting approximations <math>x_1 , \\dots , x_{k+1}</math> one could carry out a few initializing iterations with a lower value of ''k''.\n\nThe approximation <math>x_{n+k+1}</math> is calculated as follows in the ''n''th iteration. A [[Polynomial interpolation|polynomial of interpolation]] <math>p_{n,k} (x)</math> of [[Degree of a polynomial|degree]] ''k'' is fitted to the ''k'' + 1 points <math>(x_n, f(x_n)), \\dots (x_{n+k}, f(x_{n+k}))</math>. With this polynomial, the next approximation <math>x_{n+k+1}</math> of <math>\\alpha</math> is calculated as\n\n{{NumBlk|:|<math> x_{n+k+1} = x_{n+k} - \\frac{f(x_{n+k})}{p_{n,k}'(x_{n+k})}</math>|{{EquationRef|1}}}}\n\nwith <math>p_{n,k}'(x_{n+k})</math> the derivative of <math>p_{n,k}</math> at <math>x_{n+k}</math>. Having calculated <math>x_{n+k+1}</math> one calculates <math>f(x_{n+k+1})</math> and the algorithm can continue with the (''n''&nbsp;+&nbsp;1)th iteration. Clearly, this method requires the function <math>f</math> to be evaluated only once per iteration; it requires no derivatives of <math>f</math>.\n\nThe iterative cycle is stopped if an appropriate stop-criterion is met. Typically the criterion is that the last calculated approximation is close enough to the sought-after root <math>\\alpha</math>.\n\nTo execute the algorithm effectively, Sidi's method calculates the interpolating polynomial <math>p_{n,k} (x)</math> in its [[Newton polynomial|Newton form]].\n\n== Convergence ==\nSidi showed that if the function <math>f</math> is (''k''&nbsp;+&nbsp;1)-times [[Smooth function|continuously differentiable]] in an [[open interval]] <math>I</math> containing  <math>\\alpha</math> (that is, <math>f \\in C^{k+1} (I)</math>), <math>\\alpha</math> is a simple root of <math>f</math> (that is, <math>f'(\\alpha) \\neq 0</math>) and the initial approximations <math>x_1 , \\dots , x_{k+1}</math> are chosen close enough to <math>\\alpha</math>, then the sequence <math>\\{ x_i \\}</math> converges to <math>\\alpha</math>, meaning that the following [[Limit of a sequence|limit]] holds: <math>\\lim\\limits_{n \\to \\infty} x_n = \\alpha</math>.\n\nSidi furthermore showed that\n\n:<math> \\lim_{n\\to\\infty} \\frac{x_{n +1}-\\alpha}{\\prod^k_{i=0}(x_{n-i}-\\alpha)} = L = \\frac{(-1)^{k+1}} {(k+1)!}\\frac{f^{(k+1)}(\\alpha)}{f'(\\alpha)}, </math>\n\nand that the sequence [[Rate of convergence|converges]] to <math>\\alpha</math> of order <math>\\psi_k</math>, i.e.\n\n:<math> \\lim\\limits_{n \\to \\infty} \\frac{|x_{n+1}-\\alpha|}{|x_n-\\alpha|^{\\psi_k}} = |L|^{(\\psi_k-1)/k} </math>\n\nThe order of convergence <math>\\psi_k</math> is the [[Descartes's rule of signs|only positive root]] of the polynomial\n\n:<math> s^{k+1} - s^k - s^{k-1} - \\dots - s - 1 </math>\n\nWe have e.g. <math>\\psi_1 = (1+\\sqrt{5})/2</math> ≈ 1.6180, <math>\\psi_2</math> ≈ 1.8393 and <math>\\psi_3</math> ≈ 1.9276. The order approaches 2 from below if ''k'' becomes large: <math> \\lim\\limits_{k \\to \\infty} \\psi_k = 2</math>\n<ref name=\"traub\">\nTraub, J.F., \"Iterative Methods for the Solution of Equations\", Prentice Hall, Englewood Cliffs, N.J. (1964) \n</ref>\n<ref name=\"muller\">\nMuller, David E., \"A Method for Solving Algebraic Equations Using an Automatic Computer\", Mathematical Tables and Other Aids to Computation '''10''' (1956), 208–215 \n</ref>\n\n== Related algorithms ==\nSidi's method reduces to the secant method if we take ''k'' = 1. In this case the polynomial <math>p_{n,1} (x)</math> is the linear approximation of <math>f</math> around <math>\\alpha</math> which is used in the ''n''th iteration of the secant method.\n\nWe can expect that the larger we choose ''k'', the better <math>p_{n,k} (x)</math> is an approximation of <math>f(x)</math> around <math>x=\\alpha</math>. Also, the better <math>p_{n,k}' (x)</math> is an approximation of <math>f'(x)</math> around <math>x=\\alpha</math>. If we replace <math>p_{n,k}'</math> with <math>f'</math> in ({{EquationNote|1}}) we obtain that the next approximation in each iteration is calculated as\n\n{{NumBlk|:|<math> x_{n+k+1} = x_{n+k} - \\frac{f(x_{n+k})}{f'(x_{n+k})} </math>|{{EquationRef|2}}}}\n\nThis is the [[Newton's method|Newton–Raphson method]]. It starts off with a single approximation <math>x_1</math> so we can take ''k'' = 0 in ({{EquationNote|2}}). It does not require an interpolating polynomial but instead one has to evaluate the derivative <math>f'</math> in each iteration. Depending on the nature of <math>f</math> this may not be possible or practical.\n\nOnce the interpolating polynomial <math>p_{n,k} (x)</math> has been calculated, one can also calculate the next approximation <math>x_{n+k+1}</math> as a solution of <math>p_{n,k} (x)=0</math> instead of using ({{EquationNote|1}}). For ''k''&nbsp;=&nbsp;1 these two methods are identical: it is the secant method. For ''k''&nbsp;=&nbsp;2 this method is known as [[Muller's method]].<ref name=\"muller\"/> For ''k''&nbsp;=&nbsp;3 this approach involves finding the roots of a [[cubic function]], which is unattractively complicated. This problem becomes worse for even larger values of&nbsp;''k''. An additional complication is that the equation <math>p_{n,k} (x)=0</math> will in general have [[Properties of polynomial roots|multiple solutions]] and a prescription has to be given which of these solutions is the next approximation <math>x_{n+k+1}</math>. Muller does this for the case ''k''&nbsp;=&nbsp;2 but no such prescriptions appear to exist&nbsp;for ''k''&nbsp;>&nbsp;2.\n\n== References ==\n<references/>\n\n<!-- This will add a notice to the bottom of the page and won't blank it! The new template which says that your draft is waiting for a review will appear at the bottom; simply ignore the old (grey) drafted templates and the old (red) decline templates. A bot will update your article submission. Until then, please don't change anything in this text box and press \"Save page\". -->\n\n[[Category:Root-finding algorithms]]"
    },
    {
      "title": "Solving quadratic equations with continued fractions",
      "url": "https://en.wikipedia.org/wiki/Solving_quadratic_equations_with_continued_fractions",
      "text": "In [[mathematics]], a '''quadratic equation''' is a [[polynomial]] [[equation]] of the second [[Degree of a polynomial|degree]]. The general form is\n\n:<math>ax^2+bx+c=0,</math>\n\nwhere ''a'' ≠ 0.\n\nThe quadratic equation can be solved using the well-known [[Quadratic equation#Quadratic formula and its derivation|quadratic formula]], which can be derived by [[completing the square]]. That formula always gives the roots of the quadratic equation, but the solutions are expressed in a form that often involves a [[quadratic irrational]] number, which is an [[algebraic fraction]] that can be evaluated as a [[decimal|decimal fraction]] only by applying an additional [[methods of computing square roots|root extraction algorithm]].\n\nIf the roots are [[real number|real]], there is an alternative technique that obtains a rational approximation to one of the roots by manipulating the equation directly. The method works in many cases, and long ago it stimulated further development of the [[complex analysis|analytical theory]] of [[generalized continued fraction|continued fractions]].\n\n==A simple example==\nHere is a simple example to illustrate the solution of a quadratic equation using continued fractions. We begin with the equation\n\n:<math>\nx^2 = 2\n</math>\n\nand manipulate it directly. Subtracting one from both sides we obtain\n\n:<math>\nx^2 - 1 = 1.\n</math>\n\nThis is easily factored into\n\n:<math>\n(x+1)(x-1) = 1\n</math>\n\nfrom which we obtain\n\n:<math>\n(x-1) = \\frac{1}{1+x}\n</math>\n\nand finally\n\n:<math>\nx = 1+\\frac{1}{1+x}.\n</math>\n\nNow comes the crucial step. We substitute this expression for ''x'' back into itself, recursively, to obtain\n\n:<math>\nx = 1+\\cfrac{1}{1+\\left(1+\\cfrac{1}{1+x}\\right)} = 1+\\cfrac{1}{2+\\cfrac{1}{1+x}}.\n</math>\n\nBut now we can make the same recursive substitution again, and again, and again, pushing the unknown quantity ''x'' as far down and to the right as we please, and obtaining in the limit the infinite continued fraction\n\n:<math>\nx = 1+\\cfrac{1} {2+\\cfrac{1} {2+\\cfrac{1} {2+\\cfrac{1} {2+\\cfrac{1} {2+\\ddots}}}}} = \\sqrt{2}.\n</math>\n\nBy applying the [[fundamental recurrence formulas]] we may easily compute the successive [[Convergent (continued fraction)|convergents]] of this continued fraction to be 1, 3/2, 7/5, 17/12, 41/29, 99/70, 239/169, ..., where each successive convergent is formed by taking the numerator plus the denominator of the preceding term as the denominator in the next term, then adding in the preceding denominator to form the new numerator. This sequence of denominators is a particular [[Lucas sequence]] known as the [[Pell number]]s.\n\n==An algebraic explanation==\nWe can gain further insight into this simple example by considering the successive powers of\n\n:<math>\n\\omega = \\sqrt{2} - 1.\n</math>\n\nThat sequence of successive powers is given by\n\n:<math>\n\\begin{align}\n\\omega^2& = 3 - 2\\sqrt{2}, & \\omega^3& = 5\\sqrt{2} - 7, & \\omega^4& = 17 - 12\\sqrt{2}, \\\\\n\\omega^5& = 29\\sqrt{2}-41, & \\omega^6& = 99 - 70\\sqrt{2}, & \\omega^7& = 169\\sqrt{2} - 239, \\,\n\\end{align}\n</math>\n\nand so forth. Notice how the fractions derived as successive [[approximant (continued fraction)|approximant]]s to {{radic|2}} appear in this [[geometric progression]].\n\nSince 0 &lt; ''ω'' &lt; 1, the sequence {''ω''<sup>''n''</sup>} clearly tends toward zero, by well-known properties of the positive real numbers. This fact can be used to prove, rigorously, that the convergents discussed in the simple example above do in fact converge to {{radic|2}}, in the limit.\n\nWe can also find these numerators and denominators appearing in the successive powers of\n\n:<math>\n\\omega^{-1} = \\sqrt{2} + 1.\n</math>\n\nThe sequence of successive powers {''ω''<sup>&minus;''n''</sup>} does not approach zero; it grows without limit instead. But it can still be used to obtain the convergents in our simple example.\n\nNotice also that the [[Set (mathematics)|set]] obtained by forming '''all''' the combinations ''a'' + ''b''{{radic|2}}, where ''a'' and ''b'' are integers, is an example of an object known in [[abstract algebra]] as a [[ring (mathematics)|ring]], and more specifically as an [[integral domain]]. The number ω is a [[unit (ring theory)|unit]] in that integral domain.  See also [[algebraic number field]].\n\n==The general quadratic equation==\nContinued fractions are most conveniently applied to solve the general quadratic equation expressed in the form of a [[monic polynomial]]\n\n:<math>\nx^2 + bx + c = 0\n</math>\n\nwhich can always be obtained by dividing the original equation by its leading [[coefficient]]. Starting from this monic equation we see that\n\n:<math>\n\\begin{align}\nx^2 + bx& = -c\\\\\nx + b& = \\frac{-c}{x}\\\\\nx& = -b - \\frac{c}{x}\\,\n\\end{align}\n</math>\n\nBut now we can apply the last equation to itself recursively to obtain\n\n:<math>\nx = -b-\\cfrac{c} {-b-\\cfrac{c} {-b-\\cfrac{c} {-b-\\cfrac{c} {-b-\\ddots\\,}}}}\n</math>\n\nIf this infinite continued fraction [[Convergent (continued fraction)|converges]] at all, it must converge to one of the [[root of a function|roots]] of the monic polynomial ''x''<sup>2</sup> + ''bx'' + ''c'' = 0. Unfortunately, this particular continued fraction does not converge to a finite number in every case. We can easily see that this is so by considering the [[quadratic equation#Quadratic formula|quadratic formula]] and a monic polynomial with real coefficients. If the [[discriminant]] of such a polynomial is negative, then both roots of the quadratic equation have [[imaginary number|imaginary]] parts. In particular, if ''b'' and ''c'' are real numbers and ''b''<sup>2</sup> - 4''c'' &lt; 0, all the convergents of this continued fraction \"solution\" will be real numbers, and they cannot possibly converge to a root of the form ''u'' + ''iv'' (where ''v'' ≠ 0), which does not lie on the [[real number|real number line]].\n\n==A general theorem==\nBy applying a result obtained by [[Leonhard Euler|Euler]] in 1748 it can be shown that the continued fraction solution to the general monic quadratic equation with real coefficients\n\n:<math>\nx^2 + bx + c = 0\n</math>\n\ngiven by\n\n:<math>\nx = -b-\\cfrac{c} {-b-\\cfrac{c} {-b-\\cfrac{c} {-b-\\cfrac{c} {-b-\\ddots\\,}}}}\n</math>\n\n[[Convergent (continued fraction)|converges]] or not depending on both the coefficient ''b'' and the value of the [[discriminant]], ''b''<sup>2</sup> &minus; 4''c''.\n\nIf ''b'' = 0 the general continued fraction solution is totally divergent; the convergents alternate between 0 and <math>\\infty</math>. If ''b'' ≠ 0 we distinguish three cases.\n\n#If the discriminant is negative, the fraction diverges by oscillation, which means that its convergents wander around in a regular or even chaotic fashion, never approaching a finite limit.\n#If the discriminant is zero the fraction converges to the single root of multiplicity two.\n#If the discriminant is positive the equation has two real roots, and the continued fraction converges to the larger (in [[absolute value]]) of these. The rate of convergence depends on the absolute value of the ratio between the two roots: the farther that ratio is from unity, the more quickly the continued fraction converges.\n\nWhen the monic quadratic equation with real coefficients is of the form ''x''<sup>2</sup> = ''c'', the '''general''' solution described above is useless because division by zero is not well defined. As long as ''c'' is positive, though, it is always possible to transform the equation by subtracting a [[square number|perfect square]] from both sides and proceeding along the lines illustrated with {{radic|2}} above. In symbols, if\n\n:<math>\nx^2 = c\\qquad(c>0)\n</math>\n\njust choose some positive real number ''p'' such that\n\n:<math>\np^2 < c.\n</math>\n\nThen by direct manipulation we obtain\n\n:<math>\n\\begin{align}\nx^2-p^2& = c-p^2\\\\\n(x+p)(x-p)& = c-p^2\\\\\nx-p& = \\frac{c-p^2}{p+x}\\\\\nx& = p + \\frac{c-p^2}{p+x}\\\\\n& = p+\\cfrac{c-p^2} {p+\\left(p+\\cfrac{c-p^2} {p+x}\\right)}& = p+\\cfrac{c-p^2} {2p+\\cfrac{c-p^2} {2p+\\cfrac{c-p^2} {2p+\\ddots\\,}}}\\,\n\\end{align}\n</math>\n\nand this transformed continued fraction must converge because all the partial numerators and partial denominators are positive real numbers.\n\n==Complex coefficients==\nBy the [[fundamental theorem of algebra]], if the monic polynomial equation ''x''<sup>2</sup> + ''bx'' + ''c'' = 0 has complex coefficients, it must have two (not necessarily distinct) complex roots. Unfortunately, the discriminant ''b''<sup>2</sup> - 4''c'' is not as useful in this situation, because it may be a complex number. Still, a modified version of the general theorem can be proved.\n\nThe continued fraction solution to the general monic quadratic equation with complex coefficients\n\n:<math>\nx^2 + bx + c = 0\\qquad (b\\ne0)\n</math>\n\ngiven by\n\n:<math>\nx = -b-\\cfrac{c} {-b-\\cfrac{c} {-b-\\cfrac{c} {-b-\\cfrac{c} {-b-\\ddots\\,}}}}\n</math>\n\n[[Convergent (continued fraction)|converges]] or not depending on the value of the discriminant, ''b''<sup>2</sup> &minus; 4''c'', and on the relative magnitude of its two roots.\n\nDenoting the two roots by ''r''<sub>1</sub> and ''r''<sub>2</sub> we distinguish three cases.\n#If the discriminant is zero the fraction converges to the single root of multiplicity two.\n#If the discriminant is not zero, and |''r''<sub>1</sub>| ≠ |''r''<sub>2</sub>|, the continued fraction converges to the ''root of maximum modulus'' (i.e., to the root with the greater [[absolute value]]).\n#If the discriminant is not zero, and |''r''<sub>1</sub>| = |''r''<sub>2</sub>|, the continued fraction diverges by oscillation.\n\nIn case 2, the rate of convergence depends on the absolute value of the ratio between the two roots: the farther that ratio is from unity, the more quickly the continued fraction converges.\n\nThis general solution of monic quadratic equations with complex coefficients is usually not very useful for obtaining rational approximations to the roots, because the criteria are circular (that is, the relative magnitudes of the two roots must be known before we can conclude that the fraction converges, in most cases). But this solution does find useful applications in the further analysis of the [[convergence problem]] for continued fractions with complex elements.\n\n==See also==\n* [[Continued fraction]]\n* [[Generalized continued fraction]]\n* [[Lucas sequence]]\n* [[Pell's equation]]\n* [[Quadratic equation]]\n\n==References==\n{{reflist}}\n*H. S. Wall, ''Analytic Theory of Continued Fractions'', D. Van Nostrand Company, Inc., 1948 {{isbn|0-8284-0207-8}}\n\n{{DEFAULTSORT:Solving Quadratic Equations With Continued Fractions}}\n[[Category:Continued fractions]]\n[[Category:Elementary algebra]]\n[[Category:Equations]]\n[[Category:Mathematical analysis]]\n[[Category:Root-finding algorithms]]"
    },
    {
      "title": "Splitting circle method",
      "url": "https://en.wikipedia.org/wiki/Splitting_circle_method",
      "text": "In [[mathematics]], the '''splitting circle method''' is a [[numerical analysis|numerical algorithm]] for the numerical factorization of a [[polynomial]] and, ultimately, for finding its [[complex number|complex]] [[Root of a function|roots]]. It was introduced by [[Arnold Schönhage]] in his 1982 paper ''The fundamental theorem of algebra in terms of computational complexity'' (Technical report, Mathematisches Institut der Universität Tübingen). A revised algorithm was presented by [[Victor Pan]] in 1998. An implementation was provided by [[Xavier Gourdon]] in 1996 for the [[Magma computer algebra system|Magma]] and [[PARI/GP]] computer algebra systems.\n\n==General description==\n\nThe fundamental idea of the splitting circle method is to use methods of [[complex analysis]], more precisely the [[residue theorem]], to construct factors of polynomials. With those methods it is possible to construct a factor of a given polynomial <math>  p(x)=x^n+p_{n-1}x^{n-1}+\\cdots+p_0  </math> for any region of the complex plane with a piecewise smooth boundary. Most of those factors will be trivial, that is constant polynomials. Only regions that contain roots of ''p(x)'' result in nontrivial factors that have exactly those roots of ''p(x)'' as their own roots, preserving multiplicity.\n\nIn the numerical realization of this method one uses disks ''D''(''c'',''r'') (center ''c'', radius ''r'') in the complex plane as regions. The boundary circle of a disk splits the set of roots of ''p''(''x'') in two parts, hence the name of the method. To a given disk one computes approximate factors following the analytical theory and refines them using [[Newton's method]]. To avoid numerical instability one has to demand that all roots are well separated from the boundary circle of the disk. So to obtain a good splitting circle it should be embedded in a root free annulus ''A''(''c'',''r'',''R'') (center ''c'', inner radius ''r'', outer radius ''R'') with a large relative width ''R/r''.\n\nRepeating this process for the factors found, one finally arrives at an approximative factorization of the polynomial at a required precision. The factors are either linear polynomials representing well isolated zeros or higher order polynomials representing clusters of zeros.\n\n==Details of the analytical construction==\n[[Newton's identities]] are a bijective relation between the [[elementary symmetric polynomial]]s of a tuple of complex numbers and its sums of powers. Therefore, it is possible to compute the coefficients of a polynomial\n\n:<math>p(x)=x^n+p_{n-1}x^{n-1}+\\cdots+p_0=(x-z_1)\\cdots(x-z_n)</math>\n\n(or of a factor of it) from the sums of powers of its zeros\n\n:<math>t_m=z_1^m+\\cdots+z_n^m</math>, <math>m=0,1,\\dots,n</math>\n\nby solving the triangular system that is obtained by comparing the powers of ''u'' in the following identity of [[formal power series]]\n\n: <math>a_{n-1}+2\\,a_{n-2}\\,u+\\cdots+(n-1)\\,a_1\\,u^{n-2}+n\\,a_0\\,u^{n-1}</math>\n\n: <math> = -(1+a_{n-1}\\,u+\\cdots+a_1\\,u^{n-1}+a_0\\,u^n)\\cdot(t_1+t_2\\,u+t_3\\,u^2+\\dots+t_n\\,u^{n-1}+\\cdots).</math>\n\nIf <math>G\\subset\\mathbb C</math> is a domain with piecewise smooth boundary ''C'' and if the zeros of ''p''(''x'') are pairwise distinct and not on the boundary ''C'', then from the [[residue theorem]] of residual calculus one gets\n\n:<math>\n\\frac1{2\\pi\\,i}\\oint_C \\frac{p'(z)}{p(z)}z^m\\,dz\n=\\sum_{z\\in G:\\,p(z)=0}\\frac{p'(z)z^m}{p'(z)}\n=\\sum_{z\\in G:\\,p(z)=0}z^m.\n</math>\n\nThe identity of the left to the right side of this equation also holds for zeros with multiplicities. By using the Newton identities one is able to compute from those sums of powers the factor\n\n:<math>f(x):=\\prod_{z\\in G:\\,p(z)=0}(x-z)</math>\n\nof ''p''(''x'') corresponding to the zeros of ''p''(''x'') inside ''G''. By polynomial division one also obtains the second factor ''g''(''x'') in ''p''(''x'') = ''f''(''x'')''g''(''x'').\n\nThe commonly used regions are circles in the complex plane. Each circle gives raise to a split of the polynomial ''p''(''x'') in factors ''f''(''x'') and ''g''(''x''). Repeating this procedure on the factors using different circles yields finer and finer factorizations. This recursion stops after a finite number of proper splits with all factors being nontrivial powers of linear polynomials.\n\nThe challenge now consists in the conversion of this analytical procedure into a numerical algorithm with good running time. The integration is approximated by a finite sum of a numerical integration method, making use of the [[fast Fourier transform]] for the evaluation of the polynomials ''p''(''x'') and ''p''<nowiki>'</nowiki>(''x''). The polynomial ''f''(''x'') that results will only be an approximate factor. To ensure that its zeros are close to the zeros of ''p'' inside ''G'' and only to those, one must demand that all zeros of ''p'' are far away from the boundary ''C'' of the region ''G''.\n\n==Basic numerical observation==\n\n(Schönhage 1982) Let <math>p\\in\\mathbb C[X]</math> be a polynomial of degree ''n'' which has ''k'' zeros inside the circle of radius ''1/2'' and the remaining ''n-k'' zeros outside the circle of radius ''2''. With ''N=O(k)'' large enough, the approximation of the contour integrals using ''N'' points results in an approximation <math>f_0</math> of the factor ''f'' with error\n:<math>\\|f-f_0\\|\\le 2^{2k-N}\\,nk\\,100/98</math>,\nwhere the norm of a polynomial is the sum of the moduli of its coefficients.\n\nSince the zeros of a polynomial are continuous in its coefficients, one can make the zeros of <math>f_0</math> as close as wanted to the zeros of ''f'' by choosing ''N'' large enough. However, one can improve this approximation faster using a Newton method. Division of ''p'' with remainder yields an approximation <math>g_0</math> of the remaining factor ''g''. Now\n:<math>p-f_0g_0=(f-f_0)g_0+(g-g_0)f_0+(f-f_0)(g-g_0)</math>,\nso discarding the last second order term one has to solve <math>p-f_0g_0=f_0\\Delta g+g_0\\Delta f</math> using any variant of the [[extended Euclidean algorithm]] to obtain the incremented approximations <math>f_1=f_0+\\Delta f</math> and <math>g_1=g_0+\\Delta g</math>. This is repeated until the increments are zero relative to the chosen precision.\n\n==Graeffe iteration==\n\nThe crucial step in this method is to find an annulus of relative width ''4'' in the complex plane that contains no zeros of ''p'' and contains approximately as many zeros of ''p'' inside as outside of it. Any annulus of this characteristic can be transformed, by translation and scaling of the polynomial, into the annulus between the radii 1/2 and 2 around the origin. But, not every polynomial admits such a splitting annulus.\n\nTo remedy this situation, the [[Graeffe's method|Graeffe iteration]] is applied. It computes a sequence of polynomials\n\n:<math>p_0=p,\\qquad p_{j+1}(x)=(-1)^{\\deg p}p_j(\\sqrt x)\\,p_j(-\\sqrt x),</math>\n\nwhere the roots of <math>p_j(x)</math> are the <math>2^j</math>-th dyadic powers of the roots of the initial polynomial ''p''. By splitting <math>p_j(x)=e(x)+x\\,o(x)</math> into even and odd parts, the succeeding polynomial is obtained by purely arithmetic operations as <math>p_{j+1}(x)=(-1)^{\\deg p}(e(x)^2-x\\,o(x)^2)</math>. The ratios of the absolute moduli of the roots increase by the same power <math>2^j</math> and thus tend to infinity. Choosing ''j'' large enough one finally finds a splitting annulus of relative width 4 around the origin.\n\nThe approximate factorization of <math>p_j(x)\\approx f_j(x)\\,g_j(x)</math> is now to be lifted back to the original polynomial. To this end an alternation of Newton steps and [[Padé approximant|Padé approximation]]s is used. It is easy to check that\n:<math>\\frac{p_{j-1}(x)}{g_j(x^2)}\\approx \\frac{f_{j-1}(x)}{g_{j-1}(-x)}</math>\nholds. The polynomials on the left side are known in step ''j'', the polynomials on the right side can be obtained as [[Padé approximant]]s of the corresponding degrees for the power series expansion of the fraction on the left side.\n\n==Finding a good circle==\n\nMaking use of the Graeffe iteration and any known estimate for the absolute value of the largest root one can find estimates ''R'' of this absolute value of any precision. Now one computes estimates for the largest and smallest distances <math>R_j>r_j>0</math> of any root of ''p''(''x'') to any of the five center points 0, 2''R'', −2''R'', 2''Ri'', −2''Ri'' and selects the one with the largest ratio <math>R_j/r_j</math> between the two. By this construction it can be guaranteed that <math>R_j/r_j>e^{0{.}3}\\approx 1.35</math> for at least one center. For such a center there has to be a root-free annulus of relative width <math>\\textstyle e^{0{.}3/n}\\approx 1+\\frac{0{.}3}{n}</math>. After <math>\\textstyle 3+\\log_2(n)</math> Graeffe iterations, the corresponding annulus of the iterated polynomial has a relative width greater than 11 > 4, as required for the initial splitting described above (see Schönhage (1982)). After <math>\\textstyle 4+\\log_2(n)+\\log_2(2+\\log_2(n))</math> Graeffe iterations, the corresponding annulus has a relative width greater than <math>\\textstyle 2^{13{.}8}\\cdot n^{6{.}9}>(64\\cdot n^3)^2</math>, allowing a much simplified initial splitting (see Malajovich/Zubelli (1997))\n\nTo locate the best root-free annulus one uses a consequence of the [[Rouché theorem]]: For ''k'' = 1, ..., ''n''&nbsp;−&nbsp;1 the polynomial equation\n\n:<math>\\,0=\\sum_{j\\ne k}|p_j|u^j-|p_k|u^k,</math>\n\n''u'' > 0, has, by [[Descartes' rule of signs]] zero or two positive roots <math>u_k<v_k</math>. In the latter case, there are exactly ''k'' roots inside the (closed) disk <math>D(0,u_k)</math> and <math>A(0,u_k,v_k)</math> is a root-free (open) annulus.\n\n==References==\n* Schönhage, Arnold (1982): [http://www.informatik.uni-bonn.de/~schoe/fdthmrep.ps.gz ''The fundamental theorem of algebra in terms of computational complexity.''] Preliminary Report, Math. Inst. Univ. Tübingen (1982), 49 pages. (ps.gz)\n* {{cite book|last=Gourdon|first=Xavier|title=Combinatoire, Algorithmique et Geometrie des Polynomes|publisher=Ecole Polytechnique|location= Paris|year=1996|url=http://algo.inria.fr/gourdon/thesis.html}}\n* {{cite journal|author=V. Y. Pan|title=Optimal and nearly optimal algorithms for approximating polynomial zeros|journal=Comput. Math. Appl.|volume=31|year=1996|issue=12|pages=97–138|doi=10.1016/0898-1221(96)00080-6}}\n* {{cite journal|author=V. Y. Pan|title=Solving a polynomial equation: Some history and recent progresses|journal=SIAM Review|volume=39|year=1997|pages=187–220|doi=10.1137/S0036144595288554|issue=2}}\n* {{cite journal|author=Gregorio Malajovich and Jorge P. Zubelli|title=A fast and stable algorithm for splitting polynomials|journal=Computers & Mathematics with Applications|volume=No 3|issue=2|series=33|pages=1–23|year=1997|url=http://www.labma.ufrj.br/~gregorio/papers.php#splittin|doi=10.1016/S0898-1221(96)00233-7}}\n* Pan, Victor (1998). [http://algo.inria.fr/seminars/sem97-98/pan.html ''Algorithm for Approximating Complex Polynomial Zeros'']\n* Pan, Victor (2002). [http://comet.lehman.cuny.edu/vpan/pdf/JSCOptimal.pdf ''Univariate Polynomials: Nearly Optimal Algorithms for Numerical Factorization and Root-finding'']\n* Magma documentation. [http://magma.maths.usyd.edu.au/magma/handbook/text/223#2021 Real and Complex Fields: Element Operations].\n\n[[Category:Root-finding algorithms]]"
    },
    {
      "title": "Steffensen's method",
      "url": "https://en.wikipedia.org/wiki/Steffensen%27s_method",
      "text": "In [[numerical analysis]], '''Steffensen's method''' is a [[root-finding method|root-finding technique]] similar to [[Newton's method]], named after [[Johan Frederik Steffensen]]. '''Steffensen's method''' also achieves [[order of convergence|quadratic convergence]], but without using [[derivative]]s as [[Newton's method]] does.\n\n==Simple description==\nThe simplest form of the formula for Steffensen's method occurs when it is used to find the zeros, or roots, of a function <math>f</math>&nbsp;; that is: to find the value  <math>x_\\star</math>  that satisfies <math>f(x_\\star)=0</math>&nbsp;. Near the solution  <math>x_\\star</math>&nbsp;, the function <math>f</math> is supposed to approximately satisfy  <math>-1 < f'(x_\\star) < 0</math>&nbsp;; this condition makes  <math>f</math>  adequate as a correction-function for <math>x</math>  for finding its ''own'' solution, although it is not required to work efficiently. For some functions, Steffensen's method can work even if this condition is not met, but in such a case, the starting value <math>x_0\\ </math> must be ''very'' close to the actual solution  <math>x_\\star</math>&nbsp;, and convergence to the solution may be slow.\n\nGiven an adequate starting value  <math>x_0\\ </math>&nbsp;, a sequence of values  <math>x_0,\\ x_1,\\ x_2,\\dots,\\ x_n,\\dots</math>  can be generated using the formula below. When it works, each value in the sequence is much closer to the solution  <math>x_\\star</math>  than the prior value. The value <math>x_n\\ </math> from the current step generates the value  <math>x_{n+1}\\ </math> for the next step, via this formula:<ref name=\"Dahlquist\">{{cite book |first1=Germund |last1=Dahlquist |author1-link=Germund Dahlquist |first2=Åke |last2=Björck |translator-first=Ned |translator-last=Anderson |year=1974 |title=Numerical Methods |pages=230–231 |location=Englewood Cliffs, NJ |publisher=Prentice Hall}}</ref>\n\n:<math>x_{n+1} = x_n - \\frac{f(x_n)}{g(x_n)}</math>\n\nfor ''n''&nbsp;=&nbsp;0, 1, 2, 3, ...&nbsp;, where the slope function <math>g(x_n)</math> is a composite of the original function  <math>f</math>  given by the following formula:\n\n:<math>g(x_n) = \\frac{f(x_n + f(x_n))}{f(x_n)} - 1</math>\n\nor equivalently\n\n:<math>g(x_n) = \\frac{f(x_n + h) - f(x_n)}{h}</math>   where   <math>h = f(x_n)</math>.\n\nThe function  <math>g</math>  is the average value for the slope of the function  <math>f</math>  between the last sequence point  <math>(x,y) = ( x_n,\\ f(x_n) )</math>  and the auxiliary point  <math>(x,y)=( x_n + h,\\ f(x_n + h) )</math>&nbsp;, with the step  <math>h=f(x_n)\\ </math>&nbsp;. It is also called the [[divided difference|first-order divided difference]] of  <math>f</math>  between those two points.\n\nIt is only for the purpose of finding  <math>h</math>  for this auxiliary point that the value of the function  <math>f</math>  must be an adequate correction to get closer to its own solution, and for that reason fulfill the requirement that  <math>-1 < f'(x_\\star) < 0 </math>&nbsp;. For all other parts of the calculation, Steffensen's method only requires the function <math>f</math> to be continuous and to actually have a nearby solution. Several modest modifications of the step  <math>h</math>  in the slope calculation  <math>g</math>  exist to accommodate functions <math>f</math> that do not quite meet the requirement.\n\n==Advantages and drawbacks==\nThe main advantage of Steffensen's method is that it has [[quadratic convergence]]<ref name=\"Dahlquist\"/> like [[Newton's method]] &ndash; that is, both methods find roots to an equation <math>f</math> just as ‘quickly’. In this case ''quickly'' means that for both methods, the number of correct digits in the answer doubles with each step. But the formula for Newton's method requires evaluation of the function's derivative <math>f'</math> as well as the function <math>f</math>, while Steffensen's method only requires <math>f</math> itself. This is important when the derivative is not easily or efficiently available.\n\nThe price for the quick convergence is the double function evaluation: Both <math>f(x_n)</math> and <math>f(x_n + h)</math> must be calculated, which might be time-consuming if <math>f</math> is a complicated function. For comparison, the [[secant method]] needs only one function evaluation per step. The secant method increases the number of correct digits by \"only\" a factor of roughly 1.6&nbsp;per step, but one can do twice as many steps of the secant method within a given time. Since the secant method can carry out twice as many steps in the same time as Steffensen's method,<ref group=lower-alpha>Because <math>f(x_n + h)</math> requires the prior calculation of <math>h = f(x_n)</math>, the two evaluations must be done sequentially – the algorithm ''per se'' cannot be made faster by running the function evaluations in parallel. This is yet another disadvantage of Steffensen's method.</ref> when both algorithms succeed, the secant method converges faster than Steffensen's method in actual practice: The secant method achieves a factor of about (1.6)<sup>2</sup>&nbsp;≈&nbsp;2.6&nbsp;times as many digits for every two steps (two function evaluations), compared to Steffensen's factor of 2 for every one step (two function evaluations).\n\nSimilar to most other [[Root-finding algorithm#Iterative methods|iterative root-finding algorithms]], the crucial weakness in Steffensen's method is the choice of the starting value <math>x_0</math>&nbsp;. If the value of <math>x_0</math> is not ‘close enough’ to the actual solution <math>x_\\star</math>&nbsp;, the method may fail and the sequence of values <math>x_0, x_1, x_2, x_3,\\dots</math> may either flip-flop between two extremes, or diverge to infinity (possibly both!).\n\n==Derivation using Aitken's delta-squared process==\nThe version of Steffensen's method implemented in the [[MATLAB]] code shown below can be found using the [[Aitken's delta-squared process]] for accelerating convergence of a sequence. To compare the following formulae to the formulae in the section above, notice that <math>x_n = p\\ -\\ p_n</math>&nbsp;. This method assumes starting with a linearly convergent sequence and increases the rate of convergence of that sequence. If the signs of <math>p_n,\\ p_{n+1},\\ p_{n+2}</math> agree and <math>p_n\\ </math> is ‘sufficiently close’ to the desired limit of the sequence <math>p\\ </math>, we can assume the following:\n\n:<math>\\frac{p_{n+1}-p}{p_n-p}\\approx\\frac{p_{n+2}-p}{p_{n+1}-p}</math>\nthen\n:<math>(p_{n+1}-p)^2\\approx(p_{n+2}-p)(p_n-p)</math>\nso\n:<math>p_{n+1}^2-2p_{n+1}p+p^2\\approx p_{n+2}p_n-(p_n+p_{n+2})p+p^2</math>\nand hence\n:<math>(p_{n+2}-2p_{n+1}+p_n)p\\approx p_{n+2}p_n-p_{n+1}^2</math>&nbsp;.\n<br>\nSolving for the desired limit of the sequence <math>p</math> gives:\n<br>\n:<math>p\\approx \\frac{p_{n+2}p_n-p_{n+1}^2}{p_{n+2}-2p_{n+1}+p_n}</math>\n:<math>=\\frac{p_{n}^2+p_{n}p_{n+2}+2p_{n}p_{n+1}-2p_{n}p_{n+1}-p_{n}^2-p_{n+1}^2}{p_{n+2}-2p_{n+1}+p_n}</math>\n<br>\n:<math>=\\frac{(p_{n}^2+p_{n}p_{n+2}-2p_{n}p_{n+1})-(p_{n}^2-2p_{n}p_{n+1}+p_{n+1}^2)}{p_{n+2}-2p_{n+1}+p_n}</math>\n:<math>=p_n-\\frac{(p_{n+1}-p_n)^2}{p_{n+2}-2p_{n+1}+p_n},</math>\n<br>\nwhich results in the more rapidly convergent sequence:\n\n:<math>p\\approx p_{n+3}=p_n-\\frac{(p_{n+1}-p_n)^2}{p_{n+2}-2p_{n+1}+p_n}.</math>\n\n==Implementation in Matlab==\n\nHere is the source for an implementation of Steffensen's Method in [[MATLAB]].\n\n<source lang=\"matlab\">\nfunction Steffensen(f,p0,tol)\n% This function takes as inputs: a fixed point iteration function, f, \n% and initial guess to the fixed point, p0, and a tolerance, tol.\n% The fixed point iteration function is assumed to be input as an\n% inline function. \n% This function will calculate and return the fixed point, p, \n% that makes the expression f(x) = p true to within the desired \n% tolerance, tol.\n\nformat compact % This shortens the output.\nformat long    % This prints more decimal places.\n\nfor i=1:1000   % get ready to do a large, but finite, number of iterations.\n               % This is so that if the method fails to converge, we won't\n               % be stuck in an infinite loop.\n    p1=f(p0);  % calculate the next two guesses for the fixed point.\n    p2=f(p1);\n    p=p0-(p1-p0)^2/(p2-2*p1+p0) % use Aitken's delta squared method to\n                                % find a better approximation to p0.\n    if abs(p-p0)<tol  % test to see if we are within tolerance.\n        break         % if we are, stop the iterations, we have our answer.\n    end\n    p0=p;              % update p0 for the next iteration.\nend\nif abs(p-p0)>tol       % If we fail to meet the tolerance, we output a\n                       % message of failure.\n    'failed to converge in 1000 iterations.'\nend\n</source>\n\n==Implementation in Python==\n\nHere is the source for an implementation of Steffensen's Method in [[Python (programming language)|Python]].\n\n<source lang=\"python\">\ndef g(f, x):\n    '''\n    First-order divided difference function\n    parameter:\n    f(callable): Function input to g\n    x(float): Point at which to evaluate g\n    '''\n    return lambda x: f(x + f(x))/f(x) - 1\n\ndef steff(f, x):\n    '''\n    Steffenson Algorithm for finding roots\n    This recursive generator yields the x_n+1 value first then, when the generator iterates, it yields x_n+2 from the next level of recursion.\n    \n    parameters:\n    f(callable): Functio whose root we are searching for\n    x(float): Starting value upon first call, each level n that the function recurses x is x_n\n    '''\n\n    if(g(f, x)(x)!=0):\n        yield x - f(x)/g(f, x)(x)#first give x_n+1\n        yield from steff(f, x - f(x)/g(f, x)(x)) #then give new iterator\n</source>\n\n==Generalization==\nSteffensen's method can also be used to find an input <math>x = x_\\star</math> for a different kind of function  <math>F</math>  that produces output the same as its input:  <math>x_\\star = F(x_\\star)</math>  for the special value <math>x_\\star</math>&nbsp;. Solutions like  <math>x_\\star</math>  are called ''[[fixed point (mathematics)|fixed point]]s''. Many such functions can be used to find their own solutions by repeatedly recycling the result back as input, but the rate of convergence can be slow, or the function can fail to converge at all, depending on the individual function. Steffensen's method accelerates this convergence, to make it [[quadratic convergence|quadratic]].\n\nThis method for finding fixed points of a real-valued function has been generalised for functions  <math>F : X \\to X </math>  on a [[Banach space]]  <math>X</math>&nbsp;. The generalised method assumes that a  [[Indexed family|family]] of [[Bounded set|bounded]] [[linear operators]]  <math>\\{L(u,v): u, v \\in X\\}</math>  associated with  <math>u\\ </math>  and  <math>v\\ </math>  can be found to satisfy the condition<ref name=Johnson>{{cite journal |last1=Johnson |first1=L.W. |last2=Scholz |first2=D.R.  |title=On Steffensen's method |journal=SIAM Journal on Numerical Analysis |volume=5 |issue=2 |pages=296–302 |date=June 1968 |jstor=2949443 |doi=10.1137/0705026 }}</ref>\n\n:<math>F(u)- F(v)=L(u,v)\\ (u-v).</math>\n\nIn the simple form given in the section above, the function  <math>f</math>  simply takes in and produces real numbers. There, the function  <math>g</math>  is a ''[[divided difference]]''. In the generalized form here, the operator  <math>L</math>  is the analogue of a divided difference for use in the [[Banach space]]. The operator  <math>L</math>  is equivalent to a [[Matrix (mathematics)|matrix]] whose entries are all functions of [[vector (mathematics)|vector]] [[Argument of a function|arguments]]  <math>u\\ </math> and  <math>v\\ </math>.\n\nSteffensen's method is then very similar to the Newton's method, except that it uses the divided difference<math>L(F(x),x)\\ </math> instead of the derivative  <math>F'(x)\\ </math>&nbsp;. It is thus defined by\n\n: <math>x_{n+1} = x_n + [I - L(F(x_n), x_n)]^{-1}(F(x_n) - x_n),\\ </math>\n\nfor  <math>n=1,\\ 2,\\ 3,\\ ...</math>&nbsp;, and where  <math>I\\ </math>  is the identity operator.\n\nIf the operator  <math>L\\ </math>  satisfies\n\n: <math>\\|L(u,v) - L(x,y)\\| \\le k \\big( \\|u-x\\| + \\|v-y\\| \\big)</math>\n\nfor some constant <math>k</math>&nbsp;, then the method converges quadratically to a fixed point of <math>F</math> if the initial approximation  <math>x_0\\ </math>  is ‘sufficiently close’ to the desired solution  <math>x_\\star</math>&nbsp;, that satisfies  <math>x_\\star = F(x_\\star)</math>&nbsp;.\n\n==Notes==\n{{notelist}}\n\n==References==\n{{reflist}}\n\n[[Category:Root-finding algorithms]]"
    },
    {
      "title": "Direct integration of a beam",
      "url": "https://en.wikipedia.org/wiki/Direct_integration_of_a_beam",
      "text": "'''Direct integration''' is a [[structural analysis]] method for measuring internal shear, internal moment, rotation, and deflection of a beam.[[Image:Shear and moment.jpg|thumb|right|Positive directions for forces acting on an element.]]\n\nFor a beam with an applied weight <math>w(x) </math>, taking downward to be positive, the internal [[shear force]] is given by taking the negative integral of the weight:\n: <math>V(x) = -\\int w(x)\\, dx</math>\nThe internal moment M(x) is the integral of the internal shear:\n: <math>M(x) = \\int V(x)\\, dx</math> = <math> -\\int [\\int w(x)\\ \\, dx] dx </math>\n\nThe [[angle of rotation]] from the horizontal, <math> \\theta</math>, is the integral of the internal moment divided by the product of the [[Young's modulus]] and the [[second moment of area|area moment of inertia]]:\n: <math>\\theta (x) = \\frac{1}{EI} \\int M(x)\\, dx </math>\n\nIntegrating the angle of rotation obtains the vertical displacement <math> \\nu </math>:\n: <math>\\nu (x) = \\int \\theta (x) dx </math>\n\n==Integrating==\nEach time an integration is carried out, a constant of integration needs to be obtained. These constants are determined by using either the forces at supports, or at free ends.\n: For internal shear and moment, the constants can be found by analyzing the beam's [[free body diagram]].\n\n: For rotation and displacement, the constants are found using conditions dependent on the type of supports. For a cantilever beam, the fixed support has zero rotation and zero displacement. For a beam supported by a pin and roller, both the supports have zero displacement.\n\n==Sample calculations==\n[[Image:Simplebeam.JPG|frame|right|Simply supported beam with a constant 10 [[Newton (unit)|kN]] per meter load over a 15m length.]]\nTake the [[beam (structure)|beam]] shown at right supported by a fixed pin at the left and a roller at the right. There are no applied moments, the weight is a constant 10&nbsp;kN, and - due to symmetry - each support applies a 75&nbsp;kN vertical force to the beam. Taking x as the distance from the pin,\n\n: <math> \\mathbf w(x)= 10 (kN/m)</math>\n\n[[integral|Integrating]],\n\n:<math> \\mathbf V(x)= -\\int w(x)dx=-10x+C_1 (kN)</math>\n\nwhere <math>C_1</math> represents the applied loads. For these calculations, the only load having an effect on the beam is the 75&nbsp;kN load applied by the pin, applied at x=0, giving\n\n:<math> \\mathbf V(x)=-10x+75 (kN) </math>\n\nIntegrating the internal shear,\n\n:<math> \\mathbf M(x)= \\int V(x)=-5x^2 + 75x (kN \\cdot m) </math> where, because there is no applied moment, <math>C_2 =0</math>.\n\nAssuming an EI value of 1&nbsp;kN<math>\\cdot</math>m<math>\\cdot</math>m (for simplicity, real [[Young's modulus|E]][[second moment of area|I]] values for [[structural member]]s such as [[steel]] are normally greater by powers of ten)\n\n:<math>\\mathbf \\theta (x)= \\int \\frac{M(x)}{EI}= -\\frac{5}{3} x^3 + \\frac{75}{2} x^2 + C_3(\\frac{m}{m})</math>* and\n\n:<math>\\mathbf \\nu (x) = \\int \\theta (x) = -\\frac{5}{12} x^4 + \\frac{75}{6} x^3 + C_3 x + C_4 (m)</math>\n\nBecause of the vertical supports at each end of the beam, the displacement (<math>v</math>) at x = 0 and x = 15m is zero. Substituting (x = 0, v(0) = 0) and (x = 15m, v(15m) = 0), we can solve for constants <math>C_3</math>=-1406.25 and <math>C_4</math>=0, yielding\n\n:<math>\\mathbf \\theta (x)= \\int \\frac{M(x)}{EI}= -\\frac{5}{3} x^3 + \\frac{75}{2} x^2 -1406.25(\\frac{m}{m})</math> and\n:<math>\\mathbf \\nu (x) = \\int \\theta (x) = -\\frac{5}{12} x^4 + \\frac{75}{6} x^3 -1406.25x (m)</math>\n\nFor the given EI value, the maximum displacement, at x=7.5m, is approximately 500 times the length of the beam. For a more realistic situation, such as a uniform load of 1&nbsp;kN and an EI value of 5,000&nbsp;kN·m², the displacement would be approximately 1&nbsp;cm.\n\n*Note that for the rotation <math>\\theta</math> the units are meters divided by meters (or any other units of length which reduce to unity). This is because rotation is given as a [[slope]], the vertical displacement divided by the horizontal change.\n\n== See also ==\n* [[Bending]]\n* [[Beam theory]]\n* [[Euler-Bernoulli beam theory#Static beam equation|Euler-Bernoulli static beam equation]]\n*[[Solid Mechanics]]\n*[[Virtual Work]]\n\n== References ==\n*Hibbeler, R.C., Mechanics Materials, sixth edition; Pearson Prentice Hall, 2005. {{ISBN|0-13-191345-X}}.\n\n==External links==\n*[http://www.mathalino.com/reviewer/mechanics-and-strength-of-materials/double-integration-method-beam-deflections Beam Deflection by Double Integration Method]\n\n[[Category:Structural analysis|*]]"
    },
    {
      "title": "Structural analysis",
      "url": "https://en.wikipedia.org/wiki/Structural_analysis",
      "text": "{{about|structural studies in engineering|social-science usage|Structuralism|other uses|Structure (disambiguation)}}\n{{Mechanical failure modes}}\n{{Refimprove|date=December 2018}}\n\n\n\n'''Structural analysis''' is the determination of the effects of [[structural load|load]]s on physical [[structure]]s and their [[Structural engineering#Structural elements|components]].\nStructures subject to this type of analysis include all that must withstand loads, such as buildings, bridges, vehicles, furniture, attire, soil strata, prostheses and biological tissue.  Structural analysis employs the fields of [[applied mechanics]], [[materials science]] and [[applied mathematics]] to compute a structure's [[deformation (engineering)|deformation]]s, internal [[force]]s, [[stress analysis|stress]]es, support reactions, accelerations, and [[structural stability|stability]]. The results of the analysis are used to verify a structure's fitness for use, often precluding [[physical test]]s. Structural analysis is thus a key part of the [[structural engineering|engineering design of structures]].\n\n==Structures and Loads==\nA [[structure]] refers to a body or system of connected parts used to support a load. Important examples related to [[Civil Engineering]] include buildings, bridges, and towers; and in other branches of engineering, ship and aircraft frames, tanks, pressure vessels, mechanical systems, and electrical supporting structures are important. To design a structure, an engineer must account for its safety, aesthetics, and serviceability, while considering economic and environmental constraints. Other branches of [[engineering]] work on a wide variety of [[non-building structure]]s.\n\n===Classification of structures===\nA ''structural system'' is the combination of structural elements and their materials. It is important for a structural engineer to be able to classify a structure by either its form or its function, by recognizing the various [[Structural engineering#Structural elements|elements]] composing that structure. \nThe structural elements guiding the systemic forces through the materials are not only such as a connecting rod, a truss, a beam, or a column, but also a cable, an arch, a cavity or channel, and even an angle, a surface structure, or a frame.\n\n===Loads===\n{{main|Structural load}}\nOnce the dimensional requirement for a structure have been defined, it becomes necessary to determine the loads the structure must support. Structural design, therefore begins with specifying loads that act on the structure. The design loading for a structure is often specified in [[building code]]s. There are two types of codes: general building codes and design codes, engineers must satisfy all of the code's requirements in order for the structure to remain reliable.\n\nThere are two types of loads that structure engineering must encounter in the design. The first type of loads are dead loads that consist of the weights of the various structural members and the weights of any objects that are permanently attached to the structure. For example, columns, beams, girders, the floor slab, roofing, walls, windows, plumbing, electrical fixtures, and other miscellaneous attachments. The second type of loads are live loads which vary in their magnitude and location. There are many different types of live loads like building loads, highway bridge loads, railroad bridge loads, impact loads, wind loads, snow loads, earthquake loads, and other natural loads.\n\n==Analytical methods==\nTo perform an accurate analysis a structural engineer must determine information such as [[structural load]]s, [[List of structural elements|geometry]], support conditions, and material properties. The results of such an analysis typically include support reactions, [[Stress (physics)|stresses]] and [[Displacement (vector)|displacements]]. This information is then compared to criteria that indicate the conditions of failure. Advanced structural analysis may examine [[dynamic response]], [[buckling|stability]] and [[non-linear]] behavior.\nThere are three approaches to the analysis: the [[Strength of materials|mechanics of materials]] approach (also known as strength of materials), the [[3-D elasticity|elasticity theory]] approach (which is actually a special case of the more general field of [[continuum mechanics]]), and the [[finite element]] approach. The first two make use of analytical formulations which apply mostly simple linear elastic models, leading to closed-form solutions, and can often be solved by hand. The finite element approach is actually a numerical method for solving differential equations generated by theories of mechanics such as elasticity theory and strength of materials. However, the finite-element method depends heavily on the processing power of computers and is more applicable to structures of arbitrary size and complexity.\n\nRegardless of approach, the formulation is based on the same three fundamental relations: [[mechanical equilibrium|equilibrium]], [[Constitutive equation|constitutive]], and [[Compatibility (mechanics)|compatibility]]. The solutions are approximate when any of these relations are only approximately satisfied, or only an approximation of reality.\n\n===Limitations===\nEach method has noteworthy limitations. The method of mechanics of materials is limited to very simple structural elements under relatively simple loading conditions. The structural elements and loading conditions allowed, however, are sufficient to solve many useful engineering problems. The theory of elasticity allows the solution of structural elements of general geometry under general loading conditions, in principle. Analytical solution, however, is limited to relatively simple cases. The solution of elasticity problems also requires the solution of a system of partial differential equations, which is considerably more mathematically demanding than the solution of mechanics of materials problems, which require at most the solution of an ordinary differential equation. The finite element method is perhaps the most restrictive and most useful at the same time. This method itself relies upon other structural theories (such as the other two discussed here) for equations to solve. It does, however, make it generally possible to solve these equations, even with highly complex geometry and loading conditions, with the restriction that there is always some numerical error. Effective and reliable use of this method requires a solid understanding of its limitations.\n\n==Strength of materials methods (classical methods)==\nThe simplest of the three methods here discussed, the mechanics of materials method is available for simple structural members subject to specific loadings such as axially loaded bars, prismatic [[Beam (structure)|beams]] in a state of [[pure bending]], and circular shafts subject to torsion. The solutions can under certain conditions be superimposed using the [[superposition principle]] to analyze a member undergoing combined loading. Solutions for special cases exist for common structures such as thin-walled pressure vessels.\n\nFor the analysis of entire systems, this approach can be used in conjunction with statics, giving rise to the ''method of sections'' and ''method of joints'' for [[truss]] analysis, [[moment distribution method]] for small rigid frames, and ''portal frame'' and ''cantilever method'' for large rigid frames. Except for moment distribution, which came into use in the 1930s, these methods were developed in their current forms in the second half of the nineteenth century. They are still used for small structures and for preliminary design of large structures.\n\nThe solutions are based on linear isotropic infinitesimal elasticity and Euler–Bernoulli beam theory. In other words, they contain the assumptions (among others) that the materials in question are elastic, that stress is related linearly to strain, that the material (but not the structure) behaves identically regardless of direction of the applied load, that all [[deformation (engineering)|deformation]]s are small, and that beams are long relative to their depth. As with any simplifying assumption in engineering, the more the model strays from reality, the less useful (and more dangerous) the result.\n\n===Example===\nThere are 2 commonly used methods to find the truss element forces, namely the Method of Joints and the Method of Sections. Below is an example that is solved using both of these methods. The first diagram below is the presented problem for which we need to find the truss element forces. The second diagram is the loading diagram and contains the reaction forces from the joints.\n:[[Image:Truss Structure Analysis, Full Figure2.jpg|center|border|750x450px]]\nSince there is a pin joint at A, it will have 2 reaction forces. One in the x direction and the other in the y direction. At point B, we have a roller joint and hence we only have 1 reaction force in the y direction. Let us assume these forces to be in their respective positive directions (if they are not in the positive directions like we have assumed, then we will get a negative value for them).\n:[[Image:Truss Structure Analysis, FBD2.jpg|center|border|750x450px]]\nSince the system is in static equilibrium, the sum of forces in any direction is zero and the sum of moments about any point is zero.\nTherefore, the magnitude and direction of the reaction forces can be calculated.\n\n:<math>\\sum M_A=0=-10*1+2*R_B \\Rightarrow R_B=5</math>\n:<math>\\sum F_y=0=R_{Ay}+R_B-10 \\Rightarrow R_{Ay}=5</math>\n:<math>\\sum F_x=0=R_{Ax}</math>\n\n====Method of Joints====\nThis type of method uses the force balance in the x and y directions at each of the joints in the truss structure. \n:[[Image:Truss Structure Analysis, Method of Joints2.png|border|350x450px]]\n\nAt A,\n:<math>\\sum F_y=0=R_{Ay}+F_{AD}\\sin(60)=5+F_{AD}\\frac{\\sqrt{3} }{2} \\Rightarrow F_{AD}=-\\frac{10}{\\sqrt{3}}</math>\n:<math>\\sum F_x=0=R_{Ax}+F_{AD}\\cos(60)+F_{AB}=0-\\frac{10}{\\sqrt{3} }\\frac{1}{2}+F_{AB} \\Rightarrow F_{AB}=\\frac{5}{\\sqrt{3}}</math>\nAt D,\n:<math>\\sum F_y=0=-10-F_{AD}\\sin(60)-F_{BD}\\sin(60)=-10-\\left(-\\frac{10}{\\sqrt{3}}\\right)\\frac{\\sqrt{3} }{2}-F_{BD}\\frac{\\sqrt{3}}{2} \\Rightarrow F_{BD}=-\\frac{10}{\\sqrt{3}}</math>\n:<math>\\sum F_x=0=-F_{AD}\\cos(60)+F_{BD}\\cos(60)+F_{CD}=-\\frac{10}{\\sqrt{3}}\\frac{1}{2}+\\frac{10}{\\sqrt{3} }\\frac{1}{2}+F_{CD} \\Rightarrow F_{CD}=0</math>\nAt C,\n:<math>\\sum F_y=0=-F_{BC} \\Rightarrow F_{BC}=0</math>\n\nAlthough we have found the forces in each of the truss elements, it is a good practice to verify the results by completing the remaining force balances.\n:<math>\\sum F_x=-F_{CD}=-0=0 \\Rightarrow verified</math>\nAt B,\n:<math>\\sum F_y=R_B+F_{BD}\\sin(60)+F_{BC}=5+\\left(-\\frac{10}{\\sqrt{3}}\\right)\\frac{\\sqrt{3} }{2}+0=0 \\Rightarrow verified</math>\n:<math>\\sum F_x=-F_{AB}-F_{BD}\\cos(60)=\\frac{5}{\\sqrt{3}}-\\frac{10}{\\sqrt{3}}\\frac{1}{2}=0 \\Rightarrow verified</math>\n\n====Method of Sections====\n\n=====Method 1: Ignore the right side=====\n:[[Image:Truss Structure Analysis, Method of Sections Left2.jpg|border|550x450px]]\n:<math>\\sum M_D=0=-5*1+\\sqrt{3}*F_{AB} \\Rightarrow F_{AB}=\\frac{5}{\\sqrt{3} }</math>\n:<math>\\sum F_y=0=R_{Ay}-F_{BD}\\sin(60)-10=5-F_{BD}\\frac{\\sqrt{3}}{2}-10 \\Rightarrow F_{BD}=-\\frac{10}{\\sqrt{3}}</math>\n:<math>\\sum F_x=0=F_{AB}+F_{BD}\\cos(60)+F_{CD}=\\frac{5}{\\sqrt{3}}-\\frac{10}{\\sqrt{3}}\\frac{1}{2}+F_{CD} \\Rightarrow F_{CD}=0</math>\n\n=====Method 2: Ignore the left side=====\n:[[Image:Truss Structure Analysis, Method of Sections Right2.jpg|border|600x450px]]\n:<math>\\sum M_B=0=\\sqrt{3}*F_{CD} \\Rightarrow F_{CD}=0</math>\n:<math>\\sum F_y=0=F_{BD}\\sin(60)+R_B=F_{BD}\\frac{\\sqrt{3}}{2}+5 \\Rightarrow F_{BD}=-\\frac{10}{\\sqrt{3}}</math>\n:<math>\\sum F_x=0=-F_{AB}-F_{BD}\\cos(60)-F_{CD}=-F_{AB}-\\left(-\\frac{10}{\\sqrt{3}}\\right)\\frac{1}{2}-0 \\Rightarrow F_{AB}=\\frac{5}{\\sqrt{3}}</math>\n\nThe truss elements forces in the remaining members can be found by using the above method with a section passing through the remaining members.\n\n==Elasticity methods==\nElasticity methods are available generally for an elastic solid of any shape. Individual members such as beams, columns, shafts, plates and shells may be modeled. The solutions are derived from the equations of [[linear elasticity]]. The equations of elasticity are a system of 15 partial differential equations. Due to the nature of the mathematics involved, analytical solutions may only be produced for relatively simple geometries. For complex geometries, a numerical solution method such as the finite element method is necessary.\n\n==Methods using numerical approximation==\nIt is common practice to use approximate solutions of differential equations as the basis for structural analysis. This is usually done using numerical approximation techniques. The most commonly used numerical approximation in structural analysis is the [[Finite Element Method]].\n\nThe finite element method approximates a structure as an assembly of elements or components with various forms of connection between them and each element of which has an associated stiffness. Thus, a continuous system such as a plate or shell is modeled as a discrete system with a finite number of elements interconnected at finite number of nodes and the overall stiffness is the result of the addition of the stiffness of the various elements. The behaviour of individual elements is characterized by the element's stiffness (or flexibility) relation. The assemblage of the various stiffness's into a master stiffness matrix that represents the entire structure leads to the system's stiffness or flexibility relation. To establish the stiffness (or flexibility) of a particular element, we can use the ''mechanics of materials'' approach for simple one-dimensional bar elements, and the ''elasticity approach'' for more complex two- and three-dimensional elements. The analytical and computational development are best effected throughout by means of [[matrix (mathematics)|matrix algebra]], solving [[partial differential equation]]s.\n\nEarly applications of matrix methods were applied to articulated frameworks with truss, beam and column elements; later and more advanced matrix methods, referred to as \"[[finite element method in structural mechanics|finite element analysis]]\", model an entire structure with one-, two-, and three-dimensional elements and can be used for articulated systems together with continuous systems such as a [[pressure vessel]], plates, shells, and three-dimensional solids. Commercial computer software for structural analysis typically uses matrix finite-element analysis, which can be further classified into two main approaches: the displacement or [[stiffness method]] and the force or [[flexibility method]]. The stiffness method is the most popular by far thanks to its ease of implementation as well as of formulation for advanced applications. The finite-element technology is now sophisticated enough to handle just about any system as long as sufficient computing power is available. Its applicability includes, but is not limited to, linear and non-linear analysis, solid and fluid interactions, materials that are isotropic, orthotropic, or anisotropic, and external effects that are static, dynamic, and environmental factors. This, however, does not imply that the computed solution will automatically be reliable because much depends on the model and the reliability of the data input.\n\n==Timeline==\n*1452–1519 [[Leonardo da Vinci]] made many contributions\n*1638: [[Galileo Galilei]] published the book \"[[Two New Sciences]]\" in which he examined the failure of simple structures\n*1660: [[Hooke's law]] by [[Robert Hooke]]\n*1687: [[Isaac Newton]] published \"''[[Philosophiae Naturalis Principia Mathematica]]''\" which contains the [[Newton's laws of motion]]\n*1750: [[Euler–Bernoulli beam equation]]\n*1700–1782: [[Daniel Bernoulli]] introduced the principle of [[virtual work]]\n*1707–1783: [[Leonhard Euler]] developed the theory of [[buckling]] of columns\n*1826: [[Claude-Louis Navier]] published a treatise on the elastic behaviors of structures\n*1873: [[Carlo Alberto Castigliano]] presented his dissertation \"''Intorno ai sistemi elastici''\", which contains [[Castigliano's method|his theorem]] for computing displacement as partial derivative of the strain energy. This theorem includes the method of 'least work' as a special case\n*1936: [[Hardy Cross]]' publication of the moment distribution method which was later recognized as a form of the relaxation method applicable to the problem of flow in pipe-network\n*1941: [[Alexander Hrennikoff]] submitted his D.Sc thesis in [[Massachusetts Institute of Technology|MIT]] on the discretization of plane elasticity problems using a lattice framework\n*1942: [[Richard Courant|R. Courant]] divided a domain into finite subregions\n*1956: J. Turner, [[Ray W. Clough|R. W. Clough]], H. C. Martin, and L. J. Topp's paper on the \"Stiffness and Deflection of Complex Structures\" introduces the name \"finite-element method\" and is widely recognized as the first comprehensive treatment of the method as it is known today\n\n==See also==\n* [[Limit state design]]\n* [[Structural engineering theory]]\n* [[Structural integrity and failure]]\n* [[Stress–strain analysis]]\n* [[v:Probabilistic Assessment of Structures|Probabilistic Assessment of Structures]]\n\n==References==\n{{Reflist}}\n\n{{wikibooks}}\n{{wikiversity}}\n\n[[Category:Structural analysis|*]]"
    },
    {
      "title": "ATILA",
      "url": "https://en.wikipedia.org/wiki/ATILA",
      "text": "{{refimprove|date=September 2015}}\n\n{{infobox software\n| name                   = ATILA\n| operating_system       = [[Linux]], [[OS X]], [[Microsoft Windows|Windows]]\n| license                = [[Proprietary software|Proprietary]]\n| genre                  = [[Finite element analysis]]\n| website                = {{URL|http://www.mmech.com/atila-fem}}\n}}\n\n'''ATILA''' is a [[finite element analysis]] software package developed for the analysis of two and three dimensional mechanical structures that contain active [[piezoelectric]] & [[magnetostrictive]] materials. It is organized around a strong electrical/mechanical coupling and a strong fluid/structure coupling. ATILA is used in many applications including underwater acoustic, seismic, non-destructive testing, medical imaging, RF telecommunications.<ref>{{cite book |last= |first= |date= |title=Applications of ATILA FEM software to smart materials : case studies in designing devices |url= |location= |publisher= |page= |isbn=9780857096319 |author-link= }}</ref>\n\n== References ==\n{{reflist}}\n\n[[Category:Structural analysis]]\n[[Category:Dimensional analysis]]"
    },
    {
      "title": "Bending stiffness",
      "url": "https://en.wikipedia.org/wiki/Bending_stiffness",
      "text": "The '''bending [[stiffness]]''' (<math>K</math>) is the resistance of a member against bending deformation. It is a function of the [[Young's modulus]] <math>E</math>, the [[area moment of inertia]] <math>I</math> of the beam cross-section about the axis of interest, length of the beam and beam boundary condition. Bending [[stiffness]] of a beam can analytically be derived from the equation of beam deflection when it is applied by a force.\n\n:<math>K = \\frac{\\mathrm{p}}{\\mathrm{w}}</math>\n\nwhere <math>\\mathrm{p}</math> is the applied force and <math>\\mathrm{w}</math> is the deflection. According to elementary [[beam theory]], the relationship between the applied bending moment <math>M</math> and the resulting [[curvature]] <math>\\kappa</math> of the beam is:\n\n:<math>M = E I \\kappa = E I \\frac{\\mathrm{d}^2 w}{\\mathrm{d} x^2}</math>\n\nwhere <math>w</math> is the deflection of the beam and <math>x</math> is the distance along the beam. Double integration of the above equation leads to computing the deflection of the beam, and in turn, the bending stiffness of the beam.\nBending stiffness in beams is also known as [[Flexural rigidity]].\n\n==See also==\n* [[Applied mechanics]]\n* [[Beam theory]]\n* [[Bending]]\n\n==External links==\n* [http://www.efunda.com/formulae/solid_mechanics/beams/theory.cfm Efunda's beam calculator]\n\n[[Category:Continuum mechanics]]\n[[Category:Structural analysis]]"
    },
    {
      "title": "Betti's theorem",
      "url": "https://en.wikipedia.org/wiki/Betti%27s_theorem",
      "text": "'''Betti's theorem''',  also known as '''Maxwell-Betti reciprocal work theorem''', discovered by [[Enrico Betti]] in 1872, states that for a linear elastic structure subject to two sets of forces {P<sub>i</sub>} i=1,...,m and {Q<sub>j</sub>}, j=1,2,...,n, the [[Mechanical work|work]] done by the set P through the displacements produced by the set Q is equal to the work done by the set Q through the displacements produced by the set P. This theorem has applications in [[structural engineering]] where it is used to define [[influence line]]s and derive the [[boundary element method]].\n\nBetti's theorem is used in the design of compliant mechanisms by topology optimization approach.\n\n==Proof==\nConsider a solid body subjected to a pair of external force systems, referred to as <math>F^P_i</math> and <math>F^Q_i</math>.  Consider that each force system causes a displacement fields, with the displacements measured at the external force's point of application referred to as <math>d^P_i</math> and <math>d^Q_i</math>.\n\nWhen the <math>F^P_i</math> force system is applied to the structure, the balance between the work performed by the external force system and the strain energy is:\n\n:<math>\n\\frac{1}{2}\\sum^n_{i=1}F^P_id^P_i = \\frac{1}{2}\\int_\\Omega \\sigma^P_{ij}\\epsilon^P_{ij}\\,d\\Omega\n</math>\n\nThe work-energy balance associated with the <math>F^Q_i</math> force system is as follows:\n\n:<math>\n\\frac{1}{2}\\sum^n_{i=1}F^Q_id^Q_i = \\frac{1}{2}\\int_\\Omega \\sigma^Q_{ij}\\epsilon^Q_{ij}\\,d\\Omega\n</math>\n\nNow, consider that with the <math>F^P_i</math> force system applied, the <math>F^Q_i</math> force system is applied subsequently.  As the <math>F^P_i</math> is already applied and therefore won't cause any extra displacement, the work-energy balance assumes the following expression:\n\n:<math>\n\\frac{1}{2}\\sum^n_{i=1}F^Q_id^Q_i + \\sum^n_{i=1}F^P_id^Q_i = \\frac{1}{2} \\int_\\Omega \\sigma^Q_{ij}\\epsilon^Q_{ij}\\,d\\Omega + \\int_\\Omega \\sigma^P_{ij}\\epsilon^Q_{ij}\\,d\\Omega\n</math>\n\nConversely, if we consider the <math>F^Q_i</math> force system already applied and the <math>F^P_i</math> external force system applied subsequently, the work-energy balance will assume the following expression:\n\n:<math>\n\\frac{1}{2}\\sum^n_{i=1}F^P_id^P_i + \\sum^n_{i=1}F^Q_id^P_i = \\frac{1}{2}\\int_\\Omega \\sigma^P_{ij}\\epsilon^P_{ij}\\,d\\Omega + \\int_\\Omega \\sigma^Q_{ij}\\epsilon^P_{ij}\\,d\\Omega\n</math>\n\nIf the work-energy balance for the cases where the external force systems are applied in isolation are respectively subtracted from the cases where the force systems are applied simultaneously, we arrive at the following equations:\n\n:<math>\n\\sum^n_{i=1}F^P_id^Q_i = \\int_\\Omega \\sigma^P_{ij}\\epsilon^Q_{ij}\\,d\\Omega\n</math>\n\n:<math>\n\\sum^n_{i=1}F^Q_id^P_i = \\int_\\Omega \\sigma^Q_{ij}\\epsilon^P_{ij}\\,d\\Omega\n</math>\n\nIf the solid body where the force systems are applied is formed by a [[Linear elasticity|linear elastic material]] and if the force systems are such that only [[Infinitesimal strain theory|infinitesimal strains]] are observed in the body, then the body's [[constitutive equation]], which may follow [[Hooke's law]], can be expressed in the following manner:\n\n:<math>\n\\sigma_{ij}=D_{ijkl}\\epsilon_{kl}\n</math>\n\nReplacing this result in the previous set of equations leads us to the following result:\n\n:<math>\n\\sum^n_{i=1}F^P_id^Q_i = \\int_\\Omega D_{ijkl}\\epsilon^P_{ij}\\epsilon^Q_{kl}\\,d\\Omega\n</math>\n\n:<math>\n\\sum^n_{i=1}F^Q_id^P_i = \\int_\\Omega D_{ijkl}\\epsilon^Q_{ij}\\epsilon^P_{kl}\\,d\\Omega\n</math>\n\nIf we subtracting both equations then we obtain the following result:\n\n:<math>\n\\sum^n_{i=1}F^P_id^Q_i = \\sum^n_{i=1}F^Q_id^P_i\n</math>\n\n==Example==\nFor a simple example let m=1 and n=1.  Consider a horizontal [[Beam (structure)|beam]] on which two points have been defined: point 1 and point 2.  First we apply a vertical force P at point 1 and measure the vertical displacement of point 2, denoted <math>\\Delta_{P2}</math>.  Next we remove force P and apply a vertical force Q at point 2, which produces the vertical displacement at point 1 of <math>\\Delta_{Q1}</math>.  Betti's reciprocity theorem states that:\n\n:<math>P \\,\\Delta_{Q1}=Q \\,\\Delta_{P2}.</math>\n\n==See also==\n* [[D'Alembert's principle]]\n\n==References==\n{{reflist}}\n*{{cite book |title=Structural analysis: a unified classical and matrix approach |author1=A Ghali |author2=A.M. Neville |year=1972 |publisher=E & FN SPON |location=London, New York |isbn=0-419-21200-0 |page=215 }}\n\n{{DEFAULTSORT:Betti's Theorem}}\n[[Category:Structural analysis]]\n[[Category:Continuum mechanics]]\n[[Category:Physics theorems]]"
    },
    {
      "title": "Buckling",
      "url": "https://en.wikipedia.org/wiki/Buckling",
      "text": "{{short description|Structural instability inflicted by compressive stress via sudden sideways deflection}}\n{{Other uses}}\n{{distinguish|buckle}}\n{{Mechanical failure modes}}\nIn science, '''buckling''' is an instability that leads to [[structural failure]]. The failure modes can in simple cases be found by simple mathematical solutions. For complex structures the failure modes are found by numerical tools. \n\nWhen a structure is subjected to [[compression (physics)|compressive]] [[stress (mechanics)|stress]], buckling may occur. Buckling is characterized by a sudden sideways deflection of a structural member. This may occur even though the stresses that develop in the structure are well below those needed to cause failure of the material of which the structure is composed. As an applied load is increased on a member, such as a column, it will ultimately become large enough to cause the member to become unstable and it is said to have buckled. Further loading will cause significant and somewhat unpredictable deformations, possibly leading to complete loss of the member's load-carrying capacity. If the deformations that occur after buckling do not cause the complete collapse of that member, the member will continue to support the load that caused it to buckle. If the buckled member is part of a larger assemblage of components such as a building, any load applied to the buckled part of the structure beyond that which caused the member to buckle will be redistributed within the structure.\n\nIn a mathematical sense, buckling is a [[Bifurcation theory|bifurcation]] in the solution to the equations of [[Mechanical equilibrium|static equilibrium]]. At a certain point, under an increasing load, any further load is able to be sustained in one of two states of equilibrium: a purely compressed state (with no lateral deviation) or a laterally-deformed state.\n\n==Columns==\n[[File:Buckled column.svg|thumb|right|A column under a concentric axial load exhibiting the characteristic deformation of buckling]]\n[[File:Buckling beam element.svg|thumb|right|The eccentricity of the axial force results in a bending moment acting on the beam element.]]\nThe ratio of the effective length of a [[column]] to the least [[radius of gyration]] of its cross section is called the [[slenderness ratio]] (sometimes expressed with the Greek letter lambda, λ). This ratio affords a means of classifying columns and their failure mode. The slenderness ratio is important for design considerations. All the following are approximate values used for convenience.\n\nIf the load on a column is applied through the [[center of gravity]] (centroid) of its cross section, it is called an [[axis of rotation|axial]] [[Structural load|load]]. A load at any other point in the cross section is known as an [[eccentric (mechanism)|eccentric]] load. A short column under the action of an axial load will fail  by direct compression before it buckles, but a long column loaded in the same manner will fail by springing suddenly outward laterally (buckling) in a bending mode. The buckling mode of deflection is considered a failure mode, and it generally occurs before the axial compression stresses (direct compression) can cause failure of the material by yielding or fracture of that compression member. However, intermediate-length columns will fail by a combination of direct compressive stress and bending.\n\nIn particular:\n* A short [[steel]] column is one whose slenderness ratio does not exceed 50; an intermediate length steel column has a slenderness ratio ranging from about 50 to 200, and its behavior is dominated by the strength limit of the material, while a long steel column may be assumed to have a slenderness ratio greater than 200 and its behavior is dominated by the modulus of elasticity of the material.\n* A short [[concrete]] column is one having a ratio of unsupported length to least dimension of the cross section equal to or less than 10. If the ratio is greater than 10, it is considered a long column (sometimes referred to as a slender column).\n* [[Wood|Timber]] columns may be classified as short columns if the ratio of the length to least dimension of the cross section is equal to or less than 10. The dividing line between intermediate and long timber columns cannot be readily evaluated. One way of defining the lower limit of long timber columns would be to set it as the smallest value of the ratio of length to least cross sectional area that would just exceed a certain constant K of the material. Since K depends on the [[modulus of elasticity]] and the allowable compressive [[stress (physics)|stress]] parallel to the grain, it can be seen that this arbitrary limit would vary with the [[species]] of the timber. The value of K is given in most structural handbooks.\n\nThe theory of the behavior of columns was investigated in 1757 by mathematician [[Leonhard Euler]]. He derived the formula, the Euler formula, that gives the maximum axial load that a long, slender, ideal column can carry without buckling. An ideal column is one that is perfectly straight, made of a homogeneous material, and free from initial stress. When the applied load reaches the Euler load, sometimes called the critical load, the column comes to be in a state of unstable [[mechanical equilibrium|equilibrium]]. At that load, the introduction of the slightest lateral force will cause the column to fail by suddenly \"jumping\" to a new configuration, and the column is said to have buckled. This is what happens when a person stands on an empty aluminum can and then taps the sides briefly, causing it to then become instantly crushed (the vertical sides of the can understood as an infinite series of extremely thin columns). The formula derived by Euler for long slender columns is given below.\n\n: <math>F = \\frac{\\pi^2 EI}{(KL)^2}</math>\n\nTo get the mathematical demonstration read: [[Euler's critical load]]\n\nwhere\n: <math>F</math>, maximum or critical [[force]] (vertical load on column),\n: <math>E</math>, [[modulus of elasticity]],\n: <math>I</math>,  smallest [[area moment of inertia]] (second moment of area) of the cross section of the column,\n: <math>L</math>, unsupported length of column,\n: <math>K</math>, [[:File:ColumnEffectiveLength.png|column effective length factor]], whose value depends on the conditions of end support of the column, as follows.\n:: For both ends pinned (hinged, free to rotate), <math>K</math> = 1.0.\n:: For both ends fixed, <math>K</math> = 0.50.\n:: For one end fixed and the other end pinned, <math>K</math> = {{radic|2}}/2 ≈ 0.7071.\n:: For one end fixed and the other end free to move laterally, <math>K</math> = 2.0.\n: <math>K L</math> is the effective length of the column.\n\nExamination of this formula reveals the following facts with regard to the load-bearing ability of slender columns.\n* The [[elasticity (physics)|elasticity]] of the material of the column and not the compressive [[strength of materials|strength of the material]] of the column determines the column's buckling load.\n* The buckling load is directly [[proportionality (mathematics)|proportional]] to the [[second moment of area]] of the cross section.\n* The boundary conditions have a considerable effect on the critical load of slender columns. The boundary conditions determine the mode of bending of the column and the distance between inflection points on the displacement curve of the deflected column. The inflection points in the deflection shape of the column are the points at which the curvature of the column changes sign and are also the points at which the column's internal bending moments of the column are zero. The closer the inflection points are, the greater the resulting axial load capacity (bucking load) of the column.\n\n[[Image:Buckledmodel.JPG|thumb|right|A demonstration model illustrating the different \"Euler\" buckling modes. The model shows how the boundary conditions affect the critical load of a slender column. Notice that the columns are identical, apart from the boundary conditions.]]\n\nA conclusion from the above is that the buckling load of a column may be increased by changing its material to one with a higher modulus of elasticity (E), or changing the design of the column's cross section so as to increase its moment of inertia. The latter can be done without increasing the weight of the column by distributing the material as far from the principal axis of the column's cross section as possible. For most purposes, the most effective use of the material of a column is that of a tubular section.\n\nAnother insight that may be gleaned from this equation is the effect of length on critical load. Doubling the unsupported length of the column quarters the allowable load. The restraint offered by the end connections of a column also affects its critical load. If the connections are perfectly rigid (does not allowing rotation of its ends), the critical load will be four times that for a similar column where the ends are pinned (allowing rotation of its ends).\n\nSince the radius of gyration is defined as the square root of the ratio of the column's moment of inertia about an axis to its cross sectional area, the above Euler formula may be reformatted by substituting the radius of gyration A·r<sup>2</sup> for I:\n\n: <math>\\sigma = \\frac{F}{A} = \\frac{\\pi^2 E}{\\left(\\frac{\\ell}{r}\\right)^2}</math>\n\nwhere <math>F/A = \\sigma</math> is the stress that causes buckling the column, and <math>l/r</math> is the slenderness ratio.\n\nSince structural columns are commonly of intermediate length, the Euler formula has little practical application for ordinary design. Issues that cause deviation from the pure Euler column behaviour include imperfections in geometry of the column in combination with plasticity/non-linear stress strain behaviour of the column's material. Consequently, a number of empirical column formulae have been developed that agree with test data, all of which embody the slenderness ratio. Due to the uncertainty in the behavior of columns, for design, appropriate [[factor of safety|safety factors]] are introduced into these formulae. One such formula is the [[Perry Robertson formula]] which estimates the critical buckling load based on an assumed small initial curvature, hence an eccentricity of the axial load. The Rankine Gordon formula (Named for [[William John Macquorn Rankine]] and Perry Hugesworth Gordon (1899 &ndash; 1966)) is also based on experimental results and suggests that a column will buckle at a load F<sub>max</sub> given by:\n: <math>\\frac{1}{F_\\text{max}} = \\frac{1}{F_e} + \\frac{1}{F_c}</math>\n\nwhere F<sub>e</sub> is the Euler maximum load and F<sub>c</sub> is the maximum compressive load. This formula typically produces a conservative estimate of F<sub>max</sub>.\n\n===Self-buckling===\nTo get the mathematical demonstration read: [[Self-buckling]]\n\nA free-standing, vertical column, with density <math>\\rho</math>, Young's modulus <math>E</math>, and cross-sectional area <math>A</math>, will buckle under its own weight if its height exceeds a certain critical value:<ref>{{cite journal| last=Kato| first=K.| title=Mathematical Investigation on the Mechanical Problems of Transmission Line| journal=Journal of the Japan Society of Mechanical Engineers|volume=19|pages=41|year=1915}}</ref><ref>{{cite book |last=Ratzersdorfer |first=Julius |title=Die Knickfestigkeit von Stäben und Stabwerken |language=German |trans-title=The buckling resistance of members and frames |publisher=J. Springer |location=Wein, Austria |year=1936 |pages=107–109 |isbn=978-3-662-24075-5}}</ref><ref>{{cite journal|last=Cox|first=Steven J.|author2=C. Maeve McCarthy|author2-link=Maeve McCarthy|title=The Shape of the Tallest Column|journal= SIAM Journal on Mathematical Analysis|volume=29|issue=3|pages=547–554|year=1998|doi=10.1137/s0036141097314537}}</ref>\n\n: <math>h_\\text{crit} = \\left(\\frac{9B^2}{4}\\,\\frac{EI }{\\rho gA}\\right)^\\frac{1}{3}</math>\n\nwhere ''g'' is the acceleration due to gravity, ''I'' is the [[second moment of area]] of the beam cross section, and ''B'' is the first zero of the [[Bessel function]] of the first kind of order −1/3, which is equal to 1.86635086…\n\n==Buckling under tensile dead loading==\n\n[[File:Continuous model tensile buckling.jpg|thumb|right|Fig.{{nbsp}}2: Elastic beam system showing buckling under tensile dead loading.]]\nUsually buckling and instability are associated with compression, but buckling and instability can also occur in elastic structures subject to dead [[tensile load]].<ref>{{cite journal |url=http://www.ing.unitn.it/~bigoni/paper/zaccaria_bigoni_buckling_tension.pdf |first1=D. |last1=Zaccaria |first2=D. |last2=Bigoni |first3=G. |last3=Noselli |first4=D. |last4=Misseroni |title=Structures buckling under tensile dead load |journal=Proceedings of the Royal Society A |date=21 April 2011 |volume=467 |number=2130 |pages=1686–1700 |doi=10.1098/rspa.2010.0505|bibcode = 2011RSPSA.467.1686Z }}</ref>\n\nAn example of a single-degree-of-freedom structure is shown in Fig.{{nbsp}}2, where the critical load is also indicated. Another example involving flexure of a structure made up of beam elements governed by the equation of the Euler's elastica is shown in Fig.{{nbsp}}3. In both cases, there are no elements subject to compression. The instability and buckling in tension are related to the presence of the slider, the junction between the two rods, allowing only relative sliding between the connected pieces. Watch a [http://www.ing.unitn.it/~bigoni/tensile_buckling.html movie] for more details.\n\n==Constraints, curvature and multiple buckling==\n[[Image:Sketch curvature 1.jpg|thumb|Fig.{{nbsp}}3: A one-degree-of-freedom structure exhibiting a tensile (compressive) buckling load as related to the fact that the right end has to move along the circular profile labeled ''Ct'' (labeled ''Cc'').]]\nBuckling of an elastic structure strongly depends on the curvature of the constraints against which the ends of the structure are prescribed to move (see Bigoni, Misseroni, Noselli and Zaccaria, 2012<ref>{{cite journal |url=http://www.ing.unitn.it/~bigoni/multiple_bifurcations.html |first1=D. |last1=Bigoni |first2=D. |last2=Misseroni |first3=G. |last3=Noselli |first4=D. |last4=Zaccaria |title=Effects of the constraint's curvature on structural instability: tensile buckling and multiple bifurcations |journal=Proceedings of the Royal Society A |date=2012 |doi=10.1098/rspa.2011.0732|arxiv = 1201.4701 |bibcode = 2012RSPSA.468.2191B |volume=468 |issue=2144 |pages=2191–2209}}</ref>). In fact, even a single-degree-of-freedom system (see Fig.{{nbsp}}3) may exhibit a tensile (or a compressive) buckling load as related to the fact that one end has to move along the circular profile labeled ''Ct'' (labeled ''Cc'').\n\n[[Image:buckling curvature 2.gif|thumb|Fig.{{nbsp}}4: A one-degree-of-freedom structure with an S-shaped bicircular profile exhibiting multiple bifurcations (both tensile and compressive).]]\nThe two circular profiles can be arranged in a 'S'-shaped profile, as shown in Fig.{{nbsp}}4; in that case a discontinuity of the constraint's curvature is introduced, leading to multiple bifurcations. Note that the single-degree-of-freedom structure shown in Fig.{{nbsp}}4 has two buckling loads (one tensile and one compressive). Watch a [http://www.ing.unitn.it/~bigoni/multiple_bifurcations.html movie] for more details.\n\n==Flutter instability==\nStructures subject to a follower (non-conservative) load{{clarify|date=February 2015}} may suffer instabilities which are not of the buckling type and therefore are not detectable with a static approach.<ref>{{cite book |last1=Bigoni |first1=D. |title=Nonlinear Solid Mechanics: Bifurcation Theory and Material Instability |publisher=Cambridge University Press |date=2012 |isbn=9781107025417}}</ref> For instance, the so-called 'Ziegler column' is shown in Fig.{{nbsp}}5.\n\n[[Image:Ziegler column.jpg|thumb|Fig.{{nbsp}}5: A sketch of the ''Ziegler column'', a two-degree-of-freedom system subject to a follower load (the force P remains always parallel to the rod BC), exhibiting flutter and divergence instability. The two rods, of linear mass density ρ, are rigid and connected through two rotational springs of stiffness k1 and k2.]]\nThis two-degree-of-freedom system does not display a quasi-static buckling, but becomes dynamically unstable. To see this, we note that the equations of motion are\n\n: <math>\\begin{align}\n  \\frac{1}{3} \\rho l_1^2 \\left(l_1 + 3l_2\\right)\\ddot{\\alpha}_1 + \\frac{1}{2} \\rho l_1 l_2^2 \\cos\\left(\\alpha_1 - \\alpha_2\\right)\\ddot{\\alpha}_2 + \\frac{1}{2} \\rho l_1 l_2^2 \\sin\\left(\\alpha_1 - \\alpha_2\\right)\\dot{\\alpha}_2^2 +{} &\\\\\n  \\left(k_1 + k_2\\right)\\alpha_1 - k_2\\alpha_2 + \\left(\\beta_1 + \\beta_2\\right)\\dot{\\alpha}_1 - \\beta_2 \\dot{\\alpha}_2 - l_1 P \\sin\\left(\\alpha_1 - \\alpha_2\\right) &= 0, \\\\[6pt]\n  \\frac{1}{2} \\rho l_1 l_2^2 \\cos\\left(\\alpha_1 - \\alpha_2\\right)\\ddot{\\alpha}_1 + \\frac{1}{3} \\rho l_2^3\\ddot{\\alpha}_2 - \\frac{1}{2} \\rho l_1 l_2^2 \\sin\\left(\\alpha_1 - \\alpha_2\\right)\\dot{\\alpha}_1^2 -{} &\\\\\n  k_2\\left(\\alpha_1 - \\alpha_2\\right) - \\beta_2\\left(\\dot{\\alpha}_1 - \\dot{\\alpha}_2\\right) &= 0 ,\n\\end{align}</math>\n\nand their linearized version is\n\n:<math>\\begin{align}\n    \\frac{1}{3} \\rho l_1^2 \\left(l_1 + 3l_2\\right)\\ddot{\\alpha}_1 + \\frac{1}{2} \\rho l_1 l_2^2 \\ddot{\\alpha}_2 + \\left(k_1 + k_2\\right)\\alpha_1 - k_2\\alpha_2 - l_1 P \\left(\\alpha_1 - \\alpha_2\\right) &= 0, \\\\\n    \\frac{1}{2} \\rho l_1 l_2^2 \\ddot{\\alpha}_1 + \\frac{1}{3} \\rho l_2^3\\ddot{\\alpha}_2 - k_2\\left(\\alpha_1 - \\alpha_2\\right) &= 0 .\n\\end{align}</math>\n\nAssuming a time-harmonic solution in the form\n\n: <math>\\alpha_j = A_j\\, e^{-i \\Omega \\,t}, ~~~j = 1, 2,</math>\n\nwe find the critical loads for flutter (<math>\\ P_f</math>) and divergence (<math>\\ P_d</math>),\n\n: <math>P_{f,d} = \\frac{k_2}{l_1} \\cdot \\frac{k + (1 + \\lambda)^3 \\mp \\lambda\\,\\sqrt{k(3 + 4\\lambda)}}{1 + \\frac{3}{2}\\lambda}</math>\n\nwhere <math>\\lambda = l_1/l_2</math> and <math>k = k_1/k_2</math>.\n\n[[Image:deformed shapes.jpg|thumb|Fig.{{nbsp}}6: A sequence of deformed shapes at consecutive times intervals of the structure sketched in Fig.{{nbsp}}5 and exhibiting flutter (upper part) and divergence (lower part) instability.]]\nFlutter instability corresponds to a vibrational motion of increasing amplitude and is shown in Fig.{{nbsp}}6 (upper part) together with the divergence instability (lower part) consisting in an exponential growth.\n\nRecently, Bigoni and Noselli (2011)<ref>{{cite journal |url=http://www.ing.unitn.it/~bigoni |first1=D. |last1=Bigoni |first2=G. |last2=Noselli |title=Experimental evidence of flutter and divergence instabilities induced by dry friction |journal=Journal of the Mechanics and Physics of Solids |date=2011 |volume=59 |issue=10 |pages=2208–2226|bibcode = 2011JMPSo..59.2208B |doi = 10.1016/j.jmps.2011.05.007 |citeseerx=10.1.1.700.5291 }}</ref> have experimentally shown that flutter and divergence instabilities can be directly related to dry friction, watch the [http://www.ing.unitn.it/~bigoni/flutter.html movie] for more details.\n\n== Various forms of buckling ==\nBuckling is a state which defines a point where an equilibrium configuration becomes unstable under a parametric change of load and can manifest itself in several different phenomena.  All can be classified as forms of [[bifurcation theory|bifurcation]].\n\nThere are four basic forms of bifurcation associated with loss of structural stability or buckling in the case of structures with a single degree of freedom.  These comprise two types of [[pitchfork bifurcation]], one [[saddle-node bifurcation]] (often referred to as a [[limit point]]) and one [[transcritical bifurcation]]. The pitchfork bifurcations are the most commonly studied forms and include the buckling of columns, sometimes known as Euler buckling; the buckling of plates, sometimes known as local buckling, which is well known to be relatively safe (both are supercritical phenomena) and the buckling of shells, which is well-known to be a highly dangerous (subcritical phenomenon).<ref>{{cite book |title=A general theory of elastic stability |first1=J. M. T. |last1=Thompson |first2=G. W. |last2=Hunt |publisher=Wiley |date=1973 |isbn=978-0471859918}}</ref> Using the concept of potential energy, equilibrium is defined as a stationary point with respect to the degree(s) of freedom of the structure. We can then determine whether the equilibrium is stable, as in the case where the stationary point is a local minimum; or unstable, as in the case where the stationary point is a maximum point of inflection or saddle point (for multiple-degree-of-freedom structures only) – see animations below.\n\n{{Gallery\n| title=Archetypal rigid link models with a single degree of freedom (SDOF) used to demonstrate basic buckling phenomena (see bifurcation diagrams below).  All cases start at the position corresponding to q = 0.\n| File:saddle-node-strut.png | Truss with spring tie (model shallow tied arch).\n| File:supercritical-strut.png | Link-strut with rotational spring.\n| File:subcritical-strut.png | Link-strut with transverse translational spring.\n| File:transcritical-strut.png | Asymmetrically supported link-strut.\n}}\n\n{{Gallery\n| title=Animations of the variation of total potential energy (red) for various load values, P (black), in generic structural systems with the indicated bifurcation or buckling behaviour.\n| File:Saddle-node-animation.gif|Two saddle-node bifurcations (limit points).\n| File:supercritical.gif|Supercritical pitchfork bifurcation (stable-symmetric buckling point).\n| File:subcritical.gif|Subcritical pitchfork bifurcation (unstable-symmetric buckling point).\n| File:Transcritical-animation.gif|Transcritical bifurcation (asymmetric buckling point).\n}}\n\nIn Euler buckling,<ref>{{cite book |title=Buckling of Bars, Plates, and Shells |first=Robert M. |last=Jones |date=1 December 2007 |publisher=CRC |isbn=978-1560328278}}</ref><ref>{{cite conference |title=Observations on eigenvalue buckling analysis within a finite element context |first=Christopher J. |last=Earls |date=2007 |journal=Proceedings of the Structural Stability Research Council, Annual Stability Conference |location=New Orleans, LA}}</ref> when the applied load is increased by a small amount beyond the critical load, the structure deforms into a buckled configuration which is adjacent to the original configuration. For example, the Euler column pictured will start to bow when loaded slightly above its critical load, but will not suddenly collapse.\n\n[[File:bifurcation buckling.png]]\n\nIn structures experiencing limit point instability, if the load is increased infinitesimally beyond the critical load, the structure undergoes a large deformation into a different stable configuration which is not adjacent to the original configuration. An example of this type of buckling is a toggle frame (pictured) which 'snaps' into its buckled configuration.\n\n[[File:limit point instability.png]]\n\n==Plate buckling==\nA [[plate (metal)|plate]] is a 3-dimensional structure defined as having a width of comparable size to its length, with a thickness that is very small in comparison to its other two dimensions. Similar to columns, thin plates experience out-of-plane buckling deformations when subjected to critical loads; however, contrasted to column buckling, plates under buckling loads can continue to carry loads, called local buckling. This phenomenon is incredibly useful in numerous systems, as it allows systems to be engineered to provide greater loading capacities.\n\nFor a rectangular plate, supported along every edge, loaded with a uniform compressive force per unit length, the derived governing equation can be stated by:<ref name=\"auto\">{{cite book |last=Bulson |first=P. S. |title=Theory of Flat Plates |publisher=Chatto and Windus, London |date=1970}}</ref>\n\n: <math>\\frac{\\partial^4 w}{\\partial x^4} + \\frac{\\partial^4 w}{\\partial x^2 \\partial y^2} + \\frac{\\partial^4 w}{\\partial y^4} = \\frac{12\\left(1 - \\nu^2\\right)}{E t^3}\\left(-N_x \\frac{\\partial^2 w}{\\partial x^2}\\right)</math>\n\nwhere\n: <math>w</math>, out-of-plane deflection\n: <math>N_{x}</math>, uniformly distributed compressive load\n: <math>\\nu</math>, [[Poisson's ratio]]\n: <math>E</math>, [[modulus of elasticity]]\n: <math>t</math>, thickness\n\nThe solution to the deflection can be expanded into two harmonic functions shown:<ref name=\"auto\"/>\n\n: <math>w = \\sum_{m=1, 2, 3, \\ldots} \\sum_{n=1, 2, 3, \\ldots} w_{mn}\\sin\\left(\\frac{m\\pi x}{a}\\right)\\sin\\left(\\frac{n\\pi y}{b}\\right)</math>\n\nwhere \n: <math>m</math>, number of half sine curvatures that occur lengthwise\n: <math>n</math>, number of half sine curvatures that occur widthwise\n: <math>a</math>, length of specimen\n: <math>b</math>, width of specimen\n\nThe previous equation can be substituted into the earlier differential equation where <math>n</math> equals 1. <math>N_x</math> can be separated providing the equation for the critical compressive loading of a plate:<ref name=\"auto\"/>\n\n: <math>N_{x, cr} = k_{cr} \\frac{\\pi^2 Et^3}{12\\left(1 - \\nu^2\\right)b}</math>\n\nwhere\n: <math>k_{cr}</math>, buckling coefficient, given by:<ref name=\"auto\"/>\n\n: <math>k_{cr} = \\left(\\frac{mb}{a} + \\frac{a}{mb}\\right)^2</math>\n\nThe buckling coefficient is influenced by the aspect of the specimen, <math>a</math> / <math>{b}</math>, and the number of lengthwise curvatures. For an increasing number of such curvatures, the aspect ratio produces a varying buckling coefficient; but each relation provides a minimum value for each <math>m</math>. This minimum value can then be used as a constant, independent from both the aspect ratio and <math>m</math>.<ref name=\"auto\"/>\n\nGiven stress is found by the load per unit area, the following expression is found for the critical stress:\n\n: <math>\\sigma_{cr} = k_{cr}\\frac{\\pi^{2}E}{12\\left(1 - \\nu^2\\right)\\left(\\frac{b}{t}\\right)^2}</math>\n\nFrom the derived equations, it can be seen the close similarities between the critical stress for a column and for a plate. As the width <math>b</math> shrinks, the plate acts more like a column as it increases the resistance to buckling along the plate’s width. The increase of <math>a</math> allows for an increase of the number of sine waves produced by buckling along the length, but also increases the resistance from the buckling along the width.<ref name=\"auto\"/> This creates the preference of the plate to buckle in such a way to equal the number of curvatures both along the width and length. Due to boundary conditions, when a plate is loaded with a critical stress and buckles, the edges perpendicular to the load cannot deform out-of-plane and will therefore continue to carry the stresses. This creates a non-uniform compressive loading along the ends, where the stresses are imposed on half of the effective width on either side of the specimen, given by the following:<ref name=\"auto\"/>\n\n: <math>\\frac{b_\\text{eff}}{b} = \\sqrt{\\frac{\\sigma_{cr}}{\\sigma_{y}} \\left(1 - 1.022\\sqrt{\\frac{\\sigma_{cr}}{\\sigma_y}}\\right)}</math>\n\nwhere\n: <math>b_\\text{eff}</math>, effective width\n: <math>\\sigma_y</math>, yielding stress\n\nAs the loaded stress increase, the effective width continues to shrink; if the stresses on the ends ever reaches the yield stress, the plate will fail. This is what allows the buckled structure to continue supporting loadings. When the axial load over the critical load is plotted against the displacement, the fundamental path is shown. It demonstrates the plate's similarity to a column under buckling; however, past the buckling load, the fundamental path bifurcates into a secondary path that curves upward, providing the ability to be subjected to higher loads past the critical load.\n\n==Bicycle wheels==\nA conventional [[bicycle wheel]] consists of a thin rim kept under high compressive stress by the (roughly normal) inward pull of a large number of spokes. It can be considered as a loaded column that has been bent into a circle. If spoke tension is increased beyond a safe level or if part of the rim is subject to a certain lateral force, the wheel spontaneously fails into a characteristic saddle shape (sometimes called a \"taco\" or a \"[[Pringles|pringle]]\") like a three-dimensional Euler column. If this is a purely elastic deformation the rim will resume its proper plane shape if spoke tension is reduced or a lateral force from the opposite direction is applied.\n\n==Surface materials==\n[[Image:Spoorspatting Landgraaf.jpg|thumb|Railway tracks in the [[Netherlands]] affected by Sun kink.]]\nBuckling is also a failure mode in [[pavement (material)|pavement]] materials, primarily with concrete, since [[asphalt concrete|asphalt]] is more flexible. [[Radiant heat]] from the [[sun]] is absorbed in the road surface, causing it to [[thermal expansion|expand]], forcing adjacent pieces to push against each other. If the stress is great enough, the pavement can lift up and crack without warning. Going over a buckled section can be very jarring to [[automobile]] drivers, described as running over a [[speed hump]] at highway speeds.\n\nSimilarly, [[rail tracks]] also expand when heated, and can fail by buckling, a phenomenon called '''sun kink'''. It is more common for rails to move laterally, often pulling the underlying [[railroad tie|ties]] (sleepers) along.\n\n===Cause===\nThe buckling [[force]] in the track due to warming up is a [[function (mathematics)|function]] of the rise in temperature only and is independent of the track length:\n\n: <math>F = E A \\alpha_L \\Delta T</math>.\n\nDerivation of buckling force function:\n\nThe linear [[thermal expansion]] due to heating of the track is found using\n: <math>\\Delta L = \\alpha_L \\Delta T \\ L</math>\n\nwhere\n* ΔL, thermal expansion of the rail\n* L, length of the rail/track\n* α, [[coefficient of thermal expansion]]\n* ΔT, increase in temperature\n\nAccording to [[Hooke's law]] the extension due to a force (in the rail) is\n: <math>\\Delta L = \\frac{F}{EA} L</math>\n\nwhere\n* ΔL, extension of the rail/track\n* F, the force extending a rod, here the induced force in the rail\n* E, [[modulus of elasticity]] of rail material (steel)\n* A, cross section of rail\n* L, length of rail\n\nPutting these together gives\n:<math>\\begin{align}\n  \\frac{F}{E A} L &= \\alpha_L \\Delta T \\ L \\Longleftrightarrow \\\\\n                F &= E A \\alpha_L \\Delta T\n\\end{align}</math>\n\n===Accidents===\nThese accidents were deemed to be sun kink related (''more information available at [[List of rail accidents (2000–2009)]]''):\n* April 18, 2002 [[Amtrak]] ''[[Auto-Train]]'' derailment, off [[CSX]] tracks, near [[Crescent City, Florida]].\n* July 29, 2002 [[Amtrak]] ''[[Capitol Limited (Amtrak train)|Capitol Limited]]'' derailment, off [[CSX]] tracks, near [[Kensington, Maryland]].\n* July 8, 2010 CSX train derailment in [[Waxhaw, North Carolina]].\n* July 6, 2012 WMATA [[Washington Metro|Metrorail]] train derailment near [[West Hyattsville station]], [[Maryland]].<ref>{{cite news |last1=Lucero |first1=Kat |title=Misaligned Track from Heat ‘Probable Cause’ In Green Line Derailment |url=http://dcist.com/2012/07/excessive_heat_probable_cause_in_gr.php |accessdate=2019-01-21 |work=DCist |publisher=American University Radio |date=2012-07-07 |archive-url=https://web.archive.org/web/20180204181423/http://dcist.com/2012/07/excessive_heat_probable_cause_in_gr.php |archive-date=2018-02-04 |dead-url=yes }}</ref>\n\n==Energy method==\nOften it is very difficult to determine the exact buckling load in complex structures using the Euler formula, due to the difficulty in determining the constant K. Therefore, maximum buckling load is often approximated using energy conservation and referred to as an energy method in structural analysis.\n\nThe first step in this method is to assume a displacement mode and a function that represents that displacement. This function must satisfy the most important boundary conditions, such as displacement and rotation. The more accurate the displacement function, the more accurate the result.\n\nThe method assumes that the system (the column) is a conservative system in which energy is not dissipated as heat, hence the energy added to the column by the applied external forces is stored in the column in the form of strain energy.\n\n: <math>U_\\text{applied} = U_\\text{strain}</math>\n\nIn this method, there are two equations used (for small deformations) to approximate the \"strain\" energy (the potential energy stored as elastic deformation of the structure) and \"applied\" energy (the work done on the system by external forces).\n\n: <math>\\begin{align}\n   U_\\text{strain} &= \\frac{E}{2} \\int I(x)(w_{xx}(x))^2 \\, dx \\\\\n  U_\\text{applied} &= \\frac{P_\\text{crit}}{2} \\int (w_{x}(x))^2 \\, dx\n\\end{align}</math>\n\nwhere <math>w(x)</math> is the displacement function and the subscripts <math>x</math> and <math>xx</math> refer to the first and second derivatives of the displacement. Energy conservation yields:\n\n==Flexural-torsional buckling==\nFlexural-torsional buckling can be described as a combination of bending and twisting response of a member in compression. Such a deflection mode must be considered for design purposes. This mostly occurs in columns with \"open\" cross-sections and hence have a low torsional stiffness, such as channels, structural tees, double-angle shapes, and equal-leg single angles. Circular cross sections do not experience such a mode of buckling.\n\n==Lateral-torsional buckling==\n[[File:Biegedrillknicken-svg.svg |thumb|upright=1.5|Lateral-torsional buckling of an I-beam with vertical force in center: a) longitudinal view, b) cross section near support, c) cross section in center with lateral-torsional buckling]]\nWhen a simply supported beam is loaded in [[flexure]], the top side is in [[compression (physical)|compression]], and the bottom side is in [[tension (mechanics)|tension]]. If the beam is not supported in the lateral direction (i.e., perpendicular to the plane of bending), and the flexural load increases to a critical limit, the beam will experience a lateral deflection of the compression flange as it buckles locally. The lateral deflection of the compression flange is restrained by the beam web and tension flange, but for an open section the twisting mode is more flexible, hence the beam both twists and deflects laterally in a failure mode known as ''lateral-torsional buckling''. In wide-flange sections (with high lateral bending stiffness), the deflection mode will be mostly twisting in torsion. In narrow-flange sections, the bending stiffness is lower and the column's deflection will be closer to that of lateral bucking deflection mode.\n\nThe use of closed sections such as square [[Hollow structural section|hollow section]] will mitigate the effects of lateral-torsional buckling by virtue of their high [[torsional rigidity]]. \n\n===The modification factor (''C''<sub>''b''</sub>)===\n''C''<sub>''b''</sub> is a modification factor used in the equation for nominal [[flexural strength]] when determining lateral-torsional buckling.  The reason for this factor is to allow for non-uniform moment diagrams when the ends of a beam segment are braced.  The conservative value for ''C''<sub>''b''</sub> can be taken as 1, regardless of beam configuration or loading, but in some cases it may be excessively conservative. ''C''<sub>''b''</sub> is always equal to or greater than 1, never less. For [[cantilever]]s or overhangs where the free end is unbraced, C<sub>b</sub> is equal to 1. Tables of values of ''C''<sub>''b''</sub> for simply supported beams exist.\n\nIf an appropriate value of ''C''<sub>''b''</sub> is not given in tables, it can be obtained via the following formula:\n\n: <math>C_b = \\frac{12.5M_\\max}{2.5M_\\max + 3M_A + 4M_B + 3M_C}</math>\n\nwhere\n: <math>M_\\max</math>, absolute value of maximum moment in the unbraced segment,\n: <math>M_A</math>, absolute value of maximum moment at quarter point of the unbraced segment,\n: <math>M_B</math>, absolute value of maximum moment at centerline of the unbraced segment, \n: <math>M_C</math>, absolute value of maximum moment at three-quarter point of the unbraced segment,\n\nThe result is the same for all unit systems.\n\n==Plastic buckling==\nThe buckling strength of a member is less than the elastic buckling strength of a structure if the material of the member is stressed beyond the elastic material range and into the non-linear (plastic) material behavior range. When the compression load is near the buckling load, the structure will bend significantly and the material of the column will diverge from a linear stress-strain behavior. The stress-strain behavior of materials is not strictly linear even below the yield point, hence the modulus of elasticity decreases as stress increases, and significantly so as the stresses approach the material's yield strength. This reduced material rigidity reduces the buckling strength of the structure and results in a buckling load less than that predicted by the assumption of linear elastic behavior.\n\nA more accurate approximation of the buckling load can be had by the use of the tangent modulus of elasticity, E<sub>t</sub>, which is less than the elastic modulus, in place of the elastic modulus of elasticity. The tangent is equal to the elastic modulus and then decreases beyond the proportional limit. The tangent modulus is a line drawn tangent to the stress-strain curve at a particular value of strain (in the elastic section of the stress-strain curve, the tangent modulus is equal to the elastic modulus). Plots of the tangent modulus of elasticity for a variety of materials are available in standard references.\n\n==Dynamic buckling==\nIf a column is loaded suddenly and then the load released, the column can sustain a much higher load than its static (slowly applied) buckling load. This can happen in a long, unsupported column used as a drop hammer. The duration of compression at the impact end is the time required for a stress wave to travel along the column to the other (free) end and back down as a relief wave. Maximum buckling occurs near the impact end at a wavelength much shorter than the length of the rod, and at a stress many times the buckling stress of a statically-loaded column. The critical condition for buckling amplitude to remain less than about 25 times the effective rod straightness imperfection at the buckle wavelength is\n\n: <math>\\sigma L = \\rho c^2 h</math>\n\nwhere <math>\\sigma</math> is the impact stress, <math>L</math> is the length of the rod, <math>c</math> is the elastic wave speed, and <math>h</math> is the smaller lateral dimension of a rectangular rod. Because the buckle wavelength depends only on <math>\\sigma</math> and <math>h</math>, this same formula holds for thin cylindrical shells of thickness <math>h</math>.<ref>{{cite book |last1=Lindberg |first1=H. E. |last2=Florence |first2=A. L. |title=Dynamic Pulse Buckling |publisher=[[Martinus Nijhoff Publishers]] |date=1987 |pages=11–56, 297–298}}</ref>\n\n==Buckling of thin cylindrical shells subject to axial loads==\nSolutions of Donnell's eight order differential equation gives the various buckling modes of a thin cylinder under compression. But this analysis, which is in accordance with the small deflection theory gives much higher values than shown from experiments. So it is customary to find the critical buckling load for various structures which are cylindrical in shape from empirically based design curves wherein the critical buckling load F<sub>cr</sub> is plotted against the ratio R/t, where R is the radius and t is the thickness of the cylinder for various values of L/R, L the length of the cylinder. If cut-outs are present in the cylinder, critical buckling loads as well as pre-buckling modes will be affected. Presence or absence of reinforcements of cut-outs will also affect the buckling load.\n\n==Buckling of pipes and pressure vessels subject to external overpressure==\nPipes and pressure vessels subject to external overpressure, caused for example by steam cooling within the pipe and condensing into water with subsequent massive pressure drop, risk buckling due to compressive [[hoop stress]]es. Design rules for calculation of the required wall thickness or reinforcement rings are given in various piping and pressure vessel codes.\n\n== Cerebral cortex ==\nThe mechanisms of [[Cerebral cortex|cortical]] [[gyrification]] are just beginning to be understood.<ref>{{Cite journal|last=Kuhl|first=Ellen|title=Biophysics: Unfolding the brain|journal=Nature Physics|volume=12|issue=6|pages=533–534|doi=10.1038/nphys3641|bibcode = 2016NatPh..12..533K|year=2016}}</ref> Mechanical buckling forces due to the expanding [[brain]] tissue probably cause the cortical surface to fold.<ref name=\":2\">{{cite journal|last2=Voets|first2=N|last3=Rua|first3=C|last4=Alexander-Bloch|first4=A|last5=Hough|first5=M|last6=Mackay|first6=C|last7=Crow|first7=TJ|last8=James|first8=A|last9=Giedd|first9=JN|date=August 2014|title=Differential tangential expansion as a mechanism for cortical gyrification.|journal=Cerebral Cortex|volume=24|issue=8|pages=2219–28|doi=10.1093/cercor/bht082|pmid=23542881|last1=Ronan|first1=L|last10=Fletcher|first10=PC|pmc=4089386}}</ref> This is an example of how [[pattern formation]] in nature can also take place due to elastic instabilities<ref>Matsumoto, E. A., & Kamien, R. D. (2009). Elastic-Instability Triggered Pattern Formation. Retrieved from http://repository.upenn.edu/physics_papers/75</ref> instead of the classical [[reaction-diffusion]] mechanism first proposed by Alan Turing.<ref>{{cite journal |first=S. |last=Kondo |first2=T. |last2=Miura |title=Reaction-Diffusion Model as a Framework for Understanding Biological Pattern Formation |journal=[[Science (journal)|Science]] |year=2010 |volume=329 |issue=5999 |pages=1616–1620 |doi=10.1126/science.1179047 }}</ref>\n\n==See also==\n* [[Euler's critical load]]\n* [[Perry Robertson formula]]\n* [[Rail stressing]]\n* [[Stiffening]]\n* [[Wood method]]\n* [[Yoshimura buckling]]\n\n==References==\n{{reflist|30em}}\n\n==Further reading==\n*{{cite book |authorlink=Stephen Timoshenko |last=Timoshenko |first=S. P. |last2=Gere |first2=J. M. |title=Theory of Elastic Stability |edition=2nd |publisher=McGraw-Hill |year=1961 }}\n*{{cite journal |last=Nenezich |first=M. |title=Thermoplastic Continuum Mechanics |journal=Journal of Aerospace Structures |volume=4 |year=2004 }}\n*{{cite thesis |url=http://contrails.iit.edu/DigitalCollection/1970/AFFDLTR70-025.pdf |title=The Stability of Elastic Equilibrium |first=W. T. |last=Koiter |type=PhD Thesis |year=1945 }}\n*{{cite journal |first=Dhakal |last=Rajesh |first2=Koichi |last2=Maekawa |year=2002 |title=Reinforcement Stability and Fracture of Cover Concrete in Reinforced Concrete Members |journal=Journal of Structural Engineering |volume=128 |issue=10 |pages=1253–1262 |doi=10.1061/(ASCE)0733-9445(2002)128:10(1253) }}\n*{{cite book |first=Willian T. |last=Segui |year=2007 |title=Steel Design |edition=Fourth |location=United States |publisher=Thomson |isbn=0-495-24471-6 }}\n*{{cite book |title=Analysis and Design of Flight Vehicle Structures |first=E. F. |last=Bruhn |location=Indianapolis |publisher=Jacobs |year=1973 |isbn= }}\n\n==External links==\n* The complete theory and example experimental results for long columns are available as a 39-page PDF document at http://lindberglce.com/tech/buklbook.htm\n*{{cite web |title=Lateral torsional buckling |url=http://www.midasuser.com.tw/t_support/tech_pds/files/Tech%20Note-Lateral%20Torsional%20Buckling.pdf |archivedate=April 1, 2010 |archiveurl=https://web.archive.org/web/20100401042249/http://www.midasuser.com.tw/t_support/tech_pds/files/Tech%20Note-Lateral%20Torsional%20Buckling.pdf }}\n\n[[Category:Elasticity (physics)]]\n[[Category:Materials science]]\n[[Category:Mechanical failure modes]]\n[[Category:Structural analysis]]\n[[Category:Mechanics]]\n\n[[it:Instabilità delle strutture]]"
    },
    {
      "title": "Cantilever method",
      "url": "https://en.wikipedia.org/wiki/Cantilever_method",
      "text": "{{technical|date=December 2014}}\n\nThe '''[[Cantilever]] method''' is an approximate method for calculating [[shear force]]s and [[moment (physics)|moments]] developed in [[beam (structure)|beam]]s and [[column]]s of a [[structural system|frame]] or [[structure]] due to lateral loads. The applied lateral loads typically include [[wind]] loads and earthquake loads, which must be taken into consideration while designing buildings. The assumptions used in this method are that the points of [[contraflexure]] (or points of inflection of the moment diagram) in both the vertical and horizontal members are located at the midpoint of the member, and that the direct [[stress (mechanics)|stress]]es in the columns are proportional to their distances from the centroidal [[axis (mathematics)|axis]] of the frame.<ref>{{cite book | url=https://books.google.com/books/about/Theory_of_Structures.html?id=7wMkywAACAAJ | title=Theory of Structures | author=S. Ramamrutham}}</ref> The frame is analysed in step-wise (iterative) fashion, and the results can then be described by force [[diagram]]s drawn up at the end of the process. The method is quite versatile and can be used to analyse frames of any number of [[storey]]s or floors.\n\nThe position of the centroidal axis (the [[center of gravity]] line for the frame) is determined by using the areas of the end columns and interior columns. The cantilever method is considered as one of the two primary approximate methods (the other being the [[portal method]]) for indeterminate structural analysis of frames for lateral loads. Its use is recommended for frames that are taller than they are wide, and therefore behave similar to a beam cantilevered up from the ground.\n\n==See also==\n*[[Conjugate beam method]]\n\n==References==\n<references />\n\n[[Category:Civil engineering]]\n[[Category:Structural analysis]]\n\n\n{{architecture-stub}}"
    },
    {
      "title": "Cascade chart (NDI interval reliability)",
      "url": "https://en.wikipedia.org/wiki/Cascade_chart_%28NDI_interval_reliability%29",
      "text": "{{more footnotes|date=January 2018}}\n\n{{Use dmy dates|date=October 2017}}\n{{Orphan|date=May 2017}}\n\n[[File:Cyclic loads.png|thumb|Cyclic loads|300x300px]]A '''cascade chart''' is tool that can be used in [[Damage tolerance|damage tolerance analysis]] to determine the proper inspection interval, based on reliability analysis, considering all the context uncertainties. The chart is called a \"cascade chart\" because the scatter of data points and downward curvature resembles a waterfall or cascade. This name was first introduced by Dr. Alberto W Mello in his work \"Reliability prediction for structures under cyclic loads and recurring inspections\". Materials subject to cyclic loads, as shown in the graph on the right, may form and propagate cracks over time due to [[Fatigue (material)|fatigue]]. Therefore, it is essential to determine a reliable inspection interval. There are numerous factors that must be considered to determine this inspection interval. The [[Nondestructive testing|non-destructive inspection]] (NDI) technique must have a high probability of detecting a crack in the material. If missed, a crack may lead the structure to a catastrophic failure before the next inspection. On the other hand, the inspection interval cannot be too frequent that the structure's maintenance is no longer profitable.\n\n== NDI methods ==\nNDI is a process used to examine materials without causing damage to the structure. The main purpose of using NDI techniques is to comb the surface of a material for small cracks that could affect the integrity of the entire structure. Because the structure is intended to be used again, it is essential that the methods of investigating materials for cracks does not damage the structure in any way.\n\nSome of the most common NDI methods are:\n* [[Eddy current]]\n* [[Ultrasonic testing|Ultrasound]]\n* [[Dye penetrant inspection|Dye penetrant]]\n* [[Industrial radiography|X-ray]]\n* [[Visual inspection|Visual]]\nSome of the techniques are more accurate and can detect small cracks. For example, visual inspection is the least reliable method because the human eye can only resolve and identify cracks on the order of millimeters. The table below shows an important parameter of crack size for each method with a 0% chance of detection (a<sub>0</sub>). This is based on the resolution of each method. This number can be used in a Weibull-like distribution to map the probability of detection as a function of crack size.\n{| class=\"wikitable\" style=\"margin-left: auto; margin-right: auto; border: none;\"\n|+a<sub>0</sub> for various NDI methods\n!Accessibility\n!Ultrasound\n!Dye penetrant\n!X-Ray\n!Visual\n|-\n|Excellent\n|0.508&nbsp;mm\n|0.762&nbsp;mm\n|1.524&nbsp;mm\n|2.54&nbsp;mm\n|-\n|Good\n|1.016&nbsp;mm\n|1.524&nbsp;mm\n|3.048&nbsp;mm\n|5.08&nbsp;mm\n|-\n|Fair\n|2.032&nbsp;mm\n|3.048&nbsp;mm\n|6.096&nbsp;mm\n|10.16&nbsp;mm\n|-\n|Not Easy\n|3.048&nbsp;mm\n|4.572&nbsp;mm\n|9.144&nbsp;mm\n|15.24&nbsp;mm\n|-\n|Difficult\n|4.064&nbsp;mm\n|6.096&nbsp;mm\n|12.19&nbsp;mm\n|20.32&nbsp;mm\n|}\n\nAs the table shows, the minimum detectable parameter increases from the ultrasound method to the visual method and from excellent accessibility to difficult accessibility. In any case, it is important to have a maintenance plan that allows multiple opportunities to find a crack that may be small and difficult to access.\n\n== Cascade chart ==\nA cascade chart is an alternative way from the traditional [[Damage tolerance|damage tolerance analysis]] (DTA) methodology for determining a reliable inspection interval. It uses the scatter from crack growth simulations, uncertainty in material properties, and probability of detection distribution to determine the NDI interval, given a desired cumulative probability of detection under a given confidence level.\n\n=== Probability of detection ===\nThe probability of detection (POD), a function of the NDI method, accessibility, and crack size, can be modeled by the equation below.\n\n<math>p_{od} = 1 - e^{-1\\{(a-a_0)/(\\lambda-a_0)\\}^\\alpha}</math>\n\nIn this equation, ''a<sub>0</sub>'' is defined as the crack size below which detection is impossible. ''α'', and ''λ'', on the other hand, are parameters related to the chosen NDI method that determine the shape of the probability curve. The number of inspections of a structure is directly related to the probability of a detecting a crack in that structure. The more chances that an inspector has to find the crack, the more likely he or she will be to find the crack and prevent further damage to the structure. The equation below describes the total probability of detecting a crack based on each individual inspection's probability.\n\n<math>p = 1 - \\Pi_{i=1}^n(1-p_i)</math>\n\nThe variable ''p<sub>i</sub>'' represents the probability of detection for each crack size, and the variable ''n'' represents the number of inspections conducted. Due to all the factors that play a role in determining the probability of detection, there will '''always''' be a '''non-zero probability''' that a crack will be missed, no matter what NDI method is used to inspect the structure.\n\n=== Creating the chart ===\n[[File:Crack growth.png|thumb|Example Crack Growth Curve|300x300px]]The process for creating the cascade chart shown on the right begins with modeling the crack growth over a time interval, number of cycles, or number of flight hours.\n\nBased on an initial crack size, ''a<sub>i</sub>'', the crack growth curve can vary significantly, causing the crack to reach its critical size in different lengths of time. This contributes to the scatter of the cascade chart. Based on the manufacturing of different materials, the example considers a typical minimum flaw in a material as about 0.127&nbsp;mm (0.005\"). Knowing that the new structures are deeply inspected before being put in service, the example considers a maximum undetectable crack size as about 1.27&nbsp;mm (0.05\") for a new structure. To simulate the variation of possible initial crack sizes, the [[Monte Carlo method|Monte Carlo]] simulation method was used to randomly generate values between the given limits. In addition, the method randomly generated parameters for the crack growth curve. Based on typical variation of material properties, the constants ''C'' and ''m'' in the equation below can be varied to represent different crack growth rates. Uncertainties in loads and geometric factors affecting the stress intensity factor can also be incorporated to simulate different crack growth curves.\n\n<math>\\frac{da}{dN} = C(\\Delta K)^m</math>\n\nThe probability of detection distribution curve for a chosen NDI method is superimposed to the crack growth curve, and the inspection interval is systematically changed to compute the cumulative probability of detection for a crack growing from the minimum to the critical size. The simulation is repeated several times, and a distribution of inspection interval versus structural reliability can be formed. To refine the randomization of the values, the [[Latin hypercube sampling|Latin Hypercube]] procedure was also introduced.\n\nAs it is clear in the chart, the scatter in NDI decreases as the intervals are reduced and reliability is increased. It is important to emphasize that several sources of uncertainties can be included in the simulations, such as variation in material properties, the machining quality, the inspection methods, and accessibility of the crack. In the cascade chart, the reliability curve is presented with scatter (i.e. not every point is well defined by the negative quadratic curve). Therefore, it is necessary to make use of a confidence interval.\n\n=== Using the chart ===\nThere are two variables that play a role in the selection of the inspection interval using the cascade chart. These variables are the probability of detection over the lifetime of the structure and the confidence level for the stated probability. As one of the graph's axes is probability, it is fairly easy to find the data points that match a specific probability. In aerospace structural analysis, it is common to consider 99.9999% probability (0.0001% risk) to be '''improbable''' and 99.99999% probability (0.00001% risk) to be '''extremely improbable'''. Then, the confidence interval is used to select a point where a specified percentage of the data points lies to the right of the selected point. For example, a 95% confidence interval means that 95% of the simulated cases must fall to the right of this point. This specific point is marked, and the respective point on the x-axis represents the suggested inspection interval. Furthermore, to derive the estimated risk per flight hour, the risk percentage can be divided by the number of flight hours described in the inspection interval. Hopefully, using this process, the inspection interval will lead to a higher percentage of cracks being detected before failure, ensuring greater flight safety. A final important observation is that improving the NDI method can increase the number of flight hours needed before re-inspection while maintaining a relatively low risk level.\n\n== See also ==\n* [[Fracture mechanics]]\n* [[Nondestructive testing]]\n* [[Monte Carlo method]]\n* [[Latin hypercube sampling]]\n\n== References ==\n{{refbegin}}\n* Jr, Alberto W. S. Mello, and Daniel Ferreira V. Mattos. \"Reliability Prediction for Structures under Cyclic Loads and Recurring Inspections.\" Journal of Aerospace Technology and Management, vol. 1, no. 2, 28 October 2009, pp.&nbsp;201–209., doi:10.5028/jatm.2009.0102201209. Accessed 9 April 2017.\n* ASM Handbook, 1992, “Failure analysis and prevention”, 9. Ed., Materials Park, OH, (ASM International, vol. 11), pp.&nbsp;15–46.\n* Broek, D., 1989, “The practical use of fracture mechanics”. Galena, OH. Kluwer Academic, pp.&nbsp;361–390.\n* Gallagher J. P., 1984, “USAF damage tolerant design handbook: Guidelines for the analysis and design of damage tolerant aircraft structures”, Dayton Research Institute, Dayton, OH, pp.&nbsp;1.2.5–1.2.13.\n* IFI, 2005, “Análise e gerencialmento de riscos nos vôos de certificação”, MPH-830, Instituto de Fomento à Indústria. Divisão de Certficação de Aviação Civil, São José dos Campos, S.P., Brasil.\n* Knorr, E., 1974, “Reliability of the detection of flaws and of the determination of the flaw size”, AGARDograph, Quebec, No. 176, pp.&nbsp;398–412.\n* Lewis W. H. et al., 1978, “Reliability of non-destructive inspection”, SA-ALC/MME. 76-6-38-1, San Antonio, TX.\n* Manuel, L., 2002, “CE 384S – Structural reliability course: Class notes”, Department of Civil Engineering, The University of Texas at Austin, Austin, TX.\n* Mattos, D. F. V. et al., 2009, “F-5M DTA Program”. Journal of Aerospace Technology and Management. Vol. 1, No1, pp.&nbsp;113–120.\n* Mello Jr, A. W. S. et al., 2009, “Geração do ciclo de tensões para análise de fadiga, Software GCTAF F-5M”, RENG ASA-I 04/09, IAE, São José dos Campos, S.P., Brasil.\n* Provan, J. W., 2006, “Fracture, fatigue and mechanical reliability: An introduction to mechanical reliability”, Department of Mechanical Engineering, University of Victoria, Victoria, B.C.\n* USAF., 1974, “Airplane damage tolerance requirement”. Military Specification. Washington, DC. (MIL-A-83444).\n* USAF., 1974, “Airplane strength and rigidity reliability requirements, repeated loads and fatigue”. Military Specification. Washington, DC. (MIL-A-008866).\n* USAF., 2005, “Aircraft structural integrity program, airplane requirements”. Military Specification. Washington, DC. (MIL-STD-1530C).\n{{Refend}}\n\n[[Category:Reliability analysis]]\n[[Category:Fracture mechanics]]\n[[Category:Structural analysis]]\n[[Category:Nondestructive testing]]"
    },
    {
      "title": "Castigliano's method",
      "url": "https://en.wikipedia.org/wiki/Castigliano%27s_method",
      "text": "\n'''Castigliano's method''', named for [[Carlo Alberto Castigliano]], is a method for determining the displacements of a [[Linear elasticity|linear-elastic]] system based on the [[partial derivative]]s  of the [[energy]]. He is known for his two theorems. The basic concept may be easy to understand by recalling that a change in energy is equal to the causing force times the resulting displacement. Therefore, the causing force is equal to the change in energy divided by the resulting displacement. Alternatively, the resulting displacement is equal to the change in energy divided by the causing force. Partial derivatives are needed to relate causing forces and resulting displacements to the change in energy. \n* '''Castigliano's first theorem''' &ndash; for forces in an elastic structure\nCastigliano's method for calculating forces is an application of his first theorem, which states:\n:''If the strain energy of an elastic structure can be expressed as a function of generalised displacement q<sub>i</sub> then the partial derivative of the strain energy with respect to generalised displacement gives the generalised force Q<sub>i</sub>.''\nIn equation form,\n:<math>Q_i=\\frac{\\partial \\mathbf{U}}{\\partial q_i}</math>\nwhere U is the strain energy.\n\nIf the force-displacement curve is nonlinear then the complementary strain energy needs to be used instead of strain energy. <ref> History of Strength of Materials, Stephen P. Timoshenko, 1993, Dover Publications, New York</ref>\n\n\n* '''Castigliano's second theorem''' &ndash; for displacements in a linearly elastic structure.\nCastigliano's method for calculating displacements is an application of his second theorem, which states:\n:''If the strain energy of a linearly elastic structure can be expressed as a function of generalised force Q<sub>i</sub> then the partial derivative of the strain energy with respect to generalised force gives the generalised displacement q<sub>i</sub> in the direction of Q<sub>i</sub>.''\n\nAs above this can also be expressed as:\n\n:<math>q_i=\\frac{\\partial \\mathbf{U}}{\\partial Q_i}.</math>\n\n==Examples==\nFor a thin, straight cantilever beam with a load P at the end, the displacement <math>\\delta</math> at the end can be found by Castigliano's second theorem :\n\n:<math>\\delta = \\frac{\\partial \\mathbf{U}}{\\partial P}</math>\n\n:<math>\\delta = \\frac{\\partial}{\\partial P}\\int_0^L{\\frac{M^2(x)}{2EI}dx}\n= \\frac{\\partial}{\\partial P}\\int_0^L{\\frac{(Px)^2}{2EI}dx} </math>\n\nwhere E is Young's Modulus and I is the second moment of area of the cross-section, and M(x)=P x  is the expression for the internal moment at a point at distance x from the end, therefore:\n\n:<math>= \\int_0^L{\\frac{P x^2}{EI}dx}</math>\n\n:<math>= \\frac{PL^3}{3EI}.</math>\n\nThe result is the standard formula given for cantilever beams under end loads.\n\n==External links==\n* [http://www-groups.dcs.st-and.ac.uk/~history/Mathematicians/Castigliano.html Carlo Alberto Castigliano]\n* [http://www.matheplanet.com/matheplanet/nuke/html/article.php?sid=1008&mode=&order=0 Castigliano's method: some examples]{{de icon}}\n\n==References==\n{{Reflist}}\n\n[[Category:Structural analysis]]"
    },
    {
      "title": "Clapeyron's theorem (elasticity)",
      "url": "https://en.wikipedia.org/wiki/Clapeyron%27s_theorem_%28elasticity%29",
      "text": "{{Redirect|Clapeyron's theorem|Clapeyron's theorem used in bridge engineering|Theorem of three moments}}\n\nIn the linear theory of [[elasticity (physics)|elasticity]] '''Clapeyron's theorem''' states that the potential energy of deformation of a body, which is in equilibrium under a given load, is equal to half the work done by the external forces computed assuming these forces had remained constant from the initial state to the final state.<ref>Love, A.E.H., \"A Treatise on the Mathematical Theory of Elasticity\", 4th ed. Cambridge, 1927, p. 173</ref>\n\nIt is named after the French scientist [[Benoît Paul Émile Clapeyron|Benoît Clapeyron]].\n\nFor example consider a linear spring with initial length ''L''<sub>0</sub> and gradually pull on the spring until it reaches equilibrium at a length ''L''<sub>1</sub> when the pulling force is&nbsp;''F''.  By the theorem, the potential energy of deformation in the spring is given by: \n\n:<math>\\frac{1}{2}F (L_1 - L_0).</math>\n\nThe actual force increased from 0 to ''F'' during the deformation; the work done can be computed by integration in distance. Clapeyron's equation, which uses the final force only, may be puzzling at first, but is nevertheless true because it includes a corrective factor of one half.\n\nAnother theorem, the [[theorem of three moments]] used in bridge engineering is also sometimes called Clapeyron's theorem.\n\n==References==\n{{reflist}}\n* Roger Fosdick & Lev Truskinovsky (2003) [https://link.springer.com/article/10.1023/B:ELAS.0000018757.56138.f3 About Clapeyron's Theorem in Linear Elasticity], [[Journal of Elasticity]] 72(1–3): 145–72, Springer.\n\n\n{{DEFAULTSORT:Clapeyron's Theorem (Elasticity)}}\n[[Category:Structural analysis]]\n[[Category:Continuum mechanics]]\n[[Category:Physics theorems]]\n\n{{classicalmechanics-stub}}"
    },
    {
      "title": "Computers and Structures",
      "url": "https://en.wikipedia.org/wiki/Computers_and_Structures",
      "text": "{{Infobox company\n| name      =  Computers and Structures, Inc.\n| logo      = Fair use image of CSI circular logo.PNG\n| logo_size = 200px\n| foundation        =  [[Berkeley, California]], U.S. (1975)<br><small>([[headquarters]])</small>\n| key_people        =  [[Ashraf Habibullah]], [[Structural Engineer|S.E.]]<br><small>(Founder, President, and CEO)</small>\n| industry          =  [[Structural Engineering]] <br> [[Earthquake Engineering]] <br> [[Software]]\n| products          =  SAP2000 <br> CSiBridge <br> ETABS <br> SAFE <br> PERFORM-3D <br> CSiCOL\n| homepage          =  [http://www.csiamerica.com/ csiamerica.com]\n}}\n\n'''Computers and Structures, Inc.''' (CSI) is a [[structural engineering|structural]] and [[earthquake engineering|earthquake]] engineering software company founded in 1975 <ref>{{cite web |url=http://www.csiamerica.com/ |title=Computers and Structures, Inc. official website |author=Computers and Structures, Inc. |year=2011 |work=Official website summary |publisher=Computers and Structures, Inc. |accessdate=December 4, 2011}}</ref> and based in [[Walnut Creek, California]] with additional office location in [[New York City|New York]].<ref>{{Google maps | url = https://maps.google.com/maps?q=computers+and+structures+inc.+office+locations&hl=en&sll=36.879621,169.453125&sspn=155.459218,316.054688&vpsrc=0&hq=computers+and+structures+inc.+office&t=m&z=2 | title = Computers and Structures, Inc. office locations | accessdate = December 5, 2011}}</ref> The structural [[structural analysis|analysis]] and [[structural design|design]] software CSI produce include SAP2000, CSiBridge, ETABS, SAFE, PERFORM-3D, and CSiCOL.\n\nOne of Computer and Structure, Inc.'s software, ETABS, was used to create the mathematical model of the [[Burj Khalifa]], currently the world's tallest building, designed by [[Chicago, Illinois]]-based [[Skidmore, Owings & Merrill]] LLP (SOM).<ref name=\"baker\">{{Citation | last = Baker, S.E.| first = William F.|last2 = Pawlikowski, S.E. | first2 = James J. | year = 2009 | title = ''Design and construction of the world's tallest building: The Burj Dubai'' | publisher = Structural Engineer | publication-place = Fayetteville, AR | url = http://www.gostructural.com/magazine-article-gostructural.com-12-2009-design_and_construction_of_the_world_acute_s_tallest_building__the_burj_dubai-7709.html | accessdate = December 16, 2011}}</ref> In the ''Structural analysis'' section of their December 2009 Structural Engineer magazine article entitled ''Design and construction of the world's tallest building: The Burj Dubai'', since renamed to [[Burj Khalifa]], William F. Baker, S.E. and James J. Pawlikowski, S.E. mention that gravity, wind, and seismic response were all characterized using ETABS. Further, ETABS' geometric nonlinear capability provided for [[P-Delta Effect]] consideration.<ref name=\"baker\"/>\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* {{Official website|http://www.csiamerica.com/|Computers and Structures, Inc. official website}}\n\n[[Category:Structural engineering]]\n[[Category:Earthquake engineering]]\n[[Category:Structural analysis]]\n[[Category:Construction software]]\n[[Category:Companies based in Contra Costa County, California]]\n[[Category:1975 establishments in California]]"
    },
    {
      "title": "Conjugate beam method",
      "url": "https://en.wikipedia.org/wiki/Conjugate_beam_method",
      "text": "[[File:elastic load method (full).png|thumb|right|300px|(0) real beam, (1) shear and moment, (2) conjugate beam, (3) slope and displacement]]\n\n'''Conjugate beam''' is defined as the imaginary beam with the same dimensions (length) as that of the original beam but load at any point on the conjugate beam is equal to the bending moment at that point divided by EI.<ref>{{cite book | url=https://books.google.com/books?id=2IHEqp8dNWwC&pg=PT602&lpg=PT602&dq=conjugate+beam+definition&source=bl&ots=TlkHnonCPY&sig=ckL-295uva9j2r8FbkZImGv6ACw&hl=en&sa=X&ei=sl9tVOnkHtGkuQS-h4DoCA&ved=0CDYQ6AEwBQ#v=onepage&q=conjugate%20beam%20definition&f=false | title=Strength of materials | accessdate=20 November 2014}}</ref>\nThe '''conjugate-beam method''' is an engineering method to derive the slope and displacement of a beam.  The conjugate-beam method was developed by H. Müller-Breslau in 1865.  Essentially, it requires the same amount of computation as the [[Moment-Area Theorem|moment-area]] theorems to determine a beam's slope or deflection; however, this method relies only on the principles of statics, so its application will be more familiar.<ref name=\"Hibbeler 2009 328–335\">{{cite book|last=Hibbeler|first=R.C.|title=Structural Analysis|year=2009|publisher=Pearson|location=Upper Saddle River, NJ|pages=328–335}}</ref>\n\nThe basis for the method comes from the similarity of Eq. 1 and Eq 2 to Eq 3 and Eq 4.  To show this similarity, these equations are shown below.\n\n{| class=\"wikitable\"\n|-\n| <math>Eq.1\\;\\frac{dV}{dx}=w</math> || <math>Eq.3\\;\\frac{d^2 M}{dx^2}=w</math> \n|-\n| <math>Eq.2\\;\\frac{d\\theta}{dx}=\\frac{M}{EI}</math> || <math>Eq.4\\;\\frac{d^2 v}{dx^2}=\\frac{M}{EI}</math>\n|}\n\nIntegrated, the equations look like this.\n\n{| class=\"wikitable\"\n|-\n| <math>V=\\int w\\,dx</math> || <math>M=\\int\\left[\\int w\\,dx\\right]dx</math>\n|-\n| <math>\\theta=\\int\\left(\\frac{M}{EI}\\right)dx</math> || <math>v=\\int\\left[\\int\\left(\\frac{M}{EI}\\right)dx\\right]dx</math>\n|}\n\nHere the [[shear stress|shear]] V compares with the [[slope]] θ, the [[moment of inertia|moment]] M compares with the [[Angular displacement|displacement]] v, and the external load w compares with the M/EI diagram.  Below is a shear, moment, and deflection diagram.  A M/EI diagram is a moment diagram divided by the beam's [[Young's modulus]] and [[moment of inertia]].\n\nTo make use of this comparison we will now consider a beam having the same length as the real beam, but referred here as the \"conjugate beam.\"  The conjugate beam is \"loaded\" with the M/EI diagram derived from the load on the real beam.  From the above comparisons, we can state two theorems related to the conjugate beam:<ref name=\"Hibbeler 2009 328–335\"/>\n\nTheorem 1: The slope at a point in the real beam is numerically equal to the shear at the corresponding point in the conjugate beam.\n\nTheorem 2: The displacement of a point in the real beam is numerically equal to the moment at the corresponding point in the conjugate beam.<ref name=\"Hibbeler 2009 328–335\"/>\n\n==Conjugate-beam supports==\n\nWhen drawing the conjugate beam it is important that the shear and moment developed at the supports of the conjugate beam account for the corresponding slope and displacement of the real beam at its supports, a consequence of Theorems 1 and 2.  For example, as shown below, a pin or roller support at the end of the real beam provides zero displacement, but a non zero slope.  Consequently, from Theorems 1 and 2, the conjugate beam must be supported by a pin or a roller, since this support has zero moment but has a shear or end reaction.  When the real beam is fixed supported, both the slope and displacement are zero.  Here the conjugate beam has a free end, since at this end there is zero shear and zero moment.  Corresponding real and conjugate supports are shown below.  Note that, as a rule, neglecting axial forces, [[statically determinate]] real beams have statically determinate conjugate beams; and [[statically indeterminate]] real beams have unstable conjugate beams.  Although this occurs, the M/EI loading will provide the necessary \"equilibrium\" to hold the conjugate beam stable.<ref name=\"Hibbeler 2009 328–335\"/>\n\n{| class = \"wikitable\"\n|+ Real support vs Conjugate support<ref name=\"okamura_171\">[[#okamura|Okmamura (1988)]]、p.171。</ref>\n! colspan=\"2\" | Real beam\n! colspan=\"2\" | Conjugate beam\n|-\n! Fixed support\n| rowspan=\"2\" | [[File:fixed support.svg|150px]]\n! Free end\n| rowspan=\"2\" | [[File:free end.svg|150px]]\n|-\n|\n* <math>v = 0</math>\n* <math>\\theta = 0</math>\n|\n* <math>\\overline M = 0</math>\n* <math>\\overline Q = 0</math>\n|-\n! Free end\n| rowspan=\"2\" | [[File:free end.svg|150px]]\n! Fixed support\n| rowspan=\"2\" | [[File:fixed support.svg|150px]]\n|-\n|\n* <math>v \\not= 0</math>\n* <math>\\theta \\not= 0</math>\n|\n* <math>\\overline M \\not= 0</math>\n* <math>\\overline Q \\not= 0</math>\n|-\n! Hinged support\n| rowspan=\"2\" | [[File:hinged support.svg|150px]]\n! Hinged support\n| rowspan=\"2\" | [[File:hinged support.svg|150px]]\n|-\n|\n* <math>v = 0</math>\n* <math>\\theta \\not= 0</math>\n|\n* <math>\\overline M = 0</math>\n* <math>\\overline Q \\not= 0</math>\n|-\n! Middle support\n| rowspan=\"2\" | [[File:middle support.svg|150px]]\n! Middle hinge\n| rowspan=\"2\" | [[File:middle hinge.svg|150px]]\n|-\n|\n* <math>v = 0</math>\n* <math>\\theta</math>:continue\n|\n* <math>\\overline M = 0</math>\n* <math>\\overline Q</math>:continue\n|-\n! Middle hinge\n| rowspan=\"2\" | [[File:middle hinge.svg|150px]]\n! Middle support\n| rowspan=\"2\" | [[File:middle support.svg|150px]]\n|-\n|\n* <math>v</math>:continue\n* <math>\\theta</math>:discontinue\n|\n* <math>\\overline M</math>:continue\n* <math>\\overline Q</math>:discontinue\n|}\n\n{| class = \"wikitable\"\n|+ Examples of conjugate beam<ref name=\"okamura_171\" />\n! colspan=\"2\" | Real beam\n! Conjugate beam\n|-\n! Simple beam\n| [[File:simple beam.svg|300px]]\n| [[File:simple beam.svg|300px]]\n|-\n! Cantilever beam\n| [[File:cantilever beam (left supported).svg|300px]]\n| [[File:cantilever beam (right supported).svg|300px]]\n|-\n! Left-end Overhanging beam\n| [[File:left end overhanging beam.svg|300px]]\n| [[File:fixed-hinge-support beam.svg|300px]]\n|-\n! Both-end overhanging beam\n| [[File:both end overhanging beam.svg|300px]]\n| [[File:Both end fixed and 2 middle hinged beam.svg|300px]]\n|-\n! Gerber's beam (2 span)\n| [[File:2 spans Gerber's beam (left hinged).svg|300px]]\n| [[File:2 spans Gerber's beam (right hinged).svg|300px]]\n|-\n! Gerber's beam (3 span)\n| [[File:3 spans Gerber's beam (support-support-hinge).svg|300px]]\n| [[File:3 spans Gerber's beam (support-hinge-support).svg|300px]]\n|}\n\n==Procedure for analysis==\nThe following procedure provides a method that may be used to determine the [[Angular displacement|displacement]] and [[Deflection (engineering)|deflection]] at a point on the elastic curve of a beam using the conjugate-beam method.\n\n===Conjugate beam===\n* Draw the conjugate beam for the real beam. This beam has the same length as the real beam and has corresponding supports as listed above.\n* In general, if the real support allows a slope, the conjugate support must develop [[shear stress|shear]]; and if the real support allows a displacement, the conjugate support must develop a [[moment of inertia|moment]].\n* The conjugate beam is loaded with the real beam's M/EI diagram. This loading is assumed to be distributed over the conjugate beam and is directed upward when M/EI is positive and downward when M/EI is negative. In other words, the loading always acts away from the beam.<ref name=\"Hibbeler 2009 328–335\"/>\n\n===Equilibrium===\n* Using the equations of [[statics]], determine the reactions at the conjugate beams supports.\n* Section the conjugate beam at the point where the slope θ and displacement Δ of the real beam are to be determined. At the section show the unknown shear V' and M' equal to θ and Δ, respectively, for the real beam. In particular, if these values are positive, and slope is counterclockwise and the displacement is upward.<ref name=\"Hibbeler 2009 328–335\"/>\n\n==See also==\n*[[Cantilever method]]\n\n== References ==\n*{{Cite book\n |author    = OKAMURA Koichi岡村宏一\n |year      = 1988\n |title     = Kouzou kougaku (I) Doboku kyoutei sensyo\n |publisher = Kashima syuppan\n |isbn      = 4-306-02225-0\n |ref       = okamura\n }}\n\n{{Reflist|2}}\n\n[[Category:Mechanics]]\n[[Category:Structural analysis]]"
    },
    {
      "title": "Crumpling",
      "url": "https://en.wikipedia.org/wiki/Crumpling",
      "text": "{{redirects here|Crumpled|the deformation feature|Crumple zone}}\nIn [[geometry]] and [[topology]], '''crumpling''' is the process whereby a sheet of paper or other two-dimensional [[manifold]] undergoes disordered [[deformation (engineering)|deformation]] to yield a three-dimensional structure comprising a random network of [[Face_(geometry)#Ridge_or_.28n-2.29-face|ridge]]s and [[facet]]s with variable density. The geometry of crumpled structures is the subject of some interest the mathematical community within the discipline of topology.<ref>{{cite journal| last1=Cerda | first1=Enrique | last2=Chaieb |first2=Sahraoui |last3=Melo | first3=Francisco| last4=Mahadevan| first4=L |title=Conical dislocations in crumpling | journal= nature | year= 1999| volume=401| doi=10.1038/43395| bibcode=1999Natur.401...46C}}</ref> Crumpled paper balls have been studied and found to exhibit surprisingly complex structures with [[compressive strength]] resulting from [[friction]]al interactions at [[Local property|locally]] flat facets between folds.<ref>{{cite journal |last1=Cambou| first1=Anne Dominique |last2=Narayanan | first2=Menon| title=Three-dimensional structure of a sheet crumpled into a ball.| journal= Proceedings of the National Academy of Sciences |year=2011 | volume=108 |issue=36 |pages=14741–14745|arxiv=1203.5826|bibcode=2011PNAS..10814741C|doi=10.1073/pnas.1019192108|pmc=3169141}}</ref> The unusually high compressive strength of crumpled structures relative to their density is of interest in the disciplines of [[materials science]] and [[mechanical engineering]].\n\n== Significance of crumpling ==\n\nThe packing of a sheet by crumpling is a complex phenomenon that depends on material parameters and the packing protocol. Thus the crumpling behaviour of foil, paper and poly-membranes differs significantly and can be interpreted on the basis of material foldability.<ref>{{cite journal |last1=Habibi| first1=M |last2=Bonn | first2=D| title=Effect of the material properties on the crumpling of a thin sheet.| journal= Soft matter |year=2017 | volume=3 |pages=4029}}</ref> The high compressive strength exhibited by dense crumple formed [[cellulose]] paper is of interest towards impact dissipation applications and has  been proposed as an approach to [[Paper recycling|utilising waste paper]].<ref>{{cite journal| title=Mechanical properties in crumple-formed paper derived materials subjected to compression| journal= Heilyon |year=2017 | volume=3 |issue=6| pages=e00329 | url= http://www.heliyon.com/article/e00329/}} </ref>\n\n==References==\n{{Reflist}}\n\n{{geometry-stub}}\n\n[[Category:Topology]]\n[[Category:Manifolds]]\n[[Category:Deformation (mechanics)]]\n[[Category:Structural analysis]]\n[[Category:Materials science]]\n[[Category:Mechanical engineering]]"
    },
    {
      "title": "Discontinuity layout optimization",
      "url": "https://en.wikipedia.org/wiki/Discontinuity_layout_optimization",
      "text": "[[Image:Dlo.png|thumb|right| Failure mechanism of an embedded foundation (identified using DLO)]]\n\n'''Discontinuity layout optimization''' (DLO) is an [[engineering analysis]] procedure which can be used to directly establish the amount of [[Structural load|load]] that can be carried by a solid or structure prior to collapse. Using DLO the layout of failure planes, or 'discontinuities', in a collapsing solid or structure are identified using [[mathematical optimization]] methods (hence the name, 'discontinuity layout optimization'). It is assumed that failure occurs in a ductile or '[[Plasticity (physics)|plastic]]' manner.\n\n==How it works==\nThe DLO procedure involves a number of steps, as outlined below.\n\n[[File:DLO steps (plane strain).png|850px|Explains steps involved when analysing a solid body using discontinuity layout optimization (DLO)]]\n\nThe set of potential discontinuities can include discontinuities which crossover one another, allowing complex failure patterns to be identified (e.g. involving ‘fan’ mechanisms, where many discontinuities radiate from a point).\n\nDLO can be formulated in terms of equilibrium relations ('static' formulation) or in terms of displacements ('kinematic' formulation). In the latter case the objective of the mathematical optimization problem is to minimize the internal energy dissipated along discontinuities, subject to nodal compatibility constraints. This can be solved using efficient [[linear programming]] techniques and, when combined with an algorithm originally developed for truss layout optimization problems,<ref>Gilbert, M. and Tyas, A. (2003) Layout optimization of large-scale pin-jointed frames, Engineering Computations, Vol. 20, No. 8, pp.1044-1064</ref> it has been found that modern computer power can be used to directly search through very large numbers of different failure mechanism topologies (up to approx. 2<sup>1,000,000,000</sup> different topologies on current generation PCs). A full description of the application of DLO to plane strain problems has been provided by Smith and Gilbert,<ref>Smith, C.C. and Gilbert, M. (2007) Application of discontinuity layout optimization to plane plasticity problems, Proc. Royal Society A, Volume 463, Number 2086, pp.2461-2484.</ref> to masonry arch analysis by Gilbert et al,<ref>Gilbert, M., Smith, C.C. and Pritchard, T.J. (2010) Masonry arch analysis using discontinuity layout optimisation. ICE-Engineering and Computational Mechanics, Volume 163, pp.167-178.</ref> to slab problems by Gilbert et al,<ref>Gilbert, M., He, L., Smith, C.C. and Le, C.V. (2014) Automatic yield-line analysis of slabs using discontinuity layout optimization. Proceedings Royal Society A, Volume 470, paper 20140071.</ref><ref>He, L., Gilbert, M. and Shepherd, M. (2017) Automatic yield-line analysis of practical slab configurations via discontinuity layout optimization. Journal of Structural Engineering, DOI:10.1061/(ASCE)ST.1943-541X.0001700</ref><ref>He, L. and Gilbert, M. (2016) Automatic rationalization of yield-line patterns identified using discontinuity layout optimization. International Journal of Solids and Structures, Volume 84, pp.27-39.</ref> and to 3D problems by Hawksbee et al,<ref>Hawksbee, S., Smith, C.C. and Gilbert, M. (2013) Application of discontinuity layout optimization to three-dimensional plasticity problems. Proceedings Royal Society A, Volume 469, paper 20130009.</ref> and Zhang.<ref>Zhang, Y. (2017) Multi-slicing strategy for the three-dimensional discontinuity layout optimization (3D DLO). International Journal for Numerical and Analytical Methods in Geomechanics, Volume 41, pp.488-507.</ref>\n\n==DLO vs FEM==\nWhereas with [[finite element analysis]] (FEM), a widely used alternative [[engineering analysis]] procedure, mathematical relations are formed for the underlying continuum mechanics problem, DLO involves analysis of a potentially much simpler discontinuum problem, with the problem being posed entirely in terms of the individual discontinuities which interconnect nodes laid out across the body under consideration. Additionally, when general purpose finite element programs are used to analyse the collapse state often relatively complex [[non-linear]] solvers are required, in contrast to the simpler [[linear programming]] solvers generally required in the case of DLO.\n\nCompared with non-linear FEM, DLO has the following advantages and disadvantages:\n\n'''Advantages'''\n*The collapse state is analysed directly, without the need to iterate. This means that solutions can generally be obtained much more quickly.\n*The output, in the form of animated failure mechanisms is generally easier to interpret.\n*Problems involving singularities in the stress or displacement fields can be dealt with without difficulty.\n*As DLO is much simpler than non-linear FEM, users require less training in order to use the method effectively.\n\n'''Disadvantages'''\n*As with other [[limit analysis]] techniques, DLO provides no information about displacements (or stresses) prior to collapse.\n*DLO is fundamentally based in modelling [[Compatibility (mechanics)|compatible]] mechanisms for soil collapse and is therefore an [[limit analysis|upper bound method]]. As a result, the method will always predict an unconservative collapse load.\n*Although the discontinuity layout generation and [[linear programming]] optimization schemes used in DLO will usually ensure that a good approximation of the true collapse mechanism is found, there is no way of discerning by how much the predicted collapse load will exceed the true collapse load without comparison to an independent [[limit analysis|lower bound analysis]].\n*DLO is a relatively new technique so only a limited range of software tools are currently available.\n\n==Applications==\nDLO is perhaps most usefully applied to engineering problems where traditional hand calculations are difficult, or simplify the problem too much, but where recourse to more complex non-linear FEM is not justified. Applications include:\n* Analysis of [[Geotechnical| geotechnical engineering]] problems (e.g. [[slope stability]], [[bearing capacity]]<ref>Lee, Y.S., Smith C.C. and Cheuk C.Y. (2008) Bearing capacity of embedded foundations. In 2nd International Conference on Foundations, ICOF 2008, Dundee, pp.961-972.</ref> or [[retaining wall]] problems).\n* Analysis of [[concrete slab]] problems.\n* Analysis of [[metal forming]] or [[extrusion]] problems.\n\n==Software using Discontinuity Layout Optimization==\n* [http://cmd.sheffield.ac.uk/software/matlab-script-dlo-analysis-geotechnical-problems MATLAB script] (2009-) Provided by the CMD research group at the University of Sheffield, UK.\n* [http://www.limitstate.com/geo LimitState:GEO] (2008-) General purpose geotechnical software application.\n* [http://www.limitstate.com/slab LimitState:SLAB] (2015-) Slab analysis software application.\n\n==References==\n<!--- See [[Wikipedia:Footnotes]] on how to create references using <ref></ref> tags which will then appear here automatically -->\n{{Reflist}}\n\n==External links==\n* [http://www.geotech.group.shef.ac.uk/teaching/ DLO teaching resources] provided by the Geotechnical Engineering Research Group at the University of Sheffield, UK.\n\n{{DEFAULTSORT:Discontinuity Layout Optimization}}\n[[Category:Structural analysis]]\n[[Category:Geotechnical engineering]]"
    },
    {
      "title": "Dynamic substructuring",
      "url": "https://en.wikipedia.org/wiki/Dynamic_substructuring",
      "text": "{{Orphan|date=June 2016}}\n\n'''Dynamic Substructuring''' (DS) is an [[engineering]] tool used to [[Physical model|model]] and [[Analysis|analyse]] the [[Dynamics (mechanics)|dynamics]] of [[mechanical system]]s by means of its components or substructures. Using the dynamic substructuring approach one is able to analyse the dynamic behaviour of substructures separately and to later on calculate the assembled dynamics using coupling procedures. Dynamic substructuring has several advantages over the analysis of the fully assembled system:\n* Substructures can be modelled in the domain that is most appropriate, e.g. [[experiment]]ally obtained substructures can be combined with [[Finite element method in structural mechanics|numerical models]]. \n* Large and/or complex systems can be optimized on substructure level.\n* Numerical computation load can be reduced as solving several substructures is computationally less demanding than solving one large system.\n* Substructure models of different development groups can be shared and combined without exposing the modelling details.\nDynamic substructuring is particularly tailored to simulation of [[Vibration|mechanical vibrations]], which has implications for many product aspects such as [[sound]] / [[acoustics]], [[Fatigue (material)|fatigue]] / durability, comfort and [[safety]]. Also, dynamic substructuring is applicable to any scale of [[size]] and [[frequency]]. It is therefore a widely used paradigm in industrial applications ranging from [[Automotive engineering|automotive]] and [[aerospace engineering]] to design of [[Wind turbine design|wind turbines]] and [[High tech|high-tech]] [[Precision engineering|precision]] machinery.\n\n== History ==\n[[File:Two_different_levels_of_domain_decomposition.svg|500x500px|thumb|Two levels of domain decomposition in dynamic substructuring.]]\nThe roots of dynamic substructuring can be found in the field of [[Domain decomposition methods|domain decomposition]]. In 1890 the mathematician [[Hermann Schwarz]] came up with an iterative procedure for domain decomposition which allows to solve for continuous coupled subdomains. However, many of the analytical models of coupled continuous subdomains do not have [[Closed-form expression|closed-form solutions]], which led to [[discretization]] and approximation techniques such as the [[Ritz method]]<ref>{{Cite journal|last=Ritz|first=W.|date=1909|title=Über eine neue Methode zur Lösung gewisser Variations Probleme der Mathematishen Physik|url=|journal=Journal für die Reine und Angewandte Mathematik |volume=135 |pages=1–61|doi=|pmid=|access-date=}}</ref> (which is sometimes called the [[Rayleigh–Ritz method|Raleigh-Ritz method]] due the similarity between Ritz's formulation and the [[Rayleigh quotient|Raleigh ratio]]) the [[boundary element method]] (BEM) and the [[finite element method]] (FEM). These methods can be considered as \"first level\" domain decomposition techniques.\n\nThe finite element method proved to be the most efficient method and the invention of the microprocessor made it possible to easily solve a large variety of physical problems.<ref>{{Cite book|title=The Finite Element Method for Engineers|author1=Huebner, Dewhirst |author2=Smith, Byrom |publisher=Wiley|year=2001|isbn=978-0471370789|location=New York|pages=}}</ref> In order to analyse even larger and more complex problems, methods were invented to optimize the efficiency of the discretized calculations. The first step was replacing the direct solvers by iterative solvers such as the [[conjugate gradient method]].<ref>{{Cite journal|last=Hestnes,Stiefel|date=1952|title=Method of Conjugate Gradients for Solving Linear Systems|url=|journal=Journal of Engineering Mechanics |volume=86 |issue=4 |year=1960 |pages=51–69|doi=|pmid=|access-date=}}</ref> The lack of robustness and slow convergence of these solvers did not make them an interesting alternative in the beginning. The rise of [[parallel computing]] in the 1980s however sparked their popularity. Complex problems could now be solved by dividing the problem into subdomains, each processed by a separate processor, and solving for the interface coupling iteratively. This can be seen as a second level domain decomposition as is visualized in the figure.\n\nThe efficiency of dynamic modelling could be increased even further by reducing the complexity of the individual subdomains. This reduction of the subdomains (or ''substructures'' in the context of structural dynamics) is realized by representing substructures by means of their general responses. Expressing the separate substructures by means of their general response instead of their detailed discretization led to the so-called dynamic substructuring method. This reduction step also allowed for replacing the mathematical description of the domains by experimentally obtained information. This reduction step is also visualized by the reduction arrow in the figure.\n\nThe first dynamic substructuring methods were developed in the 1960s and were more commonly known under the name component mode synthesis (CMS). The benefits of dynamic substructuring were quickly discovered by the scientific and engineering communities and it became an important research topic in the field of [[structural dynamics]] and [[vibration]]s. Major developments followed, resulting in e.g. the classic Craig-Bampton method.<ref>{{Cite journal|last=Craig,Bampton|title=Coupling of Substructures for Dynamic Analysis|url=|journal=AIAA Journal |volume=6 |issue=7 |year=1968 |pages=1313–1319|doi=10.2514/3.4741|pmid=|access-date=|bibcode=1968AIAAJ...6.1313B}}</ref>\n\nDue to improvements in [[sensor]] and [[signal processing]] technology in the 1980s, substructuring techniques also became attractive for the [[experiment]]al community. Methods dealing with structural dynamic modification were created in which coupling techniques were directly applied to measured [[Frequency response|frequency response functions]] (FRFs). Broad popularity of the method was gained when Jetmundsen et al. formulated the classical frequency-based substructuring (FBS) method,<ref name=\":2\" /> which laid the ground work for frequency-based dynamic substructuring. In 2006 a systematic notation was introduced by De Klerk et al.<ref name=\":3\" /> in order to simplify the difficult and elaborate notation that had been used prior. The simplification was done by means of two [[Boolean algebra|Boolean]] matrices that handle all the \"bookkeeping\" involved in the assembly of substructures<ref name=\":0\">{{Cite journal|last=Klerk|first=D. De|last2=Rixen|first2=D. J.|last3=Voormeeren|first3=S. N.|date=2008-01-01|title=General Framework for Dynamic Substructuring: History, Review and Classification of Techniques|journal=AIAA Journal|volume=46|issue=5|pages=1169–1181|doi=10.2514/1.33274|issn=0001-1452|bibcode=2008AIAAJ..46.1169D}}</ref>\n\n== Domains ==\n[[File:Ds domains v3.png|thumbnail|Five domains typically used for dynamic substructuring.|500x500px]]\nDynamic substructuring can best be seen as a domain-independent toolset for assembly of component models, rather than a modelling method of its own. Generally, dynamic substructuring can be used for all domains that are well suited to simulate [[Multiple-input multiple-output|multiple input/multiple output]] behaviour.<ref name=\":0\"/> Five domains that are well suited for substructuring are: \n* [[Modal analysis using FEM|Physical domain]]\n* Modal domain\n* [[Frequency domain]]\n* [[Time domain]]\n* [[State-space representation|State-space domain]]\n\nThe ''physical domain'' concerns methods that are based on (linearised) mass, damping and stiffness matrices, typically obtained from numerical FEM modelling. Popular solutions to solve the associated system of second order differential equations are the [[Numerical methods for ordinary differential equations|time integration]] schemes of [[Newmark-beta method|Newmark]] <ref>{{Cite journal|last=Newmark|first=N.M.|date=1959|title=A Method of Computation for Structural Dynamics|url=|journal=Journal of the Engineering Mechanics Division |doi=|pmid=|access-date=}}</ref> and the Hilbert-Hughes-Taylor scheme.<ref name=\":1\">{{Cite book|url=http://eu.wiley.com/WileyCDA/WileyTitle/productCd-1118900200.html|title=Mechanical Vibrations: Theory and Application to Structural Dynamics, 3rd Edition|last=Geradin|first=Michel|last2=Rixen|first2=Daniel J.|publisher=John Wiley & Sons|year=2014|isbn=978-1-118-90020-8|location=|pages=}}</ref> The ''modal domain'' concerns component mode synthesis (CMS) techniques such as the Craig-Bampton, Rubin and McNeal method. These methods provide efficient modal reduction bases and assembly techniques for numerical models in the physical domain. The ''frequency domain'' is more popularly known as Frequency Based Substructuring (FBS). Based on the classic formulation of Jetmundsen et al.<ref name=\":2\">{{Cite journal|last=Jetmundsen|first=Bjorn|last2=Bielawa|first2=Richard L.|last3=Flannelly|first3=William G.|date=1988-01-01|title=Generalized Frequency Domain Substructure Synthesis|url=http://www.ingentaconnect.com/content/ahs/jahs/1988/00000033/00000001/art00006|journal=Journal of the American Helicopter Society|volume=33|issue=1|pages=55–64|doi=10.4050/JAHS.33.55}}</ref> and the reformulation of De Klerk et al.,<ref name=\":1\" /> it has become the most commonly used domain for substructuring, because of the ease of expressing the differential equations of a dynamical system (by means of [[Frequency response|Frequency Response Functions,]] FRFs) and the convenience of implementing experimentally obtained models. The ''time domain'' refers to the recently proposed concept of Impulse Based Substructuring (IBS),<ref>{{Cite journal|last=Rixen|first=Daniel J.|last2=van der Valk|first2=Paul L. C.|date=2013-12-23|title=An Impulse Based Substructuring approach for impact analysis and load case simulations|url=http://www.sciencedirect.com/science/article/pii/S0022460X13006615|journal=Journal of Sound and Vibration|volume=332|issue=26|pages=7174–7190|doi=10.1016/j.jsv.2013.08.004|bibcode=2013JSV...332.7174R}}</ref> which expresses the behaviour of a dynamic system using a set of [[Impulse response|Impulse Response Functions]] (IRFs). The state-space domain, finally, refers to methods proposed by Sjövall et al.<ref>{{Cite journal|last=Sjövall|first=Per|last2=Abrahamsson|first2=Thomas|date=2007-10-01|title=Component system identification and state-space model synthesis|url=http://www.sciencedirect.com/science/article/pii/S088832700700043X|journal=Mechanical Systems and Signal Processing|volume=21|issue=7|pages=2697–2714|doi=10.1016/j.ymssp.2007.03.002|bibcode=2007MSSP...21.2697S}}</ref> that employ [[system identification]] techniques common to [[control theory]].\n\nAn overview of the governing equations of the five herementioned domains is presented in the table below.\n\n {| class=\"wikitable\"\n|+Dynamic equations for five domains\n!Domain\n!Dynamic equation\n!Additional information\n|-\n|Physical domain\n|<math>\\mathbf f(t) = \\mathbf{M\\ddot{u}}(t)+\\mathbf{C\\dot{u}}(t)+\\mathbf{Ku}(t)</math>\n|<math>\\mathbf{M},\\mathbf{C},\\mathbf{K}</math> represent the linear(ised) mass, damping and stiffness matrix of the system. \n|-\n|Modal Domain\n|<math>\\mathbf{f_m}(t) = \\mathbf{M_m \\ddot{\\boldsymbol \\eta}}(t)+\\mathbf{C_m} \\boldsymbol{\\dot{\\eta}}(t)+\\mathbf{K_m}\\boldsymbol{\\eta}(t)</math>\n|<math>\\mathbf M_m, \\mathbf C_m, \\mathbf K_m</math> represent the modally reduced mass, damping and stiffness matrix; <math>\\boldsymbol \\eta</math> is the set of modal amplitudes.\n|-\n|Frequency Domain\n|<math>\\begin{align}\n\\mathbf f(\\omega) = \\mathbf Z(\\omega)\\mathbf u(\\omega) \\\\\n\\mathbf u(\\omega) = \\mathbf Y(\\omega) \\mathbf f(\\omega)\n\\end{align}</math>\n|<math>\\mathbf Z (\\omega)</math> is the impedance [[Frequency response|FRF]] matrix; <math>\\mathbf Y(\\omega)</math> is the admittance [[Frequency response|FRF]] matrix.\n|-\n|Time domain\n|<math>\\mathbf u(t) = \\mathbf Y(t)*\\mathbf f(t)</math>\n|<math>\\mathbf Y(t)</math> is the [[Impulse response|IRF]] matrix.\n|-\n|State-space domain\n|<math>\\begin{cases}\n\\mathbf{\\dot{x}}(t) = \\mathbf{Ax}(t) + \\mathbf{Bv}(t)\\\\\n\\mathbf{y}(t) = \\mathbf{Cx}(t)+\\mathbf{Dv}(t) \\\\\n\\end{cases}</math>\n|<math>\\mathbf A,\\mathbf B,\\mathbf C,\\mathbf D</math> are the [[State space|state-space]] matrices; <math>\\mathbf x</math>, <math>\\mathbf v</math> and <math>\\mathbf y</math> represent the state, input and output vector.\n|}\nAs dynamic substructuring is a domain-independent toolset, it is applicable to the dynamic equations of all domains. In order to establish substructure assembly in a particular domain, two interface conditions need to be implemented. This is explained next, followed by a few common substructuring techniques.\n\n== Interface Conditions ==\nTo establish substructuring coupling / decoupling in each of the above-mentioned domains, two conditions should be met: \n* Coordinate compatibility, i.e. the connecting nodes of two substructures should have equal interface ''displacement''.\n* Force equilibrium, i.e. the interface ''forces'' between connecting nodes have equal magnitude and opposing sign.\nThese are the two essential conditions that keep substructures together, hence allow to construct an assembly of multiple components. Note that the conditions are comparable with [[Kirchhoff's circuit laws|Kirchhoff's]] laws for [[Electrical network|electric circuits]], in which case similar conditions apply to currents and voltages though/over electric components in a network; see also [[Mechanical-electrical analogies]].\n\n=== Substructure connectivity ===\n[[File:Assembly of 2 substructures consisting of nodes.png|thumbnail|Assembly of two substructures A and B, connected by the DoFs <math>\\mathbf u_2</math> and interface forces <math>\\mathbf g_2</math> of the coupling nodes.|538x538px]]\nConsider two substructures A and B as depicted in the figure. The two substructures comprise a total of six nodes; the displacements of the nodes are described by a set of [[Degrees of freedom (mechanics)|Degrees of Freedom]] (DoFs). The DoFs of the six nodes are partitioned as follows:\n# DoFs of the internal nodes of substructure A;\n# DoFs of the coupling nodes of substructures A and B, i.e. interface DoFs;\n# DoFs of the internal nodes of substructure B.\nNote that the denotation 1, 2 and 3 indicates the ''function'' of the nodes/DoFs rather than the total amount. Let us define the sets of DoFs for the two substructures A and B in concatenated form. The displacements and applied forces are represented by the sets <math>\\mathbf u</math> and <math>\\mathbf f</math>. For the purpose of substructuring, a set of interface forces <math>\\mathbf g</math> is introduced which only contains non-zero entries on the interface DoFs:\n:<math>\n\\mathbf u \\triangleq \\begin{bmatrix}\n\\mathbf u_1^A \\\\\n\\mathbf u_2^A \\\\\n\\mathbf u_2^B \\\\\n\\mathbf u_3^B \\\\\n\\end{bmatrix}\n\\quad\n \\mathbf f \\triangleq \\begin{bmatrix}\n\\mathbf f_1^A \\\\\n\\mathbf f_2^A \\\\\n\\mathbf f_2^B \\\\\n\\mathbf f_3^B \\\\\n\\end{bmatrix}\n\\quad\n\\mathbf g \\triangleq \\begin{bmatrix}\n\\mathbf 0 \\\\\n\\mathbf g_2^A \\\\\n\\mathbf g_2^B \\\\\n\\mathbf 0 \\\\\n\\end{bmatrix}\n\\quad \n\\mathbf u, \\mathbf f,\\mathbf g \\in \\R^n\n</math>\nThe relation between dynamic displacements <math>\\mathbf u</math> and applied forces <math>\\mathbf f</math> of the uncoupled problem is governed by a particular dynamic equation, such as presented in the table above. The uncoupled equations of motion are augmented by extra terms/equations for compatibility and equilibrium, as discussed next.\n\n=== Compatibility ===\nThe ''compatibility condition'' requires that the interface DoFs have the same sign and value at both sides of the interface: <math>\n\\mathbf u_2^A = \\mathbf u_2^B\n</math>. This condition can be expressed using a so-called ''signed [[Boolean algebra|Boolean]] matrix'',<ref name=\":3\">{{Cite journal|author1=D. de Klerk|author2=D. Rixen|author3=J. de Jong|date=2006|title=The Frequency Based Substructuring method reformulated according to the dual domain decomposition method|url=https://www.sem.org/Proceedings/ConferencePapers-Paper.cfm?ConfPapersPaperID=21737|journal=Proceedings of the XXIV International Modal Analysis Conference (IMAC), St. Louis|doi=|pmid=|access-date=|deadurl=yes|archiveurl=https://web.archive.org/web/20160701083116/https://www.sem.org/Proceedings/ConferencePapers-Paper.cfm?ConfPapersPaperID=21737|archivedate=2016-07-01|df=}}</ref> denoted by <math>\\mathbf B </math> . For the given example this can be expressed as:\n:<math>\n\\mathbf B \\mathbf u = \\mathbf 0 \\quad \\Rightarrow \\quad \\mathbf u_2^B - \\mathbf u_2^A = \\mathbf 0 \\quad \\Rightarrow \\quad \\mathbf B \\triangleq \n\\begin{bmatrix}\n\\mathbf 0 & -\\mathbf I & \\mathbf I & \\mathbf 0\n\\end{bmatrix} \n</math>\n\nIn some cases the interface nodes of the substructures are non-conforming, e.g. when two substructures are meshed separately. In such cases a non-Boolean matrix <math>\n\\mathbf B \n</math> has to be used in order to enforce a weak interface compatibility.<ref>{{Cite journal|last=Bernardi,Maday,Patera|date=1994|title=New Nonconforming Approach to Domain Decomposition: The Mortar Element Method|url=|journal=Nonlinear Partial Differential Equations and Their Applications|doi=|pmid=|access-date=}}</ref><ref>{{Cite thesis|last=Voormeeren|first=S.N.|date=7 November 2012|title=Dynamic Substructuring Methodologies for Integrated Dynamic Analysis of Wind Turbines|type=PhD|language=en|publisher=Delft University of Technology|doi=10.4233/uuid:f45f0548-d5ec-46aa-be7e-7f1c2b57590d}}</ref>\n\nA second form in which the compatibility condition can be expressed is by means of coordinate substitution by a set of generalised coordinates <math>\n\\mathbf q \n</math>. The set <math>\n\\mathbf q \n</math> contains the unique coordinates that remain after assembly of the substructures Every matching pair of interface DoFs is described by a single generalised coordinate, which means that the compatibility condition is automatically enforced. Expressing <math>\n\\mathbf u \n</math> using <math>\n\\mathbf q \n</math> gives:\n\n:<math>\n\\mathbf u = \\mathbf L \\mathbf q \\quad \\Rightarrow \\quad\n\\begin{cases}\n\\mathbf u_1^A = \\mathbf q_1 \\\\\n\\mathbf u_2^A = \\mathbf q_2 \\\\\n\\mathbf u_2^B = \\mathbf q_2 \\\\\n\\mathbf u_3^B = \\mathbf q_3 \\\\\n\\end{cases}\n\\quad \\Rightarrow \\quad\n\\mathbf L \\triangleq \n\\begin{bmatrix}\n\\mathbf I & \\mathbf 0 & \\mathbf 0 \\\\\n\\mathbf 0 & \\mathbf I & \\mathbf 0 \\\\\n\\mathbf 0 & \\mathbf I & \\mathbf 0 \\\\\n\\mathbf 0 & \\mathbf 0 & \\mathbf I \\\\\n\\end{bmatrix} \n</math>\n\nMatrix <math>\n\\mathbf L \n</math> is referred to as the ''Boolean localisation matrix''. A useful relation between matrix <math>\n\\mathbf B \n</math> and <math>\n\\mathbf L \n</math> can be exposed by noting that compatibility should hold for any set of physical coordinates <math>\\mathbf u</math> expressed by <math>\\mathbf q</math>. Indeed, substituting <math>\\mathbf u = \\mathbf{Lq}</math> in the equation <math>\\mathbf{Bu} = \\mathbf 0</math>:\n:<math>\n\\mathbf{Bu} = \\mathbf B \\mathbf L \\mathbf q = \\mathbf 0 \\quad \\forall \\mathbf q \n</math>\n\nHence <math>\n\\mathbf L \n</math> represents the [[nullspace]] of <math>\n\\mathbf B \n</math>:\n:<math>\n\\begin{cases}\n\\mathbf L = \\text{null}(\\mathbf B)\\\\\n\\mathbf B^T = \\text{null}(\\mathbf L^T)\n\\end{cases}\n</math>\n\nThis means in practice that one only needs to define <math>\\mathbf B</math> or <math>\\mathbf L</math>; the other Boolean matrix is calculated using the nullspace property.\n\n=== Equilibrium ===\nThe second condition that has to be satisfied for substructure assembly is the ''force equilibrium'' for matching interface forces <math>\\mathbf g</math>. For the current example, this condition can be written as <math>\n\\mathbf g_2^A = -\\mathbf g_2^B\n</math>. Similar to the compatibility equation, the force equilibrium condition can be expressed using a Boolean matrix. Use is made of the transpose of the Boolean localisation matrix <math> \\mathbf L </math> that was introduced to write compatibility:\n:<math>\n\\mathbf L^T \\mathbf g = \\mathbf 0 \n\\quad \\Rightarrow \\quad\n\\begin{cases}\n\\mathbf g_1^A = \\mathbf 0 \\\\\n\\mathbf g_2^A + \\mathbf g_2^B = \\mathbf 0 \\\\\n\\mathbf g_3^B = \\mathbf 0\n\\end{cases}\n</math>\n\nThe equations for <math>\\mathbf g_1</math> and <math>\\mathbf g_3</math> state that the interface forces on internal nodes are zero, hence not present. The equation for <math>\\mathbf g_2</math> correctly establishes the force equilibrium between a matching pair of interface DoFs according to [[Newton's Third Law|Newton's third law]].\n\nA second notation in which the equilibrium condition can be expressed is by introducing a set of [[Lagrange multiplier]]s <math>\\boldsymbol \\lambda </math>. The substitution of these Lagrange multipliers is possible as <math> \\mathbf g_2^A </math> and <math> \\mathbf g_2^B </math> differ only in sign, not in value. Using again the signed Boolean matrix <math>\n\\mathbf B\n</math>:\n\n:<math>\n\\mathbf g = -\\mathbf B^T \\boldsymbol{\\lambda} \n\\quad \\Rightarrow \\quad\n\\begin{cases}\n\\mathbf g_1^A = \\mathbf 0 \\\\\n\\mathbf g_2^A = \\boldsymbol{\\lambda}\\\\\n\\mathbf g_2^B = -\\boldsymbol{\\lambda}\\\\\n\\mathbf g_3^B  =\\mathbf 0 \\\\\n\\end{cases}\n</math>\n\nThe set <math>\\boldsymbol{\\lambda}</math> defines the intensity of the interface forces <math>\\mathbf g_2</math>. Each Lagrange multiplier represents the magnitude of two matching interface forces in the assembly. By defining the interface forces <math>\\mathbf g</math> using Lagrange multipliers <math>\\boldsymbol \\lambda </math>, force equilibrium is automatically satisfied. This can be seen by substituting <math>\\mathbf g = -\\mathbf{B^T} \\boldsymbol{\\lambda}</math> into the first equilibrium equation:\n:<math>\n\\mathbf L^T \\mathbf g = -\\mathbf L^T \\mathbf B^T \\boldsymbol{\\lambda} = \\mathbf 0 \\quad \\forall \\mathbf \\mathbf g\n</math>\n\nAgain, the nullspace property of the Boolean matrices is used here, namely: <math> \\mathbf{L^T}\\mathbf{B^T} = \\mathbf 0 </math>.\n\nThe two conditions as presented above can be applied to establish coupling / decoupling in a myriad of domains and are thus independent of variables such as time, frequency, mode, etc. Some implementations of the interface conditions for the most common domains of substructuring are presented below.\n\n== Substructuring in the physical domain ==\nThe physical domain is the domain that has the most straightforward physical interpretation. For each ''[[Discretization|discrete]] [[Linearization|linearised]] [[Dynamical system|dynamic system]]'' one is able to write an equilibrium between the externally applied forces and the internal forces originating from intrinsic inertia, viscous damping and elasticity. This relation is governed by one of the most elementary formulas in [[Vibration|structural vibrations]]:\n\n:<math>\\mathbf{M} \\mathbf \\ddot{u}(t)+\\mathbf C \\mathbf \\dot{u}(t)+ \\mathbf K \\mathbf u(t) = \\mathbf f(t)</math>\n\n<math>\\mathbf M, \\mathbf C, \\mathbf K</math> represent the [[Mass matrix|mass]], [[Damping matrix|damping]] and [[Stiffness matrix|stiffness]] matrix of the system. These matrices are often obtained from [[Finite element method|finite element modelling]] (FEM), and are referred to as the numerical model of the structure. Furthermore, <math>\\mathbf u</math> represents the DoFs and <math>\\mathbf f</math> the force vector which are dependent on time <math>(t)</math>. This dependency is omitted in the following equations in order to improve readability.\n\n=== Coupling in the physical domain ===\nCoupling of <math>n</math> substructures in the physical domain first requires writing the uncoupled equations of motion of the <math>n</math> substructures in block diagonal form:\n\n:<math>\\mathbf M \\triangleq \\text{diag}(\\mathbf M^{(1)},\n\\dots, \\mathbf M^{(n)})\n=\n\\begin{bmatrix}\n\\mathbf M^{(1)} & . & . \\\\\n. & \\ddots & . \\\\\n. & . & \\mathbf M^{(n)} \\\\\n\\end{bmatrix}</math>\n:<math>\\mathbf C \\triangleq \\text{diag}(\\mathbf C^{(1)},\\dots,\\mathbf C^{(n)}) \\quad \\mathbf K \\triangleq \\text{diag}(\\mathbf K^{(1)},\\dots,\\mathbf K^{(n)})</math>\n:<math>\\mathbf u \\triangleq \n\\begin{bmatrix}\n\\mathbf u^{(1)} \\\\ \\vdots \\\\ \\mathbf u^{(n)}\n\\end{bmatrix}, \\quad\n\\mathbf f \\triangleq \n\\begin{bmatrix}\n\\mathbf f^{(1)} \\\\ \\vdots \\\\ \\mathbf f^{(n)}\n\\end{bmatrix}, \\quad\n\\mathbf g \\triangleq \n\\begin{bmatrix}\n\\mathbf g^{(1)}\\\\ \\vdots \\\\ \\mathbf g^{(n)}\n\\end{bmatrix}</math>\n\nNext, two assembly approaches can be distinguished: primal and dual assembly.\n\n==== Primal assembly ====\nFor primal assembly, a unique set of degrees of freedom <math>\\mathbf q</math> is defined in order to satisfy compatibility, <math> \\mathbf u = \\mathbf {Lq} </math>. Furthermore, a second equation is added to enforce interface force equilibrium. This results in the following coupled dynamic equilibrium equations:\n\n:<math>\\begin{cases}\n\\mathbf{ML\\ddot{q} + CL\\dot{q}+KLq = f + g}\\\\\n\\mathbf{L^Tg = 0}\n\\end{cases}</math>\n\nPre-multiplying the first equation by <math>\\mathbf L^T</math> and noting that <math>\\mathbf{L^Tg}= \\mathbf{0}</math>, the primal assembly reduces to:\n\n:<math>\\mathbf{\\tilde{M}\\ddot{q}+\\tilde{C}\\dot{q}+\\tilde{K}q = L^T f} \\quad \n\\begin{cases}\n\\mathbf{\\tilde{M} = L^TML} \\\\\n\\mathbf{\\tilde{C} = L^TCL} \\\\\n\\mathbf{\\tilde{K} = L^TKL}\n\\end{cases}</math>\n\nThe primally assembled system matrices can be used for a transient simulation by any standard [[Numerical methods for ordinary differential equations|time stepping algorithm]]. Note that the primal assembly technique is analogue to assembly of [[Superelement|super-elements]] in [[finite element method]]s.\n\n==== Dual assembly ====\nIn the dual assembly formulation the global set of DoFs is retained and an assembly is made by a priori satisfying the equilibrium condition <math>\\mathbf{g = -B^T \\boldsymbol{\\lambda}}</math>. Again, the Lagrange multipliers represent the interface forces connecting the DoFs at the interface. As these are unknowns, they are moved to the left-hand side of the equation. In order to satisfy compatibility, a second equation is added to the system, now operating on the displacements:\n\n:<math>\\begin{cases}\n\\mathbf{M\\ddot{u} + C \\dot{u} + K u + B^T \\boldsymbol{\\lambda} = f}\\\\\n\\mathbf{Bu = 0}\n\\end{cases}</math>\n\nThe dually assembled system can be written in matrix form as:\n\n:<math>\\mathbf{\\begin{bmatrix}\n\\mathbf M & \\mathbf  0 \\\\ \\mathbf  0 &\\mathbf  0\n\\end{bmatrix}}\n\\begin{bmatrix}\n\\mathbf \\ddot{u} \\\\ \\boldsymbol{\\lambda}\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n\\mathbf C & \\mathbf  0 \\\\ \\mathbf  0 & \\mathbf  0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf \\dot{u} \\\\ \\boldsymbol{\\lambda}\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n\\mathbf K & \\mathbf B^T \\\\ \\mathbf B & \\mathbf  0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf u \\\\ \\boldsymbol{\\lambda}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mathbf f \\\\ \\mathbf 0\n\\end{bmatrix}</math>\n\nThis dually assembled system can also be used in a transient simulation by means of a standard time stepping algorithm.<ref name=\":1\" />\n\n== Substructuring in the frequency domain == \nIn order to write out the equations for frequency based substructuring (FBS), the dynamic equilibrium first has to be put in the frequency domain. Starting with the dynamic equilibrium in the physical domain:\n:<math>\\mathbf{M} \\mathbf \\ddot{u}(t)+\\mathbf C \\mathbf \\dot{u}(t)+ \\mathbf K \\mathbf u(t) = \\mathbf f(t)</math>\n\nTaking the [[Fourier transform]] of this equation gives the dynamic equilibrium in the frequency domain:\n\n:<math>\n\\mathbf Z(\\omega) \\mathbf u(\\omega)  = \\mathbf f(\\omega) \n\\quad \\text{with} \\quad \\mathbf Z(\\omega) =  \\bigr[-\\omega^2 \\mathbf M+j\\omega \\mathbf C+\\mathbf K\\bigr]\n</math>\n\nMatrix <math>\\mathbf Z(\\omega)</math> is referred to as the dynamic stiffness matrix. This matrix consists of the complex-valued frequency-dependent functions that describe the force required to generate a unit harmonic displacement at a certain DoF. The inverse of the matrix <math>\\mathbf Z(\\omega)</math> is defined as <math>\\mathbf Y(\\omega)\\triangleq(\\mathbf Z(\\omega))^{-1}</math> and yields the more intuitive admittance notation:\n\n:<math>\n\\mathbf u(\\omega) = \\mathbf Y(\\omega) \\mathbf f(\\omega)\n</math>\n\nThe receptance matrix <math>\\mathbf Y(\\omega)</math> contains the [[Frequency response|frequency response functions]] (FRFs) of the structure which describe the displacement response to a unit input force. Other variants of the receptance matrix are the mobility and accelerance matrix, which respectively describe the velocity and acceleration response. The elements of the dynamic stiffness (or ''[[Mechanical impedance|impedance]]'' in general) and receptance (or ''admittance'' in general) matrix are defined as follows:\n\n:<math>\n\\begin{align}\n\\text{Impedance:} \\quad Z_{ij}(\\omega)= \\frac{f_i(\\omega)}{u_j(\\omega)} \\quad u_{k\\neq j}=0\\\\\n\\text{Admittance:} \\quad Y_{ij}(\\omega)= \\frac{u_i(\\omega)}{f_j(\\omega)}\\quad f_{k\\neq j}=0\n\\end{align}\n</math>\n\n=== Coupling in the frequency domain  ===\nIn order to couple two substructures in the frequency domain, use is made of the admittance and impedance matrices of both substructures. Using the definition of substructures A and B as introduced previously, the following impedance and admittance matrices are defined (note that the frequency dependency <math>(\\omega)\n</math> is omitted from the terms to improve readability):\n\n:<math>\\begin{array}{ll}\n\n\\mathbf Z^A \\triangleq \\begin{bmatrix}\n\\mathbf Z_{11}^A & \\mathbf Z_{12}^A \\\\\n\\mathbf Z_{21}^A & \\mathbf Z_{22}^A \\\\\n\\end{bmatrix}\n\n&\n\n\\mathbf Z^B \\triangleq \\begin{bmatrix}\n\\mathbf Z_{22}^B & \\mathbf Z_{23}^B \\\\\n\\mathbf Z_{32}^B & \\mathbf Z_{33}^B \\\\\n\\end{bmatrix}\n\n\\\\[18pt]\n\n\\mathbf Y^A \\triangleq \\begin{bmatrix}\n\\mathbf Y_{11}^A & \\mathbf Y_{12}^A \\\\\n\\mathbf Y_{21}^A & \\mathbf Y_{22}^A \\\\\n\\end{bmatrix}\n\n&\n\\mathbf Y^B \\triangleq \\begin{bmatrix}\n\\mathbf Y_{22}^B & \\mathbf Y_{23}^B \\\\\n\\mathbf Y_{32}^B & \\mathbf Y_{33}^B \\\\\n\\end{bmatrix}\n\n\\end{array}\n</math>\n\nThe two admittance and impedance matrices can be put in block diagonal form in order to align with the global set of DoFs <math>\\mathbf u\n</math>:\n\n:<math>\\begin{align}\n\\mathbf Z \\triangleq\n&\\begin{bmatrix}\n\\mathbf Z^A &\\mathbf 0 \\\\\n\\mathbf0 & \\mathbf Z^B\n\\end{bmatrix}\n\\triangleq\n\\begin{bmatrix}\n\\mathbf Z_{11}^A & \\mathbf Z_{12}^A & \\mathbf 0 & \\mathbf 0 \\\\\n\\mathbf Z_{21}^A & \\mathbf Z_{22}^A & \\mathbf 0 & \\mathbf 0 \\\\\n\\mathbf 0 & \\mathbf 0 & \\mathbf Z_{22}^B & \\mathbf Z_{23}^B \\\\\n\\mathbf 0 & \\mathbf0 & \\mathbf Z_{32}^B & \\mathbf Z_{33}^B \\\\\n\\end{bmatrix} \\\\[6pt]\n\n\\mathbf Y \\triangleq\n&\\begin{bmatrix}\n\\mathbf Y^A & \\mathbf0 \\\\\n\\mathbf 0 & \\mathbf Y^B\n\\end{bmatrix}\n\\triangleq\n\\begin{bmatrix}\n\\mathbf Y_{11}^A & \\mathbf Y_{12}^A & \\mathbf 0 & \\mathbf 0 \\\\\n\\mathbf Y_{21}^A & \\mathbf Y_{22}^A &\\mathbf  0 & \\mathbf 0 \\\\\n\\mathbf 0 & \\mathbf 0 & \\mathbf Y_{22}^B & \\mathbf Y_{23}^B \\\\\n\\mathbf 0 & \\mathbf 0 & \\mathbf Y_{32}^B & \\mathbf Y_{33}^B \\\\\n\\end{bmatrix}\n\\end{align}</math>\n\nThe off-diagonal zero terms show that at this point no coupling is present between the two substructures. In order to create this coupling, use can be made of the primal or dual assembly method. Both assembly methods make use of the dynamic equations as was defined before:\n\n:<math>\\begin{align}\n\\mathbf Z \\mathbf u & = \\mathbf f + \\mathbf g\\\\\n\\mathbf u & = \\mathbf Y(\\mathbf f+ \\mathbf g) \\\\\n\\end{align}</math>\n\nIn these equations <math>\\mathbf g</math> is again used to define the set of interface forces, which are yet unknown.\n\n==== Primal assembly ====\nIn order to obtain the primal system of equations, a unique set of coordinates <math> \\mathbf q </math> is defined: <math>\\mathbf u = \\mathbf{Lq}</math>. By definition of an appropriate Boolean localisation matrix <math>\\mathbf L</math>, a unique set of DoFs remains for which the compatibility condition is satisfied a priori (''compatibility condition''). In order to satisfy the ''equilibrium condition'' a second equation is added to the equations of motion:\n\n:<math>\\begin{cases}\n\\mathbf Z \\mathbf L \\mathbf q = \\mathbf f + \\mathbf g \\\\\n\\mathbf L^T \\mathbf g = \\mathbf 0 \n\\end{cases}</math>\n\nPre-multiplying the first equation with <math>\\mathbf L^T</math> yields the notation of the assembled equations of motion for the generalised coordinates <math>\\mathbf q</math>:\n\n:<math>\\mathbf \\tilde{Z}\\mathbf q = \\mathbf \\tilde{f} \n\\quad \\text{with} \\quad\n\\begin{cases}\n\\mathbf \\tilde{Z} = \\mathbf L^T\\mathbf Z \\mathbf L \\\\\n\\mathbf \\tilde{f} = \\mathbf L^T \\mathbf f\n\\end{cases}</math>\n\nThis result can be rewritten in admittance form as:\n\n:<math>\\mathbf q = \\mathbf \\tilde{Y}\\mathbf \\tilde{f} \n\\quad \\text{with} \\quad\n\\mathbf \\tilde{Y}\\ = (\\mathbf \\tilde{Z})^{-1}</math>\n\nThis last result gives access to the generalised responses as a result of the generalised applied forces <math>\\mathbf \\tilde{f}</math>, namely by inverting the primally assembled impedance matrix.\n\nThe primal assembly procedure is mainly of interest when one has access to the dynamics in impedance form, e.g. from finite element modelling. When one only has access to the dynamics in admittance notation,<ref>{{Cite journal|author1=Allen, M. |author2=Mayes, R|date=2007|title=Comparison of FRF and Modal Methods for Combining Experimental and Analytical Substructures|url=|journal=Proceedings of the Twenty Fifth International Modal Analysis Conference|doi=|pmid=|access-date=}}</ref> the dual formulation is a more suitable approach.\n\n==== Dual assembly ====\nA dually assembled system starts with the system written in the admittance notation. For a dually assembled system the force equilibrium condition is satisfied a priori by substituting Lagrange multipliers <math>\\boldsymbol{\\lambda}</math> for the interface forces: <math>\\mathbf g = - \\mathbf B^T\\boldsymbol{\\lambda}</math>. The compatibility condition is enforced by adding an additional equation: \n:<math>\n\\begin{cases}\n\\mathbf u = \\mathbf Y(\\mathbf f - \\mathbf B^T \\boldsymbol{\\lambda})\\\\\n\\mathbf B \\mathbf u = \\mathbf 0\n\\end{cases}\n</math>\n\nSubstituting the first line in the second and solving for <math> \\boldsymbol{\\lambda} </math> gives:\n:<math>\n\\boldsymbol{\\lambda} = (\\mathbf B \\mathbf Y \\mathbf  B^T)^{-1}\\mathbf B \\mathbf Y \\mathbf f\n</math>\n\nThe term <math> \\mathbf B \\mathbf Y \\mathbf  f </math> represents the incompatibility caused by the uncoupled responses of the substructures to the applied forces <math>\\mathbf f </math>. By multiplying the incompatibility with the combined interface stiffness, i.e. <math> (\\mathbf  B \\mathbf  Y \\mathbf B^T)^{-1}</math>, the forces <math> \\boldsymbol{\\lambda} </math> that keep the substructures together are determined. The coupled response is obtained by substituting the calculated <math> \\boldsymbol{\\lambda} </math> back into the original equation:\n:<math>\n\\begin{align}\n&\\mathbf u = \\mathbf Y \\mathbf f - \\mathbf Y \\mathbf B^T(\\mathbf{BYB^T})^{-1}\\mathbf{BYf} \n\\\\[3pt]\n&\\mathbf u = \\mathbf \\tilde{Y} \\mathbf f\n\\quad \\text{with:} \\quad\n\\mathbf \\tilde{Y} = \\big(\\mathbf I - \\mathbf Y \\mathbf B^T(\\mathbf{BYB^T})^{-1}\\mathbf{B}\\big)\\mathbf Y\n\\end{align}\n</math>\n\nThis coupling method is referred to as the Lagrange-multiplier frequency-based substructuring (LM-FBS) method.<ref name=\":3\" /> The LM-FBS method allows for quick and easy assembling of an arbitrary number of substructures in a systematic fashion. Note that the result <math>\n\\mathbf{\\tilde{Y}}\n</math> is theoretically the same as was obtained above by application of primal assembly.\n\n:\n\n=== Decoupling in the frequency domain  ===\n[[File:Decoupling of substruces.png|thumbnail|Decoupling of substructure B from assembly AB|623x623px]]In addition to coupling of substructures, one is also able to decouple substructures from assemblies.<ref>{{Cite book|title=Direct decoupling of substructures using primal and dual formulation|last=D’Ambrogio|first=Walter|last2=Fregolent|first2=Annalisa|date=2011-01-01|publisher=Springer New York|isbn=9781441993045|editor-last=Proulx|editor-first=Tom|series=Conference Proceedings of the Society for Experimental Mechanics Series|pages=47–76|language=en|doi=10.1007/978-1-4419-9305-2_5}}</ref><ref name=\":4\">{{Cite journal|last=Voormeeren|first=S. N.|last2=Rixen|first2=D. J.|date=2012-02-01|title=A family of substructure decoupling techniques based on a dual assembly approach|url=http://www.sciencedirect.com/science/article/pii/S0888327011003177|journal=Mechanical Systems and Signal Processing|volume=27|pages=379–396|doi=10.1016/j.ymssp.2011.07.028|bibcode=2012MSSP...27..379V}}</ref><ref name=\":5\">{{Cite journal|last=D'Ambrogio|first=Walter|last2=Fregolent|first2=Annalisa|date=2014-04-04|title=Inverse dynamic substructuring using the direct hybrid assembly in the frequency domain|url=http://www.sciencedirect.com/science/article/pii/S0888327013006341|journal=Mechanical Systems and Signal Processing|volume=45|issue=2|pages=360–377|doi=10.1016/j.ymssp.2013.11.007|bibcode=2014MSSP...45..360D}}</ref> Using the plus sign as a substructure coupling operator, the coupling procedure could simply be described as AB = A + B. Using a similar notation, decoupling could be formulated as AB - B = A. Decoupling procedures are often required to remove substructures that were added for measurement purposes, e.g. to fix the structure. Similar to coupling, a primal and dual formulation exists for decoupling procedures.\n\n==== Primal disassembly ====\nAs a result of the primal coupling, the impedance matrix of the assembled system can be written as follows:\n:<math>\n\\mathbf Z^{AB}=\n\\begin{bmatrix}\n\\mathbf Z_{11}^{AB} & \\mathbf Z_{12}^{AB} & \\mathbf Z_{13}^{AB} \\\\\n\\mathbf Z_{21}^{AB} & \\mathbf Z_{22}^{AB} & \\mathbf Z_{23}^{AB} \\\\\n\\mathbf Z_{31}^{AB} & \\mathbf Z_{32}^{AB} & \\mathbf Z_{33}^{AB} \n\\end{bmatrix}\n= \n\\begin{bmatrix}\n\\mathbf Z_{11}^A & \\mathbf Z_{12}^A & \\mathbf 0 \\\\\n\\mathbf Z_{21}^A & \\mathbf Z_{22}^A+ \\mathbf Z_{22}^B & \\mathbf Z_{23}^B\\\\\n\\mathbf 0 & \\mathbf Z_{32}^B & \\mathbf Z_{33}^B\n\\end{bmatrix}\n</math>\n\nUsing this relation, the following trivial subtraction operation would suffice for the decoupling of the substructure B from assembly AB:\n\n:<math>\n\\begin{bmatrix}\n\\mathbf Z_{11}^A & \\mathbf Z_{12}^A & \\mathbf 0 \\\\\n\\mathbf Z_{21}^A & \\mathbf Z_{22}^A+\\mathbf Z_{22}^B & \\mathbf Z_{23}^B\\\\\n\\mathbf 0 & \\mathbf Z_{32}^B & \\mathbf Z_{33}^B\n\\end{bmatrix}- \n\\begin{bmatrix}\n\\mathbf 0 &\\mathbf  0 &\\mathbf  0 \\\\\n\\mathbf 0 & \\mathbf Z_{22}^B & \\mathbf Z_{23}^B \\\\\n\\mathbf 0 & \\mathbf Z_{32}^B & \\mathbf Z_{33}^B \n\\end{bmatrix}\n = \n\\begin{bmatrix}\n\\mathbf Z_{11}^A & \\mathbf Z_{12}^A & \\mathbf 0 \\\\\n\\mathbf Z_{21}^A & \\mathbf Z_{22}^A & \\mathbf0 \\\\\n\\mathbf0 & \\mathbf0 & \\mathbf0\n\\end{bmatrix}\n</math>\nBy placing the impedance of AB and B in block-diagonal form, with a minus sign for the impedance of B to account for the subtraction operation, the same equation that was used for primal coupling can now be used to perform the primal decoupling procedures.\n\n:<math>\\mathbf Z \\triangleq \n\\begin{bmatrix}\n\\mathbf Z^{AB} & \\mathbf0 \\\\\n\\mathbf0 & -\\mathbf Z^{B}\n\\end{bmatrix}\n\\quad \\Rightarrow \\quad\n\\mathbf \\tilde{Z} = \\mathbf L^T\\mathbf Z \\mathbf L \n</math>\n\nwith:\n\n:<math>\\mathbf L = \\begin{bmatrix}\n\\mathbf I & \\mathbf0 & \\mathbf0 \\\\\n\\mathbf0 & \\mathbf I & \\mathbf0 \\\\\n\\mathbf0 & \\mathbf0 & \\mathbf I \\\\\n\\mathbf0 & \\mathbf I & \\mathbf0 \\\\\n\\mathbf0 & \\mathbf0 & \\mathbf I\n\\end{bmatrix}</math>\n\nThe primal disassembly can thus be understood as the assembly of structure AB with the negative impedance of substructure B. A limitation of the primal disassembly is that all DoF of the substructure that is to be decoupled have to be exactly represented in the assembled situation. For numerical decoupling situations this should not pose any problems, however for experimental cases this can be troublesome. A solution to this problem can be found in the dual disassembly.\n\n==== Dual disassembly ====\nSimilar to the dual assembly, the dual disassembly approaches the decoupling problem using the admittance matrices. Decoupling in the dual domain means finding a force that ensures compatibility, yet acts in the opposite direction. This newly found force would then counteract the force that is applied to the assembly due to the dynamics of substructure B. Writing this out in equations of motion:\n\n:<math>\n\\begin{cases}\n\\mathbf u^{AB}  = \\mathbf Y^{AB} \\mathbf f + \\mathbf Y^{AB}\\mathbf g \\\\\n\\mathbf u^B = -\\mathbf Y^B \\mathbf g\n\\end{cases}\n</math>\n\nIn order to write the dynamics of both systems in one equation, using the LM-FBS assembly notation, the following matrices are defined:\n\n:<math>\\begin{align}\n\n\\mathbf Y \\triangleq \n&\\begin{bmatrix}\n\\mathbf Y^{AB} & \\mathbf0 \\\\\n\\mathbf0 & -\\mathbf Y^{B}\n\\end{bmatrix} \\\\[3pt]\n\\mathbf u = \n&\\begin{bmatrix}\n\\mathbf u_1^{AB} \\\\\n\\mathbf u_2^{AB} \\\\\n\\mathbf u_3^{AB} \\\\\n\\mathbf u_2^{B} \\\\\n\\mathbf u_3^{B} \n\\end{bmatrix}; \n\\quad \n\\mathbf f \\triangleq\n\\begin{bmatrix}\n\\mathbf f_1^{AB}\\\\\n\\mathbf f_2^{AB}\\\\\n\\mathbf0 \\\\\n\\mathbf0 \\\\\n\\mathbf0\n\\end{bmatrix};\n\\quad\n\\mathbf g \\triangleq \n\\begin{bmatrix}\n\\mathbf0 \\\\\n\\mathbf g_2^{AB}\\\\\n\\mathbf0 \\\\\n\\mathbf g_2^B \\\\\n\\mathbf0\n\\end{bmatrix}\n\n\\end{align}\n \n</math>\n\nIn order to enforce compatibility, a similar approach is used as for the assembly task. Defining a <math>\\mathbf B</math>-matrix to enforce compatibility:\n\n:<math>\\mathbf B = \\begin{bmatrix}\n\\mathbf0 & -\\mathbf I & \\mathbf0 & \\mathbf I & \\mathbf0\n\\end{bmatrix} \n</math>\n\nUsing this notation, the disassembly procedure can be performed using exactly the same equation as was used for the dual assembly:\n\n:<math>\n\\begin{cases}\n\\mathbf u = \\mathbf Y(\\mathbf f - \\mathbf B^T \\boldsymbol{\\lambda})\\\\\n\\mathbf B \\mathbf u = \\mathbf0\n\\end{cases}\n</math>\n\nThis means that coupling and decoupling procedures using LM-FBS require identical steps, the only difference being the manner in which the global admittance matrix is defined. Indeed, the substructures to couple appear with a plus sign, whereas decoupled structures carry a minus sign:\n\n:<math>\\begin{align}\n\\text{coupling} \\quad \\mathbf Y & \\triangleq \\begin{bmatrix} \\mathbf Y_A & \\mathbf 0 \\\\ \\mathbf 0 & \\mathbf Y_B \\end{bmatrix}\n\\\\\n\\text{decoupling} \\quad \\mathbf Y & \\triangleq \\begin{bmatrix} \\mathbf Y_{AB} & \\mathbf 0 \\\\ \\mathbf 0 & -\\mathbf Y_B \\end{bmatrix}\n\\end{align} \n</math>\nMore advanced decoupling techniques use the fact that internal points of substructure B appear in both the admittances of AB and B, hence can be used to enhance the decoupling process. Such techniques are described in.<ref name=\":4\" /><ref name=\":5\" />\n\n== See also ==\n{{Div col|colwidth=22em}}\n*[[Vibration]]\n*[[Finite element method]]\n*[[FETI|Finite element tearing and interconnect (FETI)]]\n*[[Mechanical engineering]]\n*[[Acoustical engineering|Acoustic engineering]]\n*[[Mechanical resonance]]\n*[[Mode shape]]\n*[[Modal analysis]]\n*[[Modal analysis using FEM]]\n*[[Shaker (testing device)]]\n*[https://web.archive.org/web/20021210161927/http://www.sem.org/CONF-IMAC-TOP.asp SEM International Modal Analysis Conference (IMAC)]\n*[http://substructure.engr.wisc.edu/substwiki/index.php/Main_Page SEM/IMAC Dynamic Substructuring Wiki]\n*[[Structural dynamics]]\n*[[Structural acoustics]]\n*[[Noise, vibration, and harshness]]\n*[http://www.sciencedirect.com/science/article/pii/S0888327015003647 Transfer path analysis]\n*[[Vibration control]]\n*[[Vibration isolation]]\n{{div col end}}\n\n==References==\n{{refbegin}}\n{{Reflist}}\n\n[[Category:Mechanical vibrations]]\n[[Category:Dynamics (mechanics)]]\n[[Category:Continuum mechanics]]\n[[Category:Structural analysis]]"
    },
    {
      "title": "Elastic instability",
      "url": "https://en.wikipedia.org/wiki/Elastic_instability",
      "text": "[[Image:elastic instability.png|thumb|Elastic instability of a rigid beam supported by an angular spring.]]\n\n<!-- Deleted image removed: [[Image:elastic instability curve.png|thumb|Deformation paths of a rigid beam supported by an angular spring.]] -->\n\n'''Elastic instability''' is a form of instability occurring in elastic systems, such as [[buckling]] of beams and plates subject to large compressive loads.\n\nThere are a lot of ways to study this kind of instability. One of them is to use the method of [[Incremental deformations|incremetal deformations]] based on superposing a small perturbation on an equilibrium solution.\n\n==Single degree of freedom-systems==\n\nConsider as a simple example a rigid beam of length ''L'', hinged in one end and free in the other, and having an [[Spring (device)|angular spring]] attached to the hinged end. The beam is loaded in the free end by a force ''F'' acting in the compressive axial direction of the beam, see the figure to the right.\n\n===Moment equilibrium condition===\n\nAssuming a clockwise angular deflection <math>\\theta</math>, the clockwise [[Moment (physics)|moment]] exerted by the force becomes <math>M_F = F L \\sin\\theta</math>. The moment [[Mechanical equilibrium|equilibrium]] equation is given by \n\n<math>\nF L \\sin \\theta = k_\\theta \\theta \n</math>\n\nwhere <math>k_\\theta</math> is the spring constant of the angular spring (Nm/radian). Assuming <math>\\theta</math> is small enough, implementing the [[Taylor expansion]] of the [[sine]] function and keeping the two first terms yields \n\n<math>\nF L \\Bigg(\\theta - \\frac{1}{6} \\theta^3\\Bigg) \\approx k_\\theta \\theta\n</math>\n\nwhich has three solutions, the trivial <math>\\theta = 0</math>, and \n\n<math>\n\\theta \\approx \\pm \\sqrt{6 \\Bigg( 1 - \\frac{k_\\theta}{F L} \\Bigg)} \n</math>\n\nwhich is [[complex number|imaginary]] (i.e. not physical) for <math>F L < k_\\theta</math> and [[complex number|real]] otherwise. This implies that for small compressive forces, the only equilibrium state is given by <math>\\theta = 0</math>, while if the force exceeds the value <math>k_\\theta/L</math> there is suddenly another mode of deformation possible.\n\n===Energy method===\nThe same result can be obtained by considering [[energy]] relations. The energy stored in the angular spring is \n\n<math>\nE_\\mathrm{spring} = \\int k_\\theta \\theta \\mathrm{d} \\theta  = \\frac{1}{2} k_\\theta \\theta^2\n</math>\n\nand the work done by the force is simply the force multiplied by the vertical displacement of the beam end, which is <math>L (1 - \\cos\\theta)</math>. Thus, \n\n<math>\nE_\\mathrm{force} = \\int{F \\mathrm{d} x  = F L (1 - \\cos \\theta )}\n</math>\n\nThe energy equilibrium condition <math>E_\\mathrm{spring} = E_\\mathrm{force}</math> now yields <math>F = k_\\theta / L</math> as before (besides from the trivial <math>\\theta = 0</math>).\n\n===Stability of the solutions===\n\nAny solution <math>\\theta</math> is [[Stability theory|stable]] [[iff]] a small change in the deformation angle <math>\\Delta \\theta</math> results in a reaction moment trying to restore the original angle of deformation. The net clockwise moment acting on the beam is \n\n<math>\nM(\\theta) = F L \\sin \\theta - k_\\theta \\theta\n</math>\n\nAn [[infinitesimal]] clockwise change of the deformation angle <math>\\theta</math> results in a moment \n\n<math>\nM(\\theta + \\Delta \\theta) = M + \\Delta M = F L (\\sin \\theta + \\Delta \\theta \\cos \\theta ) - k_\\theta (\\theta + \\Delta \\theta) \n</math>\n\nwhich can be rewritten as \n\n<math>\n\\Delta M = \\Delta \\theta (F L \\cos \\theta - k_\\theta) \n</math>\n\nsince <math>F L \\sin \\theta = k_\\theta \\theta</math> due to the moment equilibrium condition. Now, a solution <math>\\theta</math> is stable iff a clockwise change <math>\\Delta \\theta > 0</math> results in a negative change of moment <math>\\Delta M < 0</math> and vice versa. Thus, the condition for stability becomes\n\n<math>\n\\frac{\\Delta M}{\\Delta \\theta} = \\frac{\\mathrm{d} M}{\\mathrm{d} \\theta} = FL \\cos \\theta - k_\\theta < 0\n</math>\n\nThe solution <math>\\theta = 0</math> is stable only for <math>FL < k_\\theta</math>, which is expected. By expanding the [[cosine]] term in the equation, the approximate stability condition is obtained:\n\n<math>\n|\\theta| > \\sqrt{2\\Bigg( 1 - \\frac{k_\\theta}{F L} \\Bigg)}\n</math>\n\nfor <math>FL > k_\\theta</math>, which the two other solutions satisfy. Hence, these solutions are stable.\n\n== Multiple degrees of freedom-systems ==\n\n[[Image:Elastic instability 2DOF.png|thumb|Elastic instability, 2 degrees of freedom]]\n\nBy attaching another rigid beam to the original system by means of an angular spring a two degrees of freedom-system is obtained. Assume for simplicity that the beam lengths and angular springs are equal. The equilibrium conditions become\n\n<math>\nF L ( \\sin \\theta_1 + \\sin \\theta_2 ) = k_\\theta \\theta_1\n</math>\n\n<math>\nF L \\sin \\theta_2 = k_\\theta ( \\theta_2 - \\theta_1 )\n</math>\n\nwhere <math>\\theta_1</math> and <math>\\theta_2</math> are the angles of the two beams. Linearizing by assuming these angles are small yields\n\n<math>\n\\begin{pmatrix}\nF L - k_\\theta & F L \\\\\nk_\\theta & F L - k_\\theta\n\\end{pmatrix}\n\\begin{pmatrix}\n\\theta_1 \\\\\n\\theta_2\n\\end{pmatrix} = \n\\begin{pmatrix}\n0 \\\\\n0\n\\end{pmatrix}\n</math>\n\nThe non-trivial solutions to the system is obtained by finding the roots of the [[determinant]] of the system [[Matrix (mathematics)|matrix]], i.e. for \n\n<math>\n\\frac{F L}{k_\\theta} = \\frac{3}{2} \\mp \\frac{\\sqrt{5}}{2} \\approx \\left\\{\\begin{matrix} 0.382\\\\2.618 \\end{matrix}\\right.\n</math>\n\nThus, for the two degrees of freedom-system there are two critical values for the applied force ''F''. These correspond to two different modes of deformation which can be computed from the [[nullspace]] of the system matrix. Dividing the equations by <math>\\theta_1</math> yields\n\n<math>\n\\frac{\\theta_2}{\\theta_1} \\Big|_{\\theta_1 \\ne 0} = \\frac{k_\\theta}{F L} - 1 \\approx \\left\\{\\begin{matrix} 1.618 & \\text{for } F L/k_\\theta \\approx 0.382\\\\ -0.618 & \\text{for } F L/k_\\theta \\approx 2.618 \\end{matrix}\\right.\n</math>\n\nFor the lower critical force the ratio is positive and the two beams deflect in the same direction while for the higher force they form a \"banana\" shape. These two states of deformation represent the [[buckling]] [[mode shape]]s of the system.\n\n==See also==\n\n* [[Buckling]]\n* [[Cavitation (elastomers)]]\n* [[Drucker stability]]\n\n==Further reading==\n\n*''Theory of elastic stability'', [[Stephen Timoshenko|S. Timoshenko]] and J. Gere\n\n[[Category:Continuum mechanics]]\n[[Category:Structural analysis]]\n[[Category:Mechanics]]"
    },
    {
      "title": "Euler–Bernoulli beam theory",
      "url": "https://en.wikipedia.org/wiki/Euler%E2%80%93Bernoulli_beam_theory",
      "text": "{{Refimprove|date=November 2008}}\n[[File:VibratingGlassBeam.jpg|thumb|290px|right|This vibrating glass beam may be modeled as a cantilever beam with acceleration, variable linear density, variable section modulus, some kind of dissipation, springy end loading, and possibly a point mass at the free end.]]\n\n'''Euler–Bernoulli beam theory''' (also known as '''engineer's beam theory''' or '''classical beam theory''')<ref name=Timoshenko>Timoshenko, S., (1953), ''History of strength of materials'', McGraw-Hill New York</ref> is a simplification of the [[linear elasticity|linear theory of elasticity]] which provides a means of calculating the load-carrying and [[Deflection (engineering)|deflection]] characteristics of [[Beam (structure)|beams]]. It covers the case for small deflections of a [[beam (structure)|beam]] that are subjected to lateral loads only. It is thus a special case of [[Timoshenko beam theory]]. It was first enunciated circa 1750,<ref name=Truesdell>Truesdell, C., (1960), ''The rational mechanics of flexible or elastic bodies 1638–1788'', Venditioni Exponunt Orell Fussli Turici.</ref> but was not applied on a large scale until the development of the [[Eiffel Tower]] and the [[Ferris wheel]] in the late 19th century. Following these successful demonstrations, it quickly became a cornerstone of engineering and an enabler of the [[Second Industrial Revolution]].\n\nAdditional analysis tools have been developed such as [[plate theory]] and [[finite element analysis]], but the simplicity of beam theory makes it an important tool in the sciences, especially [[structural engineering|structural]] and [[mechanical engineering]].\n\n== History ==\n[[File:Poutre definitions en.svg|thumb|350px|Schematic of cross-section of a bent beam showing the neutral axis.]]\nPrevailing consensus is that [[Galileo Galilei]] made the first attempts at developing a theory of beams, but recent studies argue that [[Leonardo da Vinci]] was the first to make the crucial observations. Da Vinci lacked [[Hooke's law]] and [[calculus]] to complete the theory, whereas Galileo was held back by an incorrect assumption he made.<ref>{{cite journal\n |last        = Ballarini\n |first       = Roberto\n |title       = The Da Vinci-Euler-Bernoulli Beam Theory?\n |journal     = Mechanical Engineering Magazine Online\n |date        = April 18, 2003\n |url         = http://www.memagazine.org/contents/current/webonly/webex418.html\n |accessdate  = 2006-07-22\n |deadurl     = yes\n |archiveurl  = https://web.archive.org/web/20060623063248/http://www.memagazine.org/contents/current/webonly/webex418.html\n |archivedate = June 23, 2006\n |df          = \n}}</ref>\n\nThe Bernoulli beam is named after [[Jacob Bernoulli]], who made the significant discoveries.  [[Leonhard Euler]] and [[Daniel Bernoulli]] were the first to put together a useful theory circa 1750.<ref>{{cite journal\n  | author = Seon M. Han, Haym Benaroya and Timothy Wei\n  | title = Dynamics of Transversely Vibrating Beams using four Engineering Theories\n  | version = final version\n  | publisher = Academic Press\n  | date = March 22, 1999\n  | url = http://csxe.rutgers.edu/research/vibration/51.pdf\n  | format = PDF\n  | accessdate = 2007-04-15 }}</ref>\nAt the time, science and [[engineering]] were generally seen as very distinct fields, and there was considerable doubt that a mathematical product of academia could be trusted for practical safety applications. Bridges and buildings continued to be designed by precedent until the late 19th century, when the [[Eiffel Tower]] and [[Ferris wheel]] demonstrated the validity of the theory on large scales.\n\n== Static beam equation ==\n\nThe Euler–Bernoulli equation describes the relationship between the beam's [[deflection (engineering)|deflection]] and the applied load:<ref name=Gere>Gere, J. M. and Timoshenko, S. P., 1997, ''Mechanics of Materials'', PWS Publishing Company.</ref>\n\n:<math>\\frac{\\mathrm{d}^2}{\\mathrm{d} x^2}\\left(EI \\frac{\\mathrm{d}^2 w}{\\mathrm{d} x^2}\\right) = q\\,</math>\n\nThe curve <math>w(x)</math> describes the deflection  of the beam in the <math>z</math> direction at some position <math>x</math> (recall that the beam is modeled as a one-dimensional object). <math>q</math> is a distributed load, in other words a force per unit length (analogous to [[pressure]] being a force per area); it may be a function of <math>x</math>, <math>w</math>, or other variables.  <math>E</math> is the [[elastic modulus]] and <math>I</math> is the [[second moment of area]] of the beam's cross-section. <math>I</math> must be calculated with respect to the axis which passes through the centroid of the cross-section and which is perpendicular to the applied loading.<ref group=\"N\">For an Euler–Bernoulli beam not under any axial loading this axis is called the [[neutral axis]].</ref> Explicitly, for a beam whose axis is oriented along ''x'' with a loading along ''z'', the beam's cross-section is in the ''yz'' plane, and the relevant second moment of area is\n:<math> I = \\iint z^2\\; dy\\; dz,</math>\nwhere it is assumed that the centroid of the cross-section occurs at ''y''&nbsp;= ''z''&nbsp;= 0.\n\nOften, the product <math>EI</math> (known as the [[flexural rigidity]]) is a constant, so that\n\n:<math>EI \\frac{\\mathrm{d}^4 w}{\\mathrm{d} x^4} = q(x).\\,</math>\n\nThis equation, describing the deflection of a uniform, static beam, is used widely in engineering practice.  Tabulated expressions for the deflection <math>w</math> for common beam configurations can be found in engineering handbooks.  For more complicated situations, the deflection can be determined by solving the Euler–Bernoulli equation using techniques such as \"[[Direct integration of a beam|direct integration]]\", \"[[Macaulay's method]]\", \"[[Moment-area theorem|moment area method]], \"[[conjugate beam method]]\", \"[[virtual work|the principle of virtual work]]\", \"[[Castigliano's method]]\", \"[[flexibility method]]\", \"[[slope deflection method]]\", \"[[moment distribution method]]\", or \"[[direct stiffness method]]\".\n\nSign conventions are defined here since different conventions can be found in the literature.<ref name=Gere />  In this article, a [[Right-hand rule|right-handed]] coordinate system is used as shown in the figure, Bending of an Euler–Bernoulli beam.  In this figure, the x and z direction of a right-handed coordinate system are shown.  Since <math>\\mathbf{e_z} \\times \\mathbf{e_x} =  \\mathbf{e_y}  </math> where <math>\\mathbf{e_x}</math>, <math>\\mathbf{e_y}</math>, and <math>\\mathbf{e_z}</math> are unit vectors in the direction of the x, y, and z axes respectively, the y axis direction is into the figure. Forces acting in the positive <math>x</math> and <math>z</math> directions are assumed positive.  The sign of the bending moment <math>M</math> is positive when the torque vector associated with the bending moment on the right hand side of the section is in the positive y direction (i.e. so that a positive value of <math>M</math> leads to a compressive stress at the bottom fibers).  With this choice of bending moment sign convention, in order to have <math> dM = Qdx </math>, it is necessary that the shear force <math> Q </math> acting on the right side of the section be positive in the z direction so as to achieve static equilibrium of moments.  To have force equilibrium with <math> dQ = qdx </math>, the loading intensity <math>q</math> must be positive in the negative z direction.  In addition to these sign conventions for scalar quantities, we also sometimes use vectors in which the directions of the vectors is made clear through the use of the unit vectors, <math>\\mathbf{e_x}</math>, <math>\\mathbf{e_y}</math>, and <math>\\mathbf{e_z}</math>.\n\nSuccessive derivatives of the deflection <math>w</math> have important physical meanings: <math>dw/dx</math> is the slope of the beam,\n:<math> M = -EI \\frac{d^2w}{dx^2}</math>\nis the [[bending moment]] in the beam, and\n:<math>Q = -\\frac{d}{d x}\\left(EI\\frac{d^2 w}{d x^2}\\right)</math>\nis the [[shear force]] in the beam.\n[[File:Euler-Bernoulli beam theory-2.svg|thumb|350px|Bending of an Euler–Bernoulli beam. Each cross-section of the beam is at 90 degrees to the neutral axis.]]\nThe stresses in a beam can be calculated from the above expressions after the deflection due to a given load has been determined.\n\n=== Derivation of bending moment equation ===\n\nBecause of the fundamental importance of the bending moment equation in engineering, we will provide a short derivation.  We change to polar coordinates.  The length of the neutral axis in the figure is <math>\\rho d \\theta.</math> The length of a fiber with a radial distance <math>z</math> below the neutral axis is  <math>(\\rho + z)d \\theta.</math> Therefore, the strain of this fiber is\n\n:<math> \\frac {\\left(\\rho + z - \\rho \\right ) \\ d \\theta} {\\rho \\ d \\theta} = \\frac {z} {\\rho}.</math>\n\nThe stress of this fiber is <math> E\\tfrac {z} {\\rho}</math> where <math>E</math> is the [[elastic modulus]] in accordance with [[Hooke's Law]].  The differential force vector, <math> d\\mathbf{F},</math> resulting from this stress is given by,\n\n:<math> d\\mathbf{F} = E \\frac{z}{\\rho} dA \\mathbf{e_x}.</math>\n\nThis is the differential force vector exerted on the right hand side of the section shown in the figure.  We know that it is in the <math>\\mathbf{e_x}</math> direction since the figure clearly shows that the fibers in the lower half are in tension. <math>dA</math> is the differential element of area at the location of the fiber. The differential bending moment vector, <math> d\\mathbf{M} </math> associated with <math> d\\mathbf{F}</math> is given by\n\n:<math> d\\mathbf{M} = -z\\mathbf{e_z} \\times  d\\mathbf{F} = -\\mathbf{e_y} E \\frac {z^2} {\\rho} dA. </math>\n\nThis expression is valid for the fibers in the lower half of the beam. The expression for the fibers in the upper half of the beam will be similar except that the moment arm vector will be in the positive z direction and the force vector will be in the -x direction since the upper fibers are in compression.  But the resulting bending moment vector will still be in the -y direction since <math>\\mathbf{e_z} \\times -\\mathbf{e_x} =  -\\mathbf{e_y}.</math>  Therefore, we integrate over the entire cross section of the beam and get for <math>\\mathbf{M}</math> the bending moment vector exerted on the right cross section of the beam the expression\n\n:<math>\\mathbf{M} =\\int d\\mathbf{M} = -\\mathbf{e_y} \\frac {E} {\\rho} \\int {z^2} \\ dA = -\\mathbf{e_y} \\frac {EI} {\\rho},</math>\n\nwhere <math>I</math> is the [[second moment of area]]. From calculus, we know that when <math>\\tfrac{dw}{dx}</math> is small as it is for an Euler–Bernoulli beam, <math>\\tfrac{1}{\\rho} = \\tfrac{d^2w}{dx^2}</math> (<math> \\rho </math> is the [[radius of curvature]]). Therefore,\n\n:<math>\\mathbf{M} = -\\mathbf{e_y} EI {d^2w \\over dx^2}. </math>\n\n== Dynamic beam equation ==\n[[File:Beam mode 6.gif|thumb|200px|right|[[Finite element method]] model of a vibration of a wide-flange beam ([[I-beam|{{ibeam}}-beam]]).]]\nThe dynamic beam equation is the [[Euler–Lagrange equation]] for the  following action\n:<math>\n  S = \\int_{t_{1}}^{t_{2}}\\int_0^L \\left[ \\frac{1}{2} \\mu \\left( \\frac{\\partial w}{\\partial t} \\right)^2 - \\frac{1}{2} EI \\left( \\frac{ \\partial^2 w}{\\partial x^2} \\right)^2 + q(x) w(x,t)\\right] dx dt.\n</math>\nThe first term represents the kinetic energy where <math>\\mu</math> is the mass per unit length; the second one represents the potential energy due to internal forces (when considered with a negative sign) and the third term represents the potential energy due to the external load <math> q(x) </math>.  The [[Euler–Lagrange equation]] is used to determine the function that minimizes the functional <math>S</math>.  For a dynamic Euler–Bernoulli beam, the Euler–Lagrange equation is\n<blockquote style=\"border: 1px solid black; padding:10px; width:300px\">\n:<math>\n  \\cfrac{\\partial^2 }{\\partial x^2}\\left(EI\\cfrac{\\partial^2 w}{\\partial x^2}\\right) = - \\mu\\cfrac{\\partial^2 w}{\\partial t^2} + q(x)\n </math>\n</blockquote>\n:{| class=\"toccolours collapsible collapsed\" width=\"60%\" style=\"text-align:left\"\n!Derivation of Euler–Lagrange equation for beams\n|-\n|Since the [[Lagrangian (field theory)|Lagrangian]] is\n:<math>\n   \\mathcal{L} := \\tfrac{1}{2} \\mu \\left( \\frac{\\partial w}{\\partial t} \\right)^2 - \\tfrac{1}{2} EI \\left( \\frac{ \\partial^2 w}{\\partial x^2} \\right)^2 + q(x) w(x,t) = \\tfrac{\\mu}{2}\\dot{w}^2 - \\tfrac{EI}{2}w_{xx}^2 + qw \\equiv \\mathcal{L}(x, t, w, \\dot{w}, w_{xx})\n </math>\nthe corresponding [[Euler–Lagrange equation#Single function of two variables with higher derivatives|Euler–Lagrange equation]] is\n:<math>\n   \\cfrac{\\partial\\mathcal{L}}{\\partial w} - \\frac{\\partial}{\\partial t}\\left(\\frac{\\partial \\mathcal{L}}{\\partial \\dot{w}}\\right)  + \\frac{\\partial^2}{\\partial x^2}\\left(\\frac{\\partial \\mathcal{L}}{\\partial w_{xx}}\\right) = 0\n </math>\nNow,\n:<math>\n   \\cfrac{\\partial\\mathcal{L}}{\\partial w} = q ~;~~ \\frac{\\partial \\mathcal{L}}{\\partial \\dot{w}} = \\mu\\dot{w}\n   ~;~~ \\frac{\\partial \\mathcal{L}}{\\partial w_{xx}} = -EI w_{xx} ~.\n </math>\nPlugging into the Euler–Lagrange equation gives\n:<math>\n   q - \\mu\\ddot{w} - (EI w_{xx})_{xx} = 0\n </math>\nor,\n:<math>\n  \\cfrac{\\partial^2 }{\\partial x^2}\\left(EI\\cfrac{\\partial^2 w}{\\partial x^2}\\right) = - \\mu\\cfrac{\\partial^2 w}{\\partial t^2} + q\n </math>\n</blockquote>\nwhich is the governing equation for the dynamics of an Euler–Bernoulli beam.\n|}\nWhen the beam is homogeneous, <math>E</math> and <math>I</math> are independent of <math>x</math>, and the beam equation is simpler:\n:<math>\n  EI\\cfrac{\\partial^4 w}{\\partial x^4} = - \\mu\\cfrac{\\partial^2 w}{\\partial t^2} + q \\,.\n </math>\n\n=== Free vibration ===\nIn the absence of a transverse load, <math>q</math>, we have the '''free vibration''' equation.  This equation can be solved using a Fourier decomposition of the displacement into the sum of harmonic vibrations of the form\n:<math>\n   w(x,t) = \\text{Re}[\\hat{w}(x)~e^{-i\\omega t}]\n </math>\nwhere <math>\\omega</math> is the frequency of vibration.  Then, for each value of frequency, we can solve an ordinary differential equation\n:<math>\n   EI~\\cfrac{\\mathrm{d}^4 \\hat{w}}{\\mathrm{d}x^4} + \\mu\\omega^2\\hat{w} = 0 \\,.\n </math>\nThe general solution of the above equation is\n:<math>\n   \\hat{w} = A_1\\cosh(\\beta x) + A_2\\sinh(\\beta x) + A_3\\cos(\\beta x) + A_4\\sin(\\beta x) \\quad \\text{with} \\quad \\beta := \\left(\\frac{\\mu\\omega^2}{EI}\\right)^{1/4}\n </math>\nwhere <math>A_1,A_2,A_3,A_4</math> are constants.  These constants are unique for a given set of boundary conditions.  However, the solution for the displacement is not unique and depends on the frequency.  These solutions are typically written as\n:<math>\n   \\hat{w}_n = A_1\\cosh(\\beta_n x) + A_2\\sinh(\\beta_n x) + A_3\\cos(\\beta_n x) + A_4\\sin(\\beta_n x) \\quad \\text{with} \\quad \\beta_n := \\left(\\frac{\\mu\\omega_n^2}{EI}\\right)^{1/4}\\,.\n </math>\nThe quantities <math>\\omega_n</math> are called the '''natural frequencies''' of the beam.  Each of the displacement solutions is called a '''mode''' and the shape of the displacement curve is called the '''mode shape'''.\n\n==== Example: Cantilevered beam ====\n[[File:BeamVibrationPlot.svg|thumb|400px|right|Mode shapes for the first four modes of a vibrating cantilever beam.]]\nThe boundary conditions for a cantilevered beam of length <math>L</math> (fixed at <math> x = 0</math>) are\n:<math>\n  \\begin{align}\n   &\\hat{w}_n = 0 ~,~~ \\frac{d\\hat{w}_n}{dx} = 0 \\quad \\text{at} ~~ x = 0 \\\\\n   &\\frac{d^2\\hat{w}_n}{dx^2} = 0 ~,~~ \\frac{d^3\\hat{w}_n}{dx^3} = 0 \\quad \\text{at} ~~ x = L \\,.\n   \\end{align}\n </math>\nIf we apply these conditions, non-trivial solutions are found to exist only if\n<math>\n   \\cosh(\\beta_n L)\\,\\cos(\\beta_n L) + 1 = 0 \\,.\n </math>\nThis nonlinear equation can be solved numerically. The first few roots are ''β<sub>1</sub> L/π'' = 0.59686..., ''β<sub>2</sub> L/π'' = 1.49418..., ''β<sub>3</sub> L/π'' = 2.50025..., ''β<sub>4</sub> L/π'' = 3.49999..., ...\n\nThe corresponding natural frequencies of vibration are\n:<math>\n  \\omega_1 = \\beta_1^2 \\sqrt{\\frac{EI}{\\mu}} = \\frac{3.5161}{L^2}\\sqrt{\\frac{EI}{\\mu}} ~,~~ \\dots\n</math>\nThe boundary conditions can also be used to determine the mode shapes from the solution for the displacement:\n:<math>\n  \\hat{w}_n = A_1 \\Bigl[(\\cosh\\beta_n x - \\cos\\beta_n x) +\n      \\frac{\\cos\\beta_n L + \\cosh\\beta_n L}{\\sin\\beta_n L + \\sinh\\beta_n L}(\\sin\\beta_n x - \\sinh\\beta_n x)\\Bigr]\n </math>\nThe unknown constant (actually constants as there is one for each <math>n</math>), <math>A_1</math>, which in general is complex, is determined by the initial conditions at <math>t = 0</math> on the velocity and displacements of the beam. Typically a value of <math>A_1 = 1</math> is used when plotting mode shapes.  Solutions to the undampened forced problem have unbounded displacements when the driving frequency matches a natural frequency <math>\\omega_n</math>, i.e., the beam can '''resonate'''.  The natural frequencies of a beam therefore correspond to the frequencies at which '''resonance''' can occur.\n\n==== Example: unsupported (free-free) beam ====\n\n[[File:FreeBeamVibrationPlot.svg|thumb|400px|The first four modes of a vibrating free-free Euler-Bernoulli beam.]]\n\nA free-free beam is a beam without any supports.<ref name=\"???\">{{cite web | url=http://www.varg.unsw.edu.au/Assets/link%20pdfs/Beam_vibration.pdf |title=Vibrations of a Free-Free Beam |last1=Caresta |first1=Mauro |date= |website= |publisher= |access-date=2019-03-20}}</ref> The boundary conditions for a free beam of length ''L'' extending from ''x''=0 to ''x''=L is given by:\n\n:<math>\n\\frac{d^2\\hat{w}_n}{dx^2} = 0 ~,~~ \\frac{d^3\\hat{w}_n}{dx^3} = 0 \\quad \\text{at} ~~ x = 0  \\,\\text{and} \\, L \\,.\n</math>\nIf we apply these conditions, non-trivial solutions are found to exist only if\n<math>\n   \\cosh(\\beta_n L)\\,\\cos(\\beta_n L) - 1 = 0 \\,.\n</math>\n\nThis nonlinear equation can be solved numerically. The first few roots are ''β<sub>1</sub> L/π'' = 1.50562..., ''β<sub>2</sub> L/π'' = 2.49975..., ''β<sub>3</sub> L/π'' = 3.50001..., ''β<sub>4</sub> L/π'' = 4.50000...\n\nThe corresponding natural frequencies of vibration are:\n:<math>\n  \\omega_1 = \\beta_1^2 \\sqrt{\\frac{EI}{\\mu}} = \\frac{22.3733}{L^2}\\sqrt{\\frac{EI}{\\mu}} ~,~~ \\dots\n</math>\nThe boundary conditions can also be used to determine the mode shapes from the solution for the displacement:\n:<math>\n  \\hat{w}_n = A_1 \\Bigl[  (\\sin\\beta_n x+\\sinh\\beta_n x )+\n      \\frac{\\sin\\beta_n L - \\sinh\\beta_n L}{\\cosh\\beta_n L - \\cos\\beta_n L}(\\cos\\beta_n x + \\cosh\\beta_n x)\\Bigr]\n </math>\nAs with the cantilevered beam, the unknown constants are determined by the initial conditions at <math>t = 0</math> on the velocity and displacements of the beam. Also, solutions to the undampened forced problem have unbounded displacements when the driving frequency matches a natural frequency <math>\\omega_n</math>.\n\n== Stress ==\nBesides deflection, the beam equation describes [[force]]s and [[Moment (physics)|moments]] and can thus be used to describe [[Stress (mechanics)|stresses]]. For this reason, the Euler–Bernoulli beam equation is widely used in [[engineering]], especially civil and mechanical, to determine the strength (as well as deflection) of beams under bending.\n\nBoth the [[bending moment]] and the [[shear force]] cause stresses in the beam. The stress due to shear force is maximum along the [[neutral axis]] of the beam (when the width of the beam, t, is constant along the cross section of the beam; otherwise an integral involving the first moment and the beam's width needs to be evaluated for the particular cross section), and the maximum tensile stress is at either the top or bottom surfaces. Thus the maximum [[principal stress]] in the beam may be neither at the surface nor at the center but in some general area. However, shear force stresses are negligible in comparison to bending moment stresses in all but the stockiest of beams as well as the fact that [[stress concentration]]s commonly occur at surfaces, meaning that the maximum stress in a beam is likely to be at the surface.\n\n=== Simple or symmetrical bending ===\n[[File:BeamBendingUpdated.svg|thumb|350px|Element of a bent beam: the fibers form concentric arcs, the top fibers are compressed and bottom fibers stretched.]]\nFor beam cross-sections that are symmetrical about a plane perpendicular to the neutral plane, it can be shown that the tensile stress experienced by the beam may be expressed as:\n\n:<math>\\sigma = \\frac{Mz}{I} = -zE ~ \\frac{\\mathrm{d}^2 w}{\\mathrm{d} x^2}.\\,</math>\n\nHere, <math>z</math> is the distance from the neutral axis to a point of interest; and <math>M</math> is the bending moment. Note that this equation implies that [[pure bending]] (of positive sign) will cause zero stress at the neutral axis, positive (tensile) stress at the \"top\" of the beam, and negative (compressive) stress at the bottom of the beam; and also implies that the maximum stress will be at the top surface and the minimum at the bottom. This bending stress may be superimposed with axially applied stresses, which will cause a shift in the neutral (zero stress) axis.\n\n=== Maximum stresses at a cross-section ===\n[[File:SectionModulusOfBeam.svg|thumb|220px|Quantities used in the definition of the section modulus of a beam.]]\nThe maximum tensile stress at a cross-section is at the location <math>z = c_1</math> and the maximum compressive stress is at the location <math>z = -c_2</math> where the height of the cross-section is <math> h = c_1 + c_2 </math>.  These stresses are\n:<math>\n   \\sigma_1 = \\cfrac{Mc_1}{I} = \\cfrac{M}{S_1} ~;~~ \\sigma_2 = -\\cfrac{Mc_2}{I} = -\\cfrac{M}{S_2}\n </math>\nThe quantities <math>S_1,S_2</math> are the [[section modulus|section moduli]]<ref name=Gere /> and are defined as\n:<math>\n   S_1 = \\cfrac{I}{c_1} ~;~~ S_2 = \\cfrac{I}{c_2}\n </math>\nThe section modulus combines all the important geometric information about a beam's section into one quantity.  For the case where a beam is doubly symmetric, <math>c_1 = c_2</math> and we have one section modulus <math>S = I/c</math>.\n\n=== Strain in an Euler–Bernoulli beam ===<!-- It's pronounced \"oiler\"; DO NOT write \"a Euler\"; it's \"an Euler\". -->\nWe need an expression for the [[infinitesimal strain theory|strain]] in terms of the deflection of the neutral surface to relate the stresses in an Euler–Bernoulli beam to the deflection.  To obtain that expression we use the assumption that normals to the neutral surface remain normal during the deformation and that deflections are small.  These assumptions imply that the beam bends into an arc of a circle of radius <math>\\rho</math> (see Figure 1) and that the neutral surface does not change in length during the deformation.<ref name=Gere />\n\nLet <math>\\mathrm{d}x</math> be the length of an element of the neutral surface in the undeformed state.  For small deflections, the element does not change its length after bending but deforms into an arc of a circle of radius <math>\\rho</math>.  If <math>\\mathrm{d}\\theta</math> is the angle subtended by this arc, then <math>\\mathrm{d}x = \\rho~\\mathrm{d}\\theta</math>.\n\nLet us now consider another segment of the element at a distance <math>z</math> above the neutral surface. The initial length of this element is <math>\\mathrm{d}x</math>.  However, after bending, the length of the element becomes <math>\\mathrm{d}x' = (\\rho-z)~\\mathrm{d}\\theta = \\mathrm{d}x - z~\\mathrm{d}\\theta</math>.  The strain in that segment of the beam is given by\n:<math>\n  \\varepsilon_x = \\cfrac{\\mathrm{d}x'-\\mathrm{d}x}{\\mathrm{d}x} = -\\cfrac{z}{\\rho} = -\\kappa~z\n </math>\nwhere <math>\\kappa</math> is the [[curvature]] of the beam.  This gives us the axial strain in the beam as a function of distance from the neutral surface.  However, we still need to find a relation between the radius of curvature and the beam deflection <math>w</math>.\n\n=== Relation between curvature and beam deflection ===\nLet P be a point on the neutral surface of the beam at a distance <math>x</math> from the origin of the <math>(x,z)</math> coordinate system.  The slope of the beam is approximately equal to the angle made by the neutral surface with the <math>x</math>-axis for the small angles encountered in beam theory.  Therefore, with this approximation,\n:<math>\n   \\theta(x) = \\cfrac{\\mathrm{d}w}{\\mathrm{d}x}\n </math>\nTherefore, for an infinitesimal element <math>\\mathrm{d}x</math>, the relation <math>\\mathrm{d}x = \\rho~\\mathrm{d}\\theta</math> can be written as\n:<math>\n    \\cfrac{1}{\\rho} = \\cfrac{\\mathrm{d}\\theta}{\\mathrm{d}x} = \\cfrac{\\mathrm{d}^2w}{\\mathrm{d}x^2} = \\kappa\n </math>\nHence the strain in the beam may be expressed as\n:<math>\n   \\varepsilon_{x} = -z\\kappa\n </math>\n\n=== Stress-strain relations ===\nFor a homogeneous [[isotropy|isotropic]] [[linear elasticity|linear elastic]] material, the stress is related to the strain by <math>\\sigma = E\\varepsilon</math>, where <math>E</math> is the [[Young's modulus]].  Hence the stress in an Euler–Bernoulli beam is given by\n:<math>\n   \\sigma_x = -zE\\cfrac{\\mathrm{d}^2w}{\\mathrm{d}x^2}\n </math>\nNote that the above relation, when compared with the relation between the axial stress and the bending moment, leads to\n:<math>\n   M = -EI\\cfrac{\\mathrm{d}^2w}{\\mathrm{d}x^2}\n </math>\nSince the shear force is given by <math>Q = \\mathrm{d}M/\\mathrm{d}x</math>, we also have\n:<math>\n   Q = -EI\\cfrac{\\mathrm{d}^3w}{\\mathrm{d}x^3}\n </math>\n\n== Boundary considerations ==\nThe beam equation contains a fourth-order derivative in <math>x</math>.  To find a unique solution <math>w(x,t)</math> we need four boundary conditions.  The boundary conditions usually model ''supports'', but they can also model point loads, distributed loads and moments.  The ''support'' or displacement boundary conditions are used to fix values of displacement (<math>w</math>) and rotations (<math>\\mathrm{d}w/\\mathrm{d}x</math>) on the boundary.  Such boundary conditions are also called [[Dirichlet boundary conditions]].  Load and moment boundary conditions involve higher derivatives of <math>w</math> and represent [[flux|momentum flux]].  Flux boundary conditions are also called [[Neumann boundary condition]]s.\n\nAs an example consider a [[cantilever]] beam that is built-in at one end and free at the other as shown in the adjacent figure.  At the built-in end of the beam there cannot be any displacement or rotation of the beam.  This means that at the left end both deflection and slope are zero.  Since no external bending moment is applied at the free end of the beam, the bending moment at that location is zero.  In addition, if there is no external force applied to the beam, the shear force at the free end is also zero.\n\nTaking the <math>x</math> coordinate of the left end as <math>0</math> and the right end as <math>L</math> (the length of the beam), these statements translate to the following set of boundary conditions (assume <math>EI</math> is a constant):\n\n:<math>w|_{x = 0} = 0 \\quad ; \\quad \\frac{\\partial w}{\\partial x}\\bigg|_{x = 0} = 0 \\qquad \\mbox{(fixed end)}\\,</math>[[File:Cantilever Beam.svg|thumb|400px|right|A cantilever beam.]]\n\n:<math>\\frac{\\partial^2 w}{\\partial x^2}\\bigg|_{x = L} = 0 \\quad ; \\quad \\frac{\\partial^3 w}{\\partial x^3}\\bigg|_{x = L} = 0 \\qquad \\mbox{(free end)}\\,</math>\n\nA simple support (pin or roller) is equivalent to a point force on the beam which is adjusted in such a way as to fix the position of the beam at that point. A fixed support or clamp, is equivalent to the combination of a point force and a point torque which is adjusted in such a way as to fix both the position and slope of the beam at that point. Point forces and torques, whether from supports or directly applied, will divide a beam into a set of segments, between which the beam equation will yield a continuous solution, given four boundary conditions, two at each end of the segment. Assuming that the product ''EI'' is a constant, and defining <math>\\lambda=F/EI</math> where ''F'' is the magnitude of a point force, and <math>\\tau=M/EI</math> where ''M'' is the magnitude of a point torque, the boundary conditions appropriate for some common cases is given in the table below. The change in a particular derivative of ''w'' across the boundary as ''x'' increases is denoted by <math>\\Delta</math> followed by that derivative. For example, <math>\\Delta w''=w''(x+)-w''(x-)</math> where <math>w''(x+)</math> is the value of <math>w''</math> at the lower boundary of the upper segment, while <math>w''(x-)</math> is the value of <math>w''</math> at the upper boundary of the lower segment. When the values of the particular derivative are not only continuous across the boundary, but fixed as well, the boundary condition is written e.g., <math>\\Delta w''=0^*</math> which actually constitutes two separate equations (e.g., <math>w''(x-) = w''(x+)</math> = fixed).\n\n:{| class=\"wikitable\"\n|-\n! Boundary\n! <math>w'''</math>\n! <math>w''</math>\n! <math>w'</math>\n! <math>w</math>\n|-\n| Clamp\n|\n|\n| <math>\\Delta w'=0^*</math>\n| <math>\\Delta w=0^*</math>\n|-\n| Simple support\n|\n| <math>\\Delta w''=0</math>\n| <math>\\Delta w'=0</math>\n| <math>\\Delta w=0^*</math>\n|-\n| Point force\n| <math>\\Delta w'''=\\lambda</math>\n| <math>\\Delta w''=0</math>\n| <math>\\Delta w'=0</math>\n| <math>\\Delta w=0</math>\n|-\n| Point torque\n| <math>\\Delta w'''=0</math>\n| <math>\\Delta w''=\\tau</math>\n| <math>\\Delta w'=0</math>\n| <math>\\Delta w=0</math>\n|-\n| Free end\n| <math>w'''=0</math>\n| <math>w''=0</math>\n|\n|\n|-\n| Clamp at end\n|\n|\n| <math>w'</math> fixed\n| <math>w</math> fixed\n|-\n| Simply supported end\n|\n| <math>w''=0</math>\n|\n| <math>w</math> fixed\n|-\n| Point force at end\n| <math>w'''=\\pm\\lambda</math>\n| <math>w''=0</math>\n|\n|\n|-\n| Point torque at end\n| <math>w'''=0</math>\n| <math>w''=\\pm\\tau</math>\n|\n|\n|}\n\nNote that in the first cases, in which the point forces and torques are located between two segments, there are four boundary conditions, two for the lower segment, and two for the upper. When forces and torques are applied to one end of the beam, there are two boundary conditions given which apply at that end. The sign of the point forces and torques at an end will be positive for the lower end, negative for the upper end.\n\n== Loading considerations ==\nApplied loads may be represented either through boundary conditions or through the function <math>q(x,t)</math> which represents an external distributed load. Using distributed loading is often favorable for simplicity. Boundary conditions are, however, often used to model loads depending on context; this practice being especially common in vibration analysis.\n\nBy nature, the distributed load is very often represented in a piecewise manner, since in practice a load isn't typically a continuous function. Point loads can be modeled with help of the [[Dirac delta function]]. For example, consider a static uniform cantilever beam of length <math>L</math> with an upward point load <math>F</math> applied at the free end. Using boundary conditions, this may be modeled in two ways.  In the first approach, the applied point load is approximated by a shear force applied at the free end.  In that case the governing equation and boundary conditions are:\n:<math>\n  \\begin{align}\n  & EI \\frac{\\mathrm{d}^4 w}{\\mathrm{d} x^4} = 0 \\\\\n  & w|_{x = 0} = 0 \\quad ; \\quad \\frac{\\mathrm{d} w}{\\mathrm{d} x}\\bigg|_{x = 0} = 0 \\quad ; \\quad\n  \\frac{\\mathrm{d}^2 w}{\\mathrm{d} x^2}\\bigg|_{x = L} = 0 \\quad ; \\quad -EI \\frac{\\mathrm{d}^3 w}{\\mathrm{d} x^3}\\bigg|_{x = L} = F\\,\n  \\end{align}\n </math>\n\nAlternatively we can represent the point load as a distribution using the Dirac function.  In that case the equation and boundary conditions are\n:<math>\n  \\begin{align}\n  & EI \\frac{\\mathrm{d}^4 w}{\\mathrm{d} x^4} = F \\delta(x - L) \\\\\n  & w|_{x = 0} = 0 \\quad ; \\quad \\frac{\\mathrm{d} w}{\\mathrm{d} x}\\bigg|_{x = 0} = 0 \\quad; \\quad\n  \\frac{\\mathrm{d}^2 w}{\\mathrm{d} x^2}\\bigg|_{x = L} = 0\\,\n  \\end{align}\n </math>\n\nNote that shear force boundary condition (third derivative) is removed, otherwise there would be a contradiction. These are equivalent [[boundary value problem]]s, and both yield the solution\n\n:<math>w = \\frac{F}{6 EI}(3 L x^2 - x^3)\\,~.</math>\n\nThe application of several point loads at different locations will lead to <math>w(x)</math> being a piecewise function. Use of the Dirac function greatly simplifies such situations; otherwise the beam would have to be divided into sections, each with four boundary conditions solved separately. A well organized family of functions called [[Singularity function]]s are often used as a shorthand for the Dirac function, its [[derivative]], and its [[antiderivatives]].\n\nDynamic phenomena can also be modeled using the static beam equation by choosing appropriate forms of the load distribution. As an example, the free [[vibration]] of a beam can be accounted for by using the load function:\n\n:<math>q(x, t) = \\mu \\frac{\\partial^2 w}{\\partial t^2}\\,</math>\n\nwhere <math>\\mu</math> is the [[linear mass density]] of the beam, not necessarily a constant. With this time-dependent loading, the beam equation will be a [[partial differential equation]]:\n\n:<math> \\frac{\\partial^2}{\\partial x^2} \\left( EI \\frac{\\partial^2 w}{\\partial x^2} \\right) = -\\mu \\frac{\\partial^2 w}{\\partial t^2}.</math>\n\nAnother interesting example describes the deflection of a beam rotating with a constant [[angular frequency]] of <math>\\omega</math>:\n\n:<math>q(x) = \\mu \\omega^2 w(x)\\,</math>\n\nThis is a [[centripetal force]] distribution. Note that in this case, <math>q</math> is a function of the displacement (the dependent variable), and the beam equation will be an autonomous [[ordinary differential equation]].\n\n== Examples ==\n\n=== Three-point bending ===\nThe [[three point flexural test|three point bending test]] is a classical experiment in mechanics. It represents the case of a beam resting on two roller supports and subjected to a concentrated load applied in the middle of the beam.  The shear is constant in absolute value: it is half the central load, P / 2. It changes sign in the middle of the beam. The bending moment varies linearly from one end, where it is 0, and the center where its absolute value is PL / 4, is where the risk of rupture is the most important.\nThe deformation of the beam is described by a polynomial of third degree over a half beam (the other half being symmetrical).\n<!--{|\n|[[File:Poutre flexion trois points diagrammes.svg|thumb|300px|a) Three-point bending.]]\n|[[File:Poutre appuis charge ponctuelle diagrammes.svg|thumb|340px|b) Generalized three-point bending]]\n|}-->\nThe bending moments (<math>M</math>), shear forces (<math>Q</math>), and deflections (<math>w</math>) for a beam subjected to a central point load and an asymmetric point load are given in the table below.<ref name=Gere />\n{| class=\"wikitable\"\n|-\n! Distribution\n! Max. value\n|-\n| colspan=\"2\" | '''Simply supported beam with central load'''\n| rowspan=\"4\" |[[File:SimpSuppBeamPointLoad.svg|350px]]\n|-\n| <math>M(x) = \\begin{cases}\n    \\frac{Px}{2},  & \\mbox{for } 0 \\le x \\le \\tfrac{L}{2} \\\\\n    \\frac{P(L-x)}{2}, & \\mbox{for } \\tfrac{L}{2} < x \\le L\n    \\end{cases}</math>\n| <math>M_{L/2} = \\tfrac{PL}{4}</math>\n|-\n| <math>Q(x) = \\begin{cases}\n    \\frac{P}{2}, & \\mbox{for } 0 \\le x \\le \\tfrac{L}{2} \\\\\n    \\frac{-P}{2}, & \\mbox{for } \\tfrac{L}{2} < x \\le L\n    \\end{cases}</math>\n| <math>|Q_0| = |Q_L| = \\tfrac{P}{2}</math>\n|-\n| <math>w(x) = \\begin{cases}\n    -\\frac{Px(4x^2-3L^2)}{48EI}, & \\mbox{for } 0 \\le x \\le \\tfrac{L}{2} \\\\\n    \\frac{P(x-L)(L^2-8Lx+4x^2)}{48EI}, & \\mbox{for } \\tfrac{L}{2} < x \\le L\n    \\end{cases}</math>\n| <math>w_{L/2} = \\tfrac{PL^3}{48EI}</math>\n|-\n| colspan=\"2\" | '''Simply supported beam with asymmetric load'''\n| rowspan=\"4\" |[[File:SimpSuppBeamPointLoadUnsymm.svg|350px]]\n|-\n| <math>M(x) = \\begin{cases}\n    \\tfrac{Pbx}{L},  & \\mbox{for } 0 \\le x \\le a  \\\\\n    \\tfrac{Pbx}{L}-P(x-a)=\\tfrac{Pa(L-x)}{L}, & \\mbox{for } a < x \\le L\n    \\end{cases}</math>\n| <math>M_B = \\tfrac{Pab}{L}</math>\n|-\n| <math>Q(x) = \\begin{cases}\n    \\tfrac{Pb}{L}, & \\mbox{for } 0 \\le x \\le a  \\\\\n    \\tfrac{Pb}{L}-P, & \\mbox{for } a < x \\le L\n    \\end{cases}</math>\n| <math>Q_A = \\tfrac{Pb}{L}</math>\n<math>Q_C = \\tfrac{Pa}{L}</math>\n|-\n| <math>w(x) = \\begin{cases}\n    \\tfrac{Pbx(L^2-b^2-x^2)}{6LEI}, &  0 \\le x \\le a \\\\\n    \\tfrac{Pbx(L^2-b^2-x^2)}{6LEI}+\\tfrac{P(x-a)^3}{6EI}, &  a < x \\le L\n    \\end{cases}</math>\n| <math>w_{\\mathrm{max}} = \\tfrac{\\sqrt{3}Pb(L^2-b^2)^{\\frac{3}{2}}}{27LEI}</math>\nat <math>x = \\sqrt{\\tfrac{L^2-b^2}{3}}</math>\n|}\n\n=== Cantilever beams ===\nAnother important class of problems involves [[cantilever]] beams.  The bending moments (<math>M</math>), shear forces (<math>Q</math>), and deflections (<math>w</math>) for a cantilever beam subjected to a point load at the free end and a uniformly distributed load are given in the table below.<ref name=Gere />\n{| class=\"wikitable\" style=\"margin:auto;\"\n|-\n! Distribution\n! Max. value\n|-\n| colspan=\"2\" | '''Cantilever beam with end load'''\n| rowspan=\"4\" |[[File:CantBeamPointLoad.svg|350px]]\n|-\n| <math>M(x) = P(L-x)</math>\n| <math>M_A = PL</math>\n|-\n| <math>Q(x) = P </math>\n| <math>Q_{\\mathrm{max}} = P</math>\n|-\n| <math>w(x) = \\tfrac{Px^2(3L-x)}{6EI} </math>\n| <math>w_C = \\tfrac{PL^3}{3EI}</math>\n|-\n| colspan=\"2\" | '''Cantilever beam with uniformly distributed load'''\n| rowspan=\"4\" |[[File:CantBeamDistLoad.svg|350px]]\n|-\n| <math>M(x) =-\\tfrac{q(L^2-2Lx+x^2)}{2}</math>\n| <math>M_A = \\tfrac{qL^2}{2}</math>\n|-\n| <math>Q(x) = q(L-x), </math>\n| <math>Q_A = qL</math>\n|-\n| <math>w(x) = \\tfrac{qx^2(6L^2-4Lx+x^2)}{24EI} </math>\n| <math>w_C = \\tfrac{qL^4}{8EI}</math>\n|}\n\nSolutions for several other commonly encountered configurations are readily available in textbooks on mechanics of materials and engineering handbooks.\n\n=== Statically indeterminate beams ===\nThe [[bending moment]]s and [[shear force]]s in Euler–Bernoulli beams can often be determined directly using static balance of [[force]]s and [[moment (physics)|moments]].  However, for certain boundary conditions, the number of reactions can exceed the number of independent equilibrium equations.<ref name=Gere />  Such beams are called ''[[statically indeterminate]]''.\n\nThe built-in beams shown in the figure below are statically indeterminate.  To determine the stresses and deflections of such beams, the most direct method is to solve the Euler–Bernoulli beam equation with appropriate boundary conditions.  But direct analytical solutions of the beam equation are possible only for the simplest cases.  Therefore, additional techniques such as linear superposition are often used to solve statically indeterminate beam problems.\n\nThe superposition method involves adding the solutions of a number of statically determinate problems which are chosen such that the boundary conditions for the sum of the individual problems add up to those of the original problem.\n\n{| class=\"wikitable\" style=\"background:white; text-align:center;\"\n|-\n|[[File:Fem1.png|center|400px]]<br />(a) Uniformly distributed load ''q''. ||[[File:Fem3.png|center|400px]]<br />\n(b) Linearly distributed load with maximum ''q''<sub>0</sub>\n|-\n|<math>M_{\\mathrm{max}} = \\cfrac{qL^2}{12} ~;~~ w_{\\mathrm{max}} = \\cfrac{qL^4}{384EI}</math>||<math>M_{\\mathrm{max}} = \\cfrac{qL^2}{300}[3\\sqrt{30} - 10] ~;~~ w_{\\mathrm{max}} = \\cfrac{qL^4}{2500EI}[75 - 7\\sqrt{105}]</math>\n|-\n|[[File:Fem2.png|center|400px]]<br />(c) Concentrated load P||[[File:Fem4.png|center|400px]]<br />(d) Moment M<sub>0</sub>\n|}\n\nAnother commonly encountered statically indeterminate beam problem is the [[cantilever|cantilevered beam]] with the free end supported on a roller.<ref name=Gere />  The bending moments, shear forces, and deflections of such a beam are listed below:\n{| class=\"wikitable\"\n|-\n! Distribution\n! Max. value\n| rowspan=\"4\" |[[File:Beam1svg.svg|350px]]\n|-\n| <math>M(x) = -\\tfrac{q}{8}(L^2-5Lx+4x^2)</math>\n| <math>\\begin{align} M_B & = -\\tfrac{9qL^2}{128} \\mbox{ at } x=\\tfrac{5L}{8}\\\\\n                      M_A & = \\tfrac{qL^2}{8} \\end{align}</math>\n|-\n| <math>Q(x) = -\\tfrac{q}{8}(8x -5L)</math>\n| <math>Q_A = -\\tfrac{5qL}{8}</math>\n|-\n| <math>w(x) = \\tfrac{qx^2}{48EI}(3L^2-5Lx+2x^2)</math>\n| <math>w_{\\mathrm{max}} = \\tfrac{(39 - 55\\sqrt{33})}{65,536} \\tfrac{qL^4}{EI} \\mbox{ at } x=\\tfrac{15-\\sqrt{33}}{16}L</math>\n|}\n\n== Extensions ==\n\nThe kinematic assumptions upon which the Euler–Bernoulli beam theory is founded allow it to be extended to more advanced analysis.  Simple superposition allows for three-dimensional transverse loading.  Using alternative [[constitutive equation]]s can allow for [[Viscoelasticity|viscoelastic]] or [[plastic bending|plastic]] beam deformation.  Euler–Bernoulli beam theory can also be extended to the analysis of curved beams, [[Buckling|beam buckling]], composite beams, and geometrically nonlinear beam deflection.\n\nEuler–Bernoulli beam theory does not account for the effects of transverse [[Shear stress|shear]] strain.  As a result, it underpredicts deflections and overpredicts natural frequencies.  For thin beams (beam length to thickness ratios of the order 20 or more) these effects are of minor importance.  For thick beams, however, these effects can be significant.  More advanced beam theories such as the [[Timoshenko beam theory]] (developed by the Russian-born scientist [[Stephen Timoshenko]]) have been developed to account for these effects.\n\n=== Large deflections ===\n[[File:EulerBernoulliBeam.png|right|thumb|400px|Euler–Bernoulli beam]]\nThe original Euler–Bernoulli theory is valid only for [[infinitesimal strain theory|infinitesimal strains]] and small rotations.  The theory can be extended in a straightforward manner to problems involving moderately large rotations provided that the strain remains small by using the [[Theodore von Kármán|von Kármán]] strains.<ref name=Reddy>Reddy, J. N., (2007), ''Nonlinear finite element analysis'', Oxford University Press.</ref>\n\nThe Euler–Bernoulli hypotheses that plane sections remain plane and normal to the axis of the beam lead to displacements of the form\n:<math>\nv_1 = v_0(x) - z \\cfrac{\\mathrm{d}w_0}{\\mathrm{d}x} ~;~~v_2 = 0 ~;~~ v_3 = w_0(x)\n</math>\nUsing the definition of the Lagrangian Green strain from [[finite strain theory]], we can find the ''von Karman strains'' for the beam that are valid for large rotations but small strains.  These strains have the form\n:<math>\n\\begin{align}\n\\varepsilon_{11} & = \\cfrac{\\mathrm{d}u_0}{dx_1} - x_3\\cfrac{\\mathrm{d}^2w_0}{\\mathrm{d}x_1^2} +\n\\frac{1}{2}\\left[\n\\left(\\cfrac{\\mathrm{d}u_0}{\\mathrm{d}x_1}-x_3\\cfrac{\\mathrm{d}^2w_0}{\\mathrm{d}x_1^2}\\right)^2 +\n \\left(\\cfrac{\\mathrm{d}w_0}{\\mathrm{d}x_1}\\right)^2\\right] \\\\\n\\varepsilon_{22} & = 0 \\\\\n\\varepsilon_{33} & = \\frac{1}{2}\\left(\\cfrac{\\mathrm{d}w_0}{\\mathrm{d}x_1}\\right)^2 \\\\\n\\varepsilon_{23} & = 0 \\\\\n\\varepsilon_{31} & =\n\\frac{1}{2}\\left(\\cfrac{\\mathrm{d}w_0}{\\mathrm{d}x_1}-\\cfrac{\\mathrm{d}w_0}{\\mathrm{d}x_1}\\right) -\n \\frac{1}{2}\\left[\\left(\\cfrac{\\mathrm{d}u_0}{\\mathrm{d}x_1}-x_3\\cfrac{\\mathrm{d}^2w_0}{\\mathrm{d}x_1^2}\\right)\n \\left(\\cfrac{\\mathrm{d}w_0}{\\mathrm{d}x_1}\\right)\\right] \\\\\n\\varepsilon_{12} & = 0\n\\end{align}\n</math>\nFrom the [[principle of virtual work]], the balance of forces and moments in the beams gives us the equilibrium equations\n:<math>\n\\begin{align}\n\\cfrac{\\mathrm{d}N_{xx}}{\\mathrm{d}x} + f(x) & = 0 \\\\\n\\cfrac{\\mathrm{d}^2M_{xx}}{\\mathrm{d}x^2} + q(x) +\n \\cfrac{\\mathrm{d}}{\\mathrm{d}x}\\left(N_{xx}\\cfrac{\\mathrm{d}w_0}{\\mathrm{d}x}\\right) & = 0\n\\end{align}\n</math>\nwhere <math>f(x)</math> is the axial load, <math>q(x)</math> is the transverse load, and\n:<math>\nN_{xx} = \\int_A \\sigma_{xx}~ \\mathrm{d}A  ~;~~ M_{xx}  = \\int_A z\\sigma_{xx}~ \\mathrm{d}A\n</math>\nTo close the system of equations we need the [[constitutive equations]] that relate stresses to strains (and hence stresses to displacements).  For large rotations and small strains these relations are\n:<math>\n\\begin{align}\nN_{xx} & =\n A_{xx}\\left[\\cfrac{\\mathrm{d}u_0}{dx} + \\frac{1}{2}\\left(\\cfrac{\\mathrm{d}w_0}{\\mathrm{d}x}\\right)^2 \\right] -\n B_{xx}\\cfrac{\\mathrm{d}^2w_0}{\\mathrm{d}x^2} \\\\\nM_{xx} & =\n B_{xx}\\left[\\cfrac{du_0}{\\mathrm{d}x} + \\frac{1}{2}\\left(\\cfrac{\\mathrm{d}w_0}{\\mathrm{d}x}\\right)^2 \\right] -\n D_{xx}\\cfrac{\\mathrm{d}^2w_0}{\\mathrm{d}x^2}\n\\end{align}\n</math>\nwhere\n:<math>\n  A_{xx}  = \\int_A E~\\mathrm{d}A ~;~~ B_{xx}  = \\int_A zE~\\mathrm{d}A ~;~~ D_{xx}  = \\int_A z^2E~\\mathrm{d}A ~.\n </math>\nThe quantity <math>A_{xx}</math> is the ''[[stiffness|extensional stiffness]]'',<math>B_{xx}</math> is the coupled ''extensional-bending stiffness'', and <math>D_{xx}</math> is the ''[[bending stiffness]]''.\n\nFor the situation where the beam has a uniform cross-section and no axial load, the governing equation for a large-rotation Euler–Bernoulli beam is\n<blockquote style=\"border: 1px solid black; padding:10px; width:400px\">\n:<math>\n   EI~\\cfrac{\\mathrm{d}^4 w}{\\mathrm{d}x^4} - \\frac{3}{2}~EA~\\left(\\cfrac{\\mathrm{d} w}{\\mathrm{d}x}\\right)^2\\left(\\cfrac{\\mathrm{d}^2 w}{\\mathrm{d}x^2}\\right) = q(x)\n </math>\n</blockquote>\n\n== See also ==\n* [[Applied mechanics]]\n* [[Bending]]\n* [[Bending moment]]\n* [[Buckling]]\n* [[Flexural rigidity]]\n* [[Generalised beam theory]]\n* [[Plate theory]]\n* [[Sandwich theory]]\n* [[Shear and moment diagram]]\n* [[Singularity function]]\n* [[Strain (materials science)]]\n* [[Timoshenko beam theory]]\n* [[Theorem of three moments]] (Clapeyron's theorem)\n* [[Three point flexural test]]\n\n== Notes ==\n{{Reflist|group=\"N\"}}\n\n== References ==\n{{Reflist}}\n* {{cite conference | author=E. A. Witmer | title=Elementary Bernoulli-Euler Beam Theory | booktitle=MIT Unified Engineering Course Notes | year=1991–1992 | pages=5–114 to 5–164}}\n\n==External links==\n{{wikiquote}}\n* [https://mechanicalc.com/reference/beam-analysis Beam stress & deflection, beam deflection tables ]\n\n{{DEFAULTSORT:Euler-Bernoulli Beam Equation}}\n[[Category:Elasticity (physics)]]\n[[Category:Solid mechanics]]\n[[Category:Structural analysis]]\n[[Category:Mechanical engineering]]\n[[Category:Equations]]\n[[Category:Leonhard Euler]]"
    },
    {
      "title": "Euler's critical load",
      "url": "https://en.wikipedia.org/wiki/Euler%27s_critical_load",
      "text": "The '''critical load''' is the maximum [[Structural load|load]] (unit: Newton, it is a force) which a [[column]] can bear while staying straight. It is given by the [[formula]]:<ref>{{Cite web|url=https://mechanicalc.com/reference/column-buckling|title=Column Buckling|date=|website=|publisher=|access-date=}}</ref>\n:[[File:ColumnEffectiveLength.png|thumb|343x343px|''Fig. 1: Column effective length factors for Euler's critical load. In practical design, it is recommended to increase the factors as shown above.'']]<math>P_{cr}=\\frac{\\pi^2 EI}{(KL)^2}</math>\nwhere\n:<math>P_{cr}</math> = Euler's critical load (longitudinal compression load on column),\n:<math>E</math> = [[modulus of elasticity]] of column material,\n:<math>I</math> = minimum [[area moment of inertia]] of the cross section of the column,\n:<math>L</math> = unsupported [[length]] of column,\n:<math>K</math> = [[:File:ColumnEffectiveLength.png|column effective length factor]]\n\n<math>   </math>\n\nThis formula was derived in [[1757]], by the [[Switzerland|Swiss]] [[mathematician]] [[Leonhard Euler]]. The column will remain straight for loads less than the critical load. The \"critical load\" is the greatest load that will not cause lateral deflection (buckling). For loads greater than the critical load, the column will deflect laterally. The critical load puts the column in a state of [[Instability|unstable]] equilibrium. A load beyond the critical load causes the column to [[Failure|fail]] by [[buckling]]. As the load is increased beyond the critical load the lateral deflections increase, until it may fail in other modes such as yielding of the material. Loading of columns beyond the critical load are not addressed in this article.\n\nAround 1900, J. B. Johnson showed that at low slenderness ratios an [[Johnson's parabolic formula|alternative formula]] should be used.\n\n== Assumptions of the model ==\nThe following assumptions are made while deriving Euler’s formula:<ref>{{Cite web|url=http://engineering.myindialist.com/2015/twelve-viva-questions-on-columns-and-struts/#.VzmNfvl97cs|title=Questions on Columns and Struts|date=|website=|publisher=|access-date=}}</ref>\n# The [[material]] of the column is homogeneous and [[Isotropy|isotropic]].\n# The compressive load on the column is axial only.\n# The column is free from initial [[Stress (mechanics)|stress]].\n# The [[weight]] of the column is neglected.\n# The column is initially straight (no eccentricity of the axial load).\n# Pin joints are [[friction]]-less (no moment constraint) and fixed ends are rigid (no rotation deflection).\n# The [[Cross section (geometry)|cross-section]] of the column is uniform throughout its length.\n# The direct stress is very small as compared to the [[bending]] stress (the material is compressed only within the elastic range of strains).\n# The length of the column is very large as compared to the cross-sectional dimensions of the column. \n# The column fails only by buckling. This is true if the compressive stress in the column does not exceed the [[yield strength]] <math>\\sigma_y</math> (see figure 2):<br />    [[File:FIG2.png|thumb|294x294px|''Fig. 2: Critical stress vs slenderness ratio for steel, for E=200GPa, Yield strength=240MPa'']]<math>   </math><math>\\sigma=\\frac{P_{cr}}{A}=\\frac{\\pi^2E}{(L_e/r)^2}<\\sigma_y</math>\n\nFor slender columns, critical stress is usually lower than yield stress, and in the elastic range. In contrast, a stocky column would have a critical buckling stress higher than the yield, i.e. it yields in shortening prior the virtual elastic buckling onset.\n\n\nWhere:\n\n<math>L_e/r</math> - Slenderness ratio\n\n<math>L_e</math> - The effective length, <math>L_e=KL</math>\n\n<math>r</math> - Radius of gyration, <math>r=\\sqrt{{\\operatorname{I}\\!\\over\\operatorname{A}\\!}}</math>\n\n<math>I</math> - Moment of inertia\n\n<math>A</math> - Area cross section\n\n== Mathematical derivation: Pin ended column ==\n\nThe following model applies to columns simply supported at each end (<math>K=1</math>).\n\nFirstly, we will put attention to the fact there are no reactions in the hinged ends, so we also have no shear force in any cross-section of the column. The reason for no reactions can be obtained from [[symmetry]] (so the reactions should be in the same direction) and from moment equilibrium (so the reactions should be in opposite directions).\n\nUsing the [[free body diagram]] in the right side of figure 3, and making a summation of moments about point A:\n\n: <math>\\Sigma M= 0  \\Rightarrow  M(x)+Pw=0 </math>\n\nwhere w is the lateral deflection.\n\nAccording to [[Euler–Bernoulli beam theory]], the [[Deflection (engineering)|deflection]] of a beam is related with its [[bending moment]] by:\n \n: <math>M = -EI\\cfrac{\\mathrm{d}^2w}{\\mathrm{d}x^2} </math>,\n\nso:\n[[File:Pin ended column under the effect of Buckling load.png|thumb|475x475px|''Fig. 3: Pin ended column under the effect of Buckling load'']]\n: <math>EI\\frac{d^2w}{dx^2}+Pw=0 </math>\n\nLet <math>\\lambda^2=\\frac{P}{EI} </math>, so:\n\n: <math>\\frac{d^2w}{dx^2}+\\lambda ^2 w=0 </math>\n\nWe get a classical homogeneous second-order [[ordinary differential equation]].\n\nThe general solutions of this equation is: <math>w(x)= A \\cos(\\lambda x) + B \\sin(\\lambda x)</math>, where <math>A </math> and <math>B </math> are constants to be determined by [[boundary conditions]], which are:\n* Left end pinned <math>  \\rightarrow w(0)= 0  \\rightarrow A=0 </math>\n* Right end pinned <math>  \\rightarrow w(l)= 0  \\rightarrow B \\sin(\\lambda l)=0 </math>\n[[File:FIG4.png|thumb|355x355px|''Fig. 4: First three modes of buckling loads'']]\nIf <math>  B=0 </math>, no bending moment exists and we get the [[trivial solution]] of <math>  w(x)=0 </math>.\n\nHowever, from the other solution  <math>\\sin(\\lambda l)=0 </math>  we get <math>  \\lambda _nl=n\\pi </math>, for <math>  n=0,1,2... </math>\n\nTogether with <math>\\lambda^2=\\frac{P}{EI} </math> as defined before, the various critical loads are:\n\n: <math>P_{n}=\\frac{n^2 \\pi^2 EI}{l^2} </math>, for <math>  n=0,1,2... </math>\n\nand depending upon the value of <math>  n </math>, different buckling [[Longitudinal mode|mode]]<nowiki/>s are produced<ref>{{Cite web|url=http://web.aeromech.usyd.edu.au/AMME2301/Documents/Chapter09.pdf|title=Buckling of Columns|date=|website=|publisher=|access-date=}}</ref> as shown in figure 4. The load and mode for n=0 is the nonbuckled mode.\n\nTheoretically, any buckling mode is possible, but in the case of a slowly applied load only the first modal shape is likely to be produced.\n\n'''The critical load of Euler''' for a pin ended column is therefore:  \n: <math>P_{cr}=\\frac{\\pi^2 EI}{l^2}</math>\n\nand the obtained shape of the buckled column in the first mode is: \n: <math>w(x) = B \\sin \\Bigl({\\pi \\over l} x\\Bigr) </math>.\n\n== Mathematical derivation: General approach ==\n[[File:FIG 5.png|thumb|400x400px|''Fig. 5: forces and moments acting on a column'']]\nThe differential equation of the axis of a beam<ref>{{Cite book|title=Theory of Elastic Stability, 2 ed., McGraw-Hill|author1=Timoshenko, S. P. |author2=Gere, J. M. |lastauthoramp=yes |publisher=|year=1961|isbn=|location=|pages=}}</ref> is:\n\n: <math>\\frac{d^4 w }{dx^4}+\\frac{P }{EI}\\frac{d^2 w }{dx^2}=\\frac{q }{EI}</math>\n\nFor a column with axial load only, the lateral load <math>q(x)</math> vanishes and substituting <math>\\lambda^2=\\frac{P}{EI} </math> , we get:\n\n: <math>\\frac{d^4 w }{dx^4}+\\lambda^2\\frac{d^2 w }{dx^2}=0</math>\n\nThis is a homogeneous fourth-order differential equation and its general solution is <math>w(x)= A\\sin(\\lambda x) + B\\cos(\\lambda x)+Cx+D </math>\n\nThe four constants <math>A,B,C,D </math> are determined by the boundary conditions (end constraints) on <math>w(x) </math>, at each end. There are three cases:\n# Pinned end:  <math> w= 0  </math>  and  <math> M= 0  \\rightarrow {d^2w \\over dx^2}=0 </math>\n# Fixed end:  <math> w= 0  </math>  and  <math> {dw \\over dx}=0 </math>\n# Free end:    <math> M= 0  \\rightarrow {d^2w \\over dx^2}=0 </math>     and        <math> V= 0  \\rightarrow {d^3w \\over dx^3}+\\lambda^2{dw \\over dx}=0 </math>\nUsing each time a different combination of these BCs, [[Eigen value problem|eigenvalue problem]]<nowiki/>s are obtained. Solving those, we get the values of Euler's critical load for each one of the cases presented in Figure 1.\n\n== See also ==\n* [[Buckling]]\n* [[Bending moment]]\n* [[Bending]]\n* [[Euler–Bernoulli beam theory]]\n\n== References ==\n{{Reflist}}\n\n[[Category:Elasticity (physics)]]\n[[Category:Materials science]]\n[[Category:Mechanical failure modes]]\n[[Category:Structural analysis]]\n[[Category:Mechanics]]\n[[Category:Leonhard Euler]]"
    },
    {
      "title": "Finite element limit analysis",
      "url": "https://en.wikipedia.org/wiki/Finite_element_limit_analysis",
      "text": "A '''finite element limit analysis''' (FELA) uses [[Optimization (mathematics)|optimisation techniques]] to directly compute the upper or lower bound plastic collapse load (or [[Limit loads|limit load]]) for a mechanical system rather than time stepping to a collapse load, as might be undertaken with conventional non-linear [[finite element]] techniques. The problem may be formulated in either a kinematic or equilibrium form.<ref>Lysmer, J. (1970). Limit analysis of plane problems in\nsoil mechanics. Journal of the Soil Mechanics and Foundations Division ASCE 96(4), 1311–1334.</ref><ref>Sloan, S. (1988). Lower bound limit analysis using finite\nelements and linear programming. Int. J. Num. Anal. Meth. in Geomech. 12(4), 61–77.</ref>\n\nThe technique has been used most significantly in the field of [[soil mechanics]] for the determination of collapse loads for geotechnical problems (e.g. [[slope stability analysis]]). An alternative technique which may be used to undertake similar direct plastic collapse computations using optimization is [[Discontinuity layout optimization]].\n\n==Software for finite element limit analysis==\n* [http://www.optumce.com OptumG2] (2014-) General purpose software for 2D geotechnical applications.\n* [http://www.optumce.com OptumG3] (2017-) General purpose software for 3D geotechnical applications.\n\n==See also==\n* [[Limit analysis]]\n\n==References==\n{{Reflist}}\n\n==Further reading==\n* Kumar, Jyant, and Debasis Mohapatra. \"Lower-bound finite elements limit analysis for Hoek-Brown materials using semidefinite programming.\" Journal of Engineering Mechanics 143.9 (2017): 04017077.{{doi|10.1061/(ASCE)EM.1943-7889.0001296}}\n* Makrodimopoulos, A., and C. M. Martin. \"Lower bound limit analysis of cohesive‐frictional materials using second‐order cone programming.\" International Journal for Numerical Methods in Engineering 66.4 (2006): 604-634.{{doi|10.1002/nme.1567}}\n* Kumar, Jyant, and Vishwas N. Khatri. \"[https://onlinelibrary.wiley.com/doi/abs/10.1002/nag.900 Bearing capacity factors of circular foundations for a general c–ϕ soil using lower bound finite elements limit analysis].\" International Journal for Numerical and Analytical Methods in Geomechanics 35.3 (2011): 393-405.\n* Tang, Chong, Kim-Chuan Toh, and Kok-Kwang Phoon. \"[https://www.researchgate.net/profile/Kim-Chuan_Toh/publication/278072706_Axisymmetric_Lower-Bound_Limit_Analysis_Using_Finite_Elements_and_Second-Order_Cone_Programming/links/55c45de908aea2d9bdc1d2f7.pdf Axisymmetric lower-bound limit analysis using finite elements and second-order cone programming].\" Journal of Engineering Mechanics 140.2 (2013): 268-278.\n* Kumar, Jyant, and Obaidur Rahaman. \"[https://tspace.library.utoronto.ca/bitstream/1807/92819/1/cgj-2017-0515.pdf Vertical uplift resistance of horizontal plate anchors for eccentric and inclined loads].\" Canadian Geotechnical Journal(2018).\n* Mohapatra D, Kumar J. Collapse loads for rectangular foundations by three‐dimensional upper bound limit analysis using radial point interpolation method. Int J Numer Anal Methods Geomech. 2018;1–20. https://doi.org/10.1002/nag.2885\n{{DEFAULTSORT:Finite element limit analysis}}\n[[Category:Structural analysis]]\n[[Category:Geotechnical engineering]]"
    },
    {
      "title": "Fixed end moment",
      "url": "https://en.wikipedia.org/wiki/Fixed_end_moment",
      "text": "The '''fixed end moments''' are reaction [[moment (physics)|moment]]s developed in a [[beam (structure)|beam]] member under certain load conditions with both ends fixed. A beam with both ends fixed is statically indeterminate to the 3rd degree, and any structural analysis method applicable on [[statically indeterminate]] beams can be used to calculate the fixed end..\n\n== Examples ==\nIn the following examples, [[clockwise]] moments are positive. The vertical reactions are not shown since they can be easily determined from [[statics]].\n{| class=\"wikitable\" style=\"background-color:rose;text-align:center;\"\n|-\n|[[Image:Fem2.png|center|400px]]<br>Concentrated load of magnitude P||[[Image:Fem3.png|center|400px]]<br>Linearly distributed load of maximum intensity q<sub>0</sub>\n|-\n|[[Image:Fem1.png|center|400px]]<br>Uniformly distributed load of intensity q||[[Image:Fem4.png|center|400px]]<br>Couple of magnitude M<sub>0</sub>\n|}\n\nThe two cases with distributed loads can be derived from the case with concentrated load by integration. For example, when a uniformly distributed load of intensity <math>q</math> is acting on a beam, then an infinitely small part <math>dx</math> distance <math>x</math> apart from the left end of this beam can be seen as being under a concentrated load of magnitude <math>qdx</math>. Then,\n:<math>M_{\\mathrm{right}}^{\\mathrm{fixed}} = \\int_{0}^{L} \\frac{q dx \\, x^2 (L-x)}{L^2} = \\frac{q L^2}{12} </math>\n:<math>M_{\\mathrm{left}}^{\\mathrm{fixed}} = \\int_{0}^{L} \\left \\{ - \\frac{q dx \\, x (L-x)^2 }{L^2} \\right \\}= - \\frac{q L^2}{12} </math>\nWhere the expressions within the integrals on the right hand sides are the fixed end moments caused by the concentrated load <math>qdx</math>.\n\nFor the case with linearly distributed load of maximum intensity <math>q_0</math>,\n:<math>M_{\\mathrm{right}}^{\\mathrm{fixed}} = \\int_{0}^{L} q_0 \\frac{x}{L} dx \\frac{ x^2 (L-x)}{L^2} = \\frac{q_0 L^2}{20}</math>\n:<math>M_{\\mathrm{left}}^{\\mathrm{fixed}} = \\int_{0}^{L} \\left \\{ - q_0 \\frac{x}{L} dx \\frac{x (L-x)^2}{L^2} \\right \\} = - \\frac{q_0 L^2}{30}</math>\n\n== See also ==\n* [[Moment distribution method]]\n* [[Statically indeterminate|Statically Indeterminate]]\n* [[Slope deflection method]]\n* [[Matrix method]]\n\n== References ==\n*{{cite book|last=Yang|first=Chang-hyeon|title=Structural Analysis|url=http://www.cmgbook.co.kr/category/sub_detail.html?no=1017|edition=4th|date=2001-01-10|publisher=Cheong Moon Gak Publishers|language=Korean|location=Seoul|isbn=89-7088-709-1}}\n\n[[Category:Structural analysis]]\n\n\n{{Civil-engineering-stub}}"
    },
    {
      "title": "Geometrically and materially nonlinear analysis with imperfections included",
      "url": "https://en.wikipedia.org/wiki/Geometrically_and_materially_nonlinear_analysis_with_imperfections_included",
      "text": "{{multiple issues|\n{{Underlinked|date=July 2014}}\n{{Orphan|date=July 2014}}\n}}\n\n'''Geometrically and materially nonlinear analysis with imperfections included (GMNIA)''', is a [[structural analysis]] method designed to verify the strength capacity of a structure, which accounts for both plasticity and [[buckling]] failure modes.<ref>{{cite journal \n|last1=Marques \n|first1=Liliana \n|first2=Luís\n|last2=Simões da Silva\n|first3=Richard\n|last3=Greiner\n|first4=Carlos\n|last4=Rebelo\n|first5=Andreas\n|last5=Taras\n|date=2013\n|title=Development of a consistent design procedure for lateral–torsional buckling of tapered beams\n|journal=Journal of Constructional Steel Research\n|volume=89\n|pages=213–235\n|doi=10.1016/j.jcsr.2013.07.009\n}}</ref> GMNIA is currently considered the most sophisticated and perspectively the most accurate method of a numerical buckling strength verification.<ref>{{cite journal|url=http://www.springerlink.com/content/xk63834841775339/|title=Stimulating Equivalent Geometric Imperfections for the Numerical Buckling Strength Verification of Axially Compressed Cylindrical Steel Shells | doi=10.1007/s00466-005-0728-8 | volume=37|journal=Computational Mechanics|pages=530–536}}</ref><ref>{{cite web\n|url=http://www.shf.tugraz.at/pdf/publication/beam/or_simul_prag03.pdf \n|title=Buckling check of members and frames based on numerical simulations \n|author=R. Ofner \n|deadurl=yes\n|archiveurl=https://web.archive.org/web/20070922184645/http://www.shf.tugraz.at/pdf/publication/beam/or_simul_prag03.pdf \n|archivedate=2007-09-22 \n|df= \n}}</ref>\n\n==References==\n{{Reflist}}\n\n[[Category:Structural analysis]]\n\n\n{{applied-math-stub}}\n{{Engineering-stub}}"
    },
    {
      "title": "Ground–structure interaction",
      "url": "https://en.wikipedia.org/wiki/Ground%E2%80%93structure_interaction",
      "text": "\n'''Ground–structure interaction''' consists of the interaction between [[soil]] (ground) and a structure built upon it. It is primarily an exchange of mutual [[Stress (mechanics)|stress]], whereby the movement of the ground-structure system is influenced by both the type of ground and the type of structure. This is especially applicable to areas of [[seismic]] activity. Various combinations of soil and structure can either amplify or diminish movement and subsequent damage. A building on stiff ground rather than deformable ground will tend to suffer greater damage. A second interaction effect, tied to mechanical properties of soil, is the sinking of foundations, worsened by a seismic event. This phenomenon is called [[soil liquefaction]].\n\n\n==Design==\n\nThe main types of foundations, based upon several building characteristics, are:\n* Isolated [[plinths]] (currently not feasible)\n* Plinths connected by foundations beams\n* Reverse beams\n* A [[Plate (metal)|plate]] (used for low-quality grounds)\nThe filing of foundations grounds takes place according to the mechanical properties of the grounds themselves: in [[Italy]], for instance, according to the new [[earthquake]]-proof norm – Ordinanza 3274/2003 – you can identify the following categories:\n* Category A: [[homogeneous]] [[Rock (geology)|rock]] formations\n* Category B: compact [[granular]] or clayey soil\n* Category C: quite compact granular or clayey soil\n* Category D: not much compact granular or clayey soil\n* Category E: [[alluvial]] [[surface of the Earth|surface]] layer grounds (very low quality soil)\nThe type of [[Foundation (architecture)|foundations]] is selected according to the type of ground; for instance, in the case of homogeneous rock formations connected plinths are selected, while in the case of very low quality grounds plates are chosen.\n\n[[Image:groundstruct.jpg]]\n\nFor further information about the various ways of building foundations see [[foundation (architecture)]].\n\nBoth grounds and structures can be more or less deformable; their combination can or cannot cause the [[Amplifier|amplification]] of the [[seismic]] effects on the structure.\nGround, in fact, is a filter with respect to all the main [[seismic waves]], as stiffer soil fosters high-frequency seismic waves while less compact soil accommodates lower frequency waves. Therefore, a stiff building, characterized by a high fundamental [[frequency]], suffers amplified damage when built on stiff ground and then subjected to higher frequencies.\n\nFor instance, suppose there are two buildings that share the same high [[stiffness]]. They stand on two different soil types: the first, stiff and rocky&mdash;the second, sandy and deformable. If subjected to the same seismic event, the building on the stiff ground suffers greater damage.\n\nThe second interaction effect, tied to mechanical properties of soil, is about the lowering (sinking) of foundations, worsened by the seismic event itself, especially about less compact grounds. This phenomenon is called [[soil liquefaction]].\n\n==Mitigation==\nThe methods most used to mitigate the problem of the ground-structure interaction consist of the employment of the before-seen isolation systems and of some ground brace techniques, which are adopted above all on the low-quality ones (categories D and E).\nThe most diffused techniques are the [[jet grouting]] technique and the [[pile work]] technique.\nThe jet-grouting technique consists of injecting in the [[subsoil]] some liquid concrete by means of a [[drill]]. When this concrete hardens it forms a sort of column that consolidates the surrounding soil. This process is repeated on all areas of the structure.\nThe pile work technique consists of using piles, which, once inserted in the ground, support the foundation and the building above, by moving the loads or the weights towards soil layers that are deeper and therefore more compact and movement-resistant.\n\n[[Image:mitigationtech.jpg]]\n\n== External links ==\n* [http://www.ingegneriasismica.net/Tematiche/4ZS/4ZSfondazioniM/4ZSfondazioniM_interazione/4ZSfondazioniM_interazione.htm Do you like to better understand what happens when seismic waves get through the ground-structure system?] {{Link language|it}}<!--Italian-->\n* [https://web.archive.org/web/20080530000048/http://sokocalo.engr.ucdavis.edu/~jeremic/ECI281a/Presentations/2001/Louie_pres.ppt Seismic soil-structure interaction]\n\n{{Geotechnical engineering}}\n\n{{DEFAULTSORT:Ground-Structure Interaction}}\n[[Category:Excavation shoring]]\n[[Category:Foundations (buildings and structures)]]\n[[Category:Geotechnical engineering]]\n[[Category:Structural analysis]]"
    },
    {
      "title": "Hogging and sagging",
      "url": "https://en.wikipedia.org/wiki/Hogging_and_sagging",
      "text": "[[Image:ShipSaggingHogging.png|thumb|right|300px|Diagram of ship hull (1) Sagging and (2) Hogging under loads. Bending is exaggerated for illustrative purposes.]]\n\n'''Hogging''' and '''sagging''' describe the shape of a beam or similar long object when loading is applied. Hogging describes a [[beam (structure)|beam]] which curves upwards in the middle, and sagging describes a beam which curves downwards.\n\n==Ships==\nHogging is the stress a ship's [[Hull (watercraft)|hull]] or [[keel]] experiences that causes the center or the keel to bend upward. Sagging is the stress a ship's hull or keel is placed under when a wave is the same length as the ship and the ship is in the trough of two waves. This causes the middle of the ship to bend down slightly, and depending of the level of bend, may cause the hull to snap or crack. \n\nSagging or dynamic hogging may have been what sank the ''[[Prestige oil spill|Prestige]]'' off Spain on 19 November 2002.\n\nThe 2013 loss of container ship ''[[MOL Comfort]]'' off the coast of [[Yemen]] was attributed to hogging. Subsequent lawsuits blamed the shipbuilder for design flaws.\n\nHogging, or \"hog\", also refers to the semi-permanent bend in the keel, especially in wooden-hulled ships, caused over time by the ship's center's being more buoyant than the bow or stern.  At the beginning of her 1992 refit, {{USS|Constitution}} had over 13&nbsp;inches (33&nbsp;cm) of hog.<ref>{{cite web|last=Otton|first=Patrick|date=1997-08-11|url=http://www.maritime.org/conf/conf-otton-const.htm|title=USS Constitution Rehabilitation And Restoration|accessdate=2006-07-02}}</ref> The keel blocks in the [[drydock]] were set up especially to support this curve.  During her three years in drydock, the center keel blocks were gradually shortened, allowing the hog to settle out.  Additionally, the diagonal riders specified in her original design to resist hogging, which had been removed in an earlier refit, were restored.  The similar-sized {{USS|Constellation|1854|6}} had 36&nbsp;inches (91&nbsp;cm) of hog before refitting, when she was condemned as unsafe in 1994.<ref>{{cite web|url=http://www.history.navy.mil/danfs/c13/constellation-ii.htm |title=Constellation|work=Dictionary of American Naval Fighting Ships|accessdate=2006-07-02}}</ref>\n\nDuring loading and discharging cargo, ships bend (hog or sag) due to the distribution of the weights in the various holds and tanks on board.\n\n== Effects on cargo loading ==\n\nSince the maximum amount of cargo that a vessel can load often depends on whether her [[Plimsoll line|Plimsoll mark]] is submerged or not, sagging can reduce her effective cargo capacity—especially if her [[Load-Line|loadline]] has already been reached prematurely due to the sag.<ref>{{cite book|title=Carefully to Carry - Measurement of bulk cargoes|year=May 2008|publisher=UK P & I Club|page=8|url=http://www.ukpandi.com/fileadmin/uploads/uk-pi/LP%20Documents/Carefully_to_Carry/Measurement%20of%20bulk%20cargoes%20-%20draught%20surveys.pdf}}</ref> \n\nThis is taken into account when calculating [[cargo]], by applying what is called a \"3/4 mean draft\". This is also called the \"two-thirds mean correction\", directly derived from [[Simpson's rules (ship stability)|Simpson's first rule]].\n\n==See also==\n* [[Glossary of nautical terms]]\n* [[Strength of ships]]\n* [[Hog chains]]\n\n== References ==\n<references/>\n*{{DANFS}}\n\n== External links ==\n<!-- do not link directly to hazegray photos, they apparently become locked out to wiki referrer -->\n*[http://www.hazegray.org/features/constellation/ ''Constellation'''s 1994 restoration article] has several photos showing the hogging.\n*[http://www.worldwideflood.com/ark/basic_hull_design/monocoque_vs_truss.htm Diagram showing hogging stress and the diagonal risers].\n\n{{DEFAULTSORT:Hogging And Sagging}}\n[[Category:Structural analysis]]\n[[Category:Nautical terminology]]"
    },
    {
      "title": "Huber's equation",
      "url": "https://en.wikipedia.org/wiki/Huber%27s_equation",
      "text": "'''Huber's equation''', first derived by a Polish engineer [[Tytus Maksymilian Huber]], is a basic formula in elastic material [[tension (physics)|tension]] calculations, an equivalent of the [[equation of state]], but applying to solids. In most simple expression and commonly in use it looks like this:<ref>Huber, M. T. \"Właściwa praca odkształcenia jako miara wytężenia materiału. Czasopismo Techniczne, 15, Lwów, 1904; see also: Specific work of strain asameasure of material effort.\" Archives of Mechanics 56.3 (2004): 173-190.</ref><ref>Huber, M. T. \"Specific work of strain as a measure of material effort.\" Archives of Mechanics 56.3 (2004): 173-190.</ref>\n\n<math>\n\\sigma_{red}=\\sqrt{({\\sigma}^2) + 3({\\tau}^2)}\n</math>\n\nwhere <math>\\sigma</math> is the [[tensile stress]], and <math>\\tau</math> is the [[shear stress]], measured in newtons per square meter (N/m², also called [[pascal (unit)|pascal]]s, Pa), while <math>\\sigma_{red}</math> - called a reduced tension, is the resultant tension of the material.\n\nVery useful in calculating the span width of the bridges like [[Golden Gate Bridge]] or [[Verrazano-Narrows Bridge]], their beam cross-sections, etc.\n\n==See also==\n* [[Yield surface]]\n* [[Stress–energy tensor]]\n* [[Tensile stress]]\n* [[von Mises yield criterion]]\n\n==References==\n{{Reflist}}\n\n[[Category:Physical quantities]]\n[[Category:Structural analysis]]\n\n\n{{classicalmechanics-stub}}"
    },
    {
      "title": "Incremental dynamic analysis",
      "url": "https://en.wikipedia.org/wiki/Incremental_dynamic_analysis",
      "text": "{{Use dmy dates|date=October 2017}}\n'''Incremental dynamic analysis''' ('''IDA''') is a computational analysis method of [[earthquake engineering]] for performing a comprehensive assessment of the behavior of structures under seismic loads.<ref name=\"vamva-cornell-EESD2002\"/> It has been developed to build upon the results of [[seismic hazard|probabilistic seismic hazard analysis]] in order to estimate the [[seismic risk]] faced by a given structure. It can be considered to be the dynamic equivalent of the [[Seismic_analysis#Nonlinear_Static_Analysis|static pushover analysis]].\n\n==Description==\n\nIDA involves performing multiple [[Seismic_analysis#Nonlinear_Dynamic_Analysis|nonlinear dynamic analyses]] of a structural model under a suite of [[Strong ground motion|ground motion records]], each scaled to several levels of seismic intensity. The scaling levels are appropriately selected to force the structure through the entire range of behavior, from elastic to inelastic and finally to global dynamic instability, where the structure essentially experiences collapse. Appropriate postprocessing can present the results in terms of IDA curves, one for each ground motion record, of the seismic intensity, typically represented by a scalar Intensity Measure (IM), versus the structural response, as measured by an engineering demand parameter (EDP).  \n\nPossible choices for the IM are scalar (or rarely vector) quantities that relate to the severity of the recorded ground motion and scale linearly or nonlinearly with its amplitude. The IM is properly chosen well so that appropriate hazard maps (hazard curves) can be produced for them by probabilistic seismic hazard analysis. In addition, the IM should be correlated with the structural response of interest to decrease the number of required response history analyses<ref name=\":0\">{{Cite journal|last=Kiani|first=Jalal|last2=Pezeshk|first2=Shahram|title=Sensitivity analysis of the seismic demands of RC moment resisting frames to different aspects of ground motions|journal=Earthquake Engineering & Structural Dynamics|volume=46|issue=15|language=en|pages=2739–2755|doi=10.1002/eqe.2928|issn=1096-9845|year=2017}}</ref>. Possible choices are the [[peak ground acceleration]], [[peak ground velocity]] or [[Arias intensity]], but the most widely used is the 5%-damped [[spectral acceleration]] at the first-mode period of the structure. The results of the recent studies show that spectrum intensity (SI) is an appropriate IM<ref name=\":0\" />.\n\nThe EDP can be any structural response quantity that relates to structural, non-structural or contents' damage. Typical choices are the maximum (over all stories and time) interstory drift, the individual peak story drifts and the peak floor accelerations.\n\n==Development history==\nIDA grew out of the typical practice of scaling accelerograms by multiplying with a constant factor to represent more or less severe ground motions than the ones that were recorded at a site. Since the natural recordings available are never enough to cover all possible needs, scaling is a simple, yet potentially problematic method (if misused) to \"fill-in\" gaps in the current catalog of events. Still, in most cases, researchers would scale only a small set of three to seven records and typically only once, just to get an estimate of response in the area of interest.\n\nIn the wake of the damage wrought by the [[Northridge Earthquake|1994 Northridge earthquake]], the [[Federal Emergency Management Agency|SAC/FEMA]] project<ref name=\"FEMA350-351\"/> was launched to resolve the issue of poor performance of steel moment-resisting frames due to the fracturing beam-column connections. Within the creative environment of research cooperation, the idea of subjecting a structure to a wider range of scaling emerged. Initially, the method was called Dynamic Pushover<ref name=\"luco-cornell-6NCEE1998\"/> and it was conceived as a way to estimate a proxy for the global collapse of the structure. It was later recognized that such a method would also enable checking for multiple limit-states, e.g. for life-safety, as is the standard for most seismic design methods, but also for lower and higher levels of intensity that represent different threat levels, such as immediate-occupancy and collapse-prevention. Thus, the idea for Incremental Dynamic Analysis<ref name=\"vamva-cornell-EESD2002\"/> was born, which was mainly adopted and later popularized by researchers at the John A. Blume Earthquake Research Center of [[Stanford University]]. This has now met with wider recognition in the earthquake research community and has spawned several different methods and concepts for estimating structural performance.\n\nA substantial debate has been raised regarding the potential bias in the IDA results due to using the scaled ground motions records that do not appropriately characterize the seismic hazard of the considered site over different earthquake intensity levels.<ref>{{Cite journal|last=Baker|first=Jack W.|last2=Allin Cornell|first2=C.|date=25 July 2006|title=Spectral shape, epsilon and record selection|journal=Earthquake Engineering & Structural Dynamics|language=en|volume=35|issue=9|pages=1077–1095|doi=10.1002/eqe.571|issn=1096-9845|citeseerx=10.1.1.585.7815}}</ref><ref name=\":1\">{{Cite journal|last=Bradley|first=Brendon A.|date=1 September 2013|title=A critical examination of seismic response uncertainty analysis in earthquake engineering|journal=Earthquake Engineering & Structural Dynamics|language=en|volume=42|issue=11|pages=1717–1729|doi=10.1002/eqe.2331|issn=1096-9845|hdl=10092/8700}}</ref><ref name=\":2\">{{Cite journal|last=Kiani|first=Jalal|last2=Khanmohammadi|first2=Mohammad|date=19 May 2015|title=New Approach for Selection of Real Input Ground Motion Records for Incremental Dynamic Analysis (IDA)|journal=Journal of Earthquake Engineering|volume=19|issue=4|pages=592–623|doi=10.1080/13632469.2014.997901|issn=1363-2469}}</ref><ref name=\":3\">Lin, T. and Baker, J. W. [2013] \"Introducing Adaptive Incremental Dynamic Analysis: A new tool for linking ground motion selection and structural response assessment,” 11th International Conference on Structural Safety & Reliability, New York.\n</ref>\n\n==See also==\n* [[C. Allin Cornell]]\n\n==References==\n{{Reflist|refs=\n<ref name=\"vamva-cornell-EESD2002\">Vamvatsikos D., Cornell C.A. (2002). Incremental Dynamic Analysis. Earthquake Engineering and Structural Dynamics, 31(3): 491–514.</ref>\n<ref name=\"FEMA350-351\">SAC/FEMA (2000). Recommended seismic design criteria for new steel moment-frame buildings, Report No. FEMA-350, and Recommended seismic evaluation and upgrade criteria for existing welded steel moment-frame buildings, Report No. FEMA-351, SAC Joint Venture, Federal Emergency Management Agency, Washington, DC.</ref>\n<ref name=\"luco-cornell-6NCEE1998\">Luco N., Cornell CA. (1998) Effects of random connection fractures on demands and reliability for a 3-storey pre-Northridge SMRF structure. Proceedings of the 6th U.S. National Conference on Earthquake Engineering, paper 244, Seattle, WA.</ref>\n}}\n\n==External links==\n*[http://www.fema.gov/library/viewRecord.do?id=1538] SAC/FEMA 350 Report\n*[http://www.fema.gov/library/viewRecord.do?id=1751] SAC/FEMA 351 Report\n*[http://users.ntua.gr/divamva/publications.html] IDA-related publications from D. Vamvatsikos at the [[National Technical University of Athens]]\n\n[[Category:Earthquake engineering]]\n[[Category:Earthquake and seismic risk mitigation]]\n[[Category:Structural analysis]]"
    },
    {
      "title": "Influence line",
      "url": "https://en.wikipedia.org/wiki/Influence_line",
      "text": "[[File:Muller-Breslau Principle - Influence Lines.JPG|thumb|upright=1.8|alt=A simply supported beam and four different influence lines.|Figure 1:  (a) This simple supported beam is shown with a unit load placed a distance ''x'' from the left end. Its influence lines for four different functions: (b) the reaction at the left support (denoted A), (c) the reaction at the right support (denoted C), (d) one for shear at a point B along the beam, and (e) one for moment also at point B.]]\n\nIn engineering, an '''influence line''' graphs the variation of a function (such as the shear felt in a structure member) at a specific point on a [[Beam (structure)|beam]] or [[truss]] caused by a unit load placed at any point along the structure.<ref name=\"onlinefreeebooks\">Kharagpur. [http://www.onlinefreeebooks.net/engineering-ebooks/civil-architectural-engineering/structural-analysis-pdf.html \"Structural Analysis.pdf, Version 2 CE IIT\"] {{webarchive|url=https://web.archive.org/web/20100819140741/http://www.onlinefreeebooks.net/engineering-ebooks/civil-architectural-engineering/structural-analysis-pdf.html |date=2010-08-19 }}. 7 August 2008. Accessed on 26 November 2010.</ref><ref name=\"iastate\">Dr. Fanous, Fouad. [http://www.public.iastate.edu/~fanous/ce332/influence/homepage.html \"Introductory Problems in Structural Analysis: Influence Lines\"]. 20 April 2000. Accessed on 26 November 2010.</ref><ref name=\"theconstructor\">[http://theconstructor.org/structural-engg/analysis/influence-line-method-of-analysis/4361/ \"Influence Line Method of Analysis\"]. The Constructor. 10 February 2010. Accessed on 26 November 2010.</ref><ref name=\"foundationcoalition\">[http://www.foundationcoalition.org/resources/ce/structanalysis/influencelines.html \"Structural Analysis: Influence Lines\"]. The Foundation Coalition. 2 December 2010. Accessed on 26 November 2010.</ref><ref name=\"Hibbeler\">Hibbeler, R.C. (2009). Structural Analysis (Seventh Edition). Pearson Prentice Hall, New Jersey. {{ISBN|0-13-602060-7}}.</ref> Some of the common functions studied with influence lines include reactions (the forces that the structure’s supports must apply in order for the structure to remain static), [[shear force|shear]], [[bending moment|moment]], and [[deflection (engineering)|deflection]] (Deformation) <ref>{{Cite journal|last=Zeinali|first=Yasha|date=December 2017|title=Framework for Flexural Rigidity Estimation in Euler-Bernoulli Beams Using Deformation Influence Lines|url=http://www.mdpi.com/2412-3811/2/4/23/pdf|journal=Infrastructures|volume=2|pages=23|via=MDPI}}</ref>. Influence lines are important in designing beams and trusses used in [[Bridge|bridges]], crane rails, [[conveyor belt]]s, floor girders, and other structures where loads will move along their span.<ref name=\"Hibbeler\"/> The influence lines show where a load will create the maximum effect for any of the functions studied.\n\nInfluence lines are both [[Scalar (mathematics)|scalar]] and [[additive map|additive]].<ref name=\"Hibbeler\"/> This means that they can be used even when the load that will be applied is not a unit load or if there are multiple loads applied. To find the effect of any non-unit load on a structure, the ordinate results obtained by the influence line are multiplied by the magnitude of the actual load to be applied. The entire influence line can be scaled, or just the maximum and minimum effects experienced along the line. The scaled maximum and minimum are the critical magnitudes that must be designed for in the beam or truss.\n\nIn cases where multiple loads may be in effect, the influence lines for the individual loads may be added together in order to obtain the total effect felt by the structure at a given point. When adding the influence lines together, it is necessary to include the appropriate offsets due to the spacing of loads across the structure. For example,  a truck load is applied to the structure. Rear axle, B, is three feet behind front axle, A, then the effect of A at ''x'' feet along the structure must be added to the effect of B at (''x'' – 3) feet along the structure—not the effect of B at ''x'' feet along the structure.\n\nMany loads are distributed rather than concentrated. Influence lines can be used with either concentrated or distributed loadings. For a concentrated (or point) load, a unit point load is moved along the structure. For a distributed load of a given width, a unit-distributed load of the same width is moved along the structure, noting that as the load nears the ends and moves off the structure only part of the total load is carried by the structure. The effect of the distributed unit load can also be obtained by integrating the point load’s influence line over the corresponding length of the structures.\n\n==Demonstration from Betti's theorem==\nInfluence lines are based on [[Betti's theorem]].  From there, consider two external force systems, <math>F^P_i</math> and <math>F^Q_i</math>, each one associated with a displacement field whose displacements measured in the force's point of application are represented by <math>d^P_i</math> and <math>d^Q_i</math>.\n\nConsider that the <math>F^P_i</math> system represents actual forces applied to the structure, which are in equilibrium.  Consider that the <math>F^Q_i</math> system is formed by a single force, <math>F^Q</math>.  The displacement field <math>d^Q_i</math> associated with this forced is defined by releasing the structural restraints acting on the point where <math>F^Q</math> is applied and imposing a relative unit displacement which is kinematically admissible in the negative direction, represented as <math>d^Q_1 = -1</math>.  From [[Betti's theorem]], we obtain the following result:\n\n<math>\n-F^P_1 + \\sum^n_{i=2}F^P_id^Q_i = F^Q\\times 0 \\iff F^P_1 = \\sum^n_{i=2}F^P_id^Q_i\n</math>\n\n==Concept==\nWhen designing a beam or truss, it is necessary to design for the scenarios causing the maximum expected reactions, shears, and moments within the structure members in order to ensure that no member will fail during the life of the structure. When dealing with [[dead and live loads|dead loads]] (loads that never move, such as the weight of the structure itself), this is relatively easy because the loads are easy to predict and plan for. For [[dead and live loads|live loads]] (any load that will be moved during the life of the structure, such as furniture and people), it becomes much harder to predict where the loads will be or how concentrated or distributed they will be throughout the life of the structure.\n\nInfluence lines graph the response of a beam or truss as a unit load travels across it. The influence line allows the designers to discover quickly where to place a live load in order to calculate the maximum resulting response for each of the following functions:  reaction, shear, or moment. The designer can then scale the influence line by the greatest expected load to calculate the maximum response of each function for which the beam or truss must be designed.\nInfluence lines can also be used to find the responses of other functions (such as deflection or axial force) to the applied unit load, but these uses of influence lines are less common.\n\n==Methods for constructing influence lines==\nThere are three methods used for constructing the influence line. The first is to tabulate the influence values for multiple points along the structure, then use those points to create the influence line.<ref name=\"Hibbeler\"/> The second is to determine the influence-line equations that apply to the structure, thereby solving for all points along the influence line in terms of ''x'', where ''x'' is the number of feet from the start of the structure to the point where the unit load is applied.<ref name=\"onlinefreeebooks\"/><ref name=\"iastate\"/><ref name=\"theconstructor\"/><ref name=\"foundationcoalition\"/><ref name=\"Hibbeler\"/>  The third method is called the [[Müller-Breslau's principle]]. It creates a [[qualitative data|qualitative]] influence line.<ref name=\"onlinefreeebooks\"/><ref name=\"iastate\"/><ref name=\"Hibbeler\"/> This influence line will still provide the designer with an accurate idea of where the unit load will produce the largest response of a function at the point being studied, but it cannot be used directly to calculate what the magnitude that response will be, whereas the influence lines produced by the first two methods can.\n\n===Tabulate values===\nIn order to tabulate the influence values with respect to some point A on the structure, a unit load must be placed at various points along the structure. [[Statics]] is used to calculate what the value of the function (reaction, shear, or moment) is at point A. Typically an upwards reaction is seen as positive. Shear and moments are given positive or negative values according to the same conventions used for [[shear and moment diagram]]s.\n\nR. C. Hibbeler states, in his book ''Structural Analysis'', “All statically determinate beams will have influence lines that consist of straight line segments.”<ref name=\"Hibbeler\"/> Therefore, it is possible to minimize the number of computations by recognizing the points that will cause a change in the slope of the influence line and only calculating the values at those points. The slope of the inflection line can change at supports, mid-spans, and joints.\n\nAn influence line for a given function, such as a reaction, axial force, shear force, or bending moment, is a graph that shows the variation of that function at any given point on a structure due to the application of a unit load at any point on the structure.\n\nAn influence line for a function differs from a shear, axial, or bending moment diagram. Influence lines can be generated by independently applying a unit load at several points on a structure and determining the value of the function due to this load, i.e. shear, axial, and moment at the desired location. The calculated values for each function are then plotted where the load was applied and then connected together to generate the influence line for the function.\n\nOnce the influence values have been tabulated, the influence line for the function at point A can be drawn in terms of ''x''. First, the tabulated values must be located. For the sections in between the tabulated points, interpolation is required. Therefore, straight lines may be drawn to connect the points. Once this is done, the influence line is complete.\n\n===Influence-line equations===\nIt is possible to create equations defining the influence line across the entire span of a structure. This is done by solving for the reaction, shear, or moment at the point A caused by a unit load placed at ''x'' feet along the structure instead of a specific distance. This method is similar to the tabulated values method, but rather than obtaining a numeric solution, the outcome is an equation in terms of ''x''.<ref name=\"Hibbeler\"/>\n\nIt is important to understanding where the slope of the influence line changes for this method because the influence-line equation will change for each linear section of the influence line. Therefore, the complete equation will be a [[piecewise linear function]] which has a separate influence-line equation for each linear section of the influence line.<ref name=\"Hibbeler\"/>\n\n=== Müller-Breslau's Principle===\nAccording to www.public.iastate.edu, “[[Muller-Breslau's principle|The Müller-Breslau Principle]] can be utilized to draw [[qualitative data|qualitative]] influence lines, which are directly proportional to the actual influence line.”<ref name=\"iastate\"/>  Instead of moving a unit load along a beam, the Müller-Breslau Principle finds the deflected shape of the beam caused by first releasing the beam at the point being studied, and then applying the function (reaction, shear, or moment) being studied to that point. The principle states that the influence line of a function will have a scaled shape that is the same as the deflected shape of the beam when the beam is acted upon by the function.\n\nIn order to understand how the beam will deflect under the function, it is necessary to remove the beam’s capacity to resist the function. Below are explanations of how to find the influence lines of a simply supported, rigid beam (such as the one displayed in Figure 1).\n\n:* When determining the reaction caused at a support, the support is replaced with a roller, which cannot resist a vertical reaction.<ref name=\"iastate\"/><ref name=\"Hibbeler\"/> Then an upward (positive) reaction is applied to the point where the support was. Since the support has been removed, the beam will rotate upwards, and since the beam is rigid, it will create a triangle with the point at the second support. If the beam extends beyond the second support as a cantilever, a similar triangle will be formed below the cantilevers position. This means that the reaction’s influence line will be a straight, sloping line with a value of zero at the location of the second support.\n\n:* When determining the shear caused at some point B along the beam, the beam must be cut and a roller-guide (which is able to resist moments but not shear) must be inserted at point B.<ref name=\"iastate\"/><ref name=\"Hibbeler\"/> Then, by applying a positive shear to that point, it can be seen that the left side will rotate down, but the right side will rotate up. This creates a discontinuous influence line which reaches zero at the supports and whose slope is equal on either side of the discontinuity. If point B is at a support, then the deflection between point B and any other supports will still create a triangle, but if the beam is cantilevered, then the entire cantilevered side will move up or down creating a rectangle.\n\n:* When determining the moment caused by at some point B along the beam, a hinge will be placed at point B, releasing it to moments but resisting shear.<ref name=\"iastate\"/><ref name=\"Hibbeler\"/> Then when a positive moment is placed at point B, both sides of the beam will rotate up. This will create a continuous influence line, but the slopes will be equal and opposite on either side of the hinge at point B. Since the beam is simply supported, its end supports (pins) cannot resist moment; therefore, it can be observed that the supports will never experience moments in a static situation regardless of where the load is placed.\n\nThe Müller-Breslau Principle can only produce qualitative influence lines.<ref name=\"iastate\"/><ref name=\"Hibbeler\"/> This means that engineers can use it to determine where to place a load to incur the maximum of a function, but the magnitude of that maximum cannot be calculated from the influence line. Instead, the engineer must use statics to solve for the functions value in that loading case.\n\n==Alternate loading cases==\n\n===Multiple loads===\nThe simplest loading case is a single point load, but influence lines can also be used to determine responses due to multiple loads and distributed loads. Sometimes it is known that multiple loads will occur at some fixed distance apart. For example, on a bridge the wheels of cars or trucks create point loads that act at relatively standard distances.\n\nIn order to calculate the response of a function to all of these point loads using an influence line, the results found with the influence line can be scaled for each load, and then the scaled magnitudes can be summed to find the total response that the structure must withstand.<ref name=\"Hibbeler\"/> The point loads can have different magnitudes themselves, but even if they apply the same force to the structure, it will be necessary to scale them separately because they act at different distances along the structure. For example, if a car's wheels are 10 feet apart, then when the first set is 13 feet onto the bridge, the second set will be only 3 feet onto the bridge. If the first set of wheels is 7 feet onto the bridge, the second set has not yet reached the bridge, and therefore only the first set is placing a load on the bridge.\n\nAlso, if, between two loads, one of the loads is heavier, the loads must be examined in both loading orders (the larger load on the right and the larger load on the left) to ensure that the maximum load is found. If there are three or more loads, then the number of cases to be examined will increase.\n\n===Distributed loads===\nMany loads do not act as point loads, but instead act over an extended length or area as distributed loads. For example, a tractor with [[continuous track]]s will apply a load distributed over the length of each track.\n\nIn order to find the effect of a distributed load, the designer can integrate an influence line, found using a point load, over the affected distance of the structure.<ref name=\"Hibbeler\"/> For example, if a three-foot-long track acts between 5 feet and 8 feet along a beam, the influence line of that beam must be integrated between 5 and 8 feet. The integration of the influence line gives the effect that would be felt if the distributed load had a unit magnitude. Therefore, after integrating, the designer must still scale the results to get the actual effect of the distributed load.\n\n==Indeterminate structures==\nWhile the influence lines of statically determinate structures (as mentioned above) are made up of straight line segments, the same is not true for indeterminate structures. Indeterminate structures are not considered rigid; therefore, the influence lines drawn for them will not be straight lines but rather curves. The methods above can still be used to determine the influence lines for the structure, but the work becomes much more complex as the properties of the beam itself must be taken into consideration.\n\n==See also==\n* [[Beam (structure)|Beam]]\n* [[Shear and moment diagram|Shear and Moment Diagram]]\n* [[Dead and live loads|Dead and Live Loads]]\n* [[Müller-Breslau's principle]]\n\n==References==\n<references/>\n\n[[Category:Structural analysis]]\n[[Category:Structural engineering]]"
    },
    {
      "title": "Johnson's parabolic formula",
      "url": "https://en.wikipedia.org/wiki/Johnson%27s_parabolic_formula",
      "text": "[[File:Euler curve illustration.jpg|thumb|right|Curve of Euler's formula for buckling. The area above the curve indicates failure while the area under the curve represents scenarios that would not cause failure.]]\nThe '''Johnson formula''' is an empirically based formula relating the slenderness ratio to the stress illustrating the critical load required to buckle a column. The formula is based on empirical results by J. B. Johnson from around 1900 as an alternative to [[Euler's critical load]] formula under low slenderness ratio conditions.\n\nBuckling refers to a mode of failure in which the structure loses stability. It is caused by a lack of structural stiffness.<ref>Rice University (2009). \"Buckling Analysis\". Retrieved from https://www.clear.rice.edu/mech403/HelpFiles/FEA_Buckling_analysis.pdf</ref> Placing a load on a long slender bar will cause a buckling failure before the specimen can fail by compression.<ref>Dornfeld, W (27 October 2016. \"Machine Design\". ''Fairfield University''. Retrieved from http://www.faculty.fairfield.edu/wdornfeld/ME311/ME311MachineDesignNotes07.pdf</ref>\n\nOne way to calculate buckling is to utilize Euler's formula, which produces a critical stress vs. slenderness curve such as the one illustrated to the right.\n\nHowever, depending on the geometry of the structure under stress, this equation is not always applicable, and the Johnson parabola should be used.\n\n== Situations in which to apply the Johnson parabola ==\nEuler's formula is displayed as such:\n<math>\\sigma_{cr}={P_{cr} \\over A}={\\pi^2EI \\over Le^2}={\\pi^2E \\over \\left ( \\frac{l}{k} \\right )^2}</math>\nwhere\n:<math>\\sigma_{cr} =</math> critical stress, \n:<math>P_{cr} = </math> critical force,\n:<math>A = </math> area of cross section,\n:<math>Le =</math> Effective length of the rod,\n:<math>E = </math> modulus of elasticity,\n:<math>I = </math> area moment of inertia of the cross section of the rod,\n:<math>{l \\over k}</math> = slenderness ratio.\n\nEuler's equation is useful in situations such as an ideal pinned-pinned column, or in cases in which the effective length can be used to adjust the existing formula (ie. Fixed-Free).<ref>MechaniCalc (2016). \"Column Buckling\". Retrieved from https://mechanicalc.com/reference/column-buckling</ref>\n\n{| class=\"wikitable\"\n|-\n!  !! Pinned-Pinned !! Fixed-Fixed !! Fixed-Pinned !! Fixed-Free\n|-\n| Effective Length, <math>L_e</math> || 1L || 0.5L || 0.7L || 2L\n|}\n(L is the original length of the specimen before the force was applied.)\n\nHowever, certain geometries are not accurately represented by the Euler formula. One of the variables in the above equation that reflects the geometry of the specimen is the slenderness ratio, which is the column's length divided by the radius of gyration.<ref>Bello, D (2016). \"Buckling\". ''Allan Hancock College''. Retrieved from http://www.ah-engr.com/som/10_buckling/text_10-1.htm</ref>\n\nThe slenderness ratio of the member can be found with <math>\\left ( \\frac{l}{k} \\right )=L_e\\sqrt{A \\over I}</math> while the critical slenderness ratio is <math>{\\left ( \\frac{l}{k} \\right )}_{cr}=\\sqrt{2\\pi^2E \\over \\sigma_y}</math>\n\nIn practical terms, the slenderness ratio is an indicator of the specimen's resistance to bending and buckling, due to its length and cross section. If the slenderness ratio is less than the critical slenderness ratio, the column is considered to be a short column. In these cases, the Johnson parabola is more applicable than the Euler formula.<ref>Engineers Edge (2016). \"Ideal Pinned Column Buckling Calculation and Equation\". Retrieved from http://www.engineersedge.com/column_buckling/column_ideal.htm</ref>\n\n== Effect of the Johnson Parabola ==\n\n[[File:Euler johnson crit NEW.jpg|thumb|right|Graph of Johnson's parabola (plotted in red) against Euler's formula, with the transition point indicated. The Johnson parabola creates a new region of failure.]]\n\nJohnson's formula rounds out the function given by Euler's formula. It creates a new failure border by fitting a parabola to the graph of failure for Euler buckling.\n\n::<math>\\sigma_{cr}=\\sigma_y-{1\\over E}{\\left ( \\frac{\\sigma_y}{2\\pi} \\right )}^2{\\left ( \\frac{l}{k} \\right )}^2</math>\n\nThere is a transition point on the graph of the Euler curve, located at the critical slenderness ratio. At slenderness values lower than this point (occurring in specimens with a relatively short length compared to their cross section), the graph will follow the Johnson parabola; in contrast, larger slenderness values will align more closely with the Euler equation.\n\n== Application of the Johnson Parabola ==\n\nOne common material in aerospace applications is Al 2024. Certain material properties of Al 2024 have been determined experimentally, such as the tensile yield strength (324 MPa) and the modulus of elasticity (73.1 GPa). <ref>CRP Meccanica. \"Aluminum 2024-T4\". Retrieved from http://www.crpmeccanica.com/PDF/aluminium-2024-t4-2024-t351.pdf</ref> The Euler formula could be used to plot a failure curve, but it would not be accurate below a certain <math>\\frac{l}{k}</math> value, the critical slenderness ratio.\n\n::<math>{\\left ( \\frac{l}{k} \\right )}_{cr}=\\sqrt{2\\pi^2E \\over \\sigma_y}=\\sqrt{\\frac{2\\pi^2 \\cdot 73.1 \\cdot 10^9}{324 \\cdot 10^6}}=66.7</math>\n\nTherefore, the Euler equation is applicable for values of  <math>\\frac{l}{k}</math> greater than 66.7.\n::Euler: <math>\\sigma_{cr}={\\pi^2E \\over \\left ( \\frac{l}{k} \\right )^2}={\\pi^2 \\cdot 73.1 \\cdot 10^9 \\over \\left ( \\frac{l}{k} \\right )^2}</math> for <math>\\frac{l}{k}>66.7</math>\n::: (units in Pascals)\n\nJohnson's parabola takes care of the smaller <math>\\frac{l}{k}</math> values.\n::Johnson: <math>\\sigma_{cr}=\\sigma_y-{1\\over E}{\\left ( \\frac{\\sigma_y}{2\\pi} \\right )}^2{\\left ( \\frac{l}{k} \\right )}^2=324 \\cdot 10^6-{1\\over 73.1 \\cdot 10^9}{\\left ( \\frac{324 \\cdot 10^6}{2\\pi} \\right )}^2{\\left ( \\frac{l}{k} \\right )}^2</math> for <math>0\\le \\frac{l}{k}\\le 66.7</math>\n\n::: (units in Pascals)\n\n[[File:Critical Stress vs slenderness ratio for Al 2024.png|frame|center|Plot of both Euler and Johnson formulas for Al 2024. The transition point occurs at the critical slenderness ratio.]]\n\n== References ==\n{{Reflist}}\n\n[[Category:Elasticity (physics)]]\n[[Category:Materials science]]\n[[Category:Mechanical failure modes]]\n[[Category:Structural analysis]]\n[[Category:Mechanics]]"
    },
    {
      "title": "Limit analysis",
      "url": "https://en.wikipedia.org/wiki/Limit_analysis",
      "text": "'''Limit analysis''' is a [[structural analysis]] field which is dedicated to the development of efficient methods to directly determine estimates of the collapse load of a given structural model without resorting to iterative or incremental analysis.  For this purpose, the field of limit analysis is based on a set of theorems, referred to as limit theorems, which are a set of theorems based on the [[law of conservation of energy]] that state properties regarding stresses and strains, lower and upper-bound limits for the collapse load and the exact collapse load.\n\n==Software for limit analysis==\n* [http://www.limitstate.com/geo LimitState:GEO] (2008-) General purpose geotechnical software limit analysis application. Uses [[discontinuity layout optimization]].\n* [http://www.optumce.com OptumG2] (2014-) General purpose software for geotechnical applications (also includes elastoplasticity, seepage, consolidation, staged construction, tunneling, and other relevant geotechnical analysis types). \n* [http://www.limitstate.com/slab LimitState:SLAB] (2015-) Limit analysis software application for slabs. Uses [[discontinuity layout optimization]].\n\n==References==\n* {{cite book|last=Chen|first=Wai-Fah|author2=Da-Jian Han |title=Plasticity for structural engineers|year=2007|publisher=J. Ross publishing classics|isbn=978-1-932159-75-2|pages=409–490}}\n\n[[Category:Structural analysis]]\n\n\n{{Civil-engineering-stub}}"
    },
    {
      "title": "Macaulay's method",
      "url": "https://en.wikipedia.org/wiki/Macaulay%27s_method",
      "text": "'''Macaulay’s method (the double integration method)''' is a technique used in [[structural analysis]] to determine the [[Deflection (engineering)|deflection]] of [[beam theory|Euler-Bernoulli beams]].  Use of Macaulay’s technique is very convenient for cases of discontinuous and/or discrete loading.  Typically partial uniformly distributed loads (u.d.l.) and uniformly varying loads (u.v.l.) over the span and a number of concentrated loads are conveniently handled using this technique.\n\nThe first English language description of the method was by [[William Herrick Macaulay|Macaulay]].<ref name=Macaulay>[https://archive.org/stream/messengerofmathe4849cambuoft#page/n137/mode/2up W. H. Macaulay, \"A note on the deflection of beams\", Messenger of Mathematics, 48 (1919), 129.]</ref>  The actual approach appears to have been developed by [[Alfred Clebsch|Clebsch]] in 1862.<ref name=Weiss>J. T. Weissenburger, ‘Integration of discontinuous expressions arising in beam theory’, AIAA\nJournal, 2(1) (1964), 106–108.</ref>  Macaulay's method has been generalized for Euler-Bernoulli beams with axial compression,<ref name=Wittrick>[[W. H. Wittrick]], \"A generalization of Macaulay’s method with applications in structural mechanics\", AIAA Journal, 3(2) (1965), 326–330.</ref> to [[Timoshenko beam theory|Timoshenko beams]],<ref name=Yavari>A. Yavari, S. Sarkani and J. N. Reddy, ‘On nonuniform Euler–Bernoulli and Timoshenko beams with jump discontinuities: application of distribution theory’, International Journal of Solids and Structures, 38(46–7) (2001), 8389–8406.</ref> to [[elastic foundation]]s,<ref name=Yavari1>A. Yavari, S. Sarkani and J. N. Reddy, ‘Generalised solutions of beams with jump discontinuities\non elastic foundations’, Archive of Applied Mechanics, 71(9) (2001), 625–639.</ref> and to problems in which the bending and shear stiffness changes discontinuously in a beam<ref name=Stephen>Stephen, N. G., (2002), \"Macaulay's method for a Timoshenko beam\", Int. J. Mech. Engg. Education, 35(4), pp. 286-292.</ref>\n\n==Method==\n\nThe starting point is the relation from [[beam theory|Euler-Bernoulli beam theory]]\n:<math>\n  \\pm EI\\dfrac{d^2w}{dx^2} = M \n</math>\nWhere <math>w</math> is the deflection and <math>M</math> is the bending moment.\nThis equation<ref name=note1>The sign on the left hand side of the equation depends on the convention that is used.  For the rest of this article we will assume that the sign convention is such that a positive sign is appropriate.</ref> is simpler than the fourth-order beam equation and can be integrated twice to find <math>w</math> if the value of <math>M</math> as a function of <math>x</math> is known.  For general loadings, <math>M</math> can be expressed in the form\n:<math>\n   M = M_1(x) + P_1\\langle x - a_1\\rangle + P_2\\langle x - a_2\\rangle + P_3\\langle x - a_3\\rangle + \\dots\n </math>\nwhere the quantities <math>P_i\\langle x - a_i\\rangle</math> represent the bending moments due to point loads and the quantity <math>\\langle x - a_i\\rangle</math> is a [[Macaulay bracket]] defined as\n:<math>\n   \\langle x - a_i\\rangle = \\begin{cases} 0 & \\mathrm{if}~ x < a_i \\\\ x - a_i & \\mathrm{if}~ x > a_i \\end{cases}\n </math>\nOrdinarily, when integrating <math>P(x-a)</math> we get\n:<math>\n   \\int P(x-a)~dx = P\\left[\\cfrac{x^2}{2} - ax\\right] + C\n </math>\nHowever, when integrating expressions containing Macaulay brackets, we have\n:<math>\n   \\int P\\langle x-a \\rangle~dx = P\\cfrac{\\langle x-a \\rangle^2}{2} + C_m\n </math>\nwith the difference between the two expressions being contained in the constant <math>C_m</math>.  Using these integration rules makes the calculation of the deflection of Euler-Bernoulli beams simple in situations where there are multiple point loads and point moments.  The Macaulay method predates more sophisticated concepts such as [[Dirac delta function]]s and [[step function]]s but achieves the same outcomes for beam problems.\n\n== Example: Simply supported beam with point load ==\n[[File:SimpSuppBeamPointLoadUnsymm.svg|right|thumb|350px|Simply supported beam with a single eccentric concentrated load.]]\nAn illustration of the Macaulay method considers a simply supported beam with a single eccentric concentrated load as shown in the adjacent figure.  The first step is to find <math>M</math>. The reactions at the supports A and C are determined from the balance of forces and moments as\n:<math>\n   R_A + R_C = P,~~ L R_C = P a\n </math>\nTherefore, <math>R_A = Pb/L</math> and the bending moment at a point D between A and B (<math> 0 < x < a</math>) is given by\n:<math>\n  M = R_A x = Pbx/L  \n </math>\nUsing the moment-curvature relation and the Euler-Bernoulli expression for the bending moment, we have \n:<math>\n   EI\\dfrac{d^2w}{dx^2} = \\dfrac{Pbx}{L}\n</math>   \nIntegrating the above equation we get, for <math> 0 < x < a</math>,\n:<math>\n  \\begin{align}\n    EI\\dfrac{dw}{dx} &= \\dfrac{Pbx^2}{2L} +C_1 & &\\quad\\mathrm{(i)}\\\\\n    EI w &= \\dfrac{Pbx^3}{6L} + C_1 x + C_2    & &\\quad\\mathrm{(ii)}\n  \\end{align}\n </math>\nAt <math>x=a_{-}</math>\n:<math>\n  \\begin{align}\n    EI\\dfrac{dw}{dx}(a_{-}) &= \\dfrac{Pba^2}{2L} +C_1 & &\\quad\\mathrm{(iii)} \\\\\n    EI w(a_{-}) &= \\dfrac{Pba^3}{6L} + C_1 a + C_2    & &\\quad\\mathrm{(iv)}\n  \\end{align}\n </math>\nFor a point D in the region BC (<math>a < x < L</math>), the bending moment is \n:<math>\n   M = R_A x - P(x-a) = Pbx/L - P(x-a)\n </math>\nIn Macaulay's approach we use the [[Macaulay bracket]] form of the above expression to represent the fact that a point load has been applied at location B, i.e.,\n:<math>\n   M = \\frac{Pbx}{L} - P\\langle x-a \\rangle\n </math>\n\nTherefore, the Euler-Bernoulli beam equation for this region has the form\n:<math>\n  EI\\dfrac{d^2w}{dx^2} = \\dfrac{Pbx}{L} - P\\langle x-a \\rangle\n</math>\nIntegrating the above equation, we get for <math>a < x < L</math>\n:<math>\n  \\begin{align}\n    EI\\dfrac{dw}{dx} &= \\dfrac{Pbx^2}{2L} - P\\cfrac{\\langle x-a \\rangle^2}{2} + D_1 & &\\quad\\mathrm{(v)}\\\\\n    EI w &= \\dfrac{Pbx^3}{6L} - P\\cfrac{\\langle x-a \\rangle^3}{6} + D_1 x + D_2    & &\\quad\\mathrm{(vi)}\n  \\end{align}\n </math>\nAt <math>x=a_{+}</math>\n:<math>\n  \\begin{align}\n    EI\\dfrac{dw}{dx}(a_{+}) &= \\dfrac{Pba^2}{2L} + D_1 & &\\quad\\mathrm{(vii)}\\\\\n    EI w(a_{+}) &= \\dfrac{Pba^3}{6L} + D_1 a + D_2    & &\\quad\\mathrm{(viii)}\n  \\end{align}\n </math>\n\nComparing equations (iii) & (vii) and (iv) & (viii) we notice that due to continuity at point B, <math>C_1 = D_1</math> and <math>C_2 = D_2</math>.  The above observation implies that for the two regions considered, though the equation for [[bending moment]] and hence for the [[curvature]] are different, the constants of integration got during successive integration of the equation for curvature for the two regions are the same.\n\nThe above argument holds true for any number/type of discontinuities in the equations for curvature, provided that in each case the equation retains the term for the subsequent region in the form <math>\\langle x-a\\rangle ^n, \\langle x-b\\rangle ^n, \\langle x-c\\rangle ^n</math> etc.\nIt should be remembered that for any x, giving the quantities within the brackets, as in the above case, -ve should be neglected, and the calculations should be made considering only the quantities which give +ve sign for the terms within the brackets.\n\nReverting to the problem, we have\n:<math>\n  EI\\dfrac{d^2w}{dx^2} = \\dfrac{Pbx}{L} - P\\langle x-a \\rangle\n</math>\nIt is obvious that the first term only is to be considered for <math>x < a</math> and both the terms for <math>x > a</math> and the solution is\n:<math>\n  \\begin{align}\n    EI\\dfrac{dw}{dx} &= \\left[\\dfrac{Pbx^2}{2L} + C_1\\right] - \\cfrac{P\\langle x-a \\rangle^2}{2} \\\\\n    EI w &= \\left[\\dfrac{Pbx^3}{6L} + C_1 x + C_2\\right] - \\cfrac{P\\langle x-a \\rangle^3}{6} \n  \\end{align}\n </math>\nNote that the constants are placed immediately after the first term to indicate that they go with the first term when <math>x < a</math> and with both the terms when <math>x > a</math>.  The Macaulay brackets help as a reminder that the quantity on the right is zero when considering points with <math>x < a</math>.\n\n=== Boundary Conditions ===\n\nAs <math>w = 0</math> at <math>x = 0</math>,  <math>C2 = 0</math>.   Also, as <math>w = 0</math> at <math>x = L</math>, \n:<math>\n  \\left[\\dfrac{PbL^2}{6} + C_1 L \\right] - \\cfrac{P(L-a)^3}{6} = 0\n </math>\nor,\n:<math>\n   C_1 = -\\cfrac{Pb}{6L}(L^2-b^2) ~.\n </math>\nHence,\n:<math>\n  \\begin{align}\n    EI\\dfrac{dw}{dx} &= \\left[\\dfrac{Pbx^2}{2L} -\\cfrac{Pb}{6L}(L^2-b^2)\\right] - \\cfrac{P\\langle x-a \\rangle^2}{2} \\\\\n    EI w &= \\left[\\dfrac{Pbx^3}{6L} -\\cfrac{Pbx}{6L}(L^2-b^2)\\right] - \\cfrac{P\\langle x-a \\rangle^3}{6} \n  \\end{align}\n </math>\n\n=== Maximum deflection ===\nFor <math>w</math> to be maximum, <math>dw/dx = 0</math>.  Assuming that this happens for <math>x < a</math> we have\n:<math>\n   \\dfrac{Pbx^2}{2L} -\\cfrac{Pb}{6L}(L^2-b^2) = 0\n </math> \nor\n:<math>\n   x = \\pm \\cfrac{(L^2-b^2)^{1/2}}{\\sqrt{3}}\n </math>\nClearly <math> x < 0</math> cannot be a solution.  Therefore, the maximum deflection is given by\n:<math>\n   EI w_{\\mathrm{max}} = \\cfrac{1}{3}\\left[\\dfrac{Pb(L^2-b^2)^{3/2}}{6\\sqrt{3}L}\\right] -\\cfrac{Pb(L^2-b^2)^{3/2}}{6\\sqrt{3}L}\n </math>\nor,\n:<math>\n   w_{\\mathrm{max}} = -\\dfrac{Pb(L^2-b^2)^{3/2}}{9\\sqrt{3}EIL}~.\n </math>\n\n=== Deflection at load application point ===\nAt <math>x = a</math>, i.e., at point B, the deflection is\n:<math>\n  EI w_B = \\dfrac{Pba^3}{6L} -\\cfrac{Pba}{6L}(L^2-b^2) = \\frac{Pba}{6L}(a^2+b^2-L^2)\n </math>\nor\n:<math>\n    w_B = -\\cfrac{Pa^2b^2}{3LEI}\n </math>\n\n=== Deflection at midpoint ===\nIt is instructive to examine the ratio of <math>w_{\\mathrm{max}}/w(L/2)</math>.  At <math>x = L/2</math>\n:<math>\n  EI w(L/2) = \\dfrac{PbL^2}{48} -\\cfrac{Pb}{12}(L^2-b^2) = -\\frac{Pb}{12}\\left[\\frac{3L^2}{4} -b^2\\right]\n </math>\nTherefore,\n:<math>\n  \\frac{w_{\\mathrm{max}}}{w(L/2)} = \\frac{4(L^2-b^2)^{3/2}}{3\\sqrt{3}L\\left[\\frac{3L^2}{4} -b^2\\right]} \n    = \\frac{4(1-\\frac{b^2}{L^2})^{3/2}}{3\\sqrt{3}\\left[\\frac{3}{4} - \\frac{b^2}{L^2}\\right]}\n    = \\frac{16(1-k^2)^{3/2}}{3\\sqrt{3}\\left(3 - 4k^2\\right)}\n </math>\nwhere <math>k = B/L</math> and for <math>a < b; 0 < k < 0.5</math>.  Even when the load is as near as 0.05L from the support, the error in estimating the deflection is only 2.6%. Hence in most of the cases the estimation of maximum deflection may be made fairly accurately with reasonable margin of error by working out deflection at the centre.\n\n=== Special case of symmetrically applied load ===\nWhen <math>a = b = L/2</math>, for <math>w</math> to be maximum\n:<math>\n   x = \\cfrac{[L^2-(L/2)^2]^{1/2}}{\\sqrt{3}} = \\frac{L}{2} \n </math>\nand the maximum deflection is\n:<math>\n   w_{\\mathrm{max}} = -\\dfrac{P(L/2)b[L^2-(L/2)^2]^{3/2}}{9\\sqrt{3}EIL} = -\\frac{PL^3}{48EI} = w(L/2)~.\n </math>\n\n== References ==\n{{Reflist}}\n\n== See also ==\n* [[Beam theory]]\n* [[Bending]]\n* [[Bending moment]]\n* [[Singularity function]]\n* [[Shear and moment diagram]]\n* [[Timoshenko beam theory]]\n\n[[Category:Structural analysis]]"
    },
    {
      "title": "Marcus' method",
      "url": "https://en.wikipedia.org/wiki/Marcus%27_method",
      "text": "{{multiple issues|\n{{Orphan|date=December 2012}}\n{{Unreferenced|date=July 2011}}\n}}\n\n'''Marcus' method''' (also referred to as '''Marcus's method''' and '''Method of Marcus''') is a [[structural analysis]] method which was designed to design [[concrete slab]]s with rectangular, orthogonal shapes.  It represents an adaptation of the strip method.Its based on elastic analysis of torsionally restrained two way rectangular slabs with a uniformly distributed load.\nMarcus introduced a correction factor (reduction factor rather) to the existing Rankine Grashoff theory in order to account for the torsional restraints at the corners.\n\n==See also==\n* [[Grashof and Rankine method]]\n\n==References==\n<references/>\n\n[[Category:Structural analysis]]\n\n\n{{engineering-stub}}"
    },
    {
      "title": "Matrix method",
      "url": "https://en.wikipedia.org/wiki/Matrix_method",
      "text": "{{Unreferenced stub|auto=yes|date=December 2009}}\nThe '''matrix method''' is a [[structural analysis]] method used as a fundamental principle in many applications in [[civil engineering]]. \n\nThe method is carried out, using either a stiffness matrix or a flexibility matrix. ''\"The flexibility method is not conducive to computer programming\" - Weaver, Gere''.\n\n==See also==\n* [[Direct stiffness method]]\n* [[Flexibility method]]\n\n{{DEFAULTSORT:Matrix Method}}\n[[Category:Structural analysis]]\n\n\n{{Civil-engineering-stub}}"
    },
    {
      "title": "Membrane analogy",
      "url": "https://en.wikipedia.org/wiki/Membrane_analogy",
      "text": "The '''elastic membrane analogy''', also known as the '''soap-film analogy''', was first published by pioneering aerodynamicist [[Ludwig Prandtl]] in 1903.\n<ref>Prandtl, L.: \"Zur torsion von prismatischen stäben\", Phys. Zeitschr., '''4''', pp. 758-770 (1903)</ref>\n<ref>Love 1944, article 224, page 322.</ref>\nIt describes the [[Stress (physics)|stress]] distribution on a long bar in [[torsion (mechanics)|torsion]].  The cross section of the bar is constant along its length, and need not be circular. The [[differential equation]] that governs the stress distribution on the bar in torsion is of the same form as the equation governing the shape of a membrane under differential pressure.  Therefore, in order to discover the stress distribution on the bar, all one has to do is cut the shape of the cross section out of a piece of wood, cover it with a soap film, and apply a differential pressure across it.  Then the slope of the soap film at any area of the cross section is directly proportional to the stress in the bar at the same point on its cross section.\nSimply when steel subjected or installed in a structures load distribution from slab to footing \nConsider a steel element which will allow transfer load through it in a steel molecular are in stress may be normal or shear stress  variation steel element trying to resist loads how much stress occurred in length of steel bar or structural element are drawn in contour lines\n\n==Application to thin-walled, open cross sections==\nWhile the membrane analogy allows the stress distribution on any cross section to be determined experimentally, it also allows the stress distribution on thin-walled, open cross sections to be determined by the same theoretical approach that describes the behavior of rectangular sections.  Using the membrane analogy, any thin-walled cross section can be \"stretched out\" into a rectangle without affecting the stress distribution under torsion.  The maximum shear stress, therefore, occurs at the edge of the midpoint of the stretched cross section, and is equal to <math>3T/bt^2</math>, where T is the [[torque]] applied, b is the length of the stretched cross section, and t is the thickness of the cross section.\n\nIt can be shown that the [[differential equation]] for the [[deflection surface]] of a homogeneous membrane, subjected to uniform lateral pressure and with uniform surface tension and with the same outline as that of the [[cross section (geometry)|cross section]] of a bar under [[Torsion (mechanics)|torsion]], has the same form as that governing the stress distribution over the cross section of a bar under [[Torsion (mechanics)|torsion]].\n\nThis analogy was originally proposed by [[Ludwig Prandtl]] in 1903.<ref>Prandtl, L.: \"Zur torsion von prismatischen stäben\", Phys. Z., '''4''', pp. 758-770 (1903).</ref>\n\n==Other applications==\n\nPrandtl's stretched-membrane concept was used extensively in the field of electron tube (\"vacuum tube\") design (1930's to 1960's) to model the trajectory of electrons within a device. The model is constructed by uniformly stretching a thin rubber sheet over a frame, and deforming the sheet upwards with physical models of electrodes, impressed into the sheet from below. The entire assembly is tilted, and steel balls (as electron analogs) rolled down the assembly and the trajectories noted. The curved surface surrounding the \"electrodes\" represents the complex increase in field strength as the electron-analog approaches the \"electrode\"; the upward distortion in the sheet is a close analogy to field strength.\n\n==References==\n{{Reflist}}\n*{{cite book | last = Bruhn  | first = Elmer Franklin\n| title = ''Analysis and Design of Flight Vehicle Structures\n| publisher = Jacobs Publishing | year = 1973 | location = Indianapolis\n| pages = | url = | isbn =  0-9615234-0-9 }}\n*{{cite book | last = Love | first = A. E. H.\n| authorlink = Augustus Edward Hough Love\n| title = A Treatise on the Mathematical Theory of Elasticity\n| publisher = Dover | year = 1944 | location = New York\n| pages = | url = \n| isbn = 0-486-60174-9}}. Especially Chapter XIV, articles 215 through 224.  \"This Dover edition, first published in 1944, is an unaltered and unabridged republication of the fourth (1927) edition.\"\n*{{cite book | last = | first = \n| title = ''Advances in Electronics Volume 2\n| publisher =  | year = 1950 | location = \n| pages =  141 | url = https://www.scribd.com/doc/73164400/26/IV-AUTOMATIC-TRAJECTORY-TRACING | isbn = }}\n\n\n{{DEFAULTSORT:Membrane Analogy}}\n[[Category:Mechanics]]\n[[Category:Solid mechanics]]\n[[Category:Structural analysis]]\n[[Category:Analogy]]"
    },
    {
      "title": "Moment distribution method",
      "url": "https://en.wikipedia.org/wiki/Moment_distribution_method",
      "text": "{{Distinguish|moment redistribution}}\nThe '''moment distribution method''' is a [[structural analysis]] method for [[statically indeterminate]] [[Beam (structure)|beam]]s and [[Framing (construction)|frames]] developed by [[Hardy Cross]]. It was published in 1930 in an [[American Society of Civil Engineers|ASCE]] journal.<ref name=\"asce1\">{{Cite news|first=Hardy|last=Cross|year=1930|title=Analysis of Continuous Frames by Distributing Fixed-End Moments|periodical=Proceedings of the American Society of Civil Engineers|publisher=ASCE|pages=919–928|postscript=<!--None-->}}</ref> The method only accounts for flexural effects and ignores axial and shear effects. From the 1930s until [[computers]] began to be widely used in the design and analysis of structures, the moment distribution method was the most widely practiced method.\n\n== Introduction ==\nIn the moment distribution method, every [[joint]] of the structure to be analysed is fixed so as to develop the ''fixed-end moments''. Then each fixed joint is sequentially released and the fixed-end moments (which by the time of release are not in equilibrium) are distributed to adjacent members until [[Mechanical equilibrium|equilibrium]] is achieved. The moment distribution method in mathematical terms can be demonstrated as the process of solving a set of [[simultaneous equations]] by means of [[iteration]].\n\nThe moment distribution method falls into the category of [[displacement method]] of structural analysis.\n\n== Implementation  ==\nIn order to apply the moment distribution method to analyse a structure, the following things must be considered.\n\n=== Fixed end moments ===\n[[Fixed end moments]] are the moments produced at member ends by external loads that does not mean the joint is fixed\n\n=== Flexural stiffness ===\nThe [[flexural stiffness]] (EI/L) of a member is represented as the product of the [[modulus of elasticity]] (E) and the [[second moment of area]] (I) divided by the length (L) of the member. What is needed in the moment distribution method is not the exact value but the [[ratio]] of flexural stiffness of all members.\n\n=== Distribution factors ===\nWhen a joint is being released and begins to rotate under the unbalanced moment, resisting forces develop at each member framed together at the joint. Although the total resistance is equal to the unbalanced moment, the magnitudes of resisting forces developed at each member differ by the members' flexural stiffness. Distribution factors can be defined as the proportions of the unbalanced moments carried by each of the members. In mathematical terms, distribution factor of member <math>k</math> framed at joint <math>j</math> is given as:\n:<math>D_{jk} = \\frac{\\frac{E_k I_k}{L_k}}{\\sum_{i=1}^{i=n} \\frac{E_i I_i}{L_i}}</math>\nwhere n is the number of members framed at the joint.\n\n=== Carryover factors ===\nWhen a joint is released, balancing moment occurs to counterbalance the unbalanced moment which is initially the same as the fixed-end moment. This balancing moment is then carried over to the member's other end. The ratio of the carried-over moment at the other end to the fixed-end moment of the initial end is the carryover factor.\n\n==== Determination of carryover factors ====\nLet one end (end A) of a fixed beam be released and applied a moment <math>M_A</math> while the other end (end B) remains fixed. This will cause end A to rotate through an angle <math>\\theta_A</math>. Once the magnitude of <math>M_B</math> developed at end B is found, the carryover factor of this member is given as the ratio of <math>M_B</math> over <math>M_A</math>:\n:<math>C_{AB} = \\frac{M_B}{M_A}</math>\nIn case of a beam of length L with constant cross-section whose flexural rigidity is <math>EI</math>, \n:<math>M_A = 4 \\frac{EI}{L} \\theta_A + 2 \\frac{EI}{L} \\theta_B = 4 \\frac{EI}{L} \\theta_A</math>\n:<math>M_B = 2 \\frac{EI}{L} \\theta_A + 4 \\frac{EI}{L} \\theta_B = 2 \\frac{EI}{L} \\theta_A</math>\ntherefore the carryover factor\n:<math>C_{AB} = \\frac{M_B}{M_A} = \\frac{1}{2}</math>\n\n=== Sign convention ===\nOnce a sign convention has been chosen, it has to be maintained for the whole structure. The traditional engineer's sign convention is not used in the calculations of the moment distribution method although the results can be expressed in the conventional way. In the BMD case, the left side moment is clockwise direction and other is anticlockwise direction so the bending is positive and is called sagging.\n\n=== Framed structures ===\nFramed structures with or without sidesway can be analysed using the moment distribution method.\n\n== Example ==\n[[Image:MomentDistributionMethod.jpg|thumb|434px|right|Example]]\nThe statically indeterminate beam shown in the figure is to be analysed.\n*Members AB, BC, CD have the same [[Span (architecture)|span]] <math> L = 10 \\ m </math>.\n*Flexural rigidities are EI, 2EI, EI respectively.\n*Concentrated load of magnitude <math> P = 10 \\ kN </math> acts at a distance <math> a = 3 \\ m </math> from the support A.\n*Uniform load of intensity <math> q = 1 \\ kN/m</math> acts on BC.\n*Member CD is loaded at its midspan with a concentrated load of magnitude <math> P = 10 \\ kN </math>.\nIn the following calculations, counterclockwise moments are positive.\n\n=== Fixed end moments ===\n{{See also|Fixed end moment}}\n:<math>M _{AB} ^f = - \\frac{Pb^2a }{L^2} = - \\frac{10 \\times 7^2 \\times 3}{10^2} = - 14.700 \\ kN\\cdot m</math>\n:<math>M _{BA} ^f = \\frac{Pa^2b}{L^2} = \\frac{10 \\times 3^2 \\times 7}{10^2} = + 6.300 \\ kN\\cdot m</math>\n:<math>M _{BC} ^f = - \\frac{qL^2}{12} =- \\frac{1 \\times 10^2}{12} = - 8.333 \\ kN\\cdot m</math>\n:<math>M _{CB} ^f = \\frac{qL^2}{12} = \\frac{1 \\times 10^2}{12} = + 8.333 \\ kN\\cdot m</math>\n:<math>M _{CD} ^f =  - \\frac{PL}{8} = -  \\frac{10 \\times 10}{8} = - 12.500 \\ kN\\cdot m</math>\n:<math>M _{DC} ^f =\\frac{PL}{8} =\\frac{10 \\times 10}{8} = + 12.500 \\ kN\\cdot m</math>\n\n=== Flexural stiffness and distribution factors ===\nThe flexural stiffness of members AB, BC and CD are <math>\\frac{3EI}{L}</math>, <math>\\frac{4\\times 2EI}{L}</math> and <math>\\frac{4EI}{L}</math>, respectively {{Disputed inline|date=August 2017}}. Therefore, expressing the results in [[repeating decimal]] notation:\n:<math>D_{BA} = \\frac{\\frac{3EI}{L}}{\\frac{3EI}{L}+\\frac{4\\times 2EI}{L}} = \\frac{\\frac{3}{10}}{\\frac{3}{10}+\\frac{8}{10}} = \\frac{3}{11} = 0.(27)</math>\n:<math>D_{BC} = \\frac{\\frac{4\\times 2EI}{L}}{\\frac{3EI}{L}+\\frac{4\\times 2EI}{L}} = \\frac{\\frac{8}{10}}{\\frac{3}{10}+\\frac{8}{10}} = \\frac{8}{11} = 0.(72)</math>\n:<math>D_{CB} = \\frac{\\frac{4\\times 2EI}{L}}{\\frac{4\\times 2EI}{L}+\\frac{4EI}{L}} = \\frac{\\frac{8}{10}}{\\frac{8}{10}+\\frac{4}{10}} = \\frac{8}{12} = 0.(66)</math>\n:<math>D_{CD} = \\frac{\\frac{4EI}{L}}{\\frac{4\\times 2EI}{L}+\\frac{4EI}{L}} = \\frac{\\frac{4}{10}}{\\frac{8}{10}+\\frac{4}{10}} = \\frac{4}{12} = 0.(33)</math>\n\nThe distribution factors of joints A and D are <math>D_{AB} = 1</math> and <math>D_{DC} = 0 </math>.\n\n=== Carryover factors ===\nThe carryover factors are <math> \\frac{1}{2} </math>, except for the carryover factor from D (fixed support) to C which is zero.\n\n=== Moment distribution ===\n{| class=\"wikitable\" style=\"background-color:white\"\n|- style=\"background=color:#EEEEEE;\"\n!\n!colspan=\"11\"|[[Image:MomentDistributionMethod2.jpg|700px]]\n|- style=\"background-color:#EEEEEE;\"\n|style=\"text-align:center; font-weight:normal;\"|\n|style=\"text-align:right; width:50px; padding:1px;\"|Joint\n|style=\"text-align:left; width:82px; padding:1px;\"|A\n|style=\"text-align:center; width:31px; padding:1px;\"|\n|style=\"text-align:right; width:82px; padding:1px;\"|Joint\n|style=\"text-align:left; width:82px; padding:1px;\"|B\n|style=\"text-align:center; width:31px; padding:1px;\"|\n|style=\"text-align:right; width:82px; padding:1px;\"|Joint\n|style=\"text-align:left; width:82px; padding:1px;\"|C\n|style=\"text-align:center; width:31px; padding:1px;\"|\n|style=\"text-align:right; width:82px; padding:1px;\"|Joint\n|style=\"text-align:left; width:44px; padding:1px;\"|D\n|- style=\"background-color:#EEEEEE; font-weight:bold;\"\n|style=\"text-align:center; font-weight:normal;\"|Distrib. factors\n|style=\"text-align:right; width:50px; padding:1px;\"|0\n|style=\"text-align:left; width:82px; padding:1px;\"|1\n|style=\"text-align:center; width:31px; padding:1px;\"|\n|style=\"text-align:right; width:82px; padding:1px;\"|0.2727\n|style=\"text-align:left; width:82px; padding:1px;\"|0.7273\n|style=\"text-align:center; width:31px; padding:1px;\"|\n|style=\"text-align:right; width:82px; padding:1px;\"|0.6667\n|style=\"text-align:left; width:82px; padding:1px;\"|0.3333\n|style=\"text-align:center; width:31px; padding:1px;\"|\n|style=\"text-align:right; width:82px; padding:1px;\"|0\n|style=\"text-align:left; width:44px; padding:1px;\"|0\n|- style=\"background-color:#EEEEEE; font-weight:bold;\"\n|style=\"text-align:center; font-weight:normal;\"|Fixed-end moments\n|style=\"text-align:right; width:50px; padding:1px;\"|\n|style=\"text-align:left; width:82px; padding:1px;\"|-14.700\n|style=\"text-align:center; width:31px; padding:1px;\"|\n|style=\"text-align:right; width:82px; padding:1px;\"|+6.300\n|style=\"text-align:left; width:82px; padding:1px;\"|-8.333\n|style=\"text-align:center; width:31px; padding:1px;\"|\n|style=\"text-align:right; width:82px; padding:1px;\"|+8.333\n|style=\"text-align:left; width:82px; padding:1px;\"|-12.500\n|style=\"text-align:center; width:31px; padding:1px;\"|\n|style=\"text-align:right; width:82px; padding:1px;\"|+12.500\n|style=\"text-align:left; width:44px; padding:1px;\"|\n|-\n|style=\"text-align:center;\"|Step 1\n|style=\"text-align:right; width:50px; padding:1px;\"|\n|style=\"text-align:left; width:82px; background-color:#F8F8F8; padding:1px;\"|+14.700\n|style=\"text-align:center; width:31px; padding:1px;\"|→\n|style=\"text-align:right; width:82px; padding:1px;\"|+7.350\n|style=\"text-align:left; width:82px; padding:1px;\"|\n|style=\"text-align:center; width:31px; padding:1px;\"|\n|style=\"text-align:right; width:82px; padding:1px;\"|\n|style=\"text-align:left; width:82px; padding:1px;\"|\n|style=\"text-align:center; width:31px; padding:1px;\"|\n|style=\"text-align:right; width:82px; padding:1px;\"|\n|style=\"text-align:left; width:44px; padding:1px;\"|\n|-\n|style=\"text-align:center;\"|Step 2\n|style=\"text-align:right; width:50px; padding:1px;\"|\n|style=\"text-align:left; width:82px; padding:1px;\"|\n|style=\"text-align:center; width:31px; padding:1px;\"|\n|style=\"text-align:right; width:82px; background-color:#F8F8F8; padding:1px;\"|-1.450\n|style=\"text-align:left; width:82px; background-color:#F8F8F8; padding:1px;\"|-3.867\n|style=\"text-align:center; width:31px; padding:1px;\"|→\n|style=\"text-align:right; width:82px; padding:1px;\"|-1.934\n|style=\"text-align:left; width:82px; padding:1px;\"|\n|style=\"text-align:center; width:31px; padding:1px;\"|\n|style=\"text-align:right; width:82px; padding:1px;\"|\n|style=\"text-align:left; width:44px; padding:1px;\"|\n|-\n|style=\"text-align:center;\"|Step 3\n|style=\"text-align:right; width:50px; padding:1px;\"|\n|style=\"text-align:left; width:82px; padding:1px;\"|\n|style=\"text-align:center; width:31px; padding:1px;\"|\n|style=\"text-align:right; width:82px; padding:1px;\"|\n|style=\"text-align:left; width:82px; padding:1px;\"|+2.034\n|style=\"text-align:center; width:31px; padding:1px;\"|←\n|style=\"text-align:right; width:82px; background-color:#F8F8F8; padding:1px;\"|+4.067\n|style=\"text-align:left; width:82px; background-color:#F8F8F8; padding:1px;\"|+2.034\n|style=\"text-align:center; width:31px; padding:1px;\"|→\n|style=\"text-align:right; width:82px; background-color:#F8F8F8; padding:1px;\"|+1.017\n|style=\"text-align:left; width:44px; padding:1px;\"|\n|-\n|style=\"text-align:center;\"|Step 4\n|style=\"text-align:right; width:50px; padding:1px;\"|\n|style=\"text-align:left; width:82px; padding:1px;\"|\n|style=\"text-align:center; width:31px; padding:1px;\"|\n|style=\"text-align:right; width:82px; background-color:#F8F8F8; padding:1px;\"|-0.555\n|style=\"text-align:left; width:82px; background-color:#F8F8F8; padding:1px;\"|-1.479\n|style=\"text-align:center; width:31px; padding:1px;\"|→\n|style=\"text-align:right; width:82px; padding:1px;\"|-0.739\n|style=\"text-align:left; width:82px; padding:1px;\"|\n|style=\"text-align:center; width:31px; padding:1px;\"|\n|style=\"text-align:right; width:82px; padding:1px;\"|\n|style=\"text-align:left; width:44px; padding:1px;\"|\n|-\n|style=\"text-align:center;\"|Step 5\n|style=\"text-align:right; width:50px; padding:1px;\"|\n|style=\"text-align:left; width:82px; padding:1px;\"|\n|style=\"text-align:center; width:31px; padding:1px;\"|\n|style=\"text-align:right; width:82px; padding:1px;\"|\n|style=\"text-align:left; width:82px; padding:1px;\"|+0.246\n|style=\"text-align:center; width:31px; padding:1px;\"|←\n|style=\"text-align:right; width:82px; background-color:#F8F8F8; padding:1px;\"|+0.493\n|style=\"text-align:left; width:82px; background-color:#F8F8F8; padding:1px;\"|+0.246\n|style=\"text-align:center; width:31px; padding:1px;\"|→\n|style=\"text-align:right; width:82px; background-color:#F8F8F8; padding:1px;\"|+0.123\n|style=\"text-align:left; width:44px; padding:1px;\"|\n|-\n|style=\"text-align:center;\"|Step 6\n|style=\"text-align:right; width:50px; padding:1px;\"|\n|style=\"text-align:left; width:82px; padding:1px;\"|\n|style=\"text-align:center; width:31px; padding:1px;\"|\n|style=\"text-align:right; width:82px; background-color:#F8F8F8; padding:1px;\"|-0.067\n|style=\"text-align:left; width:82px; background-color:#F8F8F8; padding:1px;\"|-0.179\n|style=\"text-align:center; width:31px; padding:1px;\"|→\n|style=\"text-align:right; width:82px; padding:1px;\"|-0.090\n|style=\"text-align:left; width:82px; padding:1px;\"|\n|style=\"text-align:center; width:31px; padding:1px;\"|\n|style=\"text-align:right; width:82px; padding:1px;\"|\n|style=\"text-align:left; width:44px; padding:1px;\"|\n|-\n|style=\"text-align:center;\"|Step 7\n|style=\"text-align:right; width:50px; padding:1px;\"|\n|style=\"text-align:left; width:82px; padding:1px;\"|\n|style=\"text-align:center; width:31px; padding:1px;\"|\n|style=\"text-align:right; width:82px; padding:1px;\"|\n|style=\"text-align:left; width:82px; padding:1px;\"|+0.030\n|style=\"text-align:center; width:31px; padding:1px;\"|←\n|style=\"text-align:right; width:82px; background-color:#F8F8F8; padding:1px;\"|+0.060\n|style=\"text-align:left; width:82px; background-color:#F8F8F8; padding:1px;\"|+0.030\n|style=\"text-align:center; width:31px; padding:1px;\"|→\n|style=\"text-align:right; width:82px; background-color:#F8F8F8; padding:1px;\"|+0.015\n|style=\"text-align:left; width:44px; padding:1px;\"|\n|-\n|style=\"text-align:center;\"|Step 8\n|style=\"text-align:right; width:50px; padding:1px;\"|\n|style=\"text-align:left; width:82px; padding:1px;\"|\n|style=\"text-align:center; width:31px; padding:1px;\"|\n|style=\"text-align:right; width:82px; background-color:#F8F8F8; padding:1px;\"|-0.008\n|style=\"text-align:left; width:82px; background-color:#F8F8F8; padding:1px;\"|-0.022\n|style=\"text-align:center; width:31px; padding:1px;\"|→\n|style=\"text-align:right; width:82px; background-color:#F8F8F8; padding:1px;\"|-0.011\n|style=\"text-align:left; width:82px; padding:1px;\"|\n|style=\"text-align:center; width:31px; padding:1px;\"|\n|style=\"text-align:right; width:82px; padding:1px;\"|\n|style=\"text-align:left; width:44px; padding:1px;\"|\n|-\n|style=\"text-align:center;\"|Step 9\n|style=\"text-align:right; width:50px; padding:1px;\"|\n|style=\"text-align:left; width:82px; padding:1px;\"|\n|style=\"text-align:center; width:31px; padding:1px;\"|\n|style=\"text-align:right; width:82px; padding:1px;\"|\n|style=\"text-align:left; width:82px; background-color:#F8F8F8; padding:1px;\"|+0.004\n|style=\"text-align:center; width:31px; padding:1px;\"|←\n|style=\"text-align:right; width:82px; background-color:#F8F8F8; padding:1px;\"|+0.007\n|style=\"text-align:left; width:82px; background-color:#F8F8F8; padding:1px;\"|+0.004\n|style=\"text-align:center; width:31px; padding:1px;\"|→\n|style=\"text-align:right; width:82px; background-color:#F8F8F8; padding:1px;\"|+0.002\n|style=\"text-align:left; width:44px; padding:1px;\"|\n|-\n|style=\"text-align:center;\"|Step 10\n|style=\"text-align:right; width:50px; padding:1px;\"|\n|style=\"text-align:left; width:82px; padding:1px;\"|\n|style=\"text-align:center; width:31px; padding:1px;\"|\n|style=\"text-align:right; width:82px; background-color:#F8F8F8; padding:1px;\"|-0.001\n|style=\"text-align:left; width:82px; background-color:#F8F8F8; padding:1px;\"|-0.003\n|style=\"text-align:center; width:31px; padding:1px;\"|\n|style=\"text-align:right; width:82px; padding:1px;\"|\n|style=\"text-align:left; width:82px; padding:1px;\"|\n|style=\"text-align:center; width:31px; padding:1px;\"|\n|style=\"text-align:right; width:82px; padding:1px;\"|\n|style=\"text-align:left; width:44px; padding:1px;\"|\n|- \n|style=\"background-color:#EEEEEE; font-weight:bold;\"|Sum of moments\n|style=\"text-align:right; width:50px; padding:1px;\"|\n|style=\"text-align:left; width:82px; padding:1px;\"|0\n|style=\"text-align:center; width:31px; padding:1px;\"|\n|style=\"text-align:right; width:82px; padding:1px;\"|+11.569\n|style=\"text-align:left; width:82px; padding:1px;\"|-11.569\n|style=\"text-align:center; width:31px; padding:1px;\"|\n|style=\"text-align:right; width:82px; padding:1px;\"|+10.186\n|style=\"text-align:left; width:82px; padding:1px;\"|-10.186\n|style=\"text-align:center; width:31px; padding:1px;\"|\n|style=\"text-align:right; width:82px; padding:1px;\"|+13.657\n|style=\"text-align:left; width:44px; padding:1px;\"|\n|-\n|}\n\nNumbers <span style=\"background-color:#F8F8F8; border-style:solid; border-width:1px; border-color:#AAAAAA;\">in grey</span> are balanced moments; arrows (<span style=\"border-style:solid; border-width:1px; border-color:#AAAAAA;\">&nbsp;→&nbsp;/&nbsp;←&nbsp;</span>) represent the carry-over of moment from one end to the other end of a member.* Step 1: As joint A is released, balancing moment of magnitude equal to the fixed end moment <math>M_{AB}^{f} = 14.700 \\mathrm{\\,kN \\,m}</math> develops and is carried-over from joint A to joint B.* Step 2: The unbalanced moment at joint B now is the summation of the fixed end moments <math>M_{BA}^{f}</math>, <math>M_{BC}^{f}</math> and the carry-over moment from joint A. This unbalanced moment is distributed to members BA and BC in accordance with the distribution factors <math>D_{BA} = 0.2727</math> and <math>D_{BC} = 0.7273</math>. Step 2 ends with carry-over of balanced moment <math>M_{BC}=3.867 \\mathrm{\\,kN \\,m}</math> to joint C. Joint A is a roller support which has no rotational restraint, so moment carryover from joint B to joint A is zero.* Step 3: The unbalanced moment at joint C now is the summation of the fixed end moments <math>M_{CB}^{f}</math>, <math>M_{CD}^{f}</math> and the carryover moment from joint B. As in the previous step, this unbalanced moment is distributed to each member and then carried over to joint D and back to joint B. Joint D is a fixed support and carried-over moments to this joint will not be distributed nor be carried over to joint C.* Step 4: Joint B still has balanced moment which was carried over from joint C in step 3. Joint B is released once again to induce moment distribution and to achieve equilibrium.* Steps 5 - 10: Joints are released and fixed again until every joint has unbalanced moments of size zero or neglectably small in required precision. Arithmetically summing all moments in each respective columns gives the final moment values.\n\n=== Result ===\n\n*Moments at joints determined by the moment distribution method\n:<math>M_A = 0 \\ kN \\cdot m </math>\n:<math>M_B = -11.569 \\ kN \\cdot m </math>\n:<math>M_C = -10.186 \\ kN \\cdot m </math>\n:<math>M_D = -13.657 \\ kN \\cdot m </math>\n:The conventional engineer's sign convention is used here, i.e. positive moments cause elongation at the bottom part of a beam member.\n\nFor comparison purposes, the following are the results generated using a [[matrix method]].  Note that in the analysis above, the iterative process was carried to >0.01 precision. The fact that the matrix analysis results and the moment distribution analysis results match to 0.001 precision is mere coincidence.\n*Moments at joints determined by the matrix method\n:<math>M_A = 0 \\ kN \\cdot m </math>\n:<math>M_B = -11.569 \\ kN \\cdot m </math>\n:<math>M_C = -10.186 \\ kN \\cdot m </math>\n:<math>M_D = -13.657 \\ kN \\cdot m </math>\n\nNote that the moment distribution method only determines the moments at the joints. Developing complete bending moment diagrams require additional calculations using the determined joint moments and internal section equilibrium.\n\n=== Result via displacements method ===\nAs the Hardy Cross method provides only approximate results, with a margin of error inversely proportionate to the number of iterations, it is important{{citation needed|date=September 2012}} to have an idea of how accurate this method might be. With this in mind, here is the result obtained by using an exact method: the [[displacement method]]\n\nFor this, the displacements method equation assumes the following form:\n\n<math>\\left[K\\right]\\left\\{d\\right\\} = \\left\\{-f\\right\\}</math>\n\nFor the structure described in this example, the stiffness matrix is as follows:\n\n<math>\\left[K\\right]=\\begin{bmatrix} 3\\frac{EI}{L} + 4\\frac{2EI}{L} & 2\\frac{2EI}{L} \\\\\n2\\frac{2EI}{L} & 4\\frac{2EI}{L} + 4\\frac{EI}{L} \\end{bmatrix}</math>\n\nThe equivalent nodal force vector:\n\n<math>\\left\\{f\\right\\}^T = \\left\\{-P\\frac{ab(L+a)}{2L^2}+q\\frac{L^2}{12} , -q\\frac{L^2}{12} + P\\frac{L}{8} \\right\\}\n</math>\n\nReplacing the values presented above in the equation and solving it for <math>\\left\\{d\\right\\}</math> leads to the following result:\n\n<math>\\left\\{d\\right\\}^T=\\left\\{ 6.9368 ; -5.7845\\right\\}</math>\n\nHence, the moments evaluated in node B are as follows:\n\n<math>M_{BA} = 3\\frac{EI}{L}d_1 - P\\frac{ab(L+a)}{2L^2} = -11.569</math>\n\n<math>M_{BC} = -4\\frac{2EI}{L}d_1 -2\\frac{2EI}{L}d_2 - q\\frac{L^2}{12} = -11.569</math>\n\nThe moments evaluated in node C are as follows:\n\n<math>M_{CB} = 2\\frac{2EI}{L}d_1 + 4\\frac{2EI}{L}d_2 - q\\frac{L^2}{12} = -10.186</math>\n\n<math>M_{CD} = -4\\frac{EI}{L}d_2 - P\\frac{L}{8} = -10.186</math>\n\n== See also ==\n* [[Finite element method]]\n* [[Slope deflection method]]\n\n== Notes ==\n{{reflist}}\n\n== References ==\n*{{cite book|last=Błaszkowiak|first=Stanisław|author2=Zbigniew Kączkowski|title=Iterative Methods in Structural Analysis|year=1966|publisher=Pergamon Press, Państwowe Wydawnictwo Naukowe}}\n*{{cite book|last=Norris|first=Charles Head|author2=John Benson Wilbur |author3=Senol Utku |title=Elementary Structural Analysis|edition=3rd|year=1976|publisher=McGraw-Hill|isbn=0-07-047256-4|pages=327–345}}\n*{{cite book |last1=McCormac|first1=Jack C.|first2=James K. Jr.|last2=Nelson|title=Structural Analysis: A Classical and Matrix Approach|edition=2nd |year=1997|publisher=Addison-Wesley|isbn=0-673-99753-7|pages=488–538}}\n*{{cite book|last=Yang|first=Chang-hyeon|title=Structural Analysis|url=http://www.cmgbook.co.kr/category/sub_detail.html?no=1017|edition=4th|date=2001-01-10|publisher=Cheong Moon Gak Publishers|language=Korean|location=Seoul|isbn=89-7088-709-1|pages=391–422}}\n*{{cite book|last=Volokh|first=K.Y.|title=On foundations of the Hardy Cross method|url=http://www.sciencedirect.com/science?_ob=ArticleURL&_udi=B6VJS-46DM66R-2&_user=32321&_coverDate=08%2F31%2F2002&_alid=721215136&_rdoc=3&_fmt=full&_orig=search&_cdi=6102&_sort=d&_docanchor=&view=c&_ct=7&_acct=C000004038&_version=1&_urlVersion=0&_userid=32321&md5=aabd85f9f5bb1c02b9e2f906f7f9dd19|archive-url=https://web.archive.org/web/20090902015045/http://www.sciencedirect.com/science?_ob=ArticleURL&_udi=B6VJS-46DM66R-2&_user=32321&_coverDate=08%2F31%2F2002&_alid=721215136&_rdoc=3&_fmt=full&_orig=search&_cdi=6102&_sort=d&_docanchor=&view=c&_ct=7&_acct=C000004038&_version=1&_urlVersion=0&_userid=32321&md5=aabd85f9f5bb1c02b9e2f906f7f9dd19|dead-url=yes|archive-date=2009-09-02|year=2002|publisher=International Journal of Solids and Structures, volume 39, issue 16, August 2002, Pages 4197-4200|doi=10.1016/S0020-7683(02)00345-1 }}\n\n==External links==\n* [http://www.freesoftware.com.my/Software/Stuctural_Engineering/Structural%20_Analysis/Moment_Distribution/momentdist.htm Free Moment Distribution Program in Visual Basic]\n* [https://civilengineer.webinfolist.com/mdcalc.htm Online Calculator for Moment Distribution Method]\n[[Category:Structural analysis]]"
    },
    {
      "title": "Moment-area theorem",
      "url": "https://en.wikipedia.org/wiki/Moment-area_theorem",
      "text": "The '''moment-area theorem''' is an engineering tool to derive the slope, rotation and deflection of beams and frames. This theorem was developed by [[Christian Otto Mohr|Mohr]] and later stated namely by [[Charles Ezra Greene]] in 1873. This method is advantageous when we solve problems involving beams, especially for those subjected to a series of concentrated loadings or having segments with different [[moments of inertia]]. If we draw the [[moment diagram]] for the beam and then divided it by the flexural rigidity(EI), the 'M/EI diagram' results by the following equation\n\n<math>\\theta=\\int\\left(\\frac{M}{EI}\\right)dx</math>\n\n==Theorem 1==\n\nThe change in slope between any two points on the elastic curve equals the area of the M/EI (moment) diagram between these two points.\n\n<math>\\theta_{A/B}={\\int_A}^B\\left(\\frac{M}{EI}\\right)dx</math>\n\nwhere,\n\n* M = moment\n* EI = flexural rigidity\n* <math>\\theta_{A/B}</math> = change in slope between points A and B\n* A, B = points on the elastic curve<ref>{{cite web|title=Hibbeler, R.C. (2012). Structural Analysis. Upper Saddle River, NJ: Pearson}}</ref>\n\n==Theorem 2==\n\nThe vertical deviation of a point A on an elastic curve with respect to the tangent which is extended from another point B equals the moment of the area under the M/EI diagram between those two points (A and B). This moment is computed about point A where the deviation from B to A is to be determined.\n\n<math>t_{A/B} = {\\int_A}^B \\frac{M}{EI} x \\;dx</math>\n\nwhere,\n\n* M = moment\n* EI = flexural rigidity\n* <math>t_{A/B}</math> = deviation of tangent at point A with respect to the tangent at point B\n* A, B = points on the elastic curve<ref>Hibbeler, R.C. (2012). Structural Analysis. Upper Saddle River, NJ: Pearson. pp.&nbsp;316–325.</ref>\n\n==Rule of Sign Convention==\n\nThe deviation at any point on the elastic curve is positive if the point lies above the tangent, negative if the point is below the tangent; we measured it from left tangent, if θ is counterclockwise direction, the change in slope is positive, negative if θ is clockwise direction.<ref>[http://www.mathalino.com/reviewer/mechanics-and-strength-of-materials/area-moment-method-beam-deflections Moment-Area Method Beam Deflection]</ref>\n\n==Procedure for Analysis==\n\nThe following procedure provides a method that may be used to determine the displacement and slope at a point on the elastic curve of a beam using the moment-area theorem.\n\n* Determine the reaction forces of a structure and draw the M/EI diagram of the structure.\n* If there are only concentrated loads on the structure, the problem will be easy to draw M/EI diagram which will results a series of triangular shapes.\n* If there are mixed with distributed loads and concentrated, the moment diagram (M/EI) will results parabolic curves, cubic and etc.\n* Then, assume and draw the deflection shape of the structure by looking at M/EI diagram.\n* Find the rotations, change of slopes and deflections of the structure by using the geometric mathematics.\n\n==References==\n{{reflist}}\n\n==External links==\n*[http://www.engr.mun.ca/~katna/5931/Deflections_Area-moment2p.pdf Area-Moment Method. (n.d.)]\n\n[[Category:Structural analysis]]"
    },
    {
      "title": "Müller-Breslau's principle",
      "url": "https://en.wikipedia.org/wiki/M%C3%BCller-Breslau%27s_principle",
      "text": "{{unreferenced|date=November 2012}}\n\nThe '''Müller-Breslau principle''' is a method to determine [[influence lines]]. The principle states that the influence lines of an action ([[force]] or [[Moment (physics)|moment]]) assumes the scaled form of the deflection displacement.\nOR,\nThis principle states that \"ordinate of ILD for a reactive force is given by ordinate of elastic curve if a unit deflection is applied in the direction of reactive force.\"\n\n== Example of using the Müller-Breslau principle to find qualitative influence lines ==\n[[File:Muller-Breslau Principle - Influence Lines.JPG|right]]\n\nPart (a) of the figure to the right shows a simply supported beam with a unit load traveling across it. The structure is [[Statically determinate#Statically determinate|statically determinate]]. Therefore, all influence lines will be straight lines.\n\nParts (b) and (c) of the figure shows the influence lines for the reactions in the y-direction. Releasing the vertical reaction for A allows the beam to rotate to Δ. Likewise for part (c). Δ is typically taken as positive upwards.\n\nPart (d) of the figure shows the influence line for shear at point B. Using the [[Shear and moment diagram#Normal convention|beam sign convention]] and cutting the beam at B, we can deduce the figure shown.\n\nPart (e) of the figure shows the influence line for the [[bending moment]] at point B. Again making a cut through the beam at point B and using the beam sign convention, we can deduce the figure shown.\n\nThe procedure for applying the Muller-Breslau principle is as follows:\n\n# Remove the constraint at the point of interest for the function of interest. This means if the influence line for a reaction is asked for simply start by pretending the beam is no longer attached to the reaction in question and is free to rotate about the other support. If the influence line for a moment is desired, pretend the point in question is a hinge and the subsequent two sides can rotate about their supports. If the influence line for shear is desired, again pretend the point in question is a shear release, again where both sides can rotate about their supports. \n# Consider the remaining portion of the beam to have infinite rigidity, so it is a straight line free to rotate about the support.\n#Lastly rotate whatever is free to rotate in its positive direction, but only enough to create a deflection of 1 unit total. This means if the moment IL is in question and an imaginary hinge is splitting the beam in two pieces, the two angles created between each rotated side and the original beam must add to equal 1. Similarly if the shear IL is in question the two sides will have opposite directions of rotation. So at the shear release the right side will typically be rotated upwards and the left side will be rotated downward, as this is the sign convention for shear. The total displacement between the two sides of the shear release must equal to 1.\n\n==See also==\n* [[Influence line]]\n* [[Beam (structure)|Beam]]\n* [[Shear and moment diagram|Shear and Moment Diagram]]\n* [[Dead and live loads|Dead and Live Loads]]\n\n{{DEFAULTSORT:Muller-Breslau Principle}}\n[[Category:Structural analysis]]\n\n\n{{Engineering-stub}}"
    },
    {
      "title": "Ply (layer)",
      "url": "https://en.wikipedia.org/wiki/Ply_%28layer%29",
      "text": "A '''ply''' is a [[wikt:layer|layer]] of material which has been combined with other layers in order to provide strength. The number of layers is indicated by prefixing a number, for example 4-ply, indicating material composed of 4 layers.\n\n==Etymology==\nThe word \"ply\" derives from the French verb ''plier'',<ref>Collins Dictionary of the English Language, 2nd Edition, London, 1986, p.1181</ref> \"to fold\", from the Latin verb ''plico'', from the ancient Greek verb ''πλέκω''.<ref>Cassell's Latin Dictionary, Marchant, J.R.V, & Charles, Joseph F., (Eds.), Revised Edition, 1928, p.421</ref>\n\n==Examples==\n*Yarn, where [[plying]] is a spinning technique to combine several fibres.\n*Vehicle [[tire]]s\n*[[Plywood]]\n*[[Toilet paper]]\n\n==References==\n<references/>\n\n\n\n[[Category:Structural analysis]]\n[[Category:Structural engineering]]"
    },
    {
      "title": "Prokon",
      "url": "https://en.wikipedia.org/wiki/Prokon",
      "text": "{{orphan|date=March 2015}}\n{{Infobox software\n| name                   = PROKON Structural Analysis and Design\n| logo                   = \n| screenshot             = \n| caption                = \n| author                 = \n| developer              = Prokon Software Consultant (Pty) Ltd.\n| released               = 1989\n| latest release version = 3.1\n| latest release date    = {{release date|2018|10|01}}\n| latest preview version = \n| latest preview date    = \n| operating system       = [[Windows]]<!--, [[Mac OS X]] & [[IOS (Apple)|iOS]], [[Android (operating system)|Android]]-->\n| language               = English\n| genre                  = [[Structural Analysis]] \n| license                = [[Proprietary software|Proprietary]]\n| website                = {{URL|https://prokon.com}}\n}}\n\n'''PROKON Structural Analysis and Design ''' is a [[Software suite|suite]] of  [[commercial software]] for [[Structural analysis|structural analysis and design]]. PROKON software is produced by South African company Prokon Software Consultant (Pty) Ltd. The company was founded by Karl Eschberger and Jacques Pienaar in 1989.<ref>{{cite web|title=Prokon company history|url=https://www.prokon.com/about/history|publisher=Product website}}</ref>\n\n==Name==\nThe Prokon company and PROKON software name is an acronym for the [[Afrikaans]] phrase \"PROgrammatuur KONsultante\" (programming consultants).\n\n==Overview==\nPROKON software comprises over 40 modules in the following categories:\n* Analysis: Three-dimensional frame and [[Finite element method|finite element analysis]].<ref>{{cite web|title=Sumo Structural Modeller tutorials|url=https://www.youtube.com/user/Prokonbuild/search?query=sumo|publisher=YouTube}}</ref> \n* Steel: [[Steel design|Structural steel member and connection design]].<ref>{{cite web|title=Example use of PROKON Base Plate Design module|url=https://www.structville.com/2018/03/thickness-design-of-column-base-plate.html|publisher=StrucVille}}</ref>\n* Concrete: [[Reinforced concrete]] and [[prestressed concrete]] member design.<ref>{{cite web|title=Prokon 2.6 tutorials|url=https://www.youtube.com/playlist?list=PLQTH_yz4jrGYULVv3HwazcQYK8Vm8K1JD|publisher=YouTube|accessdate=10 April 2015}}</ref>\n* Detailing: [[Rebar]] detailing and bending schedule generation.\n* Timber: [[Framing (construction)|Wood frame and truss]] design.\n* Masonry: [[Unreinforced masonry building|Unreinforced masonry]] member design.\n* General: Section properties calculation and other building analysis utilities.\n* Geotechnical: [[Slope stability analysis|Slope stability]] and [[bearing capacity]] analysis.<ref>{{cite web|title=Example use of PROKON Pile  Design module|url=https://www.structville.com/2018/05/design-of-pile-foundation-using-prokon.html|publisher=StrucVille}}</ref>\n* [[Building information modeling|Building Information Modelling (BIM)]]: Prodesk link between [[Autodesk Revit]] and PROKON modules.\n* Network analysis: Pronet [[EPANET]] engine for [[AutoCAD|AutoCAD Civil 3D]].\n\nThe software includes full support for British, European and South African design codes, with some modules also supporting North American, Australian, New Zealand, and selected Asian design codes.<ref>{{cite web|title=PROKON 3.1 Supported Design Codes|url=https://www.prokon.com/design/supported-design-codes|publisher=Product website}}</ref> PROKON appears on the Hong Kong Building Department list of pre-accepted computer programs for use in Hong Kong.<ref>{{cite web|title=HKBD Pre-Accepted Structural Programs|url=https://www.bd.gov.hk/doc/en/resources/codes-and-references/pre-accepted-programs/asp.pdf|publisher=Hong Kong Building Department}}</ref>\n\n==Workflow==\nPROKON uses a workflow that output data between analysis, design and detailing modules. It supports data exchange formats with third-party software: [[AutoCAD DXF|DXF]] and [[.dwg|DWG]] drawings, and CIS/2 CIMsteel<ref>{{cite web|title=CIS/2 interaction with Tekla Structures|url=https://teklastructures.support.tekla.com/2019/en/int_compatible_software|publisher=Tekla Structures Science and Technology Forum}}</ref> three-dimensional structural models. The Pronet module enables data exchange between [[Autodesk Revit]] and PROKON.\n\n==Use==\nPROKON is used widely in South Africa<ref>{{cite web|title=2008/09 NSTF awards|url=https://www.nstf.org.za/wp-content/uploads/2015/10/whoswho2008.pdf|page=14|publisher=National Science and Technology Forum}}</ref> and elsewhere in the world by structural engineers and academic researchers.<ref>{{cite journal |vauthors=Abobakr AA, Fathelrahman M|date=February 2015|title=Design Optimization of Reinforced Concrete Frames|url=http://dx.doi.org/10.4236/ojce.2015.51008|journal=Open Journal of Civil Engineering|volume=5|issue=1}}</ref><ref>{{cite journal |vauthors=Adil AM, Fathelrahman MA, Izeldein JI|date=February 2016|title=Investigation of Ultimate Resisting Moment of Reinforced Concrete Slabs Systems Using Yield Line Theory, BS 8110 and Computer Soft Wares|url=http://sustech.edu/staff_publications/2016022906564618.pdf|journal=International Journal of Research in Engineering and Advanced Technology|volume=4|issue=1}}</ref>\n\n==Similar Software==\n* [[STAAD|Staad.Pro]]\n* [[S-FRAME Software Inc.|S-FRAME]]\n* [[Computers and Structures|SAP 2000 and ETABS]]\n* [[List of structural engineering software]]\n\n==References==\n{{reflist}}\n\n\n\n[[Category:Structural analysis]]"
    },
    {
      "title": "Reinforced solid",
      "url": "https://en.wikipedia.org/wiki/Reinforced_solid",
      "text": "{{Orphan|date=June 2019}}\n\n[[Image:reinforced solids cube.jpg|thumb|Figure 1: Small cube of a material with reinforcing bars. The cube is cracked and the material above the crack is removed to show the reinforcement that crosses the crack.]]\n\nIn [[solid mechanics]], a '''reinforced solid''' is a [[brittle]] material that is reinforced by [[ductile]] bars or fibres. A common application is [[reinforced concrete]]. When the concrete cracks the tensile force in a crack is not carried any more by the concrete but by the steel reinforcing bars only. The reinforced concrete will continue to carry the load provided that sufficient reinforcement is present. A typical design problem is to find the smallest amount of reinforcement that can carry the [[Stress (mechanics)|stresses]] on a small cube (Fig. 1). This can be formulated as an [[Mathematical optimization|optimization]] problem.\n\n==Optimization problem==\n\nThe reinforcement is directed in the x, y and z direction. The reinforcement ratio is defined in a cross-section of a reinforcing bar as the reinforcement area <math>A_{r}</math> over the total area <math>A</math>, which is the brittle material area plus the reinforcement area.\n\n:<math>\\rho_{x}</math> = <math>A_{rx}</math> / <math>A_{x}</math>\n\n:<math>\\rho_{y}</math> = <math>A_{ry}</math> / <math>A_{y}</math>\n\n:<math>\\rho_{z}</math> = <math>A_{rz}</math> / <math>A_{z}</math>\n\nIn case of reinforced concrete the reinforcement ratios are usually between 0.1% and 2%. The [[yield stress]] of the reinforcement is denoted by <math>f_{y}</math>. The [[Stress (mechanics)|stress tensor]] of the brittle material is\n\n:<math>\n\\left[{\\begin{matrix}\n\\sigma _{xx} - \\rho_{x} f_{y} & \\sigma _{xy} & \\sigma _{xz} \\\\\n\\sigma _{xy} & \\sigma _{yy} - \\rho_{y} f_{y} & \\sigma _{yz} \\\\\n\\sigma _{xz} & \\sigma _{yz} & \\sigma _{zz} - \\rho_{z} f_{y} \\\\\n\\end{matrix}}\\right]\n</math>.\n\nThis can be interpreted as the stress tensor of the composite material minus the stresses carried by the reinforcement at yielding. This formulation is accurate for reinforcement ratio's smaller than 5%. It is assumed that the brittle material has no tensile strength. (In case of reinforced concrete this assumption is necessary because the concrete has small shrinkage cracks.) Therefore, the [[principal stresses]] of the brittle material need to be compression. The principal stresses of a stress tensor are its [[eigenvalues]].\n\nThe optimization problem is formulated as follows. Minimize <math>\\rho_{x}</math> + <math>\\rho_{y}</math> + <math>\\rho_{z}</math> subject to all eigenvalues of the brittle material stress tensor are less than or equal to zero ([[Positive-definite matrix|negative-semidefinite]]). Additional constraints are <math>\\rho_{x}</math> ≥ 0, <math>\\rho_{y}</math> ≥ 0, <math>\\rho_{z}</math> ≥ 0.\n\n==Solution==\n\nThe solution to this problem can be presented in a form most suitable for hand calculations.<ref name=\"A\"/><ref name=\"N\"/> It can be presented in graphical form.<ref name=\"F\"/> It can also be presented in a form most suitable for computer implementation.<ref name=\"H1\"/><ref name=\"H2\"/> In this article the latter method is shown.\n\nThere are 12 possible reinforcement solutions to this problem, which are shown in the table below. Every row contains a possible solution. The first column contains the number of a solution. The second column gives conditions for which a solution is valid. Columns 3, 4 and 5 give the formulas for calculating the reinforcement ratios.\n\n{| class=\"wikitable\"\n|-\n|   || Condition || <math>\\rho_{x}</math> <math>f_{y} </math> || <math>\\rho_{y}</math> <math>f_{y}</math>  || <math>\\rho_{z}</math> <math>f_{y}</math>\n|-\n| 1 || <math>I_{1}</math> ≤ 0, <math>I_{2}</math> ≥ 0, <math>I_{3}</math> ≤ 0 || 0 || 0 || 0\n|-\n| 2 || <math>\\sigma_{yy}\\sigma_{zz} - \\sigma^2_{yz}</math> > 0<br/><math>I_{1}(\\sigma_{yy}\\sigma_{zz} - \\sigma^2_{yz}) - I_{3}</math> ≤ 0<br/><math>I_{2}(\\sigma_{yy}\\sigma_{zz} - \\sigma^2_{yz}) - I_{3}(\\sigma_{yy}+\\sigma_{zz})</math> ≥ 0 || <math>\\frac{I_{3}}{\\sigma_{yy} \\sigma_{zz} - \\sigma^2_{yz}}</math> || 0 || 0\n|-\n| 3 || <math>\\sigma_{xx}\\sigma_{zz} - \\sigma^2_{xz}</math> > 0<br/><math>I_{1}(\\sigma_{xx}\\sigma_{zz} - \\sigma^2_{xz}) - I_{3}</math> ≤ 0<br/><math>I_{2}(\\sigma_{xx}\\sigma_{zz} - \\sigma^2_{xz}) - I_{3}(\\sigma_{xx}+\\sigma_{zz})</math> ≥ 0 || 0 || <math>\\frac{I_{3}}{\\sigma_{xx} \\sigma_{zz} - \\sigma^2_{xz}}</math> || 0\n|-\n| 4 || <math>\\sigma_{xx}\\sigma_{yy} - \\sigma^2_{xy}</math> > 0<br/><math>I_{1}(\\sigma_{xx}\\sigma_{yy} - \\sigma^2_{xy}) - I_{3}</math> ≤ 0<br/><math>I_{2}(\\sigma_{xx}\\sigma_{yy} - \\sigma^2_{xy}) - I_{3}(\\sigma_{xx}+\\sigma_{yy})</math> ≥ 0 || 0 || 0 || <math>\\frac{I_{3}}{\\sigma_{xx} \\sigma_{yy} - \\sigma^2_{xy}}</math>\n|-\n| 5 || <math>\\sigma_{xx}<0</math> || 0 || <math>\\sigma_{yy}- \\frac{\\sigma^2_{xy}}{\\sigma_{xx}} +|\\sigma_{yz}-\\frac{\\sigma_{xz}\\sigma_{xy}}{\\sigma_{xx}}|</math> || <math>\\sigma_{zz}-\\frac{\\sigma^2_{xz}}{\\sigma_{xx}}+|\\sigma_{yz}-\\frac{\\sigma_{xz}\\sigma_{xy}}{\\sigma_{xx}}|</math>\n|-\n| 6 || <math>\\sigma_{yy}<0</math> || <math>\\sigma_{xx}-\\frac{\\sigma^2_{xy}}{\\sigma_{yy}} +|\\sigma_{xz}-\\frac{\\sigma_{yz}\\sigma_{xy}}{\\sigma_{yy}}|</math> || 0 || <math>\\sigma_{zz}-\\frac{\\sigma^2_{yz}}{\\sigma_{yy}} +|\\sigma_{xz}-\\frac{\\sigma_{yz}\\sigma_{xy}}{\\sigma_{yy}}|</math>\n|-\n| 7 || <math>\\sigma_{zz}<0</math> || <math>\\sigma_{xx}-\\frac{\\sigma^2_{xz}}{\\sigma_{zz}} +|\\sigma_{xy}-\\frac{\\sigma_{yz}\\sigma_{xz}}{\\sigma_{zz}}|</math> || <math>\\sigma_{yy} -\\frac{\\sigma^2_{yz}}{\\sigma_{zz}} +|\\sigma_{xy} -\\frac{\\sigma_{xz}\\sigma_{yz}}{\\sigma_{zz}}|</math> || 0\n|-\n| 8 || <math>\\sigma_{yz} + \\sigma_{xz} + \\sigma_{xy}</math> ≥ 0<br/><math>\\sigma_{xz}\\sigma_{xy} + \\sigma_{yz}\\sigma_{xy} + \\sigma_{yz}\\sigma_{xz}</math> ≥ 0<br/> || <math>\\sigma_{xx} + \\sigma_{xz} + \\sigma_{xy}</math> || <math>\\sigma_{yy} + \\sigma_{yz} + \\sigma_{xy}</math> || <math>\\sigma_{zz} + \\sigma_{yz} + \\sigma_{xz}</math>\n|-\n| 9 || <math>- \\sigma_{yz} - \\sigma_{xz} + \\sigma_{xy}</math> ≥ 0<br/><math>- \\sigma_{xz}\\sigma_{xy} - \\sigma_{yz}\\sigma_{xy} + \\sigma_{yz}\\sigma_{xz}</math> ≥ 0<br/> || <math>\\sigma_{xx} - \\sigma_{xz} + \\sigma_{xy}</math> || <math>\\sigma_{yy} - \\sigma_{yz} + \\sigma_{xy}</math> || <math>\\sigma_{zz} - \\sigma_{yz} - \\sigma_{xz}</math>\n|-\n| 10 || <math>\\sigma_{yz} - \\sigma_{xz} - \\sigma_{xy}</math> ≥ 0<br/><math>\\sigma_{xz}\\sigma_{xy} - \\sigma_{yz}\\sigma_{xy} - \\sigma_{yz}\\sigma_{xz}</math> ≥ 0<br/> || <math>\\sigma_{xx} - \\sigma_{xz} - \\sigma_{xy}</math> || <math>\\sigma_{yy} + \\sigma_{yz} - \\sigma_{xy}</math> || <math>\\sigma_{zz} + \\sigma_{yz} - \\sigma_{xz}</math>\n|-\n| 11 || <math>- \\sigma_{yz} + \\sigma_{xz} - \\sigma_{xy}</math> ≥ 0<br/><math>- \\sigma_{xz}\\sigma_{xy} + \\sigma_{yz}\\sigma_{xy} - \\sigma_{yz}\\sigma_{xz}</math> ≥ 0<br/> || <math>\\sigma_{xx} + \\sigma_{xz} - \\sigma_{xy}</math> || <math>\\sigma_{yy} - \\sigma_{yz} - \\sigma_{xy}</math> || <math>\\sigma_{zz} - \\sigma_{yz} + \\sigma_{xz}</math>\n|-\n| 12 || <math>\\sigma_{xy}\\sigma_{xz}\\sigma_{yz}<0</math> || <math>\\sigma_{xx} - \\frac{\\sigma_{xz}\\sigma_{xy}}{\\sigma_{yz}}</math> || <math>\\sigma_{yy} - \\frac{\\sigma_{yz}\\sigma_{xy}}{\\sigma_{xz}}</math> || <math>\\sigma_{zz} - \\frac{\\sigma_{yz}\\sigma_{xz}}{\\sigma_{xy}}</math>\n|-\n|}\n\n<math>I_{1}</math>, <math>I_{2}</math> and <math>I_{3}</math> are the [[Stress (mechanics)|stress invariants]] of the composite material stress tensor.\n\nThe algorithm for obtaining the right solution is simple. Compute the reinforcement ratios of each possible solution that fulfills the conditions. Further ignore solutions with a reinforcement ratio less than zero. Compute the values of <math>\\rho_{x}</math> + <math>\\rho_{y}</math> + <math>\\rho_{z}</math> and select the solution for which this value is smallest. The principal stresses in the brittle material can be computed as the eigenvalues of the brittle material stress tensor, for example by [[Jacobi method|Jacobi's method]].\n\nThe formulas can be simply checked by substituting the reinforcement ratios in the brittle material stress tensor and calculating the invariants. The first invariant needs to be less than or equal to zero. The second invariant needs to be greater than or equal to zero. These provide the conditions in column 2. For solution 2 to 12, the third invariant needs to be zero.<ref name=\"F\"/>\n\n==Examples==\n\nThe table below shows computed reinforcement ratios for 10 stress tensors. The applied reinforcement yield stress is <math>f_{y}</math> = 500 N/mm². The [[Density|mass density]] of the reinforcing bars is 7800&nbsp;kg/m<sup>3</sup>. In the table <math>\\sigma_{m}</math> is the computed brittle material stress. <math>m_{r}</math> is the optimised amount of reinforcement.\n\n{| class=\"wikitable\"\n|- style=\"height: 30px;\"\n| width=\"50pt\" | || <math>\\sigma_{xx}</math> || <math>\\sigma_{yy}</math> || <math>\\sigma_{zz}</math> || <math>\\sigma_{yz}</math> || <math>\\sigma_{xz}</math> || <math>\\sigma_{xy}</math> || || <math>\\rho_{x}</math> || <math>\\rho_{y}</math> || <math>\\rho_{z}</math> || <math>\\sigma_{m}</math> || <math>m_{r}</math>\n|-\n| 1 || 1 N/mm²|| 2 N/mm²|| 3 N/mm²|| -4 N/mm²|| 3 N/mm²|| -1 N/mm²|| || 1.00%|| 1.40%|| 2.00%|| -10.65 N/mm² || 343&nbsp;kg/m<sup>3</sup>\n|-\n| 2 || -5 || 2 || 3 || 4 || 3 || 1 || || 0.00 || 1.36 || 1.88 || -10.31 || 253\n|-\n| 3 || -5 || -6 || 3 || 4 || 3 || 1 || || 0.00 || 0.00 || 1.69 || -10.15 || 132\n|-\n| 4 || -5 || -6 || -6 || 4 || 3 || 1 || || 0.00 || 0.00 || 0.00 || -10.44 || 0\n|-\n| 5 || 1 || 2 || 3 || -4 || -3 || -1 || || 0.60 || 1.00 || 2.00 || -10.58 || 281\n|-\n| 6 || 1 || -2 || 3 || -4 || 3 || 2 || || 0.50 || 0.13 || 1.80 || -10.17 || 190\n|-\n| 7 || 1 || 2 || 3 || 4 || 2 || -1 || || 0.40 || 1.00 || 1.80 || -9.36 || 250\n|-\n| 8 || 2 || -2 || 5 || 2 || -4 || 6 || || 2.40 || 0.40 || 1.40 || -15.21 || 328\n|-\n| 9 || -3 || -7 || 0 || 2 || -4 || 6 || || 0.89 || 0.00 || 0.57 || -14.76 || 114\n|-\n| 10 || 3 || 0 || 10 || 0 || 5 || 0 || || 1.60 || 0.00 || 3.00 || -10.00 || 359\n|-\n|}\n\n==Extension==\n\nThe above solution can be very useful to design reinforcement; however, it has some practical limitations. The following aspects can be included too if the problem is solved using [[convex optimization]]:\n*Multiple stress tensors in one point due to multiple loads on the structure instead of only one stress tensor \n*A constraint imposed to crack widths at the surface of the structure \n*Shear stress in the crack (aggregate interlock) \n*Reinforcement in other directions than x, y and z \n*Reinforcing bars that already have been placed in the reinforcement design process \n*The whole structure instead of one small material cube in turn \n*Large reinforcement ratio's\n*Compression reinforcement\n\n==Bars in any direction==\n\nReinforcing bars can have other directions than the x, y and z direction. In case of bars in one direction the stress tensor of the brittle material is computed by\n\n<math>\n\\left[{\\begin{matrix}\n\\sigma _{xx} & \\sigma _{xy} & \\sigma _{xz} \\\\\n\\sigma _{xy} & \\sigma _{yy} & \\sigma _{yz} \\\\\n\\sigma _{xz} & \\sigma _{yz} & \\sigma _{zz} \\\\\n\\end{matrix}}\\right]\n\n- \\rho f_{y}\n\n\\left[{\\begin{matrix}\n\\cos^2(\\alpha) & \\cos(\\alpha)\\cos(\\beta) & \\cos(\\alpha)\\cos(\\gamma) \\\\\n\\cos(\\beta)\\cos(\\alpha) & \\cos^2(\\beta)  & \\cos(\\beta)\\cos(\\gamma) \\\\\n\\cos(\\gamma)\\cos(\\alpha) & \\cos(\\gamma)\\cos(\\beta) & \\cos^2(\\gamma) \\\\\n\\end{matrix}}\\right]\n</math>\n\nwhere <math>\\alpha, \\beta, \\gamma</math> are the angles of the bars with the x, y and z axis. Bars in other directions can be added in the same way.\n\n==Utilization==\n\nOften, builders of reinforced concrete structures know, from experience, where to put reinforcing bars. Computer tools can support this by checking whether proposed reinforcement is sufficient. To this end the tension criterion,\n\nThe [[eigenvalues]] of <math>\n\\left[{\\begin{matrix}\n\\sigma _{xx} - \\rho_{x} f_{y} & \\sigma _{xy} & \\sigma _{xz} \\\\\n\\sigma _{xy} & \\sigma _{yy} - \\rho_{y} f_{y} & \\sigma _{yz} \\\\\n\\sigma _{xz} & \\sigma _{yz} & \\sigma _{zz} - \\rho_{z} f_{y} \\\\\n\\end{matrix}}\\right]\n</math> shall be less than or equal to zero.\n\nis rewritten into,\n\nThe eigenvalues of <math>\n\\left[{\\begin{matrix}\n\\frac{\\sigma _{xx}}{\\rho _{x} f _{y}}           & \\frac{\\sigma _{xy}}{\\sqrt{\\rho _{x} \\rho _{y}} f _{y}} & \\frac{\\sigma _{xz}}{\\sqrt{\\rho _{x} \\rho _{z}} f _{y}} \\\\\n\\frac{\\sigma _{xy}}{\\sqrt{\\rho _{x} \\rho _{y}} f _{y}} & \\frac{\\sigma _{yy}}{\\rho _{y} f _{y}}           & \\frac{\\sigma _{yz}}{\\sqrt{\\rho _{y} \\rho _{z}} f _{y}} \\\\\n\\frac{\\sigma _{xz}}{\\sqrt{\\rho _{x} \\rho _{z}} f _{y}} & \\frac{\\sigma _{yz}}{\\sqrt{\\rho _{y} \\rho _{z}} f _{y}} & \\frac{\\sigma _{zz}}{\\rho _{z} f _{y}} \\\\\n\\end{matrix}}\\right]\n</math> shall be less than or equal to one.\n\nThe latter matrix is the utilization tensor. The largest eigenvalue of this tensor is the utilization (unity check), which can be displayed in a [[Contour line|contour plot]] of a structure for all [[Structural load|load combinations]] related to the [[Limit state design|ultimate limit state]].\n\nFor example, the stress at some location in a structure is <math>\\sigma_{xx}</math> = 4 N/mm², <math>\\sigma_{yy}</math> = -10 N/mm², <math>\\sigma_{zz}</math> = 3 N/mm², <math>\\sigma_{yz}</math> = 3 N/mm², <math>\\sigma_{xz}</math> = -7 N/mm², <math>\\sigma_{xy}</math> = 1 N/mm². The reinforcement yield stress is <math>f_{y}</math> = 500 N/mm². The proposed reinforcement is <math>\\rho_{x}</math> = 1.4%, <math>\\rho_{y}</math> = 0.1%, <math>\\rho_{z}</math> = 1.9%. The eigenvalues of the utilization tensor are -20.11, -0.33 and 1.32. The utilization is 1.32. This shows that the bars are overloaded and 32% more reinforcement is required.\n\nCombined compression and shear failure of the concrete can be checked with the [[Mohr-Coulomb theory|Mohr-Coulomb criterion]] applied to the eigenvalues of the stress tensor of the brittle material.\n\n<math>\\frac{\\sigma_{1}}{f_{t}} + \\frac{\\sigma_{3}}{f_{c}} </math> ≤ 1, \n\nwhere <math>\\sigma_{1}</math> is the largest principal stress, <math>\\sigma_{3}</math> is the smallest principal stress, <math>f_{c}</math> is the uniaxial compressive strength (negative value) and <math>f_{t}</math> is a fictitious tensile strength based on compression and shear experiments.\n\nCracks in the concrete can be checked by replacing the yield stress <math>f _{y}</math> in the utilization tensor by the bar stress at which the maximum crack width occurs. (This bar stress depends also on the bar diameter, the bar spacing and the [[Concrete cover|bar cover]].) Clearly, crack widths need checking at the surface of a structure for stress states due to [[Structural load|load combinations]] related to the [[Limit state design|serviceability limit state]] only.\n\n==See also==\n*[[Reinforced concrete]]\n*[[Solid mechanics]]\n*[[Structural engineering]]\n\n==References==\n\n<references>\n<ref name=\"A\">Andreasen B.S., Nielsen M.P., Armiering af beton I det tredimesionale tilfælde, Bygningsstatiske meddelelser, Vol. 5 (1985), No. 2-3, pp. 25-79 (in Danish).</ref>\n<ref name=\"F\">Foster S.J., Marti P., Mojsilovic N., Design of Reinforced Concrete Solids Using Stress Analysis, ACI Structural Journal, Nov.-Dec. 2003, pp. 758-764.</ref>\n<ref name=\"H1\">Hoogenboom P.C.J., De Boer A., \"Computation of reinforcement for solid concrete\", Heron, Vol. 53 (2008), No. 4. pp. 247-271.</ref>\n<ref name=\"H2\">Hoogenboom P.C.J., De Boer A., \"Computation of optimal concrete reinforcement in three dimensions\", Proceedings of EURO-C 2010, Computational Modelling of Concrete Structures, pp. 639-646, Editors Bicanic et al. Publisher CRC Press, London.</ref>\n<ref name=\"N\">Nielsen M.P., Hoang L.C., Limit Analysis and Concrete Plasticity, third edition, CRC Press, 2011.</ref>\n</references>\n\n[[Category:Composite materials]]\n[[Category:Plasticity (physics)]]\n[[Category:Reinforced concrete]]\n[[Category:Structural analysis]]"
    },
    {
      "title": "ROHR2",
      "url": "https://en.wikipedia.org/wiki/ROHR2",
      "text": "{{Infobox software\n| name = ROHR2\n| developer = SIGMA Ingenieurgesellschaft mbH\n| latest release version = 32.1\n| programming language = [[C++]],  [[C Sharp (programming language)|C#]], [[Fortran]],\n| operating system = [[Windows]]\n| language = English, German, French\n| website = {{URL|https://www.rohr2.com}}\n}}\n\n'''ROHR2''' is a [[pipe stress analysis]] [[Computer-aided engineering|CAE]] system from SIGMA Ingenieurgesellschaft mbH, based in [[Unna]], Germany. The software performs both static and dynamic analysis of complex piping and skeletal structures, and runs on Microsoft Windows platform.<ref>{{cite web|title=ROHR2: Static And Dynamic Analysis Of Complex Piping Structures|url=http://www.oilandgasonline.com/doc.mvc/ROHR2-Static-And-Dynamic-Analysis-Of-0001|work=Oil & Gas journal|accessdate=21 August 2012}}</ref><ref>{{cite web|title=ROHR2 Update 30.3b - Pipe Stress Analysis|url=http://www.cimdata.com/newsletter/2009/10/04/10.04.13.htm|work=CIMData|accessdate=21 August 2012}}</ref>\n\nROHR2 software comes with built in industry standard stress codes; such as ASME B31.1, B31.3, B31.4, B31.5, B31.8, EN 13480, CODETI; along with several GRP pipe codes; as well as nuclear stress codes such as ASME Cl. 1-3, KTA 3201.2, KTA 3211.2.\n\n==Name==\nThe brand name comes from the German word \"Rohr\" (pronounced as “ROAR“) which means \"Pipe“.\n\n==History==\n\n===Early years as a MBP product : 1960's to 1989===\nROHR2 was created in the late 1960s by the one of the first software companies in Germany, [[:de:Mathematischer Beratungs- und Programmierungsdienst|Mathematischer Beratungs- und Programmierungsdienst]] (MBP), based in [[Dortmund]]. ROHR2 first ran on mainframes such as [[UNIVAC 1]], [[CRAY]], and later [[Prime computer]]. At the time, the program was [[Command-line interface|command line]] driven with a [[:wikt:proprietary|proprietary]] [[programming language]] to describe the piping systems and define the various load conditions. The 1987 launched version 26, was released for [[IBM PC]] as well as [[IBM PC compatible]] systems.<ref>{{cite news |title=Rohrstatik Software|work= Chemie Ingenieur Technik|doi= 10.1002/cite.330590102}}</ref>\n\n===As a EDS / SIGMA  product : 1989 to 2000 ===\nMBP was later taken over by [[Electronic Data Systems|EDS]] (then a part of [[General Motors]] Corp., now part of [[HP Enterprise Services]]). In 1989, SIGMA Ingenieurgesellschaft mbH was founded in Dortmund, and the ROHR2 development and support team moved to the new office premises of SIGMA. The [[graphical user interface]] was added in 1994 to the product, which allowed the editing of piping systems without the need of mastering the earlier required programming language.\n\n===Sigma Ingenieurgesellschaft mbH product : 2000 to present ===\nFrom the year 2000 onwards, the complete licensing and sales activities came under the management of [[SIGMA Ingenieurgesellschaft mbH]]; which by then evolved into an engineering company specializing in pipe engineering, as well as a software development firm.\n\nThe recent developments include new bi-directional interfaces based on open standards for transfer of data with other CAD/CAE products such as - [[AVEVA]] [[Plant Design Management System|PDMS]],<ref>{{cite web|title=AVEVA Releases Pipe Stress Interface for SIGMA ROHR2|url=http://www.tenlinks.com/news/PR/aveva/082112_sigma.htm|accessdate=21 August 2012|deadurl=yes|archiveurl=https://web.archive.org/web/20121119023925/http://www.tenlinks.com/news/PR/aveva/082112_sigma.htm|archivedate=19 November 2012|df=}}</ref> CADISON, [[Intergraph]]'s  [[Plant Design System|PDS]], Intergraph's SmartPlant, [[:de:HICAD|HICAD]], [[MPDS4]], [[Bentley Systems|Bentley System]]'s AutoPLANT, [[Autodesk]]'s PLANT3D and other PCF supported software. The integration of ROHR2 into the users workflow is supported by third-party interface products to ensure [[interoperability]] - a norm in the present engineering software industry.<ref>{{cite web|title=Enabling Interoperability Via Software Architecture|url=http://www.dtic.mil/cgi-bin/GetTRDoc?AD=ADA458021|work=dtic.mil|accessdate=21 August 2012}}</ref><ref>{{cite web|title=Integrated design and calculation process at WINGAS|url=http://www.itandfactory.de/EE_2010-06%20S44%20%20Integrated%20design%20and%20calculation%20process%20at%20WINGAS%20@WINGAS%20(LR).pdf|website=|accessdate=21 August 2012}}</ref>\n\n==Software environment==\nThe ROHR2 program system comes with the following software environments; consisting of '''ROHR2win''' - the graphical user interface, the '''ROHR2''' - calculation core, and various additional programs (see : [[ROHR2 (analysis software)#Related products|related products]]).\n\n==Calculation basics==\nThe static analysis includes the calculation of [[static load]]s of any value, or combination in accordance with the theories of first - and second order for linear and non-linear boundary conditions (friction, support lift).\n\nAdditional load conditions can also be applied, such as [[dynamic load]]s or [[Simple harmonic motion|harmonic excitation]]. Furthermore, the dynamic analysis include the calculation of [[Eigenvalues#Vibration analysis|eigenvalues]] and [[mode shape]]s as well as their processing in various [[Structural dynamics#Modal response|modal response]] methods - for the analysis of, for example, [[earthquake]]s and [[fluid hammer]].\n\nA non-linear time history module (ROHR2stoss) allows the analysis of dynamic events in the time domain, while taking into account non-linear components<ref name=\"Dynamic Pipe Stress Analysis with Nonlinearities \">Article Dynamic Pipe Stress Analysis with Nonlinearities in 3R International, 06/2004, p. 342 f (German title: Praktische Ermittlung von Kennwerten nichtlinearer Rohrleitungselemente in dynamische Strukturberechnungen in 3R International, 06/2004, Seite 342 ff)</ref> such as [[Mechanical snubber|snubbers]] or visco dampers based on the [[Viscoelasticity#Maxwell model|Maxwell model]].<ref>{{cite news |title=Seismic Engineering Knowledge Transfer Seminar|author=Berkovsky, Alexey | url=http://www.oecd-nea.org/nsd/csni/iage/workshops/rez-2011/documents/Seismic%20Design%20and%20Response%20of%20NPP%20Piping.pdf }}</ref>\nAn efficient superposition module enables a manifold selection and combination of static and dynamic results.\n\n==Related products==\n*'''ROHR2fesu''' - Finite Element Analysis of Substructures in ROHR2\n*'''ROHR2iso''' - Creation of isometric drawings in ROHR2\n*'''ROHR2stoss''' - Structural Analysis with Dynamic Loads using Direct Integration\n*'''ROHR2nozzle''' - Analysis of nozzles in piping systems according to API 610, 617, 661, NEMA SM23, DIN EN ISO 5199, 9905, 10437 and others\n*'''ROHR2press''' - Internal pressure analysis of piping components\n*'''SINETZ''' - Steady State Calculation of Flow Distribution, Pressure Drop and Heat Loss in Branched and Intermeshed Piping Networks for compressible and incompressible media\n*'''SINETZfluid''' - Calculation of Flow Distribution and Pressure Drop of incompressible Media in Branched and Intermeshed Piping Networks\n*'''PROBAD''' - Code-based strength calculations of pressure parts.\n\n==See also==\n*[[Piping#Stress analysis|Pipe Stress analysis]]\n\n== References ==\n{{Reflist|2|refs=}}\n\n== External links ==\n* [http://www.rohr2.com ROHR2 Homepage English]\n\n[[Category:Structural analysis]]\n[[Category:Computer-aided design]]\n[[Category:Computer-aided design software for Windows]]"
    },
    {
      "title": "Second moment of area",
      "url": "https://en.wikipedia.org/wiki/Second_moment_of_area",
      "text": "{{Use American English|date = March 2019}}\n{{Short description|Mathematical construct in engineering}}\n{{About|the geometrical property of an area, termed the second moment of area|the moment of inertia dealing with the rotation of an object with mass|Mass moment of inertia}}\n{{For|a list of equations for second moments of area of standard shapes|List of second moments of area}}\n\nThe '''2nd moment of area''', also known as the '''area moment of inertia''', or '''second area moment''', is a geometrical property of an area which reflects how its points are distributed with regard to an arbitrary axis. The second moment of area is typically denoted with either an <math>I</math> for an axis that lies in the plane or with a <math>J</math> for an axis perpendicular to the plane. In both cases, it is calculated with a [[multiple integral]] over the object in question. Its dimension is L (length) to the fourth power.   Its [[physical unit|unit]] of dimension when working with the [[International System of Units]] is meters to the fourth power, [[metre|m]]<sup>4</sup>, or inches to the fourth power, [[inch|in]]<sup>4</sup>, when working in the [[Imperial units|Imperial System of Units]].\n\nIn [[structural engineering]], the second moment of area of a [[beam (structure)|beam]] is an important property used in the calculation of the beam's [[deflection (engineering)|deflection]] and the calculation of stress caused by a moment applied to the beam. In order to maximize the second moment of area, a large fraction of the cross-sectional area of an I-beam is located at the maximum possible distance from the centroid of the I-beam's cross-section. The '''planar''' second moment of area provides insight into a beam's resistance to bending due to an applied moment, force, or distributed load perpendicular to its neutral axis, as a function of its shape. The '''polar''' second moment of area provides insight into a beam's resistance to torsional deflection, due to an applied moment parallel to its cross-section, as a function of its shape.\n\n: '''''Note:''' Different disciplines use the term ''moment of inertia'' (''MOI'') to refer to different moments. It may refer to either of the '''planar''' second moments of area (often <math> I_x = \\textstyle\\iint_{R} y^2\\, \\mathrm{d}A \\text{ or } I_y = \\textstyle\\iint_{R} x^2\\, \\mathrm{d}A</math>, with respect to some reference plane), or the '''polar''' second moment of area (<math> I = \\textstyle\\iint_{R} r^2\\, \\mathrm{d}A </math>, where r is the distance to some reference axis).  In each case the integral is over all the infinitesimal elements of ''area'', dA, in some two-dimensional cross-section.  In physics, ''moment of inertia'' is strictly the second moment of '''mass''' with respect to distance from an axis: <math> I = \\textstyle\\int_{Q} r^2 \\mathrm{d}m </math>, where r is the distance to some potential rotation axis, and the integral is over all the infinitesimal elements of ''mass'', dm, in a three-dimensional space occupied by an object&nbsp;{{mvar|Q}}.  The MOI, in this sense, is the analog of mass for rotational problems. In engineering (especially mechanical and civil), ''moment of inertia'' commonly refers to the second moment of the area.<ref>{{cite book|last1=Beer|first1=Ferdinand P.|title=Vector Mechanics for Engineers|date=2013|place=New York|publisher=McGraw-Hill|isbn=978-0-07-339813-6|edition=10th|page=471|quote=The term second moment is more proper than the term moment of inertia, since, logically, the latter should be used only to denote integrals of mass (see Sec. 9.11). In engineering practice, however, moment of inertia is used in connection with areas as well as masses.}}</ref>''\n\n==Definition==\n[[File:Moment of area of an arbitrary shape.svg|thumb|175px|An arbitrary shape.  ''ρ'' is the radial distance to the element d''A'', with projections ''x'' and ''y'' on the axes.]]\n\nThe second moment of area for an arbitrary shape&nbsp;{{mvar|R}} with respect to an arbitrary axis <math>BB'</math> is defined as\n\n: <math>J_{BB'} = \\iint\\limits_{R} {\\rho}^2 \\, \\mathrm{d}A</math>\n\nwhere\n\n: <math>\\mathrm{d}A</math> is the differential area of the arbitrary shape, and\n: <math>\\rho</math> is the distance from the axis <math>BB'</math> to <math>\\mathrm{d}A</math>.<ref>{{cite book|last1=Pilkey|first1=Walter D.|title=Analysis and Design of Elastic Beams|page=15|date=2002|publisher=John Wiley & Sons, Inc|isbn=978-0-471-38152-5}}</ref>\n\nFor example, when the desired reference axis is the x-axis, the second moment of area <math>I_{xx}</math> (often denoted as <math>I_x</math>) can be computed in [[Cartesian coordinates]] as\n\n: <math>I_{x} = \\iint\\limits_{R} y^2\\, \\mathrm{d}x\\, \\mathrm{d}y</math>\n\nThe second moment of the area is crucial in [[Euler–Bernoulli beam equation|Euler–Bernoulli theory]] of slender beams.\n\n=== Product moment of area ===\nMore generally, the '''product moment of area''' is defined as<ref>{{cite book|last1=Beer|first1=Ferdinand P.|title=Vector Mechanics for Engineers|date=2013|place=New York|publisher=McGraw-Hill|isbn=978-0-07-339813-6|edition=10th|page=495|chapter=Chapter 9.8: Product of inertia}}</ref>\n\n: <math>I_{xy} = \\iint\\limits_{R} yx\\, \\mathrm{d}x\\, \\mathrm{d}y</math>\n\n== Parallel axis theorem ==\n[[File:Parallel axis theorem.svg|thumb|175px|A shape with centroidal axis ''x''. The parallel axis theorem can be used to obtain the second moment of area with respect to the ''x''' axis.]]\n{{Main article|Parallel axis theorem}}\n\nIt is sometimes necessary to calculate the second moment of area of a shape with respect to an <math>x'</math> axis different to the centroidal axis of the shape. However, it is often easier to derive the second moment of area with respect to its centroidal axis, <math>x</math>, and use the parallel axis theorem to derive the second moment of area with respect to the <math>x'</math> axis. The parallel axis theorem states\n\n: <math>I_{x'} = I_x + A d^2</math>\n\nwhere\n\n: <math>A</math> is the area of the shape, and\n: <math>d</math> is the perpendicular distance between the <math>x</math> and <math>x'</math> axes.<ref>Hibbeler, R. C. (2004). Statics and Mechanics of Materials (Second ed.). Pearson Prentice Hall. {{ISBN|0-13-028127-1}}.</ref><ref>{{cite book|last1=Beer|first1=Ferdinand P.|title=Vector Mechanics for Engineers|date=2013|place=New York|publisher=McGraw-Hill|isbn=978-0-07-339813-6|edition=10th|page=481|chapter=Chapter 9.6: Parallel-axis theorem}}</ref>\n\nA similar statement can be made about a <math>y'</math> axis and the parallel centroidal <math>y</math> axis. Or, in general, any centroidal <math>B</math> axis and a parallel <math>B'</math> axis.\n\n== Perpendicular axis theorem ==\n{{Main article|Perpendicular axis theorem}}\nFor the simplicity of calculation, it is often desired to define the polar moment of area  (with respect to a perpendicular axis) in terms of two area moments of inertia (both with respect to in-plane axes). The simplest case relates <math>J_z</math> to <math>I_x</math> and <math>I_y</math>.\n\n: <math>J_z = \\iint\\limits_{R} \\rho^2\\,\\mathrm{d}A = \\iint\\limits_{R} \\left(x^2 + y^2\\right)\\,\\mathrm{d}A = \\iint\\limits_{R} x^2\\,\\mathrm{d}A + \\iint\\limits_{R} y^2\\,\\mathrm{d}A = I_x + I_y</math>\n\nThis relationship relies on the [[Pythagorean theorem]] which relates <math>x</math> and <math>y</math> to <math>\\rho</math> and on the [[linearity of integration]].\n\n== Composite shapes ==\nFor more complex areas, it is often easier to divide the area into a series of \"simpler\" shapes. The second moment of area for the entire shape is the sum of the second moment of areas of all of its parts about a common axis. This can include shapes that are \"missing\" (i.e. holes, hollow shapes, etc.), in which case the second moment of area of the \"missing\" areas are subtracted, rather than added. In other words, the second moment of area of \"missing\" parts are considered negative for the method of composite shapes.\n\n==Examples==\nSee [[list of second moments of area]] for other shapes.\n\n===Rectangle with centroid at the origin===\n[[File:Moment of area of a rectangle through the centroid.svg|thumb|175px|Rectangle with base ''b'' and height ''h'']]\nConsider a rectangle with base <math>b</math> and height <math>h</math> whose [[centroid]] is located at the origin. <math>I_x</math> represents the second moment of area with respect to the x-axis; <math>I_y</math> represents the second moment of area with respect to the y-axis; <math>J_z</math> represents the polar moment of inertia with respect to the z-axis.\n\n: <math>\\begin{align}\n  I_x &= \\iint\\limits_{R} y^2\\,\\mathrm{d}A = \\int^\\frac{b}{2}_{-\\frac{b}{2}} \\int^\\frac{h}{2}_{-\\frac{h}{2}} y^2 \\,\\mathrm{d}y \\,\\mathrm{d}x = \\int^\\frac{b}{2}_{-\\frac{b}{2}} \\frac{1}{3}\\frac{h^3}{4}\\,\\mathrm{d}x = \\frac{b h^3}{12} \\\\\n  I_y &= \\iint\\limits_{R} x^2\\,\\mathrm{d}A = \\int^\\frac{b}{2}_{-\\frac{b}{2}} \\int^\\frac{h}{2}_{-\\frac{h}{2}} x^2 \\,\\mathrm{d}y \\,\\mathrm{d}x = \\int^\\frac{b}{2}_{-\\frac{b}{2}} h x^2\\,\\mathrm{d}x = \\frac{b^3 h}{12}\n\\end{align}</math>\n\nUsing the [[second moment of area#Perpendicular axis theorem|perpendicular axis theorem]] we get the value of <math>J_z</math>.\n\n: <math>J_z = I_x + I_y = \\frac{b h^3}{12} + \\frac{h b^3}{12} = \\frac{b h}{12}\\left(b^2 + h^2\\right)</math>\n\n===Annulus centered at origin===\n[[File:Moment of area of an annulus.svg|thumb|175px|Annulus with inner radius ''r<sub>1</sub>'' and outer radius ''r<sub>2</sub>'']]\n\nConsider an [[annulus (mathematics)|annulus]] whose center is at the origin, outside radius is <math>r_2</math>, and inside radius is <math>r_1</math>. Because of the symmetry of the annulus, the centroid also lies at the origin. We can determine the polar moment of inertia, <math>J_z</math>, about the <math>z</math> axis by the method of composite shapes. This polar moment of inertia is equivalent to the polar moment of inertia of a circle with radius <math>r_2</math> minus the polar moment of inertia of a circle with radius <math>r_1</math>, both centered at the origin.  First, let us derive the polar moment of inertia of a circle with radius <math>r</math> with respect to the origin.  In this case, it is easier to directly calculate <math>J_z</math> as we already have <math>r^2</math>, which has both an <math>x</math> and <math>y</math> component.  Instead of obtaining the second moment of area from [[Cartesian coordinate system|Cartesian coordinates]] as done in the previous section, we shall calculate <math>I_x</math> and <math>J_z</math> directly using [[polar coordinates]].\n\n:<math>\\begin{align}\n  I_{x, circle} &= \\iint\\limits_{R} y^2\\,dA = \\iint\\limits_{R} (r\\sin{\\theta)}^2\\,\\mathrm{d}A = \\int_0^{2\\pi}\\int_0^r (r\\sin{\\theta})^2\\left(r\\,\\mathrm{d}r\\,\\mathrm{d}\\theta\\right) \\\\\n                &= \\int_0^{2\\pi}\\int_0^r r^3\\sin^2{\\theta}\\,\\mathrm{d}r\\,\\mathrm{d}\\theta = \\int_0^{2\\pi} \\frac{r^4\\sin^2{\\theta}}{4}\\,\\mathrm{d}\\theta = \\frac{\\pi}{4}r^4 \\\\\n  J_{z,circle} &= \\iint\\limits_{R} r^2\\,\\mathrm{d}A = \\int_0^{2\\pi}\\int_0^r r^2\\left(r\\,\\mathrm{d}r\\,\\mathrm{d}\\theta\\right) = \\int_0^{2\\pi}\\int_0^r r^3\\,\\mathrm{d}r\\,\\mathrm{d}\\theta \\\\\n               &= \\int_0^{2\\pi} \\frac{r^4}{4}\\,\\mathrm{d}\\theta = \\frac{\\pi}{2}r^4\n\\end{align}</math>\n\nNow, the polar moment of inertia about the <math>z</math> axis for an annulus is simply, as stated above, the difference of the second moments of area of a circle with radius <math>r_2</math> and a circle with radius <math>r_1</math>.\n\n:<math>J_z = J_{z, r_2} - J_{z, r_1} = \\frac{\\pi}{2}r_2^4 - \\frac{\\pi}{2}r_1^4 = \\frac{\\pi}{2}\\left(r_2^4 - r_1^4\\right)</math>\n\nAlternatively, we could change the limits on the <math>\\mathrm{d}r</math> integral the first time around to reflect the fact that there is a hole.  This would be done like this.\n\n:<math>\\begin{align}\n  J_{z} &= \\iint\\limits_{R} r^2\\,\\mathrm{d}A = \\int_0^{2\\pi}\\int_{r_1}^{r_2} r^2\\left(r\\,\\mathrm{d}r\\,\\mathrm{d}\\theta\\right) = \\int_0^{2\\pi}\\int_{r_1}^{r_2} r^3\\,\\mathrm{d}r\\,\\mathrm{d}\\theta \\\\\n        &= \\int_0^{2\\pi}\\left[\\frac{r_2^4}{4} - \\frac{r_1^4}{4}\\right]\\,\\mathrm{d}\\theta = \\frac{\\pi}{2}\\left(r_2^4 - r_1^4\\right)\n\\end{align}</math>\n\n===Any polygon===\n\n[[File:Moment of area of a polygon.svg|thumb|175px|A simple polygon. Here, <math>n=6</math>, notice point \"7\" is identical to point 1. ]]\n\nThe second moment of area about the origin for any [[simple polygon]] on the XY-plane can be computed in general by summing contributions from each segment of the polygon after dividing the area into a set of triangles.\nA polygon is assumed to have <math>n</math> vertices, numbered in counter-clockwise fashion. If polygon vertices are numbered clockwise, returned values will be negative, but absolute values will be correct.\n\n:<math>\\begin{align}\n     I_y &= \\frac{1}{12}\\sum_{i=1}^{n} \\left( x_i y_{i+1} - x_{i+1} y_i\\right) \\left( x_i^2 + x_i x_{i+1} + x_{i+1}^2 \\right) \\\\\n     I_x &= \\frac{1}{12}\\sum_{i=1}^{n} \\left( x_i y_{i+1} - x_{i+1} y_i\\right)\\left( y_i^2 + y_i y_{i+1} + y_{i+1}^2 \\right)  \\\\\n  I_{xy} &= \\frac{1}{24}\\sum_{i=1}^{n} \\left( x_i y_{i+1} - x_{i+1} y_i\\right) \\left( x_i y_{i+1} + 2 x_i y_i + 2 x_{i+1} y_{i+1} + x_{i+1} y_i \\right)\n\\end{align}</math>\n<ref>{{cite techreport \n|first= David\n|last= Hally\n|title= Calculation of the Moments of Polygons\n|number= Technical Memorandum 87/209\n|institution= Canadian National Defense\n|year= 1987\n|url = https://apps.dtic.mil/dtic/tr/fulltext/u2/a183444.pdf\n}}</ref>\n\nwhere <math>x_i,y_i</math> are the coordinates of the <math>i</math>-th polygon vertex, for <math>1 \\le i \\le n</math>. Also, <math>x_{n+1}, y_{n+1}</math> are assumed to be equal to the coordinates of the first vertex, i.e., <math>x_{n+1} = x_1</math> and <math>y_{n+1} = y_1</math>.\n<!-- Show similar formulae -->\n<ref>{{cite web\n | author = Steger, Carsten\n | url = https://pdfs.semanticscholar.org/fd18/036ba9b78174a2a161b184148028a43881a8.pdf\n | title = On the Calculation of Arbitrary Moments of Polygons\n | year = 1996\n}}</ref>\n<!-- Not sure it is useful -->\n<ref>{{cite web\n | author = Soerjadi, Ir. R.\n | url = https://repository.tudelft.nl/islandora/object/uuid:963296a1-8940-4439-9404-eca1bd2f8638/datastream/OBJ/download\n | title = On the Computation of the Moments of a Polygon, with some Applications\n}}</ref>\n\n<!-- Theses references are the home page of a commercial book and a web page about polygons with any references or proofs\n<ref>{{cite web\n | author = Joaquin Obregon\n | url = http://www.mecsym.org\n | title = Mechanical Symmetry\n | year = 2012\n}}</ref>\n<ref>{{cite web\n | author = Bourke, Paul\n | url = http://paulbourke.net/geometry/polygonmesh/\n | title = Polygons & Meshes\n | year = 1997\n}}</ref>\n-->\n\n==See also==\n* [[List of second moments of area]]\n* [[List of moments of inertia]]\n* [[Moment of inertia]]\n* [[Parallel axis theorem]]\n* [[Perpendicular axis theorem]]\n* [[Radius of gyration]]\n\n==References==\n{{Reflist}}\n\n{{Commons category|Second moments of area}}\n\n[[Category:Geometry]]\n[[Category:Structural analysis]]\n[[Category:Physical quantities]]\n\n==External Links==\n* [https://civilengineer.webinfolist.com/str/micalc.htm Online Calculator for second moment of area of plane sections.]"
    },
    {
      "title": "Section modulus",
      "url": "https://en.wikipedia.org/wiki/Section_modulus",
      "text": "{{Refimprove|date=October 2009}}\n\n'''Section modulus''' is a geometric property for a given cross-section used in the design of beams or flexural members. Other geometric properties used in design include [[area]] for tension and shear, [[radius of gyration]] for compression, and moment of inertia and polar moment of inertia for stiffness. Any relationship between these properties is highly dependent on the shape in question. Equations for the section moduli of common shapes are given below. There are two types of section moduli, the elastic section modulus and the plastic section modulus. The section moduli of different profiles can also be found as numerical values for common profiles in tables listing properties of such.\n\n== Notation ==\nNorth American and British/Australian convention reverse the usage of S & Z. Elastic modulus is S in North America,<ref>{{cite book |title=Specification for Structural Steel Buildings |year=2010 |page=16.1–xxxiv |publisher=American Institute of Steel Construction, Inc. |location=Chicago, Illinois |url=http://www.aisc.org/2010spec}}</ref> but Z in Britain/Australia,<ref>{{cite book |title=AS4100 - Steel Structures |year=1998 |page=21 |publisher=Standards Australia |location=Sydney, Australia |url=http://www.standards.org.au}}</ref> and vice versa for the plastic modulus. [[Eurocode]] 3 (EN 1993 - Steel Design) resolves this by using W for both, but distinguishes between them by the use of subscripts - W<sub>el</sub> and W<sub>pl</sub>.\n\n== Elastic section modulus ==\nFor general design, the elastic section modulus is used, applying up to the yield point for most metals and other common materials.\n\nThe elastic section modulus is defined as S = I / y, where I is the [[second moment of area]] (or area moment of inertia, not to be confused with moment of inertia) and y is the distance from the neutral axis to any given fibre. It is often reported using y = c, where c is the distance from the neutral axis to the most extreme fibre, as seen in the table below. It is also often used to determine the yield moment (M<sub>y</sub>) such that M<sub>y</sub> = S × σ<sub>y</sub>, where σ<sub>y</sub> is the [[Yield (engineering)|yield strength]] of the material. \n\n{| class=\"wikitable\" align=\"center\"\n|+Section modulus equations<ref name=Timo>Gere, J. M. and Timnko, S., 1997, Mechanics of Materials 4th Ed., PWS Publishing Co.</ref>\n! Cross-sectional shape\n! Figure\n! Equation\n! Comment\n|-\n| Rectangle\n| [[Image:Area moment of inertia of a rectangle.svg]]\n| <math>S = \\cfrac{bh^2}{6}</math>\n| Solid arrow represents [[neutral axis]]\n|-\n| doubly symmetric [[i-beam|{{ibeam}}-section]] (major axis)\n| [[Image:Section modulus-I-beam-strong axis.svg|x150px]]\n| <math>Sx = \\cfrac{BH^2}{6} - \\cfrac{bh^3}{6H}\n</math>\n\n<math>Sx = \\tfrac{Ix}{y} \n</math>,\n\nwith <math>y = \\cfrac{H}{2}\n</math>\n| NA indicates [[neutral axis]]\n|-\n| doubly symmetric [[i-beam|{{ibeam}}-section]] (minor axis)\n| [[Image:Section modulus-I-beam-weak axis.svg|x175px]]\n|<math>Sy = \\cfrac{B^2(H-h)}{6} + \\cfrac{(B-b)^3 h}{6B}</math><ref>https://www.engineersedge.com/material_science/section_modulus_12893.htm</ref>\n| NA indicates [[neutral axis]]\n|-\n| Circle\n| [[Image:Area moment of inertia of a circle.svg]]\n| <math>S = \\cfrac{\\pi r^3}{4} = \\cfrac{\\pi d^3}{32}</math><ref name=Timo/>\n| Solid arrow represents [[neutral axis]]\n|-\n| Circular hollow section\n| [[Image:Area moment of inertia of a circular area.svg]]      \n| <math>S = \\cfrac{\\pi\\left(r_2^4-r_1^4\\right)}{4 r_2} = \\cfrac{\\pi (d_2^4 - d_1^4)}{32d_2} </math>\n| Solid arrow represents [[neutral axis]]\n|-\n| Rectangular hollow section\n| [[File:Section modulus-rectangular tube.svg|x150px]]\n| <math>S = \\cfrac{BH^2}{6}-\\cfrac{bh^3}{6H}</math>   \n| NA indicates [[neutral axis]]\n|-\n| Diamond \n| [[File:Secion modulus-diamond.svg|x150px]]\n| <math>S = \\cfrac{BH^2}{24}</math>\n|NA indicates [[neutral axis]]\n|-\n| C-channel \n| [[Image:Section modulus-C-channel.svg|x150px]]\n| <math>S = \\cfrac{BH^2}{6} - \\cfrac{bh^3}{6H}</math>\n| NA indicates [[neutral axis]]\n|-\n|}\n\n== Plastic section modulus ==\nThe plastic section modulus is used for materials where elastic yielding is acceptable and plastic behavior is assumed to be an acceptable limit. Designs generally strive to ultimately remain below the plastic limit to avoid permanent deformations, often comparing the plastic capacity against amplified forces or stresses.\n\nThe plastic section modulus depends on the location of the plastic neutral axis (PNA). The PNA is defined as the axis that splits the cross section such that the compression force from the area in compression equals the tension force from the area in tension. So, for sections with constant yielding stress, the area above and below the PNA will be equal, but for composite sections, this is not necessarily the case.\n\nThe plastic section modulus is the sum of the areas of the cross section on each side of the PNA (which may or may not be equal) multiplied by the distance from the local centroids of the two areas to the PNA:\n\n<math> Z_P=A_Cy_C + A_Ty_T</math>\n\nthe Plastic Section Modulus can also be called the 'First moment of area'\n\n{| class=\"wikitable\"\n|-\n! Description || Figure || Equation || Comment \n|-\n| Rectangular section \n| [[Image:Area moment of inertia of a rectangle.svg]]\n| <math>Z_P = \\cfrac{bh^2}{4}</math><ref>https://www.dlsweb.rmit.edu.au/toolbox/buildright/content/bcgbc4010a/03_properties/02_section_properties/page_008.htm</ref><ref>{{cite book|last=Young|first=Warren C.|title=Roark's Formulas for Stress and Strain|year=1989|publisher=McGraw Hill|pages=217}}</ref>\n| <math>A_C = A_T = \\cfrac{bh}{2}</math> , <math>y_C = y_T = \\cfrac{h}{4}</math>\n|-\n| Rectangular hollow section \n| \n| <math>Z_P = \\cfrac{bh^2}{4}-(b-2t)(\\cfrac{h}{2}-t)^2</math>\n|where: b=width, h=height, t=wall thickness\n|-\n| For the two flanges of an [[i-beam|{{ibeam}}-beam]] with the web excluded<ref>American Institute of Steel Construction: Load and Resistance Factor Design, 3rd Edition, pp. 17-34.</ref>\n|\n| <math>Z_P = b_1t_1y_1+b_2t_2y_2\\,</math>\n|where:\n<math>b_1,b_2</math> =width, <math>t_1,t_2</math>=thickness, \n<math>y_1,y_2</math> are the distances from the neutral axis to the centroids of the flanges respectively.\n|- \n|For an I Beam including the web\n|\n| <math> Z_{P} = bt_f (d-t_f )+ 0.25t_w (d-2t_f )^2</math>\n|<ref>{{cite book|last=Megson|first=T H G|title=Structural and stress analysis|year=2005|publisher=elsever|pages=598 EQ (iv)|url=https://books.google.com/books?id=N2WyMxutXK4C&lpg=PP1&pg=PP1#v=onepage&q&f=false}}</ref>\n|-\n|For an I Beam (weak axis)\n|\n| <math> Z_{P} = (b^2t_f)/2 + 0.25t_w^2(d-2t_f )</math>\n|d = ?\n|-\n| Solid Circle \n|\n| <math>Z_P = \\cfrac{d^3}{6}</math>\n|\n|-\n| Circular hollow section\n|\n| <math>Z_P = \\cfrac{d_2^3-d_1^3}{6}</math>\n|\n|-\n|}\n\nThe plastic section modulus is used to calculate the plastic moment, M<sub>p</sub>, or full capacity of a cross-section.  The two terms are related by the yield strength of the material in question, F<sub>y</sub>, by M<sub>p</sub>=F<sub>y</sub>*Z.  '''Plastic section modulus''' and '''elastic section modulus''' are related by a '''shape factor''' which can be denoted by 'k', used for an indication of capacity beyond elastic limit of material. This could be shown mathematically with the formula :-\n\n<math>k = \\cfrac{Z}{S}</math>\n\nShape factor for a rectangular section is 1.5.\n\n== Use in structural engineering ==\n\n{{Unreferenced section|date=January 2014}}\n\nThough generally section modulus is calculated for the extreme tensile or compressive fibres in a bending beam, often compression is the most critical case due to onset of flexural torsional (F/T) buckling. Generally (except for brittle materials like concrete) tensile extreme fibres have a higher allowable stress or capacity than compressive fibres.\n\nIn the case of T-sections if there are tensile fibres at the bottom of the T they may still be more critical than the compressive fibres at the top due to a generally much larger distance from the neutral axis so despite having a higher allowable stress the elastic section modulus is also lower. In this case F/T buckling must still be assessed as the beam length and restraints may result in reduced compressive member bending allowable stress or capacity.\n\nThere may also be a number of different critical cases that require consideration, such as there being different values for orthogonal and principal axes and in the case of unequal angle sections in the principal axes there is a section modulus for each corner.\n\nFor a conservative (safe) design, civil structural engineers are often concerned with the combination of the highest load (tensile or compressive) and lowest elastic section modulus for a given section station along a beam, although if the loading is well understood one can take advantage of different section modulus for tension and compression to get more out of the design. For aeronautical and space applications where designs must be much less conservative for weight saving, structural testing is often required to ensure safety as reliance on structural analysis alone is more difficult (and expensive) to justify.\n\n== See also ==\n* [[Beam theory]]\n* [[List of area moments of inertia]]\n* [[Second moment of area]]\n\n== References ==\n{{Reflist}}\n\n\n\n[[Category:Structural analysis]]"
    },
    {
      "title": "Seismic analysis",
      "url": "https://en.wikipedia.org/wiki/Seismic_analysis",
      "text": "{{main article|Earthquake engineering}}\n{{No footnotes|date=April 2011}}\n[[Image:modes.png|right|thumb|200px|First and second modes of building seismic response]]\n\n'''Seismic analysis''' is a subset of [[structural analysis]] and is the calculation of the response of a building (or [[Nonbuilding structure|nonbuilding]]) structure to [[earthquakes]]. It is part of the process of [[structural design]], [[earthquake engineering]] or structural assessment and retrofit (see [[structural engineering]]) in regions where earthquakes are prevalent.\n\nAs seen in the figure, a building has the potential to 'wave' back and forth during an earthquake (or even a severe [[wind]] storm).  This is called the 'fundamental [[normal mode|mode]]', and is the lowest [[frequency]] of building response.  Most buildings, however, have higher modes of response, which are uniquely activated during earthquakes.  The figure just shows the second mode, but there are higher 'shimmy' (abnormal vibration) modes.  Nevertheless, the first and second modes tend to cause the most damage in most cases.\n\nThe earliest provisions for seismic resistance were the requirement to design for a lateral force equal to a proportion of the building weight (applied at each floor level). This approach was adopted in the appendix of the 1927 Uniform Building Code (UBC), which was used on the west coast of the United States. It later became clear that the dynamic properties of the structure affected the loads generated during an earthquake. In the [[Los Angeles]] County Building Code of 1943 a provision to vary the load based on the number of floor levels was adopted (based on [[Earthquake engineering research|research]] carried out at [[Caltech]] in collaboration with [[Stanford University]] and the [[U.S. Coast and Geodetic Survey]], which started in 1937). The concept of [[response spectrum|\"response spectra\"]] was developed in the 1930s, but it wasn't until 1952 that a joint committee of the San Francisco Section of the [[American Society of Civil Engineers|ASCE]] and the [[Structural Engineers Association of Northern California]] (SEAONC) proposed using the building period (the inverse of the frequency) to determine lateral forces.{{ref|Bozorgnia}}\n\nThe [[University of California, Berkeley]] was an early base for computer-based seismic analysis of structures, led by Professor [[Ray Clough]] (who coined the term [[finite element]]{{ref|Wilson}}). Students included [[Edward L. Wilson|Ed Wilson]], who went on to write the program [[Computers and Structures|SAP]] in 1970,{{ref|CUREE}} an early \"[[finite element analysis]]\" program.\n\nEarthquake engineering has developed a lot since the early days, and some of the more complex designs now use special earthquake protective elements either just in the foundation ([[base isolation]]) or [[vibration control|distributed throughout the structure]].  Analyzing these types of structures requires specialized explicit finite element computer code, which divides time into very small slices and models the actual [[physics]], much like common video games often have \"physics engines\".  Very large and complex buildings can be modeled in this way (such as the Osaka International Convention Center).\n\nStructural analysis methods can be divided into the following five categories.\n\n==Equivalent static analysis==\n\nThis approach defines a series of forces acting on a building to represent the effect of earthquake ground motion, typically defined by a seismic design [[response spectrum]]. It assumes that the building responds in its fundamental [[normal mode|mode]]. For this to be true, the building must be low-rise and must not twist significantly when the ground moves. The response is read from a design [[response spectrum]], given the [[natural frequency]] of the building (either calculated or defined by the [[building code]]). The applicability of this method is extended in many [[building code]]s by applying factors to account for higher buildings with some higher modes, and for low levels of twisting. To account for effects due to \"yielding\" of the structure, many codes apply modification factors that reduce the design forces (e.g. force reduction factors).\n\n==Response spectrum analysis==\n{{See also|Response spectrum}}\nThis approach permits the multiple modes of response of a building to be taken into account (in the [[frequency domain]]). This is required in many [[building code]]s for all except very simple or very complex structures. The response of a structure can be defined as a combination of many special shapes ([[normal mode|modes]]) that in a vibrating string correspond to the \"[[harmonics]]\". Computer analysis can be used to determine these modes for a structure. For each mode, a response is read from the design spectrum, based on the modal frequency and the modal mass, and they are then combined to provide an estimate of the total response of the structure. In this we have to calculate the magnitude of forces in all directions i.e. X, Y & Z and then see the effects on the building.. Combination methods include the following:\n*absolute – peak values are added together\n*square root of the sum of the squares (SRSS)\n*complete quadratic combination (CQC) – a method that is an improvement on SRSS for closely spaced modes\n\nThe result of a response spectrum analysis using the response spectrum from a ground motion is typically different from that which would be calculated directly from a linear dynamic analysis using that ground motion directly, since phase information is lost in the process of generating the response spectrum.\n\nIn cases where structures are either too irregular, too tall or of significance to a community in disaster response, the response spectrum approach is no longer appropriate, and more complex analysis is often required, such as [[non-linear]] static analysis or dynamic analysis.\n\n==Linear dynamic analysis==\n\nStatic procedures are appropriate when higher mode effects are not significant. This is generally true for short, regular buildings. Therefore, for tall buildings, buildings with torsional irregularities, or non-orthogonal systems, a dynamic procedure is required. In the linear dynamic procedure, the building is modelled as a multi-degree-of-freedom (MDOF) system with a linear elastic stiffness matrix and an equivalent viscous damping matrix.\n\nThe seismic input is modelled using either modal spectral analysis or time history analysis but in both cases, the corresponding internal forces and displacements are determined using linear elastic analysis. The advantage of these linear dynamic procedures with respect to linear static procedures is that higher modes can be considered. However, they are based on linear elastic response and hence the applicability decreases with increasing nonlinear behaviour, which is approximated by global force reduction factors.\n\nIn linear dynamic analysis, the response of the structure to ground motion is calculated in the [[time domain]], and all [[phase (waves)|phase]] information is therefore maintained. Only linear properties are assumed. The analytical method can use modal decomposition as a means of reducing the degrees of freedom in the analysis.\n\n==Nonlinear static analysis==\n\nIn general, linear procedures are applicable when the structure is expected to remain nearly elastic for the level of ground motion or when the design results in nearly uniform distribution of nonlinear response throughout the structure. As the performance objective of the structure implies greater inelastic demands, the uncertainty with linear procedures increases to a point that requires a high level of conservatism in demand assumptions and acceptability criteria to avoid unintended performance. Therefore, procedures incorporating inelastic analysis can reduce the uncertainty and conservatism.\n\nThis approach is also known as \"pushover\" analysis. A pattern of forces is applied to a structural model that includes non-linear properties (such as steel yield), and the total force is plotted against a reference displacement to define a capacity curve. This can then be combined with a demand curve (typically in the form of an acceleration-displacement [[response spectrum]] (ADRS)). This essentially reduces the problem to a single degree of freedom (SDOF) system.\n\nNonlinear static procedures use equivalent SDOF structural models and represent seismic ground motion with response spectra. Story drifts and component actions are related subsequently to the global demand parameter by the pushover or capacity curves that are the basis of the non-linear static procedures.\n\n==Nonlinear dynamic analysis==\n\nNonlinear dynamic analysis utilizes the combination of ground motion records with a detailed structural model, therefore is capable of producing results with relatively low uncertainty. In nonlinear dynamic analyses, the detailed structural model subjected to a ground-motion record produces estimates of component deformations for each degree of freedom in the model and the modal responses are combined using schemes such as the square-root-sum-of-squares.\n\nIn non-linear dynamic analysis, the non-linear properties of the structure are considered as part of a [[time domain]] analysis. This approach is the most rigorous, and is required by some [[building code]]s for buildings of unusual configuration or of special importance.  However, the calculated response can be very sensitive to the characteristics of the individual ground motion used as seismic input; therefore, several analyses are required using different ground motion records to achieve a reliable estimation of the [[probability distribution|probabilistic distribution]] of structural response. Since the properties of the seismic response depend on the intensity, or severity, of the seismic shaking, a comprehensive assessment calls for numerous nonlinear dynamic analyses at various levels of intensity to represent different possible earthquake scenarios. This has led to the emergence of methods like the [[incremental dynamic analysis]].{{ref|IDA}}\n\n==See also==\n\n*[[Applied element method]]\n*[[Earthquake simulation]]\n*[[Extreme Loading for Structures]] – seismic analysis software\n*[[Modal analysis using FEM]]\n*[[OpenSees]] – analysis software\n*[[Structural dynamics]]\n*[[Vibration control]]\n== References ==\n#{{note|FEMA356}}ASCE. (2000). Pre-standard and Commentary for the Seismic Rehabilitation of Buildings (FEMA-356) (Report No. FEMA 356). Reston, VA: American Society of Civil Engineers prepared for the Federal Emergency Management Agency.\n#{{note|ATC40}}ATC. (1985). Earthquake Damage Evaluation Data for California (ATC-13) (Report). Redwood, CA: Applied Technology Council.\n#{{note|Bozorgnia}}Bozorgnia, Y, Bertero, V, \"Earthquake Engineering: From Engineering Seismology to Performance-Based Engineering\", CRC Press, 2004.\n#{{note|Wilson}}[http://www.edwilson.org/History/fe-history.pdf \"Early Finite Element Research at Berkeley\"], Wilson, E. and Clough R., presented at the Fifth U.S. National Conference on Computational Mechanics, Aug. 4–6, 1999\n#{{note|CUREE}}[http://www.curee.org/image_gallery/calendar/essays/1998-CUREE_excerpt.pdf \"Historic Developments in the Evolution of Earthquake Engineering\"], illustrated essays by Robert Reitherman, CUREE, 1997, p12.\n#{{note|IDA}}Vamvatsikos D., Cornell C.A. (2002). Incremental Dynamic Analysis. Earthquake Engineering and Structural Dynamics, 31(3): 491–514.\n\n[[Category:Earthquake and seismic risk mitigation]]\n[[Category:Structural analysis]]\n[[Category:Earthquake engineering]]"
    },
    {
      "title": "Self-buckling",
      "url": "https://en.wikipedia.org/wiki/Self-buckling",
      "text": "A [[column]] can buckle due to its own weight with no other direct [[force]]s acting on it, in a failure mode called '''self-buckling'''. In conventional column [[buckling]] problems, the self-weight is often neglected since it is assumed to be small when compared to the applied axial [[Structural load|loads]]. However, when this assumption is not valid, it is important to take the self-buckling into account.\n\nElastic buckling of a \"heavy\" column i.e., column buckling under its own [[weight]], was first investigated by Greenhill at [[1881]].<ref>{{Cite web|url=https://martingillie.files.wordpress.com/2013/11/longest-column.pdf|title=Greenhill, A. G. (1881). “Determination of the greatest height consistent with stability that a vertical pole or mast can be made, and the greatest height to which a tree of given proportions can grow.” Proc. Cambridge Philos. Soc., 4, 65–73.|last=|first=|date=|website=|publisher=|access-date=}}</ref> He found that a free-standing, vertical column, with [[density]] <math>\\rho</math>, [[Young's modulus]] <math>E</math>, and [[Cross section (geometry)|cross-sectional area]] <math>A</math>, will buckle under its own weight if its [[height]] exceeds a certain critical value:\n\n<math>l_{max} = \\left(7.8373\\,\\frac{EI }{\\rho gA}\\right)^{1/3}</math>\n\nwhere <math>g</math> is the [[acceleration]] due to [[gravity]], <math>I</math> is the [[second moment of area]] of the [[Beam (structure)|beam]] cross section.\n\n<math> </math>\n\nOne interesting example for the use of the [[equation]] was suggested by Greenhill in his paper. He estimated the maximal height of a [[pine]] [[tree]], and found it cannot grow over 90 [[Foot (unit)|ft]] tall. This length sets the maximum height for trees on [[earth]] if we assume the trees to be [[prism]]atic and the [[branch]]es are neglected.\n\n== Mathematical Derivation ==\n[[File:A column exhibiting a compressive buckling load due to its own weight.2.png|thumb|''A column exhibiting a compressive buckling load due to its own weight.'']]\nSuppose a uniform column fixed in a vertical direction at its lowest point, and carried to a height <math>l</math>, in which the vertical position becomes unstable and flexure begins. There is a [[body force]] <math>q</math> per unit length <math>q=\\rho g A</math>, where <math>A</math> is the cross-sectional area of the column, <math>g</math> is the acceleration due to gravity and <math>\\rho</math> is its mass density.\n\nThe column is slightly curved under its own weight, so the [[curve]] <math>w(x)</math> describes the [[Deflection (engineering)|deflection]] of the beam in the <math>y</math> direction at some position <math>x</math>.\n\n<math> </math>\n\nLooking at any point on the column, we can write the [[Moment (physics)|moment]] equilibrium:\n\n<math>M=-\\int_{0}^{x} q(y-w)dx\n</math>\n\nwhere the right-hand side of the equation is the moment of the weight of BP about P.\n\nAccording to [[Euler–Bernoulli beam theory]]:<math> </math>     <math> </math><math>M=-EI{d^2w \\over dx^2}</math>\n\nWhere <math>E</math> is the Young's modulus of elasticity of the substance, <math>I</math> is the moment of inertia.\n\nTherefore, the [[differential equation]] of the central line of BP is:<math> </math><math> </math>\n\n<math>EI{d^2w \\over dx^2}=q\\int_{0}^{x} (y-w)dx\n</math>\n\nDifferentiating with respect to x, we get\n\n<math>EI{d^3w \\over dx^3}=q\\int_{0}^{x}\\Bigl(-{dw \\over dx}\\Bigr)dx\n</math><math> </math><math> </math>\n\nOr\n\n<math> </math><math>EI{d^3w \\over dx^3}=-qx {dw \\over dx}\n</math>\n\nWe get that the governing equation is the third order linear differential equation with a variable coefficient. The way to solve the problem is to use new variables <math>n, z, k</math> and <math>r</math>:\n\n<math>k^2={4 \\over 9}{q \\over EI}   ,</math><math> </math><math> </math> <math> </math><math> </math><math>   r^2=x^3,</math> <math> </math>         <math> </math><math>   \\sqrt{x} z={\\operatorname{d}\\!w\\over\\operatorname{d}\\!x},</math>  <math> </math>      <math> </math><math>   n^2={\\operatorname{}\\!1\\over\\operatorname{}\\!9}</math>\n\nThen, the equation transforms to the [[Bessel function|Bessel equation]]\n\n<math>r^2  {d^2z \\over dr^2}+r{dz \\over dr}+(k^2 r^2-n^2 )z=0</math>\n\nThe solution of the transformed equation is       <math>z=AJ_{\\frac{1}{3}} (kr)+BJ_{-\\frac{1}{3}} (kr) </math>\n\nWhere <math>J_{n} </math> is the Bessel function of the first kind. Then, the solution of the original equation is:\n\n<math>{\\operatorname{d}\\!w\\over\\operatorname{d}\\!x}=\\sqrt{x}\\Bigl(AJ_{\\frac{1}{3}} (kx^{\\frac{3}{2}}  )+BJ_{-\\frac{1}{3}} (kx^{\\frac{3}{2}})\\Bigr) </math>\n\n<math> </math>\n\nNow, we will use the [[Boundary value problem|boundary conditions]]:                                                        \n* No moment at <math>x=0</math>    <math>\\rightarrow </math>    <math>   {\\operatorname{d^2}\\!w\\over\\operatorname{d}\\!x^2}   (x=0)=0 </math>    <math>\\rightarrow </math>   <math>   A=0</math>            \n* Fixed at <math>x=l</math>    <math>\\rightarrow </math>  <math> </math> <math>   w(x=l)=0 </math> <math> </math>  <math>\\rightarrow </math>  <math> </math> <math>   J_{-\\frac{1}{3}} (kl^{\\frac{3}{2}})=0</math><math> </math><math> </math>         \nFrom the second B.C., we get that the critical length in which a vertical column will buckle under its own weight is:\n\n<math>   l_{max}  = \\biggl(\\frac{j_{\\frac{1}{3}}} {k}\\biggr)^{2/3}=\\Biggl(\\frac{9 (j_{\\frac{1}{3}})   ^2}{4}\\frac{  EI}{q}\\Biggl)^{1/3}</math>\n\nUsing   <math>   {j_{\\frac{1}{3}}}\\cong1.86635</math> , the first zero of the Bessel function of the first kind of order -1/3, <math>l_{max} </math> can be approximated to:\n\n<math>l_{max} = \\left(7.8373\\,\\frac{EI }{\\rho gA}\\right)^{1/3}</math>\n\n== See also ==\n*[[Buckling]]\n*[[Bending moment]]\n*[[Euler's critical load]]\n*[[Bending]]\n*[[Euler–Bernoulli beam theory]]\n\n== External links ==\n\n* [http://ocw.mit.edu/courses/mechanical-engineering/2-080j-structural-mechanics-fall-2013/course-notes/MIT2_080JF13_Lecture10.pdf Advanced Topic in Column Buckling, MIT Open-Course-Ware]\n* [https://archive.org/details/OperaMagistris Self-buckling detailed derivation] in the Opera Magistris v3.7 online reference Chapter 15, section 2.2.4.1, {{ISBN|978-2-8399-0932-7}}.\n\n== References ==\n{{Reflist}}\n\n[[Category:Elasticity (physics)]]\n[[Category:Materials science]]\n[[Category:Mechanical failure modes]]\n[[Category:Structural analysis]]\n[[Category:Mechanics]]"
    },
    {
      "title": "Shear and moment diagram",
      "url": "https://en.wikipedia.org/wiki/Shear_and_moment_diagram",
      "text": "[[File:Shear Moment Diagram.svg|thumb|400px|Shear and Bending moment diagram for a simply supported beam with a concentrated load at mid-span.]]\n\n'''Shear and bending moment diagrams''' are analytical tools used in conjunction with [[structural analysis]] to help perform [[structural design]] by determining the value of [[shear force]] and [[bending moment]] at a given point of a [[structural element]] such as a [[beam bending|beam]].  These diagrams can be used to easily determine the type, size, and material of a member in a structure so that a given set of [[structural load|load]]s can be supported without [[structural failure]]. Another application of shear and moment diagrams is that the [[deflection (engineering)|deflection]] of a beam can be easily determined using either the [[Moment-area theorem|moment area method]] or the [[conjugate beam method]].\n\n==Convention==\nAlthough these conventions are relative and any convention can be used if stated explicitly, practicing engineers have adopted a standard convention used in design practices.\n\n===Normal convention===\nThe normal convention used in most engineering applications is to label a positive shear force one that spins an element clockwise (up on the left, and down on the right). Likewise the normal convention for a positive bending moment is to warp the element in a \"u\" shape manner (Clockwise on the left, and counterclockwise on the right). Another way to remember this is if the moment is bending the beam into a \"smile\" then the moment is positive, with compression at the top of the beam and tension on the bottom.<ref>{{cite web|vauthors =Livermore C, Schmidt H, Williams J, Socrate S|title=2.001 Mechanics & Materials I, Fall 2006.|url=http://ocw.mit.edu/courses/mechanical-engineering/2-001-mechanics-materials-i-fall-2006|publisher=MIT OpenCourseWare: Massachusetts Institute of Technology|accessdate=25 October 2013|location=Lecture 5}}</ref> \n[[Image:Shear and Moment Convention.jpg|frame|Normal positive shear force convention (left) and normal bending moment convention (right).]] This convention was selected to simplify the analysis of beams. Since a horizontal member is usually analyzed from left to right and positive in the vertical direction is normally taken to be up, the positive shear convention was chosen to be up from the left, and to make all drawings consistent down from the right. The positive bending convention was chosen such that a positive shear force would tend to create a positive moment.\n\n===Alternative drawing convention===\nIn [[structural engineer]]ing and in particular [[concrete]] design the positive moment is drawn on the [[Tension (physics)|tension]] side of the member. This convention puts the positive moment below the beam described above. A convention of placing moment diagram on the tension side allows for frames to be dealt with more easily and clearly. Additionally placing the moment on the tension side of the member shows the general shape of the deformation and indicates on which side of a concrete member [[rebar]] should be placed, as concrete is weak in tension.<ref>{{cite web|title=Moment Diagram Sign Convention Poll|url=http://eng-tips.com/viewthread.cfm?qid=292028|publisher=[[Eng-Tips Forum]]|accessdate=25 October 2013}}</ref>\n\n== Calculating shear force and bending moment ==\n{{multiple image\n| align     = right\n| direction = vertical\n| width = 400\n| image1 = Sample Situation.jpg\n| caption1 = Loaded beam\n}}\nWith the loading diagram drawn the next step is to find the value of the shear force and moment at any given point along the element. For a horizontal beam one way to perform this is at any point to \"chop off\" the right end of the beam.\n\nThe example below includes a point load, a distributed load, and an applied moment. The supports include both hinged supports and a fixed end support. The first drawing shows the beam with the applied forces and displacement constraints. The second drawing is the loading diagram with the reaction values given without the calculations shown or what most people call a [[free body diagram]]. The third drawing is the [[shear force]] diagram and the fourth drawing is the [[bending moment]] diagram. For the bending moment diagram the normal sign convention was used. Below the moment diagram are the stepwise functions for the shear force and bending moment with the functions expanded to show the effects of each load on the shear and bending functions.\n\nThe example is illustrated using [[United States customary units]]. Point loads are expressed in [[Kip (unit)|kip]]s (1 kip = 1000&nbsp;lbf = 4.45&nbsp;kN), distributed loads are expressed in k/ft (1 k/ft = 1 kip/ft = 14.6&nbsp;kN/m), moments are expressed in ft-k (1&nbsp;ft-k = 1&nbsp;ft-kip = 1.356 kNm), and lengths are in ft (1&nbsp;ft = 0.3048 m).\n\n=== Step 1: Compute the reaction forces and moments ===\n{{multiple image\n| align     = right\n| direction = vertical\n| width = 400\n| image1 = Free Body Diagram of the Beam.PNG\n| caption1 = Free-body diagram of whole beam\n}}\n\nThe first step obtaining the bending moment and shear force equations is to determine the reaction forces. This is done using a free body diagram of the entire beam.\n\nThe beam has three reaction forces, ''R''<sub>a</sub>, ''R''<sub>b</sub> at the two supports and ''R''<sub>c</sub> at the clamped end.  The clamped end also has a reaction couple ''M''<sub>c</sub>.  These four quantities have to be determined using two equations, the balance of forces in the beam and the balance of moments in the beam.  Four unknowns cannot be found given two independent equations in these unknown variables and hence the beam is [[statically indeterminate]].  One way of solving this problem is to use the principle of [[linear superposition]] and break the problem up into the superposition of a number of statically determinate problems.  The extra boundary conditions at the supports have to be incorporated into the superposed solution so that the deformation of the entire beam is [[compatibility (mechanics)|compatible]].\n\nFrom the free-body diagram of the entire beam we have the two balance equations\n:<math>\n   \\sum F = 0 ~,~~ \\sum M_{A} = 0 \\,.\n </math>\nSumming the forces, we have\n:<math>\n  -10 - (1)(15) + R_a + R_b + R_c = 0  \n </math>\nand summing the moments around the free end (A) we have\n:<math>\n(R_a)(10) + (R_b)(25) + (R_c)(50) - (1)(15)(17.5) -50 + M_c= 0 \\,.\n </math>\nWe can solve these equations for ''R''<sub>b</sub> and ''R''<sub>c</sub> in terms of ''R''<sub>a</sub> and ''M''<sub>c</sub> :\n:<math>\n   R_b = 37.5 - 1.6 R_a + 0.04 M_c\n </math>\nand\n:<math>\n  R_c = -12.5 + 0.6 R_a - 0.04 M_c\\,.\n </math>\nIf we sum moments about the first support from the left of the beam we have\n:<math>\n  (10)(10) - (1)(15)(7.5) + (R_b)(15) + (R_c)(40) - 50 + M_c = 0 \\,.\n </math>\nIf we plug in the expressions for ''R''<sub>b</sub> and ''R''<sub>c</sub> we get the trivial identity ''0 = 0'' which indicates that this equation is not independent of the previous two.  Similarly, if we take moments around the second support, we have\n:<math>\n  (10)(25) - (R_a)(15) + (1)(15)(7.5) + (R_c)(25) - 50 + M_c = 0 \\,.\n </math>\nOnce again we find that this equation is not independent of the first two equations.  We could also try to compute moments around the clamped end of the beam to get\n:<math>\n  (10)(50) - (R_a)(40) - (R_b)(25) + (1)(15)(32.5) - 50 + M_c = 0 \\,.\n </math>\nThis equation also turns out not to be linearly independent from the other two equations.  Therefore, the beam is statically indeterminate and we will have to find the bending moments in segments of the beam as functions of ''R''<sub>a</sub> and ''M''<sub>c</sub>.\n\n=== Step 2: Break beam into segments ===\nAfter the reaction forces are found, you then break the beam into pieces. The location and number of external forces on the member determine the number and location of these pieces. The first piece always starts from one end and ends anywhere before the first external force.\n\n=== Step 3: Compute shear forces and moments - first piece ===\n{{multiple image\n| align     = right\n| direction = vertical\n| width = 300\n| image1 = Section One.PNG\n| caption1 = Free-body diagram of segment 1\n}}\nLet ''V''<sub>1</sub> and ''M''<sub>1</sub> be the shear force and bending moment respectively in a cross-section of the first beam segment. As the section of the beam moves towards the point of application of the external force the magnitudes of the shear force and moment may change. This makes the shear force and bending moment a function of the position of cross-section (in this example ''x'').\n\nBy summing the forces along this segment and summing the moments, the equations for the shear force and bending moment are obtained.  These equations are:\n:<math>\n  \\sum F = -10 - V_1 = 0\n </math>\nand\n:<math>\n  \\sum M_A = -V_1 x  + M_1 = 0 \\,.\n </math>\nTherefore,\n:<math>\n   V_1 = -10 \\quad \\text{and} \\quad M_1 = -10x \\,.\n </math>\n\n=== Step 4: Compute shear forces and moments - second piece ===\n{{multiple image\n| align     = right\n| direction = vertical\n| width = 300\n| image1 = Section Two.PNG\n| caption1 = Free-body diagram of segment 2\n}}\n\nTaking the second segment, ending anywhere before the second internal force, we have\n:<math>\n  \\sum F = -10 + R_a - (1)(x-10) - V_2 = 0\n </math>\nand\n:<math>\n  \\sum M_A = R_a (10) - (1)(x-10)\\frac{(x + 10)}{2} - V_2 x + M_2 = 0 \\,.\n </math>\nTherefore,\n:<math>\n   V_2 = R_a -x \\quad \\text{and} \\quad \n   M_2 = -50 + R_a (x-10) - \\frac{x^2}{2} \\,.\n </math>\nNotice that because the shear force is in terms of x, the moment equation is squared. This is due to the fact that the moment is the integral of the shear force. The tricky part of this moment is the distributed force. Since the force changes with the length of the segment, the force will be multiplied by the distance after 10&nbsp;ft. i.e. (x-10) the moment location is defined in the middle of the distributed force, which is also changing. This is where (x+10)/2 is derived from.\n\nAlternatively, we can take moments about the cross-section to get\n:<math>\n  \\sum M_A = 10x - R_a (x-10) + (1)(x-10)\\frac{(x- 10)}{2} + M_2 = 0 \\,.\n </math>\nAgain, in this case,\n:<math>\n   M_2 = -50 + R_a(x-10) - \\frac{x^2}{2} \\,.\n </math>\n\n=== Step 5: Compute shear forces and moments - third piece ===\n{{multiple image\n| align     = right\n| direction = vertical\n| width = 300\n| image1 = Section Three.PNG\n| caption1 = Free-body diagram of segment 3\n}}\n\nTaking the third segment, and summing forces, we have\n:<math>\n   -10 + R_a + R_b - (1)(15) - V_3 = 0\n </math>\nand summing moments about the cross-section, we get\n:<math>\n   (10)(x) - R_a(x-10) - R_b(x-25) + (1)(15)(x-17.5) + M_3  = 0 \\,.\n </math>\nTherefore, \n:<math>\n   V_3 = 25 - R_a - R_b = R_c\n </math>\nand\n:<math>\n   M_3 = 262.5 + R_a (x-10) + R_b (x-25)  - 25 x \n       = -675 + R_a (30  - 0.6 x) - M_c (1 - 0.04 x) + 12.5 x\\,.\n </math>\nNotice that the distributed force can now be considered one force of 15 kips acting in the middle of where it is positioned.\n\n=== Step 6: Compute shear forces and moments - fourth piece ===\n{{multiple image\n| align     = right\n| direction = vertical\n| width = 300\n| image1 = Section Four.PNG\n| caption1 = Free-body diagram of segment 4\n}}\nTaking the fourth and final segment, a balance of forces gives\n:<math>\n  -10 + R_a + R_b - (1)(15) - V_4 = 0\n </math>\nand a balance of moments around the cross-section leads to\n:<math>\n (10)(x) - R_a(x-10) - R_b(x-25) + (1)(15)(x-17.5) - 50 + M_4 = 0 \\,.\n </math>\nSolving for ''V''<sub>4</sub> and ''M''<sub>4</sub>, we have\n:<math>\n   V_4 = 25 - R_a - R_b = R_c\n </math>\nand\n:<math>\n   M_4 = 312.5 + R_a (x-10) + R_b (x-25) - 25 x = -625 + R_a(30 - 0.6x) + M_c(0.04x -1) + 12.5x\\,.\n </math>\nBy plotting each of these equations on their intended intervals, you get the bending moment and shear force diagrams for this beam.  In particular, at the clamped end of the beam, ''x'' = 50 and we have\n:<math>\n   M_4 = M_c = -937.5 + 40 R_a + 25 R_b \\,.\n </math>\n\n=== Step 7: Compute deflections of the four segments ===\nWe now use the [[Euler-Bernoulli beam theory]] to compute the deflections of the four segments.  The [[differential equation]] that relates the beam deflection (''w'') to the bending moment (''M'') is\n:<math>\n   \\frac{d^2 w}{dx^2} = - \\frac{M}{EI}\n </math>\nwhere ''E'' is the [[Young's modulus]] and ''I'' is the [[area moment of inertia]] of the beam cross-section.\n\nSubstituting the expressions for ''M''<sub>1</sub>, ''M''<sub>2</sub>, ''M''<sub>3</sub>, ''M''<sub>4</sub> into the beam equation and solving for the deflection gives us\n:<math>\n   \\begin{align}\n     w_1 & = \\frac{5}{3EI}\\,x^3 + C_1 + C_2\\,x \\\\\n     w_2 & = \\frac{1}{24EI}\\,x^2\\,\\left[x^2 + 600 - 4 R_a(x-30)\\right] + C_3 + C_4\\,x \\\\\n     w_3 & = \\frac{1}{100EI}\\left[\\frac{x^3}{3}(-625 + 30 R_a - 2 M_c) - 50 x^2(-675 + 30 R_a - M_c)\\right] + C_5 + C_6\\,x \\\\\n     w_4 & = \\frac{1}{100EI}\\left[\\frac{x^3}{3}(-625 + 30 R_a - 2 M_c) - 50 x^2(-625 + 30 R_a - M_c)\\right] + C_7 + C_8\\,x \n   \\end{align}\n </math>\n\n=== Step 8: Apply boundary conditions ===\nNow we will apply displacement boundary conditions for the four segments to determine the integration constants.\n\nFor the fourth segment of the beam, we consider the boundary conditions at the clamped end where ''w''<sub>4</sub> = d''w''/d''x'' = 0 at ''x'' = 50.  Solving for ''C''<sub>7</sub> and ''C''<sub>8</sub> gives\n:<math>\n   C_7 = -\\frac{1250}{3EI}(-625 + M_c + 30 R_a)  \\quad \\text{and} \\quad\n   C_8 = \\frac{125}{EI}(-125 + 6 R_a) \\,.\n </math>\nTherefore, we can express ''w''<sub>4</sub> as\n:<math>\n   w_4 = -\\frac{1}{300EI}(x-50)^2\\left[-5(6R_a - 125)(x-50) +2M_c(x+25)\\right] \\,.\n </math>\nNow, ''w''<sub>4</sub> = ''w''<sub>3</sub> at ''x'' = 37.5 (the point of application of the external couple).  Also, the slopes of the deflection curves at this point are the same, i.e., ''dw''<sub>4</sub>/''dx'' = ''dw''<sub>3</sub>/''dx''.  Using these boundary conditions and solving for ''C''<sub>5</sub> and ''C''<sub>6</sub>, we get\n:<math>\n   C_5 = -\\frac{625}{12EI}(-5675 + 8 M_c + 240 R_a) \\quad \\text{and} \\quad\n   C_6 = \\frac{250}{EI}\\left(3R_a -70\\right) \\,.\n </math>\nSubstitution of these constants into the expression for ''w''<sub>3</sub> gives us\n:<math>\n  \\begin{align}\n   w_3 = \\frac{1}{300EI}\\Bigl[&30 R_a (-50 + x)^3 - 2 M_c (-50 + x)^2 (25 + x) -  \\\\\n       & 625 (-141875 + x (8400 + (-162 + x) x))\\Bigr] \\,.\n  \\end{align}\n </math>\nSimilarly, at the support between segments 2 and 3 where ''x'' = 25, ''w''<sub>3</sub> = ''w''<sub>2</sub> and ''dw''<sub>3</sub>/''dx'' = ''dw''<sub>2</sub>/''dx''. Using these and solving for ''C''<sub>3</sub> and ''C''<sub>4</sub> gives\n:<math>\n   C_3 = -\\frac{3125}{24EI}(-1645 + 4 M_c + 64 R_a) \\quad \\text{and} \\quad C_4 = \\frac{25}{12EI}\\left(-40325 + 6 M_c + 120 R_a\\right)\\,.\n </math>\nTherefore,\n:<math>\n  \\begin{align}\n  w_2 = \\frac{1}{24EI}\\Bigl[ & -3125 (-1645 + 4 M_c + 64 R_a) + \\\\\n   & 50 (-4025 + 6 M_c + 120 R_a) x + 120 (5 + R_a) x^2 - 4 R_a x^3 + x^4\\Bigr] \\,.\n  \\end{align}\n </math>\nAt the support between segments 1 and 2, ''x'' = 10 and ''w''<sub>1</sub> = ''w''<sub>2</sub> and ''dw''<sub>1</sub>/''dx'' = ''dw''<sub>2</sub>/''dx''.  These boundary conditions give us\n:<math>\n  C_1 = -\\frac{125}{24EI}(-40145 + 100 M_c + 1632 R_a)  \\quad \\text{and} \\quad\n  C_2 = \\frac{25}{4EI}(-1315 + 2 M_c + 48 R_a) \\,.\n </math>\nTherefore,\n:<math>\n   w_1 = \\frac{5}{24EI}\\left[1026125 - 39450 x + 8 x^3 + 20 M_c (-125 + 3 x) + 480 R_a (-85 + 3 x)\\right] \\,.\n </math>\n\n=== Step 9: Solve for ''M''<sub>c</sub> and ''R''<sub>a</sub> ===\nBecause ''w''<sub>2</sub> = 0 at ''x'' = 25, we can solve for ''M''<sub>c</sub> in terms of ''R''<sub>a</sub> to get\n:<math>\n  M_c = 175 - 7.5 R_a \\,.\n </math>\nAlso, since ''w''<sub>1</sub> = 0 at ''x'' = 10, expressing the deflection in terms of ''R''<sub>a</sub> (after eliminating ''M''<sub>c</sub>) and solving for ''R''<sub>a</sub>, gives\n:<math>\n   R_a = 25.278  \\quad \\implies \\quad M_c = -14.585 \\,.\n </math>\n\n=== Step 10: Plot bending moment and shear force diagrams ===\n{{multiple image\n| align     = right\n| direction = vertical\n| width = 400\n| image1 = Sample Loading Diagram.jpg\n| caption1 = Free-body diagram\n| image2 = Sample Shear Diagram.jpg\n| caption2 = Shear force diagram\n| image3 = Sample Moment Diagram.jpg\n| caption3 = Bending moment diagram\n}}\nWe can now calculate the reactions ''R''<sub>b</sub> and ''R''<sub>c</sub>, the bending moments ''M''<sub>1</sub>, ''M''<sub>2</sub>, ''M''<sub>3</sub>, ''M''<sub>4</sub>, and the shear forces ''V''<sub>1</sub>, ''V''<sub>2</sub>, ''V''<sub>3</sub>, ''V''<sub>4</sub>.  These expressions can then be plotted as a function of length for each segment.Realationship\n\n=== Relationship between shear force and bending moment ===\nIt is important to note the relationship between the two diagrams. The moment diagram is a visual representation of the area under the shear force diagram. That is, the moment is the integral of the shear force. If the shear force is constant over an interval, the moment equation will be in terms of x (linear). If the shear force is linear over an interval, the moment equation will be quadratic (parabolic).\n\nAnother note on the shear force diagrams is that they show where external force and moments are applied. With no external forces, the piecewise functions should attach and show no discontinuity. The discontinuities on the graphs are the exact magnitude of either the external force or external moments that are applied. For example, at x = 10 on the shear force diagram, there is a gap between the two equations. This gap goes from -10 to 15.3. The length of this gap is 25.3, the exact magnitude of the external force at that point.  At section 3 on the moment diagram, there is a discontinuity of 50. This is from the applied moment of 50 on the structure. The maximum and minimum values on the graphs represent the max forces and moments that this beam will have under these circumstances.\n\n==Relationships between load, shear, and moment diagrams==\nSince this method can easily become unnecessarily complicated with relatively simple problems, it can be quite helpful to understand different relations between the loading, shear, and moment diagram. The first of these is the relationship between a distributed load  on the loading diagram and the shear diagram. Since a distributed load varies the shear load according to its magnitude it can be derived that the slope of the shear diagram is equal to the magnitude of the distributed load. The relationship, described by [[Schwedler's theorem]], between distributed load and shear force magnitude is:<ref>[http://emweb.unl.edu/negahban/em325/10a-shear-and-bending-moment/Shear%20stress%20in%20beams.htm Emweb.unl.edu]</ref>\n\n:<math>\\frac{dQ}{dx} = -q</math>\n\nSome direct results of this is that a shear diagram will have a point change in magnitude if a point load is applied to a member, and a linearly varying shear magnitude as a result of a constant distributed load.\nSimilarly it can be shown that the slope of the moment diagram at a given point is equal to the magnitude of the shear diagram at that distance. The relationship between distributed shear force and bending moment is:<ref>{{cite book |last=Beer |first=Ferdinand P. |author2=E. Russell Johnston |author3=John T. DeWolf  |title=Mechanics of Materials |pages=322–323 |publisher=McGraw-Hill |year=2004 |isbn=0-07-298090-7}}</ref>\n\n:<math>\\frac{dM}{dx} = Q</math>\n\nA direct result of this is that at every point the shear diagram crosses zero the moment diagram will have a local maximum or minimum. Also if the shear diagram is zero over a length of the member, the moment diagram will have a constant value over that length. By calculus it can be shown that a point load will lead to a linearly varying moment diagram, and a constant distributed load will lead to a quadratic moment diagram.\n\n==Practical considerations==\nIn practical applications the entire stepwise function is rarely written out. The only parts of the stepwise function that would be written out are the moment equations in a nonlinear portion of the moment diagram; this occurs whenever a distributed load is applied to the member. For constant portions the value of the shear and/or moment diagram is written right on the diagram, and for linearly varying portions of a member the beginning value, end value, and slope or the portion of the member are all that are required.<ref name = \"Hibbeler 146-148\">{{cite book |last=Hibbeler |first=R.C |title=Structural Analysis |publisher=Macmillan |year=1985 |pages=146–148}}</ref>\n\n==See also==\n*[[Bending]]\n*[[Euler-Bernoulli beam theory]]\n*[[Bending moment]]\n*[[Singularity function#Example beam calculation]]\n\n==References==\n{{Reflist}}\n\n==Further reading==\n*Cheng, Fa-Hwa. \"Shear Forces and Bending Moments in Beams\" Statics and Strength of Materials. New York: Glencoe, McGraw-Hill, 1997. Print.\n*Spotts, Merhyle Franklin, Terry E. Shoup, and Lee Emrey. Hornberger. \"Shear and Bending Moment Diagrams.\" Design of Machine Elements. Upper Saddle River, NJ: Pearson/Prentice Hall, 2004. Print.\n\n==External links==\n*[http://bendingmomentdiagram.com/free-calculator FREE Online Shear Force and Bending Moment Diagram (SFD & BMD) Calculator.] (Note: only free up to 3 point loads.)\n*[http://www.mathalino.com/reviewer/mechanics-and-strength-of-materials/shear-and-moment-diagrams To draw the shear and moment diagrams by writing the shear and moment equations.]\n*[http://www.mathalino.com/reviewer/mechanics-and-strength-of-materials/relation-between-load-shear-and-moment To draw the shear and moment diagrams by the relationship between load, shear, and moment.]\n\n{{wikiversity|Shear Force and Bending Moment Diagrams}}  \n{{Commons category|Shear and moment diagrams}}\n\n{{DEFAULTSORT:Shear And Moment Diagram}}\n[[Category:Continuum mechanics]]\n[[Category:Structural analysis]]\n[[Category:Diagrams]]"
    },
    {
      "title": "Slope deflection method",
      "url": "https://en.wikipedia.org/wiki/Slope_deflection_method",
      "text": "The '''slope deflection method''' is a [[structural analysis]] method for [[beam (structure)|beams]] and [[Framing (construction)|frames]] introduced in 1914 by George A. Maney.<ref name=\"history1\">{{Cite journal|first=George A.|last=Maney|year=1915|title=Studies in Engineering|publisher=University of Minnesota|location=Minneapolis|postscript=<!--None-->}}</ref> The slope deflection method was widely used for more than a decade until the [[moment distribution method]] was developed. In the book, \"The Theory and Practice of Modern Framed Structures\", written by J.B Johnson, C.W. Bryan and F.E. Turneaure, it is stated that this method was first developed,\"by Professor Otto Mohr in Germany, and later developed independently by Professor G.A. Maney\". According to this book, professor Otto Mohr introduced this method for the first time in his book,\"Evaluation of Trusses with Rigid Node Connections\" or \"Die Berechnung der Fachwerke mit Starren Knotenverbindungen\".\n\n== Introduction ==\nBy forming '''slope deflection equations''' and applying joint and shear equilibrium conditions, the rotation angles (or the slope angles) are calculated. Substituting them back into the slope deflection equations, member end moments are readily determined. Deformation of member is due to the bending moment.\n\n== Slope deflection equations ==\n\nThe slope deflection equations can also be written using the stiffness factor <math>K=\\frac{I_{ab}}{L_{ab}}</math> and the chord rotation <math>\\psi =\\frac{ \\Delta}{L_{ab}}</math>:\n\n=== Derivation of slope deflection equations ===\nWhen a [[Beam (structure)#Types of beams|simple beam]] of length <math>L_{ab}</math> and flexural rigidity <math>E_{ab} I_{ab}</math> is loaded at each end with clockwise moments <math>M_{ab}</math> and <math>M_{ba}</math>, member end rotations occur in the same direction. These rotation angles can be calculated using the [[unit force method]] or Darcy's Law.\n\n:<math>\\theta_a - \\frac{\\Delta}{L_{ab}}= \\frac{L_{ab}}{3E_{ab} I_{ab}} M_{ab} - \\frac{L_{ab}}{6E_{ab} I_{ab}} M_{ba}</math>\n:<math>\\theta_b - \\frac{\\Delta}{L_{ab}}= - \\frac{L_{ab}}{6E_{ab} I_{ab}} M_{ab} + \\frac{L_{ab}}{3E_{ab} I_{ab}} M_{ba}</math>\n\nRearranging these equations, the slope deflection equations are derived.\n\n== Equilibrium conditions ==\n\n=== Joint equilibrium ===\nJoint equilibrium conditions imply that each joint with a degree of freedom should have no unbalanced moments i.e. be in equilibrium. Therefore,\n:<math>\\Sigma \\left( M^{f} + M_{member} \\right) = \\Sigma M_{joint}</math>\nHere, <math>M_{member}</math> are the member end moments, <math>M^{f}</math> are the [[fixed end moments]], and <math>M_{joint}</math> are the external moments directly applied at the joint.\n\n=== Shear equilibrium ===\nWhen there are chord rotations in a frame, additional equilibrium conditions, namely the shear equilibrium conditions need to be taken into account.\n\n== Example ==\n[[Image:MomentDistributionMethod.jpg|thumb|434px|right|Example]]\nThe statically indeterminate beam shown in the figure is to be analysed.\n*Members AB, BC, CD have the same length <math> L = 10 \\ m </math>.\n*Flexural rigidities are EI, 2EI, EI respectively.\n*Concentrated load of magnitude <math> P = 10 \\ kN </math> acts at a distance <math> a = 3 \\ m </math> from the support A.\n*Uniform load of intensity <math> q = 1 \\ kN/m</math> acts on BC.\n*Member CD is loaded at its midspan with a concentrated load of magnitude <math> P = 10 \\ kN </math>.\nIn the following calculations, clockwise moments and rotations are positive.\n\n=== Degrees of freedom ===\nRotation angles <math>\\theta_A</math>, <math>\\theta_B</math>, <math>\\theta_C</math>, of joints A, B, C, respectively are taken as the unknowns. There are no chord rotations due to other causes including support settlement.\n\n=== Fixed end moments ===\nFixed end moments are:\n:<math>M _{AB} ^f = - \\frac{P a b^2 }{L ^2} = - \\frac{10 \\times 3 \\times 7^2}{10^2} = -14.7 \\mathrm{\\,kN \\,m}</math>\n:<math>M _{BA} ^f = \\frac{P a^2 b}{L^2} = \\frac{10 \\times 3^2 \\times 7}{10^2} = 6.3 \\mathrm{\\,kN \\,m}</math>\n:<math>M _{BC} ^f = - \\frac{qL^2}{12} = - \\frac{1 \\times 10^2}{12} = - 8.333 \\mathrm{\\,kN \\,m}</math>\n:<math>M _{CB} ^f = \\frac{qL^2}{12} = \\frac{1 \\times 10^2}{12} = 8.333 \\mathrm{\\,kN \\,m}</math>\n:<math>M _{CD} ^f = - \\frac{PL}{8} = - \\frac{10 \\times 10}{8} = -12.5 \\mathrm{\\,kN \\,m}</math>\n:<math>M _{DC} ^f = \\frac{PL}{8} = \\frac{10 \\times 10}{8} = 12.5 \\mathrm{\\,kN \\,m}</math>\n\n=== Slope deflection equations ===\nThe slope deflection equations are constructed as follows:\n:<math>M_{AB} = \\frac{EI}{L} \\left( 4 \\theta_A + 2 \\theta_B \\right) = \\frac{4EI \\theta_A + 2EI \\theta_B}{L}</math> \n:<math>M_{BA} = \\frac{EI}{L} \\left( 2 \\theta_A + 4 \\theta_B \\right) = \\frac{2EI \\theta_A + 4EI \\theta_B}{L}</math> \n:<math>M_{BC} = \\frac{2EI}{L} \\left( 4 \\theta_B + 2 \\theta_C \\right) = \\frac{8EI \\theta_B + 4EI \\theta_C}{L}</math> \n:<math>M_{CB} = \\frac{2EI}{L} \\left( 2 \\theta_B + 4 \\theta_C \\right) = \\frac{4EI \\theta_B + 8EI \\theta_C}{L}</math> \n:<math>M_{CD} = \\frac{EI}{L} \\left( 4 \\theta_C \\right) = \\frac{4EI\\theta_C}{L}</math> \n:<math>M_{DC} = \\frac{EI}{L} \\left( 2 \\theta_C \\right) = \\frac{2EI\\theta_C}{L}</math>\n\n=== Joint equilibrium equations ===\nJoints A, B, C should suffice the equilibrium condition. Therefore\n:<math>\\Sigma M_A = M_{AB} + M_{AB}^f = 0.4EI \\theta_A + 0.2EI \\theta_B  - 14.7 = 0</math>\n:<math>\\Sigma M_B = M_{BA} + M_{BA}^f + M_{BC} + M_{BC}^f = 0.2EI \\theta_A + 1.2EI \\theta_B + 0.4EI \\theta_C - 2.033 = 0</math>\n:<math>\\Sigma M_C = M_{CB} + M_{CB}^f + M_{CD} + M_{CD}^f = 0.4EI \\theta_B + 1.2EI \\theta_C  - 4.167 = 0</math>\n\n=== Rotation angles ===\nThe rotation angles are calculated from simultaneous equations above.\n:<math>\\theta_A = \\frac{40.219}{EI} </math>\n:<math>\\theta_B = \\frac{-6.937}{EI} </math>\n:<math>\\theta_C = \\frac{5.785}{EI} </math>\n\n=== Member end moments ===\nSubstitution of these values back into the slope deflection equations yields the member end moments (in kNm):\n:<math>M_{AB} = 0.4 \\times 40.219 + 0.2 \\times \\left( -6.937 \\right) - 14.7 = 0 </math>\n:<math>M_{BA} = 0.2 \\times 40.219 + 0.4 \\times \\left( -6.937 \\right) + 6.3 = 11.57 </math>\n:<math>M_{BC} = 0.8 \\times \\left( -6.937 \\right) + 0.4 \\times 5.785 - 8.333 = -11.57 </math>\n:<math>M_{CB} = 0.4 \\times \\left( -6.937 \\right) + 0.8 \\times 5.785 + 8.333 = 10.19 </math>\n:<math>M_{CD} = 0.4 \\times -5.785 - 12.5 = -10.19 </math>\n:<math>M_{DC} = 0.2 \\times -5.785 + 12.5 = 13.66 </math>\n\n== See also ==\n*[[Beam theory]]\n\n== Notes ==\n{{reflist|1}}\n\n== References ==\n*{{cite book|last=Norris|first=Charles Head|author2=John Benson Wilbur |author3=Senol Utku |title=Elementary Structural Analysis|edition=3rd|year=1976|publisher=McGraw-Hill|isbn=0-07-047256-4|pages=313–326}}\n*{{cite book|last1=McCormac|first1=Jack C.|last2=Nelson|first2=James K. Jr.|title=Structural Analysis: A Classical and Matrix Approach|edition=2nd |year=1997|publisher=Addison-Wesley|isbn=0-673-99753-7|pages=430–451}}\n*{{cite book|last=Yang|first=Chang-hyeon|title=Structural Analysis|url=http://www.cmgbook.co.kr/category/sub_detail.html?no=1017|archive-url=https://web.archive.org/web/20071008135424/http://www.cmgbook.co.kr/category/sub_detail.html?no=1017|dead-url=yes|archive-date=2007-10-08|edition=4th|date=2001-01-10|publisher=Cheong Moon Gak Publishers|language=Korean|location=Seoul|isbn=89-7088-709-1|pages=357–389}}\n\n[[Category:Structural analysis]]"
    }
  ]
}