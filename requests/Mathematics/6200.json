{
  "pages": [
    {
      "title": "Red rocket (shotgun slug)",
      "url": "https://en.wikipedia.org/wiki/Red_rocket_%28shotgun_slug%29",
      "text": "The '''Red Rocket''' is a 3D printed solid [[shotgun]] projectile, or slug.<ref name=\"dailymail.co.uk\">[http://www.dailymail.co.uk/sciencetech/article-2329105/Is-3D-printed-BULLET-YouTube-video-shows-homemade-ammunition-fired.html Is this the first 3D-printed BULLET?], DailyMail, May 22, 2013. ([https://www.webcitation.org/6SRQHtugM archive])</ref> made public around May 2013.<ref name=\"dailymail.co.uk\"/>  It was printed using the [[Solidoodle]] 3 3-D printer.<ref name=\"wired.com\">[https://www.wired.com/2013/05/3d-printed-bullets/ 3-D Printed Shotgun Slugs Blow Away Their Targets], Wired, May 22, 2013. ([https://www.webcitation.org/6SRR2wYqe archive])</ref> The slug was created by an American named Jeff Heeszel.<ref>[http://www.huffingtonpost.com/2013/05/23/3d-printed-bullets_n_3322370.html 3D-Printed Bullets Exist, And They're Terrifyingly Easy To Make], huffingtonpost, May 23, 2013. ([https://www.webcitation.org/6SRQoTSbA archive])</ref>\n\nThe printer used to create the bullet retails around $800 as of September 2014, and it took the printer about an hour to produce the slug.<ref name=\"wired.com\"/> ABS thermoplastic material was used in the production of the slug.<ref name=\"wired.com\"/> During testing, the slug fired from a Mossberg 590 shotgun,<ref name=\"dailymail.co.uk\"/> penetrated a 2×12 piece of pine wood, and created a hole in a wire reel.<ref name=\"wired.com\"/>\n\nIt is considered the first 3D printed bullet.{{citation needed|date=February 2019}} According to the creator each plastic slug was 3D-printed but a small lead shot was then added to give the bullet weight and enable it to travel and pierce the target.<ref name=\"dailymail.co.uk\"/>\n\n==See also==\n*[[List of notable 3D printed weapons and parts]]\n\n==References==\n{{Reflist}}\n\n[[Category:Ammunition]]\n[[Category:3D printed firearms]]\n\n[[Category:Weapons and ammunition introduced in 2013]]"
    },
    {
      "title": "Reprringer",
      "url": "https://en.wikipedia.org/wiki/Reprringer",
      "text": "The '''Reprringer''' is a 3D printed [[pepperbox]] firearm,<ref name=5gunrefs>[http://3dprint.com/14636/3d-prnted-guns/ 5 Different 3D Printed Gun Models Have Been Fired Since May, 2013 – Here They Are], 3D Print, September 10, 2014. ([https://www.webcitation.org/6SVllm7Dl archive])</ref><ref name=\"D-printed Reprringer Pepperbox 2013\">[http://www.guns.com/2013/09/13/introducing-3d-printed-reprringer-pepperbox-video/ Introducing the 3D-printed Reprringer Pepperbox (VIDEO)], guns.com, September 13, 2013. ([https://www.webcitation.org/6SRc8adlR archive])</ref><ref name=\"D-printed Reprringer Pepperbox 2013\"/><ref name=\"D-printed Reprringer Pepperbox 2013\"/><ref name=\"3D Printed Pepperbox Pistol\">[http://www.thefirearmblog.com/blog/2013/10/08/3d-printed-pepperbox-pistol/ 3D Printed Pepperbox Pistol], firearmblog, October 8, 2013. ([https://www.webcitation.org/6SRcXdCdQ archive])</ref><ref name=\"wired.co.uk\">[https://www.wired.co.uk/news/archive/2014-05/16/3d-printed-guns 3D printed guns a year on: from prototype to serious weapons], Wired, 16 May 2014. ([https://www.webcitation.org/6SRXdTEJs archive])</ref> made public around September 2013.<ref name=\"D-printed Reprringer Pepperbox 2013\"/> It is a 5-shot, single-action, manually-indexed [[.22 CB Cap]] revolver.<ref name=\"D-printed Reprringer Pepperbox 2013\"/><ref name=\"3D Printed Pepperbox Pistol\"/>\n\n==Design==\nUnlike the many early 3D-printed firearm designs, which are usually massively overbuilt in order to withstand the pressures and strain on the material from modern gunpowder cartridges, the Reprringer is small and only slightly larger than a gun made from steel.<ref name=\"D-printed Reprringer Pepperbox 2013\"/> It is chambered for .22 CB Cap which is considered the least powerful commercially-produced cartridge on the market.<ref name=\"D-printed Reprringer Pepperbox 2013\"/> The barrels are not rifled, the lack of theoretical accuracy is considered a non-issue in a small gun with no sights.<ref name=\"D-printed Reprringer Pepperbox 2013\"/>\n\n==See also==\n*[[List of notable 3D printed weapons and parts]]\n\n==References==\n{{Reflist}}\n\n[[Category:3D printed firearms]]\n[[Category:RepRap project]]\n\n==External links==\n\n*[https://github.com/maduce/fosscad-repo/tree/master/Firearms/Reprringer_Hexen__v3.0_Derringer_Pepperbox_Gun-franco Reprringer_Hexen__v3.0_Derringer_Pepperbox_Gun-franco]"
    },
    {
      "title": "Ruger Charger (3D printed)",
      "url": "https://en.wikipedia.org/wiki/Ruger_Charger_%283D_printed%29",
      "text": "The '''Ruger Charger (3D printed)''' is a [[3D print]]ed copy of the [[Ruger 10/22]] Charger semi-automatic pistol's [[Receiver (firearms)|receiver]]<ref name=reason>[http://reason.com/blog/2014/07/07/3d-printed-semiautomatic-22-debuts-if-yo 3D-Printed Semiautomatic .22 Debuts. \"If you take my gun, I will simply print another one.\"], Reason, July 2014. ([https://www.webcitation.org/6StQuzZ7S archive])</ref><ref name=3dprintrugger>[http://3dprint.com/8398/3d-printed-gun-semi-automatic/ 3D Printed Semi-automatic Ruger Charger Pistol is Assembled and Fired – ‘If you take my gun, I’ll print another!’], 3Dprint, July 4, 2014. ([https://www.webcitation.org/6StSBJoxR archive])</ref><ref name=ammolandrugger>[http://www.ammoland.com/2014/07/3d-printed-ruger-charger-style-pistol-by-buck-o-fama/#axzz3EUU0yfrM 3D Printed Ruger Style Pistol Demo by Buck O’ Fama ~ Video], Ammoland, July 4, 2014. ([https://www.webcitation.org/6StRzQuTO archive])</ref> made public in July 2014.<ref name=reason/> It was created by a gunmaker who goes by the pseudonym Buck O'Fama.<ref name=reason/><ref>[https://techcrunch.com/2014/07/09/gunmaker-skirts-laws-by-3d-printing-a-single-firearm-part/ Gunmaker Skirts Laws By 3D-Printing A Single Firearm Part], TechCrunch, July 9, 2014. ([https://www.webcitation.org/6T0faVy6d archive])</ref>\n\nIt was printed using a small format 3D printer, the creator did not reveal the name of the printer.<ref name=3dprintrugger/> It was printed via the [[Fused deposition modeling|Fused deposition modeling (FDM)]] method.<ref>[http://news.softpedia.com/news/3D-Printing-Lets-Man-Assemble-Ruger-Charger-Pistol-Without-Legal-Paperwork-449881.shtml 3D Printing Lets Man Assemble Ruger Charger Pistol Without Legal Paperwork], Softpedia, July 8, 2014. ([https://www.webcitation.org/6StSe5XZ5 archive])</ref> The Charger is the  pistol version of the popular [[Ruger 10/22]] rifle, and comes standard with 10-round flush magazines and can  also accept high-capacity magazines including 30 rounds or more.<ref name=reason/><ref name=3dprintrugger/> According to Ammoland, a video of the weapon was \"posted on July 4th 2014, Independence Day, from somewhere in the State of Nevada.   It is interesting that in spite of the fact that the poster did nothing illegal, they felt compelled to disguise their voice\"<ref name=ammolandrugger/>\n\n==Composition==\nThe entire gun was not 3D printed, only the [[Receiver (firearms)|receiver]] was 3D printed. The other parts were purchased from the internet and did not require any legal paperwork.<ref name=3dprintrugger/>\n\nBuck O'Fama claims that the receiver was printed using an inexpensive, small format 3D printer, in 2 sections, and then those sections were crazy-glued together.<ref name=3dprintrugger/>\n\n==Test fire==\nIn the video the creator posted online showing a test firing of the device, the creator is shown successfully firing 30 rounds using the weapon. According to 3Dprint at the end of the video, Buck O'Fama makes the following declaration: \"You may not condone the activity, but the fact remains that we are now living in a time when deadly weapons can be printed with the push of a button. The notion that any item so easily created could be eradicated from the earth is pure fantasy. The capacity to defend my family is a fundamental human right. If you take my gun, I will simply print another one.\"<ref name=3dprintrugger/>\n\n==See also==\n*[[List of notable 3D printed weapons and parts]]\n\n==References==\n{{Reflist}}\n\n[[Category:3D printed firearms]]\n[[Category:Fused filament fabrication]]"
    },
    {
      "title": "Solid Concepts 1911 DMLS",
      "url": "https://en.wikipedia.org/wiki/Solid_Concepts_1911_DMLS",
      "text": "{{Infobox weapon\n |image= The Solid Concepts 3D printed 1911 pistol.jpg\n |caption= The Solid Concepts 3D printed 1911 pistol\n |name= Solid Concepts 1911 DMLS\n |origin=[[United States]]\n |type= [[Semi-automatic pistol]]\n |is_ranged= yes\n |service= \n |used_by= \n |wars= \n |designer= [[John Browning]]\n |design_date= 1911 (original design) / 2013 (3D printed version)\n |manufacturer= [[Solid Concepts]]\n |unit_cost=\n |production_date=2013\n |variants= \n |number= \n |spec_type= [[Semi-automatic firearm|Semi-automatic]] [[pistol]]\n |part_length=\n |cartridge= [[.45 ACP]] \n |feed= 7-round standard detachable [[box magazine]]\n |action= [[Recoil operation#Short recoil operation|Short recoil operation]]\n |velocity= \n |weight= \n |length= \n}}\nThe '''Solid Concepts 1911 DMLS'''  is a 3D-printed version of the [[M1911 pistol]].<ref name=\"theguardian.com\">[https://www.theguardian.com/technology/2013/nov/08/metal-3d-printed-gun-50-shots First metal 3D printed gun is capable of firing 50 shots], The Guardian, November 8, 2013. ([https://www.webcitation.org/6SRgjjCsK archive])</ref><ref name=\"cnsnews.com\">[http://www.cnsnews.com/news/article/barbara-hollingsworth/world-s-first-3d-printed-metal-gun-successfully-fires-600-rounds World’s First 3D Printed Metal Gun Successfully Fires 600+ Rounds], CNS News, November 13, 2013. ([https://www.webcitation.org/6SRgztc8i archive])</ref> It was made public around November 2013<ref name=\"theguardian.com\"/><ref name=\"cnsnews.com\"/> and was printed via the [[direct metal laser sintering]] (DMLS) method.<ref name=\"theguardian.com\"/><ref name=\"cnsnews.com\"/> It was created by [[Solid Concepts]].<ref name=\"theguardian.com\"/><ref name=\"cnsnews.com\"/><ref name=5gunrefs>[http://3dprint.com/14636/3d-prnted-guns/ 5 Different 3D Printed Gun Models Have Been Fired Since May, 2013 – Here They Are], 3D Print, September 10, 2014. ([https://www.webcitation.org/6SVllm7Dl archive])</ref> A Solid Concepts Browning M1911 replica fired more than 600 rounds without any apparent damage to the gun.<ref name=\"cnsnews.com\"/> The metal printer used to create the weapon cost between $500,000 to $1,000,000 at the time the gun was created as of November 2013.<ref name=\"cnsnews.com\"/> The first gun, version 1.0, is made up of 34 3D-printed [[17-4 stainless steel]] components.<ref name=\"ReferenceC\">[https://www.guns.com/news/2013/11/20/solid-concepts-3d-printed-1911-gets-version-2-0/ Solid Concepts 3D-printed 1911 gets version 2.0], Guns.com, November 20, 2013. ([https://www.webcitation.org/6SRhGR1eo archive])</ref>\n\n== Specifications ==\n[[File:Looking from the “Chamber end” of the Solid Concepts 3D printed barrel.JPG|thumb|Looking from the “Chamber end” of the Solid Concepts 3D printed barrel]]\n[[File:The ability to place text inside the barrel is possible with 3D printing.JPG|thumb|The ability to place text inside the barrel is possible with 3D printing]]\nIt weighs {{convert|2.25|lbs}} when it is empty i.e is not filled with a [[magazine (firearm)|magazine]] and the trigger pull weighs {{convert|5|lbf}}. The width is {{convert|1.3|in}} wide. The sight radius is {{convert|6.4|in}} and consists of a standard GI with a square notch rear. The ratio of the twist is 1:15.8; at 6=Lands 6=Grooves.<ref name=\"archive\">[http://www.gundigest.com/guns/handgun-reviews-articles/3d-printed-metal-gun 3D Printed Metal Gun Hitting the Market], GunDigest, January 3, 2014. ([https://www.webcitation.org/6SuvIMI0G archive])</ref> The gun used [[Inconel]] 625 (a nickel-chromium alloy) material and stainless steel via the Direct Metal Laser Sintering method.<ref name=\"archive\"/>\n\nThe Solid Concepts Browning M1911 replica, version 2.0, will be composed of 34 [[Inconel 625]] components, (not including grips). The two carbon-fiber filled nylon 12 grips were also 3D printed. Unlike early 3D printed plastic guns, the barrel of the 1911 was rifled. None of the parts machined during production and assembly took less than seven minutes once the parts have been de-supported and cleaned.<ref>[http://www.gizmag.com/worlds-first-3d-printed-gun/29702/ Solid Concepts manufactures first 3D-printed metal pistol], Gizmag, November 8, 2013. ([https://www.webcitation.org/6Suvn8ygg archive])</ref>\n\n==Printer==\nThe German EOSINT M270 Direct Metal 3D Printer used<ref name=\"thetruthaboutguns.com\">[http://www.thetruthaboutguns.com/2013/12/robert-farago/gun-review-solid-concepts-1911-dmls-direct-metal-laser-sintering/ Gun Review: Solid Concepts 1911 DMLS], Truth about guns, December 10, 2013. ([https://www.webcitation.org/6SuwL2cWJ archive])</ref> to create the weapon cost between $500,000 to $1,000,000 at the time the gun was created as of November 2013 and uses a commercial-grade power source.<ref name=\"cnsnews.com\"/> The printer requires [[argon]] and [[nitrogen]] gas<ref name=\"thetruthaboutguns.com\"/>\n\n==Capability and firing tests==\nAccording to [[Sky News]], during the initial test Solid Concepts stated: \"It functions beautifully. Our resident gun expert has fired 50 successful rounds and hit a few bull's eyes at over 30 yards (27.43 metres)\".<ref>[http://news.sky.com/story/1166303/first-3d-printed-metal-gun-fired-successfully First 3D-Printed Metal Gun Fired Successfully], Sky News, November 9, 2013. ([https://www.webcitation.org/6SuwzvcBW archive])</ref> In subsequent tests it fired more than 600 rounds without any damage to the gun.<ref name=\"cnsnews.com\"/> The chamber can handle the pressure of more than {{convert|20,000|psi}} generated when the gun is fired.<ref name=\"cnsnews.com\"/>\nThe Solid Concepts Pistol fired its 5000th round on 6 September 2014.\n\n==See also==\n*[[List of 3D printed weapons and parts]]\n\n==References==\n{{Reflist}}\n\n[[Category:3D printed firearms]]\n[[Category:.45 ACP semi-automatic pistols]]\n[[Category:1911 platform]]"
    },
    {
      "title": "The Cuomo Mag",
      "url": "https://en.wikipedia.org/wiki/The_Cuomo_Mag",
      "text": "'''The Cuomo Mag''' is a 3D printed  [[AR-15]] semi automatic rifle [[magazine (firearms)|magazine]] named after the Governor of New York, [[Andrew Cuomo]], who signed the [[NY SAFE Act]] into law banning magazines capable of holding more than 10 rounds of ammunition.<ref name=FAB1>{{cite web|last=Branson|first=Michael|title=Defense Distributed Releases Printable AK Magazine|url=http://www.thefirearmblog.com/blog/2013/04/08/defense-distributed-releases-printable-ak-magazine/|accessdate=April 12, 2013|journal=The Firearm Blog|date=April 8, 2013}}. ([https://www.webcitation.org/6SiJDDXnV archive])</ref><ref name=Wired5>{{cite news |last=Beckhusen|first=Robert |title=New 3-D Printed Rifle Magazine Lets You Fire Hundreds of Rounds |url= https://www.wired.com/dangerroom/2013/02/printed-magazine/ |accessdate=April 12, 2013 |journal=[[Wired (magazine)|Wired Danger Room]]|date=February 8, 2013}}</ref>  It was created by [[Defense Distributed]] and made public around January 2013 <ref name=Forbesmag>{{cite news |last=Greenberg|first=Andy |title=Gunsmiths 3D-Print High Capacity Ammo Clips To Thwart Proposed Gun Laws |url= https://www.forbes.com/sites/andygreenberg/2013/01/14/gunsmiths-3d-print-high-capacity-ammo-clips-to-thwart-proposed-gun-laws/ |accessdate=April 12, 2013 |journal=[[Forbes|Forbes Online]] |date=January 14, 2013}} ([https://www.webcitation.org/6SQYe3oOu archive])</ref>\n\nThe initial prototype was created using the [[Objet Geometries|Objet]] Connex26 using VeroClear printing material (a transparent material) in order to show the magazine’s round count and feeding action<ref name=\"excuomo\"/><ref name=\"archive\">[https://www.forbes.com/sites/andygreenberg/2013/05/08/3d-printed-guns-blueprints-downloaded-100000-times-in-two-days-with-some-help-from-kim-dotcom/ 3D-Printed Gun's Blueprints Downloaded 100,000 Times In Two Days (With Some Help From Kim Dotcom)], forbes.com, August 5, 2013. ([https://www.webcitation.org/6SQZHYIw5 archive])</ref> via the [[Fused deposition modeling|Fused deposition modeling (FDM)]] method.<ref>[http://www.ammoland.com/2013/03/3d-printed-ar15-magazine/#axzz3ChRscSHd A Printed AR-15 Magazine], Ammoland.com, March 05 2013. ([https://www.webcitation.org/6SQZ6Vk5X archive])</ref>\n\nThe magazine holds 30 rounds<ref name=\"Forbesmag\"/><ref name=TPM1>{{cite news |last=Franzen|first=Carl |title=Defense Distributed Unveils New 3D Printed Gun Magazine ‘Cuomo’ (VIDEO) |url=http://idealab.talkingpointsmemo.com/2013/02/defense-distributed-unveils-new-3d-printed-gun-magazine-cuomo-video.php |accessdate=April 12, 2013 |journal=[[Talking Points Memo]] |date=February 7, 2013}}</ref><ref name=Wired5/> and was able to handle enough stress to fire 227+ bullets while swapping out the barrels on the rifle to keep them cool.<ref name=Wired5/> In a test at a gun range near Austin, Texas, Defense Distributed fired a total of 342 rounds using the magazine with no issues.<ref name=Wired5/>\n\n==Initial prototype and final product==\nThe magazine was created with the [[Objet Geometries|Objet]] Connex26 with VeroClear (a transparent material) so the rounds and feeding action could be observed.<ref name=excuomo>[http://www.extremetech.com/extreme/145664-3d-printed-30-round-ar-magazine-brings-us-ever-closer-to-a-fully-3d-printed-gun 3D-printed 30-round AR magazine brings us ever closer to a fully 3D-printed gun], Extreme Tech, January 14, 2014. ([https://www.webcitation.org/6SiMRYK29 archive])</ref> It was not a success, in trials they managed to fire five rounds before the magazine failed.<ref name=excuomo/>\n\nThe main problem was feeding the gun, to solve the issue, the team added graphite to the inside of the magazine’s body, modified the catch slot, and sanded the magazine once again. The final prototype was able to handle the train to fire  50 rounds, and remained intact.<ref name=excuomo/> but Defense Distributed said that though the magazine was beginning to distort from the heat, it could last beyond 100 rounds.<ref name=excuomo/>\n\nAt the time (January 2012) prices for traditional magazines were up five times from their standard price, and it was noted that disposable 3D-printed alternatives could provide a cheaper solution for gun owners.<ref name=excuomo/>\n\nAccording to Forbes and the Wired magazine the AR-15 Cuomo magazine created by Defense Distributed which fired 227+ bullets was created using a [[Stratasys]] Dimension SST 3-D printer.<ref name=\"archive\"/><ref name=Wired5/>\n\n==See also==\n*[[List of notable 3D printed weapons and parts]]\n\n==References==\n{{Reflist}}\n\n\n\n[[Category:Magazines (firearms)]]\n[[Category:3D printed firearms]]\n[[Category:Fused filament fabrication]]\n\n==External links==\n*[https://github.com/maduce/fosscad-repo/tree/master/Rifles/AR-15_DefDist_Cuomo_30_round_magazine_v4.5 AR-15_DefDist_Cuomo_30_round_magazine_v4.5]"
    },
    {
      "title": "WarFairy P-15",
      "url": "https://en.wikipedia.org/wiki/WarFairy_P-15",
      "text": "The '''WarFairy P-15''' is a [[3D print]]ed [[Fabrique Nationale P90]] [[stock (firearms)|stock]]<ref name=\"outdoorhub.com\">[http://www.outdoorhub.com/news/2013/05/22/3d-printed-hybrid-ar-15fn-p90-lower-and-12-gauge-slugs-make-web-debut/ 3D-printed Hybrid AR-15/FN P90 Lower and 12 Gauge Slugs Make Web Debut], outdoorhub, May 22, 2013. ([https://www.webcitation.org/6SRYaP5jC archive])</ref><ref name=\"guns.com\">Slowik,Max, [http://www.guns.com/2013/06/03/meet-the-charon-family-of-3d-printable-ar-lowers-photos/ Meet the Charon Family of 3D-Printable AR Lowers (PHOTOS) \"Meet the Charon Family of 3D-Printable AR Lowers (PHOTOS)′\"], 3 June 2013.</ref><ref name=\"Slowik 2013\">Slowik,Max, [http://www.guns.com/2013/07/01/3d-printing-community-updates-liberator-with-rifle-pepperbox-and-glock-powered-shuty-9 \"3D Printing Community Updates Liberator with Rifle, Pepperbox and Glock-Powered ‘Shuty-9′\"], 1 July 2013.</ref> made public around May 2013.<ref name=\"outdoorhub.com\"/> It was printed using a [[LulzBot]] Taz printer<ref name=\"Guns 2013\">[http://www.guns.com/2013/05/21/introducing-the-warfairy-p-15-3d-printed-ar-stock/ Introducing the WarFairy P-15 3D-Printed AR Stock], Guns.com, May 21, 2013. ([https://www.webcitation.org/6SRZoLls5 archive])</ref> via the [[fused deposition modeling]] (FDM) method.<ref name=\"Charon V3\">[http://grabcad.com/library/charon-v3-1 Charon V3], grabcad, September 3, 2013. ([https://www.webcitation.org/6SRZ5hiDG archive])</ref> It was created by WarFairy<ref name=\"guns.com\"/><ref name=\"Slowik 2013\"/>\n\nThe stock works a [[lower receiver]] for the FN-P90 but would work with any standard AR.<ref name=\"Guns 2013\"/>\n\nIt is an offspring of the  [[Charon (gun)|Charon]] open source project,<ref >Shanrilivan, [http://defcad.com/forums/showthread.php?655-WarFairy-Charon-V0-2B-released!&p=3686&viewfull=1#post3686 \"WarFairy Charon V0.2B released!′\"], 29 Apr 2013.</ref> which was a 3D-printable [[AR-15]] lower receiver project that was partially inspired by the P90. It began as a design exercise by a [[DEFCAD]] user to explore FDM [[additive manufacturing]] technology as a means of integrating the P90's ergonomics into a stock for the [[AR-15]], resulting in the WarFairy P-15 stock set.<ref name=\"guns.com\"/><ref name=\"Slowik 2013\"/><ref>Leghorn, Nick, [http://www.thetruthaboutguns.com/2013/05/foghorn/coming-soon-3d-printable-ar-15-lower-with-p-90-style-stock/ \"Coming Soon: 3D Printable AR-15 Lower with P-90 Style Stock\"], 21 May 2013.</ref>\n\n==See also==\n*[[List of notable 3D printed weapons and parts]]\n\n==References==\n{{Reflist}}\n\n\n\n[[Category:3D printed firearms]]\n[[Category:Firearm components]]\n[[Category:Fused filament fabrication]]\n\n==External links==\n*[https://github.com/maduce/fosscad-repo/tree/master/Rifles/AR-15_WarFairy_P-15_Stock_v2.0-WarFairy AR-15_WarFairy_P-15_Stock_v2.0-WarFairy]"
    },
    {
      "title": "Zig zag revolver",
      "url": "https://en.wikipedia.org/wiki/Zig_zag_revolver",
      "text": "{{Infobox Weapon\n|name=Zig Zag\n|image=\n|caption=\n|origin= {{flag|Japan}}\n|type=Revolver\n<!-- Type selection -->\n|is_ranged=yes\n<!-- Service history -->\n|service=\n|used_by=\n|wars=\n<!-- Production history -->\n|designer=Yoshitomo Imura\n|design_date=2014\n|manufacturer=\n|unit_cost=\n|production_date=\n|number=\n|variants=\n<!-- General specifications -->\n|spec_label=\n|weight=\n|length=\n|part_length=\n|width=\n|height=\n<!-- Ranged weapon specifications -->\n|cartridge=[[.38 Special]]\n|action=Single-action revolver\n|rate=\n|velocity=\n|range=\n|max_range=\n|feed=6-round cylinder\n|sights=None\n}}\n\nThe '''Zig Zag revolver''' is a 3D printed .38-caliber [[pepperbox|pepperbox type revolver]] made public in May 2014.<ref name=5gunrefs>[http://3dprint.com/14636/3d-prnted-guns/ 5 Different 3D Printed Gun Models Have Been Fired Since May, 2013 – Here They Are], 3D Print, September 10, 2014. ({{webarchive |url=https://www.webcitation.org/6SVllm7Dl?url=http://3dprint.com/14636/3d-prnted-guns/ |date=2014-09-11 }})</ref><ref name=\"techcrunch.com\">[https://techcrunch.com/2014/05/08/japanese-man-arrested-for-printing-his-own-revolvers/ Japanese Man Arrested For Printing His Own Revolvers], Tech Crunch, May 8, 2014. ({{webarchive |url=https://www.webcitation.org/6SRXO2gnh?url=https://techcrunch.com/2014/05/08/japanese-man-arrested-for-printing-his-own-revolvers/ |date=2014-09-08 }})</ref><ref name=\"techcrunch.com\"/> It was created using a $500 plastic 3D-printer, however the name of the printer was not revealed by the creator.<ref name=\"techcrunch.com\"/> It was created by a Japanese citizen from [[Kawasaki, Kanagawa|Kawasaki]] named Yoshitomo Imura<ref name=\"techcrunch.com\"/> He was arrested in May 2014 after he had posted a video online of himself firing a 3D printed Zig Zag revolver.<ref name=5gunrefs/> It is the first known 3D printed gun design from Japan.<ref name=\"techcrunch.com\"/>\n\nIt holds a capacity of 6 bullets and can fire .38 caliber bullets.<ref name=\"wired.co.uk\">[https://www.wired.co.uk/news/archive/2014-05/16/3d-printed-guns 3D printed guns a year on: from prototype to serious weapons], Wired, 16 May 2014. ({{webarchive |url=https://www.webcitation.org/6SRXdTEJs?url=http://www.wired.co.uk/news/archive/2014-05/16/3d-printed-guns |date=2014-09-08 }})</ref> The grip of the weapon is based on the [[Mauser C96]] and the fact that the weapon fires from the bottom barrel is based on the [[Mateba Autorevolver]].<ref name=\"youtube.com\">https://www.youtube.com/watch?v=WeWmgIqFWw8</ref>\n\nAfter Imura's arrest a gun called the Imura Revolver was designed and printed by FOSSCAD members and was named in honor of Yoshitomo Imura.<ref name=3dprintimura>[http://3dprint.com/15556/3d-printable-gun-revolver/ A New 3D Printable Gun, The ‘Imura Revolver’ is Being Designed], September 22, 2014. ({{webarchive |url=https://www.webcitation.org/6SwYBDDCM?url=http://3dprint.com/15556/3d-printable-gun-revolver/ |date=2014-09-29 }})</ref>\n\n==Name==\nThe creator decided to call it the Zig Zag, after its ratcheted barrel modeled on the [[Mauser Zig-Zag]].<ref name=\"wired.co.uk\"/>\n\n==Assembly==\nAccording to the [[Wired (magazine)|Wired]] Magazine \"Imura assembles the handgun from plastic 3-D printed pieces, a few metal pins, screws and rubber bands, then test fires it with blanks\".<ref name=\"wired.co.uk\"/>\n\n==Operating Cycle==\nTo fire, the operator loads the rounds into the barrel, and re-attaches the barrel to the rest of the weapon.  Once reattached, the operator pulls back the slide where the firing pin is attached, pulls the trigger, which releases the slide into the round, firing the weapon.<ref name=\"youtube.com\"/>\n\n==See also==\n*[[List of notable 3D printed weapons and parts]]\n\n==References==\n{{Reflist}}\n\n[[Category:3D printed firearms]]\n[[Category:Fused filament fabrication]]"
    },
    {
      "title": "3DBenchy",
      "url": "https://en.wikipedia.org/wiki/3DBenchy",
      "text": "{{Infobox tool\n| name           = 3DBenchy\n| image          = 3DBenchy.png\n| caption        = The single material 3DBenchy model\n| image2          = A two colour 3DBenchy, Side view.png\n| caption2        = The multimaterial 3DBenchy model\n| other_name     = \n| classification = [[3D modeling|3D model]]\n| types          = \n| used_with      = [[3D printing|3D printers]]\n| inventor       = Creative Tools\n| manufacturer   = \n| model          = \n| related        = \n}}\n\nThe '''3DBenchy''' is a [[3D modeling|3D computer model]] specifically designed for testing the accuracy and capabilities of [[3D printing|3D printers]]. The 3DBenchy is described by its creator, Creative Tools, as '<nowiki/>''the jolly 3D printing torture-test'''''<nowiki/>'<nowiki/>''' and was released in April 2015 with a multi part model released in July 2015.<ref name=\":0\">{{Cite web|url=http://www.3ders.org/articles/20150720-new-dual-&-multi-part-color-version-of-3dbenchy-test-print-released.html|title=New dual- & multi-part color version of #3DBenchy test print released|language=en-US|access-date=2016-07-29}}</ref><ref name=\":1\">{{Cite web|url=http://www.3ders.org/articles/20150410-test-and-calibrate-your-3d-printer-capacities-with-the-3dbency-test-project.html|title=Test and calibrate your 3D printer's capacities with the #3DBenchy test project|language=en-US|access-date=2016-07-29}}</ref><ref name=\":2\">{{Cite web|url=https://3dprint.com/57361/3dbency-calibrating-tool/|title=Creative Tools Release #3DBenchy - The Coolest 3D Printer Calibration & Benchmarking Tool Yet|date=2015-04-10|website=3DPrint.com|language=en-US|access-date=2016-07-29}}</ref>\n==Use==\nThe 3DBenchy is often used to test and benchmark 3D printers when they are reviewed as the model includes a number of difficult to print features including; [[symmetry]], overhanging curved surfaces, smooth surfaces, planar horizontal faces, large, small and slanted holes, low-slope-surfaces, first layer details and tiny surface details.<ref>{{Cite web|url=http://www.instructables.com/id/3DBenchy-The-tool-to-calibrate-and-test-your-3D-pr/|title=#3DBenchy - The tool to calibrate and test your 3D printer|access-date=2016-07-29}}</ref>\n\nThe 3DBenchy is designed to be measured from specific points to ensure accurate printing including dimensional accuracy, warping, deviations and tolerances and it has a relatively short printing time of around 1 hour.<ref name=\":1\" /><ref name=\":2\" /> The multimaterial 3DBenchy is created for 3D printers capable of printing in multiple materials or colours, the model consists of 17 individual files which can each have different settings applied to them.<ref name=\":0\" />\n\nThe 3DBenchy is free to download and is available under a [[Creative Commons Attribution-ShareAlike License|Creative Commons Attribution-Sharealike]] license meaning it can be shared and altered by anyone.<ref>{{Cite web|url=https://www.thingiverse.com/thing:763622|title=#3DBenchy - The jolly 3D printing torture-test by CreativeTools|last=Thingiverse.com|website=www.thingiverse.com|access-date=2016-07-29}}</ref>\n==Gallery==\n<gallery mode=\"packed\">\nFile:3D-printed 3DBenchy by Creative Tools.jpg|A 3DBenchy printed on a [[fused deposition modeling]] 3D printer.\nFile:3DP Biggest Large Benchy.jpg|An extra-large (60cm x 31cm x 48cm) 3DBenchy, printed in [[Polylactic acid|PLA]], made on a large-format FDM printer. \nFile:3DBenchy printed on a resin printer.jpg|A 3DBenchy printed on a [[stereolithography]] 3D printer with support material still attached.\nFile:3DBenchy created using color mixing on an FDM printer.jpg|A multimaterial 3DBenchy created on a [[Prusa i3]] using a color mixing hot end, each part of the boat has been created in a different colour.\nFile:3DBenchy measured with tiny caliper.jpg|3DBenchy being measured with a caliper-like keyring\nFile:3 3D printed 3DBenchy models with different faults.jpg|3DBenchys printed on a [[fused deposition modeling]] 3D printer showing different faults caused by miscalibration\nFile:3DBenchy.JPG|A 3DBenchy created on a poorly calibrated fused deposition printer.\nPrinting Benchys with Hangprinter v4.webm|Two 3DBenchys being printed on a [[Hangprinter]]\n</gallery>\n\n== 3D model ==\n[[File:3DBenchy is a 3D model specifically designed for testing and benchmarking 3D printers.stl|alt=View and download the 3DBenchy STL file|none|thumb|600x600px|3DBenchy as a downloadable STL file]]\n\n== See also ==\n* [[Lenna]]\n* [[Standard test image]]\n* [[Stanford bunny]]\n* [[Utah teapot]]\n\n== References ==\n{{Reflist}}\n\n==External links==\n* {{official website}}\n* [http://www.3dbenchy.com/download/ Download 3DBenchy]\n\n[[Category:DIY culture]]\n[[Category:Computing output devices]]\n[[Category:Engineering projects]]\n[[Category:3D graphics models]]\n[[Category:Test items]]\n[[Category:Fused filament fabrication]]\n\n{{Compu-graphics-stub}}"
    },
    {
      "title": "Cornell box",
      "url": "https://en.wikipedia.org/wiki/Cornell_box",
      "text": "{{about|the graphics rendering test|the sculptures|Joseph Cornell}}\n\n[[Image:Cornell_box.png|thumb|200px|right|Standard Cornell box rendered with [[POV-Ray]]]]\n[[File:Cornell Box with 3 balls of different materials.jpg|thumb|Cornell Box with 3 balls to model how different materials reflect light.]]\n\nThe '''Cornell box''' is a test aimed at determining the accuracy of [[rendering (computer graphics)|rendering]] [[software]] by comparing the rendered scene with an actual [[photograph]] of the same scene,<ref>{{Cite journal|last=Niedenthal|first=Simon|date=2002-06-01|title=Learning from the Cornell Box|journal=Leonardo|volume=35|issue=3|pages=249–254|doi=10.1162/002409402760105235|issn=0024-094X}}</ref> and has become a [[List of common 3D test models|commonly used 3D test model]]. It was created by Cindy M. Goral, Kenneth E. Torrance, [[Donald P. Greenberg]], and Bennett Battaile at the [[Cornell University]] Program of Computer Graphics for their paper ''Modeling the Interaction of Light Between Diffuse Surfaces'' published and presented at [[SIGGRAPH]]'84.<ref>[http://www.graphics.cornell.edu/online/box/history.html History of the Cornell Box]</ref><ref>Cindy M. Goral, Kenneth E. Torrance, Donald P. Greenberg, and Bennett Battaile. ''[http://design.osu.edu/carlson/history/PDFs/goral.pdf Modeling the Interaction of Light Between Diffuse Surfaces]''. Siggraph 1984.</ref>\n\nA physical model of the box is created and photographed with a [[Charge-coupled device|CCD camera]]. The exact settings are then measured from the scene: [[emission spectrum]] of the light source, [[Reflectance#Reflectance|reflectance spectra]] of all the surfaces, exact position and size of all objects, walls, [[Light#Light_sources|light source]] and camera.\n\nThe same scene is then reproduced in the renderer, and the output file is compared with the photograph.\n\nThe basic environment consists of:\n*One light source in the center of a white ceiling\n*A [[green]] right wall\n*A [[red]] left wall\n*A [[white]] back wall\n*A white floor\n\nObjects are often placed inside the box. The first objects placed inside the environment were two white boxes. Another common version first used to test [[photon mapping]] includes two spheres: one with a perfect [[mirror]] surface and one made of [[glass]].\n\nThe physical properties of the box are designed to show ''[[Diffuse reflection|diffuse interreflection]]''. For example, some light should reflect off the red and green walls and bounce onto the white walls, so parts of the white walls should appear slightly red or green.\n\nToday, the Cornell box is often used to demonstrate renderers in a similar way as the [[Stanford bunny]] and the [[Utah teapot]] are; computer scientists often use the scene just for its visual properties without comparing it to test data from a physical model.<ref>{{Cite journal|last=Tsingos|first=N.|last2=Carlbom|first2=I.|last3=Elko|first3=G.|last4=Kubli|first4=R.|last5=Funkhouser|first5=T.|date=2002-07-01|title=Validating acoustical simulations in the Bell Labs Box|journal=IEEE Computer Graphics and Applications|volume=22|issue=4|pages=28–37|doi=10.1109/MCG.2002.1016696|issn=0272-1716}}</ref>\n\n== See also ==\n*[[3D modeling]]\n*[[Utah teapot]]\n*[[Stanford bunny]]\n*[[Stanford dragon]]\n*[[List of common 3D test models]]\n\n== References ==\n<references />\n\n==External links==\n{{Commons category|Cornell box}}\n*[http://www.graphics.cornell.edu/online/box/ The Cornell Box website]\n\n{{Standard test item}}\n\n[[Category:3D graphics models]]\n[[Category:Test items]]"
    },
    {
      "title": "List of common 3D test models",
      "url": "https://en.wikipedia.org/wiki/List_of_common_3D_test_models",
      "text": "This is a list of [[3D modeling|models]] and [[Polygon mesh|meshes]] commonly used in [[3D computer graphics]] for testing and demonstrating [[Rendering (computer graphics)|rendering]] algorithms and [[visual effects]]. Their use is important for comparing results, similarly to the way [[standard test image]]s are used in image processing.\n\n== Models by year of creation ==\n{{Expand list|date=August 2011}}\n\n{| class=\"wikitable sortable\"\n! Model name\n! Year of creation\n! Creator\n! Origin\n! Model size (vertices or triangles)\n! Creation method\n! Inspiration (if any)\n! <!-- width=\"15%\" --> | Comments\n|-\n| [[Sutherland's Volkswagen|VW Bug]]<ref name=\"vw bug\">{{cite web|title=Robert Remembers: the VW Bug|url=http://www.cs.utah.edu/docs/misc/Uteapot03.pdf|accessdate=17 Dec 2015|author=[[Robert McDermot]]|year=2003}}</ref>\n| 1972\n| [[Ivan Sutherland]]\n| [[University of Utah]]\n|\n| Measured by hand\n| [[Volkswagen Beetle]] belonging to Ivan Sutherland's wife, Marsha\n| Real car, measured by hand using [[yardstick]]s\n|-\n| [[Utah teapot]], Newell teapot\n| 1975\n| [[Martin Newell (computer scientist)|Martin Newell]]\n| [[University of Utah]]\n|\n| [[3D modeling|Modeled]]\n| [[Melitta]] teapot\n| \n|-\n| [[Cornell box]]\n| 1984\n| [[Cindy M. Goral]], [[Kenneth E. Torrance]], [[Donald P. Greenberg]], [[Bennett Battaile]]\n| [[Cornell University]]\n|\n| [[3D modeling|Modeled]]\n|\n| Many different versions of the Cornell Box exist, although one of them is considered the standard Cornell Box.\n|-\n| [[Stanford Bunny]]\n| 1993-94<ref name=\"stanford 3d scanning repository\">{{cite web|title=The Stanford 3D Scanning Repository|url=http://graphics.stanford.edu/data/3Dscanrep/|publisher=[[Stanford University]]|accessdate=17 July 2011|date=22 Dec 2010}}</ref>\n| [[Greg Turk]], [[Marc Levoy]]\n| [[Stanford University]]\n| 69,451 triangles<ref name=\"stanford 3d scanning repository\"/>\n| [[3D scanner|Scanned]]\n| [[Clay]] bunny<ref name=\"stanford bunny\">{{cite web|title=The Stanford Bunny|url=http://www.cc.gatech.edu/~turk/bunny/bunny.html|accessdate=18 July 2011|author=[[Greg Turk]]|year=2000}}</ref>\n|\n|-\n| [[Happy Buddha (3D model)|Happy Buddha]]\n| 1996<ref name=\"happy buddha\">{{cite web|title=Happy Buddha|url=http://graphics.stanford.edu/software/scanview/models/happybuddha.html|publisher=[[Stanford University]]|accessdate=17 July 2011}}</ref>\n| [[Brian Curless]], [[Marc Levoy]]<ref name=\"happy buddha\" />\n| [[Stanford University]]\n| 1,087,474 triangles and 543,524 vertices\n| [[3D scanner|Scanned]]\n| [[Budai]] [[statuette]]<ref name=\"faxing happy\">{{cite web|title=Computer model and 3D fax of Happy Buddha|url=http://graphics.stanford.edu/projects/faxing/happy/|publisher=[[Stanford University]]|author=Brian Curless and Marc Levoy|accessdate=17 July 2011|date=February 10, 1997}}</ref>\n|\n|-\n| [[Stanford Dragon]]\n| 1996<ref name=\"stanford 3d scanning repository\"/>\n|\n| [[Stanford University]]\n| 1,132,830 triangles\n| [[3D scanner|Scanned]]\n|\n| [[Chinese dragon]].\n|-\n| [[Armadillo (3D model)|Armadillo]]\n| 1996<ref name=\"stanford 3d scanning repository\"/>\n|\n| [[Stanford University]]\n| 345,944 triangles\n| [[3D scanner|Scanned]]\n|\n| [[Armadillo]] [[toy]].\n|-\n| [[Suzanne (chimpanzee model)|Suzanne]]\n| 2002\n| [[Willem-Paul van Overbruggen]]\n| [[Blender (software)]]\n| 500 faces\n| [[3D modeling|Modeled]]\n| [[Orangutan]] from the movie ''[[Jay and Silent Bob Strike Back]]''\n| Chimpanzee model; reached in [[blender (software)|blender]] by clicking ''Add'' → ''Mesh'' → ''Monkey''.\n|-\n| [[Phlegmatic Dragon (3D model)|Phlegmatic Dragon]]<ref name=\"phlegmatic dragon\">{{cite web|title=EG 2007 Phlegmatic Dragon|url=http://dcgi.felk.cvut.cz/cgg/eg07/index.php?page=dragon|publisher=[[Eurographics]] 2007|accessdate=23 July 2011|date=12 May 2011}}</ref>\n| 2007\n| See comment\n| [[Eurographics]] 2007 conference\n| original: 667,214 faces; smoothed: 480,076 faces\n| [[3D scanner|Scanned]]\n|\n| \n|-\n| [[Lucy (3D Model)|Stanford Lucy]]\n| \n| \n| [[Stanford University]]\n| 14,027,872 vertices, 28,055,742 triangles\n| [[3D scanner|Scanned]]<ref name=\"large-statue-scanner\">{{cite web|last1=Levoy|first1=Marc|title=The Stanford Large Statue Scanner|url=http://graphics.stanford.edu/projects/mich/mgantry-in-lab/mgantry-in-lab.html|publisher=[[Stanford University]]|accessdate=22 September 2014|date=November 27, 1998}}</ref>\n| \n| Scanned model of Christian angel.\n|-\n| [[Asian Dragon (3D Model)|Asian Dragon]]\n| \n| \n| [[Stanford University]]\n| 3,609,455 vertices, 7,218,906 triangles\n| [[3D scanner|Scanned]]\n| \n| A different [[Chinese dragon]].\n|-\n| [[Thai Statue (3D Model)|Thai Statue]]\n| \n| \n| [[Stanford University]]\n| Original model: 19,400,000 vertices (38,800,000 triangles); model provided: 5,000,000 vertices (10,000,000 triangles)\n| [[3D scanner|Scanned]]\n| \n| Scanned model of Thai statue\n|-\n| [[David (3D model)|David]]<ref>{{cite web|last1=Levoy|first1=Marc|title=The Digital Michelangelo Project|url=http://graphics.stanford.edu/projects/mich/|publisher=[[Stanford University]]|accessdate=22 September 2014|date=August 11, 2009}}</ref><ref name=\"mich-data\">{{cite web|last1=Levoy|first1=Marc|title=The Digital Michelangelo Project Archive of 3D Models|url=http://graphics.stanford.edu/data/mich/|publisher=[[Stanford University]]|accessdate=22 September 2014|date=August 19, 2014}}</ref>\n| \n| \n| [[Stanford University]]\n| About 1 [[1,000,000,000|billion]] polygons\n| [[3D scanner|Scanned]]<ref name=\"large-statue-scanner\"/>\n| [[Michelangelo]]'s 5-meter statue of [[David (Michelangelo)|David]]\n| Only available to established scholars and for non-commercial use only.<ref name=\"mich-data\"/>\n|-\n| [[Venus (3D Model)|Venus]]\n| \n| \n| \n| \n| \n| \n| \n|-\n| [[Fertility (3D Model)|Fertility]]\n| 2009\n| \n| AIM@SHAPE Repository (scanned at [[Utrecht University]])\n| 241,607 vertices, 483,226 triangles\n| [[3D scanner|Scanned]]\n| \n| Small statue with two joined figures. Laser scanned from a stone sculpture.\n|-\n| [[Spot (3D Model)|Spot]]\n| 2012\n| [[Keenan Crane]]\n| [[The California Institute of Technology]]\n| 2,930 vertices, 5,856 triangles\n| [[3D modeling|Modeled]]\n| \n| A spotted cow homeomorphic to a sphere. Comes with Catmull-Clark control mesh, quadrangulation, triangulation, vector texture, and bitmap texture. All meshes are manifold, genus-0 embeddings.\n|-\n| [[Wooden Elk Toy]]\n| 2000\n| [[Hans-Peter Seidel]]\n| Max-Planck-Institut fuer Informatik, Computer Graphics Group\n| \n| [[Photogrammetry]]\n| \n| Often used as an example of a non-trivial object with high genus.\n|-\n| Bust of [[Max Planck]]\n| 2001\n| [[Hans-Peter Seidel]]\n| Max-Planck-Institut fuer Informatik, Computer Graphics Group\n| \n| [[3D scanner|Scanned]]\n|\n|\n|-\n| [[3DBenchy]]\n| 2015\n| Creative Tools\n| \n| \n| \n|\n|Specifically designed for testing the accuracy and capabilities of 3D printers\n|-\n| [[Nefertiti]]\n| c. 1370 BC – c. 1330 BC\n| [[Thutmose (sculptor)]]\n| \n| approx. 2 million triangles\n| [[3D scanner|Scanned]]\n|[[Nefertiti]]\n| This archive contains a bust of the Egyptian queen [[Nefertiti]], composed of about two million triangles. The mesh was scanned by Nora Al-Badri and Jan Nikolai Nelles from the Nefertiti bust, which was created in 1345 BC by [[Thutmose (sculptor)|Thutmose]].\n|}\n\n== Gallery ==\n{{Gallery\n|lines=2 <!-- Number of lines the image text can contain before it's turned into a scroll list -->\n| File:Utah teapot simple 2.png | The [[Utah teapot]]\n| File:Cornell box.png | The [[Cornell box]]\n| <!-- [[WP:NFCC]] violation: File:Stanford Bunny.png --> | The [[Stanford bunny]]\n| File:Real Stanford Dragon.jpg | A reproduction of the [[Stanford dragon]]\n| File:Suzanne.svg | [[Suzanne (3D test model)|Suzanne]]\n| File:Spot the cow.gif | [[Spot (3D test model)|Spot]]\n}}\n\n== See also ==\n*[[Standard test image]]\n\n== References ==\n{{Reflist}}\n\n== External links ==\n{{commons category|3D test models}}\n; Standard test models\n* [http://graphics.stanford.edu/data/3Dscanrep/ The Stanford 3D Scanning Repository] hosted by the [[Stanford University]]\n* [http://www.cc.gatech.edu/projects/large_models/ Large Geometric Models Archive] hosted by the [[Georgia Institute of Technology]]\n; Other repositories\n* [http://www.sci.utah.edu/~wald/animrep/ The Utah 3D Animation Repository], a small collection of animated 3D models\n* [https://web.archive.org/web/20110725034447/http://www.pbrt.org/scenes.php scene collection], by Physically Based Rendering Toolkit: a number of interesting scenes to render with [[global illumination]]\n* [http://radsite.lbl.gov/mgf/scenes.html MGF Example Scenes], a small collection of some indoor 3D scenes\n* [http://archive3d.net/ archive3D], a collection of 3D models\n* [https://hum3d.com/ Hum3D], a collection of vehicle 3D models\n* [http://3dbar.net/ 3DBar], a collection of free 3D models\n* [http://www.nasa.gov/multimedia/3d_resources/models.html NASA 3D Models], [[NASA]] 3D models to use for educational or informational purposes\n* [http://www.ocnus.com/models/ VRML Models] from ORC Incorporated, 3D models in [[VRML]] format\n* [http://www.3drender.com/challenges/index.htm 3dRender.com: Lighting Challenges], regularly held lighting challenges, complete with scene and models for each challenge\n* [http://www.mpi-inf.mpg.de/resources/mpimodel/v1.0/ MPI Informatics Building Model], a virtual reconstruction of the [[Max Planck Institute for Informatics]] building in [[Saarbrücken]]\n* [http://shape.cs.princeton.edu/search.html Princeton shape-based 3D model search engine]\n* [http://www.cs.cmu.edu/~kmcrane/Projects/ModelRepository/ Keenan's 3D Model Repository] hosted by the [[Carnegie Mellon University]]\n\n[[Category:3D graphics models]]\n[[Category:Test items]]"
    },
    {
      "title": "Sponza Palace",
      "url": "https://en.wikipedia.org/wiki/Sponza_Palace",
      "text": "[[File:SPONZA PLACE AT NIGHT, DUBROVNIK.jpg|thumb|Sponza Palace at dusk]]\n[[File:Pati del palau Sponza de Dubrovnik.JPG|thumb|The atrium of the palace]]\nThe '''Sponza Palace''' ({{lang-hr|Palača Sponza}}), also called '''Divona''' (from ''dogana'', customs), is a 16th-century palace in [[Dubrovnik]], Croatia. Its name is derived from the Latin word \"spongia\", the spot where rainwater was collected.\n\nThe rectangular building with an inner courtyard was built in a mixed [[Gothic architecture|Gothic]] and [[Renaissance architecture|Renaissance]] style between 1516 and 1522 by [[Paskoje Miličević Mihov]]. The [[loggia]] and sculptures were crafted by the brothers Andrijić and other stonecutters. \n\nThe palace has served a variety of public functions, including as a customs office and [[bonded warehouse]], [[Mint (coin)|mint]], armoury, treasury, bank and school. It became the cultural center of the [[Republic of Ragusa]] with the establishment of the ''Academia dei Concordi'', a literary academy, in the 16th century. It survived the [[1667 Dubrovnik earthquake|1667 earthquake]] without damage. The palace's atrium served as a trading center and business meetingplace. An inscription on an arch testifies to this public function:\n\n:''Fallere nostra vetant et falli pondera. Meque pondero cum merces ponderat ipse deus.''\n:\"Our weights do not permit cheating. When I measure goods, God measures with me.\"\n\nThe palace is now home to the city archives, which hold documents dating back to the 12th century, with the earliest manuscript being from 1022. These files, including more than 7000 volumes of manuscripts and about 100,000 individual manuscripts, were previously kept in the [[Rector's Palace, Dubrovnik|Rector's palace]].\n\nThe Luža square in front of the palace is used for the opening ceremony of the [[Dubrovnik Summer Festival]]. Sponza Palace itself is also used as a [[performance]] venue.<ref>{{Cite news|url=http://urednik.dubrovacki.hr/vijesti/kultura/clanak/id/553386/monodrama-o-covjeku-i-vladaru-henrik-v-zavladao-sponzom|title=MONODRAMA O ČOVJEKU I VLADARU Henrik V. zavladao Sponzom|work=Dubrovaki vjesnik|access-date=2018-11-30|language=hr-HR}}</ref>\n\n==References==\n{{reflist}}\n{{commons category|Sponza Palace}}\n*{{cite web|title=Sponza Palace|url=http://www.dubrovnikcity.com/dubrovnik/attractions/sponza_palace.htm|publisher=City of Dubrovnik|accessdate=29 July 2014}}\n\n{{coord|42.6411|N|18.1106|E|source:wikidata|display=title}}\n\n[[Category:3D graphics models]]\n[[Category:Houses completed in 1522]]\n[[Category:Buildings and structures in Dubrovnik]]\n[[Category:Palaces in Croatia]]\n[[Category:Gothic palaces]]\n[[Category:1522 establishments in Europe]]\n[[Category:16th-century establishments in Croatia]]\n[[Category:Republic of Ragusa]]"
    },
    {
      "title": "Stanford bunny",
      "url": "https://en.wikipedia.org/wiki/Stanford_bunny",
      "text": "[[File:Computer generated render of the \"Stanford Bunny\".jpg|thumb|Computer generated render of the \"Stanford Bunny\"]]\nThe '''Stanford bunny''' is a [[computer graphics]] [[3D test model]] developed by [[Greg Turk]] and [[Marc Levoy]] in 1994 at [[Stanford University]]. The model consists of data describing 69,451 triangles determined by [[3D scanner|3D scanning]] a ceramic figurine of a [[rabbit]].<ref>{{Cite book|url = https://books.google.com/books?id=XdGVSNx4iKoC|title = Virtual Reality in Medicine|last = Riener|first = Robert|last2 = Harders|first2 = Matthias|date = 2012-04-23|publisher = Springer Science & Business Media|isbn = 9781447140115|page = 55|language = en}}</ref> This figurine and others were scanned to test methods of [[range scan]]ning physical objects.<ref>{{Cite book|last=Turk|first=Greg|last2=Levoy|first2=Marc|date=1994-01-01|title=Zippered Polygon Meshes from Range Images|journal=Proceedings of the 21st Annual Conference on Computer Graphics and Interactive Techniques|series=SIGGRAPH '94|location=New York, NY, USA|publisher=ACM|pages=311–318|doi=10.1145/192161.192241|isbn=978-0897916677}}</ref>\n\nThe data can be used to test various graphics algorithms, including polygonal simplification, compression, and surface smoothing. There are a few complications with this dataset that can occur in any 3D scan data. The model is [[manifold]] connected and has holes in the data, some due to scanning limits and some due to the object being hollow.<ref>{{Cite web|url=http://www.cc.gatech.edu/~turk/bunny/bunny.html|title=The Stanford Bunny|last=Turk|first=Mark|date=August 2000|website=www.cc.gatech.edu|access-date=2016-04-06}}</ref> These complications provide a more realistic input for any algorithm that is benchmarked with the Stanford bunny,<ref>{{Cite book|last=Lindstrom|first=P.|last2=Turk|first2=G.|date=1998-10-01|title=Fast and memory efficient polygonal simplification|journal=Visualization '98. Proceedings|pages=279–286|doi=10.1109/VISUAL.1998.745314|isbn=978-0-8186-9176-8|citeseerx=10.1.1.452.5851}}</ref> though by today's standards in terms of geometric complexity and triangle count, it is considered a simple model.\n\nThe model was originally available in [[PLY (file format)|.ply]] (polygons) file format with 4 different resolutions, 69,451 polygons being the highest.\n\n==See also==\n{{Portal|Rabbits and hares}}\n*[[3D modeling]]\n*[[Stanford dragon]]\n*[[Utah teapot]]\n*[[Suzanne (3D model)]]\n*[[Cornell box]]\n*[[List of common 3D test models]]\n\n==References==\n<references />\n\n==External links==\n* [http://graphics.stanford.edu/data/3Dscanrep/#bunny The Stanford 3D Scanning Repository] provides the Stanford bunny model for download.\n\n{{Standard test item}}\n\n[[Category:3D graphics models]]\n[[Category:Test items]]\n[[Category:Fictional hares and rabbits]]\n[[Category:1994 works]]\n\n\n{{compu-graphics-stub}}"
    },
    {
      "title": "Stanford dragon",
      "url": "https://en.wikipedia.org/wiki/Stanford_dragon",
      "text": "[[Image:Real Stanford Dragon.jpg|thumb|right|A reproduction of the dragon made with a [[rapid prototyping]] machine]]\n\nThe '''Stanford dragon''' is a [[computer graphics]] [[3D test model]] created with a Cyberware 3030 Model Shop (MS) Color [[3D Scanner]] at [[Stanford University]].\n\nThe ''dragon'' consists of data describing 871,414 triangles<ref group=\"note\">Although the Stanford web page says that it has 1,132,830 triangles, the actual face count is 871,414 in the .ply file.</ref><ref>{{Cite book|url=https://books.google.com/books?id=oKEGGMgnWKcC|title=Topology for Computing|last=Zomorodian|first=Afra J.|date=2005-01-10|publisher=Cambridge University Press|isbn=9781139442633|page=3|language=en}}</ref> determined by [[3D scanner|3D scanning]] a real figurine. The data set is [[List of common 3D test models|often used]] to test various graphics algorithms, including polygonal simplification, compression, and surface smoothing.<ref>{{Cite book|url=https://books.google.com/books?id=QjptCQAAQBAJ|title=Computational Science and Its Applications - ICCSA 2003: International Conference, Montreal, Canada, May 18-21, 2003, Proceedings|last=Kumar|first=Vipin|last2=Gavrilova|first2=Marina L.|last3=Tan|first3=C. J. Kenneth|last4=L'Ecuyer|first4=Pierre|date=2003-08-03|publisher=Springer|isbn=9783540448426|page=290|language=en}}</ref> It first appeared in 1996.\n\nThe model is available in different file formats (.ply, [[vrml]], vl, ...) on the Internet for free.\n\n==See also==\n*[[List of common 3D test models]]\n*[[Stanford bunny]]\n\n==Notes==\n{{reflist|group=\"note\"}}\n\n== References ==\n<references />\n\n==External links==\n* [http://graphics.stanford.edu/data/3Dscanrep/ The Stanford 3D Scanning Repository] provides the Stanford dragon model for download\n* [http://www-static.cc.gatech.edu/projects/large_models/ Large Geometric Models Archive] at [[Georgia Institute of Technology|Georgia Tech]] provides the [http://www-static.cc.gatech.edu/projects/large_models/dragon.html Stanford dragon model] for download in standard file formats\n* [http://www.mrbluesummers.com/3572/downloads/stanford-dragon-model MrBluesummers.com 3dsMax Resources] Download the Stanford dragon model in OBJ format. (more common than .PLY)\n\n{{Standard test item}}\n\n{{compu-graphics-stub}}\n\n[[Category:3D graphics models]]\n[[Category:Test items]]\n[[Category:Dragons in art]]"
    },
    {
      "title": "Sutherland's Volkswagen",
      "url": "https://en.wikipedia.org/wiki/Sutherland%27s_Volkswagen",
      "text": "{{short description|3D test model}}\n'''Sutherland's Volkswagen''', or the '''Utah VW Bug''', is a [[3D computer graphics|3D]] [[List of common 3D test models|test model]] like the [[Utah teapot]]. It is a [[mathematical model]] of a 1967 [[Volkswagen Beetle]].\n\nThe [[Volkswagen]] model was created by students<ref>{{Cite web|url=https://excelsior.asc.ohio-state.edu/~carlson/history/tree/images/pages/vw_JPG.htm|title=vw.JPG|website=excelsior.asc.ohio-state.edu|access-date=2018-04-12}}</ref> of Professor [[Ivan Sutherland]] in 1972 <ref>{{Cite web|url=http://www.cs.utah.edu/docs/misc/Uteapot03.pdf|title=Robert Remembers: the VW Bug|last=McDermott|first=Robert|date=2003|website=|archive-url=|archive-date=|dead-url=|access-date=}}</ref> at the [[University of Utah]].\n\n== See also ==\n\n* [[3D modeling]]\n* [[Utah teapot]]\n* [[Stanford bunny]]\n* [[Stanford dragon]]\n* [[Blender (software)#Suzanne|Suzanne]]\n* [[Cornell box]]\n* [[List of common 3D test models]]\n*\n\n== References ==\n<references />\n\n== External links ==\n*{{cite web|url=https://excelsior.asc.ohio-state.edu/~carlson/history/tree/images/pages/vw_JPG.htm|website=excelsior.asc.ohio-state.edu|title=vw.JPG|accessdate=2018-04-12}}\n*{{cite web|url=https://jalopnik.com/the-first-real-object-ever-3d-scanned-and-rendered-was-494241353|website=jalopnik.com|title=The First Real Object Ever 3D Scanned And Rendered Was A VW Beetle|accessdate=2018-04-12}}\n*{{cite web|url=http://www.computerhistory.org/revolution/computer-graphics-music-and-art/15/206|title=The Utah Teapot - CHM Revolution|website=computerhistory.org|accessdate=2018-04-12}}\n*{{cite web|url=http://www.computerhistory.org/revolution/computer-graphics-music-and-art/15/206/561|title=David Evans and Ivan Sutherland - CHM Revolution|website=computerhistory.org|accessdate=2018-04-12}}\n*{{cite web|url=http://www.computerhistory.org/revolution/computer-graphics-music-and-art/15/206/560|title=Mapping Sutherland’s Volkswagen - CHM Revolution|website=computerhistory.org|accessdate=2018-04-12}}\n*{{cite web|url=http://www.computerhistory.org/revolution/computer-graphics-music-and-art/15/206/558|title=Wireframe model of Sutherland’s Volkswagen - CHM Revolution|website=computerhistory.org|accessdate=2018-04-12}}\n* http://www.computerhistory.org/revolution/computer-graphics-music-and-art/15/206/559\n\n\n\n[[Category:3D graphics models]]\n[[Category:Test items]]"
    },
    {
      "title": "Utah teapot",
      "url": "https://en.wikipedia.org/wiki/Utah_teapot",
      "text": "[[File:Utah teapot simple 2.png|thumb|right|A modern rendering of the Utah teapot model]]\nThe '''Utah teapot''', or the '''Newell teapot''', is a [[3D computer graphics|3D]] [[List of common 3D test models|test model]] that has become a [[List of common 3D test models|standard reference object]] and an [[in-joke]]<ref>{{cite web|url=http://nautil.us/blog/the-most-important-object-in-computer-graphics-history-is-this-teapot|title=The Most Important Object In Computer Graphics History Is This Teapot|publisher=''[[Nautilus (science magazine)|Nautilus]]''|last=Dunietz|first=Jesse|date=February 29, 2016|accessdate=March 3, 2019}}</ref> within the [[computer graphics]] community. It is a [[mathematical model]] of an ordinary [[teapot]] that appears solid, cylindrical, and partially convex. A teapot primitive is considered the equivalent of a \"[[Hello, World]]\" program, as a way to create an easy 3D scene with a somewhat complex model acting as a basic geometry reference for scene and light setup. Some [[Library (computing)|programming libraries]], such as the [[OpenGL Utility Toolkit]],<ref>{{cite web |website=www.opengl.org |author=Mark Kilgard |date=Feb 23, 1996 | url=http://www.opengl.org/resources/libraries/glut/spec3/node89.html | title=11.9 glutSolidTeapot, glutWireTeapot | accessdate=October 7, 2011}}</ref> even have [[Subroutine|functions]] dedicated to drawing teapots.\n\nThe teapot model was created in 1975 by early computer graphics researcher [[Martin Newell (computer scientist)|Martin Newell]], a member of the pioneering graphics program at the [[University of Utah]].<ref>{{Cite book\n | last = Torrence | first = Ann\n | doi = 10.1145/1180098.1180128\n | id = Article No. 29\n | isbn = 978-1-59593-364-5\n | title = ACM SIGGRAPH 2006 Teapot \n | pages = 29\n | year = 2006\n | chapter = Martin Newell's original teapot}}</ref>\n\n==History==\n{{ external media\n| float  = right\n| image1 = [http://www.computerhistory.org/revolution/computer-graphics-music-and-art/15/206/556 A scan of the original diagram Martin Newell drew up, to plan the Utah Teapot before inputing it digitally.]<br />Image courtesy of Computer History Museum.\n}}\nFor his work, Newell needed a simple mathematical model of a familiar object. His wife, Sandra Newell, suggested modelling their [[tea service]] since they were sitting down for tea at the time. He sketched the teapot free-hand using graph paper and a pencil.<ref>{{cite web|title=The Utah Teapot - CHM Revolution|url=http://www.computerhistory.org/revolution/computer-graphics-music-and-art/15/206|website=Computer History Museum|accessdate=20 March 2016}}</ref> Following that, he went back to the computer laboratory and edited [[bézier curve|bézier]] control points on a [[Tektronix]] [[storage tube]], again by hand. {{Citation needed|date=December 2008}}\n\n[[File:Original Utah Teapot.jpg|thumb|The actual [[Melitta]] teapot that [[Martin Newell (computer graphics)|Martin Newell]] modelled, displayed at the [[Computer History Museum]] in [[Mountain View, California]] (1990–present)]]\n\nThe teapot shape contained a number of elements that made it ideal for the graphics experiments of the time: it was round, contained [[saddle point]]s, had a [[Genus (mathematics)|genus]] greater than zero because of the hole in the handle, could project a shadow on itself, and could be displayed accurately without a surface texture.\n\nNewell made the mathematical data that described the teapot's geometry (a set of three-dimensional [[coordinates]]) publicly available, and soon other researchers began to use the same data for their computer graphics experiments. These researchers needed something with roughly the same characteristics that Newell had, and using the teapot data meant they did not have to laboriously enter geometric data for some other object. Although technical progress has meant that the act of rendering the teapot is no longer the challenge it was in 1975, the teapot continued to be used as a reference object for increasingly advanced graphics techniques.\n\nOver the following decades, editions of computer graphics journals (such as the [[Association for Computing Machinery|ACM]] [[SIGGRAPH]]'s quarterly) regularly featured versions of the teapot: faceted or smooth-shaded, wireframe, bumpy, translucent, refractive, even leopard-skin and furry teapots were created.\n\nHaving no surface to represent its base, the original teapot model was not intended to be seen from below. Later versions of the [[data set]] fixed this.\n\nThe real teapot is ~33% taller (ratio 4:3){{citation needed|date=November 2011}} than the computer model. [[Jim Blinn]] stated that he scaled the model on the vertical axis during a demo in the lab to demonstrate that they could manipulate it. They preferred the appearance of this new version and decided to save the file out of that preference.<ref>{{cite web|last1=Seymour|first1=Mike|title=Founders Series: Industry Legend Jim Blinn |url=http://www.fxguide.com/featured/founders-series-industry-legend-jim-blinn/|website=fxguide.com|accessdate=15 April 2015|archiveurl=https://web.archive.org/web/20120729175832/http://www.fxguide.com/featured/founders-series-industry-legend-jim-blinn|archivedate=2012-07-29|date=2012-07-25}}</ref>\n\nThe original, physical teapot was purchased from [[Zions Co-operative Mercantile Institution|ZCMI]] (a department store in [[Salt Lake City]]) in 1974. It was donated to the [[The Computer Museum, Boston|Boston Computer Museum]] in 1984 where it was on display until 1990. It now resides in the ephemera collection at the [[Computer History Museum]] in [[Mountain View, California]] where it is catalogued as \"Teapot used for Computer Graphics rendering\" and bears the catalogue number X00398.1984.<ref>{{cite web |title=Original Utah Teapot at the Computer History Museum |url=http://www.computerhistory.org/collections/accession/X398.84 |publisher=The Computer History Museum |archiveurl=http://www.computerhistory.org/collections/accession/X398.84 |archivedate=2012-08-20 |date=2001-09-28}}</ref> Versions of the teapot are still sold today by Friesland Porzellan in Germany,<ref name=Teekanne/> who were the original makers of the teapot as they were once part of the Melitta Group.<ref name=FrieslandPorzel/>\n\nVersions of the teapot model — or sample scenes containing it — are distributed with or freely available for nearly every current rendering and modelling program and even many graphic [[API]]s, including [[AutoCAD]], [[Houdini (software)|Houdini]], [[Lightwave 3D]], [[Modo (software)|MODO]], [[POV-Ray]], [[3ds Max]], and the [[OpenGL]] and [[Direct3D]] helper libraries. Some [[RenderMan Interface Specification|RenderMan]]-compliant [[rendering (computer graphics)|renderers]] support the teapot as a built-in geometry by calling <code>RiGeometry(\"teapot\", RI_NULL)</code>. Along with the expected cubes and spheres, the [[OpenGL Utility Toolkit|GLUT]] library even provides the function <code>glutSolidTeapot()</code> as a graphics primitive, as does its [[Direct3D]] counterpart [[D3DX]] (<code>D3DXCreateTeapot()</code>). However version 11 of DirectX does not provide this functionality anymore. Mac OS X Tiger and Leopard also include the teapot as part of [[Quartz Composer]]; Leopard's teapot supports [[bump mapping]]. [[BeOS]] included a small demo of a rotating 3D teapot, intended to show off the platform's multimedia facilities.\n\nTeapot scenes are commonly used for renderer self-tests and benchmarks.<ref>{{cite journal |last=Wald |first=Ingo |first2=Carsten |last2=Benthin |first3=Philipp |last3=Slusallek |title=A Simple and Practical Method for Interactive Ray Tracing of Dynamic Scenes |journal=Technical Report, Computer Graphics Group |year=2002 |url=http://graphics.cg.uni-saarland.de/fileadmin/cguds/papers/2002/DynRT/DynamicRayTracing.pdf |publisher=Saarland University |archiveurl=https://web.archive.org/web/20120323191204/http://graphics.cg.uni-saarland.de/fileadmin/cguds/papers/2002/DynRT/DynamicRayTracing.pdf |archivedate=2012-03-23}}</ref><ref>{{cite journal |last=Klimaszewski |first=K. |last2=Sederberg |first2=T.W. |title=Faster ray tracing using adaptive grids |journal=IEEE Computer Graphics and Applications |year=1997 |volume=17 |issue=1 |pages=42–51 |doi=10.1109/38.576857 }}</ref>\n\n==Appearances==\n[[File:The Six Platonic Solids.png|thumb|right|\"The Six Platonic Solids\", an image that includes the Utah teapot as among the standard [[Platonic solid]]s]]\nOne famous [[Ray tracing (graphics)|ray-traced]] image, by [[James Arvo]] and David Kirk in 1987,<ref name=\"Arvo_1987\">{{cite journal |first1= James |last1= Arvo |first2= David |last2= Kirk |title= Fast ray tracing by ray classification |year= 1987 |journal= [[SIGGRAPH]] |volume= 21 |issue= 4 |pages= 55–64 |doi= 10.1145/37402.37409 }}</ref> shows six stone columns, five of which are surmounted by the [[Platonic solid]]s ([[tetrahedron]], [[cube]], [[octahedron]], [[dodecahedron]], [[icosahedron]]).  The sixth column supports a teapot.<ref>{{cite web|last1=Carlson|first1=Wayne|title=A Critical History of Computer Graphics and Animation |url=http://design.osu.edu/carlson/history/lesson20.html |publisher=OSU.edu |accessdate=15 April 2015 |archiveurl=https://web.archive.org/web/20120212183002/https://design.osu.edu/carlson/history/lesson20.html |archivedate=2012-02-12 |date=2007}}</ref> The image is titled \"The Six Platonic Solids\", with Arvo and Kirk calling the teapot \"the newly discovered Teapotahedron\".<ref name=\"Arvo_1987\"/> This image appeared on the covers of several books and computer graphic journals.\n\nThe Utah teapot sometimes appears in the \"Pipes\" [[screensaver]] shipped with [[Microsoft Windows]],<ref>{{cite web |url=http://www.eeggs.com/items/493.html |title=Windows NT Easter Egg – Pipes Screensaver |website=The Easter Egg Archive |access-date=May 5, 2018}}</ref> but only in versions prior to Windows XP, and has been included<ref>{{cite web |url=http://www.jwz.org/xscreensaver/changelog.html |title=Xscreensaver changelog}}</ref> in the \"polyhedra\" [[XScreenSaver]] hack since 2008.\n\n[[Jim Blinn]] (in one of his \"[[Project MATHEMATICS!]]\" videos) proves an amusing (but [[Trivial (mathematics)|trivial]]) version of the [[Pythagorean theorem]]: Construct a (2D) teapot on each side of a [[right triangle]] and the area of the teapot on the [[hypotenuse]] is equal to the sum of the areas of the teapots on the other two sides.<ref>{{cite web |title=Project Mathematica: Theorem Of Pythagoras |date=1988 |url=https://archive.org/details/theorem_of_pythagoras |website=archive.org |author=NASA |accessdate=28 July 2015|at=14:00}}</ref>\n\n[[Loren Carpenter]]'s 1980 CGI film ''Vol Libre'' features the teapot, appearing briefly at the beginning and end of the film in the foreground with a fractal-rendered mountainscape behind it.\n\n[[Vulkan (API)|Vulkan]] and [[OpenGL]] graphics APIs feature Utah teapot along with [[Stanford Dragon]] and [[Stanford Bunny]] on their badges.<ref>{{cite web |title=Vulkan Overview - The Khronos Group Inc |url=https://web.archive.org/web/20180622100144/https://www.khronos.org/vulkan/ |website=archive.org}}</ref>\n\n==In popular culture==\nWith the advent of the first computer generated short films and proceeding full-length feature films, it has become an [[in-joke]] to hide the Utah teapot in one of the film's scenes.<ref>{{cite journal |title=Tempest in a Teapot |journal=Continuum Magazine |date=Winter 2006–2007 |url=http://continuum.utah.edu/back_issues/2006winter/teapot.html |archiveurl=https://web.archive.org/web/20140712192931/http://continuum.utah.edu/back_issues/2006winter/teapot.html |archivedate=2014-07-12}}</ref> For example, in the movie ''[[Toy Story]]'', the Utah teapot appears in a short tea-party scene. The teapot also appears in ''[[The Simpsons]]'' episode \"[[Treehouse of Horror VI]]\" in which Homer discovers the \"third dimension.\"<ref>{{cite web |last1=Groening |first1=Matt |title=Pacific Data Images on ''Homer3'' |url=http://www.mathsci.appstate.edu/~sjg/math/pacificdatahomer3d.html |archiveurl=https://web.archive.org/web/20001018104845/http://www.mathsci.appstate.edu/~sjg/math/pacificdatahomer3d.html |archivedate=2000-10-18}}</ref> It also appears in [[Pixar]]'s ''[[Monsters, Inc.]]'' on the table in Boo's bedroom. In ''[[The Sims 2]]'', a picture of the Utah teapot is one of the paintings available to buy in-game, titled \"Handle and Spout\".\n\n==3D printing==\n[[File:Utah teapot (solid).stl|thumb|A 3D [[STL (file format)|STL]] model of the teapot]]\nThrough [[3D printing]], the Utah Teapot has come full circle from being a computer model based on an actual teapot to being an actual teapot based on the computer model. It is widely available in many renderings in different materials from small plastic knick-knacks to a fully functional ceramic teapot. It is sometimes intentionally rendered as a blocky, low poly object to celebrate its origin as a computer model.{{citation needed|date=November 2014}}\n\nIn 2009, a Belgian design studio, Unfold, 3D printed the Utah Teapot in ceramic with the objective of returning the iconographic teapot to its roots as a piece of functional dishware while showing its status as an icon of the digital world.<ref>{{cite web|title=Utanalog, Ceramic Utah Teapot |url=http://unfold.be/pages/utanalog |website=Unfold |date=October 28, 2009 |accessdate=12 May 2015}}</ref>\n\nIn 2015, the California-based company and self-described \"Make-Tank\", Emerging Objects, followed suit, but this time printed the teapot, along with teacups and teaspoons, out of actual tea.<ref>{{cite web |title=The Utah Tea Set |url=http://www.emergingobjects.com/projects/the-utah-tea-set |website=Emerging Objects |accessdate=12 May 2015}}</ref>\n\n==Original teapot model==\nThe original teapot the Utah teapot was based on is still available from Friesland Porzellan.<ref name=FrieslandPorzel>{{cite tweet |user=FrieslandPorzel |author=Friesland Porzellan |number=845221850900762625 |date=2017-03-24 |title=The original Utah Teapot was always produced by Friesland. We were part of the Melitta Group once, thats right. Got yours already?}}</ref> Originally it was plainly called \"Haushaltsteekanne\" (Household Teapot);<ref name=RadioBremen>{{cite web|language=de|title=Eine Teekanne als Filmstar|url=https://www.radiobremen.de/bremenzwei/rubriken/reportagen/utah-teapot100.html|publisher=Radio Bremen|access-date=March 1, 2019}}</ref> the company only found out about their product's 'fame' in 2017, whereupon they officially renamed it \"Utah Teapot\". It is available in three different sizes, the one Martin Newell had used is the \"1,4L Utah Teapot\".<ref name=Teekanne>{{cite web |language=de |title=Teekanne 1,4l Weiß Utah Teapot |url=https://frieslandversand.de/teekanne-1-4l-weiss-utah-teapot?number=1209034011 |publisher=Friesland Versand GmbH |access-date=May 5, 2018}}</ref>\n\n==Gallery==\n<gallery mode=\"packed\">\nImage:utah teapot.png|The Utah teapot\nImage:Environment mapping.png|The teapot demonstrating [[environment mapping]]\nFile:Siggraph 2013 Teapot (pot).JPG|[[SIGGRAPH]] 2013 teapot from Pixar\n</gallery>\n\n==See also==\n*[[3D modeling]]\n*[[Stanford bunny]]\n*[[Stanford dragon]]\n*[[Suzanne (3D model)]]\n*[[Cornell box]]\n*[[List of common 3D test models]]\n*[[List of filmmaker's signatures]]\n*[[Lenna]]\n\n==References==\n{{Reflist|30em}}\n\n==External links==\n{{commons category}}\n* [http://www.computerhistory.org/collections/accession/102710359 Image of Utah teapot at the Computer History Museum]\n* [http://www.computerhistory.org/revolution/computer-graphics-music-and-art/15/206 Newell's teapot sketch at the Computer History Museum]\n* [http://www.sjbaker.org/wiki/index.php?title=The_History_of_The_Teapot S.J. Baker's History of the teapot], including patch data\n*[https://web.archive.org/web/20160821122527/http://design.osu.edu/carlson/history/lesson20.html Teapot history and images, from A Critical History of Computer Graphics and Animation]\n* [https://www.youtube.com/watch?t=6&v=DxMfblPzFNc History of the Teapot video from Udacity's on-line Interactive 3D Graphics course]\n* [http://www.realtimerendering.com/udacity/?load=demo/unit1-teapot-demo.js WebGL teapot demonstration]\n* [https://www.youtube.com/watch?v=TIxt9guMbXo The World's Most Famous Teapot] - Tom Scott explains the story of Martin Newell's digital creation (YouTube)\n{{Standard test item}}\n\n{{DEFAULTSORT:Utah Teapot}}\n[[Category:3D graphics models]]\n[[Category:Test items]]\n[[Category:Teapots]]\n[[Category:In-jokes]]"
    },
    {
      "title": "Arbitrary-precision arithmetic",
      "url": "https://en.wikipedia.org/wiki/Arbitrary-precision_arithmetic",
      "text": "{{More citations needed|date=July 2007}}\nIn [[computer science]], '''arbitrary-precision arithmetic''', also called '''bignum arithmetic''', multiple-precision arithmetic, or sometimes '''infinite-precision arithmetic''', indicates that [[calculation]]s are performed on numbers whose [[numerical digit|digits]] of [[precision (arithmetic)|precision]] are limited only by the available [[memory (computers)|memory]] of the host system.  This contrasts with the faster fixed-precision arithmetic found in most [[arithmetic logic unit]] (ALU) hardware, which typically offers between 8 and 64 [[bit]]s of precision.\n\nSeveral modern [[programming language]]s have built-in support for bignums, and others have libraries available for arbitrary-precision [[integer]] and [[floating-point]] math.  Rather than store values as a fixed number of binary [[bit]]s related to the size of the [[processor register]], these implementations typically use variable-length [[array data structure|arrays]] of digits.\n\nArbitrary precision is used in applications where the speed of [[arithmetic]] is not a limiting factor, or where [[Floating point error mitigation|precise results]] with very large numbers are required.  It should not be confused with the [[symbolic computation]] provided by many [[computer algebra system]]s, which represent numbers by expressions such as {{math|''π''·sin(2)}}, and can thus ''represent'' any [[computable number]] with infinite precision.\n\n==Applications==\n\nA common application is [[public-key cryptography]], whose algorithms commonly employ arithmetic with integers having hundreds of digits.<ref>{{cite web |url=https://arstechnica.com/news.ars/post/20070523-researchers-307-digit-key-crack-endangers-1024-bit-rsa.html |title=Researchers: 307-digit key crack endangers 1024-bit RSA |author=Jacqui Cheng |date=May 23, 2007}}</ref><ref>{{cite web|url=http://www.rsa.com/rsalabs/node.asp?id%3D2218 |title=Archived copy |accessdate=2012-03-31 |deadurl=yes |archiveurl=https://web.archive.org/web/20120401144624/http://www.rsa.com/rsalabs/node.asp?id=2218 |archivedate=2012-04-01 |df= }} recommends important RSA keys be 2048 bits (roughly 600 digits).</ref>  Another is in situations where artificial limits and [[arithmetic overflow|overflows]] would be inappropriate.  It is also useful for checking the results of fixed-precision calculations, and for determining the optimum value for coefficients needed in formulae.\n\nArbitrary precision arithmetic is also used to compute fundamental [[mathematical constant]]s such as [[pi|π]] to millions or more digits and to analyze the properties of the digit strings<ref>{{cite journal |author=R. K. Pathria |authorlink=Raj Pathria |title=A Statistical Study of the Randomness Among the First 10,000 Digits of Pi |year=1962 |journal=Mathematics of Computation |volume=16 |issue=78 |pages=188–197 |url=http://www.ams.org/journals/mcom/1962-16-078/S0025-5718-1962-0144443-7/ |accessdate=2014-01-10 |doi=10.1090/s0025-5718-1962-0144443-7}} A quote example from this article: \"Such an extreme pattern is dangerous even if diluted by one of its neighbouring blocks\"; this was the occurrence of the sequence 77 twenty-eight times in one block of a thousand digits.</ref> or more generally to investigate the precise behaviour of functions such as the [[Riemann zeta function]] where certain questions are difficult to explore via analytical methods. Another example is in rendering [[fractal]] images with an extremely high magnification, such as those found in the [[Mandelbrot set]].\n\nArbitrary-precision arithmetic can also be used to avoid [[arithmetic overflow|overflow]], which is an inherent limitation of fixed-precision arithmetic.  Similar to a 5-digit [[odometer]]'s display which changes from 99999 to 00000, a fixed-precision integer may exhibit ''[[Integer overflow|wraparound]]'' if numbers grow too large to represent at the fixed level of precision.  Some processors can instead deal with overflow by ''[[saturation arithmetic|saturation]],'' which means that if a result would be unrepresentable, it is replaced with the nearest representable value.  (With 16-bit unsigned saturation, adding any positive amount to 65535 would yield 65535.)  Some processors can generate an [[exception handling|exception]] if an arithmetic result exceeds the available precision. Where necessary, the exception can be caught and recovered from—for instance, the operation could be restarted in software using arbitrary-precision arithmetic.\n\nIn many cases, the task or the programmer can guarantee that the integer values in a specific application will not grow large enough to cause an overflow. Such guarantees may be based on pragmatic limits: a school attendance program may have a task limit of 4,000 students. A programmer may design the computation so that intermediate results stay within specified precision boundaries.\n\nSome programming languages such as [[Lisp (programming language)|Lisp]], [[Python (programming language)|Python]], [[Perl]], [[Haskell (programming language)|Haskell]] and [[Ruby (programming language)|Ruby]] use, or have an option to use, arbitrary-precision numbers for ''all'' integer arithmetic.  Although this reduces performance, it eliminates the possibility of incorrect results (or exceptions) due to simple overflow.  It also makes it possible to guarantee that arithmetic results will be the same on all machines, regardless of any particular machine's [[Word (data type)|word size]].  The exclusive use of arbitrary-precision numbers in a programming language also simplifies the language, because ''a number is a number'' and there is no need for multiple types to represent different levels of precision.\n\n==Implementation issues==\n\nArbitrary-precision arithmetic is considerably slower than arithmetic using numbers that fit entirely within processor registers, since the latter are usually implemented in [[Arithmetic logic unit|hardware arithmetic]] whereas the former must be implemented in software.  Even if the [[computer]] lacks hardware for certain operations (such as integer division, or all floating-point operations) and software is provided instead, it will use number sizes closely related to the available hardware registers: one or two words only and definitely not N words.    There are exceptions, as certain ''[[variable word length machine|variable word length]]'' machines of the 1950s and 1960s, notably the [[IBM 1620]], [[IBM 1401]] and the Honeywell ''Liberator'' series, could manipulate numbers bound only by available storage, with an extra bit that delimited the value.\n\nNumbers can be stored in a [[fixed-point arithmetic|fixed-point]] format, or in a [[floating-point]] format as a [[significand]] multiplied by an arbitrary exponent.  However, since division almost immediately introduces infinitely repeating sequences of digits (such as 4/7 in decimal, or 1/10 in binary), should this possibility arise then either the representation would be truncated at some satisfactory size or else rational numbers would be used: a large integer for the [[numerator]] and for the [[denominator]]. But even with the [[greatest common divisor]] divided out, arithmetic with rational numbers can become unwieldy very quickly: 1/99 − 1/100 = 1/9900, and if 1/101 is then added, the result is 10001/999900.\n\nThe size of arbitrary-precision numbers is limited in practice by the total storage available, the variables used to index the digit strings, and computation time. A 32-bit operating system may limit available storage to less than 4&nbsp;GB.<!-- could make thrashing statement for numbers larger than physical RAM --> A programming language using 32-bit integers can only index 4&nbsp;GB. If multiplication is done with an {{math|[[Big O notation|O]](''N''<sup>2</sup>)}} algorithm, it would take on [[Order of approximation|the order of]] {{math|10<sup>12</sup>}} steps to multiply two one-million-word numbers.\n\nNumerous [[algorithms]] have been developed to efficiently perform arithmetic operations on numbers stored with arbitrary precision.  In particular, supposing that {{math|''N''}} digits are employed, algorithms have been designed to minimize the asymptotic [[Computational complexity theory|complexity]] for large {{math|''N''}}.\n\nThe simplest algorithms are for [[addition]] and [[subtraction]], where one simply adds or subtracts the digits in sequence, carrying as necessary, which yields an {{math|O(''N'')}} algorithm (see [[big O notation]]).\n\n[[Comparison (computer programming)|Comparison]] is also very simple. Compare the high-order digits (or machine words) until a difference is found. Comparing the rest of the digits/words is not necessary. The worst case is {{math|O(''N'')}}, but usually it will go much faster.\n\nFor [[multiplication]], the most straightforward algorithms used for multiplying numbers by hand (as taught in primary school) require {{math|O(''N''<sup>2</sup>)}} operations, but [[multiplication algorithm]]s that achieve {{math|O(''N''&nbsp;log(''N'')&nbsp;log(log(''N'')))}} complexity have been devised, such as the [[Schönhage–Strassen algorithm]], based on [[fast Fourier transform]]s, and there are also algorithms with slightly worse complexity but with sometimes superior real-world performance for smaller {{math|''N''}}. The [[Karatsuba algorithm|Karatsuba]] multiplication is such an algorithm.\n\nFor [[Division (mathematics)|division]], see [[division algorithm]].\n\nFor a list of algorithms along with complexity estimates, see [[computational complexity of mathematical operations]].\n\nFor examples in [[x86]] assembly, see [[#External links|external links]].\n\n==Pre-set precision==\nIn some languages such as [[REXX]], the precision of all calculations must be set before doing a calculation. Other languages, such as [[Python (programming language)|Python]] and [[Ruby (programming language)|Ruby]] extend the precision automatically to prevent overflow.\n\n==Example==\nThe calculation of [[factorial]]s can easily produce very large numbers. This is not a problem for their usage in many formulae (such as [[Taylor series]]) because they appear along with other terms, so that—given careful attention to the order of evaluation—intermediate calculation values are not troublesome. If approximate values of factorial numbers are desired, [[Stirling's approximation]] gives good results using floating-point arithmetic. The largest representable value for a fixed-size integer variable may be exceeded even for relatively small arguments as shown in the table below. Even floating-point numbers are soon outranged, so it may help to recast the calculations in terms of the [[logarithm]] of the number.\n\nBut if exact values for large factorials are desired, then special software is required, as in the pseudocode that follows, which implements the classic algorithm to calculate 1, 1×2, 1×2×3, 1×2×3×4, etc. the successive factorial numbers.\n\n Constant Limit = 1000;            ''% Sufficient digits.''\n Constant Base = 10;               ''% The base of the simulated arithmetic.''\n Constant FactorialLimit = 365;    ''% Target number to solve, 365!''\n Array digit[1:Limit] of integer;  ''% The big number.''\n Integer carry,d;                  ''% Assistants during multiplication.''\n Integer last,i;                   ''% Indices to the big number's digits.''\n Array text[1:Limit] of character; ''% Scratchpad for the output.''\n Constant tdigit[0:9] of character = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"];\n '''BEGIN'''\n  digit:=0;                        ''% Clear the whole array.''\n  digit[1]:=1;                     ''% The big number starts with 1,''\n  last:=1;                         ''% Its highest-order digit is number 1.''\n  '''for''' n:=1 '''to''' FactorialLimit '''do'''    ''% Step through producing 1!, 2!, 3!, 4!, etc. ''\n   carry:=0;                       ''% Start a multiply by n.''\n   '''for''' i:=1 '''to''' last '''do'''             ''% Step along every digit.''\n    d:=digit[i]*n + carry;         ''% The classic multiply.''\n    digit[i]:=d '''mod''' Base;          ''% The low-order digit of the result.''\n    carry:=d '''div''' Base;             ''% The carry to the next digit.''\n   '''next''' i;\n   '''while''' carry > 0                 ''% Store the carry in the big number.            ''\n    '''if''' last >= Limit '''then''' croak(\"Overflow!\");  ''% If possible!''\n    last:=last + 1;                ''% One more digit.''\n    digit[last]:=carry '''mod''' Base;   ''% Placed.''\n    carry:=carry '''div''' Base;         ''% The carry reduced.''\n   '''Wend'''                            ''% With n > Base, maybe > 1 digit extra.''\n   text:=\" \";                      ''% Now prepare the output.''\n   '''for''' i:=1 '''to''' last '''do'''             ''% Translate from binary to text.''\n    text[Limit - i + 1]:=tdigit[digit[i]];  ''% Reversing the order.''\n   '''next''' i;                         ''% Arabic numerals put the low order last.''\n   '''Print''' text,\" = \",n,\"!\";         ''% Print the result!''\n  '''next''' n;                          ''% On to the next factorial up.''\n '''END''';\n\nWith the example in view, a number of details can be discussed. The most important is the choice of the representation of the big number. In this case, only integer values are required for digits, so an array of fixed-width integers is adequate. It is convenient to have successive elements of the array represent higher powers of the base.\n\nThe second most important decision is in the choice of the base of arithmetic, here ten. There are many considerations. The scratchpad variable {{mvar|d}} must be able to hold the result of a single-digit multiply ''plus the carry'' from the prior digit's multiply. In base ten, a sixteen-bit integer is certainly adequate as it allows up to 32767. However, this example cheats, in that the value of {{mvar|n}} is not itself limited to a single digit. This has the consequence that the method will fail for {{math|''n'' > 3200}} or so. In a more general implementation, {{mvar|n}} would also use a multi-digit representation. A second consequence of the shortcut is that after the multi-digit multiply has been completed, the last value of ''carry'' may need to be carried into multiple higher-order digits, not just one.\n\nThere is also the issue of printing the result in base ten, for human consideration. Because the base is already ten, the result could be shown simply by printing the successive digits of array ''digit'', but they would appear with the highest-order digit last (so that 123 would appear as \"321\"). The whole array could be printed in reverse order, but that would present the number with leading zeroes (\"00000...000123\") which may not be appreciated, so we decided to build the representation in a space-padded text variable and then print that. The first few results (with spacing every fifth digit and annotation added here) are:\n\n{| style=\"text-align: right; white-space: nowrap; line-height: 80%\"\n! colspan=2 style=\"text-align: center\" | Factorial numbers\n! colspan=2 style=\"text-align: center\" | Reach of computer integers\n|-\n|                                                 1 = ||  1!\n|-\n|                                                 2 = ||  2!\n|-\n|                                                 6 = ||  3!\n|-\n|                                                24 = ||  4!\n|-\n|                                               120 = ||  5!\n| 8-bit || style=\"text-align: left\" | 255\n|-\n|                                               720 = ||  6!\n|-\n|                                              5040 = ||  7!\n|-\n|                                             40320 = ||  8!\n| 16-bit || style=\"text-align: left\" | 65535\n|-\n|                                           3 62880 = ||  9!   \n|-\n|                                          36 28800 = || 10!   \n|-\n|                                         399 16800 = || 11!\n|-\n|                                        4790 01600 = || 12!\n| 32-bit || style=\"text-align: left\" | 42949 67295\n|-\n|                                       62270 20800 = || 13!   \n|-\n|                                     8 71782 91200 = || 14!   \n|-\n|                                   130 76743 68000 = || 15!   \n|-\n|                                  2092 27898 88000 = || 16!   \n|-\n|                                 35568 74280 96000 = || 17!   \n|-\n|                               6 40237 37057 28000 = || 18!   \n|-\n|                             121 64510 04088 32000 = || 19!   \n|-\n|                            2432 90200 81766 40000 = || 20!\n| 64-bit || style=\"text-align: left\" | 18446 74407 37095 51615\n|-\n|                           51090 94217 17094 40000 = || 21!   \n|-\n|                        11 24000 72777 76076 80000 = || 22!   \n|-\n|                       258 52016 73888 49766 40000 = || 23!   \n|-\n|                      6204 48401 73323 94393 60000 = || 24!   \n|-\n|                   1 55112 10043 33098 59840 00000 = || 25!   \n|-\n|                  40 32914 61126 60563 55840 00000 = || 26!   \n|-\n|                1088 88694 50418 35216 07680 00000 = || 27!   \n|-\n|               30488 83446 11713 86050 15040 00000 = || 28!   \n|-\n|             8 84176 19937 39701 95454 36160 00000 = || 29!   \n|-\n|           265 25285 98121 91058 63630 84800 00000 = || 30!   \n|-\n|          8222 83865 41779 22817 72556 28800 00000 = || 31!\n|-\n|       2 63130 83693 36935 30167 21801 21600 00000 = || 32!\n|-\n|      86 83317 61881 18864 95518 19440 12800 00000 = || 33!\n|-\n|    2952 32799 03960 41408 47618 60964 35200 00000 = || 34!\n| 128-bit || style=\"text-align: left\" | 3402 82366 92093 84634 63374 60743 17682 11455\n|-\n| 1 03331 47966 38614 49296 66651 33752 32000 00000 = || 35! \n|}\n\nWe could try to use the available arithmetic of the computer more efficiently. A simple escalation would be to use base 100 (with corresponding changes to the translation process for output), or, with sufficiently wide computer variables (such as 32-bit integers) we could use larger bases, such as 10,000. Working in a power-of-2 base closer to the computer's built-in integer operations offers advantages, although conversion to a decimal base for output becomes more difficult. On typical modern computers, additions and multiplications take constant time independent of the values of the operands (so long as the operands fit in single machine words), so there are large gains in packing as much of a bignumber as possible into each element of the digit array. The computer may also offer facilities for splitting a product into a digit and carry without requiring the two operations of ''mod'' and ''div'' as in the example, and nearly all arithmetic units provide a ''[[carry flag]]'' which can be exploited in multiple-precision addition and subtraction. This sort of detail is the grist of machine-code programmers, and a suitable assembly-language bignumber routine can run much faster than the result of the compilation of a high-level language, which does not provide access to such facilities.\n\nFor a single-digit multiply the working variables must be able to hold the value (base-1){{sup|2}} + carry, where the maximum value of the carry is (base-1). Similarly, the variables used to index the digit array are themselves limited in width. A simple way to extend the indices would be to deal with the bignumber's digits in blocks of some convenient size so that the addressing would be via (block ''i'', digit ''j'') where ''i'' and ''j'' would be small integers, or, one could escalate to employing bignumber techniques for the indexing variables. Ultimately, machine storage capacity and execution time impose limits on the problem size.\n\n==History==\nIBM's first business computer, the [[IBM 702]] (a [[vacuum-tube]] machine) of the mid-1950s, implemented integer arithmetic ''entirely in hardware'' on digit strings of any length from 1 to 511 digits.  The earliest widespread software implementation of arbitrary-precision arithmetic was probably that in [[Maclisp]].  Later, around 1980, the [[operating system]]s [[VAX/VMS]] and [[VM/CMS]] offered bignum facilities as a collection of [[literal string|string]] [[subprogram|functions]] in the one case and in the languages [[EXEC 2]] and [[REXX]] in the other.\n\nAn early widespread implementation was available via the [[IBM 1620]] of 1959–1970.  The 1620 was a decimal-digit machine which used discrete transistors, yet it had hardware (that used [[lookup table]]s) to perform integer arithmetic on digit strings of a length that could be from two to whatever memory was available.  For floating-point arithmetic, the mantissa was restricted to a hundred digits or fewer, and the exponent was restricted to two digits only.  The largest memory supplied offered 60 000 digits, however [[Fortran]] compilers for the 1620 settled on fixed sizes such as 10, though it could be specified on a control card if the default was not satisfactory.\n\n==Software libraries==\n{{See also|List of arbitrary-precision arithmetic software}}\n\nArbitrary-precision arithmetic in most computer software is implemented by calling an external [[library (computer science)|library]] that provides [[data type]]s and [[subroutine]]s to store numbers with the requested precision and to perform computations.\n\nDifferent libraries have different ways of representing arbitrary-precision numbers, some libraries work only with integer numbers, others store [[floating point]] numbers in a variety of bases (decimal or binary powers). Rather than representing a number as single value, some store numbers as a numerator/denominator pair ([[rational number|rationals]]) and some can fully represent [[computable number]]s, though only up to some storage limit. Fundamentally, [[Turing machine]]s cannot represent all [[real number]]s, as the [[cardinality]] of {{math|'''ℝ'''}} exceeds the cardinality of {{math|'''ℤ'''}}.\n\n== See also ==\n\n* [[Karatsuba algorithm]]\n* [[Toom–Cook multiplication]]\n* [[Schönhage–Strassen algorithm]]\n* [[Fürer's algorithm]]\n* [[List of arbitrary-precision arithmetic software]]\n\n== References ==\n\n{{reflist}}\n* {{Cite book|last=Knuth |first=Donald |authorlink=Donald Knuth |title=Seminumerical Algorithms |series=[[The Art of Computer Programming]] |volume=2 |year=2008 |edition=3rd |publisher=Addison-Wesley |isbn=0-201-89684-2|postscript=<!-- Bot inserted parameter. Either remove it; or change its value to \".\" for the cite to end in a \".\", as necessary. -->&#123;&#123;inconsistent citations&#125;&#125; }}, Section 4.3.1: The Classical Algorithms\n\n== External links ==\n* [http://oopweb.com/Assembly/Documents/ArtOfAssembly/Volume/Chapter_9/CH09-3.html#HEADING3-1 Chapter 9.3 of ''The Art of Assembly''] by [[Randall Hyde]] discusses multiprecision arithmetic, with examples in [[x86]]-assembly.\n* Rosetta Code task [http://rosettacode.org/wiki/Arbitrary-precision_integers_%28included%29 Arbitrary-precision integers] Case studies in the style in which over 47 programming languages compute the value of 5**4**3**2 using arbitrary precision arithmetic.\n\n{{data types}}\n\n[[Category:Computer arithmetic]]\n[[Category:Computer arithmetic algorithms]]"
    },
    {
      "title": "The Art of Computer Programming",
      "url": "https://en.wikipedia.org/wiki/The_Art_of_Computer_Programming",
      "text": "{{Infobox book\n| name         = The Art of Computer Programming\n| image        = ArtOfComputerProgramming.jpg\n| caption      = The Art of Computer Programming, Volume 1: Fundamental Algorithms\n| border       = yes\n| author       = [[Donald Knuth]]\n| country      = United States\n| language     = English\n| dewey        = 519\n| congress     = QA76.75\n| genre        = [[Non-fiction]]<br /> [[Monograph]]\n| publisher    = [[Addison-Wesley]]\n| pub_date     = 1968– (the book is still incomplete)\n| media_type   = Print ([[Hardcover]])\n| isbn         = 0-201-03801-3\n}}\n'''''The Art of Computer Programming''''' ('''''TAOCP''''') is a comprehensive [[monograph]] written by [[computer scientist]] [[Donald Knuth]] that covers many kinds of [[Computer programming|programming]] [[algorithm]]s and [[analysis of algorithms|their analysis]].\n\nKnuth began the project, originally conceived as a single book with twelve chapters, in 1962. The first three volumes of what was then expected to be a seven-volume set were published in 1968, 1969, and 1973. The first published installment of Volume 4 appeared in paperback as [[Fascicle (book)|Fascicle]] 2 in 2005.\nThe hardback Volume 4A, combining Volume 4, Fascicles 0–4, was published in 2011. Volume 4, Fascicle 6 (\"Satisfiability\") was released in December 2015; Volume 4, Fascicle 5 (\"Mathematical Preliminaries Redux; Backtracking; Dancing Links\") is planned for release in November 2019. Fascicles 5 and 6 are expected to comprise the first two-thirds of Volume 4B.  Knuth has not announced any estimated date for release of Volume 4B, although his method used for Volume 4A is to release the hardback volume some time after release of the paperback fascicles that comprise it.  There are some self-contradictory near-term publisher estimates of May and June 2019, which are almost certainly incorrect.<ref>[http://catalogue.pearsoned.co.uk/educator/product/Art-of-Computer-Programming-Volume-4B-The-Combinatorial-Algorithms/9780201038064.page Addison-Wesley Pearson webpage]</ref><ref>[http://vig.pearsoned.com/store/product/1,1207,store-19880_id-4208_isbn-0201038064,00.html Pearson Educational]</ref>\n\n==History==\n[[File:KnuthAtOpenContentAlliance.jpg|right|thumb|200px|Donald Knuth in 2005]]\n\nAfter winning a Westinghouse Talent Search scholarship, Knuth enrolled at the Case Institute of Technology (now [[Case Western Reserve University]]), where his performance was so outstanding that the faculty voted to award him a master of science upon his completion of the baccalaureate degree. During his summer vacations, Knuth was hired by the [[Burroughs Corporation]] to write [[compiler]]s, earning more in his summer months than full professors did for an entire year.<ref>{{cite web |url=https://conservancy.umn.edu/handle/11299/107413 |hdl=11299/107413 |title=An Interview with Donald E. Knuth |author-first=Philip L. |author-last=Frana |date=2001-11-08}}</ref> Such exploits made Knuth a topic of discussion among the mathematics department, which included [[Richard S. Varga]].\n\nKnuth started to write a book about compiler design in 1962, and soon realized that the scope of the book needed to be much larger. In June 1965, Knuth finished the first draft of what was originally planned to be a single volume of twelve chapters. His hand-written first-draft manuscript (completed in 1966) was {{val|3000}} pages long: he had assumed that about five hand-written pages would translate into one printed page, but his publisher said instead that about {{frac|1|1|2}} hand-written pages translated to one printed page. This meant the book would be approximately {{val|2000}} pages in length. The publisher was nervous about accepting such a project from a graduate student. At this point, Knuth received support from Richard S. Varga, who was the scientific adviser to the publisher. Varga was visiting [[Olga Taussky-Todd]] and [[John Todd (computer scientist)|John Todd]] at [[Caltech]]. With Varga's enthusiastic endorsement, the publisher accepted Knuth's expanded plans. In its expanded version, the book would be published in seven volumes, each with just one or two chapters.<ref>\n{{Cite book |chapter=Donald Knuth |author-first=Donald J. |author-last=Albers |title=Mathematical People: Profiles and Interviews |publisher=A K Peters |edition=2 |date=2008 |isbn=1-56881-340-6 |editor-first1=Donald J. |editor-last1=Albers |editor-first2=Gerald L. |editor-last2=Alexanderson |editor-link2=Gerald L. Alexanderson}}</ref> Due to the growth in the material, the plan for Volume 4 has since expanded to include Volumes 4A, 4B, 4C, 4D, and possibly more.\n\nIn 1976, Knuth prepared a second edition of Volume 2, requiring it to be [[typesetting|typeset]] again, but the style of type used in the first edition (called [[Hot metal typesetting|hot type]]) was no longer available. In 1977, he decided to spend some time creating something more suitable. Eight years later, he returned with [[TeX|T<sub>E</sub>X]], which is currently used for all volumes.\n\nThe offer of a so-called [[Knuth reward check]] worth \"one hexadecimal dollar\" (100<sub>[[Hexadecimal|HEX]]</sub> [[hexadecimal|base 16]] cents, in [[decimal]], is $2.56) for any errors found, and the correction of these errors in subsequent printings, has contributed to the highly polished and still-authoritative nature of the work, long after its first publication. Another characteristic of the volumes is the variation in the difficulty of the exercises. The level of difficulty ranges from \"warm-up\" exercises to unsolved research problems.\n\nKnuth's dedication reads:\n\n<blockquote>This series of books is affectionately dedicated<br>to the [[IBM 650|Type 650 computer]] once installed at<br>[[Case Institute of Technology]],<br>with whom I have spent many pleasant evenings.<ref group=\"lower-alpha\">The dedication was worded slightly differently in the first edition.</ref></blockquote>\n\n==Assembly language in the book==\nAll examples in the books use a language called \"MIX assembly language\", which runs on the hypothetical [[MIX]] computer. Currently, the MIX computer is being replaced by the [[MMIX]] computer, which is a [[RISC]] version. Software such as [[GNU MDK]] exists to provide emulation of the MIX architecture. Knuth considers the use of [[assembly language]] necessary for the speed and memory usage of algorithms to be judged.\n\n==Critical response==\nKnuth was awarded the 1974 [[Turing Award]] \"for his major contributions to the [[analysis of algorithms]] […], and in particular for his contributions to the 'art of computer programming' through his well-known books in a continuous series by this title.\"<ref>{{cite web |url=https://amturing.acm.org/award_winners/knuth_1013846.cfm |title=Donald E. Knuth – A. M. Turing Award Winner |website=AM Turing |access-date=2017-01-25}}</ref> ''[[American Scientist]]'' has included this work among \"100 or so Books that shaped a Century of Science\", referring to the twentieth century,<ref>{{Cite journal |author-first1=Philip |author-last1=Morrison |author-link1=Philip Morrison |author-first2=Phylis |author-last2=Morrison |url=https://www.americanscientist.org/bookshelf/pub/100-or-so-books-that-shaped-a-century-of-science |dead-url=yes |archive-url=https://web.archive.org/web/20080820030403/http://www.americanscientist.org/bookshelf/pub/100-or-so-books-that-shaped-a-century-of-science |archive-date=2008-08-20 |title=100 or so Books that shaped a Century of Science |volume=87 |issue=6 |journal=American Scientist |date=November–December 1999 |publisher=Sigma Xi, The Scientific Research Society |access-date=2008-01-11}}</ref> and within the computer science community it is regarded as the first and still the best comprehensive treatment of its subject. Covers of the third edition of Volume 1 quote [[Bill Gates]] as saying, \"If you think you're a really good programmer… read (Knuth's) ''Art of Computer Programming''… You should definitely send me a résumé if you can read the whole thing.\"<ref>{{cite web |url=http://www.businessinsider.com/bill-gates-loves-donald-knuth-the-art-of-computer-programming-2016-4 |title=Bill Gates once said 'definitely send me a résumé' if you finish this fiendishly difficult book |author-first=Matt |author-last=Weinberger |website=Business Insider |access-date=2016-06-13}}</ref><!-- <ref group=\"nb\">According to [[folklore.org]], [[Steve Jobs]] actually made the incredible claim.\n[http://www.folklore.org/StoryView.py?project=Macintosh&story=Close_Encounters_of_the_Steve_Kind.txt]</ref> comment out&nbsp;– the reference only says that Steve Jobs read the books, not that he said the quote about the resume. --> ''[[The New York Times]]'' referred to it as \"the profession's defining treatise\".<ref name=\"lohr\">{{cite news |author-last=Lohr |author-first=Steve |title=Frances E. Holberton, 84, Early Computer Programmer |work=[[The New York Times]] |date=2001-12-17 |url=https://www.nytimes.com/2001/12/17/business/17HOLB.html |access-date=2010-05-17}}</ref>\n\n==Volumes==\n===Completed===\n*Volume 1&nbsp;– Fundamental Algorithms\n:*Chapter 1&nbsp;– Basic concepts\n:*Chapter 2&nbsp;– Information [[data structure|structures]]\n*Volume 2&nbsp;– Seminumerical Algorithms\n:*Chapter 3&nbsp;– [[Statistical randomness|Random numbers]]\n:*Chapter 4&nbsp;– [[Arithmetic]]\n*Volume 3&nbsp;– [[Sorting algorithm|Sorting]] and [[Search algorithm|Searching]]\n:*Chapter 5&nbsp;– [[Sorting algorithm|Sorting]]\n:*Chapter 6&nbsp;– [[Search algorithm|Searching]]\n*Volume 4A&nbsp;– [[combinatorics|Combinatorial]] Algorithms\n:*Chapter 7&nbsp;– Combinatorial searching (part 1)\n\n===Planned===\n*Volume 4B...&nbsp;– Combinatorial Algorithms (chapters 7 & 8 released in several subvolumes)\n:*Chapter 7&nbsp;– Combinatorial searching (continued)\n:*Chapter 8&nbsp;– [[Recursion]]\n*Volume 5&nbsp;– Syntactic Algorithms ({{as of|2017|lc=on}}, estimated for release in 2025)\n:*Chapter 9&nbsp;– [[Lexical analysis|Lexical scanning]] (also includes [[String searching algorithm|string search]] and [[data compression]])\n:*Chapter 10&nbsp;– [[Parsing]] techniques\n*Volume 6&nbsp;– The Theory of [[Context-free language|Context-Free Languages]]\n*Volume 7&nbsp;– [[Compiler]] Techniques\n\n==Chapter outlines==\n===Completed===\n====Volume 1&nbsp;– Fundamental Algorithms====\n**Chapter 1&nbsp;– Basic concepts\n***1.1. [[Algorithm]]s\n***1.2. Mathematical Preliminaries\n****1.2.1. [[Mathematical induction|Mathematical Induction]]\n****1.2.2. Numbers, Powers, and Logarithms\n****1.2.3. Sums and Products\n****1.2.4. Integer Functions and Elementary Number Theory\n****1.2.5. [[Permutation]]s and [[Factorial]]s\n****1.2.6. [[Binomial coefficient|Binomial Coefficients]]\n****1.2.7. [[Harmonic number|Harmonic Numbers]]\n****1.2.8. [[Fibonacci number|Fibonacci Numbers]]\n****1.2.9. [[Generating function|Generating Functions]]\n****1.2.10. Analysis of an Algorithm\n****1.2.11. [[Asymptotic analysis|Asymptotic Representations]]\n*****1.2.11.1. The [[Big O notation|O-notation]]\n*****1.2.11.2. [[Euler's summation formula]]\n*****1.2.11.3. Some asymptotic calculations\n***1.3 [[MMIX]] ([[MIX]] in the hardback copy but updated by fascicle 1)\n****1.3.1. Description of MMIX\n****1.3.2. The MMIX Assembly Language\n****1.3.3. Applications to Permutations\n***1.4. Some Fundamental Programming Techniques\n****1.4.1. Subroutines\n****1.4.2. [[Coroutine]]s\n****1.4.3. Interpretive Routines\n*****1.4.3.1. A MIX simulator\n*****1.4.3.2. Trace routines\n****1.4.4. Input and Output\n****1.4.5. History and Bibliography\n**Chapter 2&nbsp;– Information Structures\n***2.1. Introduction\n***2.2. Linear Lists\n****2.2.1. Stacks, Queues, and Deques\n****2.2.2. Sequential Allocation\n****2.2.3. Linked Allocation\n****2.2.4. Circular Lists\n****2.2.5. Doubly Linked Lists\n****2.2.6. Arrays and Orthogonal Lists\n***2.3. [[Tree (data structure)|Trees]]\n****2.3.1. Traversing Binary Trees\n****2.3.2. Binary Tree Representation of Trees\n****2.3.3. Other Representations of Trees\n****2.3.4. Basic Mathematical Properties of Trees\n*****2.3.4.1. Free trees\n*****2.3.4.2. Oriented trees\n*****2.3.4.3. [[Kőnig's lemma|The \"infinity lemma\"]]\n*****2.3.4.4. Enumeration of trees\n*****2.3.4.5. Path length\n*****2.3.4.6. History and bibliography\n****2.3.5. Lists and Garbage Collection\n***2.4. Multilinked Structures\n***2.5. [[Dynamic memory allocation|Dynamic Storage Allocation]]\n***2.6. History and Bibliography\n\n====Volume 2&nbsp;– Seminumerical Algorithms====\n**Chapter 3&nbsp;– Random Numbers\n***3.1. Introduction\n***3.2. [[Pseudo-random number generator|Generating Uniform Random Numbers]]\n****3.2.1. The Linear Congruential Method\n*****3.2.1.1. Choice of modulus\n*****3.2.1.2. Choice of multiplier\n*****3.2.1.3. Potency\n****3.2.2. Other Methods\n***3.3. Statistical Tests\n****3.3.1. General Test Procedures for Studying Random Data\n****3.3.2. Empirical Tests\n****3.3.3. Theoretical Tests\n****3.3.4. The Spectral Test\n***3.4. [[Pseudo-random number sampling|Other Types of Random Quantities]]\n****3.4.1. Numerical Distributions\n****3.4.2. Random Sampling and Shuffling\n***3.5. What Is a [[Random Sequence]]?\n***3.6. Summary\n**Chapter 4&nbsp;– Arithmetic\n***4.1. [[Positional notation|Positional Number Systems]]\n***4.2. [[Floating point|Floating Point]] Arithmetic\n****4.2.1. Single-Precision Calculations\n****4.2.2. Accuracy of Floating Point Arithmetic\n****4.2.3. Double-Precision Calculations\n****4.2.4. Distribution of Floating Point Numbers\n***4.3. [[Arbitrary-precision arithmetic|Multiple Precision Arithmetic]]\n****4.3.1. The Classical Algorithms\n****4.3.2. Modular Arithmetic\n****4.3.3. How Fast Can We Multiply?\n***4.4. [[Radix]] Conversion\n***4.5. [[Rational number|Rational]] Arithmetic\n****4.5.1. Fractions\n****4.5.2. The Greatest Common Divisor\n****4.5.3. Analysis of [[Euclidean algorithm|Euclid's Algorithm]]\n****4.5.4. Factoring into Primes\n***4.6. [[Polynomial]] Arithmetic\n****4.6.1. Division of Polynomials\n****4.6.2. Factorization of Polynomials\n****4.6.3. Evaluation of Powers\n****4.6.4. Evaluation of Polynomials\n***4.7. Manipulation of [[Power Series]]\n\n====Volume 3&nbsp;– Sorting and Searching====\n**Chapter 5&nbsp;– [[Sorting algorithm|Sorting]]\n***5.1. Combinatorial Properties of [[Permutation]]s\n****5.1.1. Inversions\n****5.1.2. Permutations of a Multiset\n****5.1.3. Runs\n****5.1.4. Tableaux and Involutions\n***5.2. [[Internal sort]]ing\n****5.2.1. Sorting by Insertion\n****5.2.2. Sorting by Exchanging\n****5.2.3. Sorting by Selection\n****5.2.4. Sorting by Merging\n****5.2.5. Sorting by Distribution\n***5.3. Optimum Sorting\n****5.3.1. Minimum-Comparison Sorting\n****5.3.2. Minimum-Comparison Merging\n****5.3.3. Minimum-Comparison Selection\n****5.3.4. Networks for Sorting\n***5.4. [[External Sorting]]\n****5.4.1. Multiway Merging and Replacement Selection\n****5.4.2. The Polyphase Merge\n****5.4.3. The Cascade Merge\n****5.4.4. Reading Tape Backwards\n****5.4.5. The Oscillating Sort\n****5.4.6. Practical Considerations for Tape Merging\n****5.4.7. External Radix Sorting\n****5.4.8. Two-Tape Sorting\n****5.4.9. Disks and Drums\n***5.5. Summary, History, and Bibliography\n**Chapter 6&nbsp;– [[Search algorithm|Searching]]\n***6.1. Sequential Searching\n***6.2. Searching by Comparison of [[Unique key|Keys]]\n****6.2.1. Searching an Ordered Table\n****6.2.2. Binary Tree Searching\n****6.2.3. Balanced Trees\n****6.2.4. Multiway Trees\n***6.3. Digital Searching\n***6.4. [[Hash table|Hashing]]\n***6.5. Retrieval on Secondary Keys\n\n====Volume 4A&nbsp;– Combinatorial Algorithms, Part 1====\n**Chapter 7&nbsp;– Combinatorial Searching\n***7.1. [[Binary numeral system|Zeros and Ones]]\n****7.1.1. [[Two-element Boolean algebra|Boolean]] Basics\n****7.1.2. Boolean Evaluation\n****7.1.3. [[Bitwise operation|Bitwise]] Tricks and Techniques\n****7.1.4. [[Binary decision diagram|Binary Decision Diagrams]]\n***7.2. Generating All Possibilities\n****7.2.1. Generating Basic Combinatorial Patterns\n*****7.2.1.1. Generating all n-[[tuple]]s\n*****7.2.1.2. Generating all [[permutation]]s\n*****7.2.1.3. Generating all [[combination]]s\n*****7.2.1.4. Generating all [[Partition (number theory)|partitions]]\n*****7.2.1.5. Generating all [[Partition of a set|set partitions]]\n*****7.2.1.6. Generating all [[tree (graph theory)|trees]]\n*****7.2.1.7. History and further references\n\n===Planned===\n====Volume 4B, 4C, 4D – Combinatorial Algorithms====\n**Chapter 7&nbsp;– Combinatorial Searching (continued)\n***7.2. Generating all possibilities (continued)\n****7.2.2. [[Backtracking|Backtrack programming]] (online as pre-fascicle 5b)\n*****7.2.2.1. [[Dancing links]] (online as pre-fascicle 5c)\n*****7.2.2.2. [[Satisfiability]] (published in Fascicle 6)\n*****7.2.2.3. [[Constraint Satisfaction Problem|Constraint satisfaction]]\n*****7.2.2.4. [[Hamiltonian path problem|Hamiltonian paths]]\n*****7.2.2.5. [[Clique problem|Cliques]]\n*****7.2.2.6. Covers ([[Vertex cover]], [[Set cover problem]], [[Exact cover]], [[Clique cover]])\n*****7.2.2.7. Squares\n*****7.2.2.8. A potpourri of puzzles\n*****7.2.2.9. Estimating backtrack costs (chapter 6 of \"Selected Papers on Analysis of Algorithms\", and pre-fascicle 5b in Section 7.2.2 under the heading \"Running time estimates\")\n****7.2.3. Generating inequivalent patterns (includes discussion of [[Pólya enumeration theorem]])\n***7.3. [[Shortest path problem|Shortest paths]]\n***7.4. [[Graph algorithms]]\n****7.4.1. Components and traversal\n****7.4.2. Special classes of graphs\n****7.4.3. [[Expander graph]]s\n****7.4.4. [[Random graph]]s\n***7.5. Network algorithms\n****7.5.1. [[Transversal (combinatorics)|Distinct representatives]]\n****7.5.2. [[Assignment problem|The assignment problem]]\n****7.5.3. [[Network flow problem|Network flows]]\n****7.5.4. Optimum subtrees\n****7.5.5. Optimum matching\n****7.5.6. Optimum orderings\n***7.6. Independence theory\n****7.6.1. Independence structures\n****7.6.2. Efficient [[matroid]] algorithms\n***7.7. Discrete [[dynamic programming]] (see also [[Transfer-matrix method]])\n***7.8. [[Branch-and-bound]] techniques\n***7.9. Herculean tasks (aka [[NP-hard]] problems)\n***7.10. [[Approximation algorithm|Near-optimization]]\n**Chapter 8&nbsp;– [[Recursion]] (chapter 22 of \"Selected Papers on Analysis of Algorithms\")\n\n====Volume 5&nbsp;– Syntactic Algorithms====\n* ''{{as of|2017|lc=on}}, estimated for release in 2025''\n**Chapter 9&nbsp;– [[Lexical analysis|Lexical scanning]] (includes also string search and data compression)\n**Chapter 10&nbsp;– [[Parsing]] techniques\n\n====Volume 6&nbsp;– The Theory of Context-free Languages<ref>{{cite web |url=https://cs.stanford.edu/~knuth/taocp.html#future |title=TAOCP – Future plans}}</ref>====\n====Volume 7&nbsp;– Compiler Techniques====\n\n==English editions==\n\n===Current editions===\nThese are the current editions in order by volume number:\n* ''The Art of Computer Programming, Volumes 1-4A Boxed Set''. Third Edition (Reading, Massachusetts: Addison-Wesley, 2011), 3168pp. {{ISBN|978-0-321-75104-1|0-321-75104-3}}\n** ''Volume 1: Fundamental Algorithms''. Third Edition (Reading, Massachusetts: Addison-Wesley, 1997), xx+650pp. {{ISBN|978-0-201-89683-1|0-201-89683-4}}. Errata: [https://cs.stanford.edu/~knuth/all1-pre.ps.gz] (2011-01-08), [https://cs.stanford.edu/~knuth/all1.ps.gz] (2017-09-18, 27th [[printing run|printing]]). Addenda: [https://cs.stanford.edu/~knuth/1-appc.ps.gz] (2011).\n** ''Volume 2: Seminumerical Algorithms''. Third Edition (Reading, Massachusetts: Addison-Wesley, 1997), xiv+762pp. {{ISBN|978-0-201-89684-8|0-201-89684-2}}. Errata: [https://cs.stanford.edu/~knuth/all2-pre.ps.gz] (2011-01-08), [https://cs.stanford.edu/~knuth/all2.ps.gz] (2017-09-18, 26th printing). Addenda: [https://cs.stanford.edu/~knuth/2-appc.ps.gz] (2011).\n** ''Volume 3: Sorting and Searching''. Second Edition (Reading, Massachusetts: Addison-Wesley, 1998), xiv+780pp.+foldout. {{ISBN|978-0-201-89685-5|0-201-89685-0}}. Errata: [https://cs.stanford.edu/~knuth/all3-pre.ps.gz] (2011-01-08), [https://cs.stanford.edu/~knuth/all3.ps.gz] (2017-09-18, 27th printing). Addenda: [https://cs.stanford.edu/~knuth/3-appc.ps.gz] (2011).\n** ''Volume 4A: Combinatorial Algorithms, Part 1''. First Edition (Reading, Massachusetts: Addison-Wesley, 2011), xv+883pp. {{ISBN|978-0-201-03804-0|0-201-03804-8}}. Errata: [https://cs.stanford.edu/~knuth/all4a.ps.gz] (2017-09-18, ? printing).\n* ''Volume 1, Fascicle 1: MMIX&nbsp;– A RISC Computer for the New Millennium''. (Addison-Wesley, 2005-02-14) {{ISBN|0-201-85392-2}} (will be in the fourth edition of volume 1). Errata: [https://cs.stanford.edu/~knuth/all1f1.ps.gz] (2016-08-02).\n* ''Volume 4, Fascicle 5: Mathematical Preliminaries Redux; Backtracking; Dancing Links''. (Addison-Wesley, 2019-09-14) 350pp, {{ISBN|978-0-13-467179-6}} (will become part of volume 4B)\n* ''Volume 4, Fascicle 6: Satisfiability''. (Addison-Wesley, 2015-12-08) xiii+310pp, {{ISBN|978-0-13-439760-3}}. Errata: [https://cs.stanford.edu/~knuth/all4f6.ps.gz] (2017-06-01) (will become part of volume 4B)\n\n===Previous editions===\n====Complete volumes====\nThese volumes were superseded by newer editions and are in order by date.\n* ''Volume 1: Fundamental Algorithms''. First edition, 1968, xxi+634pp, {{ISBN|0-201-03801-3}}.<ref name=\"WellsReview\">{{cite journal |author-last=Wells |author-first=Mark B. |title=Review: ''The Art of Computer Programming, Volume 1. Fundamental Algorithms'' and ''Volume 2. Seminumerical Algorithms'' by Donald E. Knuth |journal=Bulletin of the American Mathematical Society |date=1973 |volume=79 |issue=3 |pages=501–509 |url=https://www.ams.org/journals/bull/1973-79-03/S0002-9904-1973-13173-8/S0002-9904-1973-13173-8.pdf |doi=10.1090/s0002-9904-1973-13173-8}}</ref>\n* ''Volume 2: Seminumerical Algorithms''. First edition, 1969, xi+624pp, {{ISBN|0-201-03802-1}}.<ref name=\"WellsReview\"/>\n* ''Volume 3: Sorting and Searching''. First edition, 1973, xi+723pp+foldout, {{ISBN|0-201-03803-X}}. Errata: [https://cs.stanford.edu/~knuth/err3-1e.ps.gz].\n* ''Volume 1: Fundamental Algorithms''. Second edition, 1973, xxi+634pp, {{ISBN|0-201-03809-9}}. Errata: [https://cs.stanford.edu/~knuth/err1-2e.ps.gz].\n* ''Volume 2: Seminumerical Algorithms''. Second edition, 1981, xiii+ 688pp, {{ISBN|0-201-03822-6}}. Errata: [https://cs.stanford.edu/~knuth/err2-2e.ps.gz].\n* ''The Art of Computer Programming, Volumes 1-3 Boxed Set''. Second Edition (Reading, Massachusetts: Addison-Wesley, 1998), pp. {{ISBN|978-0-201-48541-7|0-201-48541-9}}\n\n====Fascicles====\nVolume&nbsp;4{{'s}} [[fascicle (book)|fascicle]]s 0–4 were revised and published as Volume&nbsp;4A.\n* ''Volume 4, Fascicle 0: Introduction to Combinatorial Algorithms and Boolean Functions''. (Addison-Wesley Professional, 2008-04-28) vi+240pp, {{ISBN|0-321-53496-4}}. Errata: [https://cs.stanford.edu/~knuth/all4f0.ps.gz] (2011-01-01).\n* ''Volume 4, Fascicle 1: Bitwise Tricks & Techniques; Binary Decision Diagrams''. (Addison-Wesley Professional, 2009-03-27) viii+260pp, {{ISBN|0-321-58050-8}}. Errata: [https://cs.stanford.edu/~knuth/all4f1.ps.gz] (2011-01-01).\n* ''Volume 4, Fascicle 2: Generating All Tuples and Permutations''. (Addison-Wesley, 2005-02-14) v+127pp, {{ISBN|0-201-85393-0}}. Errata: [https://cs.stanford.edu/~knuth/all4f2.ps.gz] (2011-01-01).\n* ''Volume 4, Fascicle 3: Generating All Combinations and Partitions''. (Addison-Wesley, 2005-07-26) vi+150pp, {{ISBN|0-201-85394-9}}. Errata: [https://cs.stanford.edu/~knuth/all4f3.ps.gz] (2011-01-01).\n* ''Volume 4, Fascicle 4: Generating All Trees; History of Combinatorial Generation''. (Addison-Wesley, 2006-02-06) vi+120pp, {{ISBN|0-321-33570-8}}. Errata: [https://cs.stanford.edu/~knuth/all4f4.ps.gz] (2011-01-01).\n* ''Volume 4, Fascicle 6: Satisfiability''. (Addison-Wesley, 2015-12-18) xiii+310pp, {{ISBN|978-0-13-439760-3}}. Errata: [https://cs.stanford.edu/~knuth/all4f6.ps.gz] (2017-06-01)\n\n====Pre-fascicles====\nVolume&nbsp;4{{'s}} [[pre-fascicle]] 6A was revised and published as fascicle 6.\n* ''[https://cs.stanford.edu/~knuth/fasc5a.ps.gz Volume 4B, Pre-fascicle 5A: Mathematical Preliminaries Redux] (available for download)\n* ''[https://cs.stanford.edu/~knuth/fasc5b.ps.gz Volume 4B, Pre-fascicle 5B: Introduction to Backtracking] (available for download)\n* ''[https://cs.stanford.edu/~knuth/fasc5c.ps.gz Volume 4B, Pre-fascicle 5C: Dancing Links] (available for download)\n\n==See also==\n* ''[[Introduction to Algorithms]]''\n\n==References==\n'''Notes'''\n{{Notelist}}\n\n'''Citations'''\n{{Reflist}}\n\n'''Sources'''\n{{Refbegin}}\n* {{cite book |title=Portraits in Silicon |author-first=Robert |author-last=Slater |date=1987 |publisher=[[MIT Press]] |url=https://books.google.com/books/about/Portraits_in_Silicon.html?id=aWTtMyYmKhUC |isbn=0-262-19262-4}}\n* {{cite book |title=Out of Their Minds: The Lives and Discoveries of 15 Great Computer Scientists |author-first1=Dennis |author-last1=Shasha |author-link1=Dennis Shasha |author-first2=Cathy |author-last2=Lazere |date=1995 |publisher=Copernicus |isbn=0-387-97992-1}}\n{{Refend}}\n\n==External links==\n*[https://cs.stanford.edu/~knuth/taocp.html Overview of topics] (Knuth's personal homepage)\n*[http://purl.umn.edu/107413 Oral history interview with Donald E. Knuth] at [[Charles Babbage Institute]], University of Minnesota, Minneapolis. Knuth discusses software patenting, [[structured programming]], collaboration and his development of [[TeX]]. The oral history discusses the writing of ''The Art of Computer Programming''.\n*[https://amturing.acm.org/p3-knuth.pdf \"Robert W Floyd, In Memoriam\", by Donald E. Knuth] - (on the influence of [[Robert Floyd|Bob Floyd]])\n*[http://www.softpanorama.org/People/Knuth/taocp.shtml ''TAoCP'' and its Influence of Computer Science (Softpanorama)]\n\n{{Donald Knuth navbox}}\n\n{{DEFAULTSORT:Art Of Computer Programming, The}}\n[[Category:1968 books]]\n[[Category:Computer programming books]]\n[[Category:Computer science books]]\n[[Category:Monographs]]\n[[Category:Books by Donald Knuth]]\n[[Category:Analysis of algorithms]]\n[[Category:Computer arithmetic algorithms]]\n[[Category:American non-fiction books]]\n[[Category:1969 books]]\n[[Category:1973 books]]\n[[Category:1981 books]]\n[[Category:2011 non-fiction books]]\n[[Category:Addison-Wesley books]]"
    },
    {
      "title": "Binary splitting",
      "url": "https://en.wikipedia.org/wiki/Binary_splitting",
      "text": "In [[mathematics]], '''binary splitting''' is a technique for speeding up numerical evaluation of many types of [[series (mathematics)|series]] with rational terms. In particular, it can be used to evaluate [[hypergeometric series]] at rational points.\n\n==Method==\nGiven a series\n:<math>S(a,b) = \\sum_{n=a}^b \\frac{p_n}{q_n}</math>\nwhere ''p<sub>n</sub>'' and ''q<sub>n</sub>'' are integers, the goal of binary splitting is to compute integers ''P''(''a'', ''b'') and ''Q''(''a'', ''b'') such that\n\n:<math>S(a,b) = \\frac{P(a,b)}{Q(a,b)}.</math>\n\nThe splitting consists of setting ''m'' = [(''a''&nbsp;+&nbsp;''b'')/2] and recursively computing ''P''(''a'', ''b'') and ''Q''(''a'', ''b'') from ''P''(''a'', ''m''), ''P''(''m'', ''b''), ''Q''(''a'', ''m''), and ''Q''(''m'', ''b''). When ''a'' and ''b'' are sufficiently close, ''P''(''a'', ''b'') and ''Q''(''a'', ''b'') can be computed directly from ''p<sub>a</sub>...p<sub>b</sub>'' and ''q<sub>a</sub>...q<sub>b</sub>''.\n\n==Comparison with other methods==\nBinary splitting requires more memory than direct term-by-term summation, but is asymptotically faster since the sizes of all occurring subproducts are reduced. Additionally, whereas the most naive evaluation scheme for a rational series uses a full-precision division for each term in the series, binary splitting requires only one final division at the target precision; this is not only faster, but conveniently eliminates rounding errors. To take full advantage of the scheme, fast multiplication algorithms such as [[Toom–Cook multiplication|Toom–Cook]] and [[Schönhage–Strassen algorithm|Schönhage–Strassen]] must be used; with ordinary ''O''(''n''<sup>2</sup>) multiplication, binary splitting may render no speedup at all or be slower.\n\nSince all subdivisions of the series can be computed independently of each other, binary splitting lends well to [[parallelization]] and [[checkpointing]].\n\nIn a less specific sense, ''binary splitting'' may also refer to any [[divide and conquer algorithm]] that always divides the problem in two halves.\n\n==References==\n\n* Xavier Gourdon & Pascal Sebah. ''[http://numbers.computation.free.fr/Constants/Algorithms/splitting.html Binary splitting method]''\n* David V. Chudnovsky & Gregory V. Chudnovsky. ''Computer algebra in the service of mathematical physics and number theory''. In ''Computers and Mathematics (Stanford, CA, 1986)'', pp.&nbsp;09–232, Dekker, New York, 1990.\n* Bruno Haible, Thomas Papanikolaou. ''[http://www.ginac.de/CLN/binsplit.pdf Fast multiprecision evaluation of series of rational numbers]''. Paper distributed with the [[Class Library for Numbers|CLN library]] source code.\n* Lozier, D.W. and Olver, F.W.J. Numerical Evaluation of Special Functions. Mathematics of Computation 1943–1993: A Half-Century of Computational Mathematics, W.Gautschi, eds., Proc. Sympos. Applied Mathematics, AMS, v.48, pp.&nbsp;79–125 (1994).\n* Bach, E. The complexity of number-theoretic constants. Info. Proc. Letters, N 62, pp.&nbsp;145–152 (1997).\n* Borwein, J.M., Bradley, D.M. and Crandall, R.E. Computational strategies for the Riemann zeta function. J. of Comput. Appl. Math., v.121, N 1-2, pp.&nbsp;247–296 (2000).\n* Karatsuba, E.A. Fast evaluation of transcendental functions. (English. Russian original) Probl. Inf. Transm. 27, No.4, 339-360 (1991); translation from Probl. Peredachi Inf. 27, No.4, 76–99 (1991).\n* Ekatherina Karatsuba. ''[http://www.ccas.ru/personal/karatsuba/algen.htm  Fast Algorithms and the FEE method]''\n\n[[Category:Computer arithmetic algorithms]]"
    },
    {
      "title": "Computational complexity of mathematical operations",
      "url": "https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations",
      "text": "{{Short description|algorithmic runtime requirements for common math procedures}}\n{{refimprove|date=April 2015}}\n[[File:comparison computational complexity.svg|thumb|Graphs of functions commonly used in the analysis of algorithms, showing the number of operations ''N'' versus input size ''n'' for each function]]\nThe following tables list the [[computational complexity]] of various [[algorithm]]s for common [[mathematical operation]]s.\n\nHere, complexity refers to the [[time complexity]] of performing computations on a [[multitape Turing machine]].<ref name=\"Schonhage\">A. Schönhage, A.F.W. Grotefeld, E. Vetter: ''Fast Algorithms—A Multitape Turing Machine Implementation'', BI Wissenschafts-Verlag, Mannheim, 1994</ref> See [[big O notation]] for an explanation of the notation used.\n\nNote: Due to the variety of multiplication algorithms, ''M''(''n'') below stands in for the complexity of the chosen multiplication algorithm.\n\n== Arithmetic functions ==\n{{clear}}\n{| class=\"wikitable\" style=\"width: auto\"\n!Operation\n!Input\n!Output\n!Algorithm\n!Complexity \n|-\n|[[Addition]]\n|Two ''n''-digit numbers ''N'', ''N''\n|One ''n''+1-digit number\n|Schoolbook addition with carry\n|Θ(''n''), Θ(log(''N''))\n|-\n|[[Subtraction]]\n|Two ''n''-digit numbers ''N'', ''N''\n|One ''n''+1-digit number\n|Schoolbook subtraction with borrow\n|Θ(''n''), Θ(log(''N''))\n|-\n|rowspan=7|[[Multiplication]]\n|rowspan=7|Two ''n''-digit numbers<br />\n|rowspan=7|One 2''n''-digit number\n|[[Multiplication algorithm#Long multiplication|Schoolbook long multiplication]]\n|{{math|''O''(''n''<sup>2</sup>)}}\n|-\n|[[Karatsuba algorithm]]\n|{{math|''O''(''n''<sup>1.585</sup>)}}\n|-\n|3-way [[Toom–Cook multiplication]]\n|{{math|''O''(''n''<sup>1.465</sup>)}}\n|-\n|{{math|''k''}}-way Toom–Cook multiplication\n|{{math|''O''(''n''<sup>log (2''k'' − 1)/log ''k''</sup>)}}\n|-\n|Mixed-level Toom–Cook (Knuth 4.3.3-T)<ref>D. Knuth. ''[[The Art of Computer Programming]]'', Volume 2. Third Edition, Addison-Wesley 1997.</ref>\n|{{math|''O''(''n'' 2<sup>{{sqrt|2 log ''n''}}</sup> log ''n'')}}\n|-\n|[[Schönhage–Strassen algorithm]]\n|{{math|''O''(''n'' log ''n'' log log ''n'')}}\n|-\n|[[Fürer's algorithm]]<ref>Martin Fürer. ''[http://www.cse.psu.edu/~furer/Papers/mult.pdf Faster Integer Multiplication] {{webarchive|url=https://web.archive.org/web/20130425232048/http://www.cse.psu.edu/~furer/Papers/mult.pdf |date=2013-04-25 }}''. Proceedings of the 39th Annual [[ACM Symposium on Theory of Computing]], San Diego, California, USA, June 11–13, 2007, pp. 55–67.</ref>\n|{{math|''O''(''n'' log ''n'' 2<sup>2 [[Iterated logarithm|log*]] ''n''</sup>)}}\n|-\n|rowspan=3|[[Division (mathematics)|Division]]\n|rowspan=3|Two ''n''-digit numbers\n|rowspan=3|One ''n''-digit number\n|[[Long division|Schoolbook long division]]\n|{{math|''O''(''n''<sup>2</sup>)}}\n|-\n|Burnikel-Ziegler Divide-and-Conquer Division <ref>Christoph Burnikel and Joachim Ziegler ''[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.47.565&rep=rep1&type=pdf Fast Recursive Division]'' Im Stadtwald , D- Saarbrucken 1998.</ref>\n|{{math|''O''(''M''(''n'') log ''n'')}}\n|-\n|[[Newton–Raphson division]]\n|{{math|''O''(''M''(''n''))}}\n|-\n|[[Square root]]\n|One ''n''-digit number\n|One ''n''-digit number\n|[[Newton's method]]\n|{{math|''O''(''M''(''n''))}}\n|-\n|rowspan=3|[[Modular exponentiation]]\n|rowspan=3|Two ''n''-digit numbers and a ''k''-bit exponent\n|rowspan=3|One ''n''-digit number\n|Repeated multiplication and reduction\n|{{math|''O''(''M''(''n'') 2<sup>''k''</sup>)}}\n|-\n|[[Exponentiation by squaring]]\n|{{math|''O''(''M''(''n'') ''k'')}}\n|-\n|Exponentiation with [[Montgomery reduction]]\n|{{math|''O''(''M''(''n'') ''k'')}}\n|}\n\n== Algebraic functions ==\n\n{| class=\"wikitable\"\n!Operation\n!Input\n!Output\n!Algorithm\n!Complexity\n|-\n|rowspan=2|[[Polynomial]] evaluation\n|rowspan=2| One polynomial of degree ''n'' with fixed-size polynomial coefficients\n|rowspan=2| One fixed-size number\n|Direct evaluation\n|Θ(''n'')\n|-\n|[[Horner's method]]\n|Θ(''n'')\n|-\n|rowspan=2|Polynomial gcd (over '''Z'''[''x''] or '''F'''[''x''])\n|rowspan=2| Two polynomials of degree ''n'' with fixed-size polynomial coefficients\n|rowspan=2| One polynomial of degree at most ''n''\n|[[Euclidean algorithm]]\n|''O''(''n''<sup>2</sup>)\n|-\n|Fast Euclidean algorithm <ref>http://planetmath.org/fasteuclideanalgorithm</ref>\n|''O''(''M''(''n'') log&nbsp;''n'')\n|}\n\n== Special functions ==\nMany of the methods in this section are given in Borwein & Borwein.<ref>J. Borwein & P. Borwein. ''Pi and the AGM: A Study in Analytic Number Theory and Computational Complexity''. John Wiley 1987.</ref>\n\n=== Elementary functions ===\nThe [[elementary function]]s are constructed by composing arithmetic operations, the [[exponential function]] (exp), the [[natural logarithm]] (log), [[trigonometric function]]s (sin, cos), and their inverses. The complexity of an elementary function is equivalent to that of its inverse, since all elementary functions are [[analytic function|analytic]] and hence invertible by means of Newton's method. In particular, if either exp or log in the complex domain can be computed with some complexity, then that complexity is attainable for all other elementary functions.\n\nBelow, the size ''n'' refers to the number of digits of precision at which the function is to be evaluated.\n\n{| class=\"wikitable\"\n!Algorithm\n!Applicability\n!Complexity\n|-\n|[[Taylor series]]; repeated argument reduction (e.g. exp(2''x'') = [exp(''x'')]<sup>2</sup>) and direct summation\n|exp, log, sin, cos, arctan\n|''O''(''M''(''n'') ''n''<sup>1/2</sup>)\n|-\n|Taylor series; [[Fast Fourier transform|FFT]]-based acceleration\n|exp, log, sin, cos, arctan\n|''O''(''M''(''n'') ''n''<sup>1/3</sup> (log ''n'')<sup>2</sup>)\n|-\n|Taylor series; [[binary splitting]] + [[bit burst]] method<ref>David and Gregory Chudnovsky. Approximations and complex multiplication according to Ramanujan. ''Ramanujan revisited'', Academic Press, 1988, pp 375–472.</ref>\n|exp, log, sin, cos, arctan\n|''O''(''M''(''n'') (log ''n'')<sup>2</sup>)\n|-\n|[[Arithmetic–geometric mean]] iteration<ref>Richard P. Brent, [https://arxiv.org/abs/1004.3412 ''Multiple-precision zero-finding methods and the complexity of elementary function evaluation''], in: Analytic Computational Complexity (J. F. Traub, ed.), Academic Press, New York, 1975, 151–176.</ref>\n|exp, log, sin, cos, arctan\n|''O''(''M''(''n'') log ''n'')\n|}\n\nIt is not known whether O(''M''(''n'') log ''n'') is the optimal complexity for elementary functions. The best known lower bound is the trivial bound Ω(''M''(''n'')).\n\n=== Non-elementary functions ===\n{| class=\"wikitable\"\n!Function\n!Input\n!Algorithm\n!Complexity\n|-\n|rowspan=3|[[Gamma function]]\n|''n''-digit number\n|Series approximation of the [[incomplete gamma function]]\n|''O''(''M''(''n'') ''n''<sup>1/2</sup> (log ''n'')<sup>2</sup>)\n|-\n|Fixed rational number\n|Hypergeometric series\n|''O''(''M''(''n'') (log ''n'')<sup>2</sup>)\n|-\n|''m''/24, ''m'' an integer\n|[[Arithmetic-geometric mean]] iteration\n|''O''(''M''(''n'') log ''n'')\n|-\n|rowspan=2|[[Hypergeometric function]] ''<sub>p</sub>F<sub>q</sub>''\n|''n''-digit number\n|(As described in Borwein & Borwein)\n|''O''(''M''(''n'') ''n''<sup>1/2</sup> (log ''n'')<sup>2</sup>)\n|-\n|Fixed rational number\n|Hypergeometric series\n|''O''(''M''(''n'') (log ''n'')<sup>2</sup>)\n|}\n\n=== Mathematical constants ===\nThis table gives the complexity of computing approximations to the given constants to ''n'' correct digits.\n{| class=\"wikitable\"\n!Constant\n!Algorithm\n!Complexity\n|-\n|[[Golden ratio]], ''φ''\n|[[Newton's method]]\n|''O''(''M''(''n''))\n|-\n|[[Square root of 2]], {{sqrt|2}}\n|Newton's method\n|''O''(''M''(''n''))\n|-\n|rowspan=2|[[e (mathematical constant)|Euler's number]], ''e''\n|[[Binary splitting]] of the Taylor series for the exponential function\n|''O''(''M''(''n'') log&nbsp;''n'')\n|-\n|Newton inversion of the natural logarithm\n|''O''(''M''(''n'') log&nbsp;''n'')\n|-\n|rowspan=2|[[Pi]], ''π''\n|Binary splitting of the arctan series in [[Machin's formula]]\n|''O''(''M''(''n'') (log&nbsp;''n'')<sup>2</sup>)<ref name = brent>{{citation|url=https://arxiv.org/pdf/1802.07558.pdf|title=The Borwein Brothers, Pi and the AGM|author=Richard P. Brent|year=2018}}</ref>\n|-\n|[[Gauss–Legendre algorithm]]\n|''O''(''M''(''n'') log&nbsp;''n'')<ref name = brent/>\n|-\n|[[Euler–Mascheroni constant|Euler's constant]], ''γ''\n|Sweeney's method (approximation in terms of the [[exponential integral]])\n|''O''(''M''(''n'') (log&nbsp;''n'')<sup>2</sup>)\n|}\n\n== Number theory ==\nAlgorithms for [[number theory|number theoretical]] calculations are studied in [[computational number theory]].\n\n{| class=\"wikitable\"\n!Operation\n!Input\n!Output\n!Algorithm\n!Complexity\n|-\n|rowspan=5|[[Greatest common divisor]]\n|rowspan=5|Two ''n''-digit numbers\n|rowspan=5|One number with at most ''n'' digits\n|[[Euclidean algorithm]]\n|''O''(''n''<sup>2</sup>)\n|-\n|[[Binary GCD algorithm]]\n|''O''(''n''<sup>2</sup>)\n|-\n|Left/right ''k''-ary binary GCD algorithm<ref>{{cite journal | author = J. Sorenson. | title = Two Fast GCD Algorithms | journal = Journal of Algorithms | volume = 16 | issue = 1| pages = 110–144 | year = 1994 | doi=10.1006/jagm.1994.1006}}</ref>\n|O(''n''<sup>2</sup>/ log ''n'')\n|-\n|[[Stehlé–Zimmermann algorithm]]<ref>R. Crandall & C. Pomerance. ''Prime Numbers – A Computational Perspective''. Second Edition, Springer 2005.</ref>\n|''O''(''M''(''n'') log ''n'')\n|-\n|[[Schönhage controlled Euclidean descent algorithm]]<ref>{{cite journal | author = Möller N | title =  On Schönhage's algorithm and subquadratic integer gcd computation | journal = Mathematics of Computation | volume = 77 | issue = 261 | pages = 589–607 | doi = 10.1090/S0025-5718-07-02017-0 | year = 2008 | url=http://www.lysator.liu.se/~nisse/archive/sgcd.pdf| bibcode = 2008MaCom..77..589M }}</ref>\n|O(''M''(''n'') log ''n'')\n|-\n| rowspan=\"2\" |[[Jacobi symbol]]\n| rowspan=\"2\" |Two ''n''-digit numbers\n| rowspan=\"2\" |0, −1, or 1\n|Schönhage controlled Euclidean descent algorithm<ref>{{cite web|url=http://cr.yp.to/papers/nonsquare.ps|title=Faster Algorithms to Find Non-squares Modulo Worst-case Integers|author=Bernstein D J}}</ref>\n|''O''(''M''(''n'') log ''n'')\n|-\n|Stehlé–Zimmermann algorithm<ref>{{cite arXiv|eprint=1004.2091|class=cs.DS|author1=Richard P. Brent|author2=Paul Zimmermann|title=An O(M(n) log n) algorithm for the Jacobi symbol|year=2010}}</ref>\n|''O''(''M''(''n'') log ''n'')\n|-\n|rowspan=3|[[Factorial]]\n|rowspan=3|A fixed-size number ''m''\n|rowspan=3|One ''O''(''m'' log ''m'')-digit number\n|Bottom-up multiplication\n|''O''(M(''m''<sup>2</sup>) log ''m'')\n|-\n|Binary splitting\n|''O''(''M''(''m'' log ''m'') log ''m'')\n|-\n|Exponentiation of the prime factors of ''m''\n|''O''(''M''(''m'' log ''m'') log log ''m''),<ref>{{cite journal | last1 = Borwein | first1 = P. | year = 1985 | title = On the complexity of calculating factorials | url = | journal = Journal of Algorithms | volume = 6 | issue = 3| pages = 376–380 | doi=10.1016/0196-6774(85)90006-9}}</ref><br />O(''M''(''m'' log ''m''))<ref name=\"Schonhage\" />\n|-\n| rowspan=\"5\" |[[Primality test]]\n| rowspan=\"5\" |An number of size n\n| rowspan=\"5\" |True or false\n|[[AKS primality test]]\n|''O''((log ''n)''<sup>6</sup>)<ref name=\"lenstra_pomerance_2005\">H. W. Lenstra Jr. and Carl Pomerance, \"[http://www.math.dartmouth.edu/~carlp/PDF/complexity12.pdf Primality testing with Gaussian periods]\", preliminary version July 20, 2005.</ref>  <ref name=\"lenstra_pomerance_2011\">H. W. Lenstra jr. and Carl Pomerance, \"[http://www.math.dartmouth.edu/~carlp/aks041411.pdf Primality testing with Gaussian periods]\", version of April 12, 2011.</ref> or <math>O((\\log n)^{6+\\varepsilon})</math><ref name=\"tao-aks\">{{cite book|title=An epsilon of room, II: Pages from year three of a mathematical blog|last=Tao|first=Terence|publisher=American Mathematical Society|year=2010|isbn=978-0-8218-5280-4|series=Graduate Studies in Mathematics|volume=117|location=Providence, RI|pages=82–86|contribution=1.11 The AKS primality test|doi=10.1090/gsm/117|mr=2780010|authorlink=Terence Tao|contribution-url=https://terrytao.wordpress.com/2009/08/11/the-aks-primality-test/}}</ref><ref>{{cite paper|last1=Lenstra|first1=Jr., H.W.|author1-link=Hendrik Lenstra|last2=Pomerance|first2=Carl|author2-link=Carl Pomerance|date=December 11, 2016|title=Primality testing with Gaussian periods|url=https://math.dartmouth.edu/~carlp/aks111216.pdf}}</ref>,<br>''O''((log ''n)''<sup>3</sup>), assuming [[Agrawal's conjecture]]\n|-\n|[[Elliptic curve primality proving]]\n|<math>O((\\log n)^{4+\\varepsilon})</math> ''heuristically<ref name=\"morain\">{{cite journal|last=Morain|first=F.|year=2007|title=Implementing the asymptotically fast version of the elliptic curve primality proving algorithm|journal=[[Mathematics of Computation]]|volume=76|issue=257|pages=493–505|arxiv=math/0502097|bibcode=2007MaCom..76..493M|doi=10.1090/S0025-5718-06-01890-4|mr=2261033}}</ref>''\n|-\n|[[Baillie-PSW primality test]]\n|<math>O((\\log n)^{2+\\varepsilon})</math><ref name=\"PSW\">{{cite journal|author1=Carl Pomerance|author2=John L. Selfridge|author-link2=John L. Selfridge|author3=Samuel S. Wagstaff, Jr.|author-link3=Samuel S. Wagstaff, Jr.|date=July 1980|title=The pseudoprimes to 25·10<sup>9</sup>|url=//math.dartmouth.edu/~carlp/PDF/paper25.pdf|journal=Mathematics of Computation|volume=35|issue=151|pages=1003–1026|doi=10.1090/S0025-5718-1980-0572872-7|jstor=2006210|author-link1=Carl Pomerance}}</ref><ref name=\"lpsp\">{{cite journal|author1=Robert Baillie|author2=Samuel S. Wagstaff, Jr.|author-link2=Samuel S. Wagstaff, Jr.|date=October 1980|title=Lucas Pseudoprimes|url=http://mpqs.free.fr/LucasPseudoprimes.pdf|journal=Mathematics of Computation|volume=35|issue=152|pages=1391–1417|doi=10.1090/S0025-5718-1980-0583518-6|jstor=2006406|mr=583518}}</ref>\n|-\n|[[Miller–Rabin primality test]]\n|<math>O(k(\\log n)^{2+\\varepsilon})</math><ref name=\"monier\">{{cite journal|last=Monier|first=Louis|year=1980|title=Evaluation and comparison of two efficient probabilistic primality testing algorithms|journal=[[Theoretical Computer Science (journal)|Theoretical Computer Science]]|volume=12|issue=1|pages=97–108|doi=10.1016/0304-3975(80)90007-9|mr=582244}}</ref>\n|-\n|[[Solovay–Strassen primality test]]\n|<math>O(k(\\log n)^{2+\\varepsilon})</math><ref name=\"monier\" />\n|-\n| rowspan=\"2\" |[[Integer factorization]]\n| rowspan=\"2\" |A b-bit input number\n| rowspan=\"2\" |A set of factors\n|[[General number field sieve]]\n|O((1 + ε)<sup>b</sup>)<ref>This form of sub-exponential time is valid for all ε > 0. A more precise form of the complexity can be given as <math>O\\left(\\exp\\sqrt[3]{\\frac{64}{9} b (\\log b)^2}\\right)</math></ref>\n|-\n|[[Shor's algorithm]]\n|{{math|O(''b''<sup>3</sup>)}}\n|-\n|}\n\n== Matrix algebra ==\nThe following complexity figures assume that arithmetic with individual elements has complexity ''O''(1), as is the case with fixed-precision [[floating-point arithmetic]] or operations on a [[finite field]].\n\n{| class=\"wikitable\"\n!Operation\n!Input\n!Output\n!Algorithm\n!Complexity\n|-\n|rowspan=4|[[Matrix multiplication algorithm|Matrix multiplication]]\n|rowspan=4|Two ''n''×''n'' matrices\n|rowspan=4|One ''n''×''n'' matrix\n|[[Matrix multiplication algorithm#Iterative algorithm|Schoolbook matrix multiplication]]\n|''O''(''n''<sup>3</sup>)\n|-\n|[[Strassen algorithm]]\n|''O''(''n''<sup>2.807</sup>)\n|-\n|[[Coppersmith–Winograd algorithm]]\n|''O''(''n''<sup>2.376</sup>)\n|-\n|Optimized CW-like algorithms<ref>{{Citation | last1=Davie | first1=A.M. | last2=Stothers | first2=A.J. | title=Improved bound for complexity of matrix multiplication|journal=Proceedings of the Royal Society of Edinburgh|volume=143A| issue=2 |pages=351–370|year=2013|doi=10.1017/S0308210511001648}}</ref><ref>{{Citation | last1=Vassilevska Williams | first1=Virginia|authorlink= Virginia Vassilevska Williams | title=Breaking the Coppersmith-Winograd barrier | url=http://theory.stanford.edu/~virgi/matrixmult-f.pdf | year=2011}}</ref><ref>{{Citation | last1=Le Gall | first1=François | contribution=Powers of tensors and fast matrix multiplication | year = 2014 | arxiv=1401.7714 | title = Proceedings of the 39th International Symposium on Symbolic and Algebraic Computation (ISSAC 2014)| bibcode=2014arXiv1401.7714L | title-link=ISSAC }}</ref>\n|''O''(''n''<sup>2.373</sup>)\n|-\n|Matrix multiplication\n|One ''n''×''m'' matrix &\n\none ''m''×''p'' matrix\n|One ''n''×''p'' matrix\n|Schoolbook matrix multiplication\n|''O''(''nmp'')\n|-\n|rowspan=4|[[Matrix inversion]]{{ref|blockinversion|{{big|*}}}}\n|rowspan=4|One ''n''×''n'' matrix\n|rowspan=4|One ''n''×''n'' matrix\n|[[Gauss–Jordan elimination]]\n|''O''(''n''<sup>3</sup>)\n|-\n|Strassen algorithm\n|''O''(''n''<sup>2.807</sup>)\n|-\n|Coppersmith–Winograd algorithm\n|''O''(''n''<sup>2.376</sup>)\n|-\n|Optimized CW-like algorithms\n|''O''(''n''<sup>2.373</sup>)\n|-\n|rowspan=2|[[Singular value decomposition]]\n|rowspan=2|One ''m''×''n'' matrix\n|One ''m''×''m'' matrix, <br /> one ''m''×''n'' matrix, & <br /> one ''n''×''n'' matrix\n| Bidiagonalization and QR algorithm\n|O(''mn''<sup>2</sup> + ''m''<sup>2</sup> ''n'') <br /> (''m'' ≥ ''n'')\n|-\n|One ''m''×''n'' matrix, <br /> one ''n''×''n'' matrix, & <br /> one ''n''×''n'' matrix\n| Bidiagonalization and QR algorithm\n|O(''mn''<sup>2</sup>) <br /> (''m'' ≥ ''n'')\n|-\n|rowspan=5|[[Determinant]]\n|rowspan=5|One ''n''×''n'' matrix\n|rowspan=5|One number\n|[[Laplace expansion]]\n|''O''(''n''!)\n|-\n|Division-free algorithm<ref>http://page.mi.fu-berlin.de/rote/Papers/pdf/Division-free+algorithms.pdf</ref>\n|''O''(''n''<sup>4</sup>)\n|-\n|[[LU decomposition]]\n|''O''(''n''<sup>3</sup>)\n|-\n|[[Bareiss algorithm]]\n|''O''(''n''<sup>3</sup>)\n|-\n|Fast matrix multiplication<ref>{{citation|at=Theorem 6.6, p.&nbsp;241|title=The Design and Analysis of Computer Algorithms|first1=Alfred V.|last1=Aho|author1-link=Alfred Aho|first2=John E.|last2=Hopcroft|author2-link=John Hopcroft|first3=Jeffrey D.|last3=Ullman|author3-link=Jeffrey Ullman|publisher=Addison-Wesley|year=1974}}</ref>\n|''O''(''n''<sup>2.373</sup>)\n|-\n\n|Back substitution\n|[[Triangular matrix]]\n|''n'' solutions\n|Back substitution<ref>J. B. Fraleigh and R. A. Beauregard, \"Linear Algebra,\" Addison-Wesley Publishing Company, 1987, p 95.</ref>\n|''O''(''n<sup>2</sup>'')\n|-\n|}\n\nIn 2005, [[Henry Cohn]], [[Robert Kleinberg]], [[Balázs Szegedy]], and [[Chris Umans]] showed that either of two different conjectures would imply that the exponent of matrix multiplication is 2.<ref>Henry Cohn, Robert Kleinberg, Balazs Szegedy, and Chris Umans. Group-theoretic Algorithms for Matrix Multiplication. {{arxiv|math.GR/0511460}}. ''Proceedings of the 46th Annual Symposium on Foundations of Computer Science'', 23–25 October 2005, Pittsburgh, PA, IEEE Computer Society, pp. 379–388.</ref>\n\n{{note|blockinversion|{{big|*}}}}Because of the possibility of [[Invertible matrix#Blockwise inversion|blockwise inverting a matrix]], where an inversion of an {{math|{{var|n}}×{{var|n}}}} matrix requires inversion of two half-sized matrices and six multiplications between two half-sized matrices, and since matrix multiplication has a lower bound of {{math|[[Big O notation#Family of Bachmann–Landau notations|Ω]]({{var|n}}{{sup|2}} log {{var|n}})}} operations,<ref>[[Ran Raz]]. On the complexity of matrix product. In Proceedings of the thirty-fourth annual ACM symposium on Theory of computing. ACM Press, 2002. {{DOI|10.1145/509907.509932}}.</ref> it can be shown that a [[divide and conquer algorithm]] that uses blockwise inversion to invert a matrix runs with the same time complexity as the matrix multiplication algorithm that is used internally.<ref>T.H. Cormen, C.E. Leiserson, R.L. Rivest, C. Stein, ''Introduction to Algorithms'', 3rd ed., MIT Press, Cambridge, MA, 2009, §&nbsp;28.2.</ref>\n\n== Transforms ==\nAlgorithms for computing [[Transformation (function)|transforms]] of functions (particularly [[Integral transform|integral transforms]]) are widely used in all areas of mathematics, particularly [[Mathematical analysis|analysis]] and [[signal processing]].\n{| class=\"wikitable\"\n!Operation\n!Input\n!Output\n!Algorithm\n!Complexity\n|-\n|[[Discrete Fourier transform]]\n|Finite data sequence of size n\n|Set of complex numbers\n|[[Fast Fourier transform]]\n|''O''(''n''&nbsp;log&nbsp;''n'')\n|}\n\n== References ==\n{{reflist|40em}}\n\n== Further reading ==\n* {{cite book |last1=Brent |first1=Richard P. |authorlink1=Richard P. Brent |last2=Zimmermann |first2=Paul |authorlink2=Paul Zimmermann (mathematician) |title=Modern Computer Arithmetic |year=2010 |publisher=Cambridge University Press |isbn=978-0-521-19469-3}}\n* {{cite book |last1=Knuth |first1=Donald Ervin |authorlink=Donald Knuth |title=The Art of Computer Programming. Volume 2: Seminumerical Algorithms|year=1997| edition=3rd |publisher=Addison-Wesley |isbn=978-0-201-89684-8}}\n\n[[Category:Computer arithmetic algorithms]]\n[[Category:Computational complexity theory]]\n[[Category:Mathematics-related lists]]\n[[Category:Number theoretic algorithms]]\n[[Category:Unsolved problems in computer science]]"
    },
    {
      "title": "Fürer's algorithm",
      "url": "https://en.wikipedia.org/wiki/F%C3%BCrer%27s_algorithm",
      "text": "'''Fürer's algorithm''' is an [[integer multiplication algorithm]] for extremely large integers with very low [[asymptotic complexity]]. It was published in 2007 by the Swiss mathematician [[Martin Fürer]] of Pennsylvania State University<ref name=\"fürer_1\">M. Fürer (2007). \"[https://web.archive.org/web/20130425232048/http://www.cse.psu.edu/~furer/Papers/mult.pdf Faster Integer Multiplication]\" ''Proceedings of the 39th annual ACM Symposium on Theory of Computing'' (STOC), 55–67, San Diego, CA, June 11–13, 2007, and ''SIAM Journal on Computing'', Vol. 39 Issue 3, 979–1005, 2009.</ref> as an asymptotically faster algorithm when analysed on a multitape Turing machine than its predecessor, the [[Schönhage–Strassen algorithm]].<ref>{{cite journal |first=A. |last=Schönhage |first2=V. |last2=Strassen |year=1971 |title=Schnelle Multiplikation großer Zahlen |language=de |trans-title=Fast Multiplication of Large Numbers |journal=[[Computing (journal)|Computing]] |volume=7 |issue=3–4 |pages=281–292 |doi=10.1007/BF02242355 }}</ref>\n\n==Background==\nThe Schönhage–Strassen algorithm uses the [[fast Fourier transform]] (FFT) to compute integer products in time <math>O (n \\log n \\cdot \\log \\log n)</math> and its authors, [[Arnold Schönhage]] and [[Volker Strassen]], conjecture a lower bound of {{nowrap|<math>\\Omega(n \\log n)</math>.}} Fürer's algorithm reduces the gap between these two bounds. It can be used to multiply integers of length <math>n</math> in time <math style=\"vertical-align:-15%\">O \\left(n \\log n\\ \\cdot 2^{O(\\log^* n)} \\right)</math> where {{nowrap|{{log-star}}''n''}} is the [[iterated logarithm]]. The difference between the <math>\\log\\log n</math> and <math>2^{\\log^* n}</math> terms, from a complexity point of view, is asymptotically in the advantage of Fürer's algorithm for integers greater than <math>2^{2^{64}}</math>. However the difference between these terms for realistic values of <math>n</math> is very small.<ref name=\"fürer_1\" />\n\n==Improved algorithms==\nIn 2008 De ''et al'' gave a similar algorithm which relies on [[modular arithmetic]] instead of complex arithmetic to achieve in fact the same running time.<ref>A. De, C. Saha, P. Kurur and R. Saptharishi (2008). \"Fast integer multiplication using modular arithmetic\" ''Proceedings of the 40th annual ACM Symposium on Theory of Computing'' (STOC), 499–506, New York, NY, 2008, and ''SIAM Journal on Computing'', Vol. 42 Issue 2, 685–699, 2013. {{arXiv|0801.1416}}</ref>\nIt has been estimated that it becomes faster than Schönhage-Strassen for inputs exceeding a length of <math>10^{10^{4796}}</math>.<ref>{{cite conference|first=Christoph|last=Lüders|title=Implementation of the DKSS Algorithm for Multiplication of Large Numbers|format=pdf|conference=International Symposium on Symbolic and Algebraic Computation|pages=267-274|date=2015|url=http://wrogn.com/wp-content/uploads/2015/07/ISSAC-2015-Lueders-Implementation-DKSS.pdf}}</ref>\n\nIn 2015 Harvey, van der Hoeven and Lecerf<ref>D. Harvey, J. van der Hoeven and G. Lecerf (2015). \"Even faster integer multiplication\", February 2015. {{arXiv|1407.3360}}</ref> gave a new algorithm that achieves a running time of <math>O(n\\log n \\cdot 2^{3\\log^* n})</math> making explicit the implied constant in the <math>O(\\log^* n)</math> exponent. They also proposed a variant of their algorithm which achieves <math>O(n\\log n \\cdot 2^{2\\log^* n})</math> but whose validity relies on standard conjectures about the distribution of [[Mersenne prime]]s.\n\nIn 2015 Covanov and Thomé provided another modification of Fürer's algorithm which achieves the same running time.<ref>{{cite paper |first=S. |last=Covanov |first2=E. |last2=Thomé |year=2015 |title=Fast Arithmetic for Faster Integer Multiplication |arxiv=1502.02800v1 }} Published as {{harvtxt|Covanov|Thomé|2019}}.</ref>\nNevertheless, it remains just as impractical as all the other implementations of this algorithm.<ref>S. Covanov and E. Thomé (2014). \"Efficient implementation of an algorithm multiplying big numbers\", Internal research report, Polytechnics School, France, July 2014.</ref><ref>S. Covanov and M. Moreno Mazza (2015). \"Putting Fürer algorithm into practice\", Master research report, Polytechnics School, France, January 2015.</ref>\n\nIn 2016 Covanov and Thomé proposed an integer multiplication algorithm based on a generalization of [[Fermat primes]] that conjecturally achieves a complexity bound of <math>O(n\\log n \\cdot 2^{2\\log^* n})</math>.  This matches the 2015 conditional result of Harvey, van der Hoeven, and Lecerf but uses a different algorithm and relies on a different conjecture.<ref>{{cite journal |first=Svyatoslav |last=Covanov |first2=Emmanuel |last2=Thomé |title=Fast Integer Multiplication Using Generalized Fermat Primes |journal=[[Mathematics of Computation|Math. Comp.]] |volume=88 |year=2019 |issue= |pages=1449–1477 |doi=10.1090/mcom/3367 |ref=harv }}</ref>\n\nIn 2018 Harvey and van der Hoeven used an approach based on the existence of short lattice vectors guaranteed by [[Minkowski's theorem]] to prove an unconditional complexity bound of <math>O(n\\log n \\cdot 2^{2\\log^* n})</math>.<ref>D. Harvey and J. van der Hoeven (2018). \"Faster integer multiplication using short lattice vectors\", February 2018. {{arXiv|1802.07932}}</ref>\n\nIn 2019, Harvey and van der Hoeven published a long-sought after, asymptotically optimal <math>O(n\\log n)</math> integer multiplication algorithm.<ref>{{Cite book|url=https://hal.archives-ouvertes.fr/hal-02070778|title=Integer multiplication in time O(n log n)|last=Harvey|first=David|last2=Van Der Hoeven|first2=Joris|date=2019-04-12|publisher=|year=|isbn=|location=|pages=}}</ref><ref>{{Cite news|url=https://www.quantamagazine.org/mathematicians-discover-the-perfect-way-to-multiply-20190411/|title=Mathematicians Discover the Perfect Way to Multiply|last=Hartnett|first=Kevin|date=2019-04-14|work=Wired|access-date=2019-04-15|issn=1059-1028}}</ref> Because Schönhage and Strassen predicted that n * log(n) is the ‘best possible’ result Harvey said: “...our work is expected to be the end of the road for this problem, although we don't know yet how to prove this rigorously.”<ref>{{cite news |last1=Gilbert |first1=Lachlan |title=Maths whiz solves 48-year-old multiplication problem |url=https://newsroom.unsw.edu.au/news/science-tech/maths-whiz-solves-48-year-old-multiplication-problem |accessdate=18 April 2019 |publisher=UNSW |date=4 April 2019}}</ref>\n\n== See also ==\n* [[Number-theoretic transform]]\n\n== References ==\n{{reflist}}\n\n{{Number-theoretic algorithms}}\n\n{{DEFAULTSORT:Furer'S Algorithm}}\n[[Category:Computer arithmetic algorithms]]"
    },
    {
      "title": "Karatsuba algorithm",
      "url": "https://en.wikipedia.org/wiki/Karatsuba_algorithm",
      "text": "[[File:Karatsuba_multiplication.svg|thumb|300px|Karatsuba multiplication of az+b and cz+d (boxed), and 1234 and 567. Magenta arrows denote multiplication, amber denotes addition, silver denotes subtraction and light cyan denotes left shift. (A), (B) and (C) show recursion used to obtain intermediate values.]]\nThe '''Karatsuba algorithm''' is a fast [[multiplication algorithm]]. It was discovered by [[Anatoly Karatsuba]] in 1960 and published in 1962.<ref name=\"kara1962\">\n  {{cite journal\n  | author = A. Karatsuba and Yu. Ofman\n  | title = Multiplication of Many-Digital Numbers by Automatic Computers\n  | journal = Proceedings of the USSR Academy of Sciences\n  | volume = 145\n  | year = 1962\n  | pages = 293–294\n  | postscript = . Translation in the academic journal ''[[Physics-Doklady]]'', '''7''' (1963), pp. 595–596}}\n</ref><ref name=\"kara1995\">\n  {{cite journal\n  | author = A. A. Karatsuba\n  | title = The Complexity of Computations\n  | journal = Proceedings of the Steklov Institute of Mathematics\n  | volume = 211\n  | pages =169–183\n  | year = 1995\n  | url = http://www.ccas.ru/personal/karatsuba/divcen.pdf\n  | postscript = . Translation from Trudy Mat. Inst. Steklova, 211, 186–202 (1995)}}\n</ref><ref name=\"knuthV2\">\n  Knuth D.E. (1969) ''[[The Art of Computer Programming]]. v.2.'' Addison-Wesley Publ.Co., 724 pp.\n</ref> It reduces the multiplication of two ''n''-digit numbers to at most <math> n^{\\log_23}\\approx n^{1.58}</math> single-digit multiplications in general (and exactly <math>n^{\\log_23}</math> when ''n'' is a power of 2). It is therefore faster than the [[long multiplication|classical]] algorithm, which requires <math>n^2</math> single-digit products. For example, the Karatsuba algorithm requires 3<sup>10</sup> = 59,049 single-digit multiplications to multiply two 1024-digit numbers (''n'' = 1024 = 2<sup>10</sup>), whereas the classical algorithm requires (2<sup>10</sup>)<sup>2</sup> = 1,048,576 (a speedup of 17.75 times).\n\nThe Karatsuba algorithm was the first multiplication algorithm asymptotically faster than the quadratic \"grade school\" algorithm.\nThe [[Toom–Cook multiplication|Toom–Cook algorithm]] (1963) is a faster generalization of Karatsuba's method, and the [[Schönhage–Strassen algorithm]] (1971) is even faster, for sufficiently large ''n''.\n\n==History==\nThe standard procedure for multiplication of two ''n''-digit numbers requires a number of elementary operations proportional to <math>n^2\\,\\!</math>, or <math>O(n^2)\\,\\!</math> in [[big-O notation]]. [[Andrey Kolmogorov]] conjectured that the classical algorithm was ''[[asymptotically optimal]],'' meaning that any algorithm for that task would require <math>\\Omega(n^2)\\,\\!</math> elementary operations.\n\nIn 1960, Kolmogorov organized a seminar on mathematical problems in [[cybernetics]] at the [[Moscow State University]], where he stated the <math>\\Omega(n^2)\\,\\!</math> conjecture and other problems in the [[Computational complexity theory|complexity of computation]]. Within a week, Karatsuba, then a 23-year-old student, found an algorithm (later it was called \"divide and conquer\") that multiplies two ''n''-digit numbers in <math>O(n^{\\log_2 3})</math> elementary steps, thus disproving the conjecture. Kolmogorov was very excited about the discovery; he communicated it at the next meeting of the seminar, which was then terminated. Kolmogorov gave some lectures on the Karatsuba result at conferences all over the world (see, for example, \"Proceedings of the International Congress of Mathematicians 1962\", pp.&nbsp;351–356, and also\n\"6 Lectures delivered at the International Congress of Mathematicians in Stockholm, 1962\") and published the method in 1962, in the [[Proceedings of the USSR Academy of Sciences]]. The article had been written by Kolmogorov and contained two results on multiplication, Karatsuba's algorithm and a separate result by [[Yuri Petrovich Ofman|Yuri Ofman]]; it listed \"A. Karatsuba and Yu. Ofman\" as the authors. Karatsuba only became aware of the paper when he received the reprints from the publisher.<ref name=\"kara1995\"/>\n\n==Algorithm==\n\n===Basic step===\nThe basic step of Karatsuba's algorithm is a formula that allows one to compute the product of two large numbers <math>x</math> and <math>y</math> using three multiplications of smaller numbers, each with about half as many digits as <math>x</math> or <math>y</math>, plus some additions and digit shifts. This basic step is, in fact, a generalization of [[Multiplication algorithm#Complex multiplication algorithm|a similar complex multiplication algorithm]], where the [[imaginary unit]] {{mvar|i}} is replaced by a power of the [[radix|base]].\n\nLet <math>x</math> and <math>y</math> be represented as <math>n</math>-digit strings in some base <math>B</math>. For any positive integer <math>m</math> less than <math>n</math>, one can write the two given numbers as\n\n:<math>x = x_1 B^m + x_0,</math>\n:<math>y = y_1 B^m + y_0,</math>\n\nwhere <math>x_0</math> and <math>y_0</math> are less than <math>B^m</math>. The product is then\n\n:<math>xy = (x_1 B^m + x_0)(y_1 B^m + y_0),</math>\n:<math>xy = z_2 B^{2m} + z_1 B^m + z_0,</math>\n\nwhere\n\n:<math>z_2 = x_1 y_1,</math>\n:<math>z_1 = x_1 y_0 + x_0 y_1,</math>\n:<math>z_0 = x_0 y_0.</math>\n\nThese formulae require four multiplications and were known to [[Charles Babbage]].<ref>Charles Babbage, Chapter VIII – Of the Analytical Engine, Larger Numbers Treated, [https://books.google.com/books?id=Fa1JAAAAMAAJ&pg=PA125 Passages from the Life of a Philosopher], Longman Green, London, 1864; page 125.</ref> Karatsuba observed that <math>xy</math> can be computed in only three multiplications, at the cost of a few extra additions.  With <math>z_0</math> and <math>z_2</math> as before one can observe that\n\n:<math>z_1 = (x_1 + x_0)(y_1 + y_0) - z_2 - z_0.</math>\n\nAn issue that occurs, however, when computing <math>z_1</math> is that the above computation of <math>(x_1 + x_0)</math> and <math>(y_1 + y_0)</math> may result in overflow (will produce a result in the range <math>0 \\leq \\text{result} < 2 B^m</math>), which require a multiplier having one extra bit.  This can be avoided by noting that\n\n:<math>z_1 = (x_0 - x_1)(y_1 - y_0) + z_2 + z_0.</math>\n\nThis computation of <math>(x_0 - x_1)</math> and <math>(y_1 - y_0)</math> will produce a result in the range of <math>-B^m < \\text{result} < B^m</math>.  This method may produce negative numbers, which require one extra bit to encode signedness, and would still require one extra bit for the multiplier.  However, one way to avoid this is to record the sign and then use the absolute value of <math>(x_0 - x_1)</math> and <math>(y_1 - y_0)</math> to perform an unsigned multiplication, after which the result may be negated when both signs originally differed.  Another advantage is that even though <math>(x_0 - x_1)(y_1 - y_0)</math> may be negative, the final computation of <math>z_1</math> only involves additions.\n\n===Example===\nTo compute the product of 12345 and 6789, where ''B'' = 10, choose ''m'' = 3. Then we decompose the input operands using the resulting base (''B''<sup>''m''</sup> = ''1000''), as:\n: 12345 = '''12''' · ''1000'' + '''345'''\n:  6789 =  '''6''' · ''1000'' + '''789'''\n\nOnly three multiplications, which operate on smaller integers, are used to compute three partial results:\n: ''z''<sub>2</sub> = '''12''' '''×''' '''6''' = 72\n: ''z''<sub>0</sub> = '''345''' '''×''' '''789''' = 272205\n: ''z''<sub>1</sub> = ('''12''' + '''345''') '''×''' ('''6''' + '''789''') − ''z''<sub>2</sub> − ''z''<sub>0</sub> = 357 '''×''' 795 − 72 − 272205 = 283815 − 72 − 272205 = 11538\n\nWe get the result by just adding these three partial results, shifted accordingly (and then taking carries into account by decomposing these three inputs in base ''1000'' like for the input operands):\n: result = ''z''<sub>2</sub> · (''B''<sup>''m''</sup>)<sup>''2''</sup> + ''z''<sub>1</sub> · (''B''<sup>''m''</sup>)<sup>''1''</sup> + ''z''<sub>0</sub> · (''B''<sup>''m''</sup>)<sup>''0''</sup>, i.e.\n: result = 72 · ''1000''<sup>2</sup> + 11538 · ''1000'' + 272205 = '''83810205'''.\n\nNote that the intermediate third multiplication operates on an input domain which is less than two times larger than for the two first multiplications, its output domain is less than four times larger, and base-''1000'' carries computed from the first two multiplications must be taken into account when computing these two subtractions.\n\n===Recursive application===\nIf ''n'' is four or more, the three multiplications in Karatsuba's basic step involve operands with fewer than ''n'' digits. Therefore, those products can be computed by [[recursion|recursive]] calls of the Karatsuba algorithm. The recursion can be applied until the numbers are so small that they can (or must) be computed directly.\n\nIn a computer with a full 32-bit by 32-bit [[Binary multiplier|multiplier]], for example, one could choose ''B'' = 2<sup>31</sup> = {{val|2,147,483,648}}, and store each digit as a separate 32-bit binary word. Then the sums ''x''<sub>1</sub> + ''x''<sub>0</sub> and ''y''<sub>1</sub> + ''y''<sub>0</sub> will not need an extra binary word for storing the carry-over digit (as in [[carry-save adder]]), and the Karatsuba recursion can be applied until the numbers to multiply are only one-digit long.\n\n===Asymmetric Karatsuba-like formulae===\nKaratsuba's original formula and other generalizations are themselves symmetric. For example, \nthe following formula computes \n:<math>c(x)=c_4x^4+c_3x^3+c_2x^2+c_1x+c_0=a(x)b(x)=(a_2x^2+a_1x+a_0)(b_2x^2+b_1x+b_0)</math>\nwith 6 multiplications in <math>{\\text{GF}}(2)[x]</math>, where <math>{\\text{GF}}(2)</math> is the Galois field with two elements 0 and 1.\n\n:<math>\\begin{align}\n\\left\\{ {\n\\begin{array}{l}\nc_0 = p_0,\t\t\\\\\nc_1 = p_{012} + p_{02} + p_{12} + p_2,\t\t\\\\\nc_2 = p_{012} + p_{01} + p_{12},\t\t\t\\\\\nc_3 = p_{012} + p_{01} + p_{02} + p_0,\t\t\\\\\nc_4 = p_2,\n\\end{array}\n}\\right.\n\\end{align}</math>\nwhere <math>p_{i}=a_ib_i, \\ \\ p_{ij}=(a_i+a_j)(b_i+b_j)</math> and <math>p_{ijk}=(a_i+a_j+a_k)(b_i+b_j+b_k)</math>.\nWe note that addition and subtraction are the same in fields of characteristic 2.\n\nThis formula is symmetrical, namely, it does not change if we exchange <math>a</math> and <math>b</math> in <math>p_i, \\ \\  p_{ij}</math> and <math>p_{ijk}</math>.\n\nBased on the second [[Euclidean division|Generalized division algorithms]]\n,<ref>Haining Fan, Ming Gu, Jiaguang Sun, Kwok-Yan Lam,\"Obtaining More Karatsuba-Like Formulae over the Binary \nField\", IET Information security Vol. 6 No. 1 pp. 14-19, 2012.</ref> Fan et al. found the following asymmetric formula:\n\n:<math>\\begin{align}\n\\left\\{ {\n\\begin{array}{l}\nc_{0}=p_{0} \\\\ \nc_{1}=p_{012}+p_{2}+m_{4}+m_{5} \\\\ \nc_{2}=p_{012}+m_{3}+m_{5} \\\\ \nc_{3}=p_{012}+p_{0}+m_{3}+m_{4} \\\\ \nc_{4}=p_{2},\n\\end{array}\n}\\right.\n\\end{align}</math>\nwhere \n<math>m_{3}=(a_{1}+a_{2})(b_{0}+b_{2}), \\ \\ m_{4}=(a_{0}+a_{1})(b_{1}+b_{2})</math> and \n<math>m_{5}=(a_{0}+a_{2})(b_{0}+b_{1})</math>.\n\nIt is asymmetric because we can obtain the following new formula by exchanging <math>a</math> and <math>b</math> in \n<math>m_{3}, \\ \\  m_{4}</math> and <math>m_{5}</math>.\n\n:<math>\\begin{align}\n\\left\\{ {\n\\begin{array}{l}\nc_{0}=p_{0} \\\\ \nc_{1}=p_{012}+p_{2}+m_{4}+m_{5} \\\\ \nc_{2}=p_{012}+m_{3}+m_{5} \\\\ \nc_{3}=p_{012}+p_{0}+m_{3}+m_{4} \\\\ \nc_{4}=p_{2},\n\\end{array}\n}\\right.\n\\end{align}</math>\nwhere \n<math>m_{3}=(a_{0}+a_{2})(b_{1}+b_{2}), \\ \\ m_{4}=(a_{1}+a_{2})(b_{0}+b_{1})</math> and  <math>m_{5}=(a_{0}+a_{1})(b_{0}+b_{2})</math>.\n\n==Efficiency analysis==\nKaratsuba's basic step works for any base ''B'' and any ''m'', but the recursive algorithm is most efficient when ''m'' is equal to ''n''/2, rounded up. In particular, if ''n'' is 2<sup>''k''</sup>, for some integer ''k'', and the recursion stops only when ''n'' is 1, then the number of single-digit multiplications is 3<sup>''k''</sup>, which is ''n''<sup>''c''</sup> where ''c'' = log<sub>2</sub>3.\n\nSince one can extend any inputs with zero digits until their length is a power of two, it follows that the number of elementary multiplications, for any ''n'', is at most <math>3^{ \\lceil\\log_2 n \\rceil} \\leq 3 n^{\\log_2 3}\\,\\!</math>.\n\nSince the additions, subtractions, and digit shifts (multiplications by powers of ''B'') in Karatsuba's basic step take time proportional to ''n'', their cost becomes negligible as ''n'' increases. More precisely, if ''t''(''n'') denotes the total number of elementary operations that the algorithm performs when multiplying two ''n''-digit numbers, then\n\n:<math>T(n) = 3 T(\\lceil n/2\\rceil) + cn + d</math>\n\nfor some constants ''c'' and ''d''. For this [[recurrence relation]], the [[master theorem (analysis of algorithms)|master theorem for divide-and-conquer recurrences]] gives the [[big O notation|asymptotic]] bound <math>T(n) = \\Theta(n^{\\log_2 3})\\,\\!</math>.\n\nIt follows that, for sufficiently large ''n'', Karatsuba's algorithm will perform fewer shifts and single-digit additions than longhand multiplication, even though its basic step uses more additions and shifts than the straightforward formula. For small values of ''n'', however, the extra shift and add operations may make it run slower than the longhand method. The point of positive return depends on the [[computer platform]] and context. As a rule of thumb, Karatsuba's method is usually faster when the multiplicands are longer than 320–640 bits.<ref>{{Cite web|url=http://www.cburch.com/proj/karat/comment1.html|title=Karatsuba multiplication|website=www.cburch.com}}</ref>\n\n==Pseudocode==\nHere is the pseudocode for this algorithm, using numbers represented in base ten. For the binary representation of integers, it suffices to replace everywhere 10 by 2.<ref>{{cite book |last= Weiss |first= Mark A. |date= 2005 |title= Data Structures and Algorithm Analysis in C++ |location= |publisher= Addison-Wesley|page= 480|isbn= 0321375319}}</ref>\n\n<syntaxhighlight lang=\"C\">\nprocedure karatsuba(num1, num2)\n  if (num1 < 10) or (num2 < 10)\n    return num1*num2\n\n  /* calculates the size of the numbers */\n  m = min(size_base10(num1), size_base10(num2))\n  m2 = floor(m/2) \n  /*m2 = ceil(m/2) will also work */\n\n  /* split the digit sequences in the middle */\n  high1, low1 = split_at(num1, m2)\n  high2, low2 = split_at(num2, m2)\n\n  /* 3 calls made to numbers approximately half the size */\n  z0 = karatsuba(low1, low2)\n  z1 = karatsuba((low1 + high1), (low2 + high2))\n  z2 = karatsuba(high1, high2)\n\n  return (z2 * 10 ^ (m2 * 2)) + ((z1 - z2 - z0) * 10 ^ m2) + z0\n</syntaxhighlight>\n\n==References==\n{{Reflist}}\n\n==External links==\n* [http://www.cs.pitt.edu/~kirk/cs1501/animations/Karatsuba.html Karatsuba's Algorithm for Polynomial Multiplication]\n*{{MathWorld|urlname=KaratsubaMultiplication|title=Karatsuba Multiplication}}\n* Bernstein, D. J., \"[http://cr.yp.to/papers/m3.pdf Multidigit multiplication for mathematicians]\". Covers Karatsuba and many other multiplication algorithms.\n\n{{Number-theoretic algorithms}}\n\n[[Category:Computer arithmetic algorithms]]\n[[Category:Multiplication]]\n[[Category:Articles with example pseudocode]]"
    },
    {
      "title": "Knuth's Simpath algorithm",
      "url": "https://en.wikipedia.org/wiki/Knuth%27s_Simpath_algorithm",
      "text": "'''Simpath''' is an [[algorithm]] introduced by [[Donald Knuth]] that constructs a [[zero-suppressed decision diagram]] (ZDD) representing all simple paths between two vertices in a given graph.<ref>{{cite book|last1=Knuth|first1=Donald|title=The Art of Computer Programming, Volume 4A|date=2011|publisher=Addison-Wesley Professional: Boston, MA, USA|page=254,275}}</ref><ref>{{cite journal|title=Finding All Solutions and Instances of Numberlink and Slitherlink by ZDDs|journal=Algorithms|date=2012|volume=5|pages=176–213|doi=10.3390/a5020176}}</ref>\n\n==References==\n{{Reflist}}\n\n==External links==\n{{Wikiquote}}\n{{Commons category|Donald Ervin Knuth}}\n* [https://github.com/takemaru/graphillion Graphillion library] which implements the algorithm for manipulating large sets of paths and other structures.\n* [http://www-cs-faculty.stanford.edu/~knuth/programs.html#simpath], A CWEB implementation by Donald Knuth.\n\n\n{{Donald Knuth navbox}}\n\n\n[[Category:Computer arithmetic algorithms]]\n[[Category:Donald Knuth]]\n[[Category:Graph algorithms]]\n[[Category:Mathematical logic]]\n[[Category:Theoretical computer science]]\n\n\n{{-}}\n{{algorithm-stub}}"
    },
    {
      "title": "Methods of computing square roots",
      "url": "https://en.wikipedia.org/wiki/Methods_of_computing_square_roots",
      "text": "{{multiple issues|\n{{Original research|date=January 2012}}\n{{technical|date=September 2012}}\n{{more citations needed|date=July 2017}}\n{{very long|date=June 2019|rps=61}}\n}}\n\n[[File:Examples for calculating the square root of 2 with Heron's method.svg|thumb|The sequence we get by computing the square root of two with the Babylonian method with different starting points]]\nIn [[numerical analysis]], a branch of mathematics, there are several '''square root algorithms''' or '''methods of computing the principal [[square root]]''' of a [[nonnegative|non-negative]] [[real number]]. For the square roots of a negative or [[complex number]], see [[#Negative or complex square|below]].\n\nFinding <math>\\sqrt{S}</math> is the same as solving the equation <math>f(x) = x^2 - S = 0\\,\\!</math> for a positive <math>x</math>. Therefore, any general numerical [[root-finding algorithm]] can be used. [[Newton's method]], for example, reduces in this case to the so-called Babylonian method:\n:<math>x_{n+1}=x_n-\\frac{f(x_n)}{f'(x_n)}=x_n-\\frac{x_n^2-S}{2x_n}=\\frac{1}{2}\\left(x_n+\\frac{S}{x_n}\\right)</math>\n\nThese methods generally yield approximate results, but can be made arbitrarily precise by increasing the number of calculation steps.\n\n==Rough estimation==\nMany square root algorithms require an initial [[seed value]]. If the initial seed value is far away from the actual square root, the algorithm will be slowed down. It is therefore useful to have a rough estimate, which may be very inaccurate but easy to calculate. With <math>S</math> expressed in [[scientific notation]] as <math>a\\times10^{2n}</math> where <math> 1\\leq a<100</math> and ''n'' is an integer, the square root <math>\\sqrt{S} = \\sqrt{a}\\times10^n</math> can be estimated as\n:<math> \\sqrt{S} \\approx \\begin{cases}\n2 \\cdot 10^n & \\text{if } a < 10, \\\\\n6 \\cdot 10^n & \\text{if } a \\geq 10.\n\\end{cases}</math>\n\nThe factors two and six are used because they approximate the [[geometric mean]]s of the lowest and highest possible values with the given number of digits: <math>\\sqrt{\\sqrt{1} \\cdot \\sqrt{10}} = \\sqrt[4]{10} \\approx 2 \\,</math> and <math>\\sqrt{\\sqrt{10} \\cdot \\sqrt{100}} = \\sqrt[4]{1000} \\approx 6 \\,</math>.\n\nFor <math> S = 125348 = 12.5348 \\times 10^4</math>, the estimate is <math> \\sqrt{S} \\approx 6 \\cdot 10^2 = 600</math>.\n\nWhen working in the [[binary numeral system]] (as computers do internally), by expressing <math>S</math> as <math>a\\times2^{2n}</math> where <math> 0.1_2\\leq a<10_2</math>, the square root <math>\\sqrt{S} = \\sqrt{a}\\times2^n</math> can be estimated as <math> \\sqrt{S} \\approx 2^n</math>, since the geometric mean of the lowest and highest possible values is <math>\\sqrt{\\sqrt{0.1_2} \\cdot \\sqrt{10_2}} = \\sqrt[4]{1} = 1</math>.\n\nFor <math> S = 125348 = 1\\;1110\\;1001\\;1010\\;0100_2 = 1.1110\\;1001\\;1010\\;0100_2\\times2^{16}\\, </math> the binary approximation gives <math> \\sqrt{S} \\approx 2^8 = 1\\;0000\\;0000_2 = 256\\, .</math>\n\nThese approximations are useful to find better seeds for iterative algorithms, which results in faster convergence.\n\n==Babylonian method==<!-- this section is linked from [[Babylonian method]] -->\n{{redirect|Heron's method|the formula used to find the area of a triangle|Heron's formula}}\n[[Image:Babylonian method.svg|300px|right|thumb|\nGraph charting the use of the Babylonian method for approximating a square root of 100 (±10) using starting values\n<span style=\"color:#008\">''x''<sub>0</sub>&nbsp;=&nbsp;50</span>,\n<span style=\"color:#F00\">''x''<sub>0</sub>&nbsp;=&nbsp;1</span>,\nand <span style=\"color:#080\">''x''<sub>0</sub>&nbsp;=&nbsp;&minus;5</span>. Note that a positive starting value yields the positive root, and a negative starting value the negative root.]]\n\nPerhaps the first [[algorithm]] used for approximating <math>\\sqrt{S}</math> is known as the '''Babylonian method''', despite there being no direct evidence, beyond informed conjecture, that the eponymous [[Babylonian mathematics|Babylonian mathematicians]] employed exactly this method.<ref name=\"Fowler and Robson\">{{cite journal |last1=Fowler |first1=David |last2=Robson |first2=Eleanor |title=Square Root Approximations in Old Babylonian Mathematics: YBC 7289 in Context |journal=Historia Mathematica |volume=25 |date=1998 |issue=25 |page=376 |url=https://www.sciencedirect.com/science/article/pii/S0315086098922091 |accessdate=14 October 2018|doi=10.1006/hmat.1998.2209 }}</ref> The method is also known as '''Heron's method''', after the first-century Greek mathematician [[Hero of Alexandria]] who gave the first explicit description of the method in his [[AD 60]] work ''[[Hero of Alexandria#Bibliography|Metrica]]''.<ref>{{cite book\n  | last = Heath\n  | first = Thomas\n  | authorlink = Thomas Little Heath\n  | title = A History of Greek Mathematics, Vol. 2\n  | publisher = Clarendon Press\n  | year = 1921\n  | location = Oxford\n  | pages = 323–324\n  | url = https://books.google.com/books?id=LOA5AAAAMAAJ&pg=PR323\n  | doi =\n  | id =\n  | isbn = }}</ref> The basic idea is that if {{mvar|x}} is an overestimate to the square root of a non-negative real number {{mvar|S}} then {{math|{{sfrac|''S''|''x''}}}} will be an underestimate, or vice versa, and so the average of these two numbers may reasonably be expected to provide a better approximation (though the formal proof of that assertion depends on the [[inequality of arithmetic and geometric means]] that shows this average is always an overestimate of the square root, as noted in the article on [[Square root#Geometric construction of the square root|square roots]], thus assuring convergence).\n\nMore precisely, if {{mvar|x}} is our initial guess of <math>\\sqrt{S}</math> and {{mvar|e}} is the error in our estimate such that {{math|1=''S'' = (''x''+ ''e'')<sup>2</sup>}}, then we can expand the binomial and solve for \n:<math>e=\\frac{S-x^2}{2x+e} \\approx \\frac{S-x^2}{2x}, </math> since <math> e \\ll x </math>.\nTherefore, we can compensate for the error and update our old estimate as \n:<math>x + e \\approx x + \\frac{S-x^2}{2x} = \\frac{S+x^2}{2x} = \\frac{\\frac{S}{x}+x}{2} \\equiv x_\\text{revised}</math> \nSince the computed error was not exact, this becomes our next best guess. The process of updating is iterated until desired accuracy is obtained. This is a [[quadratic convergence|quadratically convergent]] algorithm, which means that the number of correct digits of the approximation roughly doubles with each iteration. It proceeds as follows:\n#Begin with an arbitrary positive starting value {{math|''x''<sub>0</sub>}} (the closer to the actual square root of {{mvar|S}}, the better).\n#Let {{math|''x''<sub>''n'' + 1</sub>}} be the average of {{math|''x''<sub>''n''</sub>}} and {{math|{{sfrac|''S''|''x''<sub>''n''</sub>}}}} (using the [[arithmetic mean]] to approximate the [[geometric mean]]).\n#Repeat step 2 until the desired accuracy is achieved.\nIt can also be represented as:\n:<math>x_0 \\approx \\sqrt{S},</math>\n:<math>x_{n+1} = \\frac{1}{2} \\left(x_n + \\frac{S}{x_n}\\right),</math>\n:<math>\\sqrt S = \\lim_{n \\to \\infty} x_n.</math>\n\nThis algorithm works equally well in the [[p-adic number|{{mvar|p}}-adic number]]s, but cannot be used to identify real square roots with {{mvar|p}}-adic square roots; one can, for example, construct a sequence of rational numbers by this method that converges to +3 in the reals, but to &minus;3 in the 2-adics.\n\nIt can be derived from [[Newton's method#Square_root_of_a_number|Newton's method]] (which it predates by 16 centuries):\n:Writing <math>f(x) = x^2 - S</math>,\n:<math>x_{n+1} = x_n - \\dfrac{f(x_n)}{f'(x_n)} = x_n - \\dfrac{x_n^2 - S}{2 x_n} = \\dfrac{1}{2}\\Bigl(2 x_n - \\bigl(x_n - \\dfrac{S}{x_n}\\bigr)\\Bigr) = \\dfrac{1}{2}\\Bigl(x_n + \\dfrac{S}{x_n}\\Bigr) _.</math>\n\n===Example===\nTo calculate {{math|{{sqrt|''S''}}}}, where {{mvar|S}} = 125348, to six significant figures, use the rough estimation method above to get\n\n:<math>\\begin{align}\n\\begin{array}{rlll}\nx_0 & = 6 \\cdot 10^2 && = 600.000 \\\\[0.3em]\nx_1 & = \\frac{1}{2} \\left(x_0 + \\frac{S}{x_0}\\right) & = \\frac{1}{2} \\left(600.000 + \\frac{125348}{600.000}\\right) & = 404.457 \\\\[0.3em]\nx_2 & = \\frac{1}{2} \\left(x_1 + \\frac{S}{x_1}\\right) & = \\frac{1}{2} \\left(404.457 + \\frac{125348}{404.457}\\right) & = 357.187 \\\\[0.3em]\nx_3 & = \\frac{1}{2} \\left(x_2 + \\frac{S}{x_2}\\right) & = \\frac{1}{2} \\left(357.187 + \\frac{125348}{357.187}\\right) & = 354.059 \\\\[0.3em]\nx_4 & = \\frac{1}{2} \\left(x_3 + \\frac{S}{x_3}\\right) & = \\frac{1}{2} \\left(354.059 + \\frac{125348}{354.059}\\right) & = 354.045 \\\\[0.3em]\nx_5 & = \\frac{1}{2} \\left(x_4 + \\frac{S}{x_4}\\right) & = \\frac{1}{2} \\left(354.045 + \\frac{125348}{354.045}\\right) & = 354.045\n\\end{array}\n\\end{align}</math>\n\nTherefore, {{math|{{sqrt|125348}} ≈ 354.045}}.\n\n===Convergence===\nSuppose that ''x''<sub>0</sub> > 0 and ''S'' > 0. Then for any natural number ''n'', ''x''<sub>''n''</sub> > 0. Let the [[relative error]] in ''x''<sub>''n''</sub> be defined by\n\n:<math>\\varepsilon_n = \\frac {x_n}{\\sqrt{S}} - 1 > -1</math>\n\nand thus\n:<math>x_n = \\sqrt {S} \\cdot (1 + \\varepsilon_n) .</math>\n\nThen it can be shown that\n:<math>\\varepsilon_{n+1} = \\frac {\\varepsilon_n^2}{2 (1 + \\varepsilon_n)} \\geq 0 .</math>\n\nAnd thus that\n:<math>\\varepsilon_{n+2} \\leq \\min \\left\\{\\frac {\\varepsilon_{n+1}^2}{2}, \\frac {\\varepsilon_{n+1}}{2} \\right\\}</math>\n\nand consequently that convergence is assured, and [[quadratic convergence|quadratic]].\n\n====Worst case for convergence====\nIf using the rough estimate above with the Babylonian method, then the least accurate cases in ascending order are as follows:\n\n:<math>\n\\begin{align}\nS & = 1;   & x_0 & = 2; & x_1 & = 1.250;  & \\varepsilon_1 & = 0.250. \\\\\nS & = 10;  & x_0 & = 2; & x_1 & = 3.500;  & \\varepsilon_1 & < 0.107. \\\\\nS & = 10;  & x_0 & = 6; & x_1 & = 3.833;  & \\varepsilon_1 & < 0.213. \\\\\nS & = 100; & x_0 & = 6; & x_1 & = 11.333; & \\varepsilon_1 & < 0.134.\n\\end{align}\n</math>\n\nThus in any case,\n:<math> \\varepsilon_1 \\leq 2^{-2}. \\, </math>\n:<math> \\varepsilon_2 < 2^{-5} < 10^{-1}. \\, </math>\n:<math> \\varepsilon_3 < 2^{-11} < 10^{-3}. \\, </math>\n:<math> \\varepsilon_4 < 2^{-23} < 10^{-6}. \\, </math>\n:<math> \\varepsilon_5 < 2^{-47} < 10^{-14}. \\, </math>\n:<math> \\varepsilon_6 < 2^{-95} < 10^{-28}. \\, </math>\n:<math> \\varepsilon_7 < 2^{-191} < 10^{-57}. \\, </math>\n:<math> \\varepsilon_8 < 2^{-383} < 10^{-115}. \\, </math>\n\nRounding errors will slow the convergence. It is recommended to keep at least one extra digit beyond the desired accuracy of the {{mvar|x<sub>n</sub>}} being calculated to minimize round off error.\n\n====Scaling Heron’s algorithm====\nEfficiency of Heron algorithm can be enhanced by prescaling.\n\nIf <math>S \\in [0.5,2.0]</math> then, using a starting guess <math>x_0=1.0</math>, Heron algorithm proceeds with quadratic convergence and 5 iterations of the algorithm are sufficient to compute <math>\\sqrt{S}</math> with 16 digit accuracy.\n\nPrescaling consists in finding an integer number {{mvar | k}} such that <math>S=2^{2k} \\tilde{S}</math>, where <math>\\tilde S \\in [0.5, 2.0]</math>, then solve the intermediate problem <math>z=\\sqrt{\\tilde{S}}</math> by performing a fixed number of iterations of the Heron method starting from <math> z_0=1.0</math>, finally obtain the solution <math> x = 2^k z</math>.\n\nThis technique is particularly effective when working with floating point numbers in binary format (e.g. IEEE-754), as the determination of {{mvar | k}} and <math>\\tilde{S}</math> is trivial. In fact, these numbers are represented in the format <math>1.xxx\\dots 2 ^p</math> where mantissa and exponent are immediately available. Then if {{mvar | p}} is even, then <math>\\tilde{S}=1.xxx \\dots</math> and <math>k = p/2</math>, otherwise <math>\\tilde{S}=1.xxx\\dots / 2.0</math> and <math>k = (p+1)/2</math>. Note that there is no floating point mathematical operation involved here, as multiplication and division by powers of 2 can be done with trivial integer operations on the exponent. Similarly, no floating point operation is required when computing the final solution <math> x = 2^k z</math>.\n\n==Bakhshali method==\nThis method for finding an approximation to a square root was described in an ancient Indian mathematical manuscript called the [[Bakhshali manuscript]]. It is equivalent to two iterations of the Babylonian method beginning with ''x''<sub>0</sub>. Thus, the algorithm is quartically convergent, which means that the number of correct digits of the approximation roughly quadruples with each iteration.<ref>{{cite news|title=Ancient Indian Square Roots: An Exercise in Forensic Paleo-Mathematics| last1=Bailey| first1=David| last2=Borwein|authorlink2=Jonathan_Borwein| first2=Jonathan| journal=American Mathematical Monthly|volume = 119| issue=8| pages=646–657| date=2012| url=http://crd-legacy.lbl.gov/~dhbailey/dhbpapers/india-sqrt.pdf| access-date=2017-09-14}}</ref> The original presentation, using modern notation, is as follows: To calculate <math>\\sqrt{S}</math>, let ''x''<sub>0</sub><sup>2</sup> be the initial approximation to ''S''. Then, successively iterate as:\n:<math>a_n = \\frac{S - x_n^2}{2 x_n},</math>\n\n:<math>b_n = x_n + a_n,</math>\n\n:<math>x_{n+1} = b_n - \\frac{a_n^2}{2 b_n}.</math>\n\nWritten explicitly, it becomes \n:<math> x_{n+1} = (x_n + a_n) - \\frac{a_n^2}{2 (x_n + a_n)}. </math>\n\nLet ''x''<sub>0</sub> = ''N'' be an integer which is the nearest perfect square to ''S''. Also, let the difference ''d'' = ''S'' - ''N''<sup>2</sup>, then the first iteration can be written as:\n:<math>\\sqrt{S} \\approx N + \\frac{d}{2N} - \\frac{d^2}{8N^3 + 4Nd} = \\frac{8N^4 + 8N^2 d + d^2}{8N^3 + 4Nd} = \\frac{N^4 + 6N^2S + S^2}{4N^3 + 4NS} = \\frac{N^2(N^2 + 6S) + S^2}{4N(N^2 + S)}.</math>\n\nThis gives a rational approximation to the square root.\n\n===Example===\nUsing the same example as given in Babylonian method, let <math>S = 125348.</math> Then, the first iterations gives\n\n:<math> x_0 = 600 </math>\n\n:<math> a_1 = \\frac{125348 - 600^2}{2 \\times 600} = -195.543 </math>\n\n:<math> b_1 = 600 + (-195.543) = 404.456 </math>\n\n:<math> x_1 = 404.456 - \\frac{(-195.543)^2}{2 \\times 404.456} = 357.186</math>\n\nLikewise the second iteration gives\n\n:<math> a_2 = \\frac{125348 - 357.186^2}{2 \\times 357.186} = -3.126 </math>\n\n:<math> b_2 = 357.186 + (-3.126) = 354.06 </math>\n\n:<math> x_2 = 354.06 - \\frac{(-3.1269)^2}{2 \\times 354.06} = 354.046 </math>\n\n==Digit-by-digit calculation==\nThis is a method to find each digit of the square root in a sequence. It is slower than the Babylonian method, but it has several advantages:\n* It can be easier for manual calculations.\n* Every digit of the root found is known to be correct, i.e., it does not have to be changed later.\n* If the square root has an expansion that terminates, the algorithm terminates after the last digit is found. Thus, it can be used to check whether a given integer is a [[square number]].\n* The algorithm works for any [[numeral system|base]], and naturally, the way it proceeds depends on the base chosen.\n\n[[Napier's bones]] include an aid for the execution of this algorithm. The [[shifting nth root algorithm|shifting ''n''th root algorithm]] is a generalization of this method.\n\n===Basic principle===\nFirst, consider the case of finding the square root of a number ''Z'', that is the square of a two-digit number ''XY'', where ''X'' is the tens digit and ''Y'' is the units digit. Specifically:\n\nZ = (10X + Y)<sup>2</sup> = 100X<sup>2</sup> + 20XY + Y<sup>2</sup>\n\nNow using the Digit-by-Digit algorithm, we first determine the value of ''X''. ''X'' is the largest digit such that X<sup>2</sup> is less or equal to ''Z'' from which we removed the two rightmost digits.\n\nIn the next iteration, we pair the digits, multiply ''X'' by 2, and place it in the tenth's place while we try to figure out what the value of ''Y'' is.\n\nSince this is a simple case where the answer is a perfect square root ''XY'', the algorithm stops here.\n\nThe same idea can be extended to any arbitrary square root computation next. Suppose we are able to find the square root of ''N'' by expressing it as a sum of ''n'' positive numbers such that\n:<math>N = (a_1+a_2+a_3+\\dotsb+a_n)^2.</math>\n\nBy repeatedly applying the basic identity \n:<math>(x+y)^2 = x^2 +2xy + y^2,</math> \nthe right-hand-side term can be expanded as\n:<math>\n\\begin{align}\n& (a_1+a_2+a_3+ \\dotsb +a_n)^2 \\\\\n=& \\, a_1^2 + 2a_1a_2 + a_2^2 + 2(a_1+a_2) a_3 + a_3^2 + \\dotsb + a_{n-1}^2 + 2  \\left(\\sum_{i=1}^{n-1} a_i\\right) a_n + a_n^2 \\\\\n=& \\, a_1^2 + [2a_1 + a_2] a_2 + [2(a_1+a_2) + a_3] a_3 + \\dotsb + \\left[2 \\left(\\sum_{i=1}^{n-1} a_i\\right) + a_n\\right] a_n.\n\\end{align}\n</math>\n\nThis expression allows us to find the square root by sequentially guessing the values of <math>a_i</math>s. Suppose that the numbers <math>a_1, \\ldots, a_{m-1}</math> have already been guessed, then the m-th term of the right-hand-side of above summation is given by <math>Y_{m} = [2 P_{m-1} + a_{m}]a_{m},</math> where <math>P_{m-1} = \\sum_{i=1}^{m-1} a_i</math> is the approximate square root found so far. Now each new guess <math>a_m</math> should satisfy the recursion \n:<math>X_{m} = X_{m-1} - Y_{m},</math> \nsuch that <math>X_m \\geq 0</math> for all <math>1\\leq m\\leq n,</math> with initialization <math>X_0 = N.</math> When <math>X_n = 0,</math> the exact square root has been found; if not, then the sum of <math>a_i</math>s gives a suitable approximation of the square root, with <math>X_n</math> being the approximation error.\n\nFor example, in the decimal number system we have \n:<math>N = (a_1 \\cdot 10^{n-1} + a_2 \\cdot 10^{n-2} + \\cdots + a_{n-1} \\cdot 10 + a_n)^2,</math> \nwhere <math>10^{n-i}</math> are place holders and the coefficients <math>a_i \\in \\{0,1,2,\\ldots,9\\}</math>. At any m-th stage of the square root calculation, the approximate root found so far, <math>P_{m-1}</math> and the summation term <math>Y_m</math> are given by \n:<math>P_{m-1} = \\sum_{i=1}^{m-1} a_i \\cdot 10^{n-i} = 10^{n-m+1} \\sum_{i=1}^{m-1} a_i \\cdot 10^{m-i-1},</math>\n:<math>Y_m = [2P_{m-1} + a_m \\cdot 10^{n-m}] a_m \\cdot 10^{n-m} = [20 \\sum_{i=1}^{m-1} a_i \\cdot 10^{m-i-1} + a_m ] a_m \\cdot 10^{2(n-m)}.</math>\n\nHere since the place value of <math>Y_m</math> is an even power of 10, we only need to work with the pair of most significant digits of the remaining term <math>X_{m-1}</math> at any m-th stage. The section below codifies this procedure.\n\nIt is obvious that a similar method can be used to compute the square root in number systems other than the decimal number system. For instance, finding the digit-by-digit square root in the binary number system is quite efficient since the value of <math>a_i</math> is searched from a smaller set of binary digits {0,1}. This makes the computation faster since at each stage the value of <math>Y_m</math> is either <math>Y_m = 0</math> for <math>a_m = 0</math> or <math>Y_m = 2 P_{m-1} + 1</math> for <math>a_m = 1</math>. The fact that we have only two possible options for <math>a_m</math> also makes the process of deciding the value of <math>a_m</math> at m-th stage of calculation easier. This is because we only need to check if <math>Y_m \\leq X_{m-1}</math> for <math>a_m = 1.</math> If this condition is satisfied, then we take <math>a_m = 1</math>; if not then <math>a_m = 0.</math> Also, the fact that multiplication by 2 is done by left bit-shifts helps in the computation.\n\n===Decimal (base 10)===\nWrite the original number in decimal form. The numbers are written similar to the [[long division]] algorithm, and, as in long division, the root will be written on the line above. Now separate the digits into pairs, starting from the decimal point and going both left and right. The decimal point of the root will be above the decimal point of the square. One digit of the root will appear above each pair of digits of the square.\n\nBeginning with the left-most pair of digits, do the following procedure for each pair:\n\n# Starting on the left, bring down the most significant (leftmost) pair of digits not yet used (if all the digits have been used, write \"00\") and write them to the right of the remainder from the previous step (on the first step, there will be no remainder). In other words, multiply the remainder by 100 and add the two digits. This will be the '''current value ''c'''''.\n# Find ''p'', ''y'' and ''x'', as follows:\n#* Let '''''p''''' be the '''part of the root found so far''', ignoring any decimal point. (For the first step, ''p'' = 0.)\n#* Determine the greatest digit '''''x''''' such that <math>x(20p + x) \\le c</math>. We will use a new variable '''''y''''' = ''x''(20''p'' + ''x'').\n#** Note: 20''p'' + ''x'' is simply twice ''p'', with the digit ''x'' appended to the right).\n#** Note: ''x'' can be found by guessing what ''c''/(20·''p'') is and doing a trial calculation of ''y'', then adjusting ''x'' upward or downward as necessary.\n#* Place the digit <math>x</math> as the next digit of the root, i.e., above the two digits of the square you just brought down. Thus the next ''p'' will be the old ''p'' times 10 plus ''x''.\n# Subtract ''y'' from ''c'' to form a new remainder.\n# If the remainder is zero and there are no more digits to bring down, then the algorithm has terminated. Otherwise go back to step 1 for another iteration.\n\n====Examples====\n'''Find the square root of 152.2756.'''\n\n         <u>  1  2. 3  4 </u>\n        /\n      \\/  01 52.27 56\n \n          01                   1*1 <= 1 < 2*2                 x = 1\n         <u> 01 </u>                    y = x*x = 1*1 = 1\n          00 52                22*2 <= 52 < 23*3              x = 2\n         <u> 00 44 </u>                 y = (20+x)*x = 22*2 = 44\n             08 27             243*3 <= 827 < 244*4           x = 3\n            <u> 07 29 </u>              y = (240+x)*x = 243*3 = 729\n                98 56          2464*4 <= 9856 < 2465*5        x = 4\n               <u> 98 56 </u>           y = (2460+x)*x = 2464*4 = 9856\n                00 00          Algorithm terminates: Answer is 12.34\n\n'''Find the square root of 2.'''\n\n         <u>  1. 4  1  4  2</u>\n        /\n      \\/  02.00 00 00 00\n \n          02                  1*1 <= 2 < 2*2                 x = 1\n         <u> 01 </u>                   y = x*x = 1*1 = 1\n          01 00               24*4 <= 100 < 25*5             x = 4\n         <u> 00 96 </u>                y = (20+x)*x = 24*4 = 96\n             04 00            281*1 <= 400 < 282*2           x = 1\n            <u> 02 81 </u>             y = (280+x)*x = 281*1 = 281\n             01 19 00         2824*4 <= 11900 < 2825*5       x = 4\n            <u> 01 12 96 </u>          y = (2820+x)*x = 2824*4 = 11296\n                06 04 00      28282*2 <= 60400 < 28283*3     x = 2\n                              The desired precision is achieved:\n                              The square root of 2 is about 1.4142\n\n===Binary numeral system (base 2)===\nInherent to digit-by-digit algorithms is a search and test step:  find a digit, <math>\\, e</math>, when added to the right of a current solution <math>\\, r</math>, such that <math>\\,(r+e)\\cdot(r+e) \\le x</math>, where <math>\\, x</math> is the value for which a root is desired. Expanding: <math>\\,r\\cdot r + 2re + e\\cdot e \\le x</math>.  The current value of <math>\\,r\\cdot r</math>—or, usually, the remainder—can be incrementally updated efficiently when working in binary, as the value of <math>\\, e</math> will have a single bit set (a power of 2), and the operations needed to compute <math>\\,2\\cdot r\\cdot e</math> and <math>\\,e\\cdot e</math> can be replaced with faster [[Bitwise operation#Bit shifts|bit shift]] operations.\n\n====Example====\nHere we obtain the square root of 81, which when converted into binary gives 1010001. The numbers in the left column gives the option between that number or zero to be used for subtraction at that stage of computation. The final answer is 1001, which in decimal is 9.\n\n              1 0 0 1\n             ---------\n            √ 1010001\n \n       1      1\n              1\n             ---------\n       101     01  \n                0\n              --------\n       1001     100\n                  0\n              --------\n       10001    10001\n                10001\n               -------\n                    0\n\nThis gives rise to simple computer implementations:<ref>[https://web.archive.org/web/20120306040058/http://medialab.freaknet.org/martin/src/sqrt/sqrt.c  Fast integer square root by Mr. Woo's abacus algorithm (archived)]</ref>\n\n<syntaxhighlight lang=\"c\">\nshort isqrt(short num) {\n    short res = 0;\n    short bit = 1 << 14; // The second-to-top bit is set: 1 << 30 for 32 bits\n \n    // \"bit\" starts at the highest power of four <= the argument.\n    while (bit > num)\n        bit >>= 2;\n        \n    while (bit != 0) {\n        if (num >= res + bit) {\n            num -= res + bit;\n            res = (res >> 1) + bit;\n        }\n        else\n            res >>= 1;\n        bit >>= 2;\n    }\n    return res;\n}\n</syntaxhighlight>\n\nUsing the notation above, the variable \"bit\" corresponds to <math>e_m^2</math> which is <math>(2^m)^2=4^m</math>, the variable \"res\" is equal to <math>2re_m</math>, and the variable \"num\" is equal to the current <math>X_m</math> which is the difference of the number we want the square root of and the square of our current approximation with all bits set up to <math>2^{m+1}</math>. Thus in the first loop, we want to find the highest power of 4 in \"bit\" to find the highest power of 2 in <math>e</math>. In the second loop, if num is greater than res + bit, then <math>X_m</math> is greater than <math>2re_m+e_m^2</math> and we can subtract it. The next line, we want to add <math>e_m</math> to <math>r</math> which means we want to add <math>2e^2_m</math> to <math>2re_m</math> so we want <code>res = res + bit<<1</code>. Then update <math>e_m</math> to <math>e_{m-1}</math> inside res which involves dividing by 2 or another shift to the right. Combining these 2 into one line leads to <code>res = res>>1 + bit</code>. If <math>X_m</math> isn't greater than <math>2re_m+e_m^2</math> then we just update <math>e_m</math> to <math>e_{m-1}</math> inside res and divide it by 2. Then we update <math>e_m</math> to <math>e_{m-1}</math> in bit by dividing it by 4. The final iteration of the 2nd loop has bit equal to 1 and will cause update of <math>e</math> to run one extra time removing the factor of 2 from res making it our integer approximation of the root.\n\nFaster algorithms, in binary and decimal or any other base, can be realized by using lookup tables—in effect trading [[space–time tradeoff|more storage space for reduced run time]].<ref>[http://atoms.alife.co.uk/sqrt/SquareRoot.java Integer Square Root function]</ref>\n\n==Exponential identity==\n[[calculator|Pocket calculator]]s typically implement good routines to compute the [[exponential function]] and the [[natural logarithm]], and then compute the square root of ''S'' using the identity found using the properties of logarithms (<math>\\ln x^n = n \\ln x</math>) and exponentials (<math>e^{\\ln x} = x</math>):\n\n:<math>\\sqrt{S} = e^{\\frac{1}{2}\\ln S}.</math>\nThe denominator in the fraction corresponds to the ''n''th root. In the case above the denominator is 2, hence the equation specifies that the square root is to be found. The same identity is used when computing square roots with [[logarithm table]]s or [[slide rule]]s.\n\n==Vedic duplex method for extracting a square root==\nThe Vedic duplex method from the book '[[Vedic Mathematics (book)|Vedic Mathematics]]' is a variant of the digit-by-digit method for calculating the square root.<ref>{{cite book|title=Vedic Mathematics or Sixteen Simple Mathematical Formulae from the Vedas |author=Sri Bharati Krisna Tirthaji |isbn=978-8120801639 |publisher=Motilal Banarsidass |year=2008 |origyear=1965}}</ref> The '''duplex''' is the square of the central digit plus double the cross-product of digits equidistant from the center. The duplex is computed from the quotient digits (square root digits) computed thus far, but after the initial digits. The duplex is subtracted from the dividend digit prior to the second subtraction for the product of the quotient digit times the divisor digit. For perfect squares the duplex and the dividend will get smaller and reach zero after a few steps. For non-perfect squares the decimal value of the square root can be calculated to any precision desired. However, as the decimal places proliferate, the duplex adjustment gets larger and longer to calculate. The duplex method follows the Vedic ideal for an algorithm, one-line, mental calculation. It is flexible in choosing the first digit group and the divisor. Small divisors are to be avoided by starting with a larger initial group.\n\n===Basic principle===\nWe proceed as with the digit-by-digit calculation by assuming that we want to express a number ''N'' as a square of the sum of ''n'' positive numbers as\n:<math>N = (a_0 + a_1 + \\cdots + a_{n-1})^2</math>\n:<math> = a_0^2 + 2a_0 \\sum_{i=1}^{n-1}a_i + a_1^2 + 2a_1 \\sum_{i=2}^{n-1}a_i + \\cdots + a_{n-1}^2.</math>\n\nDefine divisor as <math>q = 2a_0</math> and the duplex for a sequence of ''m'' numbers as \n:<math>d_m = \n\\begin{cases} \na_{\\lceil m/2 \\rceil}^2 + \\sum_{i=1}^{\\lfloor m/2 \\rfloor} 2 a_i a_{m-i+1} & \\text{for } m \\text{ odd} \\\\\n\\sum_{i=1}^{m/2} 2 a_i a_{m-i+1} & \\text{for } m \\text{ even}. \\\\\n\\end{cases} \n</math>\n\nThus, we can re-express the above identity in terms of the divisor and the duplexes as \n:<math>N - a_0^2 = \\sum_{i=1}^{n-1} (q a_i + d_i).</math>\n\nNow the computation can proceed by recursively guessing the values of <math>a_m</math> so that\n:<math>X_m = X_{m-1} - qa_m - d_m, </math> \nsuch that <math>X_m \\geq 0</math> for all <math>1 \\leq m \\leq n-1</math>, with initialization <math>X_0 = N - a_0^2.</math> When <math>X_m = 0</math> the algorithm terminates and the sum of <math>a_i</math>s give the square root. The method is more similar to long division where <math>X_{m-1}</math> is the dividend and <math>X_{m}</math> is the remainder.\n\nFor the case of decimal numbers, if \n:<math>N = (a_0 \\cdot 10^{n-1} + a_1 \\cdot 10^{n-2} + \\cdots + a_{n-2} \\cdot 10 + a_{n-1})^2</math> \nwhere <math>a_i \\in \\{0,1,2,\\ldots,9\\}</math>, then the initiation <math>X_0 = N - a_0^2 \\cdot 10^{2(n-1)}</math> and the divisor will be <math>q =2a_0 \\cdot 10^{n-1}</math>. Also the product at any m-th stage will be <math>qa_m \\cdot 10^{n-m-1} = 2 a_0 a_m \\cdot 10^{2n-m-2}</math> and the duplexes will be <math>d_m = \\tilde{d}_m \\cdot 10^{2n-m-3}</math>, where <math>\\tilde{d}_m</math> are the duplexes of the sequence <math>a_1, a_2, \\ldots, a_m</math>. At any m-th stage, we see that the place value of the duplex <math>\\tilde{d}_m</math> is one less than the product <math>2 a_0 a_m</math>. Thus, in actual calculations it is customary to subtract the duplex value of the m-th stage at (m+1)-th stage. Also, unlike the previous digit-by-digit square root calculation, where at any given m-th stage, the calculation is done by taking the most significant ''pair'' of digits of the remaining term <math>X_{m-1}</math>, the duplex method uses only a ''single'' most significant digit of <math>X_{m-1}</math>.\n\nIn other words, to calculate the '''duplex''' of a number, double the product of each pair of equidistant digits plus the square of the center digit (of the digits to the right of the colon).\n:{{aligned table|cols=2|class=wikitable|row1header=y|col1align=right|col2align=center\n|Number | Calculation {{=}} Duplex\n|3 | 3<sup>2</sup> {{=}} 9\n|14 |2(1·4) {{=}} 8 \n|574 | 2(5·4) + 7<sup>2</sup> {{=}} 89\n|1,421 | 2(1·1) + 2(4·2) {{=}} 2 + 16 {{=}} 18 \n|10,523 | 2(1·3) + 2(0·2) + 5<sup>2</sup> {{=}} 6+0+25 {{=}} 31 \n|406,739 | 2(4·9)+ 2(0·3)+ 2(6·7) {{=}} 72+0+84  {{=}} 156\n}} \nIn a square root calculation the quotient digit set increases incrementally for each step.\n\n===Example===\nConsider the perfect square 2809 = 53<sup>2</sup>. Use the duplex method to find the square root of 2,809.\n\n* Set down the number in '''groups of two digits'''.\n* Define a '''''divisor''''', a '''''dividend''''' and a '''''quotient''''' to find the '''''root'''''.\n* Given 2809. Consider the first group, 28.\n** Find the nearest perfect square below that group.\n** The root of that perfect square is the first digit of our '''root'''.\n** Since 28 > 25 and 25 = 5<sup>2</sup>, take 5 as the first digit in the square root.\n** For the '''divisor''' take double this first digit (2 · 5), which is 10.\n* Next, set up a division framework with a colon.\n** 28: 0 9 is the '''dividend''' and 5: is the '''quotient'''. (''Note: the quotient should always be a single digit number, and it should be such that the dividend in the next stage is non-negative.'')\n** Put a colon to the right of 28 and 5 and keep the colons lined up vertically. The '''duplex''' is calculated only on quotient digits to the right of the colon.\n* Calculate the '''remainder'''. 28: minus 25: is 3:.\n** Append the remainder on the left of the next digit to get the new dividend.\n** Here, append 3 to the next dividend digit 0, which makes the new dividend 30. The divisor 10 goes into 30 just 3 times. (No reserve needed here for subsequent deductions.)\n* Repeat the operation.\n** The zero remainder appended to 9. Nine is the next dividend.\n** This provides a digit to the right of the colon so deduct the duplex, 3<sup>2</sup> = 9.\n** Subtracting this duplex from the dividend 9, a zero remainder results.\n** Ten into zero is zero. The next root digit is zero. The next duplex is 2(3·0) = 0.\n** The dividend is zero. This is an exact square root, 53.\n\n Find the square root of 2809.\n Set down the number in groups of two digits.\n The number of groups gives the number of whole digits in the root.\n Put a colon after the first group, 28, to separate it.\n From the first group, 28, obtain the divisor, 10, since\n 28>25=5<sup>2</sup> and by doubling this first root, 2x5=10.\n        Gross dividend:     28:  0  9. Using mental math:\n               Divisor: 10)     3  0   Square: 10)  28:  <sub>3</sub>0  9\n     Duplex, Deduction:     25: xx 09  Square root:  5:   3. 0\n              Dividend:         30 00\n             Remainder:      3: 00 00\n Square Root, Quotient:      5:  3. 0\n\n==A two-variable iterative method==\nThis method is applicable for finding the square root of <math>0 < S < 3 \\,\\!</math> and converges best for <math>S \\approx 1</math>.\nThis, however, is no real limitation for a computer based calculation, as in base 2 floating point and fixed point representations, it is trivial to multiply <math>S \\,\\!</math> by an integer power of 4, and therefore <math>\\sqrt{S}</math> by the corresponding power of 2, by changing the exponent or by shifting, respectively. Therefore, <math>S \\,\\!</math> can be moved to the range <math>\\frac{1}{2} \\le S <2</math>. Moreover, the following method does not employ general divisions, but only additions, subtractions, multiplications, and divisions by powers of two, which are again trivial to implement. A disadvantage of the method is that numerical errors accumulate, in contrast to single variable iterative methods such as the Babylonian one.\n\nThe initialization step of this method is\n:<math>a_0 = S \\,\\!</math>\n:<math>c_0 = S-1 \\,\\!</math>\nwhile the iterative steps read\n:<math>a_{n+1} = a_n - a_n c_n / 2 \\,\\!</math>\n:<math>c_{n+1} = c_n^2 (c_n - 3) / 4 \\,\\!</math>\nThen, <math>a_n \\rightarrow \\sqrt{S}</math> (while <math>c_n \\rightarrow 0</math>).\n\nNote that the convergence of <math>c_n \\,\\!</math>, and therefore also of <math>a_n \\,\\!</math>, is quadratic.\n\nThe proof of the method is rather easy. First, rewrite the iterative definition of <math>c_n \\,\\!</math> as\n:<math>1 + c_{n+1} = (1 + c_n) (1 - c_n/2)^2  \\,\\!</math>.\nThen it is straightforward to prove by induction that\n:<math>S (1 + c_n) = a_n^2</math>\nand therefore the convergence of <math>a_n \\,\\!</math> to the desired result <math>\\sqrt{S}</math> is ensured by the convergence of <math>c_n \\,\\!</math> to 0, which in turn follows from <math>-1 < c_0 < 2 \\,\\!</math>.\n\nThis method was developed around 1950 by [[Maurice Wilkes|M. V. Wilkes]], [[David Wheeler (computer scientist)|D. J. Wheeler]] and [[Stanley Gill|S. Gill]]<ref>M. V. Wilkes, D. J. Wheeler and S. Gill, \"The Preparation of Programs for an Electronic Digital Computer\", Addison-Wesley, 1951.</ref> for use on [[Electronic Delay Storage Automatic Calculator|EDSAC]], one of the first electronic computers.<ref>M. Campbell-Kelly, \"Origin of Computing\", Scientific American, September 2009.</ref> The method was later generalized, allowing the computation of non-square roots.<ref>J. C. Gower, \"A Note on an Iterative Method for Root Extraction\", The Computer Journal 1(3):142&ndash;143, 1958.</ref>\n\n==Iterative methods for reciprocal square roots==\nThe following are iterative methods for finding the reciprocal square root of ''S'' which is <math>1/\\sqrt{S}</math>. Once it has been found, find <math>\\sqrt{S}</math> by simple multiplication: <math>\\sqrt{S} = S \\cdot (1/\\sqrt{S})</math>. These iterations involve only multiplication, and not division. They are therefore faster than the [[Methods of computing square roots#Babylonian method|Babylonian method]]. However, they are not stable. If the initial value is not close to the reciprocal square root, the iterations will diverge away from it rather than converge to it. It can therefore be advantageous to perform an iteration of the Babylonian method on a rough estimate before starting to apply these methods.\n\n*Applying [[Newton's method]] to the equation <math>(1/x^2) - S = 0</math> produces a method that converges quadratically using three multiplications per step:\n:<math>x_{n+1} = \\frac{x_n}{2} \\cdot (3 - S \\cdot x_n^2).</math>\n\n*Another iteration is obtained by [[Halley's method]], which is the [[Householder's method]] of order two. This [[rate of convergence|converges cubically]], but involves four multiplications per iteration:\n:<math>y_n = S \\cdot x_n^2</math>, and\n:<math>x_{n+1} = \\frac{x_n}{8} \\cdot (15 - y_n \\cdot (10 - 3 \\cdot y_n))</math>.\n\n===Goldschmidt’s algorithm===\nSome computers use Goldschmidt's algorithm to simultaneously calculate\n<math>\\sqrt{S}</math> and\n<math>1/\\sqrt{S}</math>.\nGoldschmidt's algorithm finds <math>\\sqrt{S}</math> faster than Newton-Raphson iteration on a computer with a [[fused multiply–add]] instruction and either a pipelined floating point unit or two independent floating-point units.<ref name=\"goldschmidt_algo\">\n{{cite conference |citeseerx=10.1.1.85.9648 |title=Software Division and Square Root Using Goldschmidt's Algorithms |first=Peter |last=Markstein |date=November 2004 |conference=6th Conference on Real Numbers and Computers |location=[[Dagstuhl]], Germany |url=http://www.informatik.uni-trier.de/Reports/TR-08-2004/rnc6_12_markstein.pdf |conference-url=http://cca-net.de/rnc6/}}\n</ref>\n\nThe first way of writing Goldschmidt's algorithm begins\n: <math>b_0 = S</math>\n: <math>Y_0 \\approx 1/\\sqrt{S}</math> (typically using a table lookup)\n: <math>y_0 = Y_0</math>\n: <math>x_0 = S y_0</math>\nand iterates\n:<math>b_{n+1} = b_n Y_n^2</math>\n:<math>Y_{n+1} = (3 - b_{n+1})/2</math>\n:<math>x_{n+1} = x_n Y_{n+1}</math>\n:<math>y_{n+1} = y_n Y_{n+1}</math>\nuntil <math>b_i</math> is sufficiently close to 1, or a fixed number of iterations.  The iterations converge to\n:<math>\\lim_{n \\to \\infty} x_n = \\sqrt S</math>, and\n:<math>\\lim_{n \\to \\infty} y_n = 1/\\sqrt S</math>.\nNote that it is possible to omit either <math>x_n</math> and <math>y_n</math> from the computation, and if both are desired then <math>x_n = S y_n</math> may be used at the end rather than computing it through in each iteration.\n\nA second form, using [[fused multiply-add]] operations, begins\n: <math>y_0 \\approx 1/\\sqrt{S}</math> (typically using a table lookup)\n: <math>x_0 = S y_0</math>\n: <math>h_0 = y_0/2</math>\nand iterates\n:<math>r_n = 0.5 - x_n h_n</math>\n:<math>x_{n+1} = x_n + x_n r_n</math>\n:<math>h_{n+1} = h_n + h_n r_n</math>\nuntil <math>r_i</math> is sufficiently close to 0, or a fixed number of iterations.  This converges to\n:<math>\\lim_{n \\to \\infty} x_n = \\sqrt S</math>, and\n:<math>\\lim_{n \\to \\infty} 2h_n = 1/\\sqrt S</math>.\n\n==Taylor series==\nIf ''N'' is an approximation to <math>\\sqrt{S}</math>, a better approximation can be found by using the [[Taylor series]] of the [[square root]] function:\n\n:<math>\\sqrt{N^2+d} = N\\sum_{n=0}^\\infty \\frac{(-1)^{n}(2n)!}{(1-2n)n!^2 4^n}\\frac{d^n}{N^{2n}} = N(1 + \\frac{d}{2N^2} - \\frac{d^2}{8N^4} + \\frac{d^3}{16N^6} - \\frac{5d^4}{128N^8} + \\cdots)</math>\n\nAs an iterative method, the [[order of convergence]] is equal to the number of terms used. With two terms, it is identical to the [[Methods of computing square roots#Babylonian method|Babylonian method]]. With three terms, each iteration takes almost as many operations as the [[Methods of computing square roots#Bakhshali approximation|Bakhshali approximation]], but converges more slowly.{{Citation needed|date=September 2017}} Therefore, this is not a particularly efficient way of calculation. To maximize the rate of convergence, choose ''N'' so that <math> \\frac{|d|}{N^2} \\,</math> is as small as possible.\n\n==CORDIC==\nA completely different method for computing the square root is based on the [[CORDIC]] algorithm, which uses only very simple operations (addition, subtraction, with bitshift and table lookup to implement multiplication). The square root of ''S'' may be obtained as the output <math>x_n</math> using the hyperbolic coordinate system in vectoring mode, with the following initialization:<ref name=\"Meher_2009\">{{cite journal|last1=Meher|first1=Pramod Kumar|last2=Valls|first2=Javier|last3=Juang|first3=Tso-Bing|last4=Sridharan|first4=K.|last5=Maharatna|first5=Koushik|date=2008-08-22<!-- revised November 26, 2008-11-26, 2009-04-10, first published: 2009-06-19, current version first published: 2009-09-02 -->|title=50 Years of CORDIC: Algorithms, Architectures and Applications|url=http://core.ac.uk/download/files/34/1509903.pdf|journal=IEEE Transactions on Circuits & Systems-I: Regular Papers|publication-date=2009-09-09|volume=56|issue=9|pages=1893–1907|doi=10.1109/TCSI.2009.2025803|access-date=2016-01-03}}<!-- ([http://www1.i2r.a-star.edu.sg/~pkmeher/papers/CORDIC-TUT-TACS-I.pdf]) --></ref>\n\n:<math>x_0 = S+1</math>\n:<math>y_0 = S-1</math>\n:<math>\\omega_0 = 0</math>\n\n==Continued fraction expansion==\n[[Quadratic irrational]]s (numbers of the form <math>\\frac{a+\\sqrt{b}}{c}</math>, where ''a'', ''b'' and ''c'' are integers), and in particular, square roots of integers, have [[periodic continued fraction]]s. Sometimes what is desired is finding not the numerical value of a square root, but rather its [[continued fraction]] expansion, and hence its rational approximation. Let ''S'' be the positive number for which we are required to find the square root. Then assuming ''a'' to be a number that serves as an initial guess and ''r'' to be the remainder term, we can write <math>S = a^2 + r. </math> Since we have <math>S - a^2 = (\\sqrt{S} + a)(\\sqrt{S} - a) = r</math>, we can express the square root of ''S'' as\n:<math> \\sqrt{S} = a + \\frac{r}{a + \\sqrt{S}}. </math>\n\nBy applying this expression for <math>\\sqrt{S}</math> to the denominator term of the fraction, we have\n:<math> \\sqrt{S} = a + \\frac{r}{a + (a + \\frac{r}{a + \\sqrt{S}})} = a + \\frac{r}{2a + \\frac{r}{a + \\sqrt{S}}}. </math>\n\nProceeding this way, we get a [[generalized continued fraction]] for the square root as \n:<math> \\sqrt{S} = a + \\frac{r |}{| 2a} +\\frac{r |}{| 2a} + \\frac{r |}{| 2a} + \\cdots </math>\n\nFor any ''S'' a possible choice for ''a'' and ''r'' is ''a'' = 1 and ''r'' = ''S'' - 1, yielding \n:<math> \\sqrt{S} = 1 + \\frac{S-1 |}{| 2} +\\frac{S-1 |}{| 2} + \\frac{S-1 |}{| 2} + \\cdots</math>\n\nFor example, for the square root of 2, we can take ''a'' = 1 and ''r'' = 1, giving us \n:<math> \\sqrt{2} = 1 + \\frac{1 |}{| 2} +\\frac{1 |}{| 2} + \\frac{1 |}{| 2} + \\cdots</math>\nTaking the first three denominators give the rational approximation of {{sqrt|2}} as [1;2,2,2] = 17/12 = 1.41667, correct up to first three decimal places. Taking the first five denominators gives the rational approximation to {{sqrt|2}} as [1;2,2,2,2,2] = 99/70 = 1.4142857, correct up to first five decimal places. Taking more denominators give better approximations.\n\nAs another example, for the square root of 3, we can select ''a'' = 2 and ''r'' = -1, giving us \n:<math> \\sqrt{3} = 2 - \\frac{1 |}{| 4} - \\frac{1 |}{| 4} - \\frac{1 |}{| 4} - \\cdots</math>\nThe first three denominators gives {{sqrt|3}} as 1.73214, correct up to the first four decimal places. Note that it is not necessary to choose an integer valued ''a''. For instance, we can take ''a'' = {{sqrt|2}} and ''r'' = 1, such that \n:<math> \\sqrt{3} = \\sqrt{2} + \\frac{1 |}{| 2\\sqrt{2}} + \\frac{1 |}{| 2\\sqrt{2}} + \\frac{1 |}{| 2\\sqrt{2}} + \\cdots</math>\n\nWe can do the same for the whole numbers as well. For instance,\n:<math> 2 = \\sqrt{4} = 1 + \\frac{3 |}{| 2} +\\frac{3 |}{| 2} + \\frac{3 |}{| 2} + \\cdots</math>\n\n===Algorithm===\nThe following iterative algorithm<ref>{{cite web|last1=Beceanu|first1=Marius|title=Period of the Continued Fraction of sqrt(n)|url=http://web.math.princeton.edu/mathlab/jr02fall/Periodicity/mariusjp.pdf|at=Theorem 2.3 |accessdate=21 December 2015|archiveurl=https://web.archive.org/web/20151221205104/http://web.math.princeton.edu/mathlab/jr02fall/Periodicity/mariusjp.pdf|archivedate=21 December 2015}}</ref> can be used to obtain the continued fraction expansion in canonical form (''S'' is any [[natural number]] that is not a [[square number|perfect square]]):\n\n:<math>m_0=0\\,\\!</math>\n\n:<math>d_0=1\\,\\!</math>\n\n:<math>a_0=\\left\\lfloor\\sqrt{S}\\right\\rfloor\\,\\!</math>\n\n:<math>m_{n+1}=d_na_n-m_n\\,\\!</math>\n\n:<math>d_{n+1}=\\frac{S-m_{n+1}^2}{d_n}\\,\\!</math>\n\n:<math>a_{n+1} =\\left\\lfloor\\frac{\\sqrt{S}+m_{n+1}}{d_{n+1}}\\right\\rfloor =\\left\\lfloor\\frac{a_0+m_{n+1}}{d_{n+1}}\\right\\rfloor\\!.</math>\n\nNotice that ''m''<sub>n</sub>, ''d''<sub>n</sub>, and ''a''<sub>n</sub> are always integers.\nThe algorithm terminates when this triplet is the same as one encountered before.\nThe algorithm can also terminate on a<sub>i</sub> when a<sub>i</sub> = 2 a<sub>0</sub>,<ref>{{cite book |last=Gliga |first=Alexandra Ioana |date=March 17, 2006 |title=On continued fractions of the square root of prime numbers |at=Corollary 3.3 |url=http://web.math.princeton.edu/mathlab/jr02fall/Periodicity/alexajp.pdf }}</ref> which is easier to implement.\n\nThe expansion will repeat from then on. The sequence [''a''<sub>0</sub>; ''a''<sub>1</sub>, ''a''<sub>2</sub>, ''a''<sub>3</sub>, ...] is the continued fraction expansion:\n:<math>\\sqrt{S} = a_0 + \\cfrac{1}{a_1 + \\cfrac{1}{a_2 + \\cfrac{1}{a_3+\\,\\ddots}}} </math>\n\n===Example, square root of 114 as a continued fraction===\nBegin with ''m''<sub>0</sub>&nbsp;=&nbsp;0; ''d''<sub>0</sub>&nbsp;=&nbsp;1; and ''a''<sub>0</sub>&nbsp;=&nbsp;10 (10<sup>2</sup>&nbsp;=&nbsp;100 and 11<sup>2</sup>&nbsp;=&nbsp;121&nbsp;>&nbsp;114 so 10 chosen).\n\n: <math>\n\\begin{align}\n\\sqrt{114} & = \\frac{\\sqrt{114}+0}{1} = 10+\\frac{\\sqrt{114}-10}{1} = 10+\\frac{(\\sqrt{114}-10)(\\sqrt{114}+10)}{\\sqrt{114}+10} \\\\\n& = 10+\\frac{114-100}{\\sqrt{114}+10} = 10+\\frac{1}{\\frac{\\sqrt{114}+10}{14}}.\n\\end{align}\n</math>\n\n: <math>m_{1} = d_{0} \\cdot a_{0} - m_{0} = 1 \\cdot 10 - 0 = 10 \\,.</math>\n\n: <math>d_{1} = \\frac{S-m_{1}^2}{d_0} = \\frac{114-10^2}{1} = 14 \\,.</math>\n\n: <math>a_{1} = \\left\\lfloor \\frac{a_0+m_{1}}{d_{1}} \\right\\rfloor = \\left\\lfloor \\frac{10+10}{14} \\right\\rfloor = \\left\\lfloor \\frac{20}{14} \\right\\rfloor = 1 \\,.</math>\n\nSo, ''m''<sub>1</sub>&nbsp;=&nbsp;10; ''d''<sub>1</sub>&nbsp;=&nbsp;14; and ''a''<sub>1</sub>&nbsp;=&nbsp;1.\n\n: <math>\n\\frac{\\sqrt{114}+10}{14} = 1+\\frac{\\sqrt{114}-4}{14} = 1+\\frac{114-16}{14(\\sqrt{114}+4)} = 1+\\frac{1}{\\frac{\\sqrt{114}+4}{7}}.\n</math>\n\nNext, ''m''<sub>2</sub>&nbsp;=&nbsp;4; ''d''<sub>2</sub>&nbsp;=&nbsp;7; and ''a''<sub>2</sub>&nbsp;=&nbsp;2.\n\n: <math>\n\\frac{\\sqrt{114}+4}{7} = 2+\\frac{\\sqrt{114}-10}{7} = 2+\\frac{14}{7(\\sqrt{114}+10)} = 2+\\frac{1}{\\frac{\\sqrt{114}+10}{2}}.\n</math>\n\n: <math>\\frac{\\sqrt{114}+10}{2}=10+\\frac{\\sqrt{114}-10}{2}=10+\\frac{14}{2(\\sqrt{114}+10)} = 10+\\frac{1}{\\frac{\\sqrt{114}+10}{7}}.</math>\n\n: <math>\\frac{\\sqrt{114}+10}{7}=2+\\frac{\\sqrt{114}-4}{7}=2+\\frac{98}{7(\\sqrt{114}+4)} = 2+\\frac{1}{\\frac{\\sqrt{114}+4}{14}}.</math>\n\n: <math>\\frac{\\sqrt{114}+4}{14}=1+\\frac{\\sqrt{114}-10}{14}=1+\\frac{14}{14(\\sqrt{114}+10)} = 1+\\frac{1}{\\frac{\\sqrt{114}+10}{1}}.</math>\n\n: <math>\\frac{\\sqrt{114}+10}{1}=20+\\frac{\\sqrt{114}-10}{1}=20+\\frac{14}{\\sqrt{114}+10} = 20+\\frac{1}{\\frac{\\sqrt{114}+10}{14}}.</math>\n\nNow, loop back to the second equation above.\n\nConsequently, the simple continued fraction for the square root of 114 is\n\n: <math>\\sqrt{114} = [10;1,2,10,2,1,20,1,2,10,2,1,20,1,2,10,2,1,20,\\dots].\\,</math> {{OEIS|id=A010179}}\n\nIts decimal value is approximately 10.67707 82520 31311 21....\n\n===Generalized continued fraction===\nA more rapid method is to evaluate its [[generalized continued fraction]]. From the formula derived [[generalized continued fraction#Roots of positive numbers|there]]:\n\n:<math>\n\\sqrt{z} = \\sqrt{x^2+y} = x+\\cfrac{y} {2x+\\cfrac{y} {2x+\\cfrac{y} {2x+\\ddots}}} \n= x+\\cfrac{2x \\cdot y} {2(2z-y)-y-\\cfrac{y^2} {2(2z-y)-\\cfrac{y^2} {2(2z-y)-\\ddots}}}\n</math>\n\nand the fact that 114 is 2/3 of the way between 10<sup>2</sup>=100 and 11<sup>2</sup>=121 results in\n\n:<math>\n\\sqrt{114} = \\cfrac{\\sqrt{1026}}{3} = \\cfrac{\\sqrt{32^2+2}}{3} = \\cfrac{32}{3}+\\cfrac{2/3} {64+\\cfrac{2} {64+\\cfrac{2} {64+\\cfrac{2} {64+\\ddots}}}} = \\cfrac{32}{3}+\\cfrac{2} {192+\\cfrac{18} {192+\\cfrac{18} {192+\\ddots}}},\n</math>\n\nwhich is simply the aforementioned [10;1,2, 10,2,1, 20,1,2, 10,2,1, 20,1,2, ...] evaluated at every third term. Combining pairs of fractions produces\n\n:<math>\n\\sqrt{114} = \\cfrac{\\sqrt{32^2+2}}{3} = \\cfrac{32}{3}+\\cfrac{64/3} {2050-1-\\cfrac{1} {2050-\\cfrac{1} {2050-\\ddots}}}= \\cfrac{32}{3}+\\cfrac{64} {6150-3-\\cfrac{9} {6150-\\cfrac{9} {6150-\\ddots}}},\n</math>\n\nwhich is now [10;1,2, 10,2,1,20,1,2, 10,2,1,20,1,2, ...] evaluated at the third term and every six terms thereafter.\n\n== Lucas sequence method ==\nthe [[Lucas sequence]] of the first kind ''U<sub>n</sub>''(''P'',''Q'') is defined by the [[Recurrence relation|recurrence relations]]:<blockquote><math>    U_n(P, Q)= \\begin{cases} 0 & \\text{if }n = 0 \\\\ 1 & \\text{if }n = 1 \\\\ P \\cdot U_{n -1}(P, Q) -Q \\cdot U_{n -2}(P, Q) & \\text{Otherwise} \\end{cases}\n</math></blockquote>and the characteristic equation of it is:<blockquote><math>    x^2 -P \\cdot x +Q  = 0\n</math></blockquote>it has the [[discriminant]] <math>    D = P^2 -4Q\n</math> and the roots:<blockquote><math>    \\begin{matrix} x_1 = \\frac{P +\\sqrt{D}}{2}, & x_2 = \\frac{P -\\sqrt{D}}{2}\\end{matrix}\n</math></blockquote>all that yield the following positive value:<blockquote><math>    \\lim_{n \\to \\infty} {\\frac{U_{n +1}}{U_n}} = x_1\n</math></blockquote>so when we want <math>    \\sqrt{a}\n</math>, we can choose <math>    P = 2\n</math> and <math>    Q = 1 -a\n</math>, and then calculate <math>    x_1 = 1 +\\sqrt{a}\n</math> using  <math>    U_{n +1}\n</math> and <math>    U_n\n</math>for large value of <math>    n\n</math>.\n\nThe most effective way to calculate <math>    U_{n +1}\n</math> and <math>    U_n\n</math>is:<blockquote><math>    \\begin{bmatrix} U_n \\\\ U_{n +1} \\end{bmatrix} = \\begin{bmatrix} 0 & 1 \\\\ -Q & P \\end{bmatrix} \\cdot \\begin{bmatrix} U_{n -1} \\\\ U_n \\end{bmatrix} = \\begin{bmatrix} 0 & 1 \\\\ -Q & P \\end{bmatrix}^n \\cdot \\begin{bmatrix} U_0 \\\\ U_1 \\end{bmatrix}\n</math></blockquote>'''Summary:'''<blockquote><math>    \\begin{bmatrix} 0 & 1 \\\\ a -1 & 2 \\end{bmatrix}^n \\cdot \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} U_n \\\\ U_{n +1} \\end{bmatrix}\n</math></blockquote>then when <math>    n \\to \\infty\n</math>:<blockquote><math>    \\sqrt{a} = \\frac{U_{n +1}}{U_n} -1\n</math></blockquote>\n\n==Using Pell's equation==\n[[Pell's equation]] (also known as [[Brahmagupta]] equation since he was the first to give a solution to this particular equation) and its variants yield a method for efficiently finding continued fraction convergents of square roots of integers. However, it can be complicated to execute, and usually not every convergent is generated. The ideas behind the method are as follows:\n*If (''p'', ''q'') is a solution (where ''p'' and ''q'' are integers) to the equation <math>p^2 = S \\cdot q^2 \\pm 1\\!</math>, then <math>\\frac{p}{q}</math> is a continued fraction convergent of <math>\\sqrt{S}</math>, and as such, is an excellent rational approximation to it.\n*If (''p''<sub>a</sub>, ''q''<sub>a</sub>) and (''p''<sub>b</sub>, ''q''<sub>b</sub>) are solutions, then so is:\n::<math>p = p_a p_b + S \\cdot q_a q_b\\,\\!</math>\n::<math>q = p_a q_b + p_b q_a\\,\\!</math>\n:(compare to the multiplication of [[quadratic integer]]s)\n*More generally, if (''p''<sub>1</sub>, ''q''<sub>1</sub>) is a solution, then it is possible to generate a sequence of solutions (''p''<sub>''n''</sub>, ''q''<sub>''n''</sub>) satisfying:\n::<math>p_{m+n} = p_m p_n + S \\cdot q_m q_n\\,\\!</math>\n::<math>q_{m+n} = p_m q_n + p_n q_m\\,\\!</math>\nThe method is as follows:\n*Find positive integers ''p''<sub>1</sub> and ''q''<sub>1</sub> such that <math>p_1^2 = S \\cdot q_1^2 \\pm 1</math>. This is the hard part; It can be done either by guessing, or by using fairly sophisticated techniques.\n:*To generate a long list of convergents, iterate:\n:::<math>p_{n+1} = p_1 p_n + S \\cdot q_1 q_n\\,\\!</math>\n:::<math>q_{n+1} = p_1 q_n + p_n q_1\\,\\!</math>\n:*To find the larger convergents quickly, iterate:\n:::<math>p_{2n} = p_n^2 + S \\cdot q_n^2\\,\\!</math>\n:::<math>q_{2n} = 2 p_n q_n\\,\\!</math>\n::Notice that the corresponding sequence of fractions coincides with the one given by the Hero's method starting with <math>\\textstyle\\frac{p_1}{q_1}</math>.\n*In either case, <math>\\frac{p_n}{q_n}</math> is a rational approximation satisfying\n::<math>\\left|\\frac{p_n}{q_n} - \\sqrt{S}\\right| < \\frac{1}{q_n^2 \\cdot \\sqrt{S}}.</math>\n\n==Approximations that depend on the floating point representation==\n<!--\n\n    This section needs a diagram or something similar,\n    to make it more accessible to non-techies.\n\n-->\nA number is represented in a [[floating point]] format as <math>m\\times b^p</math> which is also called [[scientific notation]]. Its square root is <math>\\sqrt{m}\\times b^{p/2}</math> and similar formulae would apply for cube roots and logarithms. On the face of it, this is no improvement in simplicity, but suppose that only an approximation is required: then just <math>b^{p/2}</math> is good to an order of magnitude. Next, recognise that some powers, ''p'', will be odd, thus for 3141.59 = 3.14159 &times; 10<sup>3</sup> rather than deal with fractional powers of the base, multiply the mantissa by the base and subtract one from the power to make it even. The adjusted representation will become the equivalent of 31.4159 &times; 10<sup>2</sup> so that the square root will be {{radic|31.4159}}  &times; 10.\n\nIf the integer part of the adjusted mantissa is taken, there can only be the values 1 to 99, and that could be used as an index into a table of 99 pre-computed square roots to complete the estimate. A computer using base sixteen would require a larger table, but one using base two would require only three entries: the possible bits of the integer part of the adjusted mantissa are 01 (the power being even so there was no shift, remembering that a [[Normalized number|normalised]] floating point number always has a non-zero high-order digit) or if the power was odd, 10 or 11, these being the first ''two'' bits of the original mantissa. Thus, 6.25 = 110.01 in binary, normalised to 1.1001 &times; 2<sup>2</sup> an even power so the paired bits of the mantissa are 01, while .625 = 0.101 in binary normalises to 1.01 &times;  2<sup>−1</sup> an odd power so the adjustment is to 10.1 &times; 2<sup>−2</sup> and the paired bits are 10. Notice that the low order bit of the power is echoed in the high order bit of the pairwise mantissa. An even power has its low-order bit zero and the adjusted mantissa will start with 0, whereas for an odd power that bit is one and the adjusted mantissa will start with 1. Thus, when the power is halved, it is as if its low order bit is shifted out to become the first bit of the pairwise mantissa.\n\nA table with only three entries could be enlarged by incorporating additional bits of the mantissa. However, with computers, rather than calculate an interpolation into a table, it is often better to find some simpler calculation giving equivalent results. Everything now depends on the exact details of the format of the representation, plus what operations are available to access and manipulate the parts of the number. For example, [[Fortran]] offers an <code>EXPONENT(x)</code> function to obtain the power. Effort expended in devising a good initial approximation is to be recouped by thereby avoiding the additional iterations of the refinement process that would have been needed for a poor approximation. Since these are few (one iteration requires a divide, an add, and a halving) the constraint is severe.\n\nMany computers follow the [[IEEE floating-point standard|IEEE]] (or sufficiently similar) representation, and a very rapid approximation to the square root can be obtained for starting Newton's method. The technique that follows is based on the fact that the floating point format (in base two) approximates the base-2 logarithm. That is <math>\\log_2(m\\times 2^p) = p + \\log_2(m)</math>\n\nSo for a 32-bit single precision floating point number in IEEE format (where notably, the power has a [[Exponent bias|bias]] of 127 added for the represented form) you can get the approximate logarithm by interpreting its binary representation as a 32-bit integer, scaling it by <math>2^{-23}</math>, and removing a bias of 127, i.e.\n:<math>x_\\text{int} \\cdot 2^{-23} - 127 \\approx \\log_2(x).</math>\n\nFor example, 1.0 is represented by a [[hexadecimal]] number 0x3F800000, which would represent <math>1065353216 = 127 \\cdot 2^{23}</math> if taken as an integer. Using the formula above you get <math>1065353216 \\cdot 2^{-23} - 127 = 0</math>, as expected from <math>\\log_2(1.0)</math>. In a similar fashion you get 0.5 from 1.5 (0x3FC00000).\n\n[[Image:Log2approx.png]]\n\nTo get the square root, divide the logarithm by 2 and convert the value back. The following program demonstrates the idea. Note that the exponent's lowest bit is intentionally allowed to propagate into the mantissa.  One way to justify the steps in this program is to assume <math>b</math> is the exponent bias and <math>n</math> is the number of explicitly stored bits in the mantissa and then show that\n:<math>(((x_\\text{int} / 2^n - b) / 2) + b) \\cdot 2^n = (x_\\text{int} - 2^n) / 2 + ((b + 1) / 2) \\cdot 2^n.</math>\n<br>\n<syntaxhighlight lang=\"c\">\n\n/* Assumes that float is in the IEEE 754 single precision floating point format\n * and that int is 32 bits. */\nfloat sqrt_approx(float z)\n{\n    int val_int = *(int*)&z; /* Same bits, but as an int */\n    /*\n     * To justify the following code, prove that\n     *\n     * ((((val_int / 2^m) - b) / 2) + b) * 2^m = ((val_int - 2^m) / 2) + ((b + 1) / 2) * 2^m)\n     *\n     * where\n     *\n     * b = exponent bias\n     * m = number of mantissa bits\n     *\n     * .\n     */\n\n    val_int -= 1 << 23; /* Subtract 2^m. */\n    val_int >>= 1; /* Divide by 2. */\n    val_int += 1 << 29; /* Add ((b + 1) / 2) * 2^m. */\n\n    return *(float*)&val_int; /* Interpret again as float */\n}\n</syntaxhighlight>\n\nThe three mathematical operations forming the core of the above function can be expressed in a single line.  An additional adjustment can be added to reduce the maximum relative error.  So, the three operations, not including the cast, can be rewritten as\n<syntaxhighlight lang=\"c\">\nval_int = (1 << 29) + (val_int >> 1) - (1 << 22) + a;\n</syntaxhighlight>\n\nwhere ''a'' is a bias for adjusting the approximation errors. For example, with ''a'' = 0 the results are accurate for even powers of 2 (e.g., 1.0), but for other numbers the results will be slightly too big (e.g.,1.5 for 2.0 instead of 1.414... with 6% error). With ''a'' = -0x4B0D2, the maximum relative error is minimized to ±3.5%.\n\nIf the approximation is to be used for an initial guess for [[Newton's method]] to the equation <math>(1/x^2) - S = 0</math>, then the reciprocal form shown in the following section is preferred.\n\n===Reciprocal of the square root===\n{{Main|Fast inverse square root}}\nA variant of the above routine is included below, which can be used to compute the [[Multiplicative inverse|reciprocal]] of the square root, i.e., <math>x^{-{1\\over2}}</math> instead, was written by Greg Walsh. The integer-shift approximation produced a relative error of less than 4%, and the error dropped further to 0.15% with one iteration of [[Newton's method]] on the following line.<ref>[http://www.lomont.org/Math/Papers/2003/InvSqrt.pdf Fast Inverse Square Root] by Chris Lomont</ref> In computer graphics it is a very efficient way to normalize a vector.\n\n<syntaxhighlight lang=\"c\">\nfloat invSqrt(float x)\n{\n    float xhalf = 0.5f*x;\n    union\n    {\n        float x;\n        int i;\n    } u;\n    u.x = x;\n    u.i = 0x5f375a86 - (u.i >> 1);\n    /* The next line can be repeated any number of times to increase accuracy */\n    u.x = u.x * (1.5f - xhalf * u.x * u.x);\n    return u.x;\n}\n</syntaxhighlight>\n\nSome VLSI hardware implements inverse square root using a second degree polynomial estimation followed by a [[Division algorithm#Goldschmidt division|Goldschmidt iteration]].<ref>\n[http://portal.acm.org/citation.cfm?id=627261 \"High-Speed Double-Precision Computation of Reciprocal, Division, Square Root and Inverse Square Root\"]\nby José-Alejandro Piñeiro and Javier Díaz Bruguera 2002 (abstract)\n</ref>\n\n==Negative or complex square==\nIf ''S''&nbsp;<&nbsp;0, then its principal square root is\n:<math>\\sqrt {S} = \\sqrt {\\vert S \\vert} \\, \\, i \\,.</math>\n\nIf ''S''&nbsp;=&nbsp;''a''+''bi'' where ''a'' and ''b'' are real and ''b''&nbsp;≠&nbsp;0, then its principal square root is\n:<math>\\sqrt {S} = \\sqrt{\\frac{\\vert S \\vert + a}{2}} \\, + \\, \\sgn (b) \\sqrt{\\frac{\\vert S \\vert - a}{2}} \\, \\, i \\,.</math>\n\nThis can be verified by squaring the root.<ref>{{cite book\n|title=Handbook of mathematical functions with formulas, graphs, and mathematical tables\n|edition=\n|first1=Miltonn\n|last1=Abramowitz\n|first2=Irene A.\n|last2=Stegun\n|publisher=Courier Dover Publications\n|year=1964\n|isbn=978-0-486-61272-0\n|page=17\n|url=https://books.google.com/books?id=MtU8uP7XMvoC}}, [http://www.math.sfu.ca/~cbm/aands/page_17.htm Section 3.7.26, p. 17]\n</ref><ref>{{cite book\n|title=Classical algebra: its nature, origins, and uses\n|first1=Roger \n|last1=Cooke\n|publisher=John Wiley and Sons\n|year=2008\n|isbn=978-0-470-25952-8\n|page=59\n|url=https://books.google.com/books?id=lUcTsYopfhkC}}, [https://books.google.com/books?id=lUcTsYopfhkC&pg=PA59 Extract: page 59]\n</ref> Here\n:<math>\\vert S \\vert = \\sqrt{a^2 + b^2}</math>\n\nis the [[absolute value|modulus]] of ''S''. The principal square root of a [[complex number]] is defined to be the root with the non-negative real part.\n\n==See also==\n*[[Alpha max plus beta min algorithm]]\n*[[Integer square root]]\n*[[Mental calculation]]\n*[[nth root algorithm|''n''th root algorithm]]\n*[[Recurrence relation]]\n*[[Shifting nth-root algorithm]]\n*[[Square root of 2]]\n\n==Notes==\n{{Reflist|30em}}\n<!--\nThese templates can be copied for additional references. ([[Template:Cite book]], [[Template:Cite journal]])\n*{{cite book |last= |first= |authorlink= |coauthors= |title= |year= |publisher= |location= |id= }}\n*{{cite journal |quotes= |last= |first= |authorlink= |coauthors= |year= |month= |title= |journal= |volume= |issue= |pages= |id= |url= |accessdate= }}\n-->\n\n==External links==\n*{{MathWorld|title=Square root algorithms|urlname=SquareRootAlgorithms}}\n*[http://www.afjarvis.staff.shef.ac.uk/maths/jarvisspec02.pdf Square roots by subtraction]\n*[http://www.andrijar.com/algorithms/algorithms.htm#qusr Integer Square Root Algorithm by Andrija Radović]\n*[http://www.hparchive.com/Journals/HPJ-1977-05.pdf Personal Calculator Algorithms I : Square Roots (William E. Egbert), Hewlett-Packard Journal (may 1977) : page 22]\n*[http://www.calculatorsquareroot.com Calculator to learn the square root]\n\n{{DEFAULTSORT:Methods Of Computing Square Roots}}\n[[Category:Root-finding algorithms]]\n[[Category:Computer arithmetic algorithms]]"
    },
    {
      "title": "MPIR (mathematics software)",
      "url": "https://en.wikipedia.org/wiki/MPIR_%28mathematics_software%29",
      "text": "{{Infobox software\n| name = MPIR: Multiple Precision Integers and Rationals\n| screenshot = Cantor-0.1-sage-screenshot.png\n| caption = [[SageMath]], a [[computer algebra system]] that uses MPIR\n| developer = William Hart and the MPIR Team\n| latest release version = 3.0.0\n| latest release date = {{Start date and age|2017|02|01}}\n| programming language = [[C (programming language)|C]], [[C++]], [[assembly language|assembly]]\n| operating system = [[Cross-platform]]\n| genre = [[Mathematical software]]\n| license = [[GNU Lesser General Public License|LGPL]]\n| website = {{URL|mpir.org}}\n}}\n\n'''Multiple Precision Integers and Rationals''' ('''MPIR''') is an [[open-source software]] [[arbitrary-precision arithmetic|multiprecision integer]]  [[library (computing)|library]] [[fork (software development)|forked]] from the [[GNU Multiple Precision Arithmetic Library]] (GMP) project. It consists of much code from past GMP releases, and some original contributed code.<ref>http://mpir.org/#about</ref>\n\nAccording to the MPIR developers, some of the main goals of the MPIR project are:\n* Developing [[parallel algorithm]]s for multiprecision arithmetic including support for [[graphics processing unit]]s (GPU) and other [[multi-core processor]]s.{{Citation needed|date=April 2014}}\n* Maintaining [[computer compatibility#Software compatibility|compatibility]] with GMP - so that MPIR can be used as a replacement for GMP.\n* Providing [[software build|build]] support for [[Linux]], [[Mac OS]], [[Solaris (operating system)|Solaris]] and [[Microsoft Windows|Windows]] systems.\n* Supporting [[software build|building]] MPIR using Microsoft based build tools for use in 32- and 64-bit versions of Windows.\n\nMPIR is optimised for many processors (CPUs). [[Assembly language]] code exists for these {{as of|2012|lc=y}}: ARM, DEC Alpha 21064, 21164, and 21264, AMD K6, K6-2, Athlon, K8 and K10, Intel Pentium, Pentium Pro-II-III, Pentium 4, generic x86, Intel IA-64, Core 2, i7, Atom, Motorola-IBM PowerPC 32 and 64, MIPS R3000, R4000, SPARCv7, SuperSPARC, generic SPARCv8, UltraSPARC.\n\n== Language bindings ==\n{| class=\"wikitable\"\n|-\n! Library name\n! Language\n! License\n|-\n| [http://mpir.org/ MPIR]\n| [[C (programming language)|C]], [[C++]]\n| [[GNU Lesser General Public License|LGPL]]\n|-\n| [https://wezeku.github.io/Mpir.NET/ Mpir.NET]\n| [[F Sharp (programming language)|F#]], [[C Sharp (programming language)|C#]], [[.NET Framework|.NET]]\n| [[GNU Lesser General Public License|LGPL]]\n|}\n\n== See also ==\n{{Portal|Free and open-source software}}\n\n* [[Arbitrary-precision arithmetic]], data type: bignum\n* [[GNU Multiple Precision Arithmetic Library]]\n* GNU Multiple Precision Floating-Point Reliably ([[MPFR]])\n* [[Class Library for Numbers]] supporting [[GiNaC]]\n\n==References==\n{{Reflist}}\n\n== External links ==\n* [http://mpir.org/ MPIR] — official site of ''MPIR: Multiple Precision Integers and Rationals''\n* [http://gmplib.org/ GMP] — official site of ''GNU Multiple Precision Arithmetic Library''\n* [http://www.mpfr.org/ MPFR] — official site of ''GNU Multiple Precision Floating-Point Reliably''\n\n{{data types}}\n\n[[Category:C libraries]]\n[[Category:Computer arithmetic]]\n[[Category:Computer arithmetic algorithms]]\n[[Category:Free software programmed in C]]\n[[Category:Numerical software]]"
    },
    {
      "title": "Multiplication algorithm",
      "url": "https://en.wikipedia.org/wiki/Multiplication_algorithm",
      "text": "{{Use dmy dates|date=May 2019|cs1-dates=y}}\nA '''multiplication algorithm''' is an [[algorithm]] (or method) to [[multiplication|multiply]] two numbers. Depending on the size of the numbers, different algorithms are in use. Efficient multiplication algorithms have existed since the advent of the decimal system.\n\n==Grid method==\n{{main|Grid method multiplication}}\nThe [[grid method multiplication|grid method]] (or box method) is an introductory method for multiple-digit multiplication that is often taught to pupils at [[primary school]] or [[elementary school]] level.  It has been a standard part of the national primary-school mathematics curriculum in England and Wales since the late 1990s.<ref>Gary Eason, [http://news.bbc.co.uk/1/hi/education/639937.stm Back to school for parents], ''[[BBC News]]'', 13 February 2000<br>[[Rob Eastaway]], [https://www.bbc.co.uk/news/magazine-11258175 Why parents can't do maths today], ''[[BBC News]]'', 10 September 2010</ref>\n\nBoth factors are broken up (\"partitioned\") into their hundreds, tens and units parts, and the products of the parts are then calculated explicitly in a relatively simple multiplication-only stage, before these contributions are then totalled to give the final answer in a separate addition stage.\n\nThe calculation 34 × 13, for example, could be computed using the grid:\n<div style=\"float:right\">\n<pre>  300\n   40\n   90\n + 12\n ————\n  442</pre></div>\n:{|class=\"wikitable\" border=1 cellspacing=0 cellpadding=15 style=\"text-align: center;\"\n! width=\"40\" scope=\"col\" | ×\n! width=\"40\" scope=\"col\" | 30\n! width=\"40\" scope=\"col\" | 4\n|-\n! scope=\"row\" | 10\n|300\n|40\n|-\n! scope=\"row\" | 3\n|90\n|12\n|}\n\nfollowed by addition to obtain 442, either in a single sum (see right), or through forming the row-by-row totals (300 + 40) + (90 + 12) = 340 + 102 = 442.\n\nThis calculation approach (though not necessarily with the explicit grid arrangement) is also known as the [[partial products algorithm]].  Its essence is the calculation of the simple multiplications separately, with all addition being left to the final gathering-up stage.\n\nThe grid method can in principle be applied to factors of any size, although the number of sub-products becomes cumbersome as the number of digits increases.   Nevertheless, it is seen as a usefully explicit method to introduce the idea of multiple-digit multiplications; and, in an age when most multiplication calculations are done using a calculator or a spreadsheet, it may in practice be the only multiplication algorithm that some students will ever need.\n\n==Long multiplication==\nIf a [[numeral system|positional numeral system]] is used, a natural way of multiplying numbers is taught in  schools\nas '''long multiplication''', sometimes called '''grade-school multiplication''', sometimes called '''Standard Algorithm''':\nmultiply the [[wikt:multiplicand|multiplicand]] by each digit of the [[wikt:multiplier|multiplier]] and then add up all the properly shifted results. It requires memorization of the [[multiplication table]] for single digits.\n\nThis is the usual algorithm for multiplying larger numbers by hand in base 10. Computers initially used a very similar [[#Shift and add|shift and add]] algorithm in base 2, but modern processors have optimized circuitry for fast multiplications using more efficient algorithms, at the price of a more complex hardware realization. A person doing long multiplication on paper will write down all the products and then add them together; an [[abacus]]-user will sum the products as soon as each one is computed.\n\n===Example===\nThis example uses ''long multiplication'' to multiply 23,958,233 (multiplicand) by 5,830 (multiplier) and arrives at 139,676,498,390 for the result (product).\n         23958233\n   ×         5830\n   ———————————————\n         00000000 ( =      23,958,233 ×     0)\n        71874699  ( =      23,958,233 ×    30)\n      191665864   ( =      23,958,233 ×   800)\n   + 119791165    ( =      23,958,233 × 5,000)\n   ———————————————\n     139676498390 ( = 139,676,498,390        )\nBelow pseudocode describes the process of above multiplication. It keeps only one row to maintain the sum which finally becomes the result. Note that the '+=' operator is used to denote sum to existing value and store operation (akin to languages such as Java and C) for compactness.\n\n<source lang=\"pascal\">\nmultiply(a[1..p], b[1..q], base)                            // Operands containing rightmost digits at index 1\n  product = [1..p+q]                                        // Allocate space for result\n  for b_i = 1 to q                                          // for all digits in b\n    carry = 0\n    for a_i = 1 to p                                        // for all digits in a\n      product[a_i + b_i - 1] += carry + a[a_i] * b[b_i]\n      carry = product[a_i + b_i - 1] / base\n      product[a_i + b_i - 1] = product[a_i + b_i - 1] mod base\n    product[b_i + p] = carry                               // last digit comes from final carry\n  return product\n</source>\n\n===Optimizing space complexity===\n{{unreferenced section|date=September 2012}}\nLet ''n'' be the total number of digits in the two input numbers in [[Radix|base]] ''D''. If the result must be kept in memory then the space complexity is trivially Θ(''n''). However, in certain applications, the entire result need not be kept in memory and instead the digits of the result can be streamed out as they are computed (for example, to system console or file). In these scenarios, long multiplication has the advantage that it can easily be formulated as a [[FL (complexity)|log space]] algorithm; that is, an algorithm that only needs working space proportional to the logarithm of the number of digits in the input ([[Bachmann-Landau notation|Θ]](log&nbsp;''n'')). This is the ''double'' logarithm of the numbers being multiplied themselves (log&nbsp;log&nbsp;''N''). Note that operands themselves still need to be kept in memory and their Θ(''n'') space is not considered in this analysis.\n\nThe method is based on the observation that each digit of the result can be computed from right to left with only knowing the carry from the previous step. Let ''a''<sub>''i''</sub> and ''b''<sub>''i''</sub> be the ''i''-th digit of the operand, with ''a'' and ''b'' padded on the left by zeros to be length ''n'', ''r''<sub>''i''</sub> be the ''i''-th digit of the result and ''c''<sub>''i''</sub> be the carry generated for ''r''<sub>''i''</sub> (i=1 is the right most digit) then\n\n:<math>\\begin{align}\nr_i &= \\left( c_{i-1} + \\sum_{j+k=i+1} a_j b_k \\right) \\mod D \\\\\nc_i &= \\left\\lfloor (c_{i-1} + \\sum_{j+k=i} a_j b_k) / D \\right\\rfloor \\\\\nc_0 &= 0\n\\end{align}</math>\nor\n:<math>\nc_i = \\left( \\sum_{m = 0}^{i - 2} \\sum_{j + k = i - m} a_j b_k \\right) / D.\n</math>\n\nA simple inductive argument shows that the carry can never exceed ''n'' and the total sum for ''r''<sub>''i''</sub> can never exceed ''D'' * ''n'': the carry into the first column is zero, and for all other columns, there are at most ''n'' digits in the column, and a carry of at most ''n'' from the previous column (by the induction hypothesis). The sum is at most ''D'' * ''n'', and the carry to the next column is at most ''D'' * ''n'' / ''D'', or ''n''. Thus both these values can be stored in O(log ''n'') digits.\n\nIn pseudocode, the log-space algorithm is:<syntaxhighlight lang=\"pascal\">\nmultiply(a[1..p], b[1..q], base)                  // Operands containing rightmost digits at index 1\n    tot = 0\n    for ri = 1 to p + q - 1                       //For each digit of result\n        for bi = MAX(1, ri - p + 1) to MIN(ri, q) //Digits from b that need to be considered\n            ai = ri − bi + 1                      //Digits from a follow \"symmetry\"\n            tot = tot + (a[ai] * b[bi])\n        product[ri] = tot mod base\n        tot = floor(tot / base)\n    product[p+q] = tot mod base                   //Last digit of the result comes from last carry\n    return product\n</syntaxhighlight>\n\n===Usage in computers===\nSome [[Integrated circuit|chips]] implement this algorithm for various integer and floating-point sizes in [[computer hardware]] or in [[microcode]]. In [[arbitrary-precision arithmetic]], it's common to use long multiplication with the base set to 2<sup>''w''</sup>, where ''w'' is the number of bits in a word, for multiplying relatively small numbers.\n\nTo multiply two numbers with ''n'' digits using this method, one needs about ''n''<sup>2</sup> operations. More formally: using a natural size metric of number of digits, the time complexity of multiplying two ''n''-digit numbers using long multiplication is [[Bachmann-Landau notation|Θ]](''n''<sup>2</sup>).\n\nWhen implemented in software, long multiplication algorithms have to deal with overflow during additions, which can be expensive. For this reason, a typical approach is to represent the number in a small base ''b'' such that, for example, 8''b'' is a representable machine integer (for example Richard Brent used this approach in his Fortran package MP<ref>Richard P. Brent. A Fortran Multiple-Precision Arithmetic Package. Australian National University. March 1978.</ref>); we can then perform several additions before having to deal with overflow. When the number becomes too large, we add part of it to the result or carry and map the remaining part back to a number less than ''b''; this process is called ''normalization''.\n\n==Lattice multiplication==\n{{main|Lattice multiplication}}\n[[File:Hindu lattice.svg|thumb|right|First, set up the grid by marking its rows and columns with the numbers to be multiplied. Then, fill in the boxes with tens digits in the top triangles and units digits on the bottom.]]\n[[File:Hindu lattice 2.svg|thumb|right|Finally, sum along the diagonal tracts and carry as needed to get the answer]]\n\nLattice, or sieve, multiplication is algorithmically equivalent to long multiplication. It requires the preparation of a lattice (a grid drawn on paper) which guides the calculation and separates all the multiplications from the [[addition]]s. It was introduced to Europe in 1202 in [[Fibonacci]]'s [[Liber Abaci]]. Fibonacci described the operation as mental, using his right and left hands to carry the intermediate calculations. [[Matrakçı Nasuh]] presented 6 different variants of this method in this 16th-century book, Umdet-ul Hisab. It was widely used in [[Enderun]] schools across the Ottoman Empire.<ref>Corlu, M. S., Burlbaw, L. M., Capraro, R. M., Corlu, M. A.,& Han, S. (2010). The Ottoman Palace School Enderun and The Man with Multiple Talents, Matrakçı Nasuh. Journal of the Korea Society of Mathematical Education Series D: Research in Mathematical Education. 14(1), pp. 19–31.</ref> [[Napier's bones]], or [[Napier's rods]] also used this method, as published by Napier in 1617, the year of his death.\n\nAs shown in the example, the multiplicand and multiplier are written above and to the right of a lattice, or a sieve. It is found in [[Muhammad ibn Musa al-Khwarizmi]]'s \"Arithmetic\", one of Leonardo's sources mentioned by Sigler, author of \"Fibonacci's Liber Abaci\", 2002.{{citation needed|date=January 2016}}\n\n*During the multiplication phase, the lattice is filled in with two-digit products of the corresponding digits labeling each row and column: the tens digit goes in the top-left corner.\n*During the addition phase, the lattice is summed on the diagonals.\n* Finally, if a carry phase is necessary, the answer as shown along the left and bottom sides of the lattice is converted to normal form by carrying ten's digits as in long addition or multiplication.\n\n===Example===\nThe pictures on the right show how to calculate 345 × 12 using lattice multiplication. As a more complicated example, consider the picture below displaying the computation of 23,958,233 multiplied by 5,830 (multiplier); the result is 139,676,498,390.  Notice 23,958,233 is along the top of the lattice and 5,830 is along the right side.  The products fill the lattice and the sum of those products (on the diagonal) are along the left and bottom sides.  Then those sums are totaled as shown.\n{|\n| rowspan=\"2\" |\n<pre>\n     2   3   9   5   8   2   3   3\n   +---+---+---+---+---+---+---+---+-\n   |1 /|1 /|4 /|2 /|4 /|1 /|1 /|1 /|\n   | / | / | / | / | / | / | / | / | 5\n 01|/ 0|/ 5|/ 5|/ 5|/ 0|/ 0|/ 5|/ 5|\n   +---+---+---+---+---+---+---+---+-\n   |1 /|2 /|7 /|4 /|6 /|1 /|2 /|2 /|\n   | / | / | / | / | / | / | / | / | 8\n 02|/ 6|/ 4|/ 2|/ 0|/ 4|/ 6|/ 4|/ 4|\n   +---+---+---+---+---+---+---+---+-\n   |0 /|0 /|2 /|1 /|2 /|0 /|0 /|0 /|\n   | / | / | / | / | / | / | / | / | 3\n 17|/ 6|/ 9|/ 7|/ 5|/ 4|/ 6|/ 9|/ 9|\n   +---+---+---+---+---+---+---+---+-\n   |0 /|0 /|0 /|0 /|0 /|0 /|0 /|0 /|\n   | / | / | / | / | / | / | / | / | 0\n 24|/ 0|/ 0|/ 0|/ 0|/ 0|/ 0|/ 0|/ 0|\n   +---+---+---+---+---+---+---+---+-\n     26  15  13  18  17  13  09  00</pre>\n||\n<pre>\n 01\n 002\n 0017\n 00024\n 000026\n 0000015\n 00000013\n 000000018\n 0000000017\n 00000000013\n 000000000009\n 0000000000000\n —————————————\n  139676498390\n</pre>\n|-\n||\n = 139,676,498,390\n|}\n\n==Peasant or binary multiplication==\n{{Main|Peasant multiplication}}\n{{unreferenced section|date=January 2013}}\nIn base 2, long multiplication reduces to a nearly trivial operation. For each '1' bit in the [[wikt:multiplier|multiplier]], shift the [[wikt:multiplicand|multiplicand]] an appropriate amount and then sum the shifted values. Depending on computer processor architecture and choice of multiplier, it may be faster to code this algorithm using hardware bit shifts and adds rather than depend on multiplication instructions, when the multiplier is fixed and the number of adds required is small.\n\nThis [[algorithm]] is also known as peasant multiplication, because it has been widely used among those who are classified as peasants and thus have not memorized the [[multiplication table]]s required by long multiplication.<ref>{{Cite web|url=https://www.cut-the-knot.org/Curriculum/Algebra/PeasantMultiplication.shtml|title=Peasant Multiplication|author-link=Alexander Bogomolny|last=Bogomolny|first= Alexander |website=www.cut-the-knot.org|access-date=2017-11-04}}</ref> The algorithm was also in use in ancient Egypt.<ref>{{Cite book |author=D. Wells |year=1987 |page=44 |title=The Penguin Dictionary of Curious and Interesting Numbers |publisher=Penguin Books}}</ref>\n\nOn paper, write down in one column the numbers you get when you repeatedly halve the multiplier, ignoring the remainder; in a column beside it repeatedly double the multiplicand. Cross out each row in which the last digit of the first number is even, and add the remaining numbers in the second column to obtain the product.\n\nThe main advantages of this method are that it can be taught quickly, no memorization is required, and it can be performed using tokens such as [[poker chip]]s if paper and pencil are not available. It does however take more steps than long multiplication so it can be unwieldy when large numbers are involved.\n\n===Examples===\nThis example uses peasant multiplication to multiply 11 by 3 to arrive at a result of 33.\n\n Decimal:     Binary:\n 11   3       1011  11\n 5    6       101  110\n 2   <s>12</s>       10  <s>1100</s>\n 1   24       1  11000\n     ——         ——————\n     33         100001\n\nDescribing the steps explicitly:\n\n* 11 and 3 are written at the top\n* 11 is halved (5.5) and 3 is doubled (6).  The fractional portion is discarded (5.5 becomes 5).\n* 5 is halved (2.5) and 6 is doubled (12).  The fractional portion is discarded (2.5 becomes 2).  The figure in the left column (2) is '''even''', so the figure in the right column (12) is discarded.\n* 2 is halved (1) and 12 is doubled (24).\n* All not-scratched-out values are summed: 3 + 6 + 24 = 33.\n\nThe method works because multiplication is [[distributivity|distributive]], so:\n\n: <math>\n\\begin{align}\n3 \\times 11 & = 3 \\times (1\\times 2^0 + 1\\times 2^1 + 0\\times 2^2 + 1\\times 2^3) \\\\\n& = 3 \\times (1 + 2 + 8) \\\\\n& = 3 + 6 + 24 \\\\\n& = 33.\n\\end{align}\n</math>\n\nA more complicated example, using the figures from the earlier examples (23,958,233 and 5,830):\n\n Decimal:             Binary:\n 5830  <s>23958233</s>       1011011000110  <s>1011011011001001011011001</s>\n 2915  47916466       101101100011  10110110110010010110110010\n 1457  95832932       10110110001  101101101100100101101100100\n 728  <s>191665864</s>       1011011000  <s>1011011011001001011011001000</s>\n 364  <s>383331728</s>       101101100  <s>10110110110010010110110010000</s>\n 182  <s>766663456</s>       10110110  <s>101101101100100101101100100000</s>\n 91  1533326912       1011011  1011011011001001011011001000000\n 45  3066653824       101101  10110110110010010110110010000000\n 22  <s>6133307648</s>       10110  <s>101101101100100101101100100000000</s>\n 11 12266615296       1011  1011011011001001011011001000000000\n 5  24533230592       101  10110110110010010110110010000000000\n 2  <s>49066461184</s>       10  <s>101101101100100101101100100000000000</s>\n 1  98132922368       1  <u>1011011011001001011011001000000000000</u>\n   ————————————          1022143253354344244353353243222210110 (before carry)\n   139676498390         10000010000101010111100011100111010110\n\n==Shift and add==\n\nHistorically, computers used a \"shift and add\" algorithm to multiply small integers.  Both base 2 [[#Long multiplication|long multiplication]] and base 2 [[peasant multiplication]] reduce to this same algorithm.\nIn base 2, multiplying by the single digit of the multiplier reduces to a simple series of [[logical AND]] operations.  Each partial product is added to a running sum as soon as each partial product is computed. Most currently available microprocessors implement this or other similar algorithms (such as [[Booth encoding]]) for various integer and floating-point sizes in [[hardware multiplier]]s or in [[microcode]].\n\nOn currently available processors, a bit-wise shift instruction is faster than a multiply instruction and can be used to multiply (shift left) and divide (shift right) by powers of two. Multiplication by a constant and [[division algorithm#Division by a constant|division by a constant]] can be implemented using a sequence of shifts and adds or subtracts. For example, there are several ways to multiply by 10 using only bit-shift and addition.\n\n ((x << 2) + x) << 1 # Here 10*x is computed as (x*2^2 + x)*2\n (x << 3) + (x << 1) # Here 10*x is computed as x*2^3 + x*2\n\nIn some cases such sequences of shifts and adds or subtracts will outperform hardware multipliers and especially dividers. A division by a number of the form <math>2^n</math> or <math>2^n \\pm 1</math> often can be converted to such a short sequence.\n\nThese types of sequences have to always be used for computers that do not have a \"multiply\" instruction,<ref>\n[http://techref.massmind.org/techref/method/math/muldiv.htm \"Novel Methods of Integer Multiplication and Division\"] by G. Reichborn-Kjennerud</ref> and can also be used by extension to floating point numbers if one replaces the shifts with computation of ''2*x'' as ''x+x'', as these are logically equivalent.\n\n==Quarter square multiplication==\nTwo quantities can be multiplied using quarter squares by employing the following identity involving the [[Floor and ceiling functions|floor function]] that some sources<ref>{{citation |title= Quarter Tables Revisited: Earlier Tables, Division of Labor in Table Construction, and Later Implementations in Analog Computers |last=McFarland |first=David|url=http://escholarship.org/uc/item/5n31064n |page=1 |year=2007}}</ref><ref>{{cite book| title=Mathematics in Ancient Iraq: A Social History |last=Robson |first=Eleanor |page=227 |year=2008 |isbn= 978-0691091822 }}</ref> attribute to [[Babylonian mathematics]] (2000–1600 BC).\n\n: <math>\n\\left\\lfloor \\frac{\\left(x+y\\right)^2}{4} \\right\\rfloor - \\left\\lfloor \\frac{\\left(x-y\\right)^2}{4} \\right\\rfloor =\n\\frac{1}{4}\\left(\\left(x^2+2xy+y^2\\right) - \\left(x^2-2xy+y^2\\right)\\right) =\n\\frac{1}{4}\\left(4xy\\right) = xy.\n</math>\n\nIf one of {{math|''x''+''y''}} and {{math|''x''&minus;''y''}} is odd, the other is odd too; this means that the fractions, if any, will cancel out, and discarding the remainders does not introduce any error. Below is a lookup table of quarter squares with the remainder discarded for the digits 0 through 18; this allows for the multiplication of numbers up to {{math|9×9}}.\n\n{| border=\"1\" cellspacing=\"0\" cellpadding=\"3\" style=\"margin:0 0 0 0.5em; background:#fff; border-collapse:collapse; border-color:#7070090;\" class=\"wikitable\"\n|- style=\"text-align:right;\"\n|{{math|''n''}}&nbsp;&nbsp; || &nbsp;&nbsp;0 || &nbsp;&nbsp;1 || &nbsp;&nbsp;2 || &nbsp;&nbsp;3 || &nbsp;&nbsp;4 || &nbsp;&nbsp;5 || &nbsp;&nbsp;6 || 7 || 8 || 9 || 10 || 11 || 12 || 13 || 14 || 15 || 16 || 17 || 18\n|- style=\"text-align:right;\"\n|{{math|⌊''n''<sup>2</sup>/4⌋}} || 0 || 0 || 1 || 2 || 4 || 6 || 9 || 12 || 16 || 20 || 25 || 30 || 36 || 42 || 49 || 56 || 64 || 72 || 81\n|}\n\nIf, for example, you wanted to multiply 9 by 3, you observe that the sum and difference are 12 and 6 respectively. Looking both those values up on the table yields 36 and 9, the difference of which is 27, which is the product of 9 and 3.\n\nAntoine Voisin published a table of quarter squares from 1 to 1000 in 1817 as an aid in multiplication. A larger table of quarter squares from 1 to 100000 was published by Samuel Laundy in 1856,<ref>{{Citation |title=Reviews |journal=The Civil Engineer and Architect's Journal |year=1857 |pages=54–55 |url=https://books.google.com/books?id=gcNAAAAAcAAJ&pg=PA54#v=onepage&f=false |postscript=.}}</ref> and a table from 1 to 200000 by Joseph Blater in 1888.<ref>{{Citation|title=Multiplying with quarter squares |first=Neville |last=Holmes| journal=The Mathematical Gazette |volume=87 |issue=509 |year=2003 |pages=296–299 |jstor=3621048|postscript=.|doi=10.1017/S0025557200172778 }}</ref>\n\nQuarter square multipliers were used in [[analog computer]]s to form an [[analog signal]] that was the product of two analog input signals. In this application, the sum and difference of two input [[voltage]]s are formed using [[operational amplifier]]s. The square of each of these is approximated using [[piecewise linear function|piecewise linear]] circuits. Finally the difference of the two squares is formed and scaled by a factor of one fourth using yet another operational amplifier.\n\nIn 1980, Everett L. Johnson proposed using the quarter square method in a [[Digital data|digital]] multiplier.<ref name=eljohnson>{{Citation |last = Everett L. |first = Johnson |date = March 1980 |title = A Digital Quarter Square Multiplier |periodical = IEEE Transactions on Computers |location = Washington, DC, USA |publisher = IEEE Computer Society |volume = C-29 |issue = 3 |pages = 258–261 |issn = 0018-9340 |doi =10.1109/TC.1980.1675558 }}</ref> To form the product of two 8-bit integers, for example, the digital device forms the sum and difference, looks both quantities up in a table of squares, takes the difference of the results, and divides by four by shifting two bits to the right. For 8-bit integers the table of quarter squares will have 2<sup>9</sup>-1=511 entries (one entry for the full range 0..510 of possible sums, the differences using only the first 256 entries in range 0..255) or 2<sup>9</sup>-1=511 entries (using for negative differences the technique of 2-complements and 9-bit masking, which avoids testing the sign of differences), each entry being 16-bit wide (the entry values are from (0²/4)=0 to (510²/4)=65025).\n\nThe Quarter square multiplier technique has also benefitted 8-bit systems that do not have any support for a hardware multiplier. Steven Judd implemented this for the [[MOS Technology 6502|6502]].<ref name=sjudd>{{Citation |last = Judd |first = Steven |date = Jan 1995 |periodical = C=Hacking |issue = 9 |url = http://www.ffd2.com/fridge/chacking/c=hacking9.txt}}</ref>\n\n==Fast multiplication algorithms for large inputs==\n{{unsolved|computer science|What is the fastest algorithm for multiplication of two <math>n</math>-digit numbers?}}\n\n=== Complex multiplication algorithm===\nComplex multiplication normally involves four multiplications and two additions.\n\n:<math>(a+bi) (c+di) = (ac-bd) + (bc+ad)i.</math>\n\nOr\n\n:<math>\n\\begin{array}{c|c|c}\n\\times & a & bi \\\\\n\\hline\nc & ac & bci \\\\\n\\hline\ndi & adi & -bd\n\\end{array}\n</math>\n\nBut there is a way of reducing the number of multiplications to three.<ref name=\"taocp-vol2-sec464-ex41\">{{Citation | last1=Knuth | first1=Donald E. | author1-link=Donald Knuth | title=The Art of Computer Programming volume 2: Seminumerical algorithms | publisher=[[Addison-Wesley]] | year=1988 | pages=519, 706| title-link=The Art of Computer Programming }}\n</ref>\n\nThe product (''a''&nbsp;+&nbsp;''bi'') · (''c''&nbsp;+&nbsp;''di'') can be calculated in the following way.\n\n:''k''<sub>1</sub> = ''c'' · (''a'' + ''b'')\n:''k''<sub>2</sub> = ''a'' · (''d'' − ''c'')\n:''k''<sub>3</sub> = ''b'' · (''c'' + ''d'')\n:Real part = ''k''<sub>1</sub> − ''k''<sub>3</sub>\n:Imaginary part  = ''k''<sub>1</sub> + ''k''<sub>2</sub>.\n\nThis algorithm uses only three multiplications, rather than four, and five additions or subtractions rather than two. If a multiply is more expensive than three adds or subtracts, as when calculating by hand, then there is a gain in speed. On modern computers a multiply and an add can take about the same time so there may be no speed gain. There is a trade-off in that there may be some loss of precision when using floating point.\n\nFor [[fast Fourier transform]]s (FFTs) (or any [[Linear map|linear transformation]]) the complex multiplies are by constant coefficients ''c''&nbsp;+&nbsp;''di'' (called [[twiddle factor]]s in FFTs), in which case two of the additions (''d''−''c'' and ''c''+''d'') can be precomputed. Hence, only three multiplies and three adds are required.<ref>P. Duhamel and M. Vetterli, [http://math.berkeley.edu/~strain/273.F10/duhamel.vetterli.fft.review.pdf Fast Fourier transforms: A tutorial review and a state of the art\"] {{webarchive|url=https://web.archive.org/web/20140529212847/http://math.berkeley.edu/~strain/273.F10/duhamel.vetterli.fft.review.pdf |date=2014-05-29 }}, ''Signal Processing'' vol. 19, pp. 259–299 (1990), section 4.1.</ref>  However, trading off a multiplication for an addition in this way may no longer be beneficial with modern [[floating-point unit]]s.<ref>S. G. Johnson and M. Frigo, \"[http://fftw.org/newsplit.pdf A modified split-radix FFT with fewer arithmetic operations],\" ''IEEE Trans. Signal Process.'' vol. 55, pp. 111–119 (2007), section IV.</ref>\n\n===Karatsuba multiplication===\n{{Main|Karatsuba algorithm}}\nFor systems that need to multiply numbers in the range of several thousand digits, such as [[computer algebra system]]s and [[bignum]] libraries, long multiplication is too slow. These systems may employ '''Karatsuba multiplication''', which was discovered in 1960 (published in 1962). The heart of [[Anatoly Karatsuba|Karatsuba]]'s method lies in the observation that two-digit multiplication can be done with only three rather than the four multiplications classically required. This is an example of what is now called a ''[[divide and conquer algorithm]]''. Suppose we want to multiply two 2-digit base-''m'' numbers: ''x''<sub>1</sub>'' m + x''<sub>2</sub> and ''y''<sub>1</sub>'' m + y''<sub>2</sub>:\n\n# compute ''x''<sub>1</sub> · ''y''<sub>1</sub>, call the result ''F''\n# compute ''x''<sub>2</sub> · ''y''<sub>2</sub>, call the result ''G''\n# compute (''x''<sub>1</sub> + ''x''<sub>2</sub>) · (''y''<sub>1</sub> + ''y''<sub>2</sub>), call the result ''H''\n# compute ''H'' − ''F'' − ''G'', call the result ''K''; this number is equal to ''x''<sub>1</sub> · ''y''<sub>2</sub> + ''x''<sub>2</sub> · ''y''<sub>1</sub>\n# compute ''F'' · ''m''<sup>2</sup> + ''K'' · ''m'' + ''G''.\n\nTo compute these three products of ''m''-digit numbers, we can employ the same trick again, effectively using [[recursion]]. Once the numbers are computed, we need to add them together (steps 4 and 5), which takes about ''n'' operations.\n\nKaratsuba multiplication has a time complexity of [[Big O notation|O]](''n''<sup>log<sub>2</sub>3</sup>) ≈ O(''n''<sup>1.585</sup>), making this method significantly faster than long multiplication. Because of the overhead of recursion, Karatsuba's multiplication is slower than long multiplication for small values of ''n''; typical implementations therefore switch to long multiplication if ''n'' is below some threshold.\n\nKaratsuba's algorithm is the first known algorithm for multiplication that is asymptotically faster than long multiplication,<ref>D. Knuth, ''The Art of Computer Programming'', vol. 2, sec. 4.3.3 (1998)</ref> and can thus be viewed as the starting point for the theory of fast multiplications.\n\nIn 1963, Peter Ungar suggested setting ''m'' to ''i'' to obtain a similar reduction in the complex multiplication algorithm.<ref name=\"taocp-vol2-sec464-ex41\"/> To multiply (''a''&nbsp;+&nbsp;''b i'') · (''c''&nbsp;+&nbsp;''d i''), follow these steps:\n# compute ''b'' · ''d'', call the result ''F''\n# compute ''a'' · ''c'', call the result ''G''\n# compute (''a'' + ''b'') · (''c'' + ''d''), call the result ''H''\n# the imaginary part of the result is ''K'' = ''H'' − ''F'' − ''G'' = ''a'' · ''d'' + ''b'' · ''c''\n# the real part of the result is ''G'' − ''F'' = ''a'' · ''c'' - ''b'' · ''d''\n\nLike the algorithm in the previous section, this requires three multiplications and five additions or subtractions.\n\n===Toom–Cook===\n{{Main|Toom–Cook multiplication}}\nAnother method of multiplication is called Toom–Cook or Toom-3. The Toom–Cook method splits each number to be multiplied into multiple parts. The Toom–Cook method is one of the generalizations of the Karatsuba method. A three-way Toom–Cook can do a size-''3N'' multiplication for the cost of five size-''N'' multiplications, improvement by a factor of 9/5 compared to the Karatsuba method's improvement by a factor of 4/3.\n\nAlthough using more and more parts can reduce the time spent on recursive multiplications further, the overhead from additions and digit management also grows. For this reason, the method of Fourier transforms is typically faster for numbers with several thousand digits, and asymptotically faster for even larger numbers.\n\n===Fourier transform methods===\n[[File:Integer multiplication by FFT.svg|thumb|350px|Demonstration of multiplying 1234 × 5678 = 7006652 using fast Fourier transforms (FFTs). [[Number-theoretic transform]]s in the integers modulo 337 are used, selecting 85 as an 8th root of unity. Base 10 is used in place of base 2<sup>''w''</sup> for illustrative purposes.]]\nThe basic idea due to [[Volker Strassen|Strassen]] (1968) is to use fast polynomial multiplication to perform fast integer multiplication. The algorithm was made practical and theoretical guarantees were provided in 1971 by [[Arnold Schönhage|Schönhage]] and Strassen resulting in the [[Schönhage–Strassen algorithm]].<ref name=\"schönhage\">A. Schönhage and V. Strassen, \"Schnelle Multiplikation großer Zahlen\", ''Computing'' '''7''' (1971), pp. 281–292.</ref> The details are the following: We choose the largest integer ''w'' that will not cause [[Integer overflow|overflow]] during the process outlined below. Then we split the two numbers into ''m'' groups of ''w'' bits as follows\n\n: <math>a=\\sum_{i=0}^{m-1} {a_i 2^{wi}}\\text{ and }b=\\sum_{j=0}^{m-1} {b_j 2^{wj}}.</math>\n\nWe look at these numbers as polynomials in ''x'', where ''x = 2<sup>w</sup>'', to get,\n\n: <math>a=\\sum_{i=0}^{m-1} {a_i x^{i}}\\text{ and }b=\\sum_{j=0}^{m-1} {b_j x^{j}}.</math>\n\nThen we can then say that,\n\n: <math>ab=\\sum_{i=0}^{m-1} \\sum_{j=0}^{m-1} a_i b_j x^{(i+j)} =  \\sum_{k=0}^{2m-2}  c_k x^{k} </math>\n\nClearly the above setting is realized by polynomial multiplication, of two polynomials ''a'' and ''b''. The crucial step now is to use [[Discrete Fourier transform#Polynomial multiplication|Fast Fourier multiplication]] of polynomials to realize the multiplications above faster than in naive ''O(m<sup>2</sup>)'' time.\n\nTo remain in the modular setting of Fourier transforms, we look for a ring with a ''2m<sup>th</sup>'' root of unity. Hence we do multiplication modulo ''N'' (and thus in the ''Z/NZ'' [[Ring (mathematics)|ring]]). Further, N must be chosen so that there is no 'wrap around', essentially, no reductions modulo N occur. Thus, the choice of N is crucial. For example, it could be done as,\n\n: <math> N = 2^{3w} + 1 </math>\n\nThe ring ''Z/NZ'' would thus have a ''2m<sup>th</sup>'' root of unity, namely 8. Also, it can be checked that ''c<sub>k</sub> < N'', and thus no wrap around will occur.\n\nThe algorithm has a time complexity of [[Bachmann-Landau notation|Θ]](''n''&nbsp;log(''n'')&nbsp;log(log(''n''))) and is used in practice for numbers with more than 10,000 to 40,000 decimal digits. In 2007 this was improved by Martin Fürer ([[Fürer's algorithm]]) <ref name=\"fürer_1\">Fürer, M. (2007). \"[https://web.archive.org/web/20130425232048/http://www.cse.psu.edu/~furer/Papers/mult.pdf Faster Integer Multiplication]\" in Proceedings of the thirty-ninth annual ACM symposium on Theory of computing, June 11–13, 2007, San Diego, California, USA</ref> to give a time complexity of ''n''&nbsp;log(''n'')&nbsp;2<sup>Θ([[iterated logarithm|log<sup>*</sup>]](''n''))</sup> using Fourier transforms over complex numbers. Anindya De, Chandan Saha, Piyush Kurur and Ramprasad Saptharishi<ref>Anindya De, Piyush P Kurur, Chandan Saha, Ramprasad Saptharishi. Fast Integer Multiplication Using Modular Arithmetic. Symposium on Theory of Computation (STOC) 2008.</ref>  gave a similar algorithm using [[modular arithmetic]] in 2008 achieving the same running time.  In context of the above material, what these latter authors have achieved is to find ''N'' much less than ''2<sup>3k</sup> + 1'', so that ''Z/NZ'' has a ''2m<sup>th</sup>'' root of unity. This speeds up computation and reduces the time complexity. However, these latter algorithms are only faster than Schönhage–Strassen for impractically large inputs.\n\nIn March 2019, [[David Harvey (mathematician)|David Harvey]] and [[Joris van der Hoeven]] ([[:de:Joris_van_der_Hoeven|de]]) released a paper describing an <math>O(n \\log n)</math>multiplication algorithm.<ref>David Harvey, Joris Van Der Hoeven. Integer multiplication in time O(n log n). 2019. ffhal-02070778</ref><ref>{{Cite web|url=https://rjlipton.wordpress.com/2019/03/29/integer-multiplication-in-nlogn-time/|title=Integer Multiplication in NlogN Time|last=KWRegan|date=2019-03-29|website=Gödel's Lost Letter and P=NP|language=en|access-date=2019-05-03}}</ref><ref>{{Cite web|url=https://www.quantamagazine.org/mathematicians-discover-the-perfect-way-to-multiply-20190411/|title=Mathematicians Discover the Perfect Way to Multiply|last=Hartnett|first=Kevin|website=Quanta Magazine|access-date=2019-05-03}}</ref><ref>{{Cite web|url=https://web.maths.unsw.edu.au/~davidharvey/papers/nlogn/|title=Integer multiplication in time O(n log n)|last=Harvey|first=David|date=|website=|archive-url=|archive-date=|dead-url=|access-date=}}</ref>\n\nUsing [[number-theoretic transform]]s instead of [[discrete Fourier transform]]s avoids [[rounding error]] problems by using modular arithmetic instead of [[floating point|floating-point]] arithmetic. In order to apply the factoring which enables the FFT to work, the length of the transform must be factorable to small primes and must be a factor of ''N''-1, where ''N'' is the field size.  In particular, calculation using a Galois Field GF(''k''<sup>2</sup>), where ''k'' is a [[Mersenne Prime]], allows the use of a transform sized to a power of 2; e.g. ''k'' = 2<sup>31</sup>-1 supports transform sizes up to 2<sup>32</sup>.\n\n==Lower bounds==\n\nThere is a trivial lower bound of Ω(''n'') for multiplying two ''n''-bit numbers on a single processor; no matching algorithm (on conventional Turing machines) nor any better lower bound is known. Multiplication lies outside of [[ACC0|AC<sup>0</sup>[''p'']]] for any prime ''p'', meaning there is no family of constant-depth, polynomial (or even subexponential) size circuits using AND, OR, NOT, and MOD<sub>''p''</sub> gates that can compute a product. This follows from a constant-depth reduction of MOD<sub>''q''</sub> to multiplication.<ref>Sanjeev Arora and Boaz Barak, ''Computational Complexity: A Modern Approach'', Cambridge University Press, 2009.</ref> Lower bounds for multiplication are also known for some classes of [[branching program]]s.<ref>Farid Ablayev and Marek Karpinski, ''A lower bound for integer multiplication on randomized ordered read-once branching programs'', Information and Computation 186 (2003), 78–89.</ref>\n\n==Polynomial multiplication==\nAll the above multiplication algorithms can also be expanded to multiply [[polynomial]]s. For instance the Strassen algorithm may be used for polynomial multiplication<ref>{{cite web|url=http://everything2.com/title/Strassen+algorithm+for+polynomial+multiplication|title=Strassen algorithm for polynomial multiplication |publisher=Everything2}}</ref>\nAlternatively the [[Kronecker substitution]] technique may be used to convert the problem of multiplying polynomials into a single binary multiplication.<ref>{{citation |first1 = Joachim |last1 = von zur Gathen | author1-link = Joachim von zur Gathen |first2 = Jürgen | last2 = Gerhard |title = Modern Computer Algebra |publisher =  Cambridge University Press |year = 1999 |isbn = 978-0-521-64176-0 |pages = 243–244 |url = https://books.google.com/books?id=AE5PN5QGgvUC&pg=PA245 }}.</ref>\n\nLong multiplication methods can be generalised to allow the multiplication of algebraic formulae:\n\n  14ac - 3ab + 2 multiplied by ac - ab + 1\n\n  14ac  -3ab   2\n    ac   -ab   1\n  ————————————————————\n  14a<sup>2</sup>c<sup>2</sup>  -3a<sup>2</sup>bc   2ac\n         -14a<sup>2</sup>bc         3 a<sup>2</sup>b<sup>2</sup>  -2ab\n                  14ac           -3ab   2\n  ———————————————————————————————————————\n  14a<sup>2</sup>c<sup>2</sup> -17a<sup>2</sup>bc   16ac  3a<sup>2</sup>b<sup>2</sup>    -5ab  +2\n  <nowiki>=======================================</nowiki><ref>{{cite book|last1=Castle|first1=Frank|title=Workshop Mathematics|date=1900|publisher=MacMillan and Co|location=London|page=74|ref=harv}}</ref>\n\nAs a further example of column based multiplication, consider multiplying 23 long tons (t), 12 hundredweight (cwt) and 2 quarters (qtr) by 47.  This example uses [[avoirdupois]] measures: 1 t = 20 cwt, 1 cwt = 4 qtr.\n\n     t    cwt  qtr\n    23     12    2\n                47 x\n  ————————————————\n   161     84   94\n   920    480\n    29     23\n  ————————————————\n  1110    587   94\n  ————————————————\n  1110      7    2\n  <nowiki>=================</nowiki>  Answer: 1110 ton 7 cwt 2 qtr\n\nFirst multiply the quarters by 2, the result 94 is written into the first workspace.  Next, multiply 12 x 47 but don't add up the partial results (84, 480) yet.  Likewise multiply 23 by 47.  The quarters column is totaled and the result placed in the second workspace (a trivial move in this case).  94 quarters is 23 cwt and 2 qtr, so place the 2 in the answer and put the 23 in the next column left.  Now add up the three entries in the cwt column giving 587.  This is 29 t 7 cwt, so write the 7 into the answer and the 29 in the column to the left.  Now add up the tons column.  There is no adjustment to make, so the result is just copied down.\n\nThe same layout and methods can be used for any traditional measurements and non-decimal currencies such as the old British [[£sd]] system.\n\n==See also==\n* [[Binary multiplier]]\n* [[Division algorithm]]\n* [[Logarithm]]\n* [[Mental calculation]]\n* [[Prosthaphaeresis]]\n* [[Slide rule]]\n* [[Trachtenberg system]]\n* [[Horner scheme]] for evaluating of a polynomial\n* [[Residue number system#Multiplication]] for another fast multiplication algorithm, specially efficient when many operations are done in sequence, such as in linear algebra\n\n==References==\n{{Reflist}}\n\n==Further reading==\n* {{Cite book |title=Hacker's Delight |first=Henry S. |last=Warren Jr. |date=2013 |edition=2 |publisher=[[Addison Wesley]] - [[Pearson Education, Inc.]] |isbn=978-0-321-84268-8|title-link=Hacker's Delight }}\n* {{cite web |title=Advanced Arithmetic Techniques |author-first=John J. G. |author-last=Savard |date=2018 |orig-year=2006 |work=quadibloc |url=http://www.quadibloc.com/comp/cp0202.htm |access-date=2018-07-16 |dead-url=no |archive-url=https://web.archive.org/web/20180703001722/http://www.quadibloc.com/comp/cp0202.htm |archive-date=2018-07-03}}\n\n==External links==\n\n===Basic arithmetic===\n* [http://www.nychold.com/em-arith.html The Many Ways of Arithmetic in UCSMP Everyday Mathematics]\n* [http://math.widulski.net/slides/CH05_MustAllGoodThings.ppt A Powerpoint presentation about ancient mathematics]\n* [http://www.pedagonet.com/maths/lattice.htm Lattice Multiplication Flash Video]\n\n===Advanced algorithms===\n* [http://gmplib.org/manual/Multiplication-Algorithms.html#Multiplication%20Algorithms Multiplication Algorithms used by GMP]\n\n{{Number-theoretic algorithms}}\n\n{{DEFAULTSORT:Multiplication Algorithm}}\n[[Category:Computer arithmetic algorithms]]\n[[Category:Multiplication]]"
    },
    {
      "title": "Quote notation",
      "url": "https://en.wikipedia.org/wiki/Quote_notation",
      "text": "'''Quote notation'''  is a representation of the [[rational number]]s based on [[Kurt Hensel]]'s [[p-adic number]]s. In quote notation, arithmetic operations take particularly simple, consistent forms, producing exact answers with no [[roundoff error]].  Quote notation’s arithmetic algorithms work in a right-to-left direction;  addition, subtraction, and multiplication algorithms are the same as for [[natural number]]s, and division is easier than the usual division algorithm.  The notation was invented by [[Eric Hehner]] of the [[University of Toronto]] and [[Nigel Horspool]], then at [[McGill University]], and published in the [[Society for Industrial and Applied Mathematics|SIAM]] ''[[SIAM Journal on Computing|Journal on Computing]]'', v.8, n.2, May 1979, pp.&nbsp;124&ndash;134.\n\n==Representation==\n\n===Introduction===\nA standard representation of rational numbers begins with a sign (+ or –;  if no sign is written the implied sign is +) and continues with a (finite or infinite) sequence of digits with a radix point (called a decimal point in base ten) somewhere in the sequence.  For examples,\n:–12.345\n:0.33333...\n\nTo make the representation finite, an overscore may be used over the repeating digits.  For example\n: <math>-0.02\\overline{34}</math>\nIt is also standard practice to leave negation and division operators in the number representation, without performing the negation or division.  For example, –1/3 (minus one-third).\n\nIn quote notation, each rational number has a unique (up to normalization) finite representation as a sequence of digits with a radix point and a quote somewhere in the sequence.  The quote means that the digits to its left are repeated infinitely to the left.  For examples,\n:12'34.56 = ...12121234.56\n:12.34'56 = ...1234123412.3456\n:123!45 = ...123123123.45\nAn exclamation mark is used when the quote and point are in the same place.  If the repeated sequence is all 0s, both the zeros and the quote can be omitted.  The radix point has its usual function; moving it left divides by the [[Number base|base]]; moving it right multiplies by the base. When the radix point is at the right end, the multiplicative factor is 1, and the point can be omitted.  This gives the natural numbers their familiar form.  [[Scientific notation]] may be used as an alternative to the radix point.\n\nThe interpretation of the leading repeating sequence is an extension of the sum of the geometric series:\n:<math>1+10^k+10^{2k}+10^{3k}+... = \\tfrac{1}{1-10^k}</math>.\nFor instance:\n:<math>1+10+100+1000+... = \\tfrac{1}{1-10}=-\\tfrac{1}{9}</math>\nand\n:<math>1+1000+1000^2+1000^3+... = \\tfrac{1}{1-1000}=-\\tfrac{1}{999}</math>.\n\nWith this convention numbers in quote notation are interpreted as:\n:3' = ...333 = 3(...+100+10+1) = –3/9 = –1/3\n:123' =...123123123 = 123(...1000000+1000+1) = –123/999\n:123'45.6 = 45.6 + 123'00 = 45.6 + 100 × 123' = 45.6 – 12300/999 \n\nThis leads to the rule:\n:abc...z' = – abc...z/999...9,\n\nwith the same number of 9's in the denominator as there are digits in the repeating part of the sequence. The general form in mathematical notation: The string\n:<math>d_{n+m}d_{n+m-1} \\dotsb d_{n+1}^{\\;\\;\\;\\;\\,\\shortmid} d_nd_{n-1} \\dotsb d_2d_1d_0</math>\nrepresents the number\n:<math>\\sum_{i=0}^n d_i b^i - \\sum_{i=n+1}^{n+m} d_i b^i / (b^m-1)</math>\nwhere <math>b\\in\\Z\\setminus \\{\\pm1,0\\}</math> is the base of the representation. The <math>d_i \\in \\{0,1, \\dotsc, b-1\\}</math> are the digits.\n\n===Natural numbers===\nThe [[natural number]]s are generally written in the way one usually expects to see them, but they can also be written using an explicit quote, an explicit radix point, or redundant zeros on either end.  For example, the integer two can be written as '''2''' or '''2.''' or '''0'2''' or '''0'2.''' or even '''000'02.000''', and the integer zero can be written as '''0''' or '''0'''' or '''0.''' or '''0!'''.\n\n===Negative integers===\n[[negative number|Negative]] [[integer]]s begin with the digit one less than the base. For example, in decimal, minus three is written as '''9'7'''. \n:'''9'7''' = 7 – 90/9 = –3\nAs\n:'''9' '''= – 9/9 = –1,\nit is easily understood that for instance:\n: –189 = –1 × 189 = '''9' '''× 189 = 1701 + 17010 + 170100 + ... = ...999811 = '''9'811''' = 811 – 1000\nor alternatively, as:\n:'''9'000 '''= –1000,\n: –189 = 811 – 1000 = 811 + '''9'000'''\n\nNumbers beginning with any other repeating sequence are not integers. For example:\n:'''6'7''' = 7 – 60/9 = 1/3 \nand \n:'''7'6''' = 6 – 70/9 = – 16/9\n\n==Interpreting quote notation==\n\n===Conversion algorithm===\nTo convert quote notation into standard notation, the following algorithm can be used.\n:Let {{mvar|x}} and {{mvar|y}} be sequences of digits, as in <math>x'y</math>.\n:Let {{mvar|z}} be the digit 1 followed by a sequence of zeros of the same length as {{mvar|y}}. \n:Let {{mvar|a}} be the largest valued digit (one less than the base). In decimal, we have {{math|1=''a'' = 9}}.\n:Let {{mvar|w}} be a sequence of {{mvar|a}}s of the same length as {{mvar|x}}.\n\nThen the number represented by <math>x'y</math> is given by <math>y-xz/w</math>.\n\nAs an example, we will take '''12'345''' and convert it to a standard notation.\n\n:{{math|1=''x'' = 12}}\n:{{math|1=''y'' = 345}}\n:{{math|1=''z'' = 1000}}\n:{{math|1=''a'' = 9}}\n:{{math|1=''w'' = 99}}\n\nThen our standard notation follows,\n\n:<math> 345 - \\dfrac{12\\times 1000}{99} = \\dfrac{7385}{33} </math>\n\n===Sign determination===\nIf the leading digit is less than the first digit after the quote, the number is positive. For example, '''123'45''' is positive because 1 is less than 4. If the leading digit is more than the first digit after the quote, the number is negative. For example, '''54'3''' is negative because 5 is more than 3 .\n\nIf the quote comes at the end, just append a zero after the radix point. For example, '''592' ''' = '''592!0''', which is negative because 5 is more than 0. And '''59.2' ''' = '''59.2'0''' which is also negative.\n\nIf the leading digit equals the first digit after the quote, then either the number is '''0!0''' = '''0''', or the representation can be shortened by rolling the repetition to the right. For example, '''23'25''' = '''32'5''' which is positive because 3 is less than 5.\n\nIn binary, if it starts with 1 it is negative, and if it starts with 0 it is nonnegative, assuming the repetition has been rolled to the right as far as possible.\n\n==Arithmetic==\n\n===Addition===\nIn our usual sign-and-magnitude notation, to add the two integers 25 and −37, one first compares signs, and determines that the addition will be performed by subtracting the magnitudes. Then one compares the magnitudes to determine which will be subtracted from which, and to determine the sign of the result. In our usual fraction notation, to add 2/3 + 4/5 requires finding a common denominator, multiplying each numerator by the new factors in this common denominator, then adding the numerators, then dividing the numerator and denominator by any factors they have in common.\n\nIn quote notation, to add, just add. There are no sign or magnitude comparisons, and no common denominators. Addition is the same as for natural numbers. Here are some examples.\n\n   9'7 minus three              9'4 minus six\n + 0'6 add plus six          +  9'2 add minus eight\n —————                        —————\n   0'3 makes plus three       9'8 6 makes minus fourteen\n\n   6'7 one-third\n + 7'6 add minus one and seven-ninths\n —————\n   4'3 makes minus one and four-ninths\n\n===Subtraction===\nIn our usual sign-and-magnitude notation, subtraction involves sign comparison and magnitude comparison, and may require adding or subtracting the magnitudes, just like addition. In our usual fraction notation, subtraction requires finding a common denominator,  multiplying, subtracting, and reducing to lowest terms, just like addition.\n\nIn quote notation, to subtract, just subtract. There are no sign or magnitude comparisons, and no common denominators. When a [[Subtraction|minuend]] digit is less than the corresponding [[Subtraction|subtrahend]] digit, do not borrow from the minuend digit to its left; instead, carry (add one) to the subtrahend digit to its left. Here are some examples.\n\n   9'7 minus three              9'4 minus six\n - 0'6 subtract plus six     -  9'2 subtract minus eight\n —————                        —————\n   9'1 makes minus nine         0'2 makes plus two\n\n   6'7 one-third\n - 7'6 subtract minus one and seven-ninths\n —————\n 8'9 1 makes plus two and one-ninth\n\n===Multiplication===\nMultiplication is the same as for natural numbers.  To recognize the repetition in the answer, it helps to add the partial results pairwise.  Here are some examples.\n\n 6'7 x 0'3 = 0'1 one-third times three makes one\n\n 6'7 x 7'6 one-third times minus one and seven-ninths:\n multiply 6'7 by 6:        0'2 answer digit 2\n multiply 6'7 by 7:      6'9\n               add:     ————\n                         6'9 answer digit 9\n multiply 6'7 by 7:    6'9\n               add:   ————\n                       3'5 answer digit 5\n multiply 6'7 by 7:  6'9\n               add: ————\n                     0'2 repetition of original\n makes 592' minus sixteen twenty-sevenths\n\nTo someone who is unfamiliar with quote notation, 592' is unfamiliar, and translation to −16/27 is helpful. To someone who normally uses quote notation, −16/27 is a formula with a negation and a division operation; performing those operations yields the answer 592' .\n\n===Division===\nThe commonly used division algorithm produces digits from left-to-right, which is opposite to addition, subtraction, and multiplication. This makes further arithmetic difficult. For example, how do we add 1.234234234234... + 5.67676767... ? Usually we use a finite number of digits and accept an approximate answer with [[Round-off error|roundoff error]]. The commonly used division algorithm also produces duplicate representations; for example, 0.499999... and 0.5 represent the same number. In decimal, there is a kind of guess for each digit, which is seen to be right or wrong as the calculation progresses.\n\nIn quote notation, division produces digits from right-to-left, the same as all other arithmetic algorithms; therefore further arithmetic is easy. Quote arithmetic is exact, with no error. Each rational number has a unique representation (if the repetition is expressed as simply as possible, and we have no meaningless 0s at the right end after a radix point). Each digit is determined by a \"division table\", which is the inverse of part of the [[multiplication table]]; there is no \"guessing\". Here is an example.\n\n 9'84 / 0'27 minus sixteen divided by twenty-seven\n since 0'27 ends in 7 and 9'84 ends in 4, ask:\n\n                           9'8 4 What times 7 ends in 4? It's 2\n multiply 0'27 by 2:       0'5 4\n           subtract:       —————\n                           9'3   What times 7 ends in 3? It's 9.\n multiply 0'27 by 9:   0'2 4 3\n           subtract:   ———————\n                       9'7 5   What times 7 ends in 5? It's 5.\n multiply 0'27 by 5: 0'1 3 5\n           subtract: ———————\n                     9'8 4 repetition of original\n makes 592' minus sixteen twenty-sevenths\n\nDivision works when the [[divisor]] and the base have no factors in common except 1. In the previous example, 27 has factors 1, 3, and 27. The base is 10, which has factors 1, 2, 5, and 10. So division worked. When there are factors in common, they must be removed. For example, to divide 4 by 15, first multiply both 4 and 15 by 2:\n 4/15 = 8/30\nAny 0s at the end of the divisor just tell where the radix point goes in the result. So now divide 8 by 3.\n\n                       0'8 What times 3 ends in 8? It's 6.\n multiply 0'3 by 6:  0'1 8\n          subtract:   ————\n                       9' What times 3 ends in 9? It's 3.\n multiply 0'3 by 3:  0'9\n          subtract: ————\n                     9' repetition of earlier difference\n makes 3'6 two and two-thirds\n Now move the decimal point one place left, to get\n 3!6 four-fifteenths\n\nRemoving common factors is annoying, and it is unnecessary if the base is a [[prime number]]. Computers use base 2, which is a prime number, so division always works. And the division tables are trivial. The only questions are: what times 1 ends in 0? and: what times 1 ends in 1. Thus the rightmost [[bit]]s in the differences are the bits in the answer. For example, one divided by three, which is 1/11, proceeds as follows.\n\n              0'1 rightmost bit is 1\n   subtract 0'1 1\n            —————\n              1'  rightmost bit is 1\n subtract 0'1 1\n          —————\n          1'0   rightmost bit is 0\n subtract   0'\n          ————\n          1'   repetition of earlier difference\n makes 01'1 one-third\n\n===Negation===\nTo negate, complement each digit, and then add 1. For example, in decimal, to negate '''12'345''', complement and get '''87'654''', and then add 1 to get '''87'655'''. In binary, flip the bits, then add 1 (same as [[Two's complement|2's complement]]). For example, to negate '''01'1''', which is one-third, flip the bits to get '''10'0''', then add 1 to get '''10'1''', and roll right to shorten it to '''01' ''' which is minus one-third.\n\n==Comparison with Other Representations==\n\nThere are two representations of the rational numbers in common use.  One uses a sign ( +  or  – ), followed by a nonnegative integer (numerator), followed by a division symbol, followed by a positive integer (denominator).  For example,  –58/2975 .  (If no sign is written, the sign is  + .)  The other is a sign followed by a sequence of digits, with a radix point (called a decimal point in base ten) somewhere in the sequence, and an overscore over one or more of the rightmost digits.  For example,  <math>-0.02\\overline{34}</math> .  (There are alternative notations to the overscore;  see Repeating decimal.)  The overscore can be thought of as saying that the digits beneath it are repeated forever to the right.  In the example, that's  –0.023434343434... .  Quote notation does not need a sign;  it has a sequence of digits with a radix point somewhere in the sequence, and a quote somewhere in the sequence.  For example,  4.3'2 .  The quote can be thought of as saying that the digits to its left are repeated forever to the left.  In the example, that's  ...43434343434.32 .  All three examples in this paragraph represent the same rational number.\n\n:\t–58/2975  =  <math>-0.02\\overline{34}</math>  =  4.3'2\n\nThe three representations can be compared in two ways:  space required for storage, and time required for arithmetic operations.\n\n===Space===\n\nQuote notation and overscore notation require essentially the same space.  But quote notation and numerator-denominator notation can differ greatly.  The worst case occurs for some prime denominators (see Fermat's little theorem).  For example,  +1/7  =  285714'3 (in binary it is  011'1 ).  To represent  +1/947  in binary as a sign and numerator and denominator requires 12 bits, and as quote notation requires 947 bits.  (Extra bits are required to delimit two variable-length numbers, but these are the same for all three representations, so ignoring them does not affect the comparison.)  The number of places required to represent a repeating group of the rational number  <math>n/d</math>  in a base  <math>b</math>  quote notation is  <math>\\operatorname{ord}_d(b)</math>  whose maximum is the [[exponent of a group|exponent]] of the [[multiplicative group of integers modulo n|multiplicative group of integers modulo  <math>d</math>]], a maximum which is reached by at least  <math>\\varphi(\\varphi(d))</math>  different  <math>b</math>.  This maximum is given by the [[Carmichael function]]  <math>\\lambda(d)</math>.\n\nOn average, the space requirements for numerator-denominator and quote notations are not very different.  The 180,000 shortest numerator-denominator representations require 15.65 bits on average, and those same numbers in quote notation require 39.48 bits on average.  Taking the shortest numerator-denominator numbers, and then translating those numbers to quote notation,  results in a biased comparison in favor of numerator-denominator.  If we take all binary quote representations up to and including 14 bits (all quote positions and all radix point positions), then discard those that are not normalized, we have 1,551,567 numbers requiring 13.26 bits on average.  If we translate them to numerator-denominator notation, then normalize the result by removing common factors, they require 26.48 bits on average.  This comparison is biased in favor of quote notation.  An unbiased comparison would seem to favor numerator-denominator, but not by much.\n\n===Time===\n\nTo add two numbers in numerator-denominator notation, for example  (+a/b) + (–c/d) , requires the following steps.\n\n• sign comparison to determine if we will be adding or subtracting;  in our example, the signs differ so we will be subtracting\n\n• then 3 multiplications;  in our example,  a×d ,  b×c ,  b×d\n\n• then, if we are subtracting, a comparison of  a×d  to  b×c  to determine which is subtrahend and which is minuend, and what is the sign of the result;  let's say a×d < b×c so the sign will be  –\n\n• then the addition or subtraction;  b×c – a×d  and we have  –(b×c – a×d)/(b×d)\n\n• finding the greatest common divisor of the new numerator and denominator\n\n• dividing numerator and denominator by their greatest common divisor to obtain a normalized result\n\nNormalizing the result is not necessary for correctness, but without it, the space requirements quickly grow during a sequence of operations.  Subtraction is almost identical to addition.\n\nAdding two numbers in overscore notation is problematic because there is no right end to start at.  The easiest way to do the addition is to translate the numbers to quote notation, then add, then translate back.  Likewise for subtraction.\n\nTo add two numbers in quote notation, just add them the same way you add two positive integers.  The repetition is recognized when the repeating parts of the two operands return to their starting digits.  Then the result may be roll-normalized by checking whether the first digit equals the first digit after the quote.  Likewise for subtraction.  For both addition and subtraction, quote notation is superior to the other two notations.\n\nMultiplication in numerator-denominator notation is two integer multiplications, finding a greatest common divisor, and then two divisions.  Multiplication in overscore notation is problematic for the same reason that addition is.  Multiplication in quote notation proceeds exactly like positive integer multiplication, comparing each new sum to previous sums in order to detect the repetition.  For multiplication, quote notation is superior to overscore notation, and may be slightly better than numerator-denominator notation.\n\nDivision in numerator-denominator notation has the same complexity as multiplication in numerator-denominator notation.  Division in overscore notation is problematic because it requires a sequence of subtractions, which are problematic in overscore notation.  Division in quote notation proceeds just like multiplication in quote notation, producing the answer digits from right to left, each one determined by the rightmost digit of the current difference and divisor (trivial in binary).  For division, quote notation is superior to both overscore and numerator-denominator notations.\n\n== Drawbacks ==\n=== Cost ===\nIt should, however, not be suppressed that the worst case cost in space (and for some operations also the cost in time) of the described quote notation is <math>\\mathcal{O}(d)</math><ref>The source does not really address the <math>\\mathcal{O}(d)</math>-problem: “But quote notation and numerator-denominator notation can\ndiffer greatly.” and mention <math>d = 947</math> which requires 946 bits in the repeating group. But there are infinitely many such denominators, all of them have a relatively big [[Euler's totient function|totient function]], e.&nbsp;g. <math>d = 802787</math> with <math>\\varphi(d) = 802786</math>.<br />In their 3rd \"Appendix added later\" they add some considerations to <math>d = 59</math>.</ref> for a rational number with denominator <math>d\\in\\N</math> — compared to <math>\\mathcal{O}(\\log d)</math> of the numerator-denominator representation, a fact which makes quote notation unsuited as a means for the ''exact'' handling of rational numbers of arbitrary size, e.&nbsp;g. in a [[Computer algebra system|computer algebra package]].\n\n{|\n|+ style=\"text-align:left; font-weight:bold;\" | Examples\n|- \n| colspan=\"2\" | <math>d = 19:</math>\n|-\n|style=\"width:3em\"| || style=\"text-align:right| −1/19 || = &thinsp;052631578947368421!\n|-\n| || style=\"text-align:right| −2/19 || = &thinsp;105263157894736842!\n|-\n| || style=\"text-align:right|  [−1/10011]<sub>2</sub> || = [000011010111100101!]<sub>2</sub>\n|-\n| || style=\"text-align:right| [−10/10011]<sub>2</sub> || = [000110101111001010!]<sub>2</sub>\n|-\n| || colspan=\"2\"| This means: <math>d-1=18</math> decimals/duals in quote notation correspond to 3 resp. 7 decimals/duals in numerator-denominator notation.\n|-\n|\n|-\n| colspan=\"2\" | <math>d = 59:</math>\n|-\n| || style=\"text-align:right| −1/59 || = &thinsp;0169491525423728813559322033898305084745762711864406779661!\n|-\n| || style=\"text-align:right| −2/59 || = &thinsp;0338983050847457627118644067796610169491525423728813559322!\n|-\n| || style=\"text-align:right|  [−1/111011]<sub>2</sub> || = [0000010001010110110001111001011111011101010010011100001101!]<sub>2</sub>\n|-\n| || style=\"text-align:right| [−10/111011]<sub>2</sub> || = [0000100010101101100011110010111110111010100100111000011010!]<sub>2</sub>\n|-\n| || colspan=\"2\"| This means: <math>d-1=58</math> decimals/duals in quote notation correspond to 3 resp. 8 decimals/duals in numerator-denominator notation.\n|}\n:Remark: The sequence of decimals/duals of a representation of numerator 2 is a [[Bitwise operation#Rotate|rotated shift]] of the representation of numerator 1.\n\n=== Rounding by truncation ===\nTruncation on the left cannot be used for rounding purposes in the quote notation system. The authors do not provide approximate versions of the addition, subtraction, multiplication, and division operators, instead they propose conversion to the overscore notation and then truncation on the right.\n\nThis means that the operations have to be expanded to the full repeating group and then be converted, which in the light of section [[#cost]] does not appear to be a practicable proposal.\n\n=== Zero-divisors ===\nIf the base <math>d=e\\cdot f</math> is composite, the ring <math>\\Z_d</math> contains zero-divisors. Let's assume <math>d=10=2\\cdot 5</math>. Because <math>\\Q_2 \\cap \\Q_5 = \\Q</math>, no rational in <math>\\Z_{10}</math> is a zero-divisor. But there exist (non-rational) numbers <math>1_2, 1_5 \\in \\Z_{10}</math> which are <math>1_2 \\equiv_{\\Z_2} 1 </math> and <math>1_5 \\equiv_{\\Z_5} 1 </math>, but the product is <math>1_2 \\cdot 1_5 =_{\\Z_{10}} 0 </math>.\n\n==References==\n* {{Citation | last1=Hehner | first1=E.C.R. | last2=Horspool | first2= R.N.S. | title=A new representation of the rational numbers for fast easy arithmetic | url=http://www.cs.toronto.edu/~hehner/ratno.pdf | publisher=SIAM J. Comput. 8 no.2 pp.124-134 |date=May 1979}}\n\n;Specific\n<references />\n\n[[Category:Computer arithmetic algorithms]]"
    },
    {
      "title": "Schönhage–Strassen algorithm",
      "url": "https://en.wikipedia.org/wiki/Sch%C3%B6nhage%E2%80%93Strassen_algorithm",
      "text": "[[File:Integer multiplication by FFT.svg|thumb|350px|The Schönhage–Strassen algorithm is based on the [[Multiplication algorithm#Fourier transform methods|Fast Fourier transform (FFT) method of integer multiplication]]. This figure demonstrates multiplying 1234 &times; 5678 = 7006652 using the simple FFT method. [[Number-theoretic transform]]s in the integers modulo 337 are used, selecting 85 as an 8th root of unity. Base 10 is used in place of base 2<sup>''w''</sup> for illustrative purposes. Schönhage–Strassen improves on this by using negacyclic convolutions.]]\nThe '''Schönhage–Strassen algorithm''' is an asymptotically fast [[multiplication algorithm]] for large [[integer]]s. It was developed by [[Arnold Schönhage]] and [[Volker Strassen]] in 1971.<ref name=\"schönhage\">A. Schönhage and V. Strassen, \"[https://link.springer.com/article/10.1007/BF02242355 Schnelle Multiplikation großer Zahlen]\", ''Computing'' '''7''' (1971), pp. 281–292.</ref> The run-time [[bit complexity]] is, in [[Big O notation]],<math>O(n \\cdot \\log n \\cdot \\log \\log n)</math>  for two ''n''-digit numbers. The algorithm uses recursive [[Fast Fourier transform]]s in [[Ring (mathematics)|rings]] with 2<sup>''n''</sup>+1 elements, a specific type of [[Discrete Fourier transform (general)#Number-theoretic transform|number theoretic transform]].\n\nThe Schönhage–Strassen algorithm was the asymptotically fastest multiplication method known from 1971 until 2007, when a new method, [[Fürer's algorithm]], was announced with lower asymptotic complexity;<ref>Martin Fürer, \"[http://www.cse.psu.edu/~furer/Papers/mult.pdf Faster integer multiplication] {{webarchive|url=https://web.archive.org/web/20130425232048/http://www.cse.psu.edu/~furer/Papers/mult.pdf |date=2013-04-25 }}\", STOC 2007 Proceedings, pp. 57–66.</ref> however, Fürer's algorithm currently only achieves an advantage for astronomically large values and is not used in practice.\n\nIn practice the Schönhage–Strassen algorithm starts to outperform older methods such as [[Karatsuba multiplication|Karatsuba]] and [[Toom–Cook multiplication]] for numbers beyond 2<sup>2<sup>15</sup></sup> to 2<sup>2<sup>17</sup></sup> (10,000 to 40,000 decimal) digits.<ref>{{cite journal |first=Rodney |last=Van Meter |first2=Kohei M. |last2=Itoh |title=Fast Quantum Modular Exponentiation |journal=Physical Review |series=A |volume=71 |year=2005 |issue=5 |pages= |doi=10.1103/PhysRevA.71.052320 }}</ref><ref>[http://magma.maths.usyd.edu.au/magma/Features/node86.html Overview of Magma V2.9 Features, arithmetic section] {{webarchive|url=https://web.archive.org/web/20060820053803/http://magma.maths.usyd.edu.au/magma/Features/node86.html |date=2006-08-20 }}: Discusses practical crossover points between various algorithms.</ref><ref>Luis Carlos Coronado García, \"[http://www.cdc.informatik.tu-darmstadt.de/~coronado/Vortrag/MoraviaCrypt-talk-s.pdf  Can Schönhage multiplication speed up the RSA encryption or decryption?]\", ''University of Technology, Darmstadt'' (2005)</ref> The [[GNU Multi-Precision Library]] uses it for values of at least 1728 to 7808 64-bit words (33,000 to 150,000 decimal digits), depending on architecture.<ref>{{cite web|title=MUL_FFT_THRESHOLD|url=http://gmplib.org/devel/MUL_FFT_THRESHOLD.html|work=GMP developers' corner|accessdate=3 November 2011|deadurl=yes|archiveurl=https://web.archive.org/web/20101124062232/http://gmplib.org/devel/MUL_FFT_THRESHOLD.html|archivedate=24 November 2010|df=}}</ref> There is a Java implementation of Schönhage–Strassen which uses it above 74,000 decimal digits.<ref>{{cite web |title=An improved BigInteger class which uses efficient algorithms, including Schönhage–Strassen |url=https://github.com/tbuktu/bigint/raw/master/src/main/java/java/math/BigInteger.java |publisher=Oracle |accessdate=2014-01-10}}</ref>\n\nApplications of the Schönhage–Strassen algorithm include [[mathematical empiricism]], such as the [[Great Internet Mersenne Prime Search]] and computing [[Approximations of π|approximations of ''π'']], as well as practical applications such as [[Kronecker substitution]], in which multiplication of polynomials with integer coefficients can be efficiently reduced to large integer multiplication; this is used in practice by GMP-ECM for [[Lenstra elliptic curve factorization]].<ref name=\"Gaudry\"/>\n\n==Details==\n\nThis section explains in detail how Schönhage–Strassen is implemented. It is based primarily on an overview of the method by Crandall and Pomerance in their ''Prime Numbers: A Computational Perspective''.<ref name=\"crandall\">R. Crandall & C. Pomerance. ''Prime Numbers – A Computational Perspective''. Second Edition, Springer, 2005. Section 9.5.6: Schönhage method, p. 502. {{ISBN|0-387-94777-9}}</ref> This variant differs somewhat from Schönhage's original method in that it exploits the [[discrete weighted transform]] to perform [[negacyclic convolution]]s more efficiently. Another source for detailed information is [[Donald Knuth|Knuth]]'s ''The Art of Computer Programming''.<ref>Donald E. Knuth, [[The Art of Computer Programming]], Volume 2: Seminumerical Algorithms (3rd Edition), 1997. Addison-Wesley Professional, {{ISBN|0-201-89684-2}}. Section 4.3.3.C: Discrete Fourier transforms, pg.305.</ref>  The section begins by discussing the building blocks and concludes with a step-by-step description of the algorithm itself. \n\n===Convolutions===\n\nSuppose we are multiplying two numbers like 123 and 456 using long multiplication with base ''B'' digits, but without performing any carrying. The result might look something like this:\n\n{|width=300 style=\"text-align:right\"\n|   ||        || 1 || 2 || 3\n|-\n|   || &times;|| 4 || 5 || 6\n|-\n|colspan=5|<hr/>\n|-\n|   ||        || 6  || 12 || 18\n|-\n|   ||      5 || 10 || 15 ||\n|-\n| 4 ||      8 || 12 ||    ||\n|-\n|colspan=5|<hr/>\n|-\n| 4 ||     13 || 28 || 27 || 18\n|}\n\nThis sequence (4, 13, 28, 27, 18) is called the ''acyclic'' or [[convolution|''linear convolution'']] of the two original sequences (1,2,3) and (4,5,6). Once we have the acyclic convolution of two sequences, computing the product of the original numbers is easy: we just perform the carrying (for example, in the rightmost column, we had keep the 8 and add the 1 to the column containing 27). In the example this yields the correct product 56088.\n\nThere are two other types of convolutions that will be useful. Suppose the input sequences have ''n'' elements (here 3). Then the acyclic convolution has ''n''+''n''−1 elements; if we take the rightmost ''n'' elements and add the leftmost ''n''−1 elements, this produces the [[cyclic convolution]]:\n\n{|width=300 style=\"text-align:right\"\n|   || 28 || 27 || 18\n|-\n| + ||    ||  4 || 13\n|-\n|colspan=5|<hr/>\n|-\n|   || 28 || 31 || 31\n|}\n\nIf we perform carrying on the cyclic convolution, the result is equivalent to the product of the inputs mod B<sup>''n''</sup>&nbsp;−&nbsp;1. In the example, 10<sup>3</sup>&nbsp;−&nbsp;1 = 999, performing carrying on (28, 31, 31) yields 3141, and 3141 ≡ 56088 (mod 999).\n\nConversely, if we take the rightmost ''n'' elements and ''subtract'' the leftmost ''n''−1 elements, this produces the [[negacyclic convolution]]:\n\n{|width=300 style=\"text-align:right\"\n|         || 28 || 27 || 18\n|-\n| − ||    ||  4 || 13\n|-\n|colspan=5|<hr/>\n|-\n|         || 28 || 23 ||  5\n|}\n\nIf we perform carrying on the negacyclic convolution, the result is equivalent to the product of the inputs mod B<sup>''n''</sup>&nbsp;+&nbsp;1. In the example, 10<sup>3</sup>&nbsp;+&nbsp;1 = 1001, performing carrying on (28, 23, 5) yields 3035, and 3035 ≡ 56088 (mod 1001). The negacyclic convolution can contain negative numbers, which can be eliminated during carrying using borrowing, as is done in long subtraction.\n\n===Convolution theorem===\n\nLike other [[Multiplication algorithm#Fourier transform methods|multiplication methods based on the Fast Fourier transform]], Schönhage–Strassen depends fundamentally on the [[convolution theorem]], which provides an efficient way to compute the cyclic convolution of two sequences. It states that:\n\n:The cyclic convolution of two vectors can be found by taking the [[discrete Fourier transform]] (DFT) of each of them, multiplying the resulting vectors element by element, and then taking the inverse discrete Fourier transform (IDFT).\n\nOr in symbols:\n\n:CyclicConvolution(''X, Y'') = IDFT(DFT(''X'') &middot; DFT(''Y''))\n\nIf we compute the DFT and IDFT using a [[fast Fourier transform]] algorithm, and invoke our multiplication algorithm recursively to multiply the entries of the transformed vectors DFT(''X'') and DFT(''Y''), this yields an efficient algorithm for computing the cyclic convolution.\n\nIn this algorithm, it will be more useful to compute the ''negacyclic'' convolution; as it turns out, a slightly modified version of the convolution theorem (see [[Discrete Fourier transform (general)#Discrete weighted transform|discrete weighted transform]]) can enable this as well. Suppose the vectors X and Y have length ''n'', and ''a'' is a [[Root_of_unity|primitive root of unity]] of [[Order (group theory)|order]] 2''n'' (that is, ''a''<sup>2''n''</sup> = 1 and ''a'' to all smaller powers is not 1). Then we can define a third vector ''A'', called the ''weight vector'', as:\n\n:''A'' = (''a''<sup>''j''</sup>), 0 &le; ''j'' < ''n''\n:''A''<sup>−1</sup> = (''a''<sup>−''j''</sup>), 0 &le; ''j'' < ''n''\n\nNow, we can state:\n\n:NegacyclicConvolution(''X'', ''Y'') = ''A''<sup>−1</sup> &middot; IDFT(DFT(''A'' &middot; ''X'') &middot; DFT(''A'' &middot; ''Y''))\n\nIn other words, it's the same as before except that the inputs are first multiplied by ''A'', and the result is multiplied by ''A''<sup>−1</sup>.\n\n===Choice of ring===\n\nThe discrete Fourier transform is an abstract operation that can be performed in any [[Ring (mathematics)|algebraic ring]]; typically it's performed in the complex numbers, but actually performing complex arithmetic to sufficient precision to ensure accurate results for multiplication is slow and error-prone. Instead, we will use the approach of the [[Discrete Fourier transform (general)#Number-theoretic transform|number theoretic transform]], which is to perform the transform in the integers mod ''N'' for some integer ''N''.\n\nJust like there are primitive roots of unity of every order in the complex plane, given any order ''n'' we can choose a suitable N such that ''b'' is a primitive root of unity of order ''n'' in the integers mod ''N'' (in other words, ''b''<sup>''n''</sup> ≡ 1 (mod ''N''), and no smaller power of ''b'' is equivalent to 1 mod ''N'').\n\nThe algorithm will spend most of its time performing recursive multiplications of smaller numbers; with a naive algorithm, these occur in a number of places:\n\n# Inside the fast Fourier transform algorithm, where the primitive root of unity ''b'' is repeatedly powered, squared, and multiplied by other values.\n# When taking powers of the primitive root of unity ''a'' to form the weight vector A and when multiplying A or A<sup>−1</sup> by other vectors.\n# When performing element-by-element multiplication of the transformed vectors.\n\nThe key insight to Schönhage–Strassen is to choose N, the modulus, to be equal to 2<sup>n</sup>&nbsp;+&nbsp;1 for some integer ''n'' that is a multiple of the number of pieces ''P''=2<sup>''p''</sup> being transformed. This has a number of benefits in standard systems that represent large integers in binary form:\n\n* Any value can be rapidly reduced modulo 2<sup>''n''</sup>&nbsp;+&nbsp;1 using only shifts and adds, as explained in the [[#Shift optimizations|next section]].\n* All roots of unity in this ring can be written in the form 2<sup>''k''</sup>; consequently we can multiply or divide any number by a root of unity using a shift, and power or square a root of unity by operating only on its exponent.\n* The element-by-element recursive multiplications of the transformed vectors can be performed using a negacyclic convolution, which is faster than an acyclic convolution and already has \"for free\" the effect of reducing its result mod 2<sup>''n''</sup>&nbsp;+&nbsp;1.\n\nTo make the recursive multiplications convenient, we will frame Schönhage–Strassen as being a specialized multiplication algorithm for computing not just the product of two numbers, but the product of two numbers mod 2<sup>''n''</sup>&nbsp;+&nbsp;1 for some given ''n''. This is not a loss of generality, since one can always choose ''n'' large enough so that the product mod 2<sup>''n''</sup>&nbsp;+&nbsp;1 is simply the product.\n\n===Shift optimizations===\n\nIn the course of the algorithm, there are many cases in which multiplication or division by a power of two (including all roots of unity) can be profitably replaced by a small number of shifts and adds. This makes use of the observation that:\n\n:(2<sup>''n''</sup>)<sup>k</sup> &equiv; (−1)<sup>k</sup> mod (2<sup>''n''</sup> + 1)\n\nNote that a ''k''-digit number in base 2<sup>''n''</sup> written in [[positional notation]] can be expressed as (''d''<sub>''k''-1</sub>,...,''d''<sub>1</sub>,''d''<sub>0</sub>). It represents the number <math>\\scriptstyle \\sum_{i=0}^{k-1}d_{i}\\cdot(2^{n})^{i}</math>. Also note that for each ''d''<sub>''i''</sub> we have 0≤''d''<sub>''i''</sub> < 2<sup>''n''</sup>.\n\nThis makes it simple to reduce a number represented in binary mod 2<sup>''n''</sup>&nbsp;+&nbsp;1: take the rightmost (least significant) ''n'' bits, subtract the next ''n'' bits, add the next ''n'' bits, and so on until the bits are exhausted. If the resulting value is still not between 0 and 2<sup>''n''</sup>, normalize it by adding or subtracting a multiple of the modulus 2<sup>''n''</sup>&nbsp;+&nbsp;1. For example, if ''n''=3 (and so the modulus is 2<sup>3</sup>+1 = 9) and the number being reduced is 656, we have:\n\n:656 = 1010010000<sub>2</sub> &equiv; 000<sub>2</sub> − 010<sub>2</sub> + 010<sub>2</sub> − 1<sub>2</sub> = 0 − 2 + 2 − 1 = −1 &equiv; 8 (mod 2<sup>3</sup> + 1).\n\nMoreover, it's possible to effect very large shifts without ever constructing the shifted result. Suppose we have a number A between 0 and 2<sup>''n''</sup>, and wish to multiply it by 2<sup>''k''</sup>. Dividing ''k'' by ''n'' we find  ''k'' = ''qn'' + ''r'' with ''r'' < ''n''. It follows that:\n\n:A(2<sup>''k''</sup>) = A(2<sup>''qn'' + ''r''</sup>) = A[(2<sup>''n''</sup>)<sup>''q''</sup>(2<sup>''r''</sup>)] &equiv; (−1)<sup>''q''</sup>(A shift-left ''r'') (mod 2<sup>''n''</sup> + 1).\n\nSince A is ≤ 2<sup>''n''</sup> and ''r'' < ''n'', A shift-left ''r'' has at most 2''n''−1 bits, and so only one shift and subtraction (followed by normalization) is needed.\n\nFinally, to divide by 2<sup>''k''</sup>, observe that squaring the first equivalence above yields:\n\n:2<sup>2''n''</sup> &equiv; 1 (mod 2<sup>''n''</sup> + 1)\n\nHence,\n\n:A/2<sup>''k''</sup> = A(2<sup>−''k''</sup>) &equiv; A(2<sup>2''n'' − ''k''</sup>) = A shift-left (2''n'' − ''k'') (mod 2<sup>''n''</sup> + 1).\n\n===Algorithm===\n\nThe algorithm follows a split, evaluate (forward FFT), pointwise multiply, interpolate (inverse FFT), and combine phases similar to Karatsuba and Toom-Cook methods.\n\nGiven input numbers ''x'' and ''y'', and an integer ''N'', the following algorithm computes the product ''xy'' mod 2<sup>''N''</sup>&nbsp;+&nbsp;1. Provided N is sufficiently large this is simply the product.\n\n# Split each input number into vectors X and Y of 2<sup>''k''</sup> parts each, where 2<sup>''k''</sup> divides ''N''. (e.g. 12345678 &rarr; (12, 34, 56, 78)).\n# In order to make progress, it's necessary to use a smaller ''N'' for recursive multiplications. For this purpose choose ''n'' as the smallest integer at least 2''N''/2<sup>''k''</sup> + ''k'' and divisible by 2<sup>''k''</sup>.\n# Compute the product of X and Y mod 2<sup>''n''</sup>&nbsp;+&nbsp;1 using the negacyclic convolution:\n## Multiply X and Y each by the weight vector A using shifts (shift the ''j''th entry left by ''jn''/2<sup>''k''</sup>).\n## Compute the DFT of X and Y using the number-theoretic FFT (perform all multiplications using shifts; for the 2<sup>''k''</sup>-th root of unity, use 2<sup>2''n''/2<sup>''k''</sup></sup>).\n## Recursively apply this algorithm to multiply corresponding elements of the transformed X and Y.\n## Compute the IDFT of the resulting vector to get the result vector C (perform all multiplications using shifts). This corresponds to interpolation phase.\n## Multiply the result vector C by A<sup>−1</sup> using shifts.\n## Adjust signs: some elements of the result may be negative. We compute the largest possible positive value for the ''j''th element of C, {{math|(''j'' + 1)2<sup>2''N''/2<sup>''k''</sup></sup>}}, and if it exceeds this we subtract the modulus 2<sup>''n''</sup>&nbsp;+&nbsp;1.\n# Finally, perform carrying mod 2<sup>''N''</sup>&nbsp;+&nbsp;1 to get the final result.\n\nThe optimal number of pieces to divide the input into is proportional to <math>\\sqrt{N}</math>, where ''N'' is the number of input bits, and this setting achieves the running time of O(''N'' log ''N'' log log ''N''),<ref name=\"schönhage\"/><ref name=\"crandall\"/> so the parameter ''k'' should be set accordingly. In practice, it is set empirically based on the input sizes and the architecture, typically to a value between 4 and 16.<ref name=\"Gaudry\"/>\n\nIn step 2, the observation is used that:\n* Each element of the input vectors has at most ''n''/2<sup>''k''</sup> bits;\n* The product of any two input vector elements has at most 2''n''/2<sup>''k''</sup> bits;\n* Each element of the convolution is the sum of at most 2<sup>''k''</sup> such products, and so cannot exceed 2''n''/2<sup>''k''</sup> + ''k'' bits.\n* ''n'' must be divisible by 2<sup>''k''</sup> to ensure that in the recursive calls the condition \"2<sup>''k''</sup> divides ''N''\" holds in step 1.\n\n<!--\n\n===Example===\n\nTODO\n\n-->\n\n==Optimizations==\n\nThis section explains a number of important practical optimizations that have been considered when implementing Schönhage–Strassen in real systems. It is based primarily on a 2007 work by Gaudry, Kruppa, and Zimmermann describing enhancements to the [[GNU Multi-Precision Library]].<ref name=\"Gaudry\">Pierrick Gaudry, Alexander Kruppa, and Paul Zimmermann. [http://www.loria.fr/~gaudry/publis/issac07.pdf A GMP-based Implementation of Schönhage–Strassen’s Large Integer Multiplication Algorithm]. Proceedings of the 2007 International Symposium on Symbolic and Algebraic Computation, pp.167–174.</ref>\n\nBelow a certain cutoff point, it's more efficient to perform the recursive multiplications using other algorithms, such as [[Toom–Cook multiplication]]. The results must be reduced mod 2<sup>''n''</sup>&nbsp;+&nbsp;1, which can be done efficiently as explained above in [[#Shift optimizations|Shift optimizations]] with shifts and adds/subtracts.\n\nComputing the IDFT involves dividing each entry by the primitive root of unity 2<sup>2''n''/2<sup>''k''</sup></sup>, an operation that is frequently combined with multiplying the vector by A<sup>−1</sup> afterwards, since both involve division by a power of two.\n\nIn a system where a large number is represented as an array of 2<sup>''w''</sup>-bit words, it's useful to ensure that the vector size 2<sup>''k''</sup> is also a multiple of the bits per word by choosing ''k'' ≥ ''w'' (e.g. choose ''k'' ≥ 5 on a 32-bit computer and ''k'' ≥ 6 on a 64-bit computer); this allows the inputs to be broken up into pieces without bit shifts, and provides a uniform representation for values mod 2<sup>''n''</sup>&nbsp;+&nbsp;1 where the high word can only be zero or one.\n\nNormalization involves adding or subtracting the modulus 2<sup>''n''</sup>&nbsp;+&nbsp;1; this value has only two bits set, which means this can be done in constant time on average with a specialized operation.\n\nIterative FFT algorithms such as the [[Cooley–Tukey FFT algorithm]], although frequently used for FFTs on vectors of complex numbers, tend to exhibit very poor cache [[Locality of reference|locality]] with the large vector entries used in Schönhage–Strassen. The straightforward recursive, not in-place implementation of FFT is more successful, with all operations fitting in the cache beyond a certain point in the call depth, but still makes suboptimal use of the cache in higher call depths. Gaudry, Kruppa, and Zimmerman used a technique combining Bailey's 4-step algorithm with higher radix transforms that combine multiple recursive steps. They also mix phases, going as far into the algorithm as possible on each element of the vector before moving on to the next one.\n\nThe \"square root of 2 trick\", first described by Schönhage, is to note that, provided ''k'' ≥ 2, 2<sup>3''n''/4</sup>−2<sup>''n''/4</sup> is a square root of 2 mod 2<sup>''n''</sup>+1, and so a 4''n''-th root of unity (since 2<sup>2''n''</sup> ≡ 1). This allows the transform length to be extended from 2<sup>''k''</sup> to 2<sup>''k'' + 1</sup>.\n\nFinally, the authors are careful to choose the right value of ''k'' for different ranges of input numbers, noting that the optimal value of ''k'' may go back and forth between the same values several times as the input size increases.\n\n==References==\n{{reflist}}\n\n{{Number-theoretic algorithms}}\n\n{{DEFAULTSORT:Schonhage-Strassen Algorithm}}\n[[Category:Computer arithmetic algorithms]]\n[[Category:Multiplication]]"
    },
    {
      "title": "Shifting nth root algorithm",
      "url": "https://en.wikipedia.org/wiki/Shifting_nth_root_algorithm",
      "text": "{{DISPLAYTITLE:Shifting ''n''th root algorithm}}\n{{unreferenced|date=May 2010}}\nThe '''shifting ''n''th root algorithm''' is an [[algorithm]] for extracting the [[nth root|''n''th root]] of a positive [[real number]] which proceeds iteratively by shifting in ''n'' [[numerical digit|digits]] of the radicand, starting with the most significant, and produces one digit of the root on each iteration, in a manner similar to [[long division]].\n\n==Algorithm==\n\n===Notation===\n\nLet ''B'' be the [[radix|base]] of the number system you are using, and ''n'' be the degree of the root to be extracted.  Let <math>x</math> be the radicand processed thus far, <math>y</math> be the root extracted thus far, and <math>r</math> be the remainder.  Let <math>\\alpha</math> be the next <math>n</math> digits of the radicand, and <math>\\beta</math> be the next digit of the root.  Let <math>x'</math> be the new value of <math>x</math> for the next iteration, <math>y'</math> be the new value of <math>y</math> for the next iteration, and <math>r'</math> be the new value of <math>r</math> for the next iteration.  These are all [[integer]]s.\n\n===Invariants===\nAt each iteration, the [[Invariant (computer science)|invariant]] <math>y^n + r = x</math> will hold.  The invariant <math>(y+1)^n>x</math> will hold.  Thus <math>y</math> is the largest integer less than or equal to the ''n''th root of <math>x</math>, and <math>r</math> is the remainder.\n\n===Initialization===\nThe initial values of <math>x, y</math>, and <math>r</math> should be 0.  The value of <math>\\alpha</math> for the first iteration should be the most significant aligned block of <math>n</math> digits of the radicand.  An aligned block of <math>n</math> digits means a block of digits aligned so that the decimal point falls between blocks.  For example, in 123.4 the most significant aligned block of two digits is 01, the next most significant is 23, and the third most significant is 40.\n\n===Main loop===\nOn each iteration we shift in <math>n</math> digits of the radicand, so we have <math>x' = B^n x + \\alpha</math> and we produce one digit of the root, so we have <math>y' = B y + \\beta </math>.  We want to choose β and <math>r'</math> so that the invariants described above hold.  It turns out that there is always exactly one such choice, as will be proved below.\n\nThe first invariant says that:\n\n:<math>x' = y'^n + r'</math>\n\nor\n\n:<math>B^n x + \\alpha = (B y + \\beta)^n + r'.</math>\n\nSo, pick the largest integer β such that\n\n:<math>(B y + \\beta)^n \\le B^n x + \\alpha</math>\n\nand let\n\n:<math>r' = B^n x + \\alpha - (B y + \\beta)^n.</math>\n\nSuch a <math>\\beta</math> always exists, since if <math>\\beta = 0</math> then the condition is <math>B^n y^n \\le B^n x + \\alpha</math>, but <math>y^n \\le x</math>, so this is always true.  Also, β must be less than ''B'', since if <math>\\beta = B</math> then we would have\n\n:<math>(B(y+1))^n \\le B^n x + \\alpha\\,</math>\n\nbut the second invariant implies that\n\n:<math>B^n x < B^n (y+1)^n\\,</math>\n\nand since <math>B^n x</math> and <math>B^n (y+1)^n</math> are both multiples of <math>B^n</math> the difference between them must be at least <math>B^n</math>, and then we have\n\n:<math>B^n x + B^n \\le B^n (y+1)^n\\,</math>\n:<math>B^n x + B^n \\le B^n x + \\alpha\\,</math>\n:<math>B^n \\le \\alpha\\,</math>\n\nbut <math>0 \\le \\alpha < B^n</math> by definition of <math>\\alpha</math>, so this can not be true, and <math>(B y + \\beta)^n</math> is a monotonically increasing function of <math>\\beta</math>, so it can not be true for larger <math>\\beta</math> either, so we conclude that there exists an integer <math>\\gamma</math> with <math>\\gamma<B</math> such that an integer <math>r'</math> with <math>r' \\ge 0</math> exists such that the first invariant holds if and only if <math>0 \\le \\beta \\le \\gamma</math>.\n\nNow consider the second invariant.  It says:\n\n:<math>(y'+1)^n > x' \\,</math>\n\nor\n\n:<math>(B y + \\beta + 1)^n>B^n x + \\alpha\\,</math>\n\nNow, if <math>\\beta</math> is not the largest admissible <math>\\beta</math> for the first invariant as described above, then <math>\\beta + 1</math> is also admissible, and we have\n\n:<math>(B y + \\beta + 1)^n \\le B^n x + \\alpha\\,</math>\n\nThis violates the second invariant, so to satisfy both invariants we must pick the largest <math>\\beta</math> allowed by the first invariant.  Thus we have proven the existence and uniqueness of <math>\\beta</math> and <math>r'</math>.\n\nTo summarize, on each iteration:\n# Let <math>\\alpha</math> be the next aligned block of digits from the radicand\n# Let <math>x' = B^n x + \\alpha</math>\n# Let <math>\\beta</math> be the largest <math>\\beta</math> such that <math>(B y + \\beta)^n \\le B^n x + \\alpha</math>\n# Let <math>y' = B y + \\beta</math>\n# Let <math>r' = x' - y'^n</math>\n\nNow, note that <math>x = y^n + r</math>, so the condition\n\n:<math>(B y + \\beta)^n \\le B^n x + \\alpha</math>\n\nis equivalent to\n\n:<math>(B y + \\beta)^n - B^n y^n \\le B^n r + \\alpha</math>\n\nand\n\n:<math>r' = x' - y'^n = B^n x + \\alpha - (B y + \\beta)^n</math>\n\nis equivalent to\n\n:<math>r' = B^n r + \\alpha - ((B y + \\beta)^n - B^n y ^n)</math>\n\nThus, we do not actually need <math>x</math>, and since <math>r = x - y^n</math> and <math>x<(y+1)^n</math>, <math>r<(y+1)^n-y^n</math> or <math>r<n y^{n-1}+O(y^{n-2})</math>, or <math>r<n x^{{n-1}\\over n} + O(x^{{n-2}\\over n})</math>, so by using <math>r</math> instead of <math>x</math> we save time and space by a factor of 1/<math>n</math>.  Also, the <math>B^n y^n</math> we subtract in the new test cancels the one in <math>(B y + \\beta)^n</math>, so now the highest power of <math>y</math> we have to evaluate is <math>y^{n-1}</math> rather than <math>y^n</math>.\n\n===Summary===\n# Initialize <math>r</math> and <math>y</math> to 0.\n# Repeat until desired [[decimal precision|precision]] is obtained:\n## Let <math>\\alpha</math> be the next aligned block of digits from the radicand.\n## Let <math>\\beta</math> be the largest <math>\\beta</math> such that <math>(B y + \\beta)^n - B^n y^n \\le B^n r + \\alpha.</math>\n## Let <math>y' = B y + \\beta</math>.\n## Let <math>r' = B^n r + \\alpha - ((B y + \\beta)^n - B^n y^n).</math>\n## Assign <math>y \\leftarrow y'</math> and <math>r \\leftarrow r'.</math>\n# <math>y</math> is the largest integer such that <math>y^n<x B^k</math>, and <math>y^n+r=x B^k</math>, where <math>k</math> is the number of digits of the radicand after the decimal point that have been consumed (a negative number if the algorithm has not reached the decimal point yet).\n\n==Paper-and-pencil ''n''th roots==\nAs noted above, this algorithm is similar to long division, and it lends itself to the same notation:\n\n      {{Font color|red||1}}.  {{Font color|green||4}}   {{Font color|blue||4}}   {{Font color|magenta||2}}   {{Font color|gold||2}}   {{Font color|cyan||4}}\n     ——————————————————————\n _ {{Font color|#444444||3}}/ 3.{{Font color|darkred||000}} {{Font color|darkgreen||000}} {{Font color|darkblue||000}} {{Font color|darkmagenta||000}} {{Font color|darkcyan||000}}\n  \\/  {{Font color|red||1}}                        = {{Font color|#444444||3}}(10×{{Font color|gray||0}})<sup>2</sup>×{{Font color|red||1}}     +{{Font color|#444444||3}}(10×{{Font color|gray||0}})×{{Font color|red||1}}<sup>2</sup>     +{{Font color|red||1}}<sup>3</sup>\n      —\n      2 {{Font color|darkred||000}}\n      1 744                    = {{Font color|#444444||3}}(10×{{Font color|red||1}})<sup>2</sup>×{{Font color|green||4}}     +{{Font color|#444444||3}}(10×{{Font color|red||1}})×{{Font color|green||4}}<sup>2</sup>     +{{Font color|green||4}}<sup>3</sup>\n      —————\n        256 {{Font color|darkgreen||000}}\n        241 984                = {{Font color|#444444||3}}(10×{{Font color|red||1}}{{Font color|green||4}})<sup>2</sup>×{{Font color|blue||4}}    +{{Font color|#444444||3}}(10×{{Font color|red||1}}{{Font color|green||4}})×{{Font color|blue||4}}<sup>2</sup>    +{{Font color|blue||4}}<sup>3</sup>\n        ———————\n         14 016 {{Font color|darkblue||000}}\n         12 458 888            = {{Font color|#444444||3}}(10×{{Font color|red||1}}{{Font color|green||4}}{{Font color|blue||4}})<sup>2</sup>×{{Font color|magenta||2}}   +{{Font color|#444444||3}}(10×{{Font color|red||1}}{{Font color|green||4}}{{Font color|blue||4}})×{{Font color|magenta||2}}<sup>2</sup>   +{{Font color|magenta||2}}<sup>3</sup>\n         ——————————\n          1 557 112 {{Font color|darkmagenta||000}}\n          1 247 791 448        = {{Font color|#444444||3}}(10×{{Font color|red||1}}{{Font color|green||4}}{{Font color|blue||4}}{{Font color|magenta||2}})<sup>2</sup>×{{Font color|gold||2}}  +{{Font color|#444444||3}}(10×{{Font color|red||1}}{{Font color|green||4}}{{Font color|blue||4}}{{Font color|magenta||2}})×{{Font color|gold||2}}<sup>2</sup>  +{{Font color|gold||2}}<sup>3</sup>\n          —————————————\n            309 320 552 {{Font color|darkcyan||000}}\n            249 599 823 424    = {{Font color|#444444||3}}(10×{{Font color|red||1}}{{Font color|green||4}}{{Font color|blue||4}}{{Font color|magenta||2}}{{Font color|gold||2}})<sup>2</sup>×{{Font color|cyan||4}} +{{Font color|#444444||3}}(10×{{Font color|red||1}}{{Font color|green||4}}{{Font color|blue||4}}{{Font color|magenta||2}}{{Font color|gold||2}})×{{Font color|cyan||4}}<sup>2</sup> +{{Font color|cyan||4}}<sup>3</sup>\n            ———————————————\n             59 720 728 576\n\nNote that after the first iteration or two the leading term dominates the\n<math>(B y + \\beta)^n - B^n y^n</math>, so we can get an often correct first guess at <math>\\beta</math> by dividing <math>B^n r + \\alpha</math> by <math>n B^{n-1} y^{n-1}</math>.\n\n==Performance==\nOn each iteration, the most time-consuming task is to select <math>\\beta</math>.  We know that there are <math>B</math> possible values, so we can find <math>\\beta</math> using <math>O(\\log(B))</math> comparisons.  Each comparison will require evaluating <math>(B y +\\beta)^n - B^n y^n</math>.  In the ''k''th iteration, <math>y</math> has <math>k</math> digits, and the polynomial can be evaluated with <math>2 n - 4</math> multiplications of up to <math>k(n-1)</math> digits and <math>n - 2</math> additions of up to <math>k(n-1)</math> digits, once we know the powers of <math>y</math> and <math>\\beta</math> up through <math>n-1</math> for <math>y</math> and <math>n</math> for <math>\\beta</math>.  <math>\\beta</math> has a restricted range, so we can get the powers of <math>\\beta</math> in constant time.  We can get the powers of <math>y</math> with <math>n-2</math> multiplications of up to <math>k(n-1)</math> digits.  Assuming <math>n</math>-digit multiplication takes time <math>O(n^2)</math> and addition takes time <math>O(n)</math>, we take time\n<math>O(k^2 n^2)</math> for each comparison, or time <math>O(k^2 n^2 \\log(B))</math> to pick <math>\\beta</math>.  The remainder of the algorithm is addition and subtraction that takes time <math>O(k)</math>, so each iteration takes <math>O(k^2 n^2 \\log(B))</math>.  For all <math>k</math> digits, we need time <math>O(k^3 n^2 \\log(B))</math>.\n\nThe only internal storage needed is <math>r</math>, which is <math>O(k)</math> digits on the ''k''th iteration.  That this algorithm does not have bounded memory usage puts an upper bound on the number of digits which can be computed mentally, unlike the more elementary algorithms of arithmetic.  Unfortunately, any bounded memory state machine with periodic inputs can only produce periodic outputs, so there are no such algorithms which can compute irrational numbers from rational ones, and thus no bounded memory root extraction algorithms.\n\nNote that increasing the base increases the time needed to pick <math>\\beta</math> by a factor of <math>O(\\log(B))</math>, but decreases the number of digits needed to achieve a given precision by the same factor, and since the algorithm is cubic time in the number of digits, increasing the base gives an overall speedup of <math>O(\\log^2(B))</math>.  When the base is larger than the radicand, the algorithm degenerates to [[binary search]], so it follows that this algorithm is not useful for computing roots with a computer, as it is always outperformed by much simpler binary search, and has the same memory complexity.\n\n==Examples==\n\n===Square root of 2 in binary===\n\n       1. 0  1  1  0  1\n     ------------------\n _  / 10.00 00 00 00 00     1\n  \\/   1                  + 1\n      -----               ----\n       1 00                100\n          0               +  0\n      --------            -----\n       1 00 00             1001\n         10 01            +   1\n      -----------         ------\n          1 11 00          10101\n          1 01 01         +    1\n          ----------      -------\n             1 11 00       101100\n                   0      +     0\n             ----------   --------\n             1 11 00 00    1011001\n             1 01 10 01          1\n             ----------\n                1 01 11 remainder\n\n===Square root of 3===\n      1. 7  3  2  0  5\n     ----------------------\n _  / 3.00 00 00 00 00\n  \\/  1 = 20×0×1+1^2\n      -\n      2 00\n      1 89 = 20×1×7+7^2 (27 x 7)\n      ----\n        11 00\n        10 29 = 20×17×3+3^2  (343 x 3)\n        -----\n           71 00\n           69 24 = 20×173×2+2^2 (3462 x 2)\n           -----\n            1 76 00\n                  0 = 20×1732×0+0^2 (34640 x 0)\n            -------\n            1 76 00 00\n            1 73 20 25 = 20×17320×5+5^2 (346405 x 5)\n            ----------\n               2 79 75\n\n===Cube root of 5===\n      1.  7   0   9   9   7\n     ----------------------\n _ 3/ 5. 000 000 000 000 000\n  \\/  1 = 300×(0^2)×1+30×0×(1^2)+1^3\n      -\n      4 000\n      3 913 = 300×(1^2)×7+30×1×(7^2)+7^3\n      -----\n         87 000\n              0 = 300×(17^2)*0+30×17×(0^2)+0^3\n        -------\n         87 000 000\n         78 443 829 = 300×(170^2)×9+30×170×(9^2)+9^3\n         ----------\n          8 556 171 000\n          7 889 992 299 = 300×(1709^2)×9+30×1709×(9^2)+9^3\n          -------------\n            666 178 701 000\n            614 014 317 973 = 300×(17099^2)×7+30×17099×(7^2)+7^3\n            ---------------\n             52 164 383 027\n \n===Fourth root of 7===\n      1.   6    2    6    5    7\n     ---------------------------\n _ 4/ 7.0000 0000 0000 0000 0000\n  \\/  1 = 4000×(0^3)×1+600×(0^2)×(1^2)+40×0×(1^3)+1^4\n      -\n      6 0000\n      5 5536 = 4000×(1^3)×6+600×(1^2)×(6^2)+40×1×(6^3)+6^4\n      ------\n        4464 0000\n        3338 7536 = 4000×(16^3)×2+600*(16^2)×(2^2)+40×16×(2^3)+2^4\n        ---------\n        1125 2464 0000\n        1026 0494 3376 = 4000×(162^3)×6+600×(162^2)×(6^2)+40×162×(6^3)+6^4\n        --------------\n          99 1969 6624 0000\n          86 0185 1379 0625 = 4000×(1626^3)×5+600×(1626^2)×(5^2)+\n          -----------------   40×1626×(5^3)+5^4\n          13 1784 5244 9375 0000\n          12 0489 2414 6927 3201 = 4000×(16265^3)×7+600×(16265^2)×(7^2)+\n          ----------------------   40×16265×(7^3)+7^4\n           1 1295 2830 2447 6799\n\n==See also==\n* [[Methods of computing square roots]]\n* [[nth root algorithm|''n''th root algorithm]]\n\n==External links==\n*[http://www.homeschoolmath.net/teaching/sqr-algorithm-why-works.php Why the square root algorithm works] \"Home School Math\". Also related pages giving examples of the long-division-like pencil and paper method for square roots.\n\n[[Category:Root-finding algorithms]]\n[[Category:Computer arithmetic algorithms]]\n[[Category:Digit-by-digit algorithms]]"
    },
    {
      "title": "Spigot algorithm",
      "url": "https://en.wikipedia.org/wiki/Spigot_algorithm",
      "text": "A '''spigot algorithm''' is an [[algorithm]] for computing the value of a mathematical constant such as [[pi|{{pi}}]] or [[e (mathematical constant)|''e'']] which generates output digits in some base (usually 2 or a power of 2) from left to right, with limited intermediate storage. The name comes from the sense of the word \"spigot\" for a [[Tap (valve)|tap or valve]] controlling the flow of a liquid.\n\nInterest in spigot algorithms was spurred in the early days of computational mathematics by extreme constraints on memory, and such an algorithm for calculating the digits of ''e'' appeared in a paper by Sale in 1968.<ref>{{cite journal |author=Sale, AHJ |year=1968 |title=The calculation of ''e'' to many significant digits |journal= The Computer Journal|volume=11 |issue=2 |pages=229–230 |url=http://comjnl.oxfordjournals.org/content/11/2/229.abstract|accessdate=8 May 2013 |doi=10.1093/comjnl/11.2.229}}</ref> The name \"Spigot algorithm\" seems to have been coined by [[Stanley Rabinowitz]] and Stan Wagon,<ref>\n{{cite journal |url=http://stanleyrabinowitz.com/bibliography/spigot.pdf |title= A Spigot Algorithm for the Digits of Pi|last1= Rabinowitz|first1= Stanley|last2=Wagon|first2=Stan|journal=American Mathematical Monthly|volume=102|year=1995|pages=195–203|accessdate=8 May 2013 |doi=10.2307/2975006 |issue=3}}</ref> whose algorithm for calculating the digits of {{pi}} is sometimes referred to as \"''the'' spigot algorithm for {{pi}}\".{{Citation needed|date=October 2017}}\n\nThe spigot algorithm of Rabinowitz and Wagon is ''bounded'', in the sense that the number of required digits must be specified in advance. [[Jeremy Gibbons]] (2004)<ref>{{cite web|first=Jeremy|last=Gibbons|date=24 May 2004|url=http://web.comlab.ox.ac.uk/oucl/work/jeremy.gibbons/publications/spigot.pdf|title=Unbounded Spigot Algorithms for the Digits of Pi}}</ref>\nuses the term \"''streaming algorithm''\" to mean one which can be run indefinitely, without a prior bound. A further refinement is an algorithm which can compute a single arbitrary digit, without first computing the preceding digits: an example is the [[Bailey–Borwein–Plouffe formula]], a digit extraction algorithm for {{pi}} which produces hexadecimal digits.\n\n==Example==\nThis example illustrates the working of a spigot algorithm by calculating the binary digits of the [[natural logarithm]] of 2 {{OEIS|id=A068426}} using the identity\n\n:<math>\\ln(2)=\\sum_{k=1}^{\\infty}\\frac{1}{k2^k}\\, .</math>\n\nTo start calculating binary digits from, as an example, the 8th place we multiply this identity by 2<sup>7</sup> (since 7 =&nbsp;8&nbsp;−&nbsp;1):\n\n:<math>2^7\\ln(2) =2^7\\sum_{k=1}^\\infty \\frac{1}{k2^k}\\, .</math>\n\nWe then divide the infinite sum into a \"head\", in which the exponents of 2 are greater than or equal to zero, and a \"tail\", in which the exponents of 2 are negative:\n\n:<math>2^7\\ln(2) =\\sum_{k=1}^{7}\\frac{2^{7-k}}{k}+\\sum_{k=8}^{\\infty}\\frac{1}{k2^{k-7}}\\, .</math>\n\nWe are only interested in the fractional part of this value, so we can replace each of the summands in the \"head\" by\n\n:<math>\\frac{2^{7-k} \\bmod k} k \\, .</math>\n\nCalculating each of these terms and adding them to a running total where we again only keep the fractional part, we have:\n:{| class=\"wikitable\"\n|-\n! ''k''\n! ''A'' = 2<sup>7−''k''</sup>\n! ''B'' = ''A'' mod ''k''\n! ''C'' = ''B'' / ''k''\n! Sum of ''C'' mod 1\n|-\n| 1\n| 64\n| 0\n| 0\n| 0\n|-\n| 2\n| 32\n| 0\n| 0\n| 0\n|-\n| 3\n| 16\n| 1\n| 1/3\n| 1/3\n|-\n| 4\n| 8\n| 0\n| 0\n| 1/3\n|-\n| 5\n| 4\n| 4\n| 4/5\n| 2/15\n|-\n| 6\n| 2\n| 2\n| 1/3\n| 7/15\n|-\n| 7\n| 1\n| 1\n| 1/7\n| 64/105\n|}\n\nWe add a few terms in the \"tail\", noting that the error introduced by truncating the sum is less than the final term:\n\n:{| class=\"wikitable\"\n|-\n! ''k''\n! ''D'' = 1/''k''2<sup>''k''−7</sup>\n! Sum of ''D''\n! Maximum error\n|-\n| 8\n| 1/16\n| 1/16\n| 1/16\n\n|-\n| 9\n| 1/36\n| 13/144\n| 1/36\n|-\n| 10\n| 1/80\n| 37/360\n| 1/80\n|}\n\nAdding the \"head\" and the first few terms of the \"tail\" together we get:\n\n:<math>2^7\\ln(2)\\bmod1 \\approx \\frac{64}{105}+\\frac{37}{360}=0.10011100 \\ldots_2 + 0.00011010 \\ldots_2 = 0.1011 \\ldots_2 \\, ,</math>\n\nso the 8th to 11th binary digits in the binary expansion of ln(2) are 1, 0, 1, 1. Note that we have not calculated the values of the first seven binary digits – indeed, all information about them has been intentionally discarded by using [[modular arithmetic]] in the \"head\" sum.\n\nThe same approach can be used to calculate digits of the binary expansion of ln(2) starting from an arbitrary ''n''<sup>th</sup> position. The number of terms in the \"head\" sum increases linearly with ''n'', but the complexity of each term only increases with the logarithm of ''n'' if an efficient method of [[modular exponentiation]] is used. The [[precision (arithmetic)|precision]] of calculations and intermediate results and the number of terms taken from the \"tail\" sum are all independent of ''n'', and only depend on the number of binary digits that are being calculated – [[single precision]] arithmetic can be used to calculate around 12 binary digits, regardless of the starting position.\n\n==References==\n{{reflist}}\n\n==Further reading==\n* Arndt, Jorg; Haenel, Christoph, ''{{pi}} unleashed'', Springer Verlag, 2000.\n* {{MathWorld|urlname=SpigotAlgorithm|title=Spigot algorithm}}\n\n[[Category:Computer arithmetic algorithms]]"
    },
    {
      "title": "Toom–Cook multiplication",
      "url": "https://en.wikipedia.org/wiki/Toom%E2%80%93Cook_multiplication",
      "text": "'''Toom–Cook''', sometimes known as '''Toom-3''', named after [[Andrei Toom]], who introduced the new algorithm with its low complexity, and [[Stephen Cook]], who cleaned the description of it, is a [[multiplication algorithm]] for large integers.\n\nGiven two large integers, ''a'' and ''b'', Toom–Cook splits up ''a'' and ''b'' into ''k'' smaller parts each of length ''l'', and performs operations on the parts. As ''k'' grows, one may combine many of the multiplication sub-operations, thus reducing the overall complexity of the algorithm. The multiplication sub-operations can then be computed recursively using Toom–Cook multiplication again, and so on. Although the terms \"Toom-3\" and \"Toom–Cook\" are sometimes incorrectly used interchangeably, Toom-3 is only a single instance of the Toom–Cook algorithm, where ''k'' = 3.\n\nToom-3 reduces 9 multiplications to 5, and runs in Θ(''n''<sup>log(5)/log(3)</sup>) ≈ Θ(''n''<sup>1.46</sup>). In general, Toom-''k'' runs in {{nowrap|Θ(''c''(''k'') ''n<sup>e</sup>'')}}, where {{nowrap|1=''e'' = log(2''k'' − 1) / log(''k'')}}, ''n<sup>e</sup>'' is the time spent on sub-multiplications, and ''c'' is the time spent on additions and multiplication by small constants.<ref name=\"Knuth, p. 296\">Knuth, p. 296</ref> The [[Karatsuba algorithm]] is a special case of Toom–Cook, where the number is split into two smaller ones. It reduces 4 multiplications to 3 and so operates at Θ(''n''<sup>log(3)/log(2)</sup>) ≈ Θ(''n''<sup>1.58</sup>). Ordinary long multiplication is equivalent to Toom-1, with complexity Θ(''n''<sup>2</sup>).\n\nAlthough the exponent ''e'' can be set arbitrarily close to 1 by increasing ''k'', the function ''c'' unfortunately grows very rapidly.<ref name=\"Knuth, p. 296\"/><ref>Crandall & Pomerance, p. 474</ref> The growth rate for mixed-level Toom-Cook schemes was still an open research problem in 2005.<ref>Crandall & Pomerance, p. 536</ref> An implementation described by [[Donald Knuth]] achieves the time complexity {{math|Θ(''n'' 2<sup>{{sqrt|2 log ''n''}}</sup> log ''n'')}}.<ref>Knuth, p. 302</ref>\n\nDue to its overhead, Toom–Cook is slower than long multiplication with small numbers, and it is therefore typically used for intermediate-size multiplications, before the asymptotically faster [[Schönhage–Strassen algorithm]] (with complexity {{nowrap|Θ(''n'' log ''n'' log log ''n'')}}) becomes practical.\n\nToom first described this algorithm in 1963, and Cook published an improved (asymptotically equivalent) algorithm in his PhD thesis in 1966.<ref>[http://cr.yp.to/bib/1966/cook.html Positive Results], chapter III of Stephen A. Cook: ''On the Minimum Computation Time of Functions''.</ref>\n\n==Details==\nThis section discusses exactly how to perform Toom-''k'' for any given value of ''k'', and is a simplification of a description of Toom–Cook polynomial multiplication described by Marco Bodrato.<ref name=\"Bodrato2007\">Marco Bodrato. Towards Optimal Toom–Cook Multiplication for Univariate and Multivariate Polynomials in Characteristic 2 and 0. In ''WAIFI'07 proceedings'', volume 4547 of LNCS, pages 116–133. June 21–22, 2007. [http://bodrato.it/papers/#WAIFI2007 author website]</ref> The algorithm has five main steps: \n# [[Toom–Cook multiplication#Splitting|Splitting]]\n# [[Toom–Cook multiplication#Evaluation|Evaluation]]\n# [[Toom–Cook multiplication#Pointwise multiplication|Pointwise multiplication]]\n# [[Toom–Cook multiplication#Interpolation|Interpolation]]\n# [[Toom–Cook multiplication#Recomposition|Recomposition]]\n\nIn a typical large integer implementation, each integer is represented as a sequence of digits in [[positional notation]], with the base or radix set to some (typically large) value ''b''; for this example we use ''b''&nbsp;=&nbsp;10000, so that each digit corresponds to a group of four decimal digits (in a computer implementation, ''b'' would typically be a power of 2 instead). Say the two integers being multiplied are:\n\n:{|\n|''m'' || = ||12||3456||7890||1234||5678||9012\n|-\n|''n'' || =\n|align=right|9||8765||4321||9876||5432||1098.\n|}\n\nThese are much smaller than would normally be processed with Toom–Cook (grade-school multiplication would be faster) but they will serve to illustrate the algorithm.\n\n=== Splitting ===\nThe first step is to select the base ''B''&nbsp;=&nbsp;''b''<sup>''i''</sup>, such that the number of digits of both ''m'' and ''n'' in base ''B'' is at most ''k'' (e.g., 3 in Toom-3). A typical choice for ''i'' is given by:\n\n:<math>i = \\max\\left\\{\\left\\lfloor\\frac{\\left\\lfloor\\log_b m\\right\\rfloor}{k}\\right\\rfloor, \\left\\lfloor\\frac{\\left\\lfloor\\log_b n\\right\\rfloor}{k}\\right\\rfloor\\right\\} + 1.</math>\n\nIn our example we'll be doing Toom-3, so we choose {{nowrap|1=''B'' = ''b''<sup>2</sup> = 10<sup>8</sup>}}. We then separate ''m'' and ''n'' into their base ''B'' digits ''m''<sub>''i''</sub>, ''n''<sub>''i''</sub>:\n\n:<math>\n\\begin{align}\nm_2 & {} = 123456 \\\\\nm_1 & {} = 78901234 \\\\\nm_0 & {} = 56789012 \\\\\nn_2 & {} = 98765 \\\\\nn_1 & {} = 43219876 \\\\\nn_0 & {} = 54321098\n\\end{align}\n</math>\n\nWe then use these digits as coefficients in degree-{{nowrap|(''k'' − 1)}} polynomials ''p'' and ''q'', with the property that ''p''(''B'')&nbsp;=&nbsp;''m'' and ''q''(''B'')&nbsp;=&nbsp;''n'':\n\n:<math>p(x) = m_2x^2 + m_1x + m_0 = 123456x^2 + 78901234x + 56789012 \\, </math>\n:<math>q(x) = n_2x^2 + n_1x + n_0 = 98765x^2 + 43219876x + 54321098 \\, </math>\n\nThe purpose of defining these polynomials is that if we can compute their product {{nowrap|1=''r''(''x'') = ''p''(''x'')''q''(''x'')}}, our answer will be {{nowrap|1=''r''(''B'') = ''m'' × ''n''}}.\n\nIn the case where the numbers being multiplied are of different sizes, it's useful to use different values of ''k'' for ''m'' and ''n'', which we'll call ''k''<sub>''m''</sub> and ''k''<sub>''n''</sub>. For example, the algorithm \"Toom-2.5\" refers to Toom–Cook with ''k''<sub>''m''</sub>&nbsp;=&nbsp;3 and ''k''<sub>''n''</sub>&nbsp;=&nbsp;2. In this case the ''i'' in ''B''&nbsp;=&nbsp;''b''<sup>''i''</sup> is typically chosen by:\n\n:<math>i = \\max\\left\\{\\left\\lfloor\\frac{\\left\\lceil\\log_b m\\right\\rceil}{k_m}\\right\\rfloor, \\left\\lfloor\\frac{\\left\\lceil\\log_b n\\right\\rceil}{k_n}\\right\\rfloor\\right\\}.</math>\n\n=== Evaluation ===\nThe Toom–Cook approach to computing the polynomial product ''p''(''x'')''q''(''x'') is a commonly used one. Note that a polynomial of degree ''d'' is uniquely determined by ''d''&nbsp;+&nbsp;1 points (for example, a line - polynomial of degree one is specified by two points). The idea is to evaluate ''p''(·) and ''q''(·) at various points. Then multiply their values at these points to get points on the product polynomial. Finally interpolate to find its coefficients.\n\nSince {{nowrap|1=deg(''pq'') = deg(''p'') + deg(''q'')}}, we will need {{nowrap|1=deg(''p'') + deg(''q'') + 1 = ''k''<sub>''m''</sub> + ''k''<sub>''n''</sub> − 1}} points to determine the final result. Call this ''d''. In the case of Toom-3, ''d''&nbsp;=&nbsp;5. The algorithm will work no matter what points are chosen (with a few small exceptions, see matrix invertibility requirement in [[Toom–Cook multiplication#Interpolation|Interpolation]]), but in the interest of simplifying the algorithm it's better to choose small integer values like 0, 1, −1, and −2.\n\nOne unusual point value that is frequently used is infinity, written ∞ or 1/0. To \"evaluate\" a polynomial ''p'' at infinity actually means to take the limit of ''p''(''x'')/''x''<sup>deg ''p''</sup> as ''x'' goes to infinity. Consequently, ''p''(∞) is always the value of its highest-degree coefficient (in the example above coefficient m<sub>2</sub>).\n\nIn our Toom-3 example, we will use the points 0, 1, −1, −2, and ∞. These choices simplify evaluation, producing the formulas:\n\n:<math>\n\\begin{array}{lrlrl}\np(0) & = & m_0 + m_1(0) + m_2(0)^2 & = & m_0 \\\\\np(1) & = & m_0 + m_1(1) + m_2(1)^2 & = & m_0 + m_1 + m_2 \\\\\np(-1) & = & m_0 + m_1(-1) + m_2(-1)^2 & = & m_0 - m_1 + m_2 \\\\\np(-2) & = & m_0 + m_1(-2) + m_2(-2)^2 & = & m_0 - 2m_1 + 4m_2 \\\\\np(\\infty) & = & m_2 & &\n\\end{array}\n</math>\n\nand analogously for ''q''. In our example, the values we get are:\n\n:{|\n|-\n|''p''(0) || = || ''m''<sub>0</sub> || = || 56789012 || = ||align=\"right\"| 56789012\n|-\n|''p''(1) || = || ''m''<sub>0</sub> + ''m''<sub>1</sub> + ''m''<sub>2</sub> || = || 56789012 + 78901234 + 123456 || = ||align=\"right\"| 135813702\n|-\n|''p''(−1) || = || ''m''<sub>0</sub> − ''m''<sub>1</sub> + ''m''<sub>2</sub> || = || 56789012 − 78901234 + 123456 || = ||align=\"right\"| −21988766\n|-\n|''p''(−2) || = || ''m''<sub>0</sub> − 2''m''<sub>1</sub> + 4''m''<sub>2</sub> || = || 56789012 − 2 × 78901234 + 4 × 123456 || = ||align=\"right\"| −100519632\n|-\n|''p''(∞) || = || ''m''<sub>2</sub> || = || 123456 || = ||align=\"right\"| 123456\n|-\n|''q''(0) || = || ''n''<sub>0</sub> || = || 54321098 || = ||align=\"right\"| 54321098\n|-\n|''q''(1) || = || ''n''<sub>0</sub> + ''n''<sub>1</sub> + ''n''<sub>2</sub> || = || 54321098 + 43219876 + 98765 || = ||align=\"right\"| 97639739\n|-\n|''q''(−1) || = || ''n''<sub>0</sub> − ''n''<sub>1</sub> + ''n''<sub>2</sub> || = || 54321098 − 43219876 + 98765 || = ||align=\"right\"| 11199987\n|-\n|''q''(−2) || = || ''n''<sub>0</sub> − 2''n''<sub>1</sub> + 4''n''<sub>2</sub> || = || 54321098 − 2 × 43219876 + 4 × 98765 || = ||align=\"right\"| −31723594\n|-\n|''q''(∞) || = || ''n''<sub>2</sub> || = || 98765 || = ||align=\"right\"| 98765.\n|}\n\nAs shown, these values may be negative.\n\nFor the purpose of later explanation, it will be useful to view this evaluation process as a matrix-vector multiplication, where each row of the matrix contains powers of one of the evaluation points, and the vector contains the coefficients of the polynomial:\n\n: <math>\n\\left(\\begin{matrix}p(0) \\\\ p(1) \\\\ p(-1) \\\\ p(-2) \\\\ p(\\infty)\\end{matrix}\\right) =\n\\left(\\begin{matrix}\n0^0 & 0^1 & 0^2 \\\\\n1^0 & 1^1 & 1^2 \\\\\n(-1)^0 & (-1)^1 & (-1)^2 \\\\\n(-2)^0 & (-2)^1 & (-2)^2 \\\\\n0 & 0 & 1\n\\end{matrix}\\right)\n\\left(\\begin{matrix}m_0 \\\\ m_1 \\\\ m_2\\end{matrix}\\right) =\n\\left(\\begin{matrix}\n1 & 0 & 0 \\\\\n1 & 1 & 1 \\\\\n1 & -1 & 1 \\\\\n1 & -2 & 4 \\\\\n0 & 0 & 1\n\\end{matrix}\\right)\n\\left(\\begin{matrix}m_0 \\\\ m_1 \\\\ m_2\\end{matrix}\\right).\n</math>\n\nThe dimensions of the matrix are ''d'' by ''k''<sub>''m''</sub> for ''p'' and ''d'' by ''k''<sub>''n''</sub> for ''q''. The row for infinity is always all zero except for a 1 in the last column.\n\n==== Faster evaluation ====\nMultipoint evaluation can be obtained faster than with the above formulas. The number of elementary operations (addition/subtraction) can be reduced. The sequence given by Bodrato<ref name=\"Bodrato2007\"/> for Toom-3, executed here over the first operand (polynomial ''p'') of the running example is the following:\n\n:{|\n|-\n|''p''<sub>0</sub> || ← || ''m''<sub>0</sub> + ''m''<sub>2</sub> || = || 56789012 + 123456 || = ||align=\"right\"| 56912468\n|-\n|''p''(0) || = || ''m''<sub>0</sub> || = || 56789012 || = ||align=\"right\"| 56789012\n|-\n|''p''(1) || = || ''p''<sub>0</sub> + ''m''<sub>1</sub> || = || 56912468 + 78901234 || = ||align=\"right\"| 135813702\n|-\n|''p''(−1) || = || ''p''<sub>0</sub> − ''m''<sub>1</sub> || = || 56912468 − 78901234 || = ||align=\"right\"| −21988766\n|-\n|''p''(−2) || = || (''p''(−1) + ''m''<sub>2</sub>) × 2 − ''m''<sub>0</sub> || = ||align=\"right\"| (− 21988766 + 123456 ) × 2 − 56789012 || = || − 100519632\n|-\n|''p''(∞) || = || ''m''<sub>2</sub> || = || 123456 || = ||align=\"right\"| 123456.\n|}\n\nThis sequence requires five addition/subtraction operations, one less than the straightforward evaluation. Moreover the multiplication by 4 in the calculation of ''p''(−2) was saved.\n\n=== Pointwise multiplication ===\nUnlike multiplying the polynomials ''p''(·) and ''q''(·), multiplying the evaluated values ''p''(''a'') and ''q''(''a'') just involves multiplying integers &mdash; a smaller instance of the original problem. We recursively invoke our multiplication procedure to multiply each pair of evaluated points. In practical implementations, as the operands become smaller, the algorithm will switch to [[Multiplication algorithm#Long multiplication|Schoolbook long multiplication]]. Letting ''r'' be the product polynomial, in our example we have:\n\n:{|\n|-\n|''r''(0) || = || ''p''(0)''q''(0) || = || 56789012 × 54321098 || = ||align=\"right\"| 3084841486175176\n|-\n|''r''(1) || = || ''p''(1)''q''(1) || = || 135813702 × 97639739 || = ||align=\"right\"| 13260814415903778\n|-\n|''r''(−1) || = || ''p''(−1)''q''(−1) || = || −21988766 × 11199987 || = ||align=\"right\"| −246273893346042\n|-\n|''r''(−2) || = || ''p''(−2)''q''(−2) || = || −100519632 × −31723594 || = ||align=\"right\"| 3188843994597408\n|-\n|''r''(∞) || = || ''p''(∞)''q''(∞) || = || 123456 × 98765 || = ||align=\"right\"| 12193131840.\n|}\n\nAs shown, these can also be negative. For large enough numbers, this is the most expensive step, the only step that is not linear in the sizes of ''m'' and ''n''.\n\n=== Interpolation ===\nThis is the most complex step, the reverse of the evaluation step: given our ''d'' points on the product polynomial ''r''(·), we need to determine its coefficients. In other words, we want to solve this matrix equation for the vector on the right-hand side:\n\n: <math>\n\\begin{align}\n\\left(\\begin{matrix}r(0) \\\\ r(1) \\\\ r(-1) \\\\ r(-2) \\\\ r(\\infty)\\end{matrix}\\right) & {} =\n\\left(\\begin{matrix}\n0^0 & 0^1 & 0^2 & 0^3 & 0^4 \\\\\n1^0 & 1^1 & 1^2 & 1^3 & 1^4 \\\\\n(-1)^0 & (-1)^1 & (-1)^2 & (-1)^3 & (-1)^4 \\\\\n(-2)^0 & (-2)^1 & (-2)^2 & (-2)^3 & (-2)^4 \\\\\n0 & 0 & 0 & 0 & 1\n\\end{matrix}\\right)\n\\left(\\begin{matrix}r_0 \\\\ r_1 \\\\ r_2 \\\\ r_3 \\\\ r_4\\end{matrix}\\right) \\\\\n & {} =\n\\left(\\begin{matrix}\n1 & 0 & 0 & 0 & 0 \\\\\n1 & 1 & 1 & 1 & 1 \\\\\n1 & -1 & 1 & -1 & 1 \\\\\n1 & -2 & 4 & -8 & 16 \\\\\n0 & 0 & 0 & 0 & 1\n\\end{matrix}\\right)\n\\left(\\begin{matrix}r_0 \\\\ r_1 \\\\ r_2 \\\\ r_3 \\\\ r_4\\end{matrix}\\right).\n\\end{align}\n</math>\n\nThis matrix is constructed the same way as the one in the evaluation step, except that it's ''d'' × ''d''. We could solve this equation with a technique like [[Gaussian elimination]], but this is too expensive. Instead, we use the fact that, provided the evaluation points were chosen suitably, this matrix is invertible, and so:\n\n: <math>\n\\begin{align}\n\\left(\\begin{matrix}r_0 \\\\ r_1 \\\\ r_2 \\\\ r_3 \\\\ r_4\\end{matrix}\\right) & {} =\n\\left(\\begin{matrix}\n1 & 0 & 0 & 0 & 0 \\\\\n1 & 1 & 1 & 1 & 1 \\\\\n1 & -1 & 1 & -1 & 1 \\\\\n1 & -2 & 4 & -8 & 16 \\\\\n0 & 0 & 0 & 0 & 1\n\\end{matrix}\\right)^{-1}\n\\left(\\begin{matrix}r(0) \\\\ r(1) \\\\ r(-1) \\\\ r(-2) \\\\ r(\\infty)\\end{matrix}\\right) \\\\\n& {} =\n\\left(\\begin{matrix}\n 1 & 0 & 0 & 0 & 0 \\\\\n \\tfrac12 & \\tfrac13 & -1 & \\tfrac16 & -2 \\\\\n -1 & \\tfrac12 & \\tfrac12 & 0 & -1 \\\\\n-\\tfrac12 & \\tfrac16 & \\tfrac12 & -\\tfrac16 & 2 \\\\\n 0 & 0 & 0 & 0 & 1\n\\end{matrix}\\right)\n\\left(\\begin{matrix}r(0) \\\\ r(1) \\\\ r(-1) \\\\ r(-2) \\\\ r(\\infty)\\end{matrix}\\right).\n\\end{align}\n</math>\n\nAll that remains is to compute this matrix-vector product. Although the matrix contains fractions, the resulting coefficients will be integers &mdash; so this can all be done with integer arithmetic, just additions, subtractions, and multiplication/division by small constants. A difficult design challenge in Toom–Cook is to find an efficient sequence of operations to compute this product; one sequence given by Bodrato<ref name=\"Bodrato2007\" /> for Toom-3 is the following, executed here over the running example:\n\n:{|\n|-\n|''r''<sub>0</sub> || ← || ''r''(0) || = || 3084841486175176\n|-\n|''r''<sub>4</sub> || ← || ''r''(∞) || = || 12193131840 \n|-\n|''r''<sub>3</sub> || ← || (''r''(−2) − ''r''(1))/3 || = || (3188843994597408 − 13260814415903778)/3 \n|-\n| || || || = || −3357323473768790\n|-\n|''r''<sub>1</sub> || ← || (''r''(1) − ''r''(−1))/2 || = || (13260814415903778 − (−246273893346042))/2 \n|-\n| || || || = || 6753544154624910\n|-\n|''r''<sub>2</sub> || ← || ''r''(−1) − ''r''(0) || = || −246273893346042 − 3084841486175176 \n|-\n| || || || = || −3331115379521218\n|-\n|''r''<sub>3</sub> || ← || (''r''<sub>2</sub> − ''r''<sub>3</sub>)/2 + 2''r''(∞) || = || (−3331115379521218 − (−3357323473768790))/2 + 2 × 12193131840 \n|-\n| || || || = || 13128433387466\n|-\n|''r''<sub>2</sub> || ← || ''r''<sub>2</sub> + ''r''<sub>1</sub> − ''r''<sub>4</sub> || = || −3331115379521218 + 6753544154624910 − 12193131840 \n|-\n| || || || = || 3422416581971852\n|-\n|''r''<sub>1</sub> || ← || ''r''<sub>1</sub> − ''r''<sub>3</sub> || = || 6753544154624910 − 13128433387466 \n|-\n| || || || = || 6740415721237444.\n|}\n\nWe now know our product polynomial ''r'':\n\n: <math>\n\\begin{array}{rrr}\nr(x) = & {} & 3084841486175176 \\\\\n & + & 6740415721237444x \\\\\n & + & 3422416581971852x^2 \\\\\n & + & 13128433387466x^3 \\\\\n & + & 12193131840x^4\n\\end{array}\n</math>\n\nIf we were using different ''k''<sub>''m''</sub>, ''k''<sub>''n''</sub>, or evaluation points, the matrix and so our interpolation strategy would change; but it does not depend on the inputs and so can be hard-coded for any given set of parameters.\n\n=== Recomposition ===\n\nFinally, we evaluate r(B) to obtain our final answer. This is straightforward since B is a power of ''b'' and so the multiplications by powers of B are all shifts by a whole number of digits in base ''b''. In the running example b = 10<sup>4</sup> and B = b<sup>2</sup> = 10<sup>8</sup>.\n\n:{|\n|-\n| || || || || || || || ||3084||8414||8617||5176\n|-\n| || || || || || ||6740||4157||2123||7444|| ||\n|-\n| || || || ||3422||4165||8197||1852|| || || ||\n|-\n| || \n|align=right| 13||1284||3338||7466|| || || || || ||\n|-\n| + ||121||9313||1840|| || || || || || || ||\n|-\n|colspan=100|<hr>\n|-\n| ||121||9326||3124||6761||1632||4937||6009||5208||5858||8617||5176\n|}\n\nAnd this is in fact the product of 1234567890123456789012 and 987654321987654321098.\n\n==Interpolation matrices for various ''k''==\n\nHere we give common interpolation matrices for a few different common small values of ''k''<sub>''m''</sub> and ''k''<sub>''n''</sub>.\n\n=== Toom-1 ===\nToom-1 (''k''<sub>''m''</sub> = ''k''<sub>''n''</sub> = 1) requires 1 evaluation point, here chosen to be 0. It degenerates to long multiplication, with an interpolation matrix of the identity matrix:\n\n: <math>\\left(\\begin{matrix}1\\end{matrix}\\right)^{-1} = \n\\left(\\begin{matrix}1\\end{matrix}\\right).</math>\n\n=== Toom-1.5 ===\nToom-1.5 (''k''<sub>''m''</sub> = 2, ''k''<sub>''n''</sub> = 1) requires 2 evaluation points, here chosen to be 0 and ∞. Its interpolation matrix is then the identity matrix:\n\n: <math>\\left(\\begin{matrix}1 & 0 \\\\ 0 & 1\\end{matrix}\\right)^{-1} =\n\\left(\\begin{matrix}1 & 0 \\\\ 0 & 1\\end{matrix}\\right).</math>\n\nThis also degenerates to long multiplication: both coefficients of one factor are multipled by the sole coefficient of the other factor.\n\n=== Toom-2 ===\nToom-2 (''k''<sub>''m''</sub> = 2, ''k''<sub>''n''</sub> = 2) requires 3 evaluation points, here chosen to be 0, 1, and ∞. It is the same as [[Karatsuba multiplication]], with an interpolation matrix of:\n\n: <math>\\left(\\begin{matrix}\n1 & 0 & 0 \\\\\n1 & 1 & 1 \\\\\n0 & 0 & 1\n\\end{matrix}\\right)^{-1} =\n\\left(\\begin{matrix}\n1 & 0 & 0 \\\\\n-1 & 1 & -1 \\\\\n0 & 0 & 1\n\\end{matrix}\\right).</math>\n\n=== Toom-2.5 ===\nToom-2.5 (''k''<sub>''m''</sub> = 3, ''k''<sub>''n''</sub> = 2) requires 4 evaluation points, here chosen to be 0, 1, −1, and ∞. It then has an interpolation matrix of:\n\n: <math>\\left(\\begin{matrix}\n1 & 0 & 0 & 0 \\\\\n1 & 1 & 1 & 1 \\\\\n1 & -1 & 1 & -1 \\\\\n0 & 0 & 0 & 1\n\\end{matrix}\\right)^{-1} =\n\\left(\\begin{matrix}\n1 & 0 & 0 & 0 \\\\\n0 & \\tfrac12 & -\\tfrac12 & -1 \\\\\n-1 & \\tfrac12 & \\tfrac12 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{matrix}\\right).</math>\n\n==Notes==\n<references/>\n\n==References==\n* D. Knuth. ''[[The Art of Computer Programming]]'', Volume 2. Third Edition, Addison-Wesley, 1997. Section 4.3.3.A: Digital methods, pg.294.\n* R. Crandall & C. Pomerance. ''Prime Numbers – A Computational Perspective''. Second Edition, Springer, 2005. Section 9.5.1: Karatsuba and Toom–Cook methods, pg.473.\n* M. Bodrato. ''[http://bodrato.it/toom-cook/ Toward Optimal Toom–Cook Multiplication...]''. In WAIFI'07, Springer, 2007.\n\n== External links ==\n* [http://gmplib.org/manual/Toom-3_002dWay-Multiplication.html Toom-Cook 3-Way Multiplication from GMP Documentations]\n\n{{Number-theoretic algorithms}}\n\n{{DEFAULTSORT:Toom-Cook multiplication}}\n[[Category:Computer arithmetic algorithms]]\n[[Category:Multiplication]]"
    },
    {
      "title": "Binary CORDIC",
      "url": "https://en.wikipedia.org/wiki/Binary_CORDIC",
      "text": "#redirect [[CORDIC#Binary]] {{R to related topic}}\n\n[[Category:Digit-by-digit algorithms]]\n[[Category:Shift-and-add algorithms]]"
    },
    {
      "title": "BKM algorithm",
      "url": "https://en.wikipedia.org/wiki/BKM_algorithm",
      "text": "{{Expand German|BKM-Algorithmus|date=December 2017}}\n{{Use dmy dates|date=May 2019|cs1-dates=y}}\n{{anchor|L-mode|E-mode}}\nThe '''BKM algorithm''' is a [[shift-and-add algorithm]] for computing [[elementary function (differential algebra)|elementary function]]s, first published in 1994 by Jean-Claude Bajard, Sylvanus Kla, and Jean-Michel Muller.  BKM is based on computing complex [[logarithm]]s (''L-mode'') and [[exponential function|exponential]]s (''E-mode'') using a method similar to the algorithm [[Henry Briggs (mathematician)|Henry Briggs]] used to compute logarithms.  By using a precomputed table of logarithms of negative powers of two, the BKM algorithm computes elementary functions using only integer add, shift, and compare operations.\n\nBKM is similar to [[CORDIC]], but uses a table of [[logarithm]]s rather than a table of [[arctangent]]s.  On each iteration, a choice of coefficient is made from a set of nine complex numbers, 1, 0, −1, i, −i, 1+i, 1−i, −1+i, −1−i, rather than only −1 or +1 as used by CORDIC.  BKM provides a simpler method of computing some elementary functions, and unlike CORDIC, BKM needs no result scaling factor.  The convergence rate of BKM is approximately one bit per iteration, like CORDIC, but BKM requires more precomputed table elements for the same precision because the table stores logarithms of complex operands.\n\nAs with other algorithms in the shift-and-add class, BKM is particularly well-suited to hardware implementation.  The relative performance of software BKM implementation in comparison to other methods such as [[polynomial]] or [[rational function|rational]] approximations will depend on the availability of fast multi-bit shifts (i.e. a [[barrel shifter]]) or hardware [[floating point]] arithmetic.\n\n==References==\n{{Reflist}}\n* {{cite journal |author-first1=Jean-Claude |author-last1=Bajard |author-first2=Sylvanus |author-last2=Kla |author-first3=Jean-Michel |author-last3=Muller |title=BKM: A new hardware algorithm for complex elementary functions |journal=[[IEEE Transactions on Computers]] |volume=43 |issue=8 |pages=955–963 |date=August 1994 |issn=0018-9340 |doi=10.1109/12.295857 |url=http://perso.ens-lyon.fr/jean-michel.muller/BKM94.pdf |access-date=2017-12-21 |dead-url=no |archive-url=https://archive.today/20171221172622/http://perso.ens-lyon.fr/jean-michel.muller/BKM94.pdf |archive-date=2017-12-21 |df= }}\n* {{cite journal |author-first1=Jean-Claude |author-last1=Bajard |author-first2=Laurent |author-last2=Imbert |editor-first=Franklin T. |editor-last=Luk |title=Evaluation of Complex Elementary Functions: A New Version of BKM |date=1999-11-02 |journal=SPIE Proceedings, Advanced Signal Processing Algorithms, Architectures, and Implementations IX |publisher=[[Society of Photo-Optical Instrumentation Engineers]] (SPIE) |volume=3807 |doi=10.1117/12.367631 |series=Advanced Signal Processing Algorithms, Architectures, and Implementations IX |pages=2–9 |url=https://www-almasty.lip6.fr/~bajard/MesPublis/Spie1999.pdf |access-date=2018-07-11|bibcode=1999SPIE.3807....2B }} [https://www.spiedigitallibrary.org/conference-proceedings-of-spie/3807/1/Evaluation-of-complex-elementary-functions--a-new-version-of/10.1117/12.367631.short]\n* {{cite journal |author-first1=Laurent |author-last1=Imbert |author-first2=Jean-Michel |author-last2=Muller |author-first3=Fabien |author-last3=Rico |title=Radix-10 BKM Algorithm for Computing Transcendentals on Pocket Computers |date=2006-05-24 |orig-year=2000-06-01, September 1999 |journal=Journal of VLSI Signal Processing |volume=25 |issue=2 |issn=0922-5773 |pages=179–186 |publisher=[[Kluwer Academic Publishers]] / [[Institut national de recherche en informatique et en automatique]] (INRIA) |type=Research report |id=RR-3754. INRIA-00072908. Theme 2 |doi=10.1023/A:1008127208220 |url=https://hal.inria.fr/inria-00072908/document |access-date=2018-07-11 |dead-url=no |archive-url=https://web.archive.org/web/20180711091703/https://hal.inria.fr/inria-00072908/document |archive-date=2018-07-11}} [https://www.researchgate.net/profile/Fabien_Rico/publication/226548529_A_Radix-10_BKM_Algorithm_for_Computing_Transcendentals_on_Pocket_Computers/links/00b49517b782924cd8000000/A-Radix-10-BKM-Algorithm-for-Computing-Transcendentals-on-Pocket-Computers.pdf<!-- https://web.archive.org/web/20180711094855/https://www.researchgate.net/profile/Fabien_Rico/publication/226548529_A_Radix-10_BKM_Algorithm_for_Computing_Transcendentals_on_Pocket_Computers/links/00b49517b782924cd8000000/A-Radix-10-BKM-Algorithm-for-Computing-Transcendentals-on-Pocket-Computers.pdf -->] [https://link.springer.com/article/10.1023/A:1008127208220]\n* {{cite book |author-first=Jean-Michel |author-last=Muller |title=Elementary Functions: Algorithms and Implementation |edition=2 |publisher=[[Birkhäuser]] |location=Boston, MA, USA |date=2006 |isbn=978-0-8176-4372-0 |lccn=2005048094}}\n* {{cite book |author-first=Jean-Michel |author-last=Muller |title=Elementary Functions: Algorithms and Implementation |edition=3 |publisher=[[Birkhäuser]] |location=Boston, MA, USA |date=2016-12-12 |isbn=978-1-4899-7981-0 }}\n\n==Further reading==\n* {{cite book |author-first1=Günter |author-last1=Jorke |author-first2=Bernhard |author-last2=Lampe |author-first3=Norbert |author-last3=Wengel |title=Arithmetische Algorithmen der Mikrorechentechnik |edition=1 |publisher=[[:de:VEB Verlag Technik|VEB Verlag Technik]] |location=Berlin, Germany |date=1989 |language=de |pages=280–282 |isbn=3-34100515-3 |id=. {{EAN|9783341005156}}. MPN 5539165. License 201.370/4/89 |url=https://books.google.com/books/about/Arithmetische_Algorithmen_der_Mikroreche.html?id=DqYWAQAAMAAJ |access-date=2015-12-01}}\n* {{cite journal |title=Pseudo Division and Pseudo Multiplication Processes |author-first=John E. |author-last=Meggitt |journal=[[IBM Journal of Research and Development]] |volume=6 |issue=2 |date=1961-08-29 |publication-date=April 1962 |pages=210–226, 287 |publisher=[[IBM Corporation]] |location=Riverton, New Jersey, USA |doi=10.1147/rd.62.0210 |url=http://dl.acm.org/citation.cfm?id=1661979.1661985 |access-date=2015-12-01}}\n* {{cite journal |title=Automatic computation of exponentials, logarithms, ratios and square roots |author-first=Tien |author-last=Chi Chen |author-link=Tien Chi Chen |publisher=[[IBM San Jose Research Laboratory]]; [[IBM Corporation]] |location=San Jose, California, USA; Riverton, New Jersey, USA |journal=[[IBM Journal of Research and Development]] |volume=16 |issue=4 |date=July 1972 |pages=380–388 |doi=10.1147/rd.164.0380 |url=http://dl.acm.org/citation.cfm?id=1661877 |access-date=2015-12-01}}\n\n==External links==\n* {{cite web |first1=Nathalie |last1=Revol |first2=Jean-Claude |last2=Yakoubsohn |title=Accelerated Shift-and-Add algorithms |publisher=Laboratoire d'Analyse Numérique et d'Optimisation (ANO) de l'[[Université des Sciences et Technologies de Lille]]; [[Kluwer Academic Publishers]] |location=Boston, USA |url=http://perso.ens-lyon.fr/nathalie.revol/publis/RY00.pdf |access-date=2017-12-21 |dead-url=no |archive-url=https://web.archive.org/web/20171221172147/http://perso.ens-lyon.fr/nathalie.revol/publis/RY00.pdf |archive-date=2017-12-21}}\n\n{{mathanalysis-stub}}\n\n[[Category:Computer arithmetic]]\n[[Category:Shift-and-add algorithms]]\n[[Category:Digit-by-digit algorithms]]\n[[Category:1994 introductions]]\n[[Category:1994 in science]]"
    },
    {
      "title": "Decimal CORDIC",
      "url": "https://en.wikipedia.org/wiki/Decimal_CORDIC",
      "text": "#redirect [[CORDIC#Decimal]] {{R to related topic}}\n\n[[Category:Digit-by-digit algorithms]]\n[[Category:Shift-and-add algorithms]]"
    },
    {
      "title": "Hyperbolic CORDIC",
      "url": "https://en.wikipedia.org/wiki/Hyperbolic_CORDIC",
      "text": "#redirect [[CORDIC#hyperbolic mode]] {{R to related topic}}\n\n[[Category:Digit-by-digit algorithms]]\n[[Category:Shift-and-add algorithms]]"
    },
    {
      "title": "Meggitt's method",
      "url": "https://en.wikipedia.org/wiki/Meggitt%27s_method",
      "text": "#redirect [[CORDIC#Meggitt]] {{R to related topic}}\n\n[[Category:Digit-by-digit algorithms]]\n[[Category:Shift-and-add algorithms]]"
    },
    {
      "title": "Pseudo-division",
      "url": "https://en.wikipedia.org/wiki/Pseudo-division",
      "text": "#REDIRECT [[CORDIC#Pseudo-division]]\n\n{{Redirect category shell|1=\n{{R to related topic}}\n{{R with possibilities}}\n}}\n\n<!-- [[Category:Numerical analysis]] -->\n[[Category:Digit-by-digit algorithms]]\n[[Category:Shift-and-add algorithms]]"
    },
    {
      "title": "Shift-and-add algorithm",
      "url": "https://en.wikipedia.org/wiki/Shift-and-add_algorithm",
      "text": "#REDIRECT [[Multiplication algorithm#Shift and add]]\n\n{{Redirect category shell|1=\n{{R to related topic}}\n{{R to section}}\n{{R with possibilities}}\n}}\n\n[[Category:Shift-and-add algorithms]]"
    },
    {
      "title": "Shift-and-add-3 algorithm",
      "url": "https://en.wikipedia.org/wiki/Shift-and-add-3_algorithm",
      "text": "#redirect [[Double dabble]] {{R from alternative name}}\n\n[[Category:Shift-and-add algorithms]]"
    },
    {
      "title": "Trigonometric CORDIC",
      "url": "https://en.wikipedia.org/wiki/Trigonometric_CORDIC",
      "text": "#redirect [[CORDIC#rotation mode]] {{R to related topic}}\n\n[[Category:Digit-by-digit algorithms]]\n[[Category:Shift-and-add algorithms]]"
    },
    {
      "title": "Wang's factor combining",
      "url": "https://en.wikipedia.org/wiki/Wang%27s_factor_combining",
      "text": "#redirect [[CORDIC#Factor combining]] {{R to related topic}}\n\n<!-- [[Category:Numerical analysis]] -->\n[[Category:Digit-by-digit algorithms]]\n[[Category:Shift-and-add algorithms]]"
    },
    {
      "title": "Bit-reversal permutation",
      "url": "https://en.wikipedia.org/wiki/Bit-reversal_permutation",
      "text": "In applied mathematics, a '''bit-reversal permutation''' is a [[permutation]] of a [[sequence]] of ''n'' items, where ''n''&nbsp;=&nbsp;2<sup>''k''</sup> is a [[power of two]]. It is defined by indexing the elements of the sequence by the numbers from 0 to ''n''&nbsp;&minus;&nbsp;1 and then reversing the [[binary representation]]s of each of these numbers (padded so that each of these binary numbers has length exactly&nbsp;''k''). Each item is then mapped to the new position given by this reversed value. The bit reversal permutation is an [[Involution (mathematics)|involution]], so repeating the same permutation twice returns to the original ordering on the items.\n\n==Example==\nConsider the sequence of eight letters ''abcdefgh''. Their indexes are the binary numbers 000, 001, 010, 011, 100, 101, 110, and 111, which when reversed become 000, 100, 010, 110, 001, 101, 011, and 111.\nThus, the letter ''a'' in position 000 is mapped to the same position (000), the letter ''b'' in position 001 is mapped to the fifth position (the one numbered 100), etc., giving the new sequence ''aecgbfdh''. Repeating the same permutation on this new sequence returns to the starting sequence.\n\nWriting the index numbers in decimal (but, as above, starting with position 0 rather than the more conventional start of 1 for a permutation), the bit-reversal permutations of size 2<sup>''k''</sup>, for ''k''&nbsp;=&nbsp;0, 1, 2, 3, ... are\n* k = 0: 0\n* k = 1:  0 1\n* k = 2:  0 2 1 3\n* k = 3:  0 4 2 6 1 5 3 7\n* k = 4:  0 8 4 12 2 10 6 14 1 9 5 13 3 11 7 15\n{{OEIS|A030109}}<br>\nEach permutation in this sequence can be generated by concatenating two sequences of numbers: the previous permutation, doubled, and the same sequence with each value increased by one.\nThus, for example doubling the length-4 permutation {{nowrap|0 2 1 3}} gives {{nowrap|0 4 2 6}}, adding one gives {{nowrap|1 5 3 7}}, and concatenating these two sequences gives the length-8 permutation {{nowrap|0 4 2 6 1 5 3 7}}.\n\n==Generalizations==\nThe generalization to ''n''&nbsp;=&nbsp;''b''<sup>''m''</sup> for an arbitrary integer ''b''&nbsp;>&nbsp;1 is a [[radix|base]]-''b'' '''digit-reversal permutation''', in which the base-''b'' (radix-''b'') digits of the index of each element are reversed to obtain the permuted index.\nA further generalization to arbitrary [[composite number|composite]] sizes is a '''[[mixed-radix]] digit reversal''' (in which the elements of the sequence are indexed by a number expressed in a mixed radix, whose digits are reversed by the permutation).\n\nPermutations that generalize the bit-reversal permutation by reversing contiguous blocks of bits within the binary representations of their indices can be used to interleave two equal-length sequences of data in-place.<ref>{{citation\n | last1 = Yang | first1 = Qingxuan\n | last2 = Ellis | first2 = John\n | last3 = Mamakani | first3 = Khalegh\n | last4 = Ruskey | first4 = Frank\n | doi = 10.1016/j.ipl.2013.02.017\n | issue = 10–11\n | journal = Information Processing Letters\n | mr = 3037467\n | pages = 386–391\n | title = In-place permuting and perfect shuffling using involutions\n | volume = 113\n | year = 2013}}.</ref>\n\nThere are two extensions of the bit-reversal permutation to sequences of arbitrary length.  These extensions coincide with bit-reversal for sequences whose length is a power of 2, and their purpose is to separate adjacent items in a sequence for the efficient operation of the [[Kaczmarz method|Kaczmarz algorithm]].  The first of these extensions, called ''Efficient Ordering''<ref>{{cite book|last1=Herman|first1=Gabor T.|author1-link=Gabor Herman|title=Fundamentals of Computerized Tomography|date=2009|publisher=Springer|location=London|isbn=978-1-85233-617-2|page=209|edition=2nd}}</ref>, operates on composite numbers, and it is based on decomposing the number into its prime components. \n\nThe second extension, called EBR (Extended Bit-Reversal)<ref>{{cite journal|last1=Gordon|first1=Dan|title=A derandomization approach to recovering bandlimited signals across a wide range of random sampling rates|journal=Numerical Algorithms|volume=77|issue=4|pages=1141–1157|date=June 2017|doi=10.1007/s11075-017-0356-3}}</ref>, is similar in spirit to bit-reversal.  Given an array of size ''n'', EBR fills the array with a permutation of the numbers in the range <math>0\\ldots n-1</math> in linear time.  Successive numbers are separated in the permutation by at least <math>\\lfloor n/4\\rfloor</math> positions.\n\n==Applications==\nBit reversal is most important for radix-2 [[Cooley–Tukey FFT algorithm]]s, where the recursive stages of the algorithm, operating [[in-place]], imply a bit reversal of the inputs or outputs.  Similarly, mixed-radix digit reversals arise in mixed-radix Cooley–Tukey FFTs.<ref>B. Gold and C. M. Rader, ''Digital Processing of Signals'' (New York: McGraw–Hill, 1969).</ref>\n\nThe bit reversal permutation has also been used to devise [[lower bound]]s in distributed computation.<ref>{{citation\n | last1 = Frederickson | first1 = Greg N.\n | last2 = Lynch | first2 = Nancy A. | author2-link = Nancy Lynch\n | contribution = The impact of synchronous communication on the problem of electing a leader in a ring\n | doi = 10.1145/800057.808719\n | pages = 493–503\n | title = Proceedings of the Sixteenth Annual ACM Symposium on Theory of Computing (STOC '84)\n | contribution-url = http://groups.csail.mit.edu/tds/papers/Lynch/stoc84.pdf\n | year = 1984| title-link = Symposium on Theory of Computing\n | isbn = 978-0897911337\n }}.</ref>\n\nThe [[Van der Corput sequence]], a [[low-discrepancy sequence]] of numbers in the [[unit interval]], is formed by reinterpreting the indexes of the bit-reversal permutation as the [[Fixed-point arithmetic|fixed-point binary representations]] of [[dyadic rational number]]s.\n\n==Algorithms==\nMainly because of the importance of [[fast Fourier transform]] algorithms, numerous efficient algorithms for applying a bit-reversal permutation to a sequence have been devised.<ref name=\"karp\">{{citation\n | last = Karp | first = Alan H.\n | doi = 10.1137/1038001\n | issue = 1\n | journal = SIAM Review\n | mr = 1379039\n | pages = 1–26\n | title = Bit reversal on uniprocessors\n | volume = 38\n | year = 1996| citeseerx = 10.1.1.24.2913\n }}. Karp surveys and compares 30 different algorithms for bit reversal, developed between 1965 and the 1996 publication of his survey.</ref>\n\nBecause the bit-reversal permutation is an involution, it may be performed easily [[In-place algorithm|in place]] (without copying the data into another array) by swapping pairs of elements. In the [[random-access machine]] commonly used in algorithm analysis, a simple algorithm that scans the indexes in input order and swaps whenever the scan encounters an index whose reversal is a larger number would perform a linear number of data moves.<ref name=\"cg98\"/> However, computing the reversal of each index may take a non-constant number of steps. Alternative algorithms can perform a bit reversal permutation in linear time while using only simple index calculations.<ref>{{citation\n | last1 = Jeong | first1 = Jechang\n | last2 = Williams | first2 = W.J.\n | contribution = A fast recursive bit-reversal algorithm\n | doi = 10.1109/ICASSP.1990.115695\n | pages = 1511–1514\n | title = International Conference on Acoustics, Speech, and Signal Processing (ICASSP-90)\n | volume = 3\n | year = 1990}}.</ref>\n\nAnother consideration that is even more important for the performance of these algorithms is the effect of the [[memory hierarchy]] on running time. Because of this effect, more sophisticated algorithms that consider the block structure of memory can be faster than this naive scan.<ref name=\"karp\"/><ref name=\"cg98\">{{citation|last1=Carter|first1=Larry|first2=Kang Su|last2=Gatlin|contribution=Towards an optimal bit-reversal permutation program|title=Proceedings of the 39th Annual Symposium on Foundations of Computer Science (FOCS)|pages=544–553|year=1998|doi=10.1109/SFCS.1998.743505|title-link=Symposium on Foundations of Computer Science|isbn=978-0-8186-9172-0|citeseerx=10.1.1.46.9319}}.</ref> An alternative to these techniques is special computer hardware that allows memory to be accessed both in normal and in bit-reversed order.<ref>{{citation\n | last1 = Harley | first1 = T. R.\n | last2 = Maheshwaramurthy | first2 = G. P.\n | doi = 10.1109/TSP.2004.827148\n | issue = 6\n | journal = IEEE Transactions on Signal Processing\n | pages = 1693–1703\n | title = Address generators for mapping arrays in bit-reversed order\n | volume = 52\n | year = 2004}}.</ref>\n\n==References==\n\n<references/>\n\n[[Category:Permutations]]\n[[Category:FFT algorithms]]\n[[Category:Combinatorial algorithms]]"
    },
    {
      "title": "Bruun's FFT algorithm",
      "url": "https://en.wikipedia.org/wiki/Bruun%27s_FFT_algorithm",
      "text": "'''Bruun's algorithm''' is a [[fast Fourier transform]] (FFT) algorithm based on an unusual recursive [[polynomial]]-factorization approach, proposed for powers of two by G. Bruun in 1978 and generalized to arbitrary even composite sizes by H. Murakami in 1996. Because its operations involve only real coefficients until the last computation stage, it was initially proposed as a way to efficiently compute the [[discrete Fourier transform]] (DFT) of real data. Bruun's algorithm has not seen widespread use, however, as approaches based on the ordinary [[Cooley–Tukey FFT algorithm]] have been successfully adapted to real data with at least as much efficiency. Furthermore, there is evidence that Bruun's algorithm may be intrinsically less accurate than Cooley–Tukey in the face of finite numerical precision  (Storn, 1993).\n\nNevertheless, Bruun's algorithm illustrates an alternative algorithmic framework that can express both itself and the Cooley–Tukey algorithm, and thus provides an interesting perspective on FFTs that permits mixtures of the two algorithms and other generalizations.\n\n== A polynomial approach to the DFT ==\n\nRecall that the DFT is defined by the formula:\n\n:<math>X_k =  \\sum_{n=0}^{N-1} x_n e^{-\\frac{2\\pi i}{N} nk }\n\\qquad\nk = 0,\\dots,N-1. </math>\n\nFor convenience, let us denote the ''N'' [[root of unity|roots of unity]] by ω<sub>''N''</sub><sup>''n''</sup> (''n''&nbsp;=&nbsp;0,&nbsp;...,&nbsp;''N''&nbsp;&minus;&nbsp;1):\n\n:<math>\\omega_N^n = e^{-\\frac{2\\pi i}{N} n }</math>\n\nand define the polynomial ''x''(''z'') whose coefficients are ''x''<sub>''n''</sub>:\n\n:<math>x(z) = \\sum_{n=0}^{N-1} x_n z^n.</math>\n\nThe DFT can then be understood as a ''reduction'' of this polynomial; that is, ''X''<sub>''k''</sub> is given by:\n\n:<math>X_k = x(\\omega_N^k) = x(z) \\mod (z - \\omega_N^k)</math>\n\nwhere '''mod''' denotes the [[Polynomial remainder theorem|polynomial remainder]] operation. The key to fast algorithms like Bruun's or Cooley–Tukey comes from the fact that one can perform this set of ''N'' remainder operations in recursive stages.\n\n== Recursive factorizations and FFTs ==\n\nIn order to compute the DFT, we need to evaluate the remainder of <math>x(z)</math> modulo ''N'' degree-1 polynomials as described above. Evaluating these remainders one by one is equivalent to the evaluating the usual DFT formula directly, and requires O(''N''<sup>2</sup>) operations.  However, one can ''combine'' these remainders recursively to reduce the cost, using the following trick: if we want to evaluate <math>x(z)</math> modulo two polynomials <math>U(z)</math> and <math>V(z)</math>, we can first take the remainder modulo their product <math>U(z)</math> <math>V(z)</math>, which reduces the [[Degree of a polynomial|degree]] of the polynomial <math>x(z)</math> and makes subsequent modulo operations less computationally expensive.\n\nThe product of all of the monomials <math>(z - \\omega_N^k)</math> for ''k''=0..''N''-1 is simply <math>z^N-1</math> (whose roots are clearly the ''N'' roots of unity). One then wishes to find a recursive factorization of <math>z^N-1</math> into polynomials of few terms and smaller and smaller degree. To compute the DFT, one takes <math>x(z)</math> modulo each level of this factorization in turn, recursively, until one arrives at the monomials and the final result. If each level of the factorization splits every polynomial into an O(1) (constant-bounded) number of smaller polynomials, each with an O(1) number of nonzero coefficients, then the modulo operations for that level take O(''N'') time; since there will be a logarithmic number of levels, the overall complexity is O (''N'' log ''N'').\n\nMore explicitly, suppose for example that <math>z^N-1 = F_1(z) F_2(z) F_3(z)</math>, and that <math>F_k(z) = F_{k,1}(z) F_{k,2}(z)</math>, and so on.  The corresponding FFT algorithm would consist of first computing ''x''<sub>''k''</sub>(''z'') = ''x''(''z'') mod\n''F''<sub>''k''</sub>(''z''), then computing ''x''<sub>''k'',''j''</sub>(''z'') = ''x''<sub>''k''</sub>(''z'') mod\n''F''<sub>''k'',''j''</sub>(''z''), and so on, recursively creating more and more remainder polynomials of smaller and smaller degree until one arrives at the final degree-0 results.\n\nMoreover, as long as the polynomial factors at each stage are [[relatively prime polynomials|relatively prime]] (which for polynomials means that they have no common roots), one can construct a dual algorithm by reversing the process with the [[Chinese Remainder Theorem]].\n\n===Cooley–Tukey as polynomial factorization===\n\nThe standard decimation-in-frequency (DIF) radix-''r'' Cooley–Tukey algorithm corresponds closely to a recursive factorization.  For example, radix-2 DIF Cooley–Tukey factors <math>z^N-1</math> into <math>F_1 = (z^{N/2}-1)</math> and <math>F_2 = (z^{N/2}+1)</math>.  These modulo operations reduce the degree of <math>x(z)</math> by 2, which corresponds to dividing the problem size by 2.  Instead of recursively factorizing <math>F_2</math>  directly, though, Cooley–Tukey instead first computes ''x''<sub>2</sub>(''z'' ω<sub>''N''</sub>), shifting all the roots (by a ''twiddle factor'') so that it can apply the recursive factorization of <math>F_1</math> to both subproblems.  That is, Cooley–Tukey ensures that all subproblems are also DFTs, whereas this is not generally true for an arbitrary recursive factorization (such as Bruun's, below).\n\n== The Bruun factorization ==\n\nThe basic Bruun algorithm for [[power of two|powers of two]] ''N''=''2''<sup>''n''</sup> factorizes ''z''<sup>''2''<sup>''n''</sup></sup>-''1'' recursively via the rules:\n\n:<math>z^{2M}-1 = (z^M - 1) (z^M + 1) \\,</math>\n:<math>z^{4M} + az^{2M} + 1 = (z^{2M} + \\sqrt{2-a}z^M+1) (z^{2M} - \\sqrt{2-a}z^M + 1)</math>\n\nwhere ''a'' is a real constant with |''a''| ≤ 2. If <math>a=2\\cos(\\phi)</math>, <math>\\phi\\in(0,\\pi)</math>, then <math>\\sqrt{2+a}=2\\cos\\tfrac\\phi2</math> and <math>\\sqrt{2-a}=2\\cos(\\tfrac\\pi 2-\\tfrac\\phi 2)</math>.\n\nAt stage ''s'', ''s''=0,1,2,''n''-1, the intermediate state consists of ''2''<sup>''s''</sup> polynomials <math>p_{s,0},\\dots,p_{s,2^s-1}</math> of degree ''2''<sup>''n''-''s''</sup> - ''1'' or less , where\n:<math>\\begin{align}\np_{s,0}(z)&= p(z) \\mod \\left(z^{2^{n-s}}-1\\right)&\\quad&\\text{and}\\\\\np_{s,m}(z) &= p(z)\\mod \\left(z^{2^{n-s}}-2\\cos\\left(\\tfrac{m}{2^s}\\pi\\right)z^{2^{n-1-s}}+1\\right)&m&=1,2,\\dots,2^s-1\n\\end{align}</math>\n\nBy the construction of the factorization of ''z''<sup>''2''<sup>''n''</sup></sup>-''1'', the polynomials ''p''<sub>''s'',''m''</sub>(''z'') each encode  2<sup>''n''-''s''</sup> values\n:<math>X_k=p(e^{2\\pi i\\tfrac{k}{2^n}})</math>\nof the Fourier transform, for ''m''=0, the covered indices are ''k''=''0'', 2<sup>''k''</sup>, 2∙2<sup>''s''</sup>, 3∙2<sup>''s''</sup>,…, (2<sup>''n''-''s''</sup>-1)∙2<sup>''s''</sup>, for ''m''>''0'' the covered indices are ''k''=''m'', 2<sup>''s''+1</sup>-''m'', 2<sup>''s''+1</sup>+''m'', 2∙2<sup>''s''+1</sup>-''m'', 2∙2<sup>''s''+1</sup>+''m'', …, 2<sup>''n''</sup>-''m''.\n\nDuring the transition to the next stage, the polynomial <math>p_{s,\\ell}(z)</math> is reduced to the polynomials <math>p_{s+1,\\ell}(z)</math> and <math>p_{s+1,2^s-\\ell}(z)</math> via polynomial division. If one wants to keep the polynomials in increasing index order, this pattern requires an implementation with two arrays. An implementation in place produces a predictable, but highly unordered sequence of indices, for example for ''N''=''16'' the final order of the ''8'' linear remainders is (''0'', ''4'', ''2'', ''6'', ''1'', ''7'', ''3'', ''5'').\n\nAt the end of the recursion, for ''s''=''n''-''1'', there remain 2<sup>''n''-''1''</sup> linear polynomials encoding two Fourier coefficients ''X''<sub>''0''</sub> and ''X''<sub>''2''<sup>''n''-1</sup></sub> for the first and for the any other ''k''th polynomial the coefficients ''X''<sub>''k''</sub> and ''X''<sub>2<sup>''n''</sup>-''k''</sub>.\n\nAt each recursive stage, all of the polynomials of the common degree ''4M''-''1'' are reduced to two parts of half the degree ''2M''-''1''. The divisor of this polynomial remainder computation is a quadratic polynomial ''z''<sup>''m''</sup>, so that all reductions can be reduced to polynomial divisions of cubic by quadratic polynomials. There are ''N''/''2''=''2''<sup>''n''-''1''</sup> of these small divisions at each stage, leading to an O (''N'' log ''N'') algorithm for the FFT.\n\nMoreover, since all of these polynomials have purely real coefficients (until the very last stage), they automatically exploit the special case where the inputs ''x''<sub>''n''</sub> are purely real to save roughly a factor of two in computation and storage.  One can also take straightforward advantage of the case of real-symmetric data for computing the [[discrete cosine transform]] (Chen and Sorensen, 1992).\n\n=== Generalization to arbitrary radices ===\n\nThe Bruun factorization, and thus the Bruun FFT algorithm, was generalized to handle arbitrary ''even'' composite lengths, i.e. dividing the polynomial degree by an arbitrary ''radix'' (factor), as follows.  First, we define a set of polynomials φ<sub>''N'',α</sub>(''z'') for positive integers ''N'' and for α in <nowiki>[0,1)</nowiki> by:\n\n:<math>\\phi_{N, \\alpha}(z) =\n\\left\\{ \\begin{matrix}\nz^{2N} - 2 \\cos (2 \\pi \\alpha) z^N + 1  & \\mbox{if } 0 < \\alpha < 1 \\\\ \\\\\nz^{2N} - 1  & \\mbox{if } \\alpha = 0\n\\end{matrix} \\right.\n</math>\n\nNote that all of the polynomials that appear in the Bruun factorization above can be written in this form. The zeroes of these polynomials are <math>e^{2\\pi i ( \\pm\\alpha + k ) / N}</math> for <math>k=0,1,\\dots,N-1</math> in the <math>\\alpha \\neq 0</math> case, and <math>e^{2\\pi i k / 2N}</math> for <math>k=0,1,\\dots,2N-1</math> in the <math>\\alpha=0</math> case. Hence these polynomials can be recursively factorized for a factor (radix) ''r'' via:\n\n:<math>\\phi_{rM, \\alpha}(z) =\n\\left\\{ \\begin{array}{ll}\n\\prod_{\\ell=0}^{r-1} \\phi_{M,(\\alpha+\\ell)/r}  & \\mbox{if } 0 < \\alpha \\leq 0.5 \\\\ \\\\\n\\prod_{\\ell=0}^{r-1} \\phi_{M,(1-\\alpha+\\ell)/r}  & \\mbox{if } 0.5 < \\alpha < 1 \\\\ \\\\\n\\prod_{\\ell=0}^{r-1} \\phi_{M,\\ell/(2r)}  & \\mbox{if } \\alpha = 0\n\\end{array} \\right.\n</math>\n\n==References==\n\n* Georg Bruun, \"''z''-Transform DFT filters and FFTs,\" ''[[IEEE]] Trans. on Acoustics, Speech and Signal Processing'' (ASSP) '''26''' (1), 56-63 (1978).\n* H. J. Nussbaumer, ''Fast Fourier Transform and Convolution Algorithms'' (Springer-Verlag: Berlin, 1990).\n* Yuhang Wu, \"New FFT structures based on the Bruun algorithm,\" ''IEEE Trans. ASSP'' '''38''' (1), 188-191 (1990)\n* Jianping Chen and Henrik Sorensen, \"An efficient FFT algorithm for real-symmetric data,\" ''Proc. ICASSP'' '''5''', 17-20 (1992).\n* Rainer Storn, \"Some results in fixed point error analysis of the Bruun-FTT {{sic}} algorithm,\" ''IEEE Trans. Signal Process.'' '''41''' (7), 2371-2375 (1993).\n* Hideo Murakami, \"Real-valued decimation-in-time and decimation-in-frequency algorithms,\" ''IEEE Trans. Circuits Syst. II: Analog and Digital Sig. Proc.'' '''41''' (12), 808-816 (1994).\n* Hideo Murakami, \"Real-valued fast discrete Fourier transform and cyclic convolution algorithms of highly composite even length,\" ''Proc. [[ICASSP]]'' '''3''', 1311-1314 (1996).\n* Shashank Mittal, Md. Zafar Ali Khan, M. B. Srinivas, \"A Comparative Study of Different FFT Architectures for Software Defined Radio\", ''Lecture Notes in Computer Science'' '''4599''' (''Embedded Computer Systems: Architectures, Modeling, and Simulation''), 375-384 (2007).  Proc. 7th Intl. Workshop, SAMOS 2007 (Samos, Greece, July 16–19, 2007).\n\n[[Category:FFT algorithms]]"
    },
    {
      "title": "Butterfly diagram",
      "url": "https://en.wikipedia.org/wiki/Butterfly_diagram",
      "text": ":''This article is about butterfly diagrams in FFT algorithms; for the sunspot diagrams of the same name, see [[Solar cycle]].''\n\n[[Image:Butterfly-FFT.png|thumb|200px|right|[[Signal-flow graph]] connecting the inputs ''x'' (left) to the outputs ''y'' that depend on them (right) for a \"butterfly\" step of a radix-2 Cooley–Tukey FFT.  This diagram resembles a [[butterfly]] (as in the [[Morpho (butterfly)|morpho butterfly]] shown for comparison), hence the name, although in some countries it is also called the hourglass diagram.]] In the context of [[fast Fourier transform]] algorithms, a '''butterfly''' is a portion of the computation that combines the results of smaller [[discrete Fourier transform]]s (DFTs) into a larger DFT, or vice versa (breaking a larger DFT up into subtransforms).  The name \"butterfly\" comes from the shape of the data-flow diagram in the radix-2 case, as described below.<ref name=Oppenheim89>Alan V. Oppenheim, Ronald W. Schafer, and John R. Buck, ''Discrete-Time Signal Processing'', 2nd edition (Upper Saddle River, NJ: Prentice Hall, 1989)</ref> The earliest occurrence in print of the term is thought to be in a 1969 [[Massachusetts Institute of Technology|MIT]] technical report.<ref>{{Cite report |author=C. J. Weinstein |date=1969-11-21 |title=Quantization Effects in Digital Filters |url=http://www.dtic.mil/cgi-bin/GetTRDoc?AD=AD0706862 |publisher=[[MIT Lincoln Laboratory]] |page=42 |accessdate=2015-02-10 |quote=This computation, referred to as a 'butterfly' }}</ref><ref>{{cite web |url=http://mathoverflow.net/a/98804 |title=FFT and Butterfly Diagram|last= Cipra  |first= Barry A.  |author-link = Barry A. Cipra|date=2012-06-04 |website=mathoverflow.net |access-date=2015-02-10}}</ref> The same structure can also be found in the [[Viterbi algorithm]], used for finding the most likely sequence of hidden states.\n\nMost commonly, the term \"butterfly\" appears in the context of the [[Cooley–Tukey FFT algorithm]], which [[recursion|recursively]] breaks down a DFT of [[composite number|composite]] size ''n''&nbsp;=&nbsp;''rm'' into ''r'' smaller transforms of size ''m'' where ''r'' is the \"radix\" of the transform.  These smaller DFTs are then combined via size-''r'' butterflies, which themselves are DFTs of size ''r'' (performed ''m'' times on corresponding outputs of the sub-transforms) pre-multiplied by [[root of unity|roots of unity]] (known as [[twiddle factor]]s).  (This is the \"decimation in time\" case; one can also perform the steps in reverse, known as \"decimation in frequency\", where the butterflies come first and are post-multiplied by twiddle factors.  See also the [[Cooley–Tukey FFT]] article.)\n\n==Radix-2 butterfly diagram==\nIn the case of the radix-2 Cooley–Tukey algorithm, the butterfly is simply a DFT of size-2 that takes two inputs (''x''<sub>0</sub>,&nbsp;''x''<sub>1</sub>) (corresponding outputs of the two sub-transforms) and gives two outputs (''y''<sub>0</sub>,&nbsp;''y''<sub>1</sub>) by the formula (not including [[twiddle factor]]s):\n\n:<math>y_0 = x_0 + x_1 \\, </math>\n:<math>y_1 = x_0 - x_1. \\, </math>\n\nIf one draws the data-flow diagram for this pair of operations, the (''x''<sub>0</sub>,&nbsp;''x''<sub>1</sub>) to (''y''<sub>0</sub>,&nbsp;''y''<sub>1</sub>) lines cross and resemble the wings of a [[butterfly]], hence the name (see also the illustration at right).\n\n[[Image:DIT-FFT-butterfly.png|thumb|300px|right|A decimation-in-time radix-2 FFT breaks a length-''N'' DFT into two length-''N''/2 DFTs followed by a combining stage consisting of many butterfly operations.]]\nMore specifically, a radix-2 decimation-in-time FFT algorithm on ''n''&nbsp;=&nbsp;2<sup>&nbsp;''p''</sup> inputs with respect to a primitive ''n''-th root of unity <math>\\omega^k_n = e^{-\\frac{2\\pi i k}{n}}</math> relies on O(''n''&nbsp;log&nbsp;''n'') butterflies of the form:\n\n:<math>y_0 = x_0 + x_1 \\omega^k_n \\, </math>\n:<math>y_1 = x_0 - x_1 \\omega^k_n, \\, </math>\n\nwhere ''k'' is an integer depending on the part of the transform being computed. Whereas the corresponding inverse transform can mathematically be performed by replacing ''&omega;'' with ''&omega;''<sup>&minus;1</sup> (and possibly multiplying by an overall scale factor, depending on the normalization convention), one may also directly invert the butterflies:\n\n:<math>x_0 = \\frac{1}{2} (y_0 + y_1) \\, </math>\n:<math>x_1 = \\frac{\\omega^{-k}_n}{2} (y_0 - y_1), \\, </math>\n\ncorresponding to a decimation-in-frequency FFT algorithm.\n\n==Other uses==\nThe butterfly can also be used to improve the randomness of large arrays of partially random numbers, by bringing every 32 or 64 bit word into causal contact with every other word through a desired hashing algorithm, so that a change in any one bit has the possibility of changing all the bits in the large array.<ref>*{{Citation | last1=Press | first1=WH | last2=Teukolsky | first2=SA | last3=Vetterling | first3=WT | last4=Flannery | first4=BP | year=2007 | title=Numerical Recipes: The Art of Scientific Computing | edition=3rd | publisher=Cambridge University Press |  publication-place=New York | isbn=978-0-521-88068-8 | chapter=Section 7.2 Completely Hashing a Large Array | chapter-url=http://apps.nrbook.com/empanel/index.html#pg=358}}</ref>\n\n== See also ==\n* [[Mathematical diagram]]\n* [[Zassenhaus lemma]]\n* [[Signal-flow graph]]\n\n== References ==\n<references/>\n\n==External links==\n* [https://web.archive.org/web/20060423170713/http://www.relisoft.com/science/Physics/fft.html explanation of the FFT and butterfly diagrams].\n* [http://www.cmlab.csie.ntu.edu.tw/cml/dsp/training/coding/transform/fft.html butterfly diagrams of various FFT implementations (Radix-2, Radix-4, Split-Radix)].\n<!-- dead link * [http://astron.berkeley.edu/~jrg/ngst/fft/fftbutfy.html explanation of butterfly diagrams specifically].-->\n\n[[Category:FFT algorithms]]\n[[Category:Diagrams]]"
    },
    {
      "title": "Chirp Z-transform",
      "url": "https://en.wikipedia.org/wiki/Chirp_Z-transform",
      "text": "{{refimprove|date=January 2013}}\nThe '''chirp Z-transform''' ('''CZT''') is a generalization of the [[discrete Fourier transform]] (DFT).  While the DFT samples the [[z-transform|Z plane]] at uniformly-spaced points along the unit circle, the chirp Z-transform samples along spiral arcs in the Z-plane, corresponding to straight lines in the [[S plane]].<ref name=Shilling>[https://krex.k-state.edu/dspace/bitstream/handle/2097/7844/LD2668R41972S43.pdf A study of the Chirp Z-transform and its applications] - Shilling, Steve Alan</ref><ref>{{Cite web|url=http://www.mathworks.com/help/signal/ref/czt.html|title=Chirp Z-transform - MATLAB czt|website=www.mathworks.com|access-date=2016-09-22}}</ref>  The DFT, real DFT, and zoom DFT can be calculated as special cases of the CZT.\n\nSpecifically, the chirp Z transform calculates the Z transform at a finite number of points z<sub>k</sub> along a [[logarithmic spiral]] contour, defined as:<ref name=Shilling/><ref>{{Cite web|url=http://prod.sandia.gov/techlib/access-control.cgi/2005/057084.pdf|title=Chirp Z-Transform Spectral Zoom Optimization with MATLAB®|last=Martin|first=Grant D.|date=November 2005|website=|access-date=}}</ref>\n\n:<math>X_k = \\sum_{n=0}^{N-1} x(n) z_{k}^{-n} </math>\n\n:<math>z_k = A\\cdot W^{-k}, k=0,1,\\dots,M-1</math>\n\nwhere ''A'' is the complex starting point, ''W'' is the complex ratio between points, and ''M'' is the number of points to calculate.\n\n==Bluestein's algorithm==\n\n'''Bluestein's algorithm'''<ref>{{Cite journal|last=Bluestein|first=L.|date=1970-12-01|title=A linear filtering approach to the computation of discrete Fourier transform|journal=IEEE Transactions on Audio and Electroacoustics|volume=18|issue=4|pages=451–455|doi=10.1109/TAU.1970.1162132|issn=0018-9278}}</ref><ref>{{cite web|url=http://www.dsprelated.com/dspbooks/mdft/Bluestein_s_FFT_Algorithm.html |title=Bluestein's FFT Algorithm |publisher=DSPRelated.com}}</ref> expresses the CZT as a [[convolution]] and implements it efficiently using [[fast Fourier transform|FFT]]/IFFT.\n\nAs the DFT is a special case of the CZT, this allows the efficient calculation of [[discrete Fourier transform]] (DFT) of arbitrary sizes, including [[prime number|prime]] sizes.  (The other algorithm for FFTs of prime sizes, [[Rader's FFT algorithm|Rader's algorithm]], also works by rewriting the DFT as a convolution.) It was conceived in 1968 by [[Leo Bluestein]].<ref>{{cite journal| doi = 10.1109/TAU.1970.1162132 |title= A linear filtering approach to the computation of discrete Fourier transform |journal=IEEE Transactions on Audio and Electroacoustics |volume=18 |issue=4 |year=1970 |pages=451–455|last1= Bluestein |first1= L. }}</ref> Bluestein's algorithm can be used to compute more general transforms than the DFT, based on the (unilateral) [[z-transform]] (Rabiner ''et al.'', 1969).\n\nRecall that the DFT is defined by the formula\n\n:<math> X_k = \\sum_{n=0}^{N-1} x_n e^{-\\frac{2\\pi i}{N} nk }\n\\qquad\nk = 0,\\dots,N-1. </math>\n\nIf we replace the product ''nk'' in the exponent by the identity\n\n:<math>n k = \\frac{-(k-n)^2}{2} + \\frac{n^2}{2} + \\frac{k^2}{2}</math>\n\nwe thus obtain:\n\n:<math> X_k = e^{-\\frac{\\pi i}{N} k^2 } \\sum_{n=0}^{N-1} \\left( x_n e^{-\\frac{\\pi i}{N} n^2 } \\right) e^{\\frac{\\pi i}{N} (k-n)^2 }\n\\qquad\nk = 0,\\dots,N-1. </math>\n\nThis summation is precisely a  convolution of the two sequences ''a''<sub>''n''</sub> and ''b''<sub>''n''</sub> defined by:\n\n:<math>a_n = x_n e^{-\\frac{\\pi i}{N} n^2 }</math>\n:<math>b_n = e^{\\frac{\\pi i}{N} n^2 },</math>\n\nwith the output of the convolution multiplied by ''N'' phase factors ''b''<sub>''k''</sub><sup>*</sup>. That is:\n\n:<math>X_k = b_k^* \\sum_{n=0}^{N-1} a_n b_{k-n} \\qquad k = 0,\\dots,N-1. </math>\n\nThis convolution, in turn, can be performed with a pair of FFTs (plus the pre-computed FFT of complex [[chirp]] ''b''<sub>''n''</sub>) via the [[convolution theorem]].  The key point is that these FFTs are not of the same length ''N'': such a convolution can be computed exactly from FFTs only by zero-padding it to a length greater than or equal to 2''N''&ndash;1. In particular, one can pad to a [[power of two]] or some other [[smooth number|highly composite]] size, for which the FFT can be efficiently performed by e.g. the [[Cooley–Tukey FFT algorithm|Cooley–Tukey algorithm]] in O(''N'' log ''N'') time.  Thus, Bluestein's algorithm provides an O(''N'' log ''N'') way to compute prime-size DFTs, albeit several times slower than the Cooley–Tukey algorithm for composite sizes.\n\nThe use of zero-padding for the convolution in Bluestein's algorithm deserves some additional comment. Suppose we zero-pad to a length ''M'' &ge; 2''N''&ndash;1. This means that ''a''<sub>''n''</sub> is extended to an array ''A''<sub>''n''</sub> of length ''M'', where ''A''<sub>''n''</sub> = ''a''<sub>''n''</sub> for 0 &le; ''n'' &lt; ''N'' and ''A''<sub>''n''</sub> = 0 otherwise&mdash;the usual meaning of \"zero-padding\".  However, because of the ''b''<sub>''k''&ndash;''n''</sub> term in the convolution, both positive and ''negative'' values of ''n'' are required for ''b''<sub>''n''</sub> (noting that ''b''<sub>&ndash;''n''</sub> = ''b''<sub>''n''</sub>). The periodic boundaries implied by the DFT of the zero-padded array mean that &ndash;''n'' is equivalent to ''M''&ndash;''n''. Thus, ''b''<sub>''n''</sub> is extended to an array ''B''<sub>''n''</sub> of length ''M'', where ''B''<sub>0</sub> = ''b''<sub>0</sub>, ''B''<sub>''n''</sub> = ''B''<sub>''M''&ndash;''n''</sub> = ''b''<sub>''n''</sub> for 0 &lt; ''n'' &lt; ''N'', and ''B''<sub>''n''</sub> = 0 otherwise.  ''A'' and ''B'' are then FFTed, multiplied pointwise, and inverse FFTed to obtain the  convolution of ''a'' and ''b'', according to the usual convolution theorem.\n\nLet us also be more precise about what type of convolution is required in Bluestein's algorithm for the DFT.  If the sequence ''b''<sub>''n''</sub> were periodic in ''n'' with period ''N'', then it would be a cyclic convolution of length ''N'', and the zero-padding would be for computational convenience only.  However, this is not generally the case:\n:<math>b_{n+N} = e^{\\frac{\\pi i}{N} (n+N)^2 } = b_n e^{\\frac{\\pi i}{N} (2Nn+N^2) } = (-1)^N b_n .</math>\nTherefore, for ''N'' [[even and odd numbers|even]] the convolution is cyclic, but in this case ''N'' is [[composite number|composite]] and one would normally use a more efficient FFT algorithm such as Cooley–Tukey.  For ''N'' odd, however, then ''b''<sub>''n''</sub> is [[antiperiodic function|antiperiodic]] and we technically have a [[negacyclic convolution]] of length ''N''.   Such distinctions disappear when one zero-pads ''a''<sub>''n''</sub> to a length of at least 2''N''&minus;1 as described above, however.  It is perhaps easiest, therefore, to think of it as a subset of the outputs of a simple linear convolution (i.e. no conceptual \"extensions\" of the data, periodic or otherwise).\n\n==  z-transforms ==\n\nBluestein's algorithm can also be used to compute a more general transform based on the (unilateral) [[z-transform]] (Rabiner ''et al.'', 1969).  In particular, it can compute any transform of the form:\n\n:<math> X_k = \\sum_{n=0}^{N-1} x_n z^{nk}\n\\qquad\nk = 0,\\dots,M-1, </math>\n\nfor an ''arbitrary'' [[complex number]] ''z'' and for ''differing'' numbers ''N'' and ''M'' of inputs and outputs.  Given Bluestein's algorithm, such a transform can be used, for example, to obtain a more finely spaced interpolation of some portion of the spectrum (although the frequency resolution is still limited by the total sampling time, similar to a Zoom FFT), enhance arbitrary poles in transfer-function analyses, etc.\n\nThe algorithm was dubbed the ''chirp'' z-transform algorithm because, for the Fourier-transform case (|''z''| = 1), the sequence ''b''<sub>''n''</sub> from above is a complex sinusoid of linearly increasing frequency, which is called a (linear) [[chirp]] in [[radar]] systems.\n\n==See also==\n*[[Fractional Fourier Transform]]\n\n==References==\n{{reflist}}\n\n===General===\n* Leo I. Bluestein, \"A linear filtering approach to the computation of the discrete Fourier transform,\" ''Northeast Electronics Research and Engineering Meeting Record'' '''10''', 218-219 (1968).\n* Lawrence R. Rabiner, Ronald W. Schafer, and Charles M. Rader, \"[https://web.archive.org/web/20130618204835/http://www3.alcatel-lucent.com/bstj/vol48-1969/articles/bstj48-5-1249.pdf The chirp z-transform algorithm and its application],\" ''Bell Syst. Tech. J.'' '''48''', 1249-1292 (1969).  Also published in: Rabiner, Shafer, and Rader, \"[https://web.archive.org/web/20150220065735/http://cronos.rutgers.edu/~lrr/Reprints/015_czt.pdf The chirp z-transform algorithm],\" ''IEEE Trans. Audio Electroacoustics'' '''17''' (2), 86&ndash;92 (1969).\n* D. H. Bailey and P. N. Swarztrauber, \"The fractional Fourier transform and applications,\" ''[[SIAM Review]]'' '''33''', 389-404 (1991).  (Note that this terminology for the z-transform is nonstandard: a [[fractional Fourier transform]] conventionally refers to an entirely different, continuous transform.)\n* Lawrence Rabiner, \"The chirp z-transform algorithm&mdash;a lesson in serendipity,\" ''IEEE Signal Processing Magazine'' '''21''', 118-119 (March 2004).  (Historical commentary.)\n\n==External links==\n* [http://www.embedded.com/design/configurable-systems/4006427/A-DSP-algorithm-for-frequency-analysis A DSP algorithm for frequency analysis] - the Chirp-Z Transform (CZT)\n\n[[Category:FFT algorithms]]"
    },
    {
      "title": "Cooley–Tukey FFT algorithm",
      "url": "https://en.wikipedia.org/wiki/Cooley%E2%80%93Tukey_FFT_algorithm",
      "text": "{{Use American English|date = March 2019}}\n{{Short description|Fast Fourier Transform algorithm}}\nThe '''Cooley–Tukey algorithm''', named after [[James Cooley|J. W. Cooley]] and [[John Tukey]], is the most common [[fast Fourier transform]] (FFT) algorithm.  It re-expresses the [[discrete Fourier transform]] (DFT) of an arbitrary [[composite number|composite]] size ''N'' = ''N''<sub>1</sub>''N''<sub>2</sub> in terms of ''N''<sub>1</sub> smaller DFTs of sizes ''N''<sub>2</sub>, [[recursion|recursively]], to reduce the computation time to O(''N'' log ''N'') for highly composite ''N'' ([[smooth number]]s). Because of the algorithm's importance, specific variants and implementation styles have become known by their own names, as described below.\n\nBecause the Cooley–Tukey algorithm breaks the DFT into smaller DFTs, it can be combined arbitrarily with any other algorithm for the DFT.  For example, [[Rader's FFT algorithm|Rader's]] or [[Bluestein's FFT algorithm|Bluestein's]] algorithm can be used to handle large prime factors that cannot be decomposed by Cooley–Tukey, or the [[prime-factor FFT algorithm|prime-factor algorithm]] can be exploited for greater efficiency in separating out [[relatively prime]] factors.\n\nThe algorithm, along with its recursive application, was invented by [[Carl Friedrich Gauss]]. Cooley and Tukey independently rediscovered and popularized it 160 years later.\n\n== History ==\nThis algorithm, including its recursive application, was invented around 1805 by [[Carl Friedrich Gauss]], who used it to interpolate the trajectories of the [[asteroid]]s [[2 Pallas|Pallas]] and [[3 Juno|Juno]], but his work was not widely recognized (being published only posthumously and in [[New Latin|neo-Latin]]).<ref>{{cite book |last1=Gauss |first1=Carl Friedrich |date=1876| orig-year=n.d. | title= Theoria Interpolationis Methodo Nova Tractata | work=Carl Friedrich Gauss Werke | volume=Band 3 | location=Göttingen | publisher=Königliche Gesellschaft der Wissenschaften |pages=265–327 |url=https://archive.org/stream/werkecarlf03gausrich#page/n277/mode/2up}}</ref><ref name=Heideman84>Heideman, M. T., D. H. Johnson, and [[C. Sidney Burrus|C. S. Burrus]], \"[http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1162257 Gauss and the history of the fast Fourier transform],\" IEEE ASSP Magazine, 1, (4), 14–21 (1984)</ref> Gauss did not analyze the asymptotic computational time, however. Various limited forms were also rediscovered several times throughout the 19th and early 20th centuries.<ref name=Heideman84/>  FFTs became popular after [[James Cooley]] of [[International Business Machines|IBM]] and [[John Tukey]] of [[Princeton University|Princeton]] published a paper in 1965 reinventing the algorithm and describing how to perform it conveniently on a computer.<ref name=CooleyTukey65>{{cite journal |last=Cooley |first=James W. |first2=John W. |last2=Tukey |title=An algorithm for the machine calculation of complex Fourier series |journal=[[Mathematics of Computation|Math. Comput.]] |volume=19 |issue= 90|pages=297–301 |year=1965 |doi=10.2307/2003354 |jstor=2003354 }}</ref>\n\nTukey reportedly came up with the idea during a meeting of President Kennedy’s Science Advisory Committee discussing ways to detect [[nuclear testing|nuclear-weapon tests]] in the [[Soviet Union]] by employing seismometers located outside the country. These sensors would generate seismological time series.  However, analysis of this data would require fast algorithms for computing DFT due to number of sensors and length of time. This task was critical for the ratification of the proposed nuclear test ban so that any violations could be detected without need to visit Soviet facilities.<ref>{{cite journal |last=Cooley |first=James W. |first2=Peter A. W. |last2=Lewis |first3=Peter D. |last3=Welch |title=Historical notes on the fast Fourier transform |journal=IEEE Transactions on Audio and Electroacoustics |volume=15 |issue=2 |pages=76–79 |year=1967 |doi=10.1109/tau.1967.1161903 |url=http://www.ece.ucdavis.edu/~bbaas/281/papers/CooleyLewisWelch.1967.HistNotesFFT.pdf|citeseerx=10.1.1.467.7209 }}</ref><ref>Rockmore, Daniel N. , ''Comput. Sci. Eng.'' '''2''' (1), 60 (2000). [http://www.cs.dartmouth.edu/~rockmore/cse-fft.pdf The FFT — an algorithm the whole family can use] Special issue on \"top ten algorithms of the century \"{{cite web |url=http://amath.colorado.edu/resources/archive/topten.pdf |title=Archived copy |accessdate=2009-03-31 |deadurl=yes |archiveurl=https://web.archive.org/web/20090407080904/http://amath.colorado.edu/resources/archive/topten.pdf |archivedate=2009-04-07 |df= }}</ref>  Another participant at that meeting, [[Richard Garwin]] of IBM, recognized the potential of the method and put Tukey in touch with Cooley however making sure that Cooley did not know the original purpose. Instead Cooley was told that this was needed to determine periodicities of the spin orientations in a 3-D crystal of [[Helium-3]].  Cooley and Tukey subsequently published their joint paper, and wide adoption quickly followed due to the simultaneous development of [[Analog-to-digital converter]]s capable of sampling at rates up to 300&nbsp;kHz.\n\nThe fact that Gauss had described the same algorithm (albeit without analyzing its asymptotic cost) was not realized until several years after Cooley and Tukey's 1965 paper.<ref name=Heideman84/>  Their paper cited as inspiration only the work by [[I. J. Good]] on what is now called the [[prime-factor FFT algorithm]] (PFA);<ref name=CooleyTukey65/> although Good's algorithm was initially thought to be equivalent to the Cooley–Tukey algorithm, it was quickly realized that PFA is a quite different algorithm (only working for sizes that have [[relatively prime]] factors and relying on the [[Chinese Remainder Theorem]], unlike the support for any composite size in Cooley–Tukey).<ref>James W. Cooley, Peter A. W. Lewis, and Peter W. Welch, \"Historical notes on the fast Fourier transform,\" ''Proc. IEEE'', vol. '''55''' (no. 10), p. 1675–1677 (1967).</ref>\n\n== The radix-2 DIT case ==\nA '''radix-2''' decimation-in-time ('''DIT''') FFT is the simplest and most common form of the Cooley–Tukey algorithm, although highly optimized Cooley–Tukey implementations typically use other forms of the algorithm as described below. Radix-2 DIT divides a DFT of size ''N'' into two interleaved DFTs (hence the name \"radix-2\") of size ''N''/2 with each recursive stage.\n\nThe discrete Fourier transform (DFT) is defined by the formula:\n:<math>      X_k = \\sum_{n=0}^{N-1} x_n e^{-\\frac{2\\pi i}{N} nk},</math>\nwhere <math>k</math> is an integer ranging from <math>0</math> to <math>N-1</math>.\n\nRadix-2 DIT first computes the DFTs of the even-indexed inputs\n<math>(x_{2m}=x_0, x_2, \\ldots, x_{N-2})</math>\nand of the odd-indexed inputs <math>(x_{2m+1}=x_1, x_3, \\ldots, x_{N-1})</math>, and then combines those two results to produce the DFT of the whole sequence. This idea can then be performed [[recursion|recursively]] to reduce the overall runtime to O(''N'' log ''N'').  This simplified form assumes that ''N'' is a [[power of two]]; since the number of sample points ''N'' can usually be chosen freely by the application (e.g. by changing the sample rate or window, zero-padding, etcetera), this is often not an important restriction.\n\nThe radix-2 DIT algorithm rearranges the DFT of the function <math>x_n</math> into two parts: a sum over the even-numbered indices <math>n={2m}</math> and a sum over the odd-numbered indices <math>n={2m+1}</math>:\n\n:<math>\n  \\begin{matrix} X_k & =\n& \\sum \\limits_{m=0}^{N/2-1} x_{2m}e^{-\\frac{2\\pi i}{N} (2m)k}   +   \\sum \\limits_{m=0}^{N/2-1} x_{2m+1} e^{-\\frac{2\\pi i}{N} (2m+1)k}\n  \\end{matrix}\n</math>\n\nOne can factor a common multiplier <math>e^{-\\frac{2\\pi i}{N}k}</math> out of the second sum, as shown in the equation below. It is then clear that the two sums are the DFT of the even-indexed part <math>x_{2m}</math> and the DFT of odd-indexed part <math>x_{2m+1}</math> of the function <math>x_n</math>. Denote the DFT of the '''''E'''''ven-indexed inputs <math>x_{2m}</math> by <math>E_k</math> and the DFT of the '''''O'''''dd-indexed inputs <math>x_{2m + 1}</math> by <math>O_k</math> and we obtain:\n\n:<math>\n\\begin{matrix} X_k= \\underbrace{\\sum \\limits_{m=0}^{N/2-1} x_{2m}   e^{-\\frac{2\\pi i}{N/2} mk}}_{\\mathrm{DFT\\;of\\;even-indexed\\;part\\;of\\;} x_n} {} +  e^{-\\frac{2\\pi i}{N}k}\n \\underbrace{\\sum \\limits_{m=0}^{N/2-1} x_{2m+1} e^{-\\frac{2\\pi i}{N/2} mk}}_{\\mathrm{DFT\\;of\\;odd-indexed\\;part\\;of\\;} x_n} =  E_k + e^{-\\frac{2\\pi i}{N}k} O_k.\n\\end{matrix}\n</math>\n\nThanks to the [https://proofwiki.org/wiki/Periodicity_of_Complex_Exponential_Function periodicity of the complex exponential], <math>X_{k+\\frac{N}{2}}</math> is also obtained from <math>E_k</math> and <math>O_k</math>:\n\n:<math>\n\\begin{align} X_{k + \\frac{N}{2}} & =  \\sum \\limits_{m=0}^{N/2-1} x_{2m}   e^{-\\frac{2\\pi i}{N/2} m(k + \\frac{N}{2})} +  e^{-\\frac{2\\pi i}{N}(k + \\frac{N}{2})}\n \\sum \\limits_{m=0}^{N/2-1} x_{2m+1} e^{-\\frac{2\\pi i}{N/2} m(k + \\frac{N}{2} )} \\\\\n & =  \\sum \\limits_{m=0}^{N/2-1} x_{2m}   e^{-\\frac{2\\pi i}{N/2} mk} e^{-2\\pi m i} +  e^{-\\frac{2\\pi i}{N}k}e^{-\\pi i}\n \\sum \\limits_{m=0}^{N/2-1} x_{2m+1} e^{-\\frac{2\\pi i}{N/2} mk} e^{-2\\pi m i} \\\\\n & =  \\sum \\limits_{m=0}^{N/2-1} x_{2m}   e^{-\\frac{2\\pi i}{N/2} mk} - e^{-\\frac{2\\pi i}{N}k}\n \\sum \\limits_{m=0}^{N/2-1} x_{2m+1} e^{-\\frac{2\\pi i}{N/2} mk} \\\\\n& =  E_k - e^{-\\frac{2\\pi i}{N}k} O_k\n\\end{align}\n</math>\n\nWe can rewrite <math> X_k </math> as:\n:<math>\n\\begin{matrix}\nX_k & =\n& E_k + e^{-\\frac{2\\pi i}{N}k} O_k \\\\\nX_{k+\\frac{N}{2}} & =\n& E_k - e^{-\\frac{2\\pi i}{N}{k}} O_k\n\\end{matrix}\n</math>\n\nThis result, expressing the DFT of length ''N'' recursively in terms of two DFTs of size ''N''/2, is the core of the radix-2 DIT fast Fourier transform. The algorithm gains its speed by re-using the results of intermediate computations to compute multiple DFT outputs.  Note that final outputs are obtained by a +/− combination of <math>E_k</math> and <math>O_k \\exp(-2\\pi i k/N)</math>, which is simply a size-2 DFT (sometimes called a [[butterfly diagram|butterfly]] in this context); when this is generalized to larger radices below, the size-2 DFT is replaced by a larger DFT (which itself can be evaluated with an FFT).\n[[Image:DIT-FFT-butterfly.png|thumb|300px|right|Data flow diagram for ''N''=8: a decimation-in-time radix-2 FFT breaks a length-''N'' DFT into two length-''N''/2 DFTs followed by a combining stage consisting of many size-2 DFTs called \"butterfly\" operations (so-called because of the shape of the data-flow diagrams).]]\n\nThis process is an example of the general technique of [[divide and conquer algorithm]]s; in many traditional implementations, however, the explicit recursion is avoided, and instead one traverses the computational tree in [[breadth-first search|breadth-first]] fashion.\n\nThe above re-expression of a size-''N'' DFT as two size-''N''/2 DFTs is sometimes called the '''[[G. C. Danielson|Danielson]]–[[Cornelius Lanczos|Lanczos]]''' [[lemma (mathematics)|lemma]], since the identity was noted by those two authors in 1942<ref>Danielson, G. C., and C. Lanczos, \"Some improvements in practical Fourier analysis and their application to X-ray scattering from liquids,\" ''J. Franklin Inst.'' '''233''', 365–380 and 435–452 (1942).</ref> (influenced by [[Carl David Tolmé Runge|Runge's]] 1903 work<ref name=Heideman84/>).  They applied their lemma in a \"backwards\" recursive fashion, repeatedly ''doubling'' the DFT size until the transform spectrum converged (although they apparently didn't realize the [[linearithmic]] [i.e., order ''N''&nbsp;log&nbsp;''N''] asymptotic complexity they had achieved).  The Danielson–Lanczos work predated widespread availability of [[History of computing hardware|mechanical or electronic computer]]s and required [[Human computer|manual calculation]] (possibly with mechanical aids such as [[adding machine]]s); they reported a computation time of 140 minutes for a size-64 DFT operating on [[Fast Fourier transform#FFT algorithms specialized for real and/or symmetric data|real inputs]] to 3–5 significant digits.  Cooley and Tukey's 1965 paper reported a running time of 0.02 minutes for a size-2048 complex DFT on an [[IBM 7094]] (probably in 36-bit [[floating point|single precision]], ~8 digits).<ref name=CooleyTukey65/>  Rescaling the time by the number of operations, this corresponds roughly to a speedup factor of around 800,000.  (To put the time for the hand calculation in perspective, 140 minutes for size 64 corresponds to an average of at most 16 seconds per floating-point operation, around 20% of which are multiplications.)\n\n===Pseudocode===\n\nIn [[pseudocode]], the below procedure could be written:<ref name=\"Johnson08\"/>\n\n ''X''<sub>0,...,''N''−1</sub> ← '''ditfft2'''(''x'', ''N'', ''s''):             ''DFT of (x''<sub>0</sub>, ''x<sub>s</sub>'', ''x''<sub>2''s''</sub>, ..., ''x''<sub>(''N''-1)''s''</sub>):\n     if ''N'' = 1 then\n         ''X''<sub>0</sub> ← ''x''<sub>0</sub>                                      ''trivial size-1 DFT base case''\n     else\n         ''X''<sub>0,...,''N''/2−1</sub> ← '''ditfft2'''(''x'', ''N''/2, 2''s'')             ''DFT of (x''<sub>0</sub>, ''x''<sub>2''s''</sub>, ''x''<sub>4''s''</sub>, ...)\n         ''X<sub>N</sub>''<sub>/2,...,''N''−1</sub> ← '''ditfft2'''(''x''+s, ''N''/2, 2''s'')           ''DFT of (x<sub>s</sub>'', ''x<sub>s</sub>''<sub>+2''s''</sub>, ''x<sub>s</sub>''<sub>+4''s''</sub>, ...)\n         for ''k'' = 0 to ''N''/2−1                           ''combine DFTs of two halves into full DFT:''\n             t ← ''X<sub>k</sub>''\n             ''X<sub>k</sub>'' ← t + exp(−2π''i'' ''k''/''N'') ''X<sub>k</sub>''<sub>+''N''/2</sub>\n             ''X<sub>k</sub>''<sub>+''N''/2</sub> ← t − exp(−2π''i'' ''k''/''N'') ''X<sub>k</sub>''<sub>+''N''/2</sub>\n         endfor\n     endif\n\nHere, <code>'''ditfft2'''</code>(''x'',''N'',1), computes ''X''=DFT(''x'') [[In-place algorithm|out-of-place]] by a radix-2 DIT FFT, where ''N'' is an integer power of 2 and ''s''=1 is the [[stride of an array|stride]] of the input ''x'' [[Array data structure|array]].  ''x''+''s'' denotes the array starting with ''x<sub>s</sub>''.\n\n(The results are in the correct order in ''X'' and no further [[bit-reversal permutation]] is required; the often-mentioned necessity of a separate bit-reversal stage only arises for certain in-place algorithms, as described below.)\n\nHigh-performance FFT implementations make many modifications to the implementation of such an algorithm compared to this simple pseudocode.  For example, one can use a larger base case than ''N''=1 to [[amortize]] the overhead of recursion, the [[twiddle factor]]s <math>\\exp[-2\\pi i k/ N]</math> can be precomputed, and larger radices are often used for [[cache (computing)|cache]] reasons; these and other optimizations together can improve the performance by an order of magnitude or more.<ref name=\"Johnson08\">S. G. Johnson and M. Frigo, \"[http://cnx.org/content/m16336/ Implementing FFTs in practice],\" in ''Fast Fourier Transforms'' (C. S. Burrus, ed.), ch. 11, Rice University, Houston TX: Connexions, September 2008.</ref>  (In many textbook implementations the [[depth-first]] recursion is eliminated entirely in favor of a nonrecursive [[breadth-first]] approach, although depth-first recursion has been argued to have better [[memory locality]].<ref name=\"Johnson08\"/><ref name=Singleton67/>) Several of these ideas are described in further detail below.\n\n== Idea ==\n[[File:Cooley-tukey-general.png|thumb|right|300px|The basic step of the Cooley–Tukey FFT for general factorizations can be viewed as re-interpreting a 1d DFT as something like a 2d DFT. The 1d input array of length ''N'' = ''N''<sub>1</sub>''N''<sub>2</sub> is reinterpreted as a 2d ''N''<sub>1</sub>&times;''N''<sub>2</sub> matrix stored in [[column-major order]]. One performs smaller 1d DFTs along the ''N''<sub>2</sub> direction (the non-contiguous direction), then multiplies by phase factors (twiddle factors), and finally performs 1d DFTs along the ''N''<sub>1</sub> direction. The transposition step can be performed in the middle, as shown here, or at the beginning or end.  This is done recursively for the smaller transforms.]]\n[[File:Row_and_column_major_order.svg|thumb|upright|Illustration of row- and column-major order]]\n\nMore generally, Cooley–Tukey algorithms recursively re-express  a DFT of a composite size ''N'' = ''N''<sub>1</sub>''N''<sub>2</sub> as:<ref name=DuhamelVe90>Duhamel, P., and M. Vetterli, \"Fast Fourier transforms: a tutorial review and a state of the art,\" ''Signal Processing'' '''19''', 259–299 (1990)</ref>\n\n# Perform ''N''<sub>1</sub> DFTs of size ''N''<sub>2</sub>.\n# Multiply by complex [[roots of unity]] (often called the [[twiddle factor]]s).\n# Perform ''N''<sub>2</sub> DFTs of size ''N''<sub>1</sub>.\n\nTypically, either ''N''<sub>1</sub> or ''N''<sub>2</sub> is a small factor (''not'' necessarily prime), called the '''radix''' (which can differ between stages of the recursion).  If ''N''<sub>1</sub> is the radix, it is called a '''decimation in time''' (DIT) algorithm, whereas if ''N''<sub>2</sub> is the radix, it is '''decimation in frequency''' (DIF, also called the Sande–Tukey algorithm). The version presented above was a radix-2 DIT algorithm; in the final expression, the phase multiplying the odd transform is the twiddle factor, and the +/- combination (''butterfly'') of the even and odd transforms is a size-2 DFT.  (The radix's small DFT is sometimes known as a [[butterfly (FFT algorithm)|butterfly]], so-called because of the shape of the [[dataflow diagram]] for the radix-2 case.)\n\n== Variations ==\n\nThere are many other variations on the Cooley–Tukey algorithm.  '''Mixed-radix''' implementations handle composite sizes with a variety of (typically small) factors in addition to two, usually (but not always) employing the O(''N''<sup>2</sup>) algorithm for the prime base cases of the recursion <nowiki>(</nowiki>it is also possible to employ an ''N''&nbsp;log&nbsp;''N'' algorithm for the prime base cases, such as [[Rader's FFT algorithm|Rader]]'s or [[Bluestein's FFT algorithm|Bluestein]]'s algorithm<nowiki>)</nowiki>.  [[Split-radix FFT algorithm|Split radix]] merges radices 2 and 4, exploiting the fact that the first transform of radix 2 requires no twiddle factor, in order to achieve what was long the lowest known arithmetic operation count for power-of-two sizes,<ref name=DuhamelVe90/> although recent variations achieve an even lower count.<ref>Lundy, T., and J. Van Buskirk, \"A new matrix approach to real FFTs and convolutions of length 2<sup>''k''</sup>,\" ''Computing'' '''80''', 23–45 (2007).</ref><ref>Johnson, S. G., and M. Frigo, \"[http://www.fftw.org/newsplit.pdf A modified split-radix FFT with fewer arithmetic operations],\" ''IEEE Trans. Signal Process.'' '''55''' (1), 111–119 (2007).</ref>  (On present-day computers, performance is determined more by [[CPU cache|cache]] and [[CPU pipeline]] considerations than by strict operation counts; well-optimized FFT implementations often employ larger radices and/or hard-coded base-case transforms of significant size.<ref name=FrigoJohnson05/>). \n\nAnother way of looking at the Cooley–Tukey algorithm is that it re-expresses a size ''N'' one-dimensional DFT as an ''N''<sub>1</sub> by ''N''<sub>2</sub> two-dimensional DFT (plus twiddles), where the output matrix is [[transpose]]d. The net result of all of these transpositions, for a radix-2 algorithm, corresponds to a bit reversal of the input (DIF) or output (DIT) indices.  If, instead of using a small radix, one employs a radix of roughly {{radic|''N''}} and explicit input/output matrix transpositions, it is called a '''four-step''' algorithm (or ''six-step'', depending on the number of transpositions), initially proposed to improve memory locality,<ref name=GenSande66>Gentleman W. M., and G. Sande, \"Fast Fourier transforms—for fun and profit,\" ''Proc. AFIPS'' '''29''', 563–578 (1966).</ref><ref name=Bailey90>Bailey, David H., \"FFTs in external or hierarchical memory,\" ''J. Supercomputing'' '''4''' (1), 23–35 (1990)</ref> e.g. for cache optimization or [[out-of-core]] operation, and was later shown to be an optimal [[cache-oblivious algorithm]].<ref name=Frigo99>M. Frigo, C. E. Leiserson, H. Prokop, and S. Ramachandran. Cache-oblivious algorithms. In ''Proceedings of the 40th IEEE Symposium on Foundations of Computer Science'' (FOCS 99), p.285-297. 1999. [http://ieeexplore.ieee.org/iel5/6604/17631/00814600.pdf?arnumber=814600 Extended abstract at IEEE], [http://citeseer.ist.psu.edu/307799.html at Citeseer].</ref>\n\nThe general Cooley–Tukey factorization rewrites the indices ''k'' and ''n'' as <math>k = N_2 k_1 + k_2</math> and <math>n = N_1 n_2 + n_1</math>, respectively, where the indices ''k''<sub>a</sub> and ''n''<sub>a</sub> run from 0..''N''<sub>a</sub>-1 (for ''a'' of 1 or 2).  That is, it re-indexes the input (''n'') and output (''k'') as ''N''<sub>1</sub> by ''N''<sub>2</sub> two-dimensional arrays in [[column-major order|column-major]] and [[row-major order]], respectively; the difference between these indexings is a transposition, as mentioned above.  When this re-indexing is substituted into the DFT formula for ''nk'', the <math>N_1 n_2 N_2 k_1</math> cross term vanishes (its exponential is unity), and the remaining terms give\n\n:<math>X_{N_2 k_1 + k_2} =\n      \\sum_{n_1=0}^{N_1-1} \\sum_{n_2=0}^{N_2-1}\n         x_{N_1 n_2 + n_1}\n         e^{-\\frac{2\\pi i}{N_1 N_2} \\cdot (N_1 n_2 + n_1) \\cdot (N_2 k_1 + k_2) }</math>\n::<math>= \n    \\sum_{n_1=0}^{N_1-1} \n      \\left[ e^{-\\frac{2\\pi i}{N} n_1 k_2 } \\right]\n      \\left( \\sum_{n_2=0}^{N_2-1} x_{N_1 n_2 + n_1}  \n              e^{-\\frac{2\\pi i}{N_2} n_2 k_2 } \\right)\n      e^{-\\frac{2\\pi i}{N_1} n_1 k_1 }\n</math>\n\nwhere each inner sum is a DFT of size ''N''<sub>2</sub>, each outer sum is a DFT of size ''N''<sub>1</sub>, and the <nowiki>[...]</nowiki> bracketed term is the twiddle factor.\n\nAn arbitrary radix ''r'' (as well as mixed radices) can be employed, as was shown by both Cooley and Tukey<ref name=CooleyTukey65/> as well as Gauss (who gave examples of radix-3 and radix-6 steps).<ref name=Heideman84/>  Cooley and Tukey originally assumed that the radix butterfly required O(''r''<sup>2</sup>) work and hence reckoned the complexity for a radix ''r'' to be O(''r''<sup>2</sup>&nbsp;''N''/''r''&nbsp;log<sub>''r''</sub>''N'') = O(''N''&nbsp;log<sub>2</sub>(''N'')&nbsp;''r''/log<sub>2</sub>''r''); from calculation of values of ''r''/log<sub>2</sub>''r'' for integer values of ''r'' from 2 to 12 the optimal radix is found to be 3 (the closest integer to ''[[e (mathematical constant)|e]]'', which minimizes ''r''/log<sub>2</sub>''r'').<ref name=CooleyTukey65/><ref>Cooley, J. W., P. Lewis and P. Welch, \"The Fast Fourier Transform and its Applications\", ''IEEE Trans on Education'' '''12''', 1, 28–34 (1969)</ref>  This analysis was erroneous, however: the radix-butterfly is also a DFT and can be performed via an FFT algorithm in O(''r''  log ''r'') operations, hence the radix ''r'' actually cancels in the complexity O(''r''&nbsp;log(''r'')&nbsp;''N''/''r''&nbsp;log<sub>''r''</sub>''N''), and the optimal ''r'' is determined by more complicated considerations.  In practice, quite large ''r'' (32 or 64) are important in order to effectively exploit e.g. the large number of [[processor register]]s on modern processors,<ref name=FrigoJohnson05/> and even an unbounded radix ''r''={{radic|''N''}} also achieves O(''N''&nbsp;log&nbsp;''N'') complexity and has theoretical and practical advantages for large ''N'' as mentioned above.<ref name=GenSande66/><ref name=Bailey90/><ref name=Frigo99/>\n\n== Data reordering, bit reversal, and in-place algorithms ==\nAlthough the abstract Cooley–Tukey factorization of the DFT, above, applies in some form to all implementations of the algorithm, much greater diversity exists in the techniques for ordering and accessing the data at each stage of the FFT. Of special interest is the problem of devising an [[in-place algorithm]] that overwrites its input with its output data using only O(1) auxiliary storage.\n\nThe most well-known reordering technique involves explicit '''bit reversal''' for in-place radix-2 algorithms.  [[Bit-reversal permutation|Bit reversal]] is the [[permutation]] where the data at an index ''n'', written in [[binary numeral system|binary]] with digits ''b''<sub>4</sub>''b''<sub>3</sub>''b''<sub>2</sub>''b''<sub>1</sub>''b''<sub>0</sub> (e.g. 5 digits for ''N''=32 inputs), is transferred to the index with reversed digits ''b''<sub>0</sub>''b''<sub>1</sub>''b''<sub>2</sub>''b''<sub>3</sub>''b''<sub>4</sub> . Consider the last stage of a radix-2 DIT algorithm like the one presented above, where the output is written in-place over the input: when <math>E_k</math> and <math>O_k</math> are combined with a size-2 DFT, those two values are overwritten by the outputs.  However, the two output values should go in the first and second ''halves'' of the output array, corresponding to the ''most'' significant bit ''b''<sub>4</sub> (for ''N''=32); whereas the two inputs <math>E_k</math> and <math>O_k</math> are interleaved in the even and odd elements, corresponding to the ''least'' significant bit ''b''<sub>0</sub>.  Thus, in order to get the output in the correct place, ''b''<sub>0</sub> should take the place of ''b''<sub>4</sub> and the index becomes ''b''<sub>0</sub>''b''<sub>4</sub>''b''<sub>3</sub>''b''<sub>2</sub>''b''<sub>1</sub>. And for next recursive stage, those 4 least significant bits will become ''b''<sub>1</sub>''b''<sub>4</sub>''b''<sub>3</sub>''b''<sub>2</sub>, If you include all of the recursive stages of a radix-2 DIT algorithm, ''all'' the bits must be reversed and thus one must pre-process the input (''or'' post-process the output) with a bit reversal to get in-order output. (If each size-''N''/2 subtransform is to operate on contiguous data, the DIT ''input'' is pre-processed by bit-reversal.) Correspondingly, if you perform all of the steps in reverse order, you obtain a radix-2 DIF algorithm with bit reversal in post-processing (or pre-processing, respectively).\n\nThe following is pseudocode for iterative radix-2 FFT algorithm implemented using bit-reversal permutation.<ref name=\"clrs\">{{cite book|last1=al.]|first1=Thomas H. Cormen ... [et|last2=Leiserson|first2=Charles|last3=Rivest|first3=Ronald|last4=Stein|first4=Clifford|title=Introduction to algorithms|date=2009|publisher=MIT Press|location=Cambridge, Mass.|isbn=978-0-262-03384-8|pages=915–918|edition=3rd}}</ref>\n\n \n \n  '''algorithm''' iterative-fft '''is'''\n     '''input:''' Array ''a'' of ''n'' complex values where n is a power of 2\n     '''output:''' Array ''A'' the DFT of a\n \n     bit-reverse-copy(a,A)\n     ''n'' ← ''a''.length \n     '''for''' ''s'' = 1 '''to''' log(''n'')\n         ''m'' ← 2<sup>''s''</sup>\n         ''ω''<sub>''m''</sub> ← exp(−2π''i''/''m'') \n         '''for''' ''k'' = 0 '''to''' ''n''-1 '''by''' ''m''\n             ''ω'' ← 1\n             '''for''' ''j'' = 0 '''to''' ''m''/''2'' – 1\n                 ''t'' ← ''ω'' ''A''[''k'' + ''j'' + ''m''/2]\n                 ''u'' ← ''A''[''k'' + ''j'']\n                 ''A''[''k'' + ''j''] ← ''u'' + ''t''\n                 ''A''[''k'' + ''j'' + ''m''/2] ← ''u'' – ''t''\n                 ''ω'' ← ''ω'' ''ω''<sub>''m''</sub>\n    \n     '''return''' ''A''\n\nThe bit-reverse-copy procedure can be implemented as follows.\n\n \n \n  '''algorithm''' bit-reverse-copy(''a'',''A'') '''is'''\n    '''input:''' Array ''a'' of ''n'' complex values where n is a power of 2,\n    '''output:''' Array ''A'' of size ''n''\n \n    ''n'' ← ''a''.length\n    '''for''' ''k'' = 0 ''to'' ''n'' – 1\n        ''A''[rev(k)] = ''a''[k]\n\nAlternatively, some applications (such as convolution) work equally well on bit-reversed data, so one can perform forward transforms, processing, and then inverse transforms all without bit reversal to produce final results in the natural order.\n\nMany FFT users, however, prefer natural-order outputs, and a separate, explicit bit-reversal stage can have a non-negligible impact on the computation time,<ref name=FrigoJohnson05/> even though bit reversal can be done in O(''N'') time and has been the subject of much research.<ref>{{cite journal |last=Karp |first=Alan H. |title=Bit reversal on uniprocessors |journal=SIAM Review |volume=38 |issue=1 |pages=1–26 |year=1996 |jstor=2132972 |doi=10.1137/1038001|citeseerx=10.1.1.24.2913 }}</ref><ref>{{Cite book|last=Carter |first=Larry |first2=Kang Su |last2=Gatlin |title=Towards an optimal bit-reversal permutation program |journal=Proc. 39th Ann. Symp. On Found. Of Comp. Sci. (FOCS) |pages=544–553 |year=1998 |doi=10.1109/SFCS.1998.743505 |isbn=978-0-8186-9172-0 |citeseerx=10.1.1.46.9319 }}</ref><ref>{{cite journal |last=Rubio |first=M. |first2=P. |last2=Gómez |first3=K. |last3=Drouiche |title=A new superfast bit reversal algorithm |journal=Intl. J. Adaptive Control and Signal Processing |volume=16 |issue=10 |pages=703–707 |year=2002 |doi=10.1002/acs.718 }}</ref> Also, while the permutation is a bit reversal in the radix-2 case, it is more generally an arbitrary (mixed-base) digit reversal for the mixed-radix case, and the permutation algorithms become more complicated to implement. Moreover, it is desirable on many hardware architectures to re-order intermediate stages of the FFT algorithm so that they operate on consecutive (or at least more localized) data elements. To these ends, a number of alternative implementation schemes have been devised for the Cooley–Tukey algorithm that do not require separate bit reversal and/or involve additional permutations at intermediate stages.\n\nThe problem is greatly simplified if it is '''out-of-place''': the output array is distinct from the input array or, equivalently, an equal-size auxiliary array is available.  The '''Stockham auto-sort''' algorithm<ref>Originally attributed to Stockham in W. T. Cochran ''et al.'', [https://dx.doi.org/10.1109/PROC.1967.5957 What is the fast Fourier transform?], ''Proc. IEEE'' vol. 55, 1664–1674 (1967).</ref><ref name=Swarztrauber84/> performs every stage of the FFT out-of-place, typically writing back and forth between two arrays, transposing one \"digit\" of the indices with each stage, and has been especially popular on [[SIMD]] architectures.<ref name=Swarztrauber84>P. N. Swarztrauber, [https://dx.doi.org/10.1016/S0167-8191(84)90413-7 FFT algorithms for vector computers], ''Parallel Computing'' vol. 1, 45–63 (1984).</ref><ref>{{cite book |last=Swarztrauber |first=P. N. |chapter=Vectorizing the FFTs |editor-first=G. |editor-last=Rodrigue |title=Parallel Computations |publisher=Academic Press |location=New York |year=1982 |pages=51–83 |isbn=978-0-12-592101-5 }}</ref>  Even greater potential SIMD advantages (more consecutive accesses) have been proposed for the '''Pease''' algorithm,<ref>{{cite journal |last=Pease |first=M. C. |title=An adaptation of the fast Fourier transform for parallel processing |journal=J. ACM |volume=15 |issue=2 |pages=252–264 |year=1968 |doi=10.1145/321450.321457 }}</ref> which also reorders out-of-place with each stage, but this method requires separate bit/digit reversal and O(''N'' log ''N'') storage.  One can also directly apply the Cooley–Tukey factorization definition with explicit ([[depth-first search|depth-first]]) recursion and small radices, which produces natural-order out-of-place output with no separate permutation step (as in the pseudocode above) and can be argued to have [[cache-oblivious algorithm|cache-oblivious]] locality benefits on systems with [[cache (computing)|hierarchical memory]].<ref name=Singleton67>{{cite journal |last=Singleton |first=Richard C. |title=On computing the fast Fourier transform |journal=Commun. ACM |volume=10 |issue=10 |year=1967 |pages=647–654 |doi=10.1145/363717.363771 }}</ref><ref name=FrigoJohnson05>{{cite journal |last=Frigo |first=M. |first2=S. G. |last2=Johnson |url=http://www.fftw.org/fftw-paper-ieee.pdf |title=The Design and Implementation of FFTW3 |journal=Proceedings of the IEEE |volume=93 |issue=2 |pages=216–231 |year=2005 |doi=10.1109/JPROC.2004.840301|citeseerx=10.1.1.66.3097 }}</ref><ref>{{cite web |last=Frigo |first=Matteo |first2=Steven G. |last2=Johnson |title=FFTW |url=http://www.fftw.org/ }} A free ([[GNU General Public License|GPL]]) C library for computing discrete Fourier transforms in one or more dimensions, of arbitrary size, using the Cooley–Tukey algorithm</ref>\n\nA typical strategy for in-place algorithms without auxiliary storage and without separate digit-reversal passes involves small matrix transpositions (which swap individual pairs of digits) at intermediate stages, which can be combined with the radix butterflies to reduce the number of passes over the data.<ref name=FrigoJohnson05/><ref>{{cite journal |last=Johnson |first=H. W. |first2=C. S. |last2=Burrus |title=An in-place in-order radix-2 FFT |journal=Proc. ICASSP |volume= |issue= |pages=28A.2.1–28A.2.4 |year=1984 |doi= }}</ref><ref>{{cite journal |last=Temperton |first=C. |title=Self-sorting in-place fast Fourier transform |journal=SIAM Journal on Scientific and Statistical Computing |volume=12 |issue=4 |pages=808–823 |year=1991 |doi=10.1137/0912043 }}</ref><ref>{{cite journal |last=Qian |first=Z. |first2=C. |last2=Lu |first3=M. |last3=An |first4=R. |last4=Tolimieri |title=Self-sorting in-place FFT algorithm with minimum working space |journal=IEEE Trans. ASSP |volume=52 |issue=10 |pages=2835–2836 |year=1994 |doi=10.1109/78.324749 }}</ref><ref>{{cite journal |last=Hegland |first=M. |title=A self-sorting in-place fast Fourier transform algorithm suitable for vector and parallel processing |journal=Numerische Mathematik |volume=68 |issue=4 |pages=507–547 |year=1994 |doi=10.1007/s002110050074 |citeseerx=10.1.1.54.5659 }}</ref>\n\n==References==\n{{reflist|30em}}\n\n== External links ==\n* [http://en.dsplib.org/content/fft_dec_in_time.html Radix-2 Decimation in Time FFT Algorithm]\n* [http://en.dsplib.org/content/fft_dec_in_freq.html Radix-2 Decimation in Frequency FFT Algorithm]\n* [http://www.librow.com/articles/article-10 A simple, pedagogical radix-2 Cooley–Tukey FFT algorithm in C++]\n* [http://sourceforge.net/projects/kissfft/ KISSFFT]: a simple mixed-radix Cooley–Tukey implementation in C (open source)\n\n{{DEFAULTSORT:Cooley-Tukey FFT algorithm}}\n[[Category:FFT algorithms]]\n[[Category:Articles with example pseudocode]]"
    },
    {
      "title": "Cyclotomic fast Fourier transform",
      "url": "https://en.wikipedia.org/wiki/Cyclotomic_fast_Fourier_transform",
      "text": "The '''cyclotomic fast Fourier transform''' is a type of [[fast Fourier transform]] algorithm over [[finite field]]s.<ref>S.V. Fedorenko and P.V. Trifonov, {{cite journal |last=Fedorenko |first=S. V. |first2=P. V.. |last2=Trifonov |url=http://dcn.ftk.spbstu.ru/~petert/papers/pushkin2.pdf |title=On Computing the Fast Fourier Transform over Finite Fields |journal=Proceedings of International Workshop on Algebraic and Combinatorial Coding Theory |pages=108–111|year=2003}}</ref> This algorithm first decomposes a DFT into several circular convolutions, and then derives the DFT results from the circular convolution results. When applied to a DFT over <math>GF(2^m)</math>, this algorithm has a very low multiplicative complexity. In practice, since there usually exist efficient algorithms for circular convolutions with specific lengths, this algorithm is very efficient.<ref name=\"complexityAnalysis\">{{cite journal | last=Wu | first=Xuebin | last2=Wang | first2=Ying |last3=Yan | first3=Zhiyuan | title=On Algorithms and Complexities of Cyclotomic Fast Fourier Transforms Over Arbitrary Finite Fields| journal=IEEE Transactions on Signal Processing|volume=60|issue=3|year=2012|pages=1149–1158 | doi=10.1109/tsp.2011.2178844}}</ref>\n\n== Background ==\nThe [[discrete Fourier transform]] over [[finite field]]s finds widespread application in the decoding of [[error-correcting code]]s such as [[BCH codes]] and [[Reed–Solomon error correction|Reed–Solomon codes]]. Generalized from the [[complex number|complex field]], a discrete Fourier transform of a sequence <math>\\{f_i\\}_{0}^{N-1}</math> over a finite field GF(''p''<sup>''m''</sup>) is defined as\n\n:<math>F_j=\\sum_{i=0}^{N-1}f_i\\alpha^{ij}, 0\\le j\\le N-1, </math>\n\nwhere <math>\\alpha</math> is the ''N''-th [[primitive nth root of unity|primitive root]] of 1 in GF(''p''<sup>''m''</sup>). If we define the polynomial representation of <math>\\{f_i\\}_{0}^{N-1}</math> as\n\n:<math>f(x) = f_0+f_1x+f_2x^2+\\cdots+f_{N-1}x^{N-1}=\\sum_{0}^{N-1}f_ix^i,</math>\n\nit is easy to see that <math>F_j</math> is simply <math>f(\\alpha^j)</math>. That is, the discrete Fourier transform of a sequence converts it to a polynomial evaluation problem. \n\nWritten in matrix format, \n:<math>\\mathbf{F}=\\left[\\begin{matrix}F_0\\\\F_1\\\\ \\vdots \\\\ F_{N-1}\\end{matrix}\\right]=\n\\left[\\begin{matrix}\n\\alpha^0&\\alpha^0 &\\cdots & \\alpha^0\\\\\n\\alpha^0 & \\alpha^1 &\\cdots &\\alpha^{N-1}\\\\\n\\vdots &\\vdots & \\ddots & \\vdots \\\\\n\\alpha^{0} & \\alpha^{N-1} &\\cdots & \\alpha^{(N-1)(N-1)}\n\\end{matrix}\\right]\n\\left[\\begin{matrix}f_0\\\\f_1\\\\\\vdots\\\\f_{N-1}\\end{matrix}\\right]=\\mathcal{F}\\mathbf{f}.\n</math>\n\nDirect evaluation of DFT has an <math>O(N^2)</math> complexity. Fast Fourier transforms are just efficient algorithms evaluating the above matrix-vector product.\n\n== Algorithm ==\n\nFirst, we define a [[linearized polynomial]] over GF(p<sup>m</sup>) as \n\n:<math>L(x) = \\sum_{i} l_i x^{p^i}, l_i \\in \\mathrm{GF}(p^m).</math>\n\n<math>L(x)</math> is called linearized because <math>L(x_1+x_2) = L(x_1) + L(x_2)</math>, which comes from the fact that for elements <math>x_1,x_2 \\in \\mathrm{GF}(p^m),</math><math>(x_1+x_2)^p=x_1^p+x_2^p.</math>\n\nNotice that <math>p</math> is invertible modulo <math>N</math> because <math>N</math> must divide the order <math>p^m-1</math> of the multiplicative group of the field <math>\\mathrm{GF}(p^m)</math>. So, the elements <math>\\{0, 1, 2, \\ldots, N-1\\}</math> can be partitioned into <math>l+1</math> cyclotomic cosets modulo <math>N</math>: \n:<math>\\{0\\},</math> \n:<math>\\{k_1, pk_1, p^2k_1, \\ldots, p^{m_1-1}k_1\\},</math>\n:<math>\\ldots,</math> \n:<math>\\{k_l, pk_l, p^2k_l, \\ldots, p^{m_l-1}k_l\\},</math>\nwhere <math>k_i=p^{m_i}k_i \\pmod{N}</math>. Therefore, the input to the Fourier transform can be rewritten as \n\n:<math>f(x)=\\sum_{i=0}^l L_i(x^{k_i}),\\quad L_i(y) = \\sum_{t=0}^{m_i-1}y^{p^t}f_{p^tk_i\\bmod{N}}.</math>\n\nIn this way, the polynomial representation is decomposed into a sum of linear polynomials, and hence <math>F_j</math> is given by \n:<math>F_j=f(\\alpha^j)=\\sum_{i=0}^lL_i(\\alpha^{jk_i})</math>. \nExpanding <math>\\alpha^{jk_i} \\in \\mathrm{GF}(p^{m_i})</math> with the proper basis <math>\\{\\beta_{i,0}, \\beta_{i,1}, \\ldots, \\beta_{i,m_i-1}\\}</math>, we have <math>\\alpha^{jk_i} = \\sum_{s=0}^{m_i-1}a_{ijs}\\beta_{i,s}</math> where <math>a_{ijs} \\in \\mathrm{GF}(p)</math>, and by the property of the linearized polynomial <math>L_i(x)</math>, we have\n\n:<math>F_j=\\sum_{i=0}^l\\sum_{s=0}^{m_i-1}a_{ijs}\\left(\\sum_{t=0}^{m_i-1}\\beta_{i,s}^{p^t}f_{p^{t}k_i\\bmod{N}}\\right)</math>\n\nThis equation can be rewritten in matrix form as <math>\\mathbf{F}=\\mathbf{AL\\Pi f}</math>, where <math>\\mathbf{A}</math> is an <math>N\\times N</math> matrix over GF(''p'') that contains the elements <math>a_{ijs}</math>, <math>\\mathbf{L}</math> is a block diagonal matrix, and <math>\\mathbf{\\Pi}</math> is a permutation matrix regrouping the elements in <math>\\mathbf{f}</math> according to the cyclotomic coset index. \n\nNote that if the [[normal basis]] <math>\\{\\gamma_i^{p^0}, \\gamma_i^{p^1}, \\cdots, \\gamma_i^{p^{m_i-1}}\\} </math> is used to expand the field elements of <math>\\mathrm{GF}(p^{m_i})</math>,  the i-th block of <math>\\mathbf{L}</math> is given by: \n:<math>\\mathbf{L}_i=\n\\begin{bmatrix}\n  \\gamma_i^{p^0}  &\\gamma_i^{p^1}  &\\cdots  &\\gamma_i^{p^{m_i-1}}\\\\\n  \\gamma_i^{p^1}  &\\gamma_i^{p^2}  &\\cdots  &\\gamma_i^{p^{0}}\\\\\n  \\vdots & \\vdots & \\ddots & \\vdots\\\\\n  \\gamma_i^{p^{m_i-1}}  &\\gamma_i^{p^0}  &\\cdots  &\\gamma_i^{p^{m_i-2}}\\\\\n\\end{bmatrix}\n</math> \nwhich is a [[circulant matrix]]. It is well known that a circulant matrix-vector product can be efficiently computed by [[convolution]]s. Hence we successfully reduce the discrete Fourier transform into short convolutions.\n\n== Complexity ==\n\nWhen applied to a [[Characteristic (algebra)|characteristic]]-2 field GF(2<sup>''m''</sup>), the matrix <math>\\mathbf{A}</math> is just a binary matrix. Only addition is used when calculating the matrix-vector product of <math>\\mathrm{A}</math> and <math>\\mathrm{L\\Pi f}</math>. It has been shown that the multiplicative complexity of the cyclotomic algorithm is given by <math>O(n(\\log_2n)^{\\log_2\\frac{3}{2}})</math>, and the additive complexity is given by <math>O(n^2/(\\log_2 n)^{\\log_2\\frac{8}{3}})</math>.<ref name=\"complexityAnalysis\"/>\n\n== References ==\n{{reflist}}\n\n[[Category:Discrete transforms]]\n[[Category:FFT algorithms]]"
    },
    {
      "title": "Fast Fourier transform",
      "url": "https://en.wikipedia.org/wiki/Fast_Fourier_transform",
      "text": "{{redirect|FFT}}\n{{Use American English|date=March 2019}}\n{{Short description|O(N logN) divide and conquer algorithm to calculate the discrete Fourier transforms}}\n[[File:DIT-FFT-butterfly.png|thumb|An example FFT algorithm structure, using a decomposition into half-size FFTs]]\n[[File:FFT of Cosine Summation Function.png|thumb|A discrete Fourier analysis of a sum of cosine waves at 10, 20, 30, 40, and 50&nbsp;Hz]]\n\nA '''fast Fourier transform''' ('''FFT''') is an [[algorithm]] that computes the [[discrete Fourier transform]] (DFT) of a sequence, or its inverse (IDFT). [[Fourier analysis]] converts a signal from its original domain (often time or space) to a representation in the [[frequency domain]] and vice versa. The DFT is obtained by decomposing a [[sequence]] of values into components of different frequencies.<ref name=\"Heideman_Johnson_Burrus_1984\"/> This operation is useful in many fields, but computing it directly from the definition is often too slow to be practical. An FFT rapidly computes such transformations by [[Matrix decomposition|factorizing]] the [[DFT matrix]] into a product of [[Sparse matrix|sparse]] (mostly zero) factors.<ref name=\"Loan_1992\"/> As a result, it manages to reduce the [[Computational complexity theory|complexity]] of computing the DFT from <math>O(N^2)</math>, which arises if one simply applies the definition of DFT, to <math>O(N \\log N)</math>, where <math>N</math> is the data size. The difference in speed can be enormous, especially for long data sets where ''N'' may be in the thousands or millions. In the presence of [[round-off error]], many FFT algorithms are much more accurate than evaluating the DFT definition directly. There are many different FFT algorithms based on a wide range of published theories, from simple [[complex number|complex-number arithmetic]] to [[group theory]] and [[number theory]].\n\nFast Fourier transforms are widely used for [[discrete Fourier transform#Applications|applications]] in engineering, science, and mathematics. The basic ideas were popularized in 1965, but some algorithms had been derived as early as 1805.<ref name=\"Heideman_Johnson_Burrus_1984\"/> In 1994, [[Gilbert Strang]] described the FFT as \"the most important [[numerical analysis|numerical algorithm]] of our lifetime\",<ref name=\"Strang_1994\"/><ref name=\"Kent_2002\"/> and it was included in Top 10 Algorithms of 20th Century by the [[IEEE]] magazine ''[[Computing in Science & Engineering]]''.<ref name=\"Dongarra_Sullivan_2000\"/>\n\nThe best-known FFT algorithms depend upon the [[factorization]] of ''N'', but there are FFTs with [[Big O notation|O(''N''&nbsp;log&nbsp;''N'')]] [[Computational complexity theory|complexity]] for all ''N'', even for [[prime number|prime]]&nbsp;''N''. Many FFT algorithms only depend on the fact that <math>e^{-2\\pi i/N}</math> is an ''N''-th [[primitive root of unity]], and thus can be applied to analogous transforms over any [[finite field]], such as [[number-theoretic transform]]s. Since the inverse DFT is the same as the DFT, but with the opposite sign in the exponent and a 1/''N'' factor, any FFT algorithm can easily be adapted for it.\n\n== History ==\nThe development of fast algorithms for DFT can be traced to [[Carl Friedrich Gauss|Gauss]]'s unpublished work in 1805 when he needed it to interpolate the orbit of asteroids [[2 Pallas|Pallas]] and [[3 Juno|Juno]] from sample observations.<ref name=\"Gauss_1866\"/><ref name=\"Heideman_Johnson_Burrus_1985\"/> His method was very similar to the one published in 1965 by [[James Cooley|Cooley]] and [[John Tukey|Tukey]], who are generally credited for the invention of the modern generic FFT algorithm. While Gauss's work predated even [[Joseph Fourier|Fourier]]'s results in 1822, he did not analyze the computation time and eventually used other methods to achieve his goal.\n\nBetween 1805 and 1965, some versions of FFT were published by other authors. [[Frank Yates]] in 1932 published his version called ''interaction algorithm'', which provided efficient computation of [[Hadamard transform|Hadamard and Walsh transforms]].<ref name=\"Yates_1937\"/> Yates' algorithm is still used in the field of statistical design and analysis of experiments. In 1942, [[G. C. Danielson]] and [[Cornelius Lanczos]] published their version to compute DFT for [[x-ray crystallography]], a field where calculation of Fourier transforms presented a formidable bottleneck.<ref name=\"Danielson_Lanczos_1942\"/><ref name=\"Lanczos_1956\"/> While many methods in the past had focused on reducing the constant factor for <math>O(N^2)</math> computation by taking advantage of \"symmetries\", Danielson and Lanczos realized that one could use the \"periodicity\" and apply a \"doubling trick\" to get <math>O(N \\log N)</math> runtime.<ref name=\"Cooley_Lewis_Welch_1967\"/>\n\n[[James Cooley]] and [[John Tukey]] published a [[Cooley–Tukey FFT algorithm|more general version of FFT]] in 1965 that is applicable when ''N'' is composite and not necessarily a power of 2.<ref name=\"Cooley_Tukey_1965\"/> Tukey came up with the idea during a meeting of [[President Kennedy]]'s Science Advisory Committee where a discussion topic involved detecting nuclear tests by the Soviet Union by setting up sensors to surround the country from outside. To analyze the output of these sensors, a fast Fourier transform algorithm would be needed. In discussion with Tukey, [[Richard Garwin]] recognized the general applicability of the algorithm not just to national security problems, but also to a wide range of problems including one of immediate interest to him, determining the periodicities of the spin orientations in a 3-D crystal of Helium-3.<ref name=\"Cooley_1987\"/> Garwin gave Tukey's idea to Cooley (both worked at [[Thomas J. Watson Research Center|IBM's Watson labs]]) for implementation.<ref name=\"Garwin_1969\"/> Cooley and Tukey published the paper in a relatively short time of six months.<ref name=\"Rockmore_2000\"/> As Tukey did not work at IBM, the patentability of the idea was doubted and the algorithm went into the public domain, which, through the computing revolution of the next decade, made FFT one of the indispensable algorithms in digital signal processing.\n\n==Definition==\nLet ''x''<sub>0</sub>, ...., ''x''<sub>''N''−1</sub> be [[complex number]]s. The [[discrete Fourier transform|DFT]] is defined by the formula\n\n:<math> X_k = \\sum_{n=0}^{N-1} x_n e^{-i2\\pi k n/N} \\qquad k = 0,\\ldots,N-1, </math>\n\nwhere <math>e^{i 2\\pi/N} </math> is a [[Primitive root of unity|primitive]] {{mvar|N}}th root of 1.\n\nEvaluating this definition directly requires <math>O(N^2)</math> operations: there are ''N'' outputs ''X''<sub>''k''</sub>, and each output requires a sum of ''N'' terms. An FFT is any method to compute the same results in <math>O(N \\log N)</math> operations. All known FFT algorithms require [[Big O notation#Use in computer science|Θ]]<math>(N \\log N)</math> operations, although there is no known proof that a lower complexity score is impossible.<ref name=\"Frigo_Johnson_2007\"/>\n\nTo illustrate the savings of an FFT, consider the count of complex multiplications and additions for N=4096 data points. Evaluating the DFT's sums directly involves ''N''<sup>2</sup> complex multiplications and ''N''(''N''−1) complex additions, of which <math>O(N)</math> operations can be saved by eliminating trivial operations such as multiplications by 1, leaving about 30 million operations. On the other hand, the radix-2 [[#Cooley–Tukey algorithm|Cooley–Tukey algorithm]], for ''N'' a power of 2, can compute the same result with only (''N''/2)log<sub>2</sub>(''N'') complex multiplications (again, ignoring simplifications of multiplications by 1 and similar) and ''N''&nbsp;log<sub>2</sub>(''N'') complex additions, in total about 30,000 operations - a thousand times less than with direct evaluation. In practice, actual performance on modern computers is usually dominated by factors other than the speed of arithmetic operations and the analysis is a complicated subject (see, e.g., Frigo & [[Steven G. Johnson|Johnson]], 2005),<ref name=\"Frigo_Johnson_2005\"/> but the overall improvement from <math>O(N^2)</math> to <math>O(N \\log N)</math> remains.\n\n==Algorithms==\n\n===Cooley–Tukey algorithm===\n{{main|Cooley–Tukey FFT algorithm}}\n\nBy far the most commonly used FFT is the Cooley–Tukey algorithm. This is a [[divide and conquer algorithm]] that [[recursion|recursively]] breaks down a DFT of any [[composite number|composite]] size ''N'' = ''N''<sub>1</sub>''N''<sub>2</sub> into many smaller DFTs of sizes ''N''<sub>1</sub> and ''N''<sub>2</sub>, along with O(''N'') multiplications by complex [[roots of unity]] traditionally called [[twiddle factor]]s (after Gentleman and Sande, 1966<ref name=\"Gentleman_Sande_1966\"/>).\n\nThis method (and the general idea of an FFT) was popularized by a publication of Cooley and Tukey in 1965,<ref name=\"Cooley_Tukey_1965\"/> but it was later discovered<ref name=\"Heideman_Johnson_Burrus_1984\"/> that those two authors had independently re-invented an algorithm known to [[Carl Friedrich Gauss]] around 1805<ref name=\"Gauss_1805\"/> (and subsequently rediscovered several times in limited forms).\n\nThe best known use of the Cooley–Tukey algorithm is to divide the transform into two pieces of size ''N''/2 at each step, and is therefore limited to power-of-two sizes, but any factorization can be used in general (as was known to both Gauss and Cooley/Tukey<ref name=\"Heideman_Johnson_Burrus_1984\"/>). These are called the ''radix-2'' and ''mixed-radix'' cases, respectively (and other variants such as the [[split-radix FFT]] have their own names as well).  Although the basic idea is recursive, most traditional implementations rearrange the algorithm to avoid explicit recursion. Also, because the Cooley–Tukey algorithm breaks the DFT into smaller DFTs, it can be combined arbitrarily with any other algorithm for the DFT, such as those described below.\n\n===Other FFT algorithms===\n{{main|Prime-factor FFT algorithm|Bruun's FFT algorithm|Rader's FFT algorithm|Bluestein's FFT algorithm|Hexagonal Fast Fourier Transform}}\n\nThere are other FFT algorithms distinct from Cooley–Tukey.\n\n[[Cornelius Lanczos]] did pioneering work on the FFT and FFS ([[fast Fourier sampling]] method) with [[G. C. Danielson]] (1940).\n\nFor ''N'' = ''N''<sub>1</sub>''N''<sub>2</sub> with [[coprime]] ''N''<sub>1</sub> and ''N''<sub>2</sub>, one can use the [[Prime-factor FFT algorithm|prime-factor]] (Good–Thomas) algorithm (PFA), based on the [[Chinese remainder theorem]], to factorize the DFT similarly to Cooley–Tukey but without the twiddle factors. The Rader–Brenner algorithm (1976)<ref name=\"Brenner_Rader_1976\"/> is a Cooley–Tukey-like factorization but with purely imaginary twiddle factors, reducing multiplications at the cost of increased additions and reduced [[numerical stability]]; it was later superseded by the [[split-radix FFT algorithm|split-radix]] variant of Cooley–Tukey (which achieves the same multiplication count but with fewer additions and without sacrificing accuracy). Algorithms that recursively factorize the DFT into smaller operations other than DFTs include the Bruun and [[Quick Fourier transform algorithm|QFT]] algorithms. (The Rader–Brenner<ref name=\"Brenner_Rader_1976\"/> and QFT algorithms were proposed for power-of-two sizes, but it is possible that they could be adapted to general composite ''N''. Bruun's algorithm applies to arbitrary even composite sizes.) [[Bruun's FFT algorithm|Bruun's algorithm]], in particular, is based on interpreting the FFT as a recursive factorization of the [[polynomial]] ''z''<sup>''N''</sup>&nbsp;−&nbsp;1, here into real-coefficient polynomials of the form ''z''<sup>''M''</sup>&nbsp;−&nbsp;1 and ''z''<sup>2''M''</sup>&nbsp;+&nbsp;''az''<sup>''M''</sup>&nbsp;+&nbsp;1.\n\nAnother polynomial viewpoint is exploited by the Winograd FFT algorithm,<ref name=\"Winograd_1978\"/><ref name=\"Winograd_1979\"/> which factorizes ''z''<sup>''N''</sup>&nbsp;−&nbsp;1 into [[cyclotomic polynomial]]s—these often have coefficients of 1,&nbsp;0,&nbsp;or&nbsp;−1, and therefore require few (if any) multiplications, so Winograd can be used to obtain minimal-multiplication FFTs and is often used to find efficient algorithms for small factors. Indeed, Winograd showed that the DFT can be computed with only O(''N'') irrational multiplications, leading to a proven achievable lower bound on the number of multiplications for power-of-two sizes; unfortunately, this comes at the cost of many more additions, a tradeoff no longer favorable on modern [[central processing unit|processors]] with [[floating-point unit|hardware multipliers]]. In particular, Winograd also makes use of the PFA as well as an algorithm by Rader for FFTs of ''prime'' sizes.\n\n[[Rader's FFT algorithm|Rader's algorithm]], exploiting the existence of a [[generating set of a group|generator]] for the multiplicative [[group (mathematics)|group]] modulo prime ''N'', expresses a DFT of prime size ''N'' as a cyclic [[convolution]] of (composite) size ''N''−1, which can then be computed by a pair of ordinary FFTs via the [[convolution theorem]] (although Winograd uses other convolution methods). Another prime-size FFT is due to L. I. Bluestein, and is sometimes called the [[chirp-z algorithm]]; it also re-expresses a DFT as a convolution, but this time of the ''same'' size (which can be zero-padded to a [[power of two]] and evaluated by radix-2 Cooley–Tukey FFTs, for example), via the identity\n\n: <math>nk = -\\frac{(k-n)^2} 2 + \\frac{n^2} 2 + \\frac{k^2} 2.</math>\n\n[[Hexagonal Fast Fourier Transform]] aims at computing an efficient FFT for the hexagonally sampled data by using a new addressing scheme for hexagonal grids, called Array Set Addressing (ASA).\n\n==FFT algorithms specialized for real and/or symmetric data==\nIn many applications, the input data for the DFT are purely real, in which case the outputs satisfy the symmetry\n\n:<math>X_{N-k} = X_k^*</math>\n\nand efficient FFT algorithms have been designed for this situation (see e.g. Sorensen, 1987).<ref name=\"Sorensen_Jones_Heideman_Burrus_1987_1\"/><ref name=\"Sorensen_Jones_Heideman_Burrus_1987_2\"/> One approach consists of taking an ordinary algorithm (e.g. Cooley–Tukey) and removing the redundant parts of the computation, saving roughly a factor of two in time and memory. Alternatively, it is possible to express an ''even''-length real-input DFT as a complex DFT of half the length (whose real and imaginary parts are the even/odd elements of the original real data), followed by O(''N'') post-processing operations.\n\nIt was once believed that real-input DFTs could be more efficiently computed by means of the [[discrete Hartley transform]] (DHT), but it was subsequently argued that a specialized real-input DFT algorithm (FFT) can typically be found that requires fewer operations than the corresponding DHT algorithm (FHT) for the same number of inputs.<ref name=\"Sorensen_Jones_Heideman_Burrus_1987_1\"/> Bruun's algorithm (above) is another method that was initially proposed to take advantage of real inputs, but it has not proved popular.\n\nThere are further FFT specializations for the cases of real data that have [[even and odd functions|even/odd]] symmetry, in which case one can gain another factor of (roughly) two in time and memory and the DFT becomes the discrete cosine/sine transform(s) ([[discrete cosine transform|DCT]]/[[discrete sine transform|DST]]).  Instead of directly modifying an FFT algorithm for these cases, DCTs/DSTs can also be computed via FFTs of real data combined with O(''N'') pre/post processing.\n\n==Computational issues==\n\n===Bounds on complexity and operation counts===\n{{unsolved|computer science|What is the lower bound on the complexity of fast Fourier transform algorithms?  Can they be faster than <math>O(N\\log N)</math>?}}\nA fundamental question of longstanding theoretical interest is to prove lower bounds on the [[computational complexity theory|complexity]] and exact operation counts of fast Fourier transforms, and many open problems remain. It is not even rigorously proved whether DFTs truly require Ω(''N''&nbsp;log&nbsp;''N'') (i.e., order ''N''&nbsp;log&nbsp;''N'' or greater) operations, even for the simple case of [[power of two]] sizes, although no algorithms with lower complexity are known. In particular, the count of arithmetic operations is usually the focus of such questions, although actual performance on modern-day computers is determined by many other factors such as [[Cache (computing)|cache]] or [[pipeline (computing)|CPU pipeline]] optimization.\n\nFollowing pioneering work by [[Shmuel Winograd|Winograd]] (1978),<ref name=\"Winograd_1978\"/> a tight Θ(''N'') lower bound ''is'' known for the number of real multiplications required by an FFT. It can be shown that only <math>4N-2\\log_2^2 N-2\\log_2 N-4</math> irrational real multiplications are required to compute a DFT of power-of-two length <math>N=2^m</math>. Moreover, explicit algorithms that achieve this count are known (Heideman & [[Charles Sidney Burrus|Burrus]], 1986;<ref name=\"Heideman_Burrus_1986\"/> Duhamel, 1990<ref name=\"Duhamel_1990\"/>). Unfortunately, these algorithms require too many additions to be practical, at least on modern computers with hardware multipliers (Duhamel, 1990;<ref name=\"Duhamel_1990\"/> Frigo & [[Steven G. Johnson|Johnson]], 2005).<ref name=\"Frigo_Johnson_2005\"/>\n\nA tight lower bound is ''not'' known on the number of required additions, although lower bounds have been proved under some restrictive assumptions on the algorithms. In 1973, Morgenstern<ref name=\"Morgenstern_1973\"/> proved an Ω(''N''&nbsp;log&nbsp;''N'') lower bound on the addition count for algorithms where the multiplicative constants have bounded magnitudes (which is true for most but not all FFT algorithms). This result, however, applies only to the unnormalized Fourier transform (which is a scaling of a unitary matrix by a factor of <math>\\sqrt N</math>), and does not explain why the Fourier matrix is harder to compute than any other unitary matrix (including the identity matrix)  under the same scaling. [[Victor Pan|Pan]] (1986)<ref name=\"Pan_1986\"/> proved an Ω(''N''&nbsp;log&nbsp;''N'') lower bound assuming a bound on a measure of the FFT algorithm's \"asynchronicity\", but the generality of this assumption is unclear. For the case of power-of-two ''N'', [[Christos Papadimitriou|Papadimitriou]] (1979)<ref name=\"Papadimitriou_1979\"/> argued that the number <math>N \\log_2 N</math> of complex-number additions achieved by Cooley–Tukey algorithms is ''optimal'' under certain assumptions on the [[Graph (discrete mathematics)|graph]] of the algorithm (his assumptions imply, among other things, that no additive identities in the roots of unity are exploited). (This argument would imply that at least <math>2 N \\log_2 N</math> real additions are required, although this is not a tight bound because extra additions are required as part of complex-number multiplications.) Thus far, no published FFT algorithm has achieved fewer than <math>N \\log_2 N</math> complex-number additions (or their equivalent) for power-of-two&nbsp;''N''.\n\nA third problem is to minimize the ''total'' number of real multiplications and additions, sometimes called the \"arithmetic complexity\" (although in this context it is the exact count and not the asymptotic complexity that is being considered).  Again, no tight lower bound has been proven. Since 1968, however, the lowest published count for power-of-two ''N'' was long achieved by the [[split-radix FFT algorithm]], which requires <math>4N\\log_2 N-6N+8</math> real multiplications and additions for ''N'' > 1. This was recently reduced to <math>\\sim \\frac{34}{9} N \\log_2 N</math> (Johnson and Frigo, 2007;<ref name=\"Frigo_Johnson_2007\"/> Lundy and Van Buskirk, 2007<ref name=\"Lundy_Buskirk_2007\"/>). A slightly larger count (but still better than split radix for ''N''≥256) was shown to be provably optimal for ''N''≤512 under additional restrictions on the possible algorithms (split-radix-like flowgraphs with unit-modulus multiplicative factors), by reduction to a [[satisfiability modulo theories]] problem solvable by [[Proof by exhaustion|brute force]] (Haynal & Haynal, 2011).<ref name=\"Haynal_2011\"/>\n\nMost of the attempts to lower or prove the complexity of FFT algorithms have focused on the ordinary complex-data case, because it is the simplest. However, complex-data FFTs are so closely related to algorithms for related problems such as real-data FFTs, [[discrete cosine transform]]s, [[discrete Hartley transform]]s, and so on, that any improvement in one of these would immediately lead to improvements in the others (Duhamel & Vetterli, 1990).<ref name=\"Duhamel_Vetterli_1990\"/>\n\n===Approximations===\nAll of the FFT algorithms discussed above compute the DFT exactly (i.e. neglecting [[floating-point]] errors). A few \"FFT\" algorithms have been proposed, however, that compute the DFT ''approximately'', with an error that can be made arbitrarily small at the expense of increased computations. Such algorithms trade the approximation error for increased speed or other properties. For example, an approximate FFT algorithm by Edelman et al. (1999)<ref name=\"Edelman_McCorquodale_Toledo_1999\"/> achieves lower communication requirements for [[parallel computing]] with the help of a [[fast multipole method]]. A [[wavelet]]-based approximate FFT by Guo and Burrus (1996)<ref name=\"Guo_Burrus_1996\"/> takes sparse inputs/outputs (time/frequency localization) into account more efficiently than is possible with an exact FFT. Another algorithm for approximate computation of a subset of the DFT outputs is due to Shentov et al. (1995).<ref name=\"Shentov_Mitra_Heute_Hossen_1995\"/> The Edelman algorithm works equally well for sparse and non-sparse data, since it is based on the compressibility (rank deficiency) of the Fourier matrix itself rather than the compressibility (sparsity) of the data. Conversely, if the data are sparse—that is, if only ''K'' out of ''N'' Fourier coefficients are nonzero—then the complexity can be reduced to O(''K''&nbsp;log(''N'')log(''N''/''K'')), and this has been demonstrated to lead to practical speedups compared to an ordinary FFT for ''N''/''K''&nbsp;>&nbsp;32 in a large-''N'' example (''N''&nbsp;=&nbsp;2<sup>22</sup>) using a probabilistic approximate algorithm (which estimates the largest ''K'' coefficients to several decimal places).<ref name=\"Hassanieh_2012\"/>\n\n===Accuracy===\nEven the \"exact\" FFT algorithms have errors when finite-precision floating-point arithmetic is used, but these errors are typically quite small; most FFT algorithms, e.g. Cooley–Tukey, have excellent numerical properties as a consequence of the [[pairwise summation]] structure of the algorithms.  The upper bound on the [[approximation error|relative error]] for the Cooley–Tukey algorithm is O(''ε'' log ''N''), compared to O(''εN''<sup>3/2</sup>) for the naïve DFT formula,<ref name=\"Gentleman_Sande_1966\"/> where ε is the machine floating-point relative precision. In fact, the [[root mean square]] (rms) errors are much better than these upper bounds, being only O(''ε'' {{radic|log ''N''}}) for Cooley–Tukey and O(''ε'' {{radic|''N''}}) for the naïve DFT (Schatzman, 1996).<ref name=\"Schatzman_1996\"/> These results, however, are very sensitive to the accuracy of the twiddle factors used in the FFT (i.e. the [[trigonometric function]] values), and it is not unusual for incautious FFT implementations to have much worse accuracy, e.g. if they use inaccurate [[generating trigonometric tables|trigonometric recurrence]] formulas. Some FFTs other than Cooley–Tukey, such as the Rader–Brenner algorithm, are intrinsically less stable.\n\nIn [[fixed-point arithmetic]], the finite-precision errors accumulated by FFT algorithms are worse, with rms errors growing as O({{radic|''N''}}) for the Cooley–Tukey algorithm (Welch, 1969).<ref name=\"Welch_1969\"/> Moreover, even achieving this accuracy requires careful attention to scaling to minimize loss of precision, and fixed-point FFT algorithms involve rescaling at each intermediate stage of decompositions like Cooley–Tukey.\n\nTo verify the correctness of an FFT implementation, rigorous guarantees can be obtained in O(''N''&nbsp;log&nbsp;''N'') time by a simple procedure checking the linearity, impulse-response, and time-shift properties of the transform on random inputs (Ergün, 1995).<ref name=\"Ergün_1995\"/>\n\n==Multidimensional FFTs==\n<!-- This section is linked from [[Discrete Fourier transform]] -->\n\nAs defined in the [[Discrete Fourier transform#Multidimensional DFT|multidimensional DFT]] article, the multidimensional DFT\n\n:<math>X_\\mathbf{k} = \\sum_{\\mathbf{n}=0}^{\\mathbf{N}-1} e^{-2\\pi i \\mathbf{k} \\cdot (\\mathbf{n} / \\mathbf{N})} x_\\mathbf{n}</math>\n\ntransforms an array ''x''<sub>'''n'''</sub> with a ''d''-dimensional [[coordinate vector|vector]] of indices <math>\\mathbf{n}=(n_1, \\ldots, n_d)</math> by a set of ''d'' nested summations (over <math>n_j = 0 \\ldots N_j-1</math> for each ''j''), where the division '''n'''/'''N''', defined as <math>\\mathbf{n} / \\mathbf{N} = (n_1/N_1, \\ldots, n_d/N_d)</math>, is performed element-wise.  Equivalently, it is the composition of a sequence of ''d'' sets of one-dimensional DFTs, performed along one dimension at a time (in any order).\n\nThis compositional viewpoint immediately provides the simplest and most common multidimensional DFT algorithm, known as the '''row-column''' algorithm (after the two-dimensional case, below). That is, one simply performs a sequence of ''d'' one-dimensional FFTs (by any of the above algorithms): first you transform along the ''n''<sub>1</sub> dimension, then along the ''n''<sub>2</sub> dimension, and so on (or actually, any ordering works). This method is easily shown to have the usual O(''N''&nbsp;log&nbsp;''N'') complexity, where <math>N = N_1 \\cdot N_2 \\cdot \\ldots \\cdot N_d</math> is the total number of data points transformed. In particular, there are ''N''/''N''<sub>1</sub> transforms of size ''N''<sub>1</sub>, etcetera, so the complexity of the sequence of FFTs is:\n\n:<math>\\begin{align}\n& \\frac{N}{N_1} O(N_1 \\log N_1) + \\cdots + \\frac{N}{N_d} O(N_d \\log N_d) \\\\[6pt]\n= {} & O\\left(N \\left[\\log N_1 + \\cdots + \\log N_d\\right]\\right) = O(N \\log N).\n\\end{align}</math>\n\nIn two dimensions, the ''x''<sub>'''k'''</sub> can be viewed as an <math>n_1 \\times n_2</math> [[matrix (mathematics)|matrix]], and this algorithm corresponds to first performing the FFT of all the rows (resp. columns), grouping the resulting transformed rows (resp. columns) together as another <math>n_1 \\times n_2</math> matrix, and then performing the FFT on each of the columns (resp. rows) of this second matrix, and similarly grouping the results into the final result matrix.\n\nIn more than two dimensions, it is often advantageous for [[Cache (computing)|cache]] locality to group the dimensions recursively.  For example, a three-dimensional FFT might first perform two-dimensional FFTs of each planar \"slice\" for each fixed ''n''<sub>1</sub>, and then perform the one-dimensional FFTs along the ''n''<sub>1</sub> direction. More generally, an [[asymptotically optimal]] [[cache-oblivious]] algorithm consists of recursively dividing the dimensions into two groups <math>(n_1, \\ldots, n_{d/2})</math> and <math>(n_{d/2+1}, \\ldots, n_d)</math> that are transformed recursively (rounding if ''d'' is not even) (see Frigo and Johnson, 2005).<ref name=\"Frigo_Johnson_2005\"/> Still, this remains a straightforward variation of the row-column algorithm that ultimately requires only a one-dimensional FFT algorithm as the base case, and still has O(''N''&nbsp;log&nbsp;''N'') complexity.  Yet another variation is to perform matrix [[transpose|transpositions]] in between transforming subsequent dimensions, so that the transforms operate on contiguous data; this is especially important for [[out-of-core]] and [[distributed memory]] situations where accessing non-contiguous data is extremely time-consuming.\n\nThere are other multidimensional FFT algorithms that are distinct from the row-column algorithm, although all of them have O(''N''&nbsp;log&nbsp;''N'') complexity. Perhaps the simplest non-row-column FFT is the [[vector-radix FFT algorithm]], which is a generalization of the ordinary Cooley–Tukey algorithm where one divides the transform dimensions by a vector <math>\\mathbf{r}=(r_1, r_2, \\ldots, r_d)</math> of radices at each step. (This may also have cache benefits.) The simplest case of vector-radix is where all of the radices are equal (e.g. vector-radix-2 divides ''all'' of the dimensions by two), but this is not necessary.  Vector radix with only a single non-unit radix at a time, i.e. <math>\\mathbf{r}=(1, \\ldots, 1, r, 1, \\ldots, 1)</math>, is essentially a row-column algorithm. Other, more complicated, methods include polynomial transform algorithms due to Nussbaumer (1977),<ref name=\"Nussbaumer_1977\"/> which view the transform in terms of convolutions and polynomial products. See Duhamel and Vetterli (1990)<ref name=\"Duhamel_Vetterli_1990\"/> for more information and references.\n\n==Other generalizations==\nAn O(''N''<sup>5/2</sup>log&nbsp;''N'') generalization to [[spherical harmonics]] on the sphere ''S''<sup>2</sup> with ''N''<sup>2</sup> nodes was described by Mohlenkamp,<ref name=\"Mohlenkamp_1999\"/> along with an algorithm conjectured (but not proven) to have O(''N''<sup>2</sup> log<sup>2</sup>(''N'')) complexity; Mohlenkamp also provides an implementation in the libftsh library.<ref name=\"libftsh\"/> A spherical-harmonic algorithm with O(''N''<sup>2</sup>log&nbsp;''N'') complexity is described by Rokhlin and Tygert.<ref name=\"Rokhlin_Tygert_2006\"/>\n\nThe [[fast folding algorithm]] is analogous to the FFT, except that it operates on a series of binned waveforms rather than a series of real or complex scalar values. Rotation (which in the FFT is multiplication by a complex phasor) is a circular shift of the component waveform.\n\nVarious groups have also published \"FFT\" algorithms for non-equispaced data, as reviewed in Potts ''et al.'' (2001).<ref name=\"Potts_Steidl_Tasche_2001\"/> Such algorithms do not strictly compute the DFT (which is only defined for equispaced data), but rather some approximation thereof (a [[non-uniform discrete Fourier transform]], or NDFT, which itself is often computed only approximately). More generally there are various other methods of [[spectral estimation]].\n\n==Applications==\nFFT's importance derives from the fact that in signal processing and image processing it has made working in frequency domain equally computationally feasible as working in temporal or spatial domain. Some of the important applications of FFT includes,<ref name=\"Rockmore_2000\"/><ref name=\"Chu_George_1999\"/>\n* Fast large integer and polynomial multiplication\n* Efficient matrix-vector multiplication for [[Toeplitz matrix|Toeplitz]], [[Circulant matrix|circulant]] and other structured matrices\n* Filtering algorithms (see [[Overlap-add method|overlap-add]] and [[Overlap-save method|overlap-save]] methods)\n* Fast algorithms for discrete cosine or sine transforms (example, [[Discrete cosine transform|Fast DCT]] used for JPEG, MP3/MPEG encoding)\n* Fast [[Chebyshev approximation]]\n* Fast discrete [[Hartley transform]]\n* Solving [[Recurrence relation|difference equation]]s\n* Computation of [[Mass spectrometry|isotopic distributions]].<ref name=\"Fernandez-de-Cossio_2012\"/>\n\n==Research areas==\n* '''Big FFTs''': With the explosion of big data in fields such as astronomy, the need for 512k FFTs has arisen for certain interferometry calculations. The data collected by projects such as [[Wilkinson Microwave Anisotropy Probe|WMAP]] and [[LIGO]] require FFTs of tens of billions of points. As this size does not fit into main memory, so called out-of-core FFTs are an active area of research.<ref name=\"Cormen_Nicol_1998\"/>\n* '''Approximate FFTs''': For applications such as MRI, it is necessary to compute DFTs for nonuniformly spaced grid points and/or frequencies. Multipole based approaches can compute approximate quantities with factor of runtime increase.<ref name=\"Dutt_Rokhlin_1993\"/>\n* '''[[Fourier transform on finite groups|Group FFTs]]''': The FFT may also be explained and interpreted using [[Group representation|group representation theory]] that allows for further generalization. A function on any compact group, including non cyclic, has an expansion in terms of a basis of irreducible matrix elements. It remains active area of research to find efficient algorithm for performing this change of basis. Applications including efficient [[spherical harmonics|spherical harmonic]] expansion, analyzing certain [[Markov process]]es, robotics etc.<ref name=\"Rockmore_2004\"/>\n* '''[[Quantum Fourier transform|Quantum FFTs]]''': Shor's fast algorithm for [[integer factorization]] on a quantum computer has a subroutine to compute DFT of a binary vector. This is implemented as sequence of 1- or 2-bit quantum gates now known as quantum FFT, which is effectively the Cooley–Tukey FFT realized as a particular factorization of the Fourier matrix. Extension to these ideas is currently being explored.\n\n==Language reference==\n{| class=\"wikitable\"\n!Language\n!Command/Method\n!Pre-requisites\n|-\n|[[R (programming language)|R]]\n|stats::fft(x)\n|None\n|-\n|[[Octave programming language|Octave]]/[[MATLAB]]\n|fft(x)\n|None\n|-\n|[[Python (programming language)|Python]]\n|fft.fft(x)\n|[[numpy]]\n|-\n|[[Wolfram Mathematica|Mathematica]]\n|Fourier[x]\n|None\n|-\n|[[Julia (programming language)|Julia]]\n|fft(A [,dims])\n|[[FFTW]]\n|}\n\n==See also==\nFFT-related algorithms:\n\n* [[Cooley–Tukey FFT algorithm]]\n* [[Prime-factor FFT algorithm]]\n* [[Bruun's FFT algorithm]]\n* [[Rader's FFT algorithm]]\n* [[Bluestein's FFT algorithm]]\n* [[Goertzel algorithm]] – Computes individual terms of discrete Fourier transform\n\nFFT implementations:\n* [[ALGLIB]] – C++ and C# library with real/complex FFT implementation.\n* [[FFTW]] \"Fastest Fourier Transform in the West\" – C library for the discrete Fourier transform (DFT) in one or more dimensions.\n* [https://github.com/anthonix/ffts FFTS] – The Fastest Fourier Transform in the South.\n* [[FFTPACK]] – another Fortran FFT library (public domain)\n* [[Math Kernel Library]]\n* [[cuFFT]] - FFT for GPU accelerated CUDA\n\nOther links:\n* [[Overlap add]]/[[Overlap save]] – efficient convolution methods using FFT for long signals\n* [[Odlyzko–Schönhage algorithm]] applies the FFT to finite [[Dirichlet series]].\n* [[Schönhage–Strassen algorithm]] - asymptotically fast multiplication algorithm for large integers\n* [[Butterfly diagram]] – a diagram used to describe FFTs.\n* [[Spectral music]] (involves application of FFT analysis to musical composition)\n* [[Spectrum analyzer]] – any of several devices that perform an FFT\n* [[Time series]]\n* [[Fast Walsh–Hadamard transform]]\n* [[Generalized distributive law]]\n* [[Multidimensional transform]]\n* [[Multidimensional discrete convolution]]\n* [[DFT matrix]]\n\n==References==\n{{Reflist|refs=\n<ref name=\"Loan_1992\">{{cite book |author-first=Charles |author-last=Van Loan |title=Computational Frameworks for the Fast Fourier Transform |publisher=[[SIAM]] |date=1992}}</ref>\n<ref name=\"Heideman_Johnson_Burrus_1984\">{{cite journal |author-last1=Heideman |author-first1=Michael T. |author-first2=Don H. |author-last2=Johnson |author-first3=Charles Sidney |author-last3=Burrus |author-link3=Charles Sidney Burrus |doi=10.1109/MASSP.1984.1162257 |title=Gauss and the history of the fast Fourier transform |journal=IEEE ASSP Magazine |volume=1 |issue=4 |pages=14–21 |date=1984 |url=http://www.cis.rit.edu/class/simg716/Gauss_History_FFT.pdf}}</ref>\n<ref name=\"Heideman_Burrus_1986\">{{cite journal |author-first1=Michael T. |author-last1=Heideman |author-first2=Charles Sidney |author-last2=Burrus |author-link2=Charles Sidney Burrus |date=1986 |doi=10.1109/TASSP.1986.1164785 |title=On the number of multiplications necessary to compute a length-2<sup>''n''</sup> DFT |journal=[[IEEE Transactions on Acoustics, Speech, and Signal Processing]] |volume=34 |issue=1 |pages=91–95}}</ref>\n<ref name=\"Dongarra_Sullivan_2000\">{{cite journal |title=Guest Editors Introduction to the top 10 algorithms |url=http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=814652 |journal=Computing in Science Engineering |date=January 2000 |issn=1521-9615 |pages=22–23 |volume=2 |issue=1 |doi=10.1109/MCISE.2000.814652 |author-first1=Jack |author-last1=Dongarra |author-first2=Francis |author-last2=Sullivan}}</ref>\n<ref name=\"Brenner_Rader_1976\">{{cite journal |author-first1=Norman M. |author-last1=Brenner |author-first2=Charles M. |author-last2=Rader |date=1976 |title=A New Principle for Fast Fourier Transformation |journal=[[IEEE Transactions on Acoustics, Speech, and Signal Processing]] |volume=24 |issue=3 |doi=10.1109/TASSP.1976.1162805 |pages=264–266}}</ref>\n<ref name=\"Kent_2002\">{{cite book |author-last1=Kent |author-first1=Ray D. |author-last2=Read |author-first2=Charles |date=2002 |title=Acoustic Analysis of Speech |isbn=0-7693-0112-6 |id={{ISBN|978-0-7693-0112-9}}}}</ref>\n<ref name=\"Strang_1994\">{{cite journal |author-last=Strang |author-first=Gilbert |author-link=Gilbert Strang |date=May–June 1994 |title=Wavelets |journal=[[American Scientist]] |volume=82 |issue=3 |pages=250–255 |jstor=29775194}}</ref>\n<ref name=\"Ergün_1995\">{{cite book |author-first=Funda |author-last=Ergün |date=1995 |doi=10.1145/225058.225167 |title=Testing multivariate linear functions: Overcoming the generator bottleneck |journal=Proceedings of the 27th ACM Symposium On the Theory of Computing |location=Kyoto, Japan |pages=407–416 |isbn=978-0897917186}}</ref>\n<ref name=\"Frigo_Johnson_2005\">{{cite journal |author-last1=Frigo |author-first1=Matteo |author-last2=Johnson |author-first2=Steven G. |date=2005 |title=The Design and Implementation of FFTW3 |url=http://fftw.org/fftw-paper-ieee.pdf |journal=[[Proceedings of the IEEE]] |volume=93 |issue=2 |pages=216–231 |doi=10.1109/jproc.2004.840301 |citeseerx=10.1.1.66.3097}}</ref>\n<ref name=\"Frigo_Johnson_2007\">{{cite journal |author-last1=Frigo |author-first1=Matteo |author-last2=Johnson |author-first2=Steven G. |url=http://ieeexplore.ieee.org/document/4034175/?reload=true |title=A Modified Split-Radix FFT With Fewer Arithmetic Operations |date=January 2007  |orig-year=2006-12-19 |journal=[[IEEE Transactions on Signal Processing]] |volume=55 |issue=1 |pages=111–119 |doi=10.1109/tsp.2006.882087 |citeseerx=10.1.1.582.5497}}</ref>\n<ref name=\"Duhamel_1990\">{{cite journal |author-first1=Pierre |author-last1=Duhamel |date=1990 |doi=10.1109/29.60070 |title=Algorithms meeting the lower bounds on the multiplicative complexity of length-2<sup>n</sup> DFTs and their connection with practical algorithms |journal=[[IEEE Transactions on Acoustics, Speech, and Signal Processing]] |volume=38 |issue=9 |pages=1504–1511}}</ref>\n<ref name=\"Duhamel_Vetterli_1990\">{{cite journal |author-first1=Pierre |author-last1=Duhamel |author-first2=Martin |author-last2=Vetterli |author-link2=Martin Vetterli |date=1990 |doi=10.1016/0165-1684(90)90158-U |title=Fast Fourier transforms: a tutorial review and a state of the art |journal=Signal Processing |volume=19 |issue=4 |pages=259–299 |url=http://infoscience.epfl.ch/record/59946}}</ref>\n<ref name=\"Edelman_McCorquodale_Toledo_1999\">{{cite journal |author-first1=Alan |author-last1=Edelman |author-first2=Peter |author-last2=McCorquodale |author-first3=Sivan |author-last3=Toledo |date=1999 |doi=10.1137/S1064827597316266 |title=The Future Fast Fourier Transform? |journal=[[SIAM Journal on Scientific Computing]] |volume=20 |issue=3 |pages=1094–1114 |url=http://www.cs.tau.ac.il/~stoledo/Bib/Pubs/pp97-fft.pdf |citeseerx=10.1.1.54.9339}}</ref>\n<ref name=\"Guo_Burrus_1996\">{{cite journal |author-first1=Haitao |author-last1=Guo |author-first2=Charles Sidney |author-last2=Burrus |author-link2=Charles Sidney Burrus |date=1996 |doi=10.1117/12.255236 |title=Fast approximate Fourier transform via wavelets transform |journal=[[Proceedings of SPIE]] |volume=2825 |pages=250–259 |citeseerx=10.1.1.54.3984 |series=Wavelet Applications in Signal and Image Processing IV}}</ref>\n<ref name=\"Shentov_Mitra_Heute_Hossen_1995\">{{cite journal |author-first1=Ognjan V. |author-last1=Shentov |author-first2=Sanjit K. |author-last2=Mitra |author-first3=Ulrich |author-last3=Heute |author-first4=Abdul N. |author-last4=Hossen |date=1995 |doi=10.1016/0165-1684(94)00103-7 |title=Subband DFT. I. Definition, interpretations and extensions |journal=Signal Processing |volume=41 |issue=3 |pages=261–277}}</ref>\n<ref name=\"Hassanieh_2012\">{{cite journal |author-first1=Haitham |author-last1=Hassanieh |author-first2=Piotr |author-last2=Indyk |author-link2=Piotr Indyk |author-first3=Dina |author-last3=Katabi |author-first4=Eric |author-last4=Price |url=http://www.mit.edu/~ecprice/papers/sparse-fft-soda.pdf |title=Simple and Practical Algorithm for Sparse Fourier Transform |journal=ACM-SIAM Symposium on Discrete Algorithms (SODA) |date=January 2012}} (NB. See also the [http://groups.csail.mit.edu/netmit/sFFT/ sFFT Web Page].)</ref>\n<ref name=\"Haynal_2011\">{{cite journal |author-first1=Steve |author-last1=Haynal |author-first2=Heidi |author-last2=Haynal |title=Generating and Searching Families of FFT Algorithms |journal=Journal on Satisfiability, Boolean Modeling and Computation |volume=7 |pages=145–187 |date=2011 |url=http://jsat.ewi.tudelft.nl/content/volume7/JSAT7_13_Haynal.pdf |dead-url=yes |archive-url=https://web.archive.org/web/20120426031804/http://jsat.ewi.tudelft.nl/content/volume7/JSAT7_13_Haynal.pdf |archive-date=2012-04-26}}</ref>\n<ref name=\"Lundy_Buskirk_2007\">{{cite journal |author-first1=Thomas J. |author-last1=Lundy |author-first2=James |author-last2=Van Buskirk |date=2007 |title=A new matrix approach to real FFTs and convolutions of length 2<sup>k</sup> |journal=[[Computing (journal)|Computing]] |volume=80 |issue=1 |pages=23–45 |doi=10.1007/s00607-007-0222-6}}</ref>\n<ref name=\"Papadimitriou_1979\">{{cite journal |author-first=Christos H. |author-last=Papadimitriou |date=1979 |doi=10.1145/322108.322118 |title=Optimality of the fast Fourier transform |journal=[[Journal of the ACM]] |volume=26 |pages=95–102}}</ref>\n<ref name=\"Sorensen_Jones_Heideman_Burrus_1987_1\">{{cite journal |author-first1=Henrik V. |author-last1=Sorensen |author-first2=Douglas L. |author-last2=Jones |author-link2=Douglas L. Jones |author-first3=Michael T. |author-last3=Heideman |author-first4=Charles Sidney |author-last4=Burrus |author-link4=Charles Sidney Burrus |date=1987 |doi=10.1109/TASSP.1987.1165220 |title=Real-valued fast Fourier transform algorithms |journal=[[IEEE Transactions on Acoustics, Speech, and Signal Processing]] |volume=35 |issue=6 |pages=849–863 |citeseerx=10.1.1.205.4523}}</ref>\n<ref name=\"Sorensen_Jones_Heideman_Burrus_1987_2\">{{cite journal |doi=10.1109/TASSP.1987.1165284 |author-last1=Sorensen |author-first1=Henrik V. |author-last2=Jones |author-first2=Douglas L. |author-link2=Douglas L. Jones |author-last3=Heideman |author-first3=Michael T. |author-last4=Burrus |author-first4=Charles Sidney |author-link4=Charles Sidney Burrus |date=1987 |pages=1353 |issue=9 |volume=35 |title=Corrections to \"Real-valued fast Fourier transform algorithms\" |journal=[[IEEE Transactions on Acoustics, Speech, and Signal Processing]]}}</ref>\n<ref name=\"Winograd_1978\">{{cite journal |author-first=Shmuel |author-last=Winograd |date=1978 |doi=10.1090/S0025-5718-1978-0468306-4 |title=On computing the discrete Fourier transform |journal=[[Mathematics of Computation]] |volume=32 |issue=141 |pages=175–199 |jstor=2006266 |pmc=430186 |pmid=16592303}}</ref>\n<ref name=\"Winograd_1979\">{{cite journal |author-first=Shmuel |author-last=Winograd |title=On the multiplicative complexity of the discrete Fourier transform |journal=[[Advances in Mathematics]] |volume=32 |issue=2 |date=1979 |pages=83–117 |doi=10.1016/0001-8708(79)90037-9}}</ref>\n<ref name=\"Morgenstern_1973\">{{cite journal |author-first1=Jacques |author-last1=Morgenstern |date=1973 |doi=10.1145/321752.321761 |title=Note on a lower bound of the linear complexity of the fast Fourier transform |journal=[[Journal of the ACM]] |volume=20 |issue=2 |pages=305–306}}</ref>\n<ref name=\"Mohlenkamp_1999\">{{cite journal |doi=10.1007/BF01261607 |author-first1=Martin J. |author-last1=Mohlenkamp |date=1999 |title=A Fast Transform for Spherical Harmonics |journal=Journal of Fourier Analysis and Applications<!-- J. Fourier Anal. Appl. --> |volume=5 |issue=2–3 |pages=159–184 |url=http://www.ohiouniversityfaculty.com/mohlenka/research/MOHLEN1999P.pdf |access-date=2018-01-11 |citeseerx=10.1.1.135.9830}}</ref>\n<ref name=\"Schatzman_1996\">{{cite journal |author-last1=Schatzman |author-first1=James C. |date=1996 |title=Accuracy of the discrete Fourier transform and the fast Fourier transform |url=http://portal.acm.org/citation.cfm?id=240432 |journal=[[SIAM Journal on Scientific Computing]] |volume=17 |issue=5 |pages=1150–1166 |doi=10.1137/s1064827593247023 |citeseerx=10.1.1.495.9184}}</ref>\n<ref name=\"Nussbaumer_1977\">{{cite journal |author-first1=Henri J. |author-last1=Nussbaumer |date=1977 |doi=10.1049/el:19770280 |title=Digital filtering using polynomial transforms |journal=[[Electronics Letters]] |volume=13 |issue=13 |pages=386–387}}</ref>\n<ref name=\"Pan_1986\">{{cite journal |author-first=Victor Ya. |author-last=Pan |date=1986-01-02 |doi=10.1016/0020-0190(86)90035-9 |title=The trade-off between the additive complexity and the asynchronicity of linear and bilinear algorithms |journal=[[Information Processing Letters]] |volume=22 |issue=1 |pages=11–14 |url=https://dl.acm.org/citation.cfm?id=8013 |access-date=2017-10-31 |dead-url=no}}</ref>\n<ref name=\"Potts_Steidl_Tasche_2001\">{{cite book |author-first1=Daniel |author-last1=Potts |author-first2=Gabriele |author-last2=Steidl |author-first3=Manfred |author-last3=Tasche |date=2001 |chapter-url=http://www.tu-chemnitz.de/~potts/paper/ndft.pdf |chapter=Fast Fourier transforms for nonequispaced data: A tutorial |editor-first1=J. J. |editor-last1=Benedetto |editor-first2=P. |editor-last2=Ferreira |title=Modern Sampling Theory: Mathematics and Applications |publisher=[[Birkhäuser]]}}</ref>\n<ref name=\"Rokhlin_Tygert_2006\">{{cite journal |author-first1=Vladimir |author-last1=Rokhlin |author-first2=Mark |author-last2=Tygert |date=2006 |title=Fast Algorithms for Spherical Harmonic Expansions |journal=[[SIAM Journal on Scientific Computing]] |volume=27 |issue=6 |doi=10.1137/050623073 |pages=1903–1928 |url=http://tygert.com/sph2.pdf |access-date=2014-09-18 |citeseerx=10.1.1.125.7415}} [http://www.cs.yale.edu/publications/techreports/tr1309.pdf]</ref>\n<ref name=\"Welch_1969\">{{cite journal |author-first1=Peter D. |author-last1=Welch |date=1969 |doi=10.1109/TAU.1969.1162035 |title=A fixed-point fast Fourier transform error analysis |journal=[[IEEE Transactions on Audio and Electroacoustics]] |volume=17 |issue=2 |pages=151–157}}</ref>\n<ref name=\"Gauss_1866\">{{cite book |author-first=Carl Friedrich |author-last=Gauss |author-link=Carl Friedrich Gauss |chapter-url=https://babel.hathitrust.org/cgi/pt?id=uc1.c2857678;view=1up;seq=279 |title=Nachlass |chapter=Theoria interpolationis methodo nova tractata |type=Unpublished manuscript |trans-chapter=Theory regarding a new method of interpolation |series=Werke |location=Göttingen, Germany |publisher=Königlichen Gesellschaft der Wissenschaften zu Göttingen |date=1866 |volume=3 |pages=265–303 |language=Latin, German}}</ref>\n<ref name=\"Heideman_Johnson_Burrus_1985\">{{cite journal |title=Gauss and the history of the fast Fourier transform |journal=Archive for History of Exact Sciences |date=1985-09-01 |issn=0003-9519 |pages=265–277 |volume=34 |issue=3 |doi=10.1007/BF00348431 |author-first1=Michael T. |author-last1=Heideman |author-first2=Don H. |author-last2=Johnson |author-first3=Charles Sidney |author-last3=Burrus |author-link3=Charles Sidney Burrus |citeseerx=10.1.1.309.181}}</ref>\n<ref name=\"Yates_1937\">{{cite journal |title=The design and analysis of factorial experiments |author-last=Yates |author-first=Frank |author-link=Frank Yates |date=1937 |journal=Technical Communication no. 35 of the Commonwealth Bureau of Soils}}</ref>\n<ref name=\"Cooley_Lewis_Welch_1967\">{{cite journal |title=Historical notes on the fast Fourier transform |url=http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1161903 |journal=[[IEEE Transactions on Audio and Electroacoustics]] |date=June 1967 |issn=0018-9278 |pages=76–79 |volume=15 |issue=2 |doi=10.1109/TAU.1967.1161903 |author-first1=James W. |author-last1=Cooley |author-link1=James Cooley |author-first2=Peter A. W. |author-last2=Lewis |author-first3=Peter D. |author-last3=Welch |citeseerx=10.1.1.467.7209}}</ref>\n<ref name=\"Cooley_Tukey_1965\">{{cite journal |title=An algorithm for the machine calculation of complex Fourier series |url=http://www.ams.org/mcom/1965-19-090/S0025-5718-1965-0178586-1/ |journal=[[Mathematics of Computation]] |date=1965 |issn=0025-5718 |pages=297–301 |volume=19 |issue=90 |doi=10.1090/S0025-5718-1965-0178586-1 |author-first1=James W. |author-last1=Cooley |author-link1=James Cooley |author-first2=John W. |author-last2=Tukey |author-link2=John Tukey}}</ref>\n<ref name=\"Danielson_Lanczos_1942\">{{cite journal |title=Some improvements in practical Fourier analysis and their application to x-ray scattering from liquids |author-first1=Gordon C. |author-last1=Danielson |author-link1=Gordon C. Danielson |author-first2=Cornelius |author-last2=Lanczos |author-link2=Cornelius Lanczos |date=1942 |journal=Journal of the Franklin Institute |doi=10.1016/S0016-0032(42)90767-1 |volume=233 |issue=4 |pages=365–380}}</ref>\n<ref name=\"Lanczos_1956\">{{cite book |author-first=Cornelius |author-last=Lanczos |author-link=Cornelius Lanczos |title=Applied Analysis |date=1956 |publisher=[[Prentice–Hall]]}}</ref>\n<ref name=\"Fernandez-de-Cossio_2012\">{{cite journal |author-last1=Fernandez-de-Cossio Diaz |author-first1=Jorge |author-last2=Fernandez-de-Cossio |author-first2=Jorge |date=2012-08-08 |title=Computation of Isotopic Peak Center-Mass Distribution by Fourier Transform |journal=Analytical Chemistry<!-- Anal. Chem. --> |volume=84 |issue=16 |pages=7052–7056 |doi=10.1021/ac301296a |pmid=22873736 |issn=0003-2700}}</ref>\n<ref name=\"Chu_George_1999\">{{cite book |title=Inside the FFT Black Box: Serial and Parallel Fast Fourier Transform Algorithms |author-last1=Chu |author-first1=Eleanor |author-last2=George |author-first2=Alan |publisher=[[CRC Press]] |isbn=978-1-42004996-1 |pages=153–168 |orig-year=1999-11-11 |chapter=Chapter 16 |date=1999-11-11}}</ref>\n<ref name=\"Rockmore_2000\">{{cite journal |title=The FFT: an algorithm the whole family can use |url=http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=814659 |journal=Computing in Science Engineering |date=January 2000 |issn=1521-9615 |pages=60–64 |volume=2 |issue=1 |doi=10.1109/5992.814659 |author-first=Daniel N. |author-last=Rockmore |citeseerx=10.1.1.17.228}}</ref>\n<ref name=\"Rockmore_2004\">{{cite book |title=Recent Progress and Applications in Group FFTs |publisher=Springer Netherlands |journal=Computational Noncommutative Algebra and Applications |date=2004 |isbn=978-1-4020-1982-1 |pages=227–254 |series=NATO Science Series II: Mathematics, Physics and Chemistry |volume=136 |author-first=Daniel N. |author-last=Rockmore |editor-first=Jim |editor-last=Byrnes |doi=10.1007/1-4020-2307-3_9 |citeseerx=10.1.1.324.4700}}</ref>\n<ref name=\"Gentleman_Sande_1966\">{{cite journal |author-last1=Gentleman |author-first1=W. Morven |author-last2=Sande |author-first2=G. |date=1966 |title=Fast Fourier transforms—for fun and profit |journal=[[Proceedings of the AFIPS]] |volume=29 |pages=563–578 |doi=10.1145/1464291.1464352}}</ref>\n<ref name=\"Gauss_1805\">{{cite book |author-first=Carl Friedrich |author-last=Gauss |author-link=Carl Friedrich Gauss |date=1866 |orig-year=1805 |url=https://www.deutsche-digitale-bibliothek.de/item/VAK3TPRSI57PWW6OSOSWAU52CN7CNRJP |title=Theoria interpolationis methodo nova tractata |series=Werke |volume=3 |pages=265–327 |location=Göttingen, Germany |publisher=Königliche Gesellschaft der Wissenschaften |language=Latin, German}}</ref>\n<ref name=\"Cooley_1987\">{{cite book |author-first=James W. |author-last=Cooley |author-link=James W. Cooley |url=https://carma.newcastle.edu.au/jon/Preprints/Talks/CARMA-CE/FFT.pdf |title=The Re-Discovery of the Fast Fourier Transform Algorithm |work=Microchimica Acta |location=Vienna, Austria |date=1987 |volume=III |pages=33–45}}</ref>\n<ref name=\"Garwin_1969\">{{cite journal |author-first=Richard |author-last=Garwin |url=https://fas.org/rlg/690600-fft.pdf |title=The Fast Fourier Transform As an Example of the Difficulty in Gaining Wide Use for a New Technique |journal=[[IEEE Transactions on Audio and Electroacoustics]] |volume=AU-17 |issue=2 |date=June 1969 |pages=68–72}}</ref>\n<ref name=\"libftsh\">[http://www.math.ohiou.edu/~mjm/research/libftsh.html libftsh library]</ref>\n<ref name=\"Cormen_Nicol_1998\">{{cite journal |url=http://historical.ncstrl.org/tr/pdf/icase/TR-96-70.pdf |title=Performing out-of-core FFTs on parallel disk systems |author-first1=Thomas H. |author-last1=Cormen |author-first2=David M. |author-last2=Nicol |date=1998 |journal=Parallel Computing |doi=10.1016/S0167-8191(97)00114-2 |volume=24 |issue=1 |pages=5–20 |citeseerx=10.1.1.44.8212}}</ref>\n<ref name=\"Dutt_Rokhlin_1993\">{{cite journal |title=Fast Fourier Transforms for Nonequispaced Data |journal=[[SIAM Journal on Scientific Computing]] |date=1993-11-01 |issn=1064-8275 |pages=1368–1393 |volume=14 |issue=6 |doi=10.1137/0914081 |author-first1=Alok |author-last1=Dutt |author-first2=Vladimir |author-last2=Rokhlin}}</ref>\n}}\n\n==Further reading==\n* {{cite document |author-first=E. Oran |author-last=Brigham |title=The Fast Fourier Transform |publication-place=New York, USA |publisher=[[Prentice-Hall]] |date=2002}}\n* {{cite book |author-first1=Thomas H. |author-last1=Cormen |author-link1=Thomas H. Cormen |author-first2=Charles E. |author-last2=Leiserson |author-link2=Charles E. Leiserson |author-first3=Ronald L. |author-last3=Rivest |author-link3=Ronald L. Rivest |author-first4=Clifford |author-last4=Stein |author-link4=Clifford Stein |date=2001 |title=Introduction to Algorithms |title-link=Introduction to Algorithms |edition=2 |publisher=[[MIT Press]] / [[McGraw-Hill]] |isbn=0-262-03293-7 |id={{ISBN|978-0-262-03293-3}} |chapter=Chapter 30: Polynomials and the FFT}}\n* {{cite book |author-first1=Douglas F. |author-last1=Elliott |author-first2=K. Ramamohan |author-last2=Rao |date=1982 |title=Fast transforms: Algorithms, analyses, applications |location=New York, USA |publisher=[[Academic Press]]}}\n* {{cite book |author-first1=Haitao |author-last1=Guo |author-first2=Gary A. |author-last2=Sitton |author-first3=Charles Sidney |author-last3=Burrus |author-link3=Charles Sidney Burrus |date=1994 |doi=10.1109/ICASSP.1994.389994 |title=The Quick Discrete Fourier Transform |journal=[[Proceedings on the IEEE Conference on Acoustics, Speech, and Signal Processing]] ([[ICASSP]]) |volume=3 |pages=445–448 |isbn=978-0-7803-1775-8}}\n* {{cite journal |author-last1=Johnson |author-first1=Steven G. |author-last2=Frigo |author-first2=Matteo |date=2007 |title=A modified split-radix FFT with fewer arithmetic operations |url=http://www.fftw.org/newsplit.pdf |journal=[[IEEE Transactions on Signal Processing]] |volume=55 |issue=1 |pages=111–119 |doi=10.1109/tsp.2006.882087 |citeseerx=10.1.1.582.5497}}\n* {{cite book |author-last1=Press |author-first1=William H. |author-last2=Teukolsky |author-first2=Saul A. |author-last3=Vetterling |author-first3=William T. |author-last4=Flannery |author-first4=Brian P. |date=2007 |title=Numerical Recipes: The Art of Scientific Computing |edition=3 |publisher=[[Cambridge University Press]] |publication-place=New York, USA |isbn=978-0-521-88068-8 |chapter=Chapter 12. Fast Fourier Transform |chapter-url=http://apps.nrbook.com/empanel/index.html#pg=600}}\n* {{cite book |author-first=Richard Collom |author-last=Singleton |chapter=A Short Bibliography on the Fast Fourier Transform |title=Special Issue on Fast Fourier Transform |journal=[[IEEE Transactions on Audio and Electroacoustics]] |publisher=IEEE Audio and Electroacoustics Group |volume=AU-17 |number=2 |date=June 1969 |pages=166–169 |chapter-url=http://ieeexplore.ieee.org/stamp/stamp.jsp?reload=true&tp=&arnumber=1162029 |access-date=2017-10-31 |dead-url=no}} (NB. Contains extensive bibliography.)\n\n==External links==\n* [http://www.cs.pitt.edu/~kirk/cs1501/animations/FFT.html Fast Fourier Algorithm]\n* ''[http://cnx.org/content/col10550/ Fast Fourier Transforms]'', [[OpenStax CNX|Connexions]] online book edited by Charles Sidney Burrus, with chapters by Charles Sidney Burrus, Ivan Selesnick, Markus Pueschel, Matteo Frigo, and [[Steven G. Johnson]] (2008).\n* [http://www.fftw.org/links.html Links to FFT code and information online.]\n* [https://web.archive.org/web/20100430161231/http://www.cmlab.csie.ntu.edu.tw/cml/dsp/training/coding/transform/fft.html National Taiwan University – FFT]\n* [http://www.librow.com/articles/article-10 FFT programming in C++ — Cooley–Tukey algorithm.]\n* [http://www.jjj.de/fxt/ Online documentation, links, book, and code.]\n* [http://www.vosesoftware.com/ModelRiskHelp/index.htm#Aggregate_distributions/Aggregate_modeling_-_Fast_Fourier_Transform_FFT_method.htm Using FFT to construct aggregate probability distributions]\n* Sri Welaratna, \"[http://www.dataphysics.com/30_Years_of_FFT_Analyzers_by_Sri_Welaratna.pdf Thirty years of FFT analyzers]\", ''Sound and Vibration'' (January 1997, 30th anniversary issue).  A historical review of hardware FFT devices.\n* [http://www.virtins.com/doc/D1002/FFT_Basics_and_Case_Study_using_Multi-Instrument_D1002.pdf FFT Basics and Case Study Using Multi-Instrument]\n* [http://numericalmethods.eng.usf.edu/topics/fft.html FFT Textbook notes, PPTs, Videos] at Holistic Numerical Methods Institute.\n* [http://www.alglib.net/fasttransforms/fft.php ALGLIB FFT Code] GPL Licensed multilanguage (VBA, C++, Pascal, etc.) numerical analysis and data processing library.\n* [http://groups.csail.mit.edu/netmit/sFFT/ MIT's sFFT] MIT Sparse FFT algorithm and implementation.\n* [https://web.archive.org/web/20130928020959/http://www.borgdesign.ro/fft.zip VB6 FFT] VB6 optimized library implementation with source code.\n* [http://www.etti.unibw.de/labalive/experiment/fft/ Fast Fourier transform illustrated] Demo examples and FFT calculator.\n* [http://www.akiti.ca/FourierTransform.html Discrete Fourier Transform (Forward)] A JavaScript implementation of FFTPACK code by Swarztrauber.\n* [https://www.slideshare.net/akliluw/fft-23703437 Fourier Transforms of Discrete Signals (Microlink IT College)]\n* [https://www.karlsims.com/fft.html Interactive FFT Tutorial] A visual interactive intro to Fourier transforms and FFT methods.\n\n[[Category:FFT algorithms]]\n[[Category:Digital signal processing]]\n[[Category:Discrete transforms]]"
    },
    {
      "title": "FFTPACK",
      "url": "https://en.wikipedia.org/wiki/FFTPACK",
      "text": "{{Infobox Software\n| name = FFTPACK\n| logo = \n| screenshot =\n| caption =\n| author =\n| developer = Paul N. Swarztrauber\n| released = April 1985\n| latest release version = \n| latest release date = \n| latest preview version = \n| latest preview date = \n| operating system =\n| platform =\n| programming_language = [[Fortran]]\n| genre = [[Numerical software]]\n| license = [[public domain]]\n| website = [http://www.netlib.org/fftpack/ www.netlib.org/fftpack/]\n}}\n\n'''FFTPACK''' is a package of [[Fortran]] [[subroutine]]s for the [[fast Fourier transform]].  It includes [[complex number|complex]], [[real number|real]], [[sine]], [[cosine]], and quarter-wave transforms. It was developed by Paul Swarztrauber of the [[National Center for Atmospheric Research]], and is included in the general-purpose mathematical library [[SLATEC]].\n\nMuch of the package is also available in [[C (programming language)|C]] and [[Java (programming language)|Java]] translations.\n\n== References ==\n* http://www.netlib.org/fftpack/\n* P.N. Swarztrauber (1982). Vectorizing the FFTs. pp.&nbsp;51–83 in: ''Parallel Computations'' (ed. G. Rodrigue), [[Academic Press]]. {{ISBN|978-0-12-592101-5}}\n* [http://sites.google.com/site/piotrwendykier/software/jtransforms JTransforms - open source, multithreaded FFT library written in pure Java]\n\n== See also ==\n* [[FFTW]]\n* [[LAPACK]]\n\n[[Category:Computer libraries]]\n[[Category:FFT algorithms]]\n[[Category:Public-domain software with source code]]\n\n{{compu-library-stub}}"
    },
    {
      "title": "FFTW",
      "url": "https://en.wikipedia.org/wiki/FFTW",
      "text": "{{third-party|date=April 2018}}\n{{Infobox Software\n| name = FFTW\n| logo = Fftw-logo-med.gif\n| logo caption = The FFTW logo\n| screenshot =\n| caption =\n| author =\n| developer = Matteo Frigo and [[Steven G. Johnson]]\n| released = {{Start date|1997|03|24|df=yes}}\n| latest release version = 3.3.8\n| latest release date = {{Start date and age|2018|05|28|df=yes}}\n| operating system =\n| platform =\n| programming_language = [[C (programming language)|C]], [[OCaml]]\n| genre = [[Numerical software]]\n| license = [[GPL]], commercial\n}}\nThe '''Fastest Fourier Transform in the West''' ('''FFTW''') is a software [[library (computer science)|library]] for computing [[discrete Fourier transform]]s (DFTs) developed by Matteo Frigo and [[Steven G. Johnson]] at the [[Massachusetts Institute of Technology]].<ref name=\"Frigo2005\">{{cite journal |vauthors=Frigo M, Johnson SG |url=http://www.fftw.org/fftw-paper-ieee.pdf |title=The design and implementation of FFTW3 |journal=Proceedings of the IEEE |volume=93 |issue=2 |date=February 2005 |pages=216–231 |doi=10.1109/JPROC.2004.840301|citeseerx=10.1.1.66.3097 }}</ref><ref name=\"Frigo1998\">{{cite book |vauthors=Frigo M, Johnson SG |title=FFTW: an adaptive software architecture for the FFT |journal=Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing |volume=3 |pages=1381–1384 |year=1998 |url=http://ieeexplore.ieee.org/stamp/stamp.jsp&arnumber=681704&isnumber=14979 |doi=10.1109/ICASSP.1998.681704|isbn=978-0-7803-4428-0 |citeseerx=10.1.1.47.8661 }}</ref><ref name=\"Johnson08\">{{cite book |vauthors=Johnson SG, Frigo M |last-author-amp=yes |chapter-url=http://cnx.org/content/m16336/ |chapter=ch.11: Implementing FFTs in practice |title=Fast Fourier Transforms |editor=C. S. Burrus |publisher=Rice University |location=Houston TX: Connexions |date=September 2008}}</ref>\n\nFFTW is known as the fastest [[free software]] implementation of the [[fast Fourier transform]] (FFT) (upheld by regular [[benchmark (computing)|benchmarks]]<ref>Homepage, second paragraph [http://www.fftw.org/], and benchmarks page [http://www.fftw.org/benchfft/]</ref>). Like many other implementations, it can compute transforms of real and [[complex number|complex]]-valued arrays of arbitrary size and dimension in [[Big O notation|O]]([[Linearithmic function|''n''&nbsp;log&nbsp;''n'']]) time.\n\n==Library==\nIt does this by supporting a variety of algorithms and choosing the one (a particular decomposition of the transform into smaller transforms) it [[heuristic (computer science)|estimates]] or measures to be preferable in the particular circumstances. It works best on arrays of sizes with small [[prime factor]]s, with [[power of two|powers of two]] being optimal and large [[prime number|prime]]s being worst case (but still [[Big O notation|O]]([[Linearithmic function|n log n]])). To decompose transforms of [[composite number|composite]] sizes into smaller transforms, it chooses among several variants of the [[Cooley–Tukey FFT algorithm]] (corresponding to different factorizations and/or different memory-access patterns), while for prime sizes it uses either [[Rader's FFT algorithm|Rader's]] or [[Bluestein's FFT algorithm]].<ref name=\"Frigo2005\"/> Once the transform has been broken up into subtransforms of sufficiently small sizes, FFTW uses [[hard-coded]] [[Loop unwinding|unrolled]] FFTs for these small sizes that were produced (at [[compile time]], not at [[run time (program lifecycle phase)|run time]]) by [[automatic programming|code generation]]; these routines use a variety of algorithms including Cooley–Tukey variants, Rader's algorithm, and [[prime-factor FFT algorithm]]s.<ref name=\"Frigo2005\"/>\n\nFor a sufficiently large number of repeated transforms it is advantageous to measure the performance of some or all of the supported algorithms on the given array size and [[platform (computing)|platform]]. These measurements, which the authors refer to as \"wisdom\", can be stored in a file or string for later use.\n\nFFTW has a \"guru interface\" that intends \"to expose as much as possible of the flexibility in the underlying FFTW architecture\". This allows, among other things, multi-dimensional transforms and multiple transforms in a single call (e.g., where the data is interleaved in memory).\n\nFFTW has limited support for ''out-of-order transforms'' (using the [[Message Passing Interface]] (MPI) version). The [[Cooley–Tukey FFT algorithm#Data reordering, bit reversal, and in-place algorithms|data reordering]] incurs an overhead, which for in-place transforms of arbitrary size and dimension is non-trivial to avoid. It is undocumented for which transforms this overhead is significant.\n\nFFTW is licensed under the [[GNU General Public License]]. It is also licensed commercially (for a cost of up to $12,500) by [[MIT]]<ref>{{Cite web | url=http://technology.mit.edu/technologies/15054_fastest-fourier-transform-in-the-west-fftw-version-3-3-x-fftw-v-3-3-x | title=View Technologies &#124; MIT Technology Licensing Office}}</ref> and is used in the commercial [[MATLAB]]<ref>[http://www.mathworks.com/company/newsletters/articles/faster-finite-fourier-transforms-matlab.html Faster Finite Fourier Transforms: MATLAB 6 incorporates FFTW]</ref> matrix package for calculating FFTs. FFTW is written in the [[C (programming language)|C]] language, but [[Fortran]] and [[Ada (programming language)|Ada]] interfaces exist, as well as interfaces for a few other languages. While the library itself is C, the code is actually generated from a program called '<code>genfft</code>', which is written in [[OCaml]].<ref name=\"FFTW FAQ\">[http://www.fftw.org/faq/section2.html#languages \"FFTW FAQ\"]</ref>\n\nIn 1999, FFTW won the [[J. H. Wilkinson Prize for Numerical Software]].\n\n==See also==\n{{Portal|Free and open-source software}}\n* [[FFTPACK]]\n\n==References==\n{{Reflist|2}}\n\n==External links==\n* {{Official website}}\n\n{{DEFAULTSORT:Fftw}}\n[[Category:Numerical libraries]]\n[[Category:FFT algorithms]]\n[[Category:OCaml software]]\n[[Category:Free mathematics software]]\n[[Category:Massachusetts Institute of Technology software]]"
    },
    {
      "title": "Goertzel algorithm",
      "url": "https://en.wikipedia.org/wiki/Goertzel_algorithm",
      "text": "The '''Goertzel algorithm''' is a technique in [[digital signal processing]] (DSP) for efficient evaluation of the individual terms of the [[discrete Fourier transform]] (DFT). It is useful in certain practical applications, such as recognition of [[dual-tone multi-frequency signaling]] (DTMF) tones produced by the push buttons of the keypad of a traditional analog [[telephone]]. The algorithm was first described by [[Gerald Goertzel]] in 1958.<ref>{{Citation |first=G. |last=Goertzel |date=January 1958 |title= An Algorithm for the Evaluation of Finite Trigonometric Series |journal= American Mathematical Monthly |volume=65 |issue=1 |pages=34&ndash;35 |doi=10.2307/2310304 |jstor=2310304 }}</ref>\n\nLike the DFT, the Goertzel algorithm analyses one selectable frequency component from a [[discrete signal]].<ref>{{Citation |last=Mock |first=P. |url=http://focus.ti.com/lit/an/spra168/spra168.pdf |title=Add DTMF Generation and Decoding to DSP-μP Designs |journal=EDN |date=March 21, 1985 |issn=0012-7515 |doi= }}; also found in DSP Applications with the TMS320 Family, Vol. 1, Texas Instruments, 1989.</ref><ref>{{Citation |first=Chiouguey J. |last=Chen |url=http://focus.ti.com/lit/an/spra066/spra066.pdf |title=Modified Goertzel Algorithm in DTMF Detection Using the TMS320C80 DSP| publisher=Texas Instruments |series=Application Report |id=SPRA066 |date=June 1996 |doi=}}</ref><ref>{{Citation |last=Schmer |first=Gunter |url=http://focus.ti.com/lit/an/spra096a/spra096a.pdf |title=DTMF Tone Generation and Detection: An Implementation Using the TMS320C54x |publisher=Texas Instruments |series=Application Report |id=SPRA096a |date= May 2000 |doi= }}</ref>  Unlike direct DFT calculations, the Goertzel algorithm applies a single real-valued coefficient at each iteration, using real-valued arithmetic for real-valued input sequences. For covering a full spectrum, the Goertzel algorithm has a [[Computational complexity theory|higher order of complexity]] than [[fast Fourier transform]] (FFT) algorithms, but for computing a small number of selected frequency components, it is more numerically efficient. The simple structure of the Goertzel algorithm makes it well suited to small processors and embedded applications.\n\nThe Goertzel algorithm can also be used \"in reverse\" as a sinusoid synthesis function, which requires only 1 multiplication and 1 subtraction per generated sample.<ref>http://haskell.cs.yale.edu/wp-content/uploads/2011/01/AudioProc-TR.pdf.</ref>\n\n== The algorithm ==\n{{cleanup|section|reason=inconsistent styles between equations and labels|date=February 2014}}\nThe main calculation in the Goertzel algorithm has the form of a [[digital filter]], and for this reason the algorithm is often called a ''Goertzel filter''. The filter operates on an input sequence <math>x[n]</math> in a cascade of two stages with a parameter <math>\\omega_0</math>, giving the frequency to be analysed, normalised to [[radian]]s per sample.\n\nThe first stage calculates an intermediate sequence, <math>s[n]</math>:\n{{NumBlk|:|<math> s[n] = x[n] + 2 \\cos(\\omega_0) s[n-1] - s[n-2]. </math>|1}}\nThe second stage applies the following filter to <math>s[n]</math>, producing output sequence <math>y[n]</math>:\n{{NumBlk|:|<math>y[n] = s[n] - e^{-j \\omega_0} s[n-1].</math>|2}}\n\nThe first filter stage can be observed to be a second-order [[Infinite impulse response|IIR filter]] with a [[Digital filter#Direct form I|direct-form]] structure. This particular structure has the property that its internal state variables equal the past output values from that stage. Input values <math>x[n]</math> for <math>n < 0</math> are presumed all equal to 0. To establish the initial filter state so that evaluation can begin at sample <math>x[0]</math>, the filter states are assigned initial values <math>s[-2] = s[-1] = 0</math>. To avoid [[aliasing]] hazards, frequency <math>\\omega_0</math> is often restricted to the range 0 to π (see [[Nyquist–Shannon sampling theorem]]); using a value outside this range is not meaningless, but is equivalent to using an aliased frequency inside this range, since the exponential function is periodic with a period of 2π in <math>\\omega_0</math>.\n\nThe second-stage filter can be observed to be a [[Finite impulse response|FIR filter]], since its calculations do not use any of its past outputs.\n\n[[Z-transform]] methods can be applied to study the properties of the filter cascade. The Z transform of the first filter stage given in equation (1) is\n{{NumBlk|:|<math>\\begin{align}\n \\frac{S(z)}{X(z)} &= \\frac{1}{1 - 2 \\cos(\\omega_0) z^{-1} + z^{-2}} \\\\\n  & = \\frac{1}{(1 - e^{+j \\omega_0} z^{-1})(1 - e^{-j \\omega_0} z^{-1})}.\n\\end{align}</math>|3}}\nThe Z transform of the second filter stage given in equation (2) is\n{{NumBlk|:|<math>\\frac{Y(z)}{S(z)} = 1 - e^{-j \\omega_0}z^{-1}.</math>|4}}\nThe combined transfer function of the cascade of the two filter stages is then  \n{{NumBlk|:|<math>\\begin{align} \n \\frac{S(z)}{X(z)} \\frac{Y(z)}{S(z)} = \\frac{Y(z)}{X(z)} &= \\frac{(1 - e^{-j \\omega_0}z^{-1})}{(1 - e^{+j \\omega_0} z^{-1})(1 - e^{-j \\omega_0} z^{-1})} \\\\\n &= \\frac{1}{1 - e^{+j \\omega_0} z^{-1}}.\n\\end{align}</math>|5}}\nThis can be transformed back to an equivalent time-domain sequence, and the terms unrolled back to the first input term at index <math>n = 0</math>:{{citation needed|date=February 2014}}\n{{NumBlk|:|<math> \\begin{align}\n y[n] &= x[n] + e^{+j \\omega_0} y[n-1] \\\\\n  &= \\sum_{k=-\\infty}^{n}x[k] e^{+j \\omega_0 (n-k)} \\\\ \n  &= e^{j \\omega_0 n} \\sum_{k=0}^{n} x[k] e^{-j \\omega_0 k} \\qquad \\text{since }\\forall k < 0, x[k] = 0.\n\\end{align} </math>|6}}\n\n== Numerical stability ==\nIt can be observed that the [[Pole (complex analysis)|poles]] of the filter's [[Z transform]] are located at <math>e^{+j \\omega_0}</math> and <math>e^{-j \\omega_0}</math>, on a circle of unit radius centered on the origin of the complex Z-transform plane. This property indicates that the filter process is [[Marginal stability|marginally stable]] and vulnerable to [[Numerical stability|numerical-error accumulation]] when computed using low-precision arithmetic and long input sequences.<ref>{{cite journal|last1=Gentleman|first1=W. M.|title=An error analysis of Goertzel's (Watt's) method for computing Fourier coefficients|journal=The Computer Journal|date=1 February 1969|volume=12|issue=2|pages=160–164|doi=10.1093/comjnl/12.2.160|url=http://comjnl.oxfordjournals.org/content/12/2/160.full.pdf|accessdate=28 December 2014}}</ref> A numerically stable version was proposed by Christian Reinsch.<ref>{{Citation |first1=J. |last1=Stoer |first2=R. |last2=Bulirsch |date=2002 |title= Introduction to Numerical Analysis |journal= Springer }}</ref>\n\n== DFT computations ==\nFor the important case of computing a DFT term, the following special restrictions are applied.\n* The filtering terminates at index <math>n = N</math>, where <math>N</math> is the number of terms in the input sequence of the DFT.\n* The frequencies chosen for the Goertzel analysis are restricted to the special form \n{{NumBlk|::|<math>\\omega_0 = 2 \\pi \\frac{k}{N}.</math>|7}}\n* The index number <math>k</math> indicating the \"frequency bin\" of the DFT is selected from the set of index numbers\n{{NumBlk|::|<math>k \\in \\{ 0, 1, 2, ..., N-1 \\}. </math>|8}}\n\nMaking these substitutions into equation (6) and observing that the term <math>e^{+j 2 \\pi k} = 1</math>, equation (6) then takes the following form:\n{{NumBlk|:|<math>y[N] =  \\sum_{n=0}^{N} x[n]e^{-j 2 \\pi \\frac{n k}{N}}. </math>|9}}\n\nWe can observe that the right side of equation (9) is extremely similar to the defining formula for DFT term <math>X[k]</math>, the DFT term for index number <math>k</math>, but not exactly the same. The summation shown in equation (9) requires <math>N+1</math> input terms, but only <math>N</math> input terms are available when evaluating a DFT. A simple but inelegant expedient is to extend the input sequence <math>x[n]</math> with one more artificial value <math>x[N] = 0</math>.<ref>{{cite web|url=http://cnx.org/content/m12024/latest/ |title=Goertzel's Algorithm |publisher=Cnx.org |date=2006-09-12 |accessdate=2014-02-03}}</ref>  We can see from equation (9) that the mathematical effect on the final result is the same as removing term <math>x[N]</math> from the summation, thus delivering the intended DFT value.\n\nHowever, there is a more elegant approach that avoids the extra filter pass. From equation (1), we can note that when the extended input term <math>x[N] = 0</math> is used in the final step, \n{{NumBlk|:|<math>s[N] =  2 \\cos(\\omega_0) s[N-1] - s[N-2].</math>|10}}\nThus, the algorithm can be completed as follows:\n* terminate the IIR filter after processing input term <math>x[N-1]</math>,\n* apply equation (10) to construct <math>s[N]</math> from the prior outputs <math>s[N-1]</math> and <math>s[N-2]</math>,\n* apply equation (2) with the calculated <math>s[N]</math> value and with <math>s[N-1]</math> produced by the final direct calculation of the filter.\n\nThe last two mathematical operations are simplified by combining them algebraically:\n{{NumBlk|:|<math>\\begin{align}\n y[N] & = s[N] - e^{-j 2 \\pi \\frac{k}{N}} s[N-1] \\\\\n  & = (2 \\cos(\\omega_0) s[N-1] - s[N-2]) - e^{-j 2 \\pi \\frac{k}{N}} s[N-1] \\\\\n  & =  e^{j 2 \\pi \\frac{k}{N}} s[N-1] - s[N-2].\n\\end{align}</math>|11}}\n\nNote that stopping the filter updates at term <math>N-1</math> and immediately applying equation (2) rather than equation (11) misses the final filter state updates, yielding a result with incorrect phase.<ref>{{cite web|url=http://www.eetimes.com/design/signal-processing-dsp/4024443/The-Goertzel-Algorithm |title=Electronic Engineering Times &#124; Connecting the Global Electronics Community |publisher=EE Times |date= |accessdate=2014-02-03}}</ref>\n\nThe particular filtering structure chosen for the Goertzel algorithm is the key to its efficient DFT calculations. We can observe that only one output value <math>y[N]</math> is used for calculating the DFT, so calculations for all the other output terms are omitted. Since the FIR filter is not calculated, the IIR stage calculations <math>s[0], s[1]</math>, etc. can be discarded immediately after updating the first stage's internal state.\n\nThis seems to leave a paradox: to complete the algorithm, the FIR filter stage must be evaluated once using the final two outputs from the IIR filter stage, while for computational efficiency the IIR filter iteration discards its output values. This is where the properties of the direct-form filter structure are applied. The two internal state variables of the IIR filter provide the last two values of the IIR filter output, which are the terms required to evaluate the FIR filter stage.\n\n== Applications ==\n=== Power-spectrum terms ===\nExamining equation (6), a final IIR filter pass to calculate term <math> y[N]</math> using a supplemental input value <math> x[N]=0 </math> applies a complex multiplier of magnitude 1 to the previous term <math>y[N-1]</math>. Consequently, <math>y[N]</math> and <math>y[N-1]</math> represent equivalent signal power. It is equally valid to apply equation (11) and calculate the signal power from term <math>y[N]</math> or to apply equation (2) and calculate the signal power from term <math>y[N-1]</math>. Both cases lead to the following expression for the signal power represented by DFT term <math>X[k]</math>:\n{{NumBlk|:|<math>\\begin{align}\n  X[k] \\, X'[k] & = y[N] \\, y'[N] = y[N-1] \\, y'[N-1] \\\\\n  & = s^2[N-1] + s^2[N-2] - 2 \\cos \\left(2 \\pi \\frac{k}{N} \\right) \\, s[N-1] \\, s[N-2].\n\\end{align} </math>|12}}\n\nIn the [[pseudocode]] below, the variables <code>sprev</code> and <code>sprev2</code> temporarily store output history from the IIR filter, while <code>x[n]</code> is an indexed element of the [[array data type|array]] <code>x</code>, which stores the input.\n\n<pre>\nNterms defined here\nKterm selected here\nω = 2 * π * Kterm / Nterms;\ncr = cos(ω);\nci = sin(ω);\ncoeff = 2 * cr;\n\nsprev = 0;\nsprev2 = 0;\nfor each index n in range 0 to Nterms-1\n  s = x[n] + coeff * sprev - sprev2;\n  sprev2 = sprev;\n  sprev = s;\nend\n\npower = sprev2 * sprev2 + sprev * sprev - coeff * sprev * sprev2;\n</pre>\n\nIt is possible<ref>{{cite web |date=August 25, 2011 |first=Wilfried |last=Elmenreich |title=Efficiently detecting a frequency using a Goertzel filter |url=http://netwerkt.wordpress.com/2011/08/25/goertzel-filter/ |accessdate=16 September 2014}}</ref> to organise the computations so that incoming samples are delivered singly to a [[Object-oriented programming|software object]] that maintains the filter state between updates, with the final power result accessed after the other processing is done.\n\n=== Single DFT term with real-valued arithmetic ===\nThe case of real-valued input data arises frequently, especially in embedded systems where the input streams result from direct measurements of physical processes. Comparing to the illustration in the previous section, when the input data are real-valued, the filter internal state variables <code>sprev</code> and <code>sprev2</code> can be observed also to be real-valued, consequently, no complex arithmetic is required in the first IIR stage. Optimizing for real-valued arithmetic typically is as simple as applying appropriate real-valued data types for the variables.\n\nAfter the calculations using input term <math>x[N-1]</math>, and filter iterations are terminated, equation (11) must be applied to evaluate the DFT term. The final calculation uses complex-valued arithmetic, but this can be converted into real-valued arithmetic by separating real and imaginary terms:\n{{NumBlk|:|<math>\\begin{align} \n  c_r  &= \\cos(2 \\pi \\tfrac{k}{N}), \\\\\n  c_i  &= \\sin(2 \\pi \\tfrac{k}{N}), \\\\\n  y[N] &= c_r s[N-1] - s[N-2] +  j c_i s[N-1].\n\\end{align} \n</math>|13}}\n\nComparing to the power-spectrum application, the only difference are the calculation used to finish:\n\n<pre>\n(Same IIR filter calculations as in the signal power implementation)\nXKreal = sprev * cr - sprev2;\nXKimag = sprev * ci;\n</pre>\n\n=== Phase detection ===\nThis application requires the same evaluation of DFT term <math>X[k]</math>, as discussed in the previous section, using a real-valued or complex-valued input stream.  Then the signal phase can be evaluated as\n{{NumBlk|:|<math> \\phi = \\tan^{-1}\\frac{\\Im(X[k])}{ \\Re(X[k])}, </math>|14}}\ntaking appropriate precautions for singularities, quadrant, and so forth when computing the inverse tangent function.\n\n=== Complex signals in real arithmetic ===\nSince complex signals decompose linearly into real and imaginary parts, the Goertzel algorithm can be computed in real arithmetic separately over the sequence of real parts, yielding <math>y_\\text{r}[n]</math>, and over the sequence of imaginary parts, yielding <math>y_\\text{i}[n]</math>. After that, the two complex-valued partial results can be recombined:\n{{NumBlk|:|<math>y[n] = y_\\text{r}[n] + j y_\\text{i}[n].</math>|15}}\n\n== Computational complexity ==\n{{Refimprove section|date=February 2014}}\n* According to [[computational complexity theory]], computing a set of <math>M</math> DFT terms using <math>M</math> applications of the Goertzel algorithm on a data set with <math>N</math> values with a \"cost per operation\" of <math>K</math> has [[big O notation|complexity]] <math>O(KNM)</math>.\n\n: To compute a single [[Discrete Fourier transform|DFT]] bin <math>X(f)</math> for a complex input sequence of length <math>N</math>, the Goertzel algorithm requires <math>2 N</math> multiplications and <math>4\\ N</math> additions/subtractions within the loop, as well as 4 multiplications and 4 final additions/subtractions, for a total of <math>2 N + 4</math> multiplications and <math>4 N + 4</math> additions/subtractions. This is repeated for each of the <math>M</math> frequencies.\n\n* In contrast, using an [[Fast Fourier transform|FFT]] on a data set with <math>N</math> values has complexity <math>O(KN\\log N)</math>.\n\n: This is harder to apply directly because it depends on the FFT algorithm used, but a typical example is a radix-2 FFT, which requires <math>2 \\log_2(N)</math> multiplications and <math>3 \\log_2(N)</math> additions/subtractions per [[Discrete Fourier transform|DFT]] bin, for each of the <math>N</math> bins.\n\nIn the complexity order expressions, when the number of calculated terms <math>M</math> is smaller than <math>\\log N</math>,  the advantage of the Goertzel algorithm is clear. But because FFT code is comparatively complex,\nthe \"cost per unit of work\" factor <math>K</math> is often larger for an FFT, and the practical advantage favours the Goertzel algorithm even for <math>M</math> several times larger than <math>\\log_2(N)</math>.\n\nAs a rule-of-thumb for determining whether a radix-2 FFT or a Goertzel algorithm is more efficient, adjust the number of terms <math>N</math> in the data set upward to the nearest exact power of 2, calling this <math>N_2</math>, and the Goertzel algorithm is likely to be faster if \n:<math>M \\le \\frac{5 N_2}{6 N} \\log_2(N_2)</math>\n\nFFT implementations and processing platforms have a significant impact on the relative performance. Some FFT implementations<ref>{{Citation |last1=Press|last2=Flannery|last3=Teukolsky|last4=Vetterling|title=Numerical Recipes, The Art of Scientific Computing|publisher=Cambridge University Press|year=2007|chapter=Chapter 12|doi= }}</ref> perform internal complex-number calculations to generate coefficients on-the-fly, significantly increasing their \"cost K per unit of work.\"  FFT and DFT algorithms can use tables of pre-computed coefficient values for better numerical efficiency, but this requires more accesses to coefficient values buffered in external memory, which can lead to increased cache contention that counters some of the numerical advantage.\n\nBoth algorithms gain approximately a factor of 2 efficiency when using real-valued rather than complex-valued input data. However, these gains are natural for the Goertzel algorithm but will not be achieved for the FFT without using certain algorithm variants {{which|date=February 2014}} specialised for [[Fast Fourier transform|transforming real-valued data]].\n\n== See also ==\n* [[Bluestein's FFT algorithm]] (chirp-Z)\n* [[Frequency-shift keying]] (FSK)\n* [[Phase-shift keying]] (PSK)\n\n== References ==\n{{reflist|30em}}\n\n==Further reading==\n* {{Citation |last=Proakis |first= J. G. |first2=D. G. |last2= Manolakis |title=Digital Signal Processing: Principles, Algorithms, and Applications |location= Upper Saddle River, NJ |publisher=Prentice Hall |year=1996 |pages=480–481 |isbn= |doi= |ref=none}}\n\n==External links==\n* {{webarchive |url=https://web.archive.org/web/20180628024641/http://en.dsplib.org/content/goertzel/goertzel.html |title=Goertzel Algorithm}}\n* [http://www.embedded.com/design/configurable-systems/4006427/A-DSP-algorithm-for-frequency-analysis A DSP algorithm for frequency analysis]\n* [http://www.embedded.com/print/4024443 The Goertzel Algorithm by Kevin Banks]\n\n[[Category:FFT algorithms]]\n[[Category:Digital signal processing]]"
    },
    {
      "title": "Irrational base discrete weighted transform",
      "url": "https://en.wikipedia.org/wiki/Irrational_base_discrete_weighted_transform",
      "text": "In [[mathematics]], the '''irrational base discrete weighted transform''' (IBDWT) is a variant of the [[fast Fourier transform]] using an [[irrational number|irrational]] base; it was developed by [[Richard Crandall]] ([[Reed College]]), [[Barry Fagin]] ([[Dartmouth College]]) and [[Joshua Doenias]] ([[NeXT|NeXT Software]]){{fact|date=April 2018}} in the early 1990s using [[Mathematica]].{{fact|date=April 2018}}\n\nThe IBDWT is used in the [[Great Internet Mersenne Prime Search]]'s client [[Prime95]] to perform [[FFT multiplication]], as well as in other programs implementing [[Lucas-Lehmer test]], such as CUDALucas and Glucas.\n\n== References ==\n\n* [[Richard Crandall]], [[Barry Fagin]]: ''Discrete weighted transforms and large-integer arithmetic'', Mathematics of Computation 62, 205, 305-324, January 1994 ([http://www.faginfamily.net/barry/Papers/Discrete%20Weighted%20Transforms.pdf PDF file])\n* [[Richard Crandall]]: ''Topics in Advanced Scientific Computation'', TELOS/Springer-Verlag\n\n[[Category:FFT algorithms]]\n[[Category:Discrete transforms]]\n\n{{Mathanalysis-stub}}"
    },
    {
      "title": "Prime-factor FFT algorithm",
      "url": "https://en.wikipedia.org/wiki/Prime-factor_FFT_algorithm",
      "text": "{{Use American English|date = March 2019}}\n{{Short description|Fast Fourier Transform algorithm}}\nThe '''prime-factor algorithm (PFA)''', also called the '''Good–Thomas algorithm''' (1958/1963), is a [[fast Fourier transform]] (FFT) algorithm that re-expresses the [[discrete Fourier transform]] (DFT) of a size ''N'' = ''N''<sub>1</sub>''N''<sub>2</sub> as a two-dimensional ''N''<sub>1</sub>×''N''<sub>2</sub> DFT, but ''only'' for the case where ''N''<sub>1</sub> and ''N''<sub>2</sub> are [[relatively prime]]. These smaller transforms of size ''N''<sub>1</sub> and ''N''<sub>2</sub> can then be evaluated by applying PFA [[recursion|recursively]] or by using some other FFT algorithm.\n\nPFA should not be confused with the '''mixed-radix''' generalization of the popular [[Cooley–Tukey FFT algorithm|Cooley–Tukey algorithm]], which also subdivides a DFT of size ''N'' = ''N''<sub>1</sub>''N''<sub>2</sub> into smaller transforms of size ''N''<sub>1</sub> and ''N''<sub>2</sub>. The latter algorithm can use ''any'' factors (not necessarily relatively prime), but it has the disadvantage that it also requires extra multiplications by roots of unity called [[twiddle factor]]s, in addition to the smaller transforms. On the other hand, PFA has the disadvantages that it only works for relatively prime factors (e.g. it is useless for [[power of two|power-of-two]] sizes) and that it requires a more complicated re-indexing of the data based on the [[Chinese remainder theorem]] (CRT). Note, however, that PFA can be combined with mixed-radix Cooley–Tukey, with the former factorizing ''N'' into relatively prime components and the latter handling repeated factors.\n\nPFA is also closely related to the nested [[Winograd FFT algorithm]], where the latter performs the decomposed ''N''<sub>1</sub> by ''N''<sub>2</sub> transform via more sophisticated two-dimensional convolution techniques. Some older papers therefore also call Winograd's algorithm a PFA FFT.\n\n(Although the PFA is distinct from the Cooley–Tukey algorithm, Good's 1958 work on the PFA was cited as inspiration by Cooley and Tukey in their 1965 paper, and there was initially some confusion about whether the two algorithms were different. In fact, it was the only prior FFT work cited by them, as they were not then aware of the earlier research by Gauss and others.)\n\n==Algorithm==\nRecall that the DFT is defined by the formula:\n\n:<math>X_k = \\sum_{n=0}^{N-1} x_n e^{-\\frac{2\\pi i}{N} nk }\n\\qquad\nk = 0,\\dots,N-1. </math>\n\nThe PFA involves a re-indexing of the input and output arrays, which when substituted into the DFT formula transforms it into two nested DFTs (a two-dimensional DFT).\n\n===Re-indexing===\nSuppose that ''N'' = ''N''<sub>1</sub>''N''<sub>2</sub>, where ''N''<sub>1</sub> and ''N''<sub>2</sub> are relatively prime. In this case, we can define a [[bijection|bijective]] re-indexing of the input ''n'' and output ''k'' by:\n\n:<math>n = n_1 N_2 + n_2 N_1 \\mod N,</math>\n:<math>k = k_1 N_2^{-1} N_2 + k_2 N_1^{-1} N_1 \\mod N,</math>\n\nwhere ''N''<sub>1</sub><sup>−1</sup> denotes the [[modular multiplicative inverse]] of ''N''<sub>1</sub> [[modular arithmetic|modulo]] ''N''<sub>2</sub> and vice versa for ''N''<sub>2</sub><sup>−1</sup>; the indices ''k''<sub>''a''</sub> and ''n''<sub>''a''</sub> run from 0,...,''N''<sub>''a''</sub>−1 (for ''a'' = 1, 2). These inverses only exist for relatively prime ''N''<sub>1</sub> and ''N''<sub>2</sub>, and that condition is also required for the first mapping to be bijective.\n\nThis re-indexing of ''n'' is called the ''Ruritanian'' mapping (also ''Good's'' mapping), while this re-indexing of ''k'' is called the ''CRT'' mapping. The latter refers to the fact that ''k'' is the solution to the Chinese remainder problem ''k'' = ''k''<sub>1</sub> mod ''N''<sub>1</sub> and ''k'' = ''k''<sub>2</sub> mod ''N''<sub>2</sub>.\n\n(One could instead use the Ruritanian mapping for the output ''k'' and the CRT mapping for the input ''n'', or various intermediate choices.)\n\nA great deal of research has been devoted to schemes for evaluating this re-indexing efficiently, ideally [[in-place algorithm|in-place]], while minimizing the number of costly modulo (remainder) operations (Chan, 1991, and references).\n\n===DFT re-expression===\nThe above re-indexing is then substituted into the formula for the DFT, and in particular into the product ''nk'' in the exponent. Because ''e''<sup>2''πi''</sup> = 1, this exponent is evaluated modulo ''N'': any ''N''<sub>1</sub>''N''<sub>2</sub> = ''N'' cross term in the ''nk'' product can be set to zero. (Similarly, ''X''<sub>''k''</sub> and ''x''<sub>''n''</sub> are implicitly periodic in ''N'', so their subscripts are evaluated modulo ''N''.) The remaining terms give:\n\n:<math>X_{k_1 N_2^{-1} N_2 + k_2 N_1^{-1} N_1} = \n \\sum_{n_1=0}^{N_1-1} \n \\left( \\sum_{n_2=0}^{N_2-1} x_{n_1 N_2 + n_2 N_1} \n e^{-\\frac{2\\pi i}{N_2} n_2 k_2 } \\right)\n e^{-\\frac{2\\pi i}{N_1} n_1 k_1 }.\n\n</math>\n\nThe inner and outer sums are simply DFTs of size ''N''<sub>2</sub> and ''N''<sub>1</sub>, respectively.\n\n(Here, we have used the fact that ''N''<sub>1</sub><sup>−1</sup>''N''<sub>1</sub> is unity when evaluated modulo ''N''<sub>2</sub> in the inner sum's exponent, and vice versa for the outer sum's exponent.)\n\n==References==\n*{{cite journal |first=I. J. |last=Good |title=The interaction algorithm and practical Fourier analysis |journal=Journal of the Royal Statistical Society, Series B |volume=20 |issue=2 |pages=361–372 |year=1958 |jstor=2983896 }} Addendum, ''ibid.'' '''22''' (2), 373-375 (1960) {{JSTOR|2984108}}.\n*{{cite book |first=L. H. |last=Thomas |chapter=Using a computer to solve problems in physics |title=Applications of Digital Computers |publisher=Ginn |location=Boston |year=1963 |isbn= }}\n*{{cite journal |first=P. |last=Duhamel |first2=M. |last2=Vetterli |title=Fast Fourier transforms: a tutorial review and a state of the art |journal=Signal Processing |volume=19 |issue=4 |pages=259–299 |year=1990 |doi=10.1016/0165-1684(90)90158-U |url=http://infoscience.epfl.ch/record/59946 }}\n*{{cite journal |first=S. C. |last=Chan |first2=K. L. |last2=Ho |title=On indexing the prime-factor fast Fourier transform algorithm |journal=IEEE Trans. Circuits and Systems |volume=38 |issue=8 |pages=951–953 |year=1991 |doi=10.1109/31.85638 }}\n\n==See also==\n* [[Rader's FFT algorithm]]\n* [[Bluestein's FFT algorithm]]\n\n[[Category:FFT algorithms]]"
    },
    {
      "title": "Rader's FFT algorithm",
      "url": "https://en.wikipedia.org/wiki/Rader%27s_FFT_algorithm",
      "text": "'''Rader's algorithm''' (1968),<ref>C. M. Rader, \"Discrete Fourier transforms when the number of data samples is prime,\" ''Proc. IEEE'' 56, 1107–1108 (1968).</ref> named for Charles M. Rader of [[MIT Lincoln Laboratory]], is a [[fast Fourier transform]] (FFT) algorithm that computes the [[discrete Fourier transform]] (DFT) of [[prime number|prime]] sizes by re-expressing the DFT as a cyclic [[convolution]] (the other algorithm for FFTs of prime sizes, [[Bluestein's FFT algorithm|Bluestein's algorithm]], also works by rewriting the DFT as a convolution).\n\nSince Rader's algorithm only depends upon the periodicity of the DFT kernel, it is directly applicable to any other transform (of prime order) with a similar property, such as a [[number-theoretic transform]] or the [[discrete Hartley transform]].\n\nThe algorithm can be modified to gain a factor of two savings for the case of DFTs of real data, using a slightly modified re-indexing/permutation to obtain two half-size cyclic convolutions of real data;<ref>S. Chu and C. Burrus, \"A prime factor FTT <nowiki>[</nowiki>''sic''<nowiki>]</nowiki> algorithm using distributed arithmetic,\" '' IEEE Transactions on Acoustics, Speech, and Signal Processing'' '''30''' (2), 217&ndash;227 (1982).</ref> an alternative adaptation for DFTs of real data uses the [[discrete Hartley transform]].<ref name=Frigo05>Matteo Frigo and [[Steven G. Johnson]], \"[http://fftw.org/fftw-paper-ieee.pdf The Design and Implementation of FFTW3],\" ''Proceedings of the IEEE'' '''93''' (2), 216–231 (2005).</ref>\n\nWinograd extended Rader's algorithm to include prime-power DFT sizes <math>p^m</math>,<ref>S. Winograd, \"On Computing the Discrete Fourier Transform\", ''Proc. National Academy of Sciences USA'', '''73'''(4), 1005&ndash;1006 (1976).</ref><ref>S. Winograd, \"On Computing the Discrete Fourier Transform\", ''Mathematics of Computation'', '''32'''(141), 175&ndash;199 (1978).</ref> and today Rader's algorithm is sometimes described as a special case of [[Fast Fourier transform#Other FFT algorithms|Winograd's FFT algorithm]], also called the ''multiplicative Fourier transform algorithm'' (Tolimieri et al., 1997),<ref>R. Tolimieri, M. An, and C.Lu, ''Algorithms for Discrete Fourier Transform and Convolution'', Springer-Verlag, 2nd ed., 1997.</ref> which applies to an even larger class of sizes.  However, for [[composite number|composite]] sizes such as prime powers, the [[Cooley–Tukey FFT algorithm]] is much simpler and more practical to implement, so Rader's algorithm is typically only used for large-prime [[Base case (recursion)|base case]]s of Cooley–Tukey's [[Recursion (computer science)|recursive]] decomposition of the DFT.<ref name=Frigo05/>\n\n==Algorithm==\n[[File:FFT visual Rader 11.jpg|thumb|Visual representation of [[DFT matrix]] in Rader's FFT algorithm, The array consists of colored clocks represent DFT matrix of size 11. By permuting rows and columns using a sequence generated by primitive root of 11 (which is 2) except the 1st row and column,  the original DFT matrix becomes a [[circulant matrix]]. Multiplying a circulant matrix to a data sequence is equivalent to the [[cyclic convolution]].]]\n:<math> X_k = \\sum_{n=0}^{N-1} x_n e^{-\\frac{2\\pi i}{N} nk }\n\\qquad\nk = 0,\\dots,N-1. </math>\n\nIf ''N'' is a prime number, then the set of non-zero indices ''n'' = 1,...,''N''&ndash;1 forms a [[group (mathematics)|group]] under multiplication [[modular arithmetic|modulo]] ''N''.  One consequence of the [[number theory]] of such groups is that there exists a [[generating set of a group|generator]] of the group (sometimes called a [[Primitive root modulo n|primitive root]], which can be found quickly by exhaustive search or slightly better algorithms<ref>Donald E. Knuth, ''The Art of Computer Programming, vol. 2: Seminumerical Algorithms'', 3rd edition, section 4.5.4, p. 391 (Addison–Wesley, 1998).</ref>), an integer ''g'' such that ''n'' = ''g''<sup>''q''</sup> (mod ''N'') for any non-zero index ''n'' and for a unique ''q'' in 0,...,''N''&ndash;2 (forming a [[bijection]] from ''q'' to non-zero ''n'').  Similarly ''k'' = ''g''<sup>&ndash;''p''</sup>  (mod ''N'') for any non-zero index ''k'' and for a unique ''p'' in 0,...,''N''&ndash;2, where the negative exponent denotes the [[modular multiplicative inverse|multiplicative inverse]] of ''g''<sup>''p''</sup> modulo ''N''.  That means that we can rewrite the DFT using these new indices ''p'' and ''q'' as:\n\n:<math> X_0 =  \\sum_{n=0}^{N-1} x_n,</math>\n\n:<math> X_{g^{-p}} = x_0 +  \\sum_{q=0}^{N-2} x_{g^q} e^{-\\frac{2\\pi i}{N} g^{-(p-q)} }\n\\qquad\np = 0,\\dots,N-2. </math>\n\n(Recall that ''x''<sub>''n''</sub> and ''X''<sub>''k''</sub> are implicitly periodic in ''N'', and also that ''e''<sup>2πi</sup>=1.  Thus, all indices and exponents are taken modulo ''N'' as required by the group arithmetic.)\n\nThe final summation, above, is precisely a cyclic convolution of the two sequences ''a''<sub>''q''</sub> and ''b''<sub>''q''</sub> of length ''N''&ndash;1 (''q'' = 0,...,''N''&ndash;2) defined by:\n\n:<math>a_q = x_{g^q}</math>\n:<math>b_q = e^{-\\frac{2\\pi i}{N} g^{-q} }.</math>\n\n===Evaluating the convolution===\n\nSince ''N''&ndash;1 is composite, this convolution can be performed directly via the [[convolution theorem]] and more conventional FFT algorithms.  However, that may not be efficient if ''N''&ndash;1 itself has large prime factors, requiring recursive use of Rader's algorithm.  Instead, one can compute a length-(''N''&ndash;1) cyclic convolution exactly by zero-padding it to a length of at least 2(''N''&ndash;1)&ndash;1, say to a [[power of two]], which can then be evaluated in O(''N'' log ''N'') time without the recursive application of Rader's algorithm.\n\nThis algorithm, then, requires O(''N'') additions plus O(''N'' log ''N'') time for the convolution.  In practice, the O(''N'') additions can often be performed by absorbing the additions into the convolution: if the convolution is performed by a pair of FFTs, then the sum of ''x''<sub>''n''</sub> is given by the DC (0th) output of the FFT of ''a''<sub>''q''</sub> plus ''x''<sub>0</sub>, and ''x''<sub>0</sub> can be added to all the outputs by adding it to the DC term of the convolution prior to the inverse FFT.  Still, this algorithm requires intrinsically more operations than FFTs of nearby composite sizes, and typically takes 3&ndash;10 times as long in practice.\n\nIf Rader's algorithm is performed by using FFTs of size ''N''&ndash;1 to compute the convolution, rather than by zero padding as mentioned above, the efficiency depends strongly upon ''N'' and the number of times that Rader's algorithm must be applied recursively.  The worst case would be if ''N''&ndash;1 were 2''N''<sub>2</sub> where ''N''<sub>2</sub> is prime, with ''N''<sub>2</sub>&ndash;1 = 2''N''<sub>3</sub> where ''N''<sub>3</sub> is prime, and so on.  In such cases, supposing that the chain of primes extended all the way down to some bounded value, the recursive application of Rader's algorithm would actually require O(''N''<sup>2</sup>) time.  Such ''N''<sub>j</sub> are called [[Sophie Germain prime]]s, and such a sequence of them is called a [[Cunningham chain]] of the first kind.  The lengths of Cunningham chains, however, are observed to grow more slowly than log<sub>2</sub>(''N''), so Rader's algorithm applied in this way is probably not [[Big O notation|O]](''N''<sup>2</sup>), though it is possibly worse than O(''N'' log ''N'') for the worst cases.  Fortunately, a guarantee of O(''N'' log ''N'') complexity can be achieved by zero padding.\n\n==References==\n\n<references/>\n\n[[Category:FFT algorithms]]"
    },
    {
      "title": "Split-radix FFT algorithm",
      "url": "https://en.wikipedia.org/wiki/Split-radix_FFT_algorithm",
      "text": "The '''split-radix FFT''' is a [[fast Fourier transform]] (FFT) algorithm for computing the [[discrete Fourier transform]] (DFT), and was first described in an initially little-appreciated paper by [[R. Yavne]] (1968) and subsequently rediscovered simultaneously by various authors in 1984. (The name \"split radix\" was coined by two of these reinventors, [[Pierre Duhamel|P. Duhamel]] and [[Henk D. L. Hollmann|H. Hollmann]].)  In particular, split radix is a variant of the [[Cooley–Tukey FFT algorithm]] that uses a blend of radices 2 and 4: it [[recursion|recursively]] expresses a DFT of length ''N'' in terms of one smaller DFT of length ''N''/2 and two smaller DFTs of length ''N''/4.\n\nThe split-radix FFT, along with its variations, long had the distinction of achieving the lowest published arithmetic operation count (total exact number of required [[real number|real]] additions and multiplications) to compute a DFT of [[power of two|power-of-two]] sizes ''N''.  The arithmetic count of the original split-radix algorithm was improved upon in 2004 (with the initial gains made in unpublished work by J. Van Buskirk via hand optimization for ''N''=64 [http://groups.google.com/group/comp.dsp/msg/9e002292accb8a8b] [https://web.archive.org/web/20061130110013/http://home.comcast.net/~kmbtib/]), but it turns out that one can still achieve the new lowest count by a modification of split radix (Johnson and Frigo, 2007). Although the number of arithmetic operations is not the sole factor (or even necessarily the dominant factor) in determining the time required to compute a DFT on a [[computer]], the question of the minimum possible count is of longstanding theoretical interest.  (No tight lower bound on the operation count has currently been proven.)\n\nThe split-radix algorithm can only be applied when ''N'' is a multiple of 4, but since it breaks a DFT into smaller DFTs it can be combined with any other FFT algorithm as desired.\n\n==Split-radix decomposition==\nRecall that the DFT is defined by the formula:\n:<math> X_k =  \\sum_{n=0}^{N-1} x_n \\omega_N^{nk} </math>\nwhere <math>k</math> is an integer ranging from <math>0</math> to <math>N-1</math> and <math>\\omega_N</math> denotes the primitive [[root of unity]]:\n:<math>\\omega_N = e^{-\\frac{2\\pi i}{N}},</math>\nand thus: <math>\\omega_N^N = 1</math>.\n\nThe split-radix algorithm works by expressing this summation in terms of three smaller summations.  (Here, we give the \"decimation in time\" version of the split-radix FFT; the dual decimation in frequency version is essentially just the reverse of these steps.)\n\nFirst, a summation over the [[even and odd numbers|even]] indices <math>x_{2n_2}</math>.  Second, a summation over the odd indices broken into two pieces: <math>x_{4n_4+1}</math> and <math>x_{4n_4+3}</math>, according to whether the index is 1 or 3 [[modulo operation|modulo]] 4.  Here, <math>n_m</math> denotes an index that runs from 0 to <math>N/m-1</math>.  The resulting summations look like:\n\n:<math> X_k =  \\sum_{n_2=0}^{N/2-1} x_{2n_2} \\omega_{N/2}^{n_2 k} \n+ \\omega_N^k \\sum_{n_4=0}^{N/4-1} x_{4n_4+1} \\omega_{N/4}^{n_4 k}\n+ \\omega_N^{3k} \\sum_{n_4=0}^{N/4-1} x_{4n_4+3} \\omega_{N/4}^{n_4 k}\n</math>\n\nwhere we have used the fact that <math>\\omega_N^{m n k} = \\omega_{N/m}^{n k}</math>.  These three sums correspond to ''portions'' of [[Cooley–Tukey FFT algorithm#The radix-2 DIT case|radix-2]] (size ''N''/2) and radix-4 (size ''N''/4) Cooley–Tukey steps, respectively.  (The underlying idea is that the even-index subtransform of radix-2 has no multiplicative factor in front of it, so it should be left as-is, while the odd-index subtransform of radix-2 benefits by combining a second recursive subdivision.)\n\nThese smaller summations are now exactly DFTs of length ''N''/2 and ''N''/4, which can be performed recursively and then recombined.\n\nMore specifically, let <math>U_k</math> denote the result of the DFT of length ''N''/2 (for <math>k = 0,\\ldots,N/2-1</math>), and let <math>Z_k</math> and <math>Z'_k</math> denote the results of the DFTs of length ''N''/4 (for <math>k = 0,\\ldots,N/4-1</math>).  Then the output <math>X_k</math> is simply:\n:<math>X_k = U_k + \\omega_N^k Z_k + \\omega_N^{3k} Z'_k.</math>\n\nThis, however, performs unnecessary calculations, since <math>k \\geq N/4</math> turn out to share many calculations with <math>k < N/4</math>.  In particular, if we add ''N''/4 to ''k'', the size-''N''/4 DFTs are not changed (because they are periodic in ''k''), while the size-''N''/2 DFT is unchanged if we add ''N''/2 to ''k''. So, the only things that change are the <math>\\omega_N^k</math> and <math>\\omega_N^{3k}</math> terms, known as [[twiddle factor]]s.  Here, we use the identities:\n:<math>\\omega_N^{k+N/4} = -i \\omega_N^k</math>\n:<math>\\omega_N^{3(k+N/4)} = i \\omega_N^{3k}</math>\nto finally arrive at:\n:<math>X_k = U_k + \\left( \\omega_N^k Z_k + \\omega_N^{3k} Z'_k \\right),</math>\n:<math>X_{k+N/2} = U_k - \\left( \\omega_N^k Z_k + \\omega_N^{3k} Z'_k \\right),</math>\n:<math>X_{k+N/4} = U_{k+N/4} - i \\left( \\omega_N^k Z_k - \\omega_N^{3k} Z'_k \\right),</math>\n:<math>X_{k+3N/4} = U_{k+N/4} + i \\left( \\omega_N^k Z_k - \\omega_N^{3k} Z'_k \\right),</math>\nwhich gives all of the outputs <math>X_k</math> if we let <math>k</math> range from <math>0</math> to <math>N/4-1</math> in the above four expressions.\n\nNotice that these expressions are arranged so that we need to combine the various DFT outputs by pairs of additions and subtractions, which are known as [[butterfly diagram|butterflies]].  In order to obtain the minimal operation count for this algorithm, one needs to take into account special cases for <math>k = 0</math> (where the twiddle factors are unity) and for <math>k = N/8</math> (where the twiddle factors are <math>(1 \\pm i)/\\sqrt{2}</math> and can be multiplied more quickly); see, e.g. Sorensen ''et al.'' (1986).  Multiplications by <math>\\pm 1</math> and <math>\\pm i</math> are ordinarily counted as free (all negations can be absorbed by converting additions into subtractions or vice versa).\n\nThis decomposition is performed recursively when ''N'' is a power of two.  The base cases of the recursion are ''N''=1, where the DFT is just a copy <math>X_0 = x_0</math>, and ''N''=2, where the DFT is an addition <math>X_0 = x_0 + x_1</math> and a subtraction <math>X_1 = x_0 - x_1</math>.\n\nThese considerations result in a count: <math>4 N \\log_2 N - 6N + 8</math> real additions and multiplications, for ''N''&gt;1 a power of two.  This count assumes that, for odd powers of 2, the leftover factor of 2 (after all the split-radix steps, which divide ''N'' by 4) is handled directly by the DFT definition (4 real additions and multiplications), or equivalently by a radix-2 Cooley–Tukey FFT step.\n\n==References==\n* R. Yavne, \"An economical method for calculating the discrete Fourier transform,\" in ''Proc. AFIPS Fall Joint Computer Conf.'' '''33''', 115–125 (1968).\n* P. Duhamel and H. Hollmann, \"Split-radix FFT algorithm,\" ''Electron. Lett.'' '''20''' (1), 14–16 (1984).\n* M. Vetterli and H. J. Nussbaumer, \"Simple FFT and DCT algorithms with reduced number of operations,\" ''Signal Processing'' '''6''' (4), 267–278 (1984).\n* J. B. Martens, \"Recursive cyclotomic factorization—a new algorithm for calculating the discrete Fourier transform,\" ''IEEE Trans. Acoust., Speech, Signal Processing'' '''32''' (4), 750–761 (1984).\n* P. Duhamel and M. Vetterli, \"Fast Fourier transforms: a tutorial review and a state of the art,\" ''Signal Processing'' '''19''', 259–299 (1990).\n* S. G. Johnson and M. Frigo, \"[http://www.fftw.org/newsplit.pdf A modified split-radix FFT with fewer arithmetic operations],\" ''IEEE Trans. Signal Process.'' '''55''' (1), 111–119 (2007).\n* Douglas L. Jones, \"[http://cnx.org/content/m12031/latest/ Split-radix FFT algorithms],\" ''[http://cnx.org/ Connexions]'' web site (Nov. 2, 2006).\n* H. V. Sorensen, M. T. Heideman, and C. S. Burrus, \"On computing the split-radix FFT\", ''IEEE Trans. Acoust., Speech, Signal Processing'' '''34''' (1), 152–156 (1986).\n\n[[Category:FFT algorithms]]"
    },
    {
      "title": "Twiddle factor",
      "url": "https://en.wikipedia.org/wiki/Twiddle_factor",
      "text": "A '''twiddle factor''', in [[fast Fourier transform]] (FFT) algorithms, is any of the [[trigonometric function|trigonometric]] constant coefficients that are multiplied by the data in the course of the algorithm.  This term was apparently coined by Gentleman &amp; Sande in 1966, and has since become widespread in thousands of papers of the FFT literature.\n\nMore specifically, \"twiddle factors\" originally referred to the [[root of unity|root-of-unity]] [[complex number|complex]] multiplicative constants in the [[butterfly diagram|butterfly]] operations of the [[Cooley–Tukey FFT algorithm]], used to [[recursion|recursively]] combine smaller [[discrete Fourier transform]]s.  This remains the term's most common meaning, but it may also be used for any data-independent multiplicative constant in an FFT.\n\nThe [[prime-factor FFT algorithm]] is one unusual case in which an FFT can be performed without twiddle factors, albeit only for restricted factorizations of the transform size.\n\nFor example, W8² is a twiddling factor used in 8 point radix-2 FFT.\n==References==\n* W. M. Gentleman and G. Sande, \"Fast Fourier transforms&mdash;for fun and profit,\" ''Proc. AFIPS'' '''29''', 563–578 (1966). {{doi|10.1145/1464291.1464352}}\n\n[[Category:FFT algorithms]]"
    },
    {
      "title": "Vector-radix FFT algorithm",
      "url": "https://en.wikipedia.org/wiki/Vector-radix_FFT_algorithm",
      "text": "The '''vector-radix FFT algorithm''', is a multidimensional [[fast Fourier transform]] (FFT) algorithm, which is a generalization of the ordinary [[Cooley–Tukey FFT algorithm]] that divides the transform dimensions by arbitrary radices. It breaks a multidimensional (MD) [[discrete Fourier transform]] (DFT) down into successively smaller MD DFTs until, ultimately, only trivial MD DFTs need to be evaluated.<ref name=\"Dudgeon83\">{{cite book|last1=Dudgeon|first1=Dan|last2=Russell|first2=Mersereau|title=Multidimensional Digital Signal Processing|date=September 1983|publisher=Prentice Hall|isbn=0136049591|pages=76}}</ref>\n\nThe most common multidimensional [[FFT]] algorithm is the row-column algorithm, which means transforming the array first in one index and then in the other, see more in [[FFT]]. Then a radix-2 direct 2-D FFT has been developed,<ref name=\"Rivard77\">{{cite journal|last1=Rivard|first1=G.|title=Direct fast Fourier transform of bivariate functions|journal=IEEE Transactions on Acoustics, Speech, and Signal Processing|volume=25|pages=250–252|doi=10.1109/TASSP.1977.1162951|url=http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1162951&isnumber=26125}}</ref> and it can eliminate 25% of the multiplies as compared to the conventional row-column approach. And this algorithm has been extended to rectangular arrays and arbitrary radices,<ref name=\"Harris77\">{{cite journal|last1=Harris|first1=D.|last2=McClellan|first2=J.|last3=Chan|first3=D.|last4=Schuessler|first4=H.|title=Vector radix fast Fourier transform|journal=IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP '77|volume=2|pages=548–551|doi=10.1109/ICASSP.1977.1170349|url=http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1170349&isnumber=26347}}</ref> which is the general vector-radix algorithm.\n\nVector-radix FFT algorithm can reduce the number of complex multiplications significantly, compared to row-vector algorithm. For example, for a <math>N^M</math> element matrix (M dimensions, and size N on each dimension), the number of complex multiples of vector-radix FFT algorithm for radix-2 is <math>\\frac{2^M -1}{2^M} N^M \\log_2 N</math>, meanwhile, for row-column algorithm, it is <math>\\frac{M N^M} 2 \\log_2 N</math>. And generally, even larger savings in multiplies are obtained when this algorithm is operated on larger radices and on higher dimensional arrays.<ref name=Harris77/>\n\nOverall, the vector-radix algorithm significantly reduces the structural complexity of the traditional DFT having a better indexing scheme, at the expense of a slight increase in arithmetic operations. So this algorithm is widely used for many applications in engineering, science, and mathematics, for example, implementations in image processing,<ref name=\"Buijs74\">{{cite journal|last1=Buijs|first1=H.|last2=Pomerleau|first2=A.|last3=Fournier|first3=M.|last4=Tam|first4=W.|title=Implementation of a fast Fourier transform (FFT) for image processing applications|journal=IEEE Transactions on Acoustics, Speech, and Signal Processing|date=Dec 1974|volume=22|pages=420–424|doi=10.1109/TASSP.1974.1162620|url=http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1162620&isnumber=26107}}</ref> and high speed FFT processor designing.<ref name=\"Badar15\">{{cite journal|last1=Badar|first1=S.|last2=Dandekar|first2=D.|title=High speed FFT processor design using radix −4 pipelined architecture|journal=2015 International Conference on Industrial Instrumentation and Control (ICIC), Pune, 2015|pages=1050–1055|doi=10.1109/IIC.2015.7150901|url=http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7150901&isnumber=7150576}}</ref>\n\n== 2-D DIT case ==\nAs with [[Cooley–Tukey FFT algorithm]], two dimensional vector-radix FFT is derived by decomposing the regular 2-D DFT into sums of smaller DFT's multiplied by \"twiddle\" factor.\n\nA decimation-in-time ('''DIT''') algorithm means the decomposition is based on time domain <math>x</math>, see more in [[Cooley–Tukey FFT algorithm]].\n\nWe suppose the 2-D DFT\n:<math>X(k_1,k_2) = \\sum_{n_1=0}^{N_1-1} \\sum_{n_2=0}^{N_2-1}  x[n_1, n_2] \\cdot W_{N_1}^{k_1 n_1} W_{N_2}^{k_2 n_2}, </math>\nwhere <math>k_1 = 0,\\dots,N_1-1</math>,and <math>k_2 = 0,\\dots,N_2-1</math>, and <math>x[n_1, n_2]</math> is a <math>N_1 \\times N_2</math> matrix, and <math>W_N = \\exp(-j 2\\pi /N)</math>.\n\nFor simplicity, let us assume that <math>N_1=N_2=N</math>, and radix-<math>(r\\times r)</math>(<math>N/r</math> are integers).\n\nUsing the change of variables:\n* <math>n_i=rp_i+q_i</math>, where <math>p_i=0,\\ldots,(N/r)-1; q_i = 0,\\ldots,r-1;</math>\n* <math>k_i=u_i+v_i N/r</math>, where <math>u_i=0,\\ldots,(N/r)-1; v_i = 0,\\ldots,r-1;</math>\nwhere <math>i = 1</math> or <math>2</math>, then the two dimensional DFT can be written as:<ref name=\"Chan92\">{{cite journal|last1=Chan|first1=S. C.|last2=Ho|first2=K. L.|title=Split vector-radix fast Fourier transform|journal=IEEE Transactions on Signal Processing|volume=40|pages=2029–2039|doi=10.1109/78.150004|url=http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=150004&isnumber=3966}}</ref>\n:<math> X(u_1+v_1 N/r,u_2+v_2 N/r)=\\sum_{q_1=0}^{r-1} \\sum_{q_2=0}^{r-1} \\left[ \\sum_{p_1=0}^{N/r-1} \\sum_{p_2=0}^{N/r-1} x[rp_1+q_1, rp_1+q_1] W_{N/r}^{p_1 u_1} W_{N/r}^{p_2 u_2} \\right] \\cdot W_N^{q_1 u_1+q_2 u_2} W_r^{q_1 v_1} W_r^{q_2 v_2},</math>\n\n[[File:2-D DIT-FFT-butterfly.png|thumb|400px|One stage \"butterfly\" for DIT vector-radix 2x2 FFT]]\n\nThe equation above defines the basic structure of the 2-D DIT radix-<math>(r\\times r)</math> \"butterfly\". (See 1-D \"butterfly\" in [[Cooley–Tukey FFT algorithm]])\n\nWhen <math>r=2</math>, the equation can be broken into four summations: one over those samples of x for which both <math>n_1</math> and <math>n_2</math> are even, one for which <math>n_1</math> is even and <math>n_2</math> is odd, one of which <math>n_1</math> is odd and <math>n_2</math> is even, and one for which both <math>n_1</math> and <math>n_2</math> are odd,<ref name=Dudgeon83/> and this leads to:\n\n:<math> X(k_1,k_2) = S_{00}(k_1,k_2) + S_{01}(k_1,k_2) W_N^{k_2} +S_{10}(k_1,k_2) W_N^{k_1} + S_{11}(k_1,k_2) W_N^{k_1+k_2},</math>\n\nwhere <math>S_{ij}(k_1,k_2)=\\sum_{n_1=0}^{N/2-1} \\sum_{n_2=0}^{N/2-1} x[2 n_1 + i, 2 n_2 + j] \\cdot W_{N/2}^{n_1 k_1} W_{N/2}^{n_2 k_2}.</math>\n\n== 2-D DIF case ==\nSimilarly, a decimation-in-frequency ('''DIF''', also called the Sande–Tukey algorithm) algorithm means the decomposition is based on frequency domain <math>X</math>, see more in [[Cooley–Tukey FFT algorithm]].\n\nUsing the change of variables:\n* <math>n_i=p_i+q_i N/r</math>, where <math>p_i=0,\\ldots,(N/r)-1; q_i = 0,\\ldots,r-1;</math>\n* <math>k_i=r u_i+v_i</math>, where <math>u_i=0,\\ldots,(N/r)-1; v_i = 0,\\ldots,r-1;</math>\nwhere <math>i = 1</math> or <math>2</math>, and the DFT equation can be written as:<ref name=Chan92/>\n:<math> X(r u_1+v_1,r u_2+v_2)=\\sum_{p_1=0}^{N/r-1} \\sum_{p_2=0}^{N/r-1} \\left[ \\sum_{q_1=0}^{r-1} \\sum_{q_2=0}^{r-1} x[p_1+q_1 N/r, p_1+q_1 N/r] W_{r}^{q_1 v_1} W_{r}^{q_2 v_2} \\right] \\cdot W_{N}^{p_1 v_1+p_2 v_2} W_{N/r}^{p_1 u_1} W_{N/r}^{p_2 u_2},</math>\n\n== Other approaches ==\nThe [[split-radix FFT algorithm]] has been proved to be a useful method for 1-D DFT. And this method has been applied to the vector-radix FFT to obtain a split vector-radix FFT.<ref name=Chan92/><ref name=\"Pei87\">{{cite journal|last1=Pei|first1=Soo-Chang|last2=Wu|first2=Ja-Lin|title=Split vector radix 2D fast Fourier transform|journal=IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP '87.|date=April 1987|pages=1987–1990|doi=10.1109/ICASSP.1987.1169345|url=http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1169345&isnumber=26345}}</ref>\n\nIn conventional 2-D vector-radix algorithm, we decompose the indices <math>k_1,k_2</math> into 4 groups:\n\n: <math>\n\\begin{array}{lcl}\nX(2 k_1, 2 k_2) & : & \\text{even-even} \\\\\nX(2 k_1, 2 k_2 +1) & : & \\text{even-odd} \\\\\nX(2 k_1 +1, 2 k_2) & : & \\text{odd-even} \\\\\nX(2 k_1+1, 2 k_2+1) & : & \\text{odd-odd}\n\\end{array}\n</math>\n\nBy the split vector-radix algorithm, the first three groups remain unchanged, the fourth odd-odd group is further decomposed into another four sub-groups, and seven groups in total:\n\n: <math>\n\\begin{array}{lcl}\nX(2 k_1, 2 k_2) & : & \\text{even-even} \\\\\nX(2 k_1, 2 k_2 +1) & : & \\text{even-odd} \\\\\nX(2 k_1 +1, 2 k_2) & : & \\text{odd-even} \\\\\nX(4 k_1+1, 4 k_2+1) & : & \\text{odd-odd} \\\\\nX(4 k_1+1, 4 k_2+3) & : & \\text{odd-odd} \\\\\nX(4 k_1+3, 4 k_2+1) & : & \\text{odd-odd} \\\\\nX(4 k_1+3, 4 k_2+3) & : & \\text{odd-odd} \n\\end{array}\n</math>\n\nThat means the fourth term in 2-D DIT radix-<math>(2\\times 2)</math> equation, <math>S_{11}(k_1,k_2) W_{N}^{k_1+k_2}</math> becomes:<ref name=\"Wu89\">{{cite journal|last1=Wu|first1=H.|last2=Paoloni|first2=F.|title=On the two-dimensional vector split-radix FFT algorithm|journal=IEEE Transactions on Acoustics, Speech, and Signal Processing|date=Aug 1989|volume=37|pages=1302–1304|doi=10.1109/29.31283|url=http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=31283&isnumber=1348}}</ref>\n\n: <math> A_{11}(k_1,k_2) W_N^{k_1+k_2} + A_{13}(k_1,k_2) W_N^{k_1+3 k_2} +A_{31}(k_1,k_2) W_N^{3 k_1+k_2} + A_{33}(k_1,k_2) W_N^{3(k_1+k_2)},</math>\n\nwhere <math>A_{ij}(k_1,k_2)=\\sum_{n_1=0}^{N/4-1} \\sum_{n_2=0}^{N/4-1} x[4 n_1 + i, 4 n_2 + j] \\cdot W_{N/4}^{n_1 k_1} W_{N/4}^{n_2 k_2}</math>\n\nThe 2-D N by N DFT is then obtained by successive use of the above decomposition, up to the last stage.\n\nIt has been shown that the split vector radix algorithm has saved about 30% of the complex multiplications and about the same number of the complex additions for typical <math>1024\\times 1024</math> array, compared with the vector-radix algorithm.<ref name=Pei87/>\n\n==References==\n{{reflist|30em}}\n\n[[Category:FFT algorithms]]\n[[Category:Digital signal processing]]\n[[Category:Discrete transforms]]"
    },
    {
      "title": "Beam and Warming scheme",
      "url": "https://en.wikipedia.org/wiki/Beam_and_Warming_scheme",
      "text": "In numerical mathematics, '''Beam and Warming scheme ''' or '''Beam–Warming implicit scheme''' introduced in 1978 by Richard M. Beam and R. F. Warming,<ref name=\"An Implicit Finite-Difference Algorithm for Hyperbolic Systems in Conservation-Law Form\">{{cite journal | title=An Implicit Finite-Difference Algorithm for Hyperbolic Systems in Conservation-Law Form | author=Richard M Beam, R.F Warming | journal=Journal of Computational Physics |date=September 1976  | volume=22 | issue=1 | pages=87–110 | doi=10.1016/0021-9991(76)90110-8}}</ref><ref name=\"An Implicit Factored Scheme for the Compressible Navier–Stokes Equations\">{{cite journal | title=An Implicit Factored Scheme for the Compressible Navier–Stokes Equations | author=Richard M. Beam; R. F. Warming | journal=AIAA Journal |date=April 1978  | volume=16 | issue=4 | doi=10.2514/3.60901}}</ref> is a second order accurate [[Explicit and implicit methods|implicit scheme]], mainly used for solving non-linear hyperbolic equation.  It is not used much nowadays.\n\n==Introduction==\nThis scheme is a spatially factored, non iterative, [[Alternating direction implicit method|ADI]] scheme and uses [[Backward Euler method|implicit Euler]] to perform the time Integration. The algorithm is in '''delta-form''',  linearized through implementation of a [[Taylor series|Taylor-series]]. Hence observed as increments of the conserved variables. In this an efficient factored algorithm is obtained by evaluating the spatial cross derivatives explicitly. This allows for direct derivation of scheme and efficient solution using this computational algorithm. The efficiency is because although it is three-time-level scheme, but requires only two time levels of data storage.  This results in unconditional stability. It is centered and needs the artificial dissipation operator to guarantee numerical stability.<ref name=\"An Implicit Finite-Difference Algorithm for Hyperbolic Systems in Conservation-Law Form\" />\n\nThe delta form of the equation produced has the advantageous property of stability (if existing) independent of the size of the time step.<ref name=\"Computational Fluid Mechanics and Heat Transfer, Third Edition\">{{cite book | title=Computational Fluid Mechanics and Heat Transfer, Third Edition | publisher=CRC Press | author=Richard H. Pletcher | year=2012 | isbn=978-1591690375}}</ref>\n\n==The method==\n[[File:Steps in beam and warming.png|thumb|right|300px]]\nConsider the inviscid [[Burgers' equation]] in one dimension\n: <math> \\frac{\\partial u}{\\partial t} = -u \\frac{\\partial u}{\\partial x} \\quad \\text{with } x\\in R </math>\n\nBurgers' equation in conservation form,\n:<math> \\frac{\\partial u}{\\partial t} = - \\frac{\\partial E}{\\partial x} </math>\n\nwhere :<math>  E = \\frac{u^2}{2} </math>\n<!--===Initial conditions ===\n:<math> u(x,0) = \\left\\{ \\begin{array}{cc}\n1 & 0 \\leq x < 2  \\\\\n0 & 2 \\leq x \\leq 4 \\\\ \\end{array} \\right. </math>-->\n\n===Taylor series expansion===\n[[File:Basis of Beam-warming.png|thumb|right|300px]]\nThe expansion of :<math>u^{n+1}_i</math>\n:<math>\nu^{n+1}_i = u^n_i + \\frac{1}{2} \\left[\\left. \\frac{\\partial u}{\\partial t} \\right|^{n}_i + \\left. \\frac{\\partial u}{\\partial t} \\right|^{n+1}_i \\right] \\, \\Delta t + O(\\Delta t^3)\n</math>\n\nThis is also known as the [[Trapezoidal rule|trapezoidal formula]].\n\n:<math>\n\\therefore \\frac{u^{n+1}_i - u^n_i}{\\Delta t} = -\\frac{1}{2} \\left( \\left.\\frac{\\partial E}{\\partial x}\\right|^{n+1}_i + \\left.\\frac{\\partial E}{\\partial x}\\right|^n_i + \\frac{\\partial}{\\partial x} \\left[ A(u^{n+1}_i - u^n_i)\\right] \\right)\n</math>\n:<math >\\because \\frac{\\partial u}{\\partial t} = -\\frac{ \\partial E}{\\partial x} </math>\n\n=== Tri-diagonal system ===\nResulting tri-diagonal system:\n:<math>\n\\begin{align}\n& - \\frac{\\Delta t}{4 \\, \\Delta x} \\left( A^n_{i-1} u^{n+1}_{i-1}\\right) + u^{n+1}_i + \\frac{\\Delta t}{4 \\, \\Delta x} \\left( A^n_{i+1} u^{n+1}_{i+1} \\right) \\\\[6pt]\n= {} & u^n_i - \\frac{1}{2} \\frac{\\Delta t}{\\Delta x} \\left( E^n_{i+1} - E^n_{i-1} \\right) + \\frac{\\Delta t}{4 \\, \\Delta x} \\left( A^n_{i+1} u^n_{i+1} - A^n_{i-1} u^n_{i-1} \\right)\n\\end{align}\n</math>\nThis resulted system of linear equations can be solved using the modified [[tridiagonal matrix algorithm]], also known as the Thomas algorithm.<ref name=\"Computational Fluid Dynamics\">{{cite book | title=Computational Fluid Dynamics, 2nd Edition | publisher=Cambridge University Press | author=Chung, T.J. | year=2010 | isbn=978-0521769693}}</ref>\n\n== Dissipation term ==\nUnder the condition of shock wave, dissipation term is required for [[Hyperbolic partial differential equation|nonlinear hyperbolic equations]] such as this. This is done to keep the solution under control and maintain convergence of the solution.\n:<math> D = -\\varepsilon_e (u^n_{i+2} - 4u^n_{i+1} + 6u^n_i - 4u^n_{i-1} + u^n_{i-2}) </math>\n\nThis term is added explicitly at level <math>n</math> to the right hand side. This is always used for successful computation where high-frequent oscillations are observed and must be suppressed.\n\n== Smoothing term ==\nIf only the stable solution is required, then in the equation to the right hand side a second-order [[Relaxation (iterative method)|smoothing term]] is added on the implicit layer.\nThe other term in the same equation can be second-order because it has no influence on the stable solution if\n:<math> \\nabla^n(U) = 0 </math>\n\nThe addition of smoothing term increases the number of steps required by three.\n\n==Properties==\nThis scheme is produced by combining the trapezoidal formula, linearization, factoring, Padt spatial differencing, the homogeneous property of the flux vectors (where applicable), and hybrid spatial differencing and is most suitable for nonlinear systems in conservation-law form. ADI algorithm retains the order of accuracy and the steady-state property while reducing the bandwidth of the system of equations.<ref name=\"Simplification of Beam and Warming's implicit scheme for two-dimensional compressible flows\">{{cite journal | title=Simplification of Beam and Warming's implicit scheme for two-dimensional compressible flows | author=Lee, Jon | journal=AIAA Journal |date=January 1992  | volume=30 | pages=266–268 | doi=10.2514/3.10908}}</ref>\nStability of the equation is\n:<math> L^2</math>-stable under CFL : <math>|a| \\, \\Delta t \\le 2 \\, \\Delta x </math>\nThe order of Truncation error is\n:<math> O ((\\Delta t)^2 + (\\Delta x) ^2)</math>\nThe result is smooth with considerable overshoot (that does not grow much with time).\n\n==References==\n{{reflist|30em}}\n\n[[Category:Finite differences]]\n[[Category:Numerical differential equations]]\n[[Category:Computational fluid dynamics]]"
    },
    {
      "title": "Carlson's theorem",
      "url": "https://en.wikipedia.org/wiki/Carlson%27s_theorem",
      "text": "In [[mathematics]], in the area of [[complex analysis]], '''Carlson's theorem''' is a [[uniqueness theorem]] which was discovered by [[Fritz Carlson|Fritz David Carlson]]. Informally, it states that two different analytic functions which do not grow very fast at infinity can not coincide at the integers. The theorem may be obtained from the [[Phragmén–Lindelöf theorem]], which is itself an extension of the [[maximum-modulus theorem]].\n\nCarlson's theorem is typically invoked to defend the uniqueness of a [[Newton series]] expansion. Carlson's theorem has generalized analogues for other expansions.\n\n==Statement of theorem==\n\nAssume that {{math|''f''}} satisfies the following three conditions: the first two conditions bound the growth of {{math|''f''}} at infinity, whereas the third one states that {{math|''f''}} vanishes on the non-negative integers.\n\n* {{math|''f''(''z'')}} is an [[entire function]] of [[exponential type]], meaning that\n\n::<math>|f(z)| \\leq C e^{\\tau|z|}, \\quad z \\in \\mathbb{C}</math>\n\n:for some real values {{math|''C''}}, {{math|''τ''}}.\n\n* There exists {{math|''c'' < {{pi}}}} such that\n\n::<math>|f(iy)| \\leq C e^{c|y|}, \\quad y \\in \\mathbb{R} </math>\n\n* {{math|1=''f''(''n'') = 0}} for any non-negative integer {{math|''n''}}.\n\nThen {{math|''f''}} is identically zero.\n\n==Sharpness==\n\n===First condition===\n\nThe first condition may be relaxed: it is enough to assume that {{math|''f''}} is analytic in {{math|Re ''z'' > 0}}, continuous in {{math|Re ''z'' ≥ 0}}, and satisfies\n\n:<math>|f(z)| \\leq C e^{\\tau|z|}, \\quad \\operatorname{Re} z > 0</math>\n\nfor some real values {{math|''C''}}, {{math|''τ''}}.\n\n===Second condition===\n\nTo see that the second condition is sharp, consider the function {{math|1=''f''(''z'') = sin({{pi}}''z'')}}. It vanishes on the integers; however, it grows exponentially on the imaginary axis with a growth rate of {{math|1=''c'' = {{pi}}}}, and indeed it is not identically zero.\n\n===Third condition===\nA result, due to {{harvtxt|Rubel|1956}}, relaxes the condition that {{math|''f''}} vanish on the integers. Namely, Rubel showed that the conclusion of the theorem remains valid if {{math|''f''}} vanishes on a subset {{math|''A'' ⊂ {{mset|0, 1, 2, …}}}} of [[upper density]] 1, meaning that\n\n:<math> \\limsup_{n \\to \\infty} \\frac{\\left| A  \\cap \\{0,1,\\cdots,n-1\\} \\right|}{n} = 1. </math>\n\nThis condition is sharp, meaning that the theorem fails for sets {{math|''A''}} of upper density smaller than 1.\n\n==Applications==\nSuppose {{math|''f''(''z'')}} is a function that possess all finite [[forward difference]]s <math>\\Delta^n f(0)</math>.  Consider then the [[Newton series]]\n\n:<math>g(z)=\\sum_{n=0}^\\infty {z \\choose n} \\Delta^n f(0)</math>\n\nwith <math>{z \\choose n}</math> is the [[binomial coefficient]] and <math>\\Delta^n f(0)</math> is the {{math|''n''}}-th [[forward difference]].  By construction, one then has that {{math|1=''f''(''k'') = ''g''(''k'')}} for all non-negative integers {{math|''k''}}, so that the difference {{math|1=''h''(''k'') = ''f''(''k'') − ''g''(''k'') = 0}}.  This is one of the conditions of Carlson's theorem; if {{math|''h''}} obeys the others,  then {{math|''h''}} is identically zero, and the finite differences for {{math|''f''}} uniquely determine its Newton series.  That is, if a Newton series for {{math|''f''}} exists, and the difference satisfies the Carlson conditions, then {{math|''f''}} is unique.\n\n==See also==\n*[[Newton series]]\n*[[Table of Newtonian series]]\n\n==References==\n* F. Carlson, ''Sur une classe de séries de Taylor'', (1914) Dissertation, Uppsala, Sweden, 1914.\n* {{cite journal | last1 = Riesz | first1 = M. | authorlink = M. Riesz | year = 1920 | title = Sur le principe de Phragmén–Lindelöf | url = | journal = Proceedings of the Cambridge Philosophical Society | volume = 20 | issue = | pages = 205–107 }}, cor '''21'''(1921) p.&nbsp;6. \n* {{cite journal | last1 = Hardy | first1 = G.H. | authorlink = G.H. Hardy | year = 1920 | title = On two theorems of F. Carlson and S. Wigert | url = | journal = Acta Mathematica | volume = 42 | issue = | pages = 327–339 | doi=10.1007/bf02404414}}\n* [[E.C. Titchmarsh]], ''The Theory of Functions (2nd Ed)'' (1939) Oxford University Press ''(See section 5.81)''\n* R. P. Boas, Jr., ''Entire functions'', (1954) Academic Press, New York.\n* {{cite journal | last1 = DeMar | first1 = R. | year = 1962 | title = Existence of interpolating functions of exponential type | url = | journal = Trans. Amer. Math. Soc. | volume = 105 | issue = 3| pages = 359–371 | doi=10.1090/s0002-9947-1962-0141920-6}}\n* {{cite journal | last1 = DeMar | first1 = R. | year = 1963 | title = Vanishing Central Differences | url = | journal = Proc. Amer. Math. Soc. | volume = 14 | issue = | pages = 64–67 | doi=10.1090/s0002-9939-1963-0143907-2}}\n* {{citation|mr=0081944|last=Rubel|first=L. A.|title=Necessary and sufficient conditions for Carlson's theorem on entire functions|journal=Trans. Amer. Math. Soc.|volume=83|issue=2|year=1956|pages=417&ndash;429|jstor=1992882|doi=10.1090/s0002-9947-1956-0081944-8|pmc=528143|pmid=16578453}}\n\n[[Category:Factorial and binomial topics]]\n[[Category:Finite differences]]\n[[Category:Theorems in complex analysis]]"
    },
    {
      "title": "Central differencing scheme",
      "url": "https://en.wikipedia.org/wiki/Central_differencing_scheme",
      "text": "[[File:Comparison .jpg|thumb|Figure 1.Comparison of different schemes]]\nIn [[applied mathematics]], the '''central differencing scheme''' is a [[finite difference method]]. The finite difference method optimizes the approximation for the differential operator in the central node of the considered patch and provides numerical solutions to differential equations.<ref>Computational fluid dynamics –T CHUNG, {{ISBN|0-521-59416-2}}</ref> The central differencing scheme is one of the schemes used to solve the integrated [[convection-diffusion equation]] and to calculate the transported property Φ at the e and w faces. The advantages of this method are that it is easy to understand and to implement, at least for simple material relations, and that its convergence rate is faster than some other finite differencing methods, such as forward and backward differencing.  The right hand side of the convection-diffusion equation which basically highlights the diffusion terms  can be represented using central difference approximation. Thus, in order to simplify the solution and analysis, linear interpolation can be used logically to compute the cell face values for the left hand side of this equation which is nothing but the convective terms. Therefore, cell face values of  property for a uniform grid can be written as <ref>An introduction to computational fluid dynamics by HK VERSTEEG  and W.MALALASEKERA, {{ISBN|0-582-21884-5}}</ref>\n\n: <math>\\Phi_e =(\\Phi_P + \\Phi_E)/2</math>\n\n: <math>\\Phi_w =(\\Phi_W + \\Phi_P)/2</math>\n\n==Steady-state convection diffusion equation==\n\nThe [[convection–diffusion equation]] is a collective representation of both diffusion and convection equations and describes or explains every physical phenomenon involving the two processes: convection and diffusion in transferring of particles, energy or other physical quantities inside a physical system. The convection-diffusion is as follows:<ref>An introduction to computational fluid dynamics by HK VERSTEEG  and W.MALALASEKERA, {{ISBN|0-582-21884-5}}</ref>\n\n: <math>\\operatorname{div}(\\rho u\\varphi)=\\operatorname{div}(\\Gamma\\nabla\\varphi)+S_\\varphi; \\,</math>\n\nhere Г is [[diffusion coefficient]] and Φ is the [[property]]\n\n==Formulation of steady-state convection diffusion equation==\n\nFormal [[Integral|integration]] of steady-state convection–diffusion equation over a [[control volume]] gives\n\n: <math>\\int\\limits_A \\, n\\cdot(\\rho u\\varphi)\\,dA = \\int\\limits_A \\,n \\cdot (\\Gamma\\nabla\\varphi)\\,dA+\\int\\limits_{CV}\\,S_\\varphi \\,dV</math> → Equation 1.\n\nThis equation represents flux balance in a control volume. The left hand side gives the net convective flux and the right hand side contains the net diffusive flux and the generation or destruction of the property within the control volume.\n\nIn the absence of source term equation one becomes\n\n: <math>{d \\over dx}(\\rho u\\varphi) = {d \\over dx}\\left( {d\\varphi \\over dx}\\right) </math> → Equation 2.\n\n[[Continuity equation]]:\n: <math>{d \\over dx}(\\rho u)=0 </math> → Equation 3.\n\n[[File:Interpolation value.png|thumb|Figure 2. Interpolation method]]\n\nAssuming a control volume and integrating equation 2 over control volume gives:\n    \n: <math>(\\rho u\\varphi A)_e  - (\\rho u\\varphi A)_w = (\\Gamma A d\\varphi /dx)_e -   (\\Gamma A d\\varphi /dx)_w</math> → '''Integrated convection–diffusion equation'''\n\nIntegration of equation 3 yields:\n\n: <math>(\\rho uA)_e - (\\rho uA)_w = 0</math>                                           →  '''Integrated continuity equation'''\n\nIt is convenient to define two variables to represent the convective mass flux per unit area and diffusion conductance at cell faces which is as follows :\n\n: <math>F = \\rho u</math>\n\n: <math>D = \\Gamma / \\delta x</math>\n\nAssuming <math>A_e = A_w</math> ,   we can write integrated convection–diffusion equation as:\n\n: <math>F_e \\varphi_e - F_w \\varphi_w = D_e( \\varphi_E - \\varphi_P ) - D_w(\\varphi_P - \\varphi_W)</math>\n\nAnd integrated continuity equation as:\n\n: <math>F_e - F_w = 0</math>\n\nIn central differencing scheme we try linear interpolation to compute cell face values for convection terms.\n\nFor a uniform grid we can write cell face values of property Φ as\n\n: <math>\\varphi_e = (\\varphi_E + \\varphi_P)/2, \\quad \\varphi_w = (\\varphi_P + \\varphi_W)/2</math>\n\nOn substituting this into integrated convection – diffusion equation we obtain,\n\n: <math>F_e\\frac{\\varphi_E + \\varphi_P}2 - F_w\\frac{\\varphi_W + \\varphi_P}2 = D_e(\\varphi_E - \\varphi_P) -  D_w(\\varphi_P - \\varphi_W)</math>\n\nAnd on rearranging,\n\n: <math>\\left[\\left(D_w + \\frac{F_w}2\\right) + \\left(D_e - \\frac{F_e}2\\right) + (F_e - F_w)\\right]\\varphi_P = \\left(D_w + \\frac{F_w}2 \\right)\\varphi_W + \\left(D_e - \\frac{F_e}2 \\right)\\varphi_E</math>\n\n<math>a_P \\varphi_P =  a_W \\varphi_W + a_E\\varphi_E</math>\n\n==Different aspects of central differencing scheme==\n\n=== Conservativeness ===\n\nConservation is ensured in central differencing scheme since overall flux balance is obtained by summing the net flux through each control volume taking into account the boundary fluxes for the control volumes around nodes 1 and 4. \n[[File:Typical Illustration.png|thumb|Figure 3.Typical illustration]]\n \nBoundary flux for control volume around node 1 and 4\n\n: <math>\n\\begin{align}\n& \\left[\\frac{\\Gamma_{e_1} (\\varphi_2 - \\varphi_1)}{ \\delta x} - q_A\\right] + \\left[ \\frac {\\Gamma_{e_2} (\\varphi_3 - \\varphi_2)}{ \\delta x} - \\frac{ \\Gamma_{w_2} (\\varphi_2 - \\varphi_1)}{ \\delta x}\\right] \\\\[10pt]\n+ {} & \\left[ \\frac{ \\Gamma_{e_3} (\\varphi_4 - \\varphi_3)}{\\delta x} - \\frac{\\Gamma_{w_3} (\\varphi_3 - \\varphi_2)}{\\delta x}\\right] + \\left[q_B -  \\frac{\\Gamma_{w_4} (\\varphi_4 - \\varphi_3)}{\\delta x}\\right] = q_B - q_A\n\\end{align}\n</math>\n\nbecause <math>\\Gamma_{e_1} = \\Gamma_{w_2} , \\Gamma_{e_2} = \\Gamma_{w_3} , \\Gamma_{e_3} = \\Gamma_{w_4}</math>\n\n=== Boundedness ===\n\nCentral differencing scheme satisfies first condition of [[Bounded set|Boundedness]].\n \nSince <math>F_e - F_w = 0</math> from continuity equation, therefore; <math>a_P \\varphi_P =  a_W \\varphi_W + a_E\\varphi_E</math>\n\nAnother essential requirement for Boundedness is that all coefficients of the discretised equations should have the same sign (usually all positive). But this is only satisfied when    ([[peclet number]]) <math>F_e/D_e < 2</math> because for a unidirectional flow(<math>F_e >0, F_w > 0</math>)   <math>a_E = (D_e - F_e/2)</math> is always positive if  <math>D_e > F_e/2</math>\n\n=== Transportiveness ===\n\nIt requires that transportiveness changes according to magnitude of peclet number i.e. when pe is zero  <math>\\varphi</math> is spread in all directions equally  and as Pe increases (convection&nbsp;>&nbsp;diffusion) <math>\\varphi</math>  at a point largely depends on upstream value and less on downstream value. But central differencing scheme does not possess Transportiveness at higher pe since Φ at a point is average of neighbouring nodes for all Pe.\n\n=== Accuracy ===\n\nThe [[Taylor series]] truncation error of the central differencing scheme is second order. \nCentral differencing scheme will be accurate only if Pe < 2.\nOwing to this limitation central differencing is not a suitable discretisation practice for general purpose flow calculations.\n\n==Applications of central differencing scheme==\n\n*Central difference type schemes are currently being applied on a regular basis in the solution of the [[Euler equations]] and [[Navier–Stokes equations]].\n*The results using central differencing approximation have demonstrated noticeable improvements in accuracy in smooth regions.\n*[[Shock wave]] representation and [[boundary-layer]] definition can be improved on coarse meshes<ref>{{cite journal | doi=10.1007/s002110050345 | volume=79 | issue=3 | title=Third order nonoscillatory central scheme for hyperbolic conservation laws | journal=Numerische Mathematik | pages=397–425|year = 1998|last1 = Liu|first1 = Xu-Dong| last2=Tadmor | first2=Eitan | citeseerx=10.1.1.26.4631 }}</ref>\n\n==Advantages==\n\n*The central-difference schemes are simpler to program and require less computer time per time step, and work well with multigrid [[acceleration]] techniques.\n*The central difference schemes have a free parameter in conjunction with the fourth-difference dissipation. \n*This dissipation is needed to approach a steady state.\n*This scheme is more accurate than the first order upwind scheme if Peclet number is less than 2.<ref>{{cite journal | doi=10.1007/s002110050345 | volume=79 | issue=3 | title=Third order nonoscillatory central scheme for hyperbolic conservation laws | journal=Numerische Mathematik | pages=397–425|year = 1998|last1 = Liu|first1 = Xu-Dong| last2=Tadmor | first2=Eitan | citeseerx=10.1.1.26.4631 }}</ref>\n\n==Disadvantages==\n\n* The central differencing scheme is somewhat more dissipative.\n* This scheme leads to [[oscillations]] in the solution or divergence if the local Peclet number is larger than 2.<ref>http://www.bakker.org/dartmouth06/engs150/05-solv.ppt</ref>\n\n==See also==\n\n*[[Finite difference method]]\n*[[Finite difference]]\n*[[Taylor series]]\n*[[Taylor theorem]]\n*[[Convection–diffusion equation]]\n*[[Diffusion]]\n*[[Convection]]\n*[[Peclet number]]\n*[[Linear interpolation]]\n*[[Symmetric derivative]]\n*[[Upwind differencing scheme for convection]]\n\n== References==\n{{Reflist}}\n\n== Further reading==\n\n* ''Computational Fluid Dynamics: The Basics with Applications'' – John D. Anderson, {{ISBN|0-07-001685-2}}\n* ''Computational Fluid Dynamics'' volume 1 – Klaus A. Hoffmann,  Steve T. Chiang, {{ISBN|0-9623731-0-9}}\n\n==External links==\n*[https://www.thermalfluidscentral.org/encyclopedia/index.php/ One-Dimensional_Steady-State_Convection_and_Diffusion#Central_Difference_Scheme]\n*[http://www.iue.tuwien.ac.at/phd/heinzl/node27.html Finite Differences]\n*[http://www.phy.davidson.edu/fachome/dmb/py200/centraldiff.htm Central Difference Methods]\n*[https://arxiv.org/abs/1303.3769/ A Conservative Finite Difference Scheme for Poisson–Nernst–Planck Equations]\n\n[[Category:Computational fluid dynamics]]\n[[Category:Finite differences]]\n[[Category:Numerical differential equations]]"
    },
    {
      "title": "Crank–Nicolson method",
      "url": "https://en.wikipedia.org/wiki/Crank%E2%80%93Nicolson_method",
      "text": "In [[numerical analysis]], the '''Crank–Nicolson method''' is a [[finite difference method]] used for numerically solving the [[heat equation]] and similar [[partial differential equations]].<ref>{{cite book | title = Convective Heat Transfer | author = Tuncer Cebeci | publisher = Springer | year = 2002 | isbn = 0-9668461-4-1 | url = https://books.google.com/?id=xfkgT9Fd4t4C&pg=PA257&dq=%22Crank-Nicolson+method%22 }}</ref> It is a [[Big O notation|second-order]] method in time. It is [[Explicit and implicit methods|implicit]] in time and can be written as an [[implicit Runge–Kutta method]], and it is [[Numerical stability|numerically stable]]. The method was developed by [[John Crank]] and [[Phyllis Nicolson]] in the mid 20th century.<ref>{{Cite journal \n | title = A practical method for numerical evaluation of solutions of partial differential equations of the heat conduction type\n | journal = Proc. Camb. Phil. Soc.\n | volume = 43 \n | issue = 1\n | year = 1947\n | pages = 50&ndash;67\n | doi = 10.1007/BF02127704 \n | last1 = Crank \n | first1 = J. \n | last2 = Nicolson \n | first2 = P. \n | postscript = .\n}}.</ref>\n\nFor [[diffusion equation]]s (and many other equations), it can be shown the Crank–Nicolson method is unconditionally [[Numerical stability|stable]].<ref>{{Cite book | last1=Thomas | first1=J. W. | title=Numerical Partial Differential Equations: Finite Difference Methods | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Texts in Applied Mathematics | isbn=978-0-387-97999-1 | year=1995 |volume=22 | postscript=. }}. Example 3.3.2 shows that Crank–Nicolson is unconditionally stable when applied to <math>u_t=au_{xx}</math>.</ref> However, the approximate solutions can still contain (decaying) spurious oscillations if the ratio of time step &Delta;{{var|t}} times the [[thermal diffusivity]] to the square of space step, &Delta;{{var|x}}{{sup|2}}, is large (typically larger than 1/2 per [[Von Neumann stability analysis]]). For this reason, whenever large time steps or high spatial resolution is necessary, the less accurate [[backward Euler method]] is often used, which is both stable and immune to oscillations.{{citation needed|reason=Why Crank-Nicolson method could contain oscillations but backward Euler method couldn't?|date=July 2016}}\n\n==The method==\n[[File:Crank-Nicolson-stencil.svg|thumb|200px|right|The Crank–Nicolson stencil for a 1D problem.]]\n\nThe Crank–Nicolson method is based on the [[trapezoidal rule (differential equations)|trapezoidal rule]], giving second-order convergence in time. For example, in one dimension, if the [[partial differential equation]] is\n\n:<math>\\frac{\\partial u}{\\partial t} = F\\left(u,\\, x,\\, t,\\, \\frac{\\partial u}{\\partial x},\\, \\frac{\\partial^2 u}{\\partial x^2}\\right)</math>\n\nthen, letting <math>u(i \\Delta x,\\, n \\Delta t) = u_{i}^{n}\\,</math>, the equation for Crank–Nicolson method is a combination of the [[Euler method|forward Euler method]] at <math>n</math> and the [[backward Euler method]] at ''n''&nbsp;+&nbsp;1 (note, however, that the method itself is ''not'' simply the average of those two methods, as the backward Euler equation has an implicit dependence on the solution):\n\n:<math>\\frac{u_{i}^{n + 1} - u_{i}^{n}}{\\Delta t} = \nF_{i}^{n}\\left(u,\\, x,\\, t,\\, \\frac{\\partial u}{\\partial x},\\, \\frac{\\partial^2 u}{\\partial x^2}\\right) \\qquad \\mbox{(forward Euler)}</math>\n\n:<math>\\frac{u_{i}^{n + 1} - u_{i}^{n}}{\\Delta t} = \nF_{i}^{n + 1}\\left(u,\\, x,\\, t,\\, \\frac{\\partial u}{\\partial x},\\, \\frac{\\partial^2 u}{\\partial x^2}\\right) \\qquad \\mbox{(backward Euler)}</math>\n\n:<math>\\frac{u_{i}^{n + 1} - u_{i}^{n}}{\\Delta t} = \n\\frac{1}{2}\\left[\nF_{i}^{n + 1}\\left(u,\\, x,\\, t,\\, \\frac{\\partial u}{\\partial x},\\, \\frac{\\partial^2 u}{\\partial x^2}\\right) + \nF_{i}^{n}\\left(u,\\, x,\\, t,\\, \\frac{\\partial u}{\\partial x},\\, \\frac{\\partial^2 u}{\\partial x^2}\\right)\n\\right] \\qquad \\mbox{(Crank-Nicolson)}.</math>\n\nNote that this is an ''implicit method'': to get the \"next\" value of ''u'' in time, a system of algebraic equations must be solved. If the partial differential equation is nonlinear, the [[temporal discretization|discretization]] will also be nonlinear so that advancing in time will involve the solution of a system of nonlinear algebraic equations, though linearizations are possible. In many problems, especially linear diffusion, the algebraic problem is [[tridiagonal]] and may be efficiently solved with the [[tridiagonal matrix algorithm]], which gives a fast <math>\\mathcal{O}(n)</math> direct solution as opposed to the usual <math>\\mathcal{O}(n^3)</math> for a full matrix.\n\n==Example: 1D diffusion==\n\nThe Crank–Nicolson method is often applied to [[Diffusion equation|diffusion problems]]. As an example, for linear diffusion,\n\n:<math>{\\partial u \\over \\partial t} = a \\frac{\\partial^2 u}{\\partial x^2}</math>\n\napplying a [[finite difference]] spatial discretization for the right hand side, the Crank–Nicolson discretization is then :\n\n:<math>\\frac{u_{i}^{n + 1} - u_{i}^{n}}{\\Delta t} = \\frac{a}{2 (\\Delta x)^2}\\left(\n(u_{i + 1}^{n + 1} - 2 u_{i}^{n + 1} + u_{i - 1}^{n + 1}) + \n(u_{i + 1}^{n} - 2 u_{i}^{n} + u_{i - 1}^{n})\n\\right)</math>\n\nor, letting <math>r = \\frac{a \\Delta t}{2(\\Delta x)^2}</math>:\n\n:<math>-r u_{i + 1}^{n + 1} + (1 + 2 r)u_{i}^{n + 1} - r u_{i - 1}^{n + 1} = r u_{i + 1}^{n} + (1 - 2 r)u_{i}^{n} + r u_{i - 1}^{n}\\,</math>\n\ngiven that the terms on the right-hand side of the equation are known, this is a [[tridiagonal]] problem, so that <math>u_{i}^{n + 1}\\,</math> may be efficiently solved by using the [[tridiagonal matrix algorithm]] in favor of a much more costly [[matrix inversion]].\n\nA quasilinear equation, such as (this is a minimalistic example and not general)\n\n:<math>\\frac{\\partial u}{\\partial t} = a(u) \\frac{\\partial^2 u}{\\partial x^2}</math>\n\nwould lead to a nonlinear system of algebraic equations which could not be easily solved as above; however, it is possible in some cases to linearize the problem by using the old value for <math>a</math>, that is <math>a_{i}^{n}(u)\\,</math> instead of <math>a_{i}^{n + 1}(u)\\,</math>. Other times, it may be possible to estimate <math>a_{i}^{n + 1}(u)\\,</math> using an explicit method and maintain stability.\n\n==Example: 1D diffusion with advection for steady flow, with multiple channel connections==\nThis is a solution usually employed for many purposes when there is a contamination problem in streams or rivers under steady flow conditions but information is given in one dimension only. Often the problem can be simplified into a 1-dimensional problem and still yield useful information.\n\nHere we model the concentration of a solute contaminant in water. This problem is composed of three parts: the known diffusion equation (<math>D_x</math> chosen as constant), an advective component (which means the system is evolving in space due to a velocity field), which we choose to be a constant ''Ux'', and a lateral interaction between longitudinal channels (k).\n\n{{NumBlk|:|<math>\\frac{\\partial C}{\\partial t} = D_x \\frac{\\partial^2 C}{\\partial x^2} - U_x \\frac{\\partial C}{\\partial x}- k (C-C_N)-k(C-C_M)</math>|{{EquationRef|1}}}}\n\nwhere ''C'' is the concentration of the contaminant and subscripts ''N'' and ''M'' correspond to ''previous'' and ''next'' channel.\n\nThe Crank–Nicolson method (where ''i'' represents position and ''j'' time) transforms each component of the PDE into the following:\n\n{{NumBlk|:|<math>\\frac{\\partial C}{\\partial t} \\Rightarrow \\frac{C_{i}^{j + 1} - C_{i}^{j}}{\\Delta t}</math>|{{EquationRef|2}}}}\n\n{{NumBlk|:|<math>\\frac{\\partial^2 C}{\\partial x^2}\\Rightarrow \\frac{1}{2 (\\Delta x)^2}\\left(\n(C_{i + 1}^{j + 1} - 2 C_{i}^{j + 1} + C_{i - 1}^{j + 1}) + \n(C_{i + 1}^{j} - 2 C_{i}^{j} + C_{i - 1}^{j})\n\\right)</math>|{{EquationRef|3}}}}\n\n{{NumBlk|:|<math>\\frac{\\partial C}{\\partial x} \\Rightarrow \\frac{1}{2}\\left(\n\\frac{(C_{i + 1}^{j + 1} - C_{i - 1}^{j + 1})}{2 (\\Delta x)} + \n \\frac{(C_{i + 1}^{j} - C_{i - 1}^{j})}{2 (\\Delta x)}\n\\right)</math>|{{EquationRef|4}}}}\n\n{{numBlk|:|<math>C\\Rightarrow \\frac{1}{2} (C_{i}^{j+1} + C_{i}^{j})</math>|{{EquationRef|5}}}}\n\n{{NumBlk|:|<math>C_N\\Rightarrow \\frac{1}{2} (C_{Ni}^{j+1} + C_{Ni}^{j})</math>|{{EquationRef|6}}}}\n\n{{numBlk|:|<math>C_M\\Rightarrow \\frac{1}{2} (C_{Mi}^{j+1} + C_{Mi}^{j}).</math>|{{EquationRef|7}}}}\n\nNow we create the following constants to simplify the algebra:\n\n:<math> \\lambda= \\frac{D_x\\Delta t}{2 \\Delta x^2}</math>\n\n:<math> \\alpha= \\frac{U_x\\Delta t}{4 \\Delta x}</math>\n\n:<math> \\beta= \\frac{k\\Delta t}{2}</math>\n\nand substitute ({{EquationNote|2}}), ({{EquationNote|3}}), ({{EquationNote|4}}), ({{EquationNote|5}}), ({{EquationNote|6}}), ({{EquationNote|7}}), ''α'', ''β'' and ''λ'' into ({{EquationNote|1}}). We then put the ''new time'' terms on the left (''j''&nbsp;+&nbsp;1) and the ''present time'' terms on the right (''j'') to get:\n\n:<math> -\\beta C_{Ni}^{j+1}-(\\lambda+\\alpha)C_{i-1}^{j+1} +(1+2\\lambda+2\\beta)C_{i}^{j+1}-(\\lambda-\\alpha)C_{i+1}^{j+1}-\\beta C_{Mi}^{j+1} = \\beta C_{Ni}^{j}+(\\lambda+\\alpha)C_{i-1}^{j} +(1-2\\lambda-2\\beta)C_{i}^{j}+(\\lambda-\\alpha)C_{i+1}^{j}+\\beta C_{Mi}^{j}.</math>\n\nTo model the ''first'' channel, we realize that it can only be in contact with the following channel (''M''), so the expression is simplified to:\n\n:<math> -(\\lambda+\\alpha)C_{i-1}^{j+1} +(1+2\\lambda+\\beta)C_{i}^{j+1}-(\\lambda-\\alpha)C_{i+1}^{j+1}-\\beta C_{Mi}^{j+1} = +(\\lambda+\\alpha)C_{i-1}^{j} +(1-2\\lambda-\\beta)C_{i}^{j}+(\\lambda-\\alpha)C_{i+1}^{j}+\\beta C_{Mi}^{j}.</math>\n\nIn the same way, to model the ''last'' channel, we realize that it can only be in contact with the previous channel (''N''), so the expression is simplified to:\n\n:<math> -\\beta C_{Ni}^{j+1}-(\\lambda+\\alpha)C_{i-1}^{j+1} +(1+2\\lambda+\\beta)C_{i}^{j+1}-(\\lambda-\\alpha)C_{i+1}^{j+1}= \\beta C_{Ni}^{j}+(\\lambda+\\alpha)C_{i-1}^{j} +(1-2\\lambda-\\beta)C_{i}^{j}+(\\lambda-\\alpha)C_{i+1}^{j}.</math>\n\nTo solve this linear system of equations we must now see that boundary conditions must be given first to the beginning of the channels:\n\n<math> C_0^{j}</math>: initial condition for the channel at present time step <br />\n<math> C_{0}^{j+1}</math>: initial condition for the channel at next time step <br />\n<math> C_{N0}^{j}</math>: initial condition for the previous channel to the one analyzed at present time step <br />\n<math> C_{M0}^{j}</math>: initial condition for the next channel to the one analyzed at present time step.\n\nFor the last cell of the channels (''z'') the most convenient condition becomes an adiabatic one, so\n\n:<math>\\frac{\\partial C}{\\partial x}_{x=z} = \n\\frac{(C_{i + 1} - C_{i - 1})}{2 \\Delta x}  = 0.</math>\n\nThis condition is satisfied if and only if (regardless of a null value)\n\n:<math> C_{i + 1}^{j+1} = C_{i - 1}^{j+1}. \\, </math>\n\nLet us solve this problem (in a matrix form) for the case of 3 channels and 5 nodes (including the initial boundary condition). We express this as a linear system problem:\n\n:<math> \\begin{bmatrix}AA\\end{bmatrix}\\begin{bmatrix}C^{j+1}\\end{bmatrix}=[BB][C^{j}]+[d]</math>\n\nwhere\n\n:<math> \\mathbf{C^{j+1}} = \\begin{bmatrix}\nC_{11}^{j+1}\\\\ C_{12}^{j+1} \\\\ C_{13}^{j+1} \\\\ C_{14}^{j+1} \n\\\\ C_{21}^{j+1}\\\\ C_{22}^{j+1} \\\\ C_{23}^{j+1} \\\\ C_{24}^{j+1} \n\\\\ C_{31}^{j+1}\\\\ C_{32}^{j+1} \\\\ C_{33}^{j+1} \\\\ C_{34}^{j+1} \n\\end{bmatrix}</math> &nbsp; and &nbsp;  <math>\\mathbf{C^{j}} = \\begin{bmatrix}\nC_{11}^{j}\\\\ C_{12}^{j} \\\\ C_{13}^{j} \\\\ C_{14}^{j} \n\\\\ C_{21}^{j}\\\\ C_{22}^{j} \\\\ C_{23}^{j} \\\\ C_{24}^{j} \n\\\\ C_{31}^{j}\\\\ C_{32}^{j} \\\\ C_{33}^{j} \\\\ C_{34}^{j}\n\\end{bmatrix}.</math>\n\nNow we must realize that ''AA'' and ''BB'' should be arrays made of four different subarrays (remember that only three channels are considered for this example but it covers the main part discussed above).\n\n: <math>\\mathbf{AA} = \\begin{bmatrix}\nAA1 & AA3 & 0\\\\\nAA3 & AA2 & AA3\\\\\n0 & AA3 & AA1\\end{bmatrix}</math> &nbsp; and &nbsp;\n\n: <math>\\mathbf{BB} = \\begin{bmatrix}\nBB1 & -AA3 & 0\\\\\n-AA3 & BB2 & -AA3\\\\\n0 & -AA3 & BB1\\end{bmatrix}</math> &nbsp;\n\nwhere the elements mentioned above correspond to the next arrays and an additional 4x4 full of zeros. Please note that the sizes of AA and BB are 12x12:\n\n: <math>\\mathbf{AA1} = \\begin{bmatrix}\n(1+2\\lambda+\\beta) & -(\\lambda-\\alpha) & 0 & 0 \\\\\n-(\\lambda+\\alpha) & (1+2\\lambda+\\beta) & -(\\lambda-\\alpha) & 0 \\\\\n0 & -(\\lambda+\\alpha) & (1+2\\lambda+\\beta) & -(\\lambda-\\alpha)\\\\\n0 & 0 & -2\\lambda & (1+2\\lambda+\\beta)\\end{bmatrix}</math> &nbsp; ,&nbsp;\n\n: <math>\\mathbf{AA2} = \\begin{bmatrix}\n(1+2\\lambda+2\\beta) & -(\\lambda-\\alpha) & 0 & 0 \\\\\n-(\\lambda+\\alpha) & (1+2\\lambda+2\\beta) & -(\\lambda-\\alpha) & 0 \\\\\n0 & -(\\lambda+\\alpha) & (1+2\\lambda+2\\beta) & -(\\lambda-\\alpha)\\\\\n0 & 0 & -2\\lambda & (1+2\\lambda+2\\beta) \\end{bmatrix}</math> &nbsp; ,&nbsp;\n\n: <math>\\mathbf{AA3} = \\begin{bmatrix}\n-\\beta & 0 & 0 & 0 \\\\\n0 & -\\beta & 0 & 0 \\\\\n0 & 0 & -\\beta & 0 \\\\\n0 & 0 & 0 & -\\beta \\end{bmatrix}</math> &nbsp; ,&nbsp;\n\n: <math>\\mathbf{BB1} = \\begin{bmatrix}\n(1-2\\lambda-\\beta) & (\\lambda-\\alpha) & 0 & 0 \\\\\n(\\lambda+\\alpha) & (1-2\\lambda-\\beta) & (\\lambda-\\alpha) & 0 \\\\\n0 & (\\lambda+\\alpha) & (1-2\\lambda-\\beta) & (\\lambda-\\alpha)\\\\\n0 & 0 & 2\\lambda & (1-2\\lambda-\\beta)\\end{bmatrix}</math> &nbsp; & &nbsp;\n\n: <math>\\mathbf{BB2} = \\begin{bmatrix}\n(1-2\\lambda-2\\beta) & (\\lambda-\\alpha) & 0 & 0 \\\\\n(\\lambda+\\alpha) & (1-2\\lambda-2\\beta) & (\\lambda-\\alpha) & 0 \\\\\n0 & (\\lambda+\\alpha) & (1-2\\lambda-2\\beta) & (\\lambda-\\alpha)\\\\\n0 & 0 & 2\\lambda & (1-2\\lambda-2\\beta) \\end{bmatrix}.</math>\n\nThe ''d'' vector here is used to hold the boundary conditions. In this example it is a 12x1 vector:\n\n: <math>\\mathbf{d} = \\begin{bmatrix}\n(\\lambda+\\alpha)(C_{10}^{j+1}+C_{10}^{j}) \\\\ 0 \\\\ 0 \\\\ 0 \\\\ (\\lambda+\\alpha)(C_{20}^{j+1}+C_{20}^{j}) \\\\ 0 \\\\ 0 \\\\ 0 \\\\ (\\lambda+\\alpha)(C_{30}^{j+1}+C_{30}^{j}) \\\\\n0\\\\\n0\\\\\n0\\end{bmatrix}.</math>\n\nTo find the concentration at any time, one must iterate the following equation:\n:<math> \\begin{bmatrix}C^{j+1}\\end{bmatrix}=\\begin{bmatrix}AA^{-1}\\end{bmatrix}([BB][C^{j}]+[d]).</math>\n\n==Example: 2D diffusion==\nWhen extending into two dimensions on a uniform [[Cartesian grid]], the derivation is similar and the results may lead to a system of [[Banded matrix|band-diagonal]] equations rather than [[Tridiagonal matrix|tridiagonal]] ones. The two-dimensional heat equation\n\n:<math>\\frac{\\partial u}{\\partial t} = a \\left(\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}\\right)</math>\n\ncan be solved with the Crank–Nicolson discretization of\n\n:<math>\\begin{align}u_{i,j}^{n+1} &= u_{i,j}^n + \\frac{1}{2} \\frac{a \\Delta t}{(\\Delta x)^2} \\big[(u_{i+1,j}^{n+1} + u_{i-1,j}^{n+1} + u_{i,j+1}^{n+1} + u_{i,j-1}^{n+1} - 4u_{i,j}^{n+1}) \\\\ & \\qquad {} + (u_{i+1,j}^{n} + u_{i-1,j}^{n} + u_{i,j+1}^{n} + u_{i,j-1}^{n} - 4u_{i,j}^{n})\\big]\\end{align}</math>\n\nassuming that a square grid is used so that <math>\\Delta x = \\Delta y</math>. This equation can be simplified somewhat by rearranging terms and using the [[Courant number|CFL number]]\n\n:<math>\\mu = \\frac{a \\Delta t}{(\\Delta x)^2}.</math>\n\nFor the Crank–Nicolson numerical scheme, a low [[Courant number|CFL number]] is not required for stability, however it is required for numerical accuracy. We can now write the scheme as:\n\n:<math>\\begin{align}&(1 + 2\\mu)u_{i,j}^{n+1} - \\frac{\\mu}{2}\\left(u_{i+1,j}^{n+1} + u_{i-1,j}^{n+1} + u_{i,j+1}^{n+1} + u_{i,j-1}^{n+1}\\right) \\\\ & \\quad = (1 - 2\\mu)u_{i,j}^{n} + \\frac{\\mu}{2}\\left(u_{i+1,j}^{n} + u_{i-1,j}^{n} + u_{i,j+1}^{n} + u_{i,j-1}^{n}\\right).\\end{align}</math>\n\nSolving such a linear system is not practical due to extremely high time complexity of solving a linear system by the means of [[Gaussian elimination]] or even [[Strassen algorithm]]. Hence an [[Alternating direction implicit method]] can be implemented to solve the numerical PDE whereby one dimension is treated implicitly and other dimension explicitly for half of the assigned time-step and vice versa for the remainder half of the time-step. The benefit of this strategy is that the implicit solver only requires a [[Tridiagonal matrix algorithm]] to be solved. The difference between the true Crank–Nicolson solution and ADI approximated solution has an order of accuracy of <math>O(\\Delta t^4)</math> and hence can be ignored with a sufficiently small time-step.<ref>{{cite web|title=Multi -Dimensional Parabolic Problems|url=http://www.cs.rpi.edu/~flaherje/pdf/pde5.pdf|website=Computer Science Department|publisher=RPI|accessdate=29 May 2016}}</ref>\n\n==Application in financial mathematics==\n{{Further information|Finite difference methods for option pricing}}\nBecause a number of other phenomena can be [[Mathematical model|modeled]] with the [[heat equation]] (often called the diffusion equation in [[financial mathematics]]), the Crank–Nicolson method has been applied to those areas as well.<ref>{{cite book | title = The Mathematics of Financial Derivatives: A Student Introduction | author1 = Wilmott, P. | author2 = Howison, S. | author3 = Dewynne, J. | publisher = Cambridge Univ. Press | year = 1995 | isbn = 0-521-49789-2 | url = https://books.google.com/books?hl=en&q=The%20Mathematics%20of%20Financial%20Derivatives%20Wilmott&um=1&ie=UTF-8&sa=N&tab=wp }}</ref> Particularly, the [[Black–Scholes]] option pricing model's [[differential equation]] can be transformed into the heat equation, and thus  [[numerical methods|numerical solutions]] for [[Valuation of options|option pricing]] can be obtained with the Crank–Nicolson method.\n\nThe importance of this for finance is that option pricing problems, when extended beyond the standard assumptions (e.g. incorporating changing dividends), cannot be solved in closed form, but can be solved using this method.  Note however, that for non-smooth final conditions (which happen for most financial instruments), the Crank–Nicolson method is not satisfactory as numerical oscillations are not damped.  For [[vanilla option]]s, this results in oscillation in the [[Greeks (finance)#Gamma|gamma value]] around the [[strike price]].  Therefore, special damping initialization steps are necessary (e.g., fully implicit finite difference method).\n\n==See also==\n*[[Financial mathematics]]\n*[[Trapezoidal rule]]\n\n==References==\n\n<references/>\n<!-- *Fitzpatrick R. (2003) ''[http://farside.ph.utexas.edu/~rfitzp/teaching/329/lectures/node80.html The Crank–Nicolson scheme]''. Retrieved May 4, 2005.\n*Pitman E. Bruce. (1999) ''[http://www.math.buffalo.edu/~pitman/courses/mth438/na/node16.html Parabolic equations]''. Retrieved May 4, 2005. -->\n\n\n==External links==\n*[http://www3.nd.edu/~dbalsara/Numerical-PDE-Course/ Numerical PDE Techniques for Scientists and Engineers], open access Lectures and Codes for Numerical PDEs\n*[http://scicomp.stackexchange.com/questions/7399/how-to-discretize-the-advection-equation-using-the-crank-nicolson-methods An example of how to apply and implement the Crank-Nicolson method for the Advection equation]\n\n{{Numerical integrators}}\n\n{{Numerical PDE}}\n\n{{DEFAULTSORT:Crank-Nicolson Method}}\n[[Category:Mathematical finance]]\n[[Category:Numerical differential equations]]\n[[Category:Finite differences]]"
    },
    {
      "title": "Delta operator",
      "url": "https://en.wikipedia.org/wiki/Delta_operator",
      "text": "In [[mathematics]], a '''delta operator''' is a shift-equivariant [[linear transformation|linear]] operator ''<math>Q\\colon\\mathbb{K}[x] \\longrightarrow \\mathbb{K}[x]</math>'' on the [[vector space]] of [[polynomial]]s in a variable <math>x</math> over a [[field (mathematics)|field]] <math>\\mathbb{K}</math> that reduces degrees by one.\n\nTo say that <math>Q</math> is '''shift-equivariant''' means that if <math>g(x) = f(x + a)</math>, then\n\n:<math>{ (Qg)(x) = (Qf)(x + a)}.\\,</math>\n\nIn other words, if ''<math>f</math>'' is a \"'''shift'''\" of ''<math>g</math>'', then ''<math>Qf</math>'' is also a shift of ''<math>Qg</math>'', and has the same \"'''shifting vector'''\" ''<math>a</math>''.\n\nTo say that ''an operator reduces degree by one'' means that if ''<math>f</math>'' is a polynomial of degree ''<math>n</math>'', then ''<math>Qf</math>'' is either a polynomial of degree <math>n-1</math>, or, in case <math>n = 0</math>, ''<math>Qf</math>'' is 0.\n\nSometimes a ''delta operator'' is defined to be a shift-equivariant linear transformation on polynomials in ''<math>x</math>'' that maps ''<math>x</math>'' to a nonzero constant.  Seemingly weaker than the definition given above, this latter characterization can be shown to be equivalent to the stated definition when <math> \\mathbb{K}</math> has characteristic zero, since shift-equivariance is a fairly strong condition.\n\n==Examples==\n\n* The forward [[difference operator]]\n\n:: <math> (\\Delta f)(x) = f(x + 1) - f(x)\\, </math>\n\n:is a delta operator.\n\n* [[Derivative|Differentiation]] with respect to ''x'', written as ''D'', is also a delta operator.\n* Any operator of the form\n::<math>\\sum_{k=1}^\\infty c_k D^k</math>\n: (where ''D''<sup>''n''</sup>(&fnof;) = &fnof;<sup>(''n'')</sup> is the ''n''<sup>th</sup> derivative) with <math>c_1\\neq0</math> is a delta operator.  It can be shown that all delta operators can be written in this form.  For example, the difference operator given above can be expanded as\n::<math>\\Delta=e^D-1=\\sum_{k=1}^\\infty \\frac{D^k}{k!}.</math>\n\n* The generalized derivative of [[time scale calculus]] which unifies the forward difference operator with the derivative of standard [[calculus]] is a delta operator.\n* In [[computer science]] and [[cybernetics]], the term \"discrete-time delta operator\" (&delta;) is generally taken to mean a difference operator\n\n:: <math>{(\\delta f)(x) = {{ f(x+\\Delta t) - f(x) }  \\over {\\Delta t} }}, </math>\n\n: the [[Euler approximation]] of the usual derivative with a discrete sample time <math>\\Delta t</math>. The delta-formulation obtains a significant number of numerical advantages compared to the shift-operator at fast sampling.\n\n==Basic polynomials==\n\nEvery delta operator ''<math>Q</math>'' has a unique sequence of \"basic polynomials\", a [[polynomial sequence]] defined by three conditions:\n\n* <math>p_0(x)=1 ;</math>\n* <math>p_{n}(0)=0;</math>\n* <math>(Qp_n)(x)=np_{n-1}(x), \\; \\forall n \\in \\mathbb N.</math>\n\nSuch a sequence of basic polynomials is always of [[binomial type]], and it can be shown that no other sequences of binomial type exist.  If the first two conditions above are dropped, then the third condition says this polynomial sequence is a [[Sheffer sequence]]—a more general concept.\n\n== See also ==\n\n* [[Pincherle derivative]]\n* [[Shift operator]]\n* [[Umbral calculus]]\n\n== References ==\n* {{Citation | last1=Nikol'Skii | first1=Nikolai Kapitonovich | title=Treatise on the shift operator: spectral function theory | publisher=[[Springer-Verlag]] | location=Berlin, New York | isbn=978-0-387-15021-5 | year=1986}}\n\n[[Category:Linear algebra]]\n[[Category:Polynomials]]\n[[Category:Finite differences]]"
    },
    {
      "title": "Difference polynomials",
      "url": "https://en.wikipedia.org/wiki/Difference_polynomials",
      "text": "{{expert-subject|Mathematics|date=November 2009}}\nIn [[mathematics]], in the area of [[complex analysis]], the '''general difference polynomials''' are a [[polynomial sequence]], a certain subclass of the [[Sheffer polynomials]], which include the [[Newton polynomial]]s, '''Selberg's polynomials''', and the '''Stirling interpolation polynomials''' as special cases. \n\n==Definition==\nThe general difference polynomial sequence is given by\n\n:<math>p_n(z)=\\frac{z}{n} {{z-\\beta n -1} \\choose {n-1}}</math>\n\nwhere <math>{z \\choose n}</math> is the [[binomial coefficient]].  For <math>\\beta=0</math>, the generated polynomials <math>p_n(z)</math> are the Newton polynomials\n\n:<math>p_n(z)= {z \\choose n} = \\frac{z(z-1)\\cdots(z-n+1)}{n!}.</math>\n\nThe case of <math>\\beta=1</math> generates Selberg's polynomials, and the case of <math>\\beta=-1/2</math> generates Stirling's interpolation polynomials.\n\n==Moving differences==\nGiven an [[analytic function]] <math>f(z)</math>, define the '''moving difference''' of ''f'' as\n\n:<math>\\mathcal{L}_n(f) = \\Delta^n f (\\beta n)</math>\n\nwhere <math>\\Delta</math> is the [[forward difference operator]]. Then, provided that ''f'' obeys certain summability conditions, then it may be represented in terms of these polynomials as\n\n:<math>f(z)=\\sum_{n=0}^\\infty p_n(z) \\mathcal{L}_n(f).</math>\n\nThe conditions for summability (that is, convergence) for this sequence is a fairly complex topic; in general, one may say that a necessary condition is that the analytic function be of less than [[exponential type]]. Summability conditions are discussed in detail in Boas & Buck.\n\n==Generating function==\nThe [[generating function]] for the general difference polynomials is given by\n\n:<math>e^{zt}=\\sum_{n=0}^\\infty p_n(z) \n\\left[\\left(e^t-1\\right)e^{\\beta t}\\right]^n.</math>\n\nThis generating function can be brought into the form of the [[generalized Appell representation]] \n\n:<math>K(z,w) = A(w)\\Psi(zg(w)) = \\sum_{n=0}^\\infty p_n(z) w^n</math>\n\nby setting <math>A(w)=1</math>, <math>\\Psi(x)=e^x</math>, <math>g(w)=t</math> and <math>w=(e^t-1)e^{\\beta t}</math>.\n\n==See also==\n* [[Carlson's theorem]]\n* [[Bernoulli polynomials of the second kind]]\n\n==References==\n{{reflist}}\n* [[Ralph P. Boas Jr.|Ralph P. Boas, Jr.]] and [[Robert Creighton Buck|R. Creighton Buck]], ''Polynomial Expansions of Analytic Functions (Second Printing Corrected)'', (1964) Academic Press Inc., Publishers New York, Springer-Verlag, Berlin. Library of Congress Card Number 63-23263.\n\n[[Category:Polynomials]]\n[[Category:Finite differences]]\n[[Category:Factorial and binomial topics]]"
    },
    {
      "title": "Discrete Laplace operator",
      "url": "https://en.wikipedia.org/wiki/Discrete_Laplace_operator",
      "text": "{{refimprove|date=December 2007}}\n:''For the discrete equivalent of the [[Laplace transform]], see [[Z-transform]].''\n\nIn [[mathematics]], the '''discrete Laplace operator''' is an analog of the continuous [[Laplace operator]], defined so that it has meaning on a [[Graph (discrete mathematics)|graph]] or a [[lattice (group)|discrete grid]]. For the case of a finite-dimensional graph (having a finite number of edges and vertices), the discrete Laplace operator is more commonly called the [[Laplacian matrix]].\n\nThe discrete Laplace operator occurs in [[physics]] problems such as the [[Ising model]] and [[loop quantum gravity]], as well as in the study of discrete [[dynamical system]]s. It is also used in [[numerical analysis]] as a stand-in for the continuous Laplace operator. Common applications include [[image processing]], where it is known as the [[Laplace filter]], and in machine learning for [[cluster analysis|clustering]] and [[semi-supervised learning]] on neighborhood graphs.\n\n==Definitions==\n\n===Graph Laplacians===\nThere are various definitions of the ''discrete Laplacian'' for [[Graph (discrete mathematics)|graphs]], differing by sign and scale factor (sometimes one averages over the neighboring vertices, other times one just sums; this makes no difference for a [[regular graph]]). The traditional definition of the graph Laplacian, given below, corresponds to the '''negative''' continuous Laplacian on a domain with a free boundary.\n\nLet <math>G = (V,E)</math> be a graph with vertices <math>\\scriptstyle V</math> and edges <math>\\scriptstyle E</math>. Let <math>\\phi\\colon V\\to R</math> be a [[function (mathematics)|function]] of the vertices taking values in a [[ring (mathematics)|ring]]. Then, the discrete Laplacian <math>\\Delta</math> acting on <math>\\phi</math> is defined by\n\n:<math>(\\Delta \\phi)(v)=\\sum_{w:\\,d(w,v)=1}\\left[\\phi(v)-\\phi(w)\\right]</math>\n\nwhere <math>d(w,v)</math> is the [[Distance (graph theory)|graph distance]] between vertices w and v. Thus, this sum is over the nearest neighbors of the vertex ''v''. For a graph with a finite number of edges and vertices, this definition is identical to that of the [[Laplacian matrix]]. That is, <math> \\phi</math> can be written as a column vector; and so <math>\\Delta\\phi</math> is the product of the column vector and the Laplacian matrix, while <math>(\\Delta \\phi)(v)</math> is just the ''v'''th entry of the product vector.\n\nIf the graph has weighted edges, that is, a weighting function <math>\\gamma\\colon E\\to R</math> is given, then the definition can be generalized to\n\n:<math>(\\Delta_\\gamma\\phi)(v)=\\sum_{w:\\,d(w,v)=1}\\gamma_{wv}\\left[\\phi(v)-\\phi(w)\\right]</math>\n\nwhere <math>\\gamma_{wv}</math> is the weight value on the edge <math>wv\\in E</math>.\n\nClosely related to the discrete Laplacian is the '''averaging operator''':\n\n:<math>(M\\phi)(v)=\\frac{1}{\\deg v}\\sum_{w:\\,d(w,v)=1}\\phi(w).</math>\n\n===Mesh Laplacians===\n\nIn addition to considering the connectivity of nodes and edges in a graph, mesh laplace operators take into account the geometry of a surface (e.g. the angles at the nodes). For a [[manifold]] triangle mesh, the [[Laplace-Beltrami operator]] of a scalar function <math>u</math> at a vertex <math>i</math> can be approximated as\n\n:<math>\n(\\Delta u)_{i} \\approx \\frac{1}{2A_i} \\Sigma_{j} (\\cot \\alpha_{ij} + \\cot \\beta_{ij}) (u_j - u_i),\n</math>\n\nwhere the sum is over all adjacent vertices <math>j</math> of <math>i</math>, <math>\\alpha_{ij}</math> and <math>\\beta_{ij}</math> are the two angles opposite of the edge <math>ij</math>, and <math>A_i</math> is the ''vertex area'' of <math>i</math>; that is, one third of the summed areas of triangles incident to <math>i</math>. The above cotangent formula can be derived using many different methods among which are [[finite element method|piecewise linear finite elements]], [[finite volume method|finite volumes]] (see [http://ddg.cs.columbia.edu/SGP2014/LaplaceBeltrami.pdf] for a derivation), and [[discrete exterior calculus]] (see [https://www.cs.cmu.edu/~kmcrane/Projects/DDG/paper.pdf]).\n\nTo facilitate computation, the Laplacian is encoded in a matrix <math>L\\in\\mathbb{R}^{|V|\\times|V|}</math> such that <math> Lu = (\\Delta u)_i </math>. Let <math>C</math> be the (sparse) ''cotangent matrix'' with entries\n\n<math>\nC_{ij} =  \n\\begin{cases} \n      \\frac{1}{2}(\\cot \\alpha_{ij} + \\cot \\beta_{ij}) & ij \\text{ is an edge}, \\\\\n      -\\sum\\limits_{i \\neq j}C_{ij} & i = j,  \\\\\n      0 & \\text{otherwise,}\n     \n\\end{cases}\n</math>\n\nand let <math> M </math> be the diagonal ''mass matrix'' <math> M </math> whose <math>i</math>-th entry along the diagonal is the vertex area <math> A_i </math>. Then <math> L=M^{-1}C </math> is the sought discretization of the Laplacian.\n\nA more general over view of mesh operators is given in.<ref name=\"reuter06\">{{cite journal\n |author = Reuter, M. and Biasotti, S. and Giorgi, D. and Patane, G. and Spagnuolo, M.\"\n | year = 2009\n | title = Discrete Laplace-Beltrami operators for shape analysis and segmentation\n | journal = Computers & Graphics\n | volume = 33\n | issue = 3\n | pages = 381–390df\n | doi=10.1016/j.cag.2009.03.005\n| citeseerx = 10.1.1.157.757\n }}</ref>\n\n=== Finite differences ===\n\nApproximations of the [[Laplacian]], obtained by the [[finite-difference method]] or by the [[finite-element method]], can also be called '''discrete Laplacians'''.  For example, the Laplacian in two dimensions can be approximated using the [[five-point stencil]] [[finite-difference method]], resulting in\n\n:<math> \\Delta f(x,y) \\approx \\frac{f(x-h,y) + f(x+h,y) + f(x,y-h) + f(x,y+h) - 4f(x,y)}{h^2}, </math>\n\nwhere the grid size is ''h'' in both dimensions, so that the five-point stencil of a point (''x'',&nbsp;''y'') in the grid is\n\n:<math>\\{(x-h, y), (x, y), (x+h, y), (x, y-h), (x, y+h)\\}.</math>\n\nIf the grid size ''h'' = 1, the result is the '''negative''' discrete Laplacian on the graph, which is the [[square lattice|square lattice grid]]. There are no constraints here on the values of the function ''f''(''x'', ''y'') on the boundary of the lattice grid, thus this is the case of the homogeneous [[Neumann boundary condition]], i.e., free boundary. Other types of boundary conditions, e.g., the homogeneous [[Dirichlet boundary condition]], where ''f''(''x'', ''y'') = 0 on the boundary of the grid, are rarely used for graph Laplacians, but are common in other applications.\n\nMultidimensional discrete Laplacians on [[Cuboid#Rectangular cuboid|rectangular cuboid]] [[regular grid]]s have very special properties, e.g., they are [[Kronecker product#Kronecker sum and exponentiation|Kronecker sums]] of one-dimensional discrete Laplacians, see [[Kronecker sum of discrete Laplacians]], in which case all its [[eigenvalue]]s and [[eigenvectors]] can be explicitly calculated.\n\n===Finite-element method===\n\nIn this approach, the domain is discretized into smaller elements, often triangles or tetrahedra, but other elements such as rectangles or cuboids are possible. The solution space is then approximated using so called form-functions of a pre-defined degree. The differential equation containing the Laplace operator is then transformed into a variational formulation, and a system of equations is constructed (linear or eigenvalue problems). The resulting matrices are usually very sparse and can be solved with iterative methods.\n\n===Image Processing===\nDiscrete Laplace operator is often used in image processing e.g. in edge detection and motion estimation applications.<ref name=\"forsyth03\">{{cite journal\n |author1=Forsyth, D. A.  |author2=Ponce, J.\n | year = 2003\n | title = Computer Vision\n |journal=Computers & Graphics\n |volume=33\n |issue=3\n |pages=381–390\n | doi=10.1016/j.cag.2009.03.005\n|citeseerx=10.1.1.157.757\n }}</ref> The discrete Laplacian is defined as the sum of the second derivatives [[Laplace operator#Coordinate expressions]] and calculated as sum of differences over the nearest neighbours of the central pixel.\n\n====Implementation via operator discretization====\nFor one-, two- and three-dimensional signals, the discrete Laplacian can be given as [[convolution]] with the following kernels:\n:1D filter: <math>\\vec{D}^2_x=\\begin{bmatrix}1 & -2 & 1\\end{bmatrix}</math>,\n:2D filter: <math>\\mathbf{D}^2_{xy}=\\begin{bmatrix}0 & 1 & 0\\\\1 & -4 & 1\\\\0 & 1 & 0\\end{bmatrix}</math>.\n<math>\\mathbf{D}^2_{xy}</math> corresponds to the finite-difference formula seen previously.  Other approximations include the diagonals:\n:2D filter: <math>\\mathbf{D}^2_{xy}=\\begin{bmatrix}0.25 & 0.5 & 0.25\\\\0.5 & -3 & 0.5\\\\0.25 & 0.5 & 0.25\\end{bmatrix}</math>,\n:3D filter: <math>\\mathbf{D}^2_{xyz}</math> is given by: first plane = <math>\\begin{bmatrix}0 & 0 & 0\\\\0 & 1 & 0\\\\0 & 0 & 0\\end{bmatrix}</math>; second plane = <math>\\begin{bmatrix}0 & 1 & 0\\\\1 & -6 & 1\\\\0 & 1 & 0\\end{bmatrix}</math>; third plane = <math>\\begin{bmatrix}0 & 0 & 0\\\\0 & 1 & 0\\\\0 & 0 & 0\\end{bmatrix}</math>.\n:''{{var|n}}D filter'': For the element <math>a_{x_1, x_2, \\dots , x_n}</math> of the kernel <math>\\mathbf{D}^2_{x_1, x_2, \\dots , x_n},</math>\n::<math>a_{x_1, x_2, \\dots , x_n} = \\left\\{\\begin{array}{ll}\n-2n & \\text{if } s = n, \\\\\n1 & \\text{if } s = n - 1, \\\\\n0 & \\text{otherwise,}\n\\end{array}\\right.</math>\n:where {{math|{{var|x}}{{sub|{{var|i}}}}}} is the position (either {{math|−1}}, {{math|0}} or {{math|1}}) of the element in the kernel in the {{var|i}}-th direction, and {{math|{{var|s}}}} is the number of directions {{math|{{var|i}}}} for which {{math|{{var|x}}{{sub|{{var|i}}}} {{=}} 0}}.\n\nNote that the ''n''D version, which is based on the graph generalization of the Laplacian, assumes all neighbors to be at an equal distance, and hence leads to the following 2D filter with diagonals included, rather than the version above:\n:2D filter: <math>\\mathbf{D}^2_{xy}=\\begin{bmatrix}1 & 1 & 1\\\\1 & -8 & 1\\\\1 & 1 & 1\\end{bmatrix}.</math>\n\nThese kernels are deduced by using discrete differential quotients.\n\nIn can be shown<ref name=lin90>[http://www.nada.kth.se/~tony/abstracts/Lin90-PAMI.html Lindeberg, T., \"Scale-space for discrete signals\", PAMI(12), No. 3, March 1990, pp. 234–254.]</ref><ref name=lin94>[http://www.csc.kth.se/~tony/book.html Lindeberg, T., Scale-Space Theory in Computer Vision, Kluwer Academic Publishers, 1994], {{isbn|0-7923-9418-6}}.</ref> that the following discrete approximation of the two-dimensional Laplacian operator as a convex combination of difference operators\n\n:<math>\\nabla^2_{\\gamma}= (1 - \\gamma) \\nabla^2_{5} + \\gamma \\nabla ^2_{\\times} \n         = (1 - \\gamma) \\begin{bmatrix}0 & 1 & 0\\\\1 & -4 & 1\\\\0 & 1 & 0\\end{bmatrix}\n            + \\gamma \\begin{bmatrix}1/2 & 0 & 1/2\\\\0 & -2 & 0\\\\1/2 & 0 & 1/2\\end{bmatrix}\n</math>\n\nfor γ ∈ [0, 1] is compatible with discrete scale-space properties, where specifically the value γ = 1/3 gives the best approximation of rotational symmetry. Regarding three-dimensional signals, it is shown<ref name=lin94/> that the Laplacian operator can be approximated by the two-parameter family of difference operators\n\n:<math>\n\\nabla^2_{\\gamma_1,\\gamma_2} \n         = (1 - \\gamma_1 - \\gamma_2) \\, \\nabla_7^2  + \\gamma_1 \\, \\nabla_{+^3}^2 +  \\gamma_2 \\, \\nabla_{\\times^3}^2 ),\n</math>\n\nwhere\n\n:<math>\n  (\\nabla_7^2 f)_{0, 0, 0}\n       =\n        f_{-1, 0, 0} + f_{+1, 0, 0} + f_{0, -1, 0} + f_{0, +1, 0} + f_{0, 0, -1} + f_{0, 0, +1} - 6 f_{0, 0, 0},\n</math>\n:<math>\n  (\\nabla_{+^3}^2 f)_{0, 0, 0}\n      = \\frac{1}{4}\n          (f_{-1, -1, 0} + f_{-1, +1, 0} + f_{+1, -1, 0} + f_{+1, +1, 0} \n        + f_{-1, 0, -1} + f_{-1, 0, +1} + f_{+1, 0, -1} + f_{+1, 0, +1} \n        + f_{0, -1, -1} + f_{0, -1, +1} + f_{0, +1, -1} + f_{0, +1, +1}\n          - 12 f_{0, 0, 0}),\n</math>\n:<math>\n   (\\nabla_{\\times^3}^2 f)_{0, 0, 0}\n  = \\frac{1}{4}\n          (f_{-1, -1, -1} + f_{-1, -1, +1} + f_{-1, +1, -1} + f_{-1, +1, +1}\n         + f_{+1, -1, -1} + f_{+1, -1, +1} + f_{+1, +1, -1} + f_{+1, +1, +1}\n           -  8 f_{0, 0, 0}).\n</math>\n\n====Implementation via continuous reconstruction====\nA discrete signal, comprising images,  can be viewed as a discrete representation of a continuous function <math>f(\\bar r)</math>, where the coordinate vector <math>\\bar r \\in R^n </math> and the value domain is real <math>f\\in R</math>.\nDerivation operation is therefore directly applicable to the continuous function, <math>f</math>.\nIn particular any discrete image, with reasonable presumptions on the discretization process, e.g. assuming band limited functions, or wavelets expandable functions, etc. can be reconstructed by means of well-behaving interpolation functions underlying the reconstruction formulation <ref name=\"bigun06vd\">{{cite book\n |author1=Bigun, J.\n | year = 2006\n | title =  Vision with Direction\n | publisher = Springer\n | doi=10.1007/b138918\n| isbn = 978-3-540-27322-6\n }}</ref>,\n:<math>\nf(\\bar r)=\\sum_{k\\in K}f_k \\mu_k(\\bar r) \n</math>\nwhere <math>f_k\\in R</math> are discrete representations of <math>f</math> on grid <math>K</math> and <math>\\mu_k </math> are interpolation functions specific to the grid <math>K</math>. On a uniform grid, such as images,  and for bandlimited functions,   interpolation functions are shift invariant amounting to  <math>\\mu_k(\\bar r)= \\mu(\\bar r-\\bar r_k) </math> with <math>\\mu </math> being an appropriately dilated [[sinc function]] defined in <math>n</math>-dimensions i.e. <math>\\bar r=(x_1,x_2...x_n)^T</math>. Other approximations of <math>\\mu</math> on uniform grids, are appropriately dilated [[Gaussian function]]s in <math>n</math>-dimensions.  Accordingly the discrete Laplacian becomes a discrete version of the Laplacian  of the continuous <math>f(\\bar r)</math>  \n:<math>\n\\nabla^2 f(\\bar r_k)= \\sum_{k'\\in K}f_{k'} (\\nabla^2 \\mu(\\bar r-\\bar r_{k'}))|_{\\bar r= \\bar r_k}\n</math>\nwhich in turn is a convolution with the Laplacian of the interpolation function on the uniform (image) grid <math>K</math>.  \nAn advantage of using Gaussians as interpolation functions is that they yield linear operators, including Laplacians,  that are free from rotational artifacts of the coordinate frame in which <math>f</math> is represented via <math>f_k</math>, in <math>n</math>-dimensions, and are frequency aware by definition.  A linear operator has not only a limited range in the <math>\\bar r</math> domain but also an effective range in the frequency domain (alternatively Gaussian scale space) which can be controlled explicitly via the variance of the Gaussian in a principled manner. The resulting filtering can be implemented by separable filters and [[decimation (signal processing)]]/[[pyramid (image processing)]] representations for further computational efficiency in <math>n</math>-dimensions. In other words, the discrete Laplacian filter of any size can be generated conveniently as the sampled Laplacian of Gaussian with spatial size befitting the needs of a particular application as controlled by its variance. Monomials which are non-linear operators can also be implemented using a similar reconstruction and approximation approach provided that the signal is sufficiently over-sampled.  Thereby, such non-linear operators e.g.  [[Structure Tensor]], and [[Generalized Structure Tensor]] which are used in pattern recognition for their total least-square optimality in orientation estimation, can be realized.\n\n==Spectrum==\nThe spectrum of the discrete Laplacian on an infinite grid is of key interest; since it is a [[self-adjoint operator]], it has a real spectrum. For the convention <math>\\Delta = I - M</math> on <math>Z</math>, the spectrum lies within <math>[0,2]</math> (as the averaging operator has spectral values in <math>[-1,1]</math>). This may also be seen by applying the Fourier transform. Note that the discrete Laplacian on an infinite grid has purely absolutely continuous spectrum, and therefore, no eigenvalues or eigenfunctions.\n\n==Theorems==\nIf the graph is an infinite [[square lattice|square lattice grid]], then this definition of the Laplacian can be shown to correspond to the continuous Laplacian in the limit of an infinitely fine grid.  Thus, for example, on a one-dimensional grid we have\n\n:<math>\\frac{\\partial^2F}{\\partial x^2} = \n\\lim_{\\epsilon \\rightarrow 0} \n  \\frac{[F(x+\\epsilon)-F(x)]+[F(x-\\epsilon)-F(x)]}{\\epsilon^2}.\n</math>\n\nThis definition of the Laplacian is commonly used in [[numerical analysis]] and in [[image processing]]. In image processing, it is considered to be a type of [[digital filter]], more specifically an [[edge filter]], called the [[Laplace filter]].\n\n==Discrete Schrödinger operator==\nLet <math>P:V\\rightarrow R</math> be a [[potential]] function defined on the graph.  Note that ''P'' can be considered to be a multiplicative operator acting diagonally on <math>\\phi</math>\n\n:<math>(P\\phi)(v)=P(v)\\phi(v).</math>\n\nThen <math>H=\\Delta+P</math> is the '''discrete Schrödinger operator''', an analog of the continuous [[Schrödinger equation|Schrödinger operator]].\n\nIf the number of edges meeting at a vertex is uniformly bounded, and the potential is bounded, then ''H'' is bounded and [[self-adjoint]].\n\nThe [[spectrum of an operator|spectral properties]] of this Hamiltonian can be studied with [[Stone space|Stone's theorem]]; this is a consequence of the duality between [[poset]]s and [[Boolean algebra (structure)|Boolean algebra]]s.\n\nOn regular lattices, the operator typically has both traveling-wave as well as [[Anderson localization]] solutions, depending on whether the potential is periodic or random.\n\n==Discrete Green's function==\nThe [[Green's function]] of the discrete [[Schrödinger operator]] is given in the [[resolvent formalism]] by \n:<math>G(v,w;\\lambda)=\\left\\langle\\delta_v\\left| \\frac{1}{H-\\lambda}\\right| \\delta_w\\right\\rangle </math>\nwhere <math>\\delta_w</math> is understood to be the [[Kronecker delta]] function on the graph: <math>\\delta_w(v)=\\delta_{wv}</math>; that is, it equals ''1'' if ''v''=''w'' and ''0'' otherwise.\n\nFor fixed <math>w\\in V</math> and <math>\\lambda</math> a complex number, the Green's function considered to be a function of ''v'' is the unique solution to\n\n:<math>(H-\\lambda)G(v,w;\\lambda)=\\delta_w(v).</math>\n\n== ADE classification ==\n{{further|ADE classification}}\nCertain equations involving the discrete Laplacian only have solutions on the simply-laced [[Dynkin diagram]]s (all edges multiplicity 1), and are an example of the [[ADE classification]]. Specifically, the only positive solutions to the homogeneous equation:\n:<math>\\Delta \\phi = \\phi,</math>\nin words,\n:\"Twice any label is the sum of the labels on adjacent vertices,\"\nare on the extended (affine) ADE Dynkin diagrams, of which there are 2 infinite families (A and D) and 3 exceptions (E). The resulting numbering is unique up to scale, and if the smallest value is set at 1, the other numbers are integers, ranging up to 6.\n\nThe ordinary ADE graphs are the only graphs that admit a positive labeling with the following property:\n:Twice any label minus two is the sum of the labels on adjacent vertices.\nIn terms of the Laplacian, the positive solutions to the inhomogeneous equation:\n:<math>\\Delta \\phi = \\phi - 2.</math>\nThe resulting numbering is unique (scale is specified by the \"2\"), and consists of integers; for E<sub>8</sub> they range from 58 to 270, and have been observed as early as 1968.<ref name=\"Bourbaki\">\n{{citation\n| authorlink = Nicolas Bourbaki\n| first = Nicolas | last = Bourbaki | year = 1968 | title = Groupes et algebres de Lie | chapter = Chapters 4–6 | publisher = Hermann | location = Paris }}\n</ref>\n\n== See also ==\n* [[Spectral shape analysis]]\n* [[Electrical network]]\n* [[Kronecker sum of discrete Laplacians]]\n\n==References==\n{{reflist}}\n* [[Toshikazu Sunada|T.Sunada]], Discrete geometric analysis, ''Proceedings of Symposia in Pure Mathematics'' (ed. by P. Exner, J. P. Keating, P. Kuchment, T. Sunada, A. Teplyaev), '''77'''(2008), 51-86\n\n==External links==\n*[http://www.yann-ollivier.org/specgraph/specgraph.html Definition and application to spectral gap]\n*<!--[https://docs.google.com/viewer?a=v&pid=sites&srcid=ZGVmYXVsdGRvbWFpbnxkYXZlZGNsdWJ8Z3g6Mzk2ZTUxYzA4MzI5MzBiNw Layered networks, the discrete Laplacian, and a continued fraction identity], Owen D. Biesel, David V. Ingerman, James A. Morrow, and William T. Shore-->\n\n[[Category:Operator theory]]\n[[Category:Graph theory]]\n[[Category:Numerical differential equations]]\n[[Category:Finite differences]]\n[[Category:Feature detection (computer vision)]]\n[[Category:Geometry processing]]"
    },
    {
      "title": "Discrete Poisson equation",
      "url": "https://en.wikipedia.org/wiki/Discrete_Poisson_equation",
      "text": "{{Use American English|date = March 2019}}\n{{Short description|Finite difference equation}}\n{{Use mdy dates|date = March 2019}}\n{{More footnotes|date=April 2009}}\nIn [[mathematics]], the '''discrete Poisson equation''' is the [[finite difference]] analog of the [[Poisson equation]]. In it, the [[discrete Laplace operator]] takes the place of the [[Laplace operator]]. The discrete Poisson equation is frequently used in [[numerical analysis]] as a stand-in for the continuous Poisson equation, although it is also studied in its own right as a topic in [[discrete mathematics]].\n\n==On a two-dimensional rectangular grid==\nUsing the [[finite difference]] numerical method to discretize\nthe 2-dimensional Poisson equation (assuming a uniform spatial discretization, <math>\\Delta x=\\Delta y</math>) on an ''m''&nbsp;&times;&nbsp;''n'' grid gives the following formula:<ref>{{citation|title=Numerical Methods for Engineers and Scientists|edition=2nd|first=Joe|last=Hoffman|year=2001|chapter=Chapter 9.  Elliptic partial differential equations|publisher=McGraw&ndash;Hill|isbn=0-8247-0443-6}}.</ref>\n\n:<math>\n( {\\nabla}^2 u )_{ij} = \\frac{1}{\\Delta x^2} (u_{i+1,j} + u_{i-1,j} + u_{i,j+1} + u_{i,j-1} - 4 u_{ij}) = g_{ij}\n</math>\n\nwhere <math> 2 \\le i \\le m-1 </math> and <math> 2 \\le j \\le n-1 </math>.  The preferred arrangement of the solution vector is to use [[Enumeration|natural ordering]] which, prior to removing boundary elements, would look like:\n\n:<math>\n\\vec{u} =\n\\begin{bmatrix} u_{11} , u_{21} , \\ldots , u_{m1} , u_{12} , u_{22} , \\ldots , u_{m2} , \\ldots , u_{mn}\n\\end{bmatrix}^T\n</math>\n\nThis will result in an ''mn''&nbsp;&times;&nbsp;''mn'' linear system:\n\n:<math> A\\vec{u} = \\vec{b} </math>\n\nwhere\n\n:<math>\nA =\n\\begin{bmatrix}\n        ~D & -I & ~0 & ~0 & ~0 & \\ldots & ~0 \\\\\n        -I & ~D & -I & ~0 & ~0 & \\ldots & ~0 \\\\\n        ~0 & -I & ~D & -I & ~0 & \\ldots & ~0 \\\\\n        \\vdots & \\ddots & \\ddots & \\ddots & \\ddots & \\ddots & \\vdots \\\\\n        ~0 & \\ldots & ~0 & -I & ~D & -I & ~0 \\\\\n        ~0 & \\ldots & \\ldots & ~0 & -I & ~D & -I \\\\\n        ~0 & \\ldots & \\ldots & \\ldots & ~0 & -I & ~D\n\\end{bmatrix},\n</math>\n\n<math> I </math> is the ''m''&nbsp;&times;&nbsp;''m'' [[identity matrix]], and <math> D </math>, also ''m''&nbsp;&times;&nbsp;''m'', is given by:\n\n:<math>\nD =\n\\begin{bmatrix}\n        ~4 & -1 & ~0 & ~0 & ~0 & \\ldots & ~0 \\\\\n        -1 & ~4 & -1 & ~0 & ~0 & \\ldots & ~0 \\\\\n        ~0 & -1 & ~4 & -1 & ~0 & \\ldots & ~0 \\\\\n        \\vdots & \\ddots & \\ddots & \\ddots & \\ddots & \\ddots & \\vdots \\\\\n        ~0 & \\ldots & ~0 & -1 & ~4 & -1 & ~0 \\\\\n        ~0 & \\ldots & \\ldots & ~0 & -1 & ~4 & -1 \\\\\n        ~0 & \\ldots & \\ldots & \\ldots & ~0 & -1 & ~4\n\\end{bmatrix},\n</math>\n\n<ref>Golub, Gene H. and C. F. Van Loan, ''Matrix Computations, 3rd Ed.'',\nThe Johns Hopkins University Press, Baltimore, 1996, pages 177–180.</ref>\nand <math>\\vec{b}</math> is defined by\n\n:<math>\n\\vec{b} =\n-\\Delta x^2\\begin{bmatrix} g_{11} , g_{21} , \\ldots , g_{m1} , g_{12} , g_{22} , \\ldots , g_{m2} , \\ldots , g_{mn}\n\\end{bmatrix}^T.\n</math>\n\nFor each <math> u_{ij} </math> equation, the columns of <math> D </math> correspond to a block of <math> m </math> components in  <math> u </math>:\n\n:<math>\n\\begin{bmatrix}\n        u_{1j} , & u_{2j} , & \\ldots, & u_{i-1,j}  , & u_{ij} , & u_{i+1,j} , & \\ldots , & u_{mj}\n\\end{bmatrix}^{T}\n\n</math>\n\nwhile the columns of <math> I </math> to the left and right of <math> D </math> each correspond to other blocks of <math> m </math> components within <math> u </math>:\n\n:<math>\n\\begin{bmatrix}\n        u_{1,j-1} , & u_{2,j-1} , & \\ldots, & u_{i-1,j-1}  , & u_{i,j-1} , & u_{i+1,j-1} , & \\ldots , & u_{m,j-1}\n\\end{bmatrix}^{T}\n</math>\n\nand\n\n:<math>\n\\begin{bmatrix}\n        u_{1,j+1} , & u_{2,j+1} , & \\ldots, & u_{i-1,j+1}  , & u_{i,j+1} , & u_{i+1,j+1} , & \\ldots , & u_{m,j+1}\n\\end{bmatrix}^{T}\n</math>\n\nrespectively.\n\nFrom the above, it can be inferred that there are <math>n</math> block columns of <math> m </math> in <math> A </math>.  It is important to note that prescribed values of <math> u </math> (usually lying on the boundary) would have their corresponding elements removed from <math> I </math> and <math> D </math>.  For the common case that all the nodes on the boundary are set, we have <math> 2 \\le i \\le m - 1 </math> and <math> 2 \\le j \\le n - 1 </math>, and the system would have the dimensions (''m''&nbsp;&minus;&nbsp;2)(''n''&nbsp;&minus;&nbsp;2)&nbsp;&times;&nbsp;(''m''&nbsp;&minus;&nbsp;2)(''n''&nbsp;&minus;&nbsp;2), where <math> D </math> and <math> I </math> would have dimensions&nbsp;(''m''&nbsp;&minus;&nbsp;2)&nbsp;&times;&nbsp;(''m''&nbsp;&minus;&nbsp;2).\n\n== Example ==\n\nFor a 5×5 ( <math> m = 5  </math> and <math> n = 5 </math> ) grid with all the boundary nodes prescribed,\nthe system would look like:\n\n:<math>\n\\begin{bmatrix} U \\end{bmatrix} =\n\\begin{bmatrix} u_{22}, u_{32}, u_{42}, u_{23}, u_{33}, u_{43}, u_{24}, u_{34}, u_{44}\n\\end{bmatrix}^{T}\n</math>\n\nwith\n\n:<math>\nA =\n\\left[\\begin{array}{ccc|ccc|ccc}\n       ~4 & -1 & ~0 & -1 & ~0 & ~0 & ~0 & ~0 & ~0 \\\\\n       -1 & ~4 & -1 & ~0 & -1 & ~0 & ~0 & ~0 & ~0 \\\\\n       ~0 & -1 & ~4 & ~0 & ~0 & -1 & ~0 & ~0 & ~0 \\\\\n       \\hline\n       -1 & ~0 & ~0 & ~4 & -1 & ~0 & -1 & ~0 & ~0 \\\\\n       ~0 & -1 & ~0 & -1 & ~4 & -1 & ~0 & -1 & ~0 \\\\\n       ~0 & ~0 & -1 & ~0 & -1 & ~4 & ~0 & ~0 & -1 \\\\\n       \\hline\n       ~0 & ~0 & ~0 & -1 & ~0 & ~0 & ~4 & -1 & ~0 \\\\\n       ~0 & ~0 & ~0 & ~0 & -1 & ~0 & -1 & ~4 & -1 \\\\\n       ~0 & ~0 & ~0 & ~0 & ~0 & -1 & ~0 & -1 & ~4\n\\end{array}\\right]\n</math>\n\nand\n\n:<math>\n\\vec{b} =\n\\left[\\begin{array}{l}\n        -\\Delta x^2 g_{22} + u_{12} + u_{21} \\\\\n        -\\Delta x^2 g_{32} + u_{31} ~~~~~~~~ \\\\\n        -\\Delta x^2 g_{42} + u_{52} + u_{41} \\\\\n        -\\Delta x^2 g_{23} + u_{13} ~~~~~~~~ \\\\\n        -\\Delta x^2 g_{33}  ~~~~~~~~~~~~~~~~ \\\\\n        -\\Delta x^2 g_{43} + u_{53} ~~~~~~~~ \\\\\n        -\\Delta x^2 g_{24} + u_{14} + u_{25} \\\\\n        -\\Delta x^2 g_{34} + u_{35} ~~~~~~~~ \\\\\n        -\\Delta x^2 g_{44} + u_{54} + u_{45}\n\\end{array}\\right].\n</math>\n\nAs can be seen, the boundary <math> u </math>'s are brought to the right-hand-side\nof the equation.<ref>Cheny, Ward and David Kincaid, ''Numerical Mathematics and Computing 2nd Ed.'',\nBrooks/Cole Publishing Company, Pacific Grove, 1985, pages 443–448.</ref>  The entire system is 9&nbsp;&times;&nbsp;9 while <math> D </math> and <math> I </math> are 3&nbsp;&times;&nbsp;3 and given by:\n\n:<math>\nD =\n\\begin{bmatrix}\n       ~4 & -1 & ~0 \\\\\n       -1 & ~4 & -1 \\\\\n       ~0 & -1 & ~4 \\\\\n\\end{bmatrix}\n</math>\n\nand\n\n:<math>\n-I =\n\\begin{bmatrix}\n       -1 & ~0 & ~0 \\\\\n       ~0 & -1 & ~0 \\\\\n       ~0 & ~0 & -1\n\\end{bmatrix}.\n</math>\n\n== Methods of solution ==\n\nBecause <math> \\begin{bmatrix} A \\end{bmatrix} </math> is block tridiagonal and sparse, many methods of solution\nhave been developed to optimally solve this linear system for <math> \\begin{bmatrix} U \\end{bmatrix} </math>.\nAmong the methods are a generalized [[Thomas algorithm]] with a resulting computational complexity of <math> O(n^{2.5}) </math>, [[cyclic reduction]], [[successive overrelaxation]] that has a complexity of <math> O(n^{1.5}) </math>, and [[Fast Fourier transform]]s which is <math> O(n \\log(n)) </math>. An optimal <math> O(n) </math> solution can also be computed using [[multigrid methods]]. <ref>CS267: Notes for Lectures 15 and 16, Mar 5 and 7, 1996, https://people.eecs.berkeley.edu/~demmel/cs267/lecture24/lecture24.html</ref>\n[[File:Convergence of Iterative Numerical Methods for Poisson System with 16384 elements.svg|thumb|center|500px|Poisson convergence of various iterative methods with infinity norms of residuals against iteration count and computer time.]]\n\n== Applications ==\n\nIn [[computational fluid dynamics]], for the solution of an incompressible flow problem, the incompressibility condition acts as a constraint for the pressure. There is no explicit form available for pressure in this case due to a strong coupling of the velocity and pressure fields. In this condition, by taking the divergence of all terms in the momentum equation, one obtains the pressure poisson equation.\n\nFor an incompressible flow this constraint is given by:\n:<math>\n\\frac{ \\partial v_x }{ \\partial x} + \\frac{ \\partial v_y }{ \\partial y} + \\frac{\\partial v_z}{\\partial z} = 0\n</math>\n\nwhere <math> v_x </math> is the velocity in the <math> x </math> direction, <math> v_y </math> is\nvelocity in <math> y </math> and <math> v_z </math> is the velocity in the <math> z </math> direction. Taking divergence of the momentum equation and using the incompressibility constraint, the pressure poisson equation is formed given by:\n:<math>\n\\nabla^2 p = f(\\nu,V)\n</math>\n\nwhere <math> \\nu </math> is the kinematic viscosity of the fluid and <math> V </math> is the velocity vector.<ref>\nFletcher, Clive A. J., ''Computational Techniques for Fluid Dynamics: Vol I'', 2nd Ed., Springer-Verlag, Berlin, 1991, page 334–339.\n</ref>\n\nThe discrete Poisson's equation arises in the theory of \n[[Markov chain]]s.    It appears as the relative value function for the dynamic programming equation in a  [[Markov decision process]], and  as the ''[[control variate]]'' for application in simulation variance reduction.<ref name=MCSS> S. P. Meyn and R.L. Tweedie, 2005.  [http://probability.ca/MT/ Markov Chains and Stochastic Stability]. \nSecond edition to appear, Cambridge University Press, 2009.</ref><ref name=CTCN> S. P. Meyn, 2007.  [http://www.meyn.ece.ufl.edu/archive/spm_files/CTCN/CTCN.html Control Techniques for Complex Networks], Cambridge University Press, 2007.  </ref><ref name=AG07> Asmussen, Søren, Glynn, Peter W., 2007. \"Stochastic Simulation: Algorithms and Analysis\".  Springer.  Series: Stochastic Modelling and Applied Probability, Vol. 57,  2007.</ref>\n\n==Footnotes==\n\n{{reflist|30em}}\n\n==References==\n*Hoffman, Joe D., '' Numerical Methods for Engineers and Scientists, 4th Ed.'', McGraw–Hill Inc., New York, 1992.\n*Sweet, Roland A., '' SIAM Journal on Numerical Analysis, Vol. 11, No. 3 '', June 1974, 506–520.\n*{{Cite book | last1=Press | first1=WH | last2=Teukolsky | first2=SA | last3=Vetterling | first3=WT | last4=Flannery | first4=BP | year=2007 | title=Numerical Recipes: The Art of Scientific Computing | edition=3rd | publisher=Cambridge University Press |  publication-place=New York | isbn=978-0-521-88068-8 | chapter=Section 20.4. Fourier and Cyclic Reduction Methods | chapter-url=http://apps.nrbook.com/empanel/index.html#pg=1053}}\n\n\n[[Category:Finite differences]]\n[[Category:Numerical differential equations]]"
    },
    {
      "title": "Divided differences",
      "url": "https://en.wikipedia.org/wiki/Divided_differences",
      "text": "{{multiple issues|\n{{lead rewrite|date=April 2015}}\n{{tone|date=April 2015}}\n{{technical|date=April 2015}}\n}}\nIn [[mathematics]], '''divided differences''' is an [[algorithm]], historically used for computing tables of logarithms and trigonometric functions.{{Citation needed|date=October 2017}}  [[Charles Babbage]]'s [[difference engine]], an early [[mechanical calculator]], was designed to use this algorithm in its operation.<ref name=\"Isaacson2014\">{{cite book|last1=Isaacson|first1=Walter|title=The Innovators|date=2014|publisher=Simon & Schuster|isbn=978-1-4767-0869-0|page=20|ref=Isaacson2014}}</ref>\n\nDivided differences is a [[recursion|recursive]] [[division (mathematics)|division]] process.  The method can be used to calculate the coefficients in the [[polynomial interpolation|interpolation polynomial]] in the [[Newton form]].\n\n==Definition==\nGiven ''k''&nbsp;+&nbsp;1 data points\n\n:<math>(x_0, y_0),\\ldots,(x_{k}, y_{k})</math>\n\nThe '''forward divided differences''' are defined as:\n\n:<math>[y_\\nu] := y_\\nu, \\qquad \\nu \\in \\{ 0,\\ldots,k\\}</math>\n:<math>[y_\\nu,\\ldots,y_{\\nu+j}] := \\frac{[y_{\\nu+1},\\ldots , y_{\\nu+j}] - [y_{\\nu},\\ldots , y_{\\nu+j-1}]}{x_{\\nu+j}-x_\\nu}, \\qquad \\nu\\in\\{0,\\ldots,k-j\\},\\ j\\in\\{1,\\ldots,k\\}.</math>\n\nThe '''backward divided differences''' are defined as:\n\n:<math>[y_\\nu] := y_{\\nu},\\qquad \\nu \\in \\{ 0,\\ldots,k\\}</math>\n:<math>[y_\\nu,\\ldots,y_{\\nu-j}] := \\frac{[y_\\nu,\\ldots , y_{\\nu-j+1}] - [y_{\\nu-1},\\ldots , y_{\\nu-j}]}{x_\\nu - x_{\\nu-j}}, \\qquad \\nu\\in\\{j,\\ldots,k\\},\\ j\\in\\{1,\\ldots,k\\}.</math>\n\n==Notation==\nIf the data points are given as a function ''&fnof;'',\n\n:<math>(x_0, f(x_0)),\\ldots,(x_{k}, f(x_{k}))</math>\n\none sometimes writes\n\n:<math>f[x_\\nu] := f(x_{\\nu}), \\qquad \\nu \\in \\{ 0,\\ldots,k \\}</math>\n:<math>f[x_\\nu,\\ldots,x_{\\nu+j}] := \\frac{f[x_{\\nu+1},\\ldots , x_{\\nu+j}] - f[x_\\nu,\\ldots , x_{\\nu+j-1}]}{x_{\\nu+j}-x_\\nu}, \\qquad \\nu\\in\\{0,\\ldots,k-j\\},\\ j\\in\\{1,\\ldots,k\\}.</math>\n\nSeveral notations for the divided difference of the function ''&fnof;'' on the nodes ''x''<sub>0</sub>,&nbsp;...,&nbsp;''x''<sub>''n''</sub> are used:\n\n: <math>[x_0,\\ldots,x_n]f,</math>\n: <math>[x_0,\\ldots,x_n;f],</math>\n: <math>D[x_0,\\ldots,x_n]f</math>\n\netc.\n\n==Example==\n\nDivided differences for  <math>\\nu=0</math> and the first few values of <math>j</math>:\n\n:<math>\n\\begin{align}\n  \\mathopen[y_0] &= y_0 \\\\\n  \\mathopen[y_0,y_1] &= \\frac{y_1-y_0}{x_1-x_0} \\\\\n  \\mathopen[y_0,y_1,y_2]\n&= \\frac{\\mathopen[y_1,y_2]-\\mathopen[y_0,y_1]}{x_2-x_0}\n =  \\frac{\\frac{y_2-y_1}{x_2-x_1}-\\frac{y_1-y_0}{x_1-x_0}}{x_2-x_0}\n = \\frac{y_2-y_1}{(x_2-x_1)(x_2-x_0)}-\\frac{y_1-y_0}{(x_1-x_0)(x_2-x_0)}\n\\\\\n  \\mathopen[y_0,y_1,y_2,y_3] &= \\frac{\\mathopen[y_1,y_2,y_3]-\\mathopen[y_0,y_1,y_2]}{x_3-x_0}\n\\end{align}\n</math> <!-- the \\mathopen command is there because latex otherwise thinks that [...] denotes an optional argument -->\n\nTo make the recursive process more clear, the divided differences can be put in a tabular form:\n\n:<math>\n\\begin{matrix}\nx_0 & y_0 = [y_0] &           &               & \\\\\n        &       & [y_0,y_1] &               & \\\\\nx_1 & y_1 = [y_1] &           & [y_0,y_1,y_2] & \\\\\n        &       & [y_1,y_2] &               & [y_0,y_1,y_2,y_3]\\\\\nx_2 & y_2 = [y_2] &           & [y_1,y_2,y_3] & \\\\\n        &       & [y_2,y_3] &               & \\\\\nx_3 & y_3 = [y_3] &           &               & \\\\\n\\end{matrix}\n</math>\n\n==Properties==\n* [[Linear functional|Linearity]]\n:: <math>(f+g)[x_0,\\dots,x_n] = f[x_0,\\dots,x_n] + g[x_0,\\dots,x_n]</math>\n:: <math>(\\lambda\\cdot f)[x_0,\\dots,x_n] = \\lambda\\cdot f[x_0,\\dots,x_n]</math>\n\n* [[Leibniz rule (generalized product rule)|Leibniz rule]]\n:: <math>(f\\cdot g)[x_0,\\dots,x_n] = f[x_0]\\cdot g[x_0,\\dots,x_n] + f[x_0,x_1]\\cdot g[x_1,\\dots,x_n] + \\dots + f[x_0,\\dots,x_n]\\cdot g[x_n]</math>\n\n* Divided differences are symmetric: If <math>\\sigma : \\{0, \\dots, n\\} \\to \\{0, \\dots, n\\}</math> is a permutation then \n:: <math>f[x_0, \\dots, x_n] = f[x_{\\sigma(0)}, \\dots, x_{\\sigma(n)}]</math>\n\n*From the [[mean value theorem for divided differences]] it follows that\n::<math>f[x_0,\\dots,x_n] = \\frac{f^{(n)}(\\xi)}{n!}</math> where <math>\\xi</math> is in the open interval determined by the smallest and largest of the <math>x_k</math>'s.\n\n===Matrix form===\nThe divided difference scheme can be put into an upper [[triangular matrix]].\nLet <math>T_f(x_0,\\dots,x_n)=\n\\begin{pmatrix}\nf[x_0] & f[x_0,x_1] & f[x_0,x_1,x_2] & \\ldots & f[x_0,\\dots,x_n] \\\\\n0 & f[x_1] & f[x_1,x_2] & \\ldots & f[x_1,\\dots,x_n] \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\ldots & f[x_n]\n\\end{pmatrix}</math>.\n\nThen it holds\n* <math>T_{f+g} x = T_f x + T_g x</math>\n* <math>T_{f\\cdot g} x = T_f x \\cdot T_g x</math>\n:: This follows from the Leibniz rule. It means that multiplication of such matrices is [[commutativity|commutative]]. Summarised, the matrices of divided difference schemes with respect to the same set of nodes form a [[commutative ring]].\n* Since <math> T_f x </math> is a triangular matrix, its [[eigenvalue]]s are obviously <math> f(x_0), \\dots, f(x_n) </math>.\n* Let <math>\\delta_\\xi</math> be a [[Kronecker delta]]-like function, that is\n:: <math>\\delta_\\xi(t) = \\begin{cases}1 &: t=\\xi , \\\\0 &: \\mbox{else}.\\end{cases}</math>\n: Obviously <math>f\\cdot \\delta_\\xi = f(\\xi)\\cdot \\delta_\\xi</math>, thus <math>\\delta_\\xi</math> is an [[eigenfunction]] of the pointwise function multiplication. That is <math>T_{\\delta_{x_i}} x</math> is somehow an \"[[eigenmatrix]]\" of <math>T_f x</math>: <math> T_f x \\cdot T_{\\delta_{x_i}} x = f(x_i) \\cdot T_{\\delta_{x_i}} x </math>. However, all columns of <math>T_{\\delta_{x_i}} x</math> are multiples of each other, the [[matrix rank]] of <math>T_{\\delta_{x_i}} x</math> is 1. So you can compose the matrix of all eigenvectors from the <math>i</math>-th column of each <math>T_{\\delta_{x_i}} x</math>. Denote the matrix of eigenvectors with <math>U x</math>. Example\n:: <math> U(x_0,x_1,x_2,x_3) = \\begin{pmatrix}\n1 & \\frac{1}{(x_1-x_0)} & \\frac{1}{(x_2-x_0)\\cdot(x_2-x_1)} & \\frac{1}{(x_3-x_0)\\cdot(x_3-x_1)\\cdot(x_3-x_2)} \\\\\n0 & 1 & \\frac{1}{(x_2-x_1)} & \\frac{1}{(x_3-x_1)\\cdot(x_3-x_2)} \\\\\n0 & 0 & 1 & \\frac{1}{(x_3-x_2)} \\\\\n0 & 0 & 0 & 1\n\\end{pmatrix} </math>\n:The [[diagonalizable matrix|diagonalization]] of <math> T_f x </math> can be written as\n:: <math> U x \\cdot \\operatorname{diag}(f(x_0),\\dots,f(x_n)) = T_f x \\cdot U x </math>.\n\n==Alternative definitions==\n\n===Expanded form===\n<math>\n\\begin{align}\nf[x_0] &= f(x_0) \\\\\nf[x_0,x_1] &= \\frac{f(x_0)}{(x_0-x_1)} + \\frac{f(x_1)}{(x_1-x_0)} \\\\\nf[x_0,x_1,x_2] &= \\frac{f(x_0)}{(x_0-x_1)\\cdot(x_0-x_2)} + \\frac{f(x_1)}{(x_1-x_0)\\cdot(x_1-x_2)} + \\frac{f(x_2)}{(x_2-x_0)\\cdot(x_2-x_1)} \\\\\nf[x_0,x_1,x_2,x_3] &= \\frac{f(x_0)}{(x_0-x_1)\\cdot(x_0-x_2)\\cdot(x_0-x_3)} + \\frac{f(x_1)}{(x_1-x_0)\\cdot(x_1-x_2)\\cdot(x_1-x_3)} + \\frac{f(x_2)}{(x_2-x_0)\\cdot(x_2-x_1)\\cdot(x_2-x_3)} +\\\\&\\quad\\quad \\frac{f(x_3)}{(x_3-x_0)\\cdot(x_3-x_1)\\cdot(x_3-x_2)} \\\\\nf[x_0,\\dots,x_n] &=\n\\sum_{j=0}^{n} \\frac{f(x_j)}{\\prod_{k\\in\\{0,\\dots,n\\}\\setminus\\{j\\}} (x_j-x_k)}\n\\end{align}\n</math>\n\nWith the help of a [[polynomial function]] <math>q</math> with <math>q(\\xi) = (\\xi-x_0) \\cdots (\\xi-x_n)</math> this can be written as\n\n:<math>\nf[x_0,\\dots,x_n] = \\sum_{j=0}^{n} \\frac{f(x_j)}{q'(x_j)}.\n</math>\n\nAlternatively, we can allow counting backwards from the start of the sequence by defining <math>x_k = x_{k + n + 1} = x_{k - (n + 1)}</math>  whenever <math>k < 0</math> or <math> n < k </math>. This definition allows <math>x_{-1}</math> to be interpreted as <math>x_{n}</math>, <math>x_{-2}</math> to be interpreted as <math>x_{n-1}</math>, <math>x_{-n}</math> to be interpreted as <math>x_{0}</math>, etc. The expanded form of the divided difference thus becomes\n\n<math>\nf[x_0,\\dots,x_n] = \\sum_{j=0}^{n} \\frac{f(x_j)}{\\prod\\limits_{k=j-n}^{j-1} (x_j - x_k)} + \\sum_{j=0}^{n} \\frac{f(x_j)}{\\prod\\limits_{k=j+1}^{j+n} (x_j - x_k)}\n</math>\n\nYet another characterization utilizes limits:\n\n<math>\nf[x_0,\\dots,x_n] = \\sum_{j=0}^{n} \\lim_{x \\rightarrow x_j} \\left[ \\frac{f(x_j)(x - x_j)}{\\prod\\limits_{k=0}^{n} (x - x_k)} \\right]\n</math>\n\n====Partial fractions====<!-- This section is linked from [[Partial fraction]] -->\nYou can represent [[partial fraction]]s using the expanded form of divided differences. (This does not simplify computation, but is interesting in itself.) If <math>p</math> and <math>q</math> are [[polynomial function]]s, where <math>\\mathrm{deg}\\ p < \\mathrm{deg}\\ q</math> and <math>q</math> is given in terms of [[linear factor]]s by <math>q(\\xi) = (\\xi-x_1)\\cdot \\dots \\cdot(\\xi-x_n)</math>, then it follows from partial fraction decomposition that\n:<math>\\frac{p(\\xi)}{q(\\xi)} = \\left(t\\to\\frac{p(t)}{\\xi-t}\\right)[x_1,\\dots,x_n].</math>\nIf [[limit of a function|limits]] of the divided differences are accepted, then this connection does also hold, if some of the <math>x_j</math> coincide.\n\nIf <math>f</math> is a polynomial function with arbitrary degree\nand it is decomposed by <math>f(x) = p(x) + q(x)\\cdot d(x)</math> using [[polynomial division]] of <math>f</math> by <math>q</math>,\nthen\n\n: <math>\\frac{p(\\xi)}{q(\\xi)} = \\left(t\\to\\frac{f(t)}{\\xi-t}\\right)[x_1,\\dots,x_n].</math>\n\n===Peano form===\nThe divided differences can be expressed as\n\n:<math>f[x_0,\\ldots,x_n] = \\frac{1}{n!} \\int_{x_0}^{x_n} f^{(n)}(t)B_{n-1}(t) \\, dt</math>\n\nwhere <math>B_{n-1}</math> is a [[B-spline]] of degree <math>n-1</math> for the data points <math>x_0,\\dots,x_n</math> and <math>f^{(n)}</math> is the <math>n</math>-th [[derivative]] of the function <math>f</math>.\n\nThis is called the '''Peano form''' of the divided differences and <math>B_{n-1}</math> is called the [[Peano kernel]] for the divided differences, both named after [[Giuseppe Peano]].\n\n===Taylor form===\n\n====First order====\nIf nodes are cumulated, then the numerical computation of the divided differences is inaccurate, because you divide almost two zeros, each of which with a high [[relative error]] due to [[Loss of significance|differences of similar values]]. However we know, that [[difference quotient]]s approximate the [[derivative]] and vice versa:\n:<math>\\frac{f(y)-f(x)}{y-x} \\approx f'(x)</math> for <math>x \\approx y</math>\n\nThis approximation can be turned into an identity whenever [[Taylor's theorem]] applies.\n:<math>f(y) = f(x) + f'(x)\\cdot(y-x) + f''(x)\\cdot\\frac{(y-x)^2}{2!} + f'''(x)\\cdot\\frac{(y-x)^3}{3!} + \\dots </math>\n:<math>\\Rightarrow \\frac{f(y) - f(x)}{y-x} = f'(x) + f''(x)\\cdot\\frac{y-x}{2!} + f'''(x)\\cdot\\frac{(y-x)^2}{3!} + \\dots </math>\n\nYou can eliminate the odd powers of <math>y-x</math> by expanding the [[Taylor series]] at the center between <math>x</math> and <math>y</math>:\n:<math>x = m-h, y=m+h</math>, that is <math>m = \\frac{x+y}{2}, h=\\frac{y-x}{2}</math>\n:<math>f(m+h) = f(m) + f'(m)\\cdot h + f''(m)\\cdot\\frac{h^2}{2!} + f'''(m)\\cdot\\frac{h^3}{3!} + \\dots </math>\n:<math>f(m-h) = f(m) - f'(m)\\cdot h + f''(m)\\cdot\\frac{h^2}{2!} - f'''(m)\\cdot\\frac{h^3}{3!} + \\dots </math>\n:<math>\\frac{f(y) - f(x)}{y-x} = \\frac{f(m+h) - f(m-h)}{2\\cdot h} =\n f'(m) + f'''(m)\\cdot\\frac{h^2}{3!} + \\dots </math>\n\n====Higher order====\nThe Taylor series or any other representation with [[function series]] can in principle be used to approximate divided differences. Taylor series are infinite sums of [[monomial|power function]]s. The mapping from a function <math>f</math> to a divided difference <math>f[x_0,\\dots,x_n]</math> is a [[linear functional]]. We can as well apply this functional to the function summands.\n\nExpress power notation with an ordinary function: <math>p_n(x) = x^n.</math>\n\nRegular Taylor series is a weighted sum of power functions: <math>f = f(0)\\cdot p_0 + f'(0)\\cdot p_1 + \\frac{f''(0)}{2!}\\cdot p_2 + \\frac{f'''(0)}{3!}\\cdot p_3 + \\dots </math>\n\nTaylor series for divided differences: <math>f[x_0,\\dots,x_n] = f(0)\\cdot p_0[x_0,\\dots,x_n] + f'(0)\\cdot p_1[x_0,\\dots,x_n] + \\frac{f''(0)}{2!}\\cdot p_2[x_0,\\dots,x_n] + \\frac{f'''(0)}{3!}\\cdot p_3[x_0,\\dots,x_n] + \\dots </math>\n\nWe know that the first <math>n</math> terms vanish, because we have a higher difference order than polynomial order, and in the following term the divided difference is one:\n:<math>\n\\begin{array}{llcl}\n\\forall j<n & p_j[x_0,\\dots,x_n] &=& 0 \\\\\n & p_n[x_0,\\dots,x_n] &=& 1 \\\\\n & p_{n+1}[x_0,\\dots,x_n] &=& x_0 + \\dots + x_n \\\\\n & p_{n+m}[x_0,\\dots,x_n] &=& \\sum_{a\\in\\{0,\\dots,n\\}^m \\text{ with } a_1 \\le a_2 \\le \\dots \\le a_m} \\prod_{j\\in a} x_j. \\\\\n\\end{array}\n</math>\nIt follows that the Taylor series for the divided difference essentially starts with <math>\\frac{f^{(n)}(0)}{n!}</math> which is also a simple approximation of the divided difference, according to the [[mean value theorem for divided differences]].\n\nIf we would have to compute the divided differences for the power functions in the usual way, we would encounter the same numerical problems that we had when computing the divided difference of <math>f</math>. The nice thing is, that there is a simpler way.\nIt holds\n:<math>\nt^n = (1 - x_0\\cdot t) \\dots \\cdot (1 - x_n\\cdot t) \\cdot\n(p_0[x_0,\\dots,x_n] + p_1[x_0,\\dots,x_n]\\cdot t + p_2[x_0,\\dots,x_n]\\cdot t^2 + \\dots) .\n</math>\nConsequently, we can compute the divided differences of <math>p_n</math> by a [[power series|division]] of [[formal power series]]. See how this reduces to the successive computation of powers when we compute <math>p_n[h]</math> for several <math>n</math>.\n\nIf you need to compute a whole divided difference scheme with respect to a Taylor series, see the section about divided differences of [[#Polynomials and power series|power series]].\n\n==Polynomials and power series==\nDivided differences of polynomials are particularly interesting, because they can benefit from the Leibniz rule.\nThe matrix <math>J</math> with\n:<math>\nJ=\n\\begin{pmatrix}\nx_0 & 1 & 0 & 0 & \\cdots & 0 \\\\\n0 & x_1 & 1 & 0 & \\cdots & 0 \\\\\n0 & 0 & x_2 & 1 &        & 0 \\\\\n\\vdots & \\vdots & & \\ddots & \\ddots & \\\\\n0 & 0 & 0 & 0 &          & x_n\n\\end{pmatrix}\n</math>\n\ncontains the divided difference scheme for the [[identity function]] with respect to the nodes <math>x_0,\\dots,x_n</math>,\nthus <math>J^n</math> contains the divided differences for the [[monomial|power function]] with [[exponent]] <math>n</math>.\nConsequently, you can obtain the divided differences for a [[polynomial function]] <math>\\varphi(p)</math>\nwith respect to the [[polynomial]] <math>p</math>\nby applying <math>p</math> (more precisely: its corresponding matrix polynomial function <math>\\varphi_{\\mathrm{M}}(p)</math>) to the matrix <math>J</math>.\n:<math>\\varphi(p)(\\xi) = a_0 + a_1\\cdot \\xi + \\dots + a_n\\cdot \\xi^n</math>\n:<math>\\varphi_{\\mathrm{M}}(p)(J) = a_0 + a_1\\cdot J + \\dots + a_n\\cdot J^n</math>\n::<math>= \\begin{pmatrix}\n\\varphi(p)[x_0] & \\varphi(p)[x_0,x_1] & \\varphi(p)[x_0,x_1,x_2] & \\ldots & \\varphi(p)[x_0,\\dots,x_n] \\\\\n0 & \\varphi(p)[x_1] & \\varphi(p)[x_1,x_2] & \\ldots & \\varphi(p)[x_1,\\dots,x_n] \\\\\n\\vdots & \\ddots & \\ddots & \\ddots & \\vdots \\\\\n0 & \\ldots & 0 & 0 & \\varphi(p)[x_n]\n\\end{pmatrix}\n</math>\nThis is known as ''Opitz' formula''.<ref>[[Carl de Boor|de Boor, Carl]], ''Divided Differences'', Surv. Approx. Theory  1  (2005), 46–69, [http://www.emis.ams.org/journals/SAT/papers/2/]</ref>\n<ref>Opitz, G. ''Steigungsmatrizen'', Z. Angew. Math. Mech. (1964), 44, T52–T54</ref>\n\nNow consider increasing the degree of <math>p</math> to infinity,\ni.e. turn the Taylor polynomial to a [[Taylor series]].\nLet <math>f</math> be a function which corresponds to a [[power series]].\nYou can compute a divided difference scheme by computing the according matrix series applied to <math>J</math>.\nIf the nodes <math>x_0,\\dots,x_n</math> are all equal,\nthen <math>J</math> is a [[Jordan block]] and\ncomputation boils down to generalizing a scalar function to a [[matrix function]] using [[Jordan normal form|Jordan decomposition]].\n\n==Forward differences==\n{{details|Finite difference}}\n\nWhen the data points are equidistantly distributed we get the special case called '''forward differences'''. They are easier to calculate than the more general divided differences.\n\nNote that the \"divided portion\" from '''forward divided difference''' must still be computed, to recover the '''forward divided difference''' from the '''forward difference'''.\n\n===Definition===\nGiven ''n'' data points\n:<math>(x_0, y_0),\\ldots,(x_{n-1}, y_{n-1})</math>\n\nwith\n\n:<math>x_{\\nu} = x_0 + \\nu h,\\ h > 0,\\ \\nu=0,\\ldots,n-1</math>\n\nthe divided differences can be calculated via '''forward differences''' defined as\n\n:<math>\\Delta^{(0)} y_i := y_i</math>\n:<math>\\Delta^{(k)}y_i := \\Delta^{(k-1)}y_{i+1} - \\Delta^{(k-1)}y_i,\\ k \\ge 1.</math>\n\nThe relationship between divided differences and forward differences is<ref>{{cite book|last1=Burden|first1=Richard L.|last2=Faires|first2=J. Douglas|title=Numerical Analysis|date=2011|page=129|edition=9th}}</ref>\n\n:<math>f[x_0, x_1, \\ldots , x_k] = \\frac{1}{k!h^k}\\Delta^{(k)}f(x_0).</math>\n\n===Example===\n\n:<math>\n\\begin{matrix}\ny_0 &               &                   &                  \\\\\n    & \\Delta y_0 &                   &                  \\\\\ny_1 &               & \\Delta^2 y_0 &                  \\\\\n    & \\Delta y_1 &                   & \\Delta^3 y_0\\\\\ny_2 &               & \\Delta^2 y_1 &                  \\\\\n    & \\Delta y_2 &                   &                  \\\\\ny_3 &               &                   &                  \\\\\n\\end{matrix}\n</math>\n\n== See also ==\n* [[Difference quotient]]\n* [[Neville's algorithm]]\n* [[Polynomial interpolation]]\n* [[Mean value theorem for divided differences]]\n* [[Nörlund–Rice integral]]\n* [[Pascal's triangle]]\n\n==References==\n{{reflist|1}}\n* {{cite book|author=[[L. M. Milne-Thomson|Louis Melville Milne-Thomson]]|title=The Calculus of Finite Differences|year=2000|origyear=1933|publisher=American Mathematical Soc.|isbn=978-0-8218-2107-7|at=Chapter 1:  Divided Differences}}\n* {{cite book|author1=Myron B. Allen|author2=Eli L. Isaacson|title=Numerical Analysis for Applied Science|year=1998|publisher=John Wiley & Sons|isbn=978-1-118-03027-1|at=Appendix A}}\n* {{cite book|author=Ron Goldman|title=Pyramid Algorithms: A Dynamic Programming Approach to Curves and Surfaces for Geometric Modeling|year=2002|publisher=Morgan Kaufmann|isbn=978-0-08-051547-2|at=Chapter 4:Newton Interpolation and Difference Triangles}}\n\n== External links  ==\n* An [http://code.henning-thielemann.de/htam/src/Numerics/Interpolation/DividedDifference.hs implementation] in [[Haskell (programming language)|Haskell]].\n\n[[Category:Finite differences]]\n\n[[de:Polynominterpolation#Bestimmung der Koeffizienten: Schema der dividierten Differenzen]]"
    },
    {
      "title": "Eigenvalues and eigenvectors of the second derivative",
      "url": "https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors_of_the_second_derivative",
      "text": "{{Use American English|date = April 2019}}\n{{Short desc|mathematical functions and constants}}{{Use mdy dates|date = April 2019}}\n\nExplicit formulas for '''eigenvalues and eigenvectors of the [[second derivative]]''' with different boundary conditions are provided both for the continuous and discrete cases. In the discrete case, the standard [[Central difference#Higher-order differences|central difference approximation of the second derivative]] is used on a uniform grid.\n\nThese formulas are used to derive the expressions for [[eigenfunctions]] of [[Laplacian]] in case of [[separation of variables]], as well as to find [[eigenvalue]]s and [[eigenvector]]s of multidimensional [[Discrete Laplace operator|discrete Laplacian]] on a [[regular grid]], which is presented as a [[Kronecker sum of discrete Laplacians]] in one-dimension.\n\n==The continuous case==\nThe index j represents the jth eigenvalue or eigenvector and runs from 1 to <math> \\infty </math>. Assuming the equation is defined on the domain <math>x \\in [0,L]</math>, the following are the eigenvalues and normalized eigenvectors. The eigenvalues are ordered in descending order.\n\n===Pure Dirichlet boundary conditions===\n:<math> \\lambda_j = -\\frac{j^2 \\pi^2}{L^2}</math>\n\n:<math> v_j(x) = \\sqrt{\\frac{2}{L}} \\sin\\left(\\frac{j \\pi x}{L}\\right) </math>\n\n===Pure Neumann boundary conditions===\n:<math> \\lambda_j = -\\frac{(j - 1)^2 \\pi^2}{L^2}</math>\n \n:<math>\nv_j(x) = \n\\left\\{\n\\begin{array}{lr}\nL^{- \\frac{1}{2}} & j = 1\\\\\n\\sqrt{\\frac{2}{L}} \\cos\\left(\\frac{(j - 1) \\pi x}{L} \\right) & otherwise\n\\end{array}\n\\right.\n</math>\n\n===Periodic boundary conditions===\n:<math>\\lambda_j = \n\\left\\{\n\\begin{array}{lr}\n-\\frac{j^2 \\pi^2}{L^2} & \\mbox{j is even.}\\\\\n-\\frac{(j-1)^2 \\pi^2}{L^2} & \\mbox{j is odd.}\n\\end{array}\n\\right.\n</math>\n\n(That is: <math>0</math> is a simple eigenvalue and all further eigenvalues are given by <math>\\frac{j^2 \\pi^2}{L^2}</math>, <math>j=1,2,\\ldots</math>, each with multiplicity 2).\n\n:<math> v_j(x) = \\begin{cases}\nL^{-\\frac{1}{2}} & \\mbox{if } j = 1.\\\\\n\\sqrt{\\frac{2}{L}} \\sin\\left(\\frac{j \\pi x}{L}\\right) & \\mbox{ if j is even.}\\\\\n\\sqrt{\\frac{2}{L}} \\cos\\left(\\frac{(j-1) \\pi x}{L}\\right) & \\mbox{ if j is odd.}\n\\end{cases} </math>\n\n===Mixed Dirichlet-Neumann boundary conditions===\n:<math> \\lambda_j = -\\frac{(2j - 1)^2 \\pi^2}{4 L^2} </math>\n\n:<math> v_j(x) = \\sqrt{\\frac{2}{L}} \\sin\\left(\\frac{(2j - 1) \\pi x}{2 L}\\right) </math>\n\n===Mixed Neumann-Dirichlet boundary conditions===\n:<math> \\lambda_j = -\\frac{(2j - 1)^2 \\pi^2}{4 L^2} </math>\n\n:<math> v_j(x) = \\sqrt{\\frac{2}{L}} \\cos\\left(\\frac{(2j - 1) \\pi x}{2 L}\\right) </math>\n\n==The discrete case==\nNotation: The index j represents the jth eigenvalue or eigenvector. The index i represents the ith component of an eigenvector. Both i and j go from 1 to n, where the matrix is size n x n. Eigenvectors are normalized. The eigenvalues are ordered in descending order.\n\n===Pure Dirichlet boundary conditions===\n:<math> \\lambda_j = -\\frac{4}{h^2} \\sin^2\\left(\\frac{\\pi j}{2(n + 1)}\\right)</math>\n\n:<math>v_{i,j} = \\sqrt{\\frac{2}{n+1}} \\sin\\left(\\frac{i j \\pi}{n+1}\\right)</math> <ref>F. Chung, S.-T. Yau, Discrete Green's Functions, Journal of Combinatorial Theory A 91, 191-214 (2000).</ref>\n\n===Pure Neumann boundary conditions===\n:<math>\\lambda_j = -\\frac{4}{h^2} \\sin^2\\left(\\frac{\\pi (j - 1)}{2n}\\right)</math>\n\n:<math> v_{i,j} = \\begin{cases}\nn^{- \\frac{1}{2}} & \\mbox{j = 1}\\\\\n\\sqrt{\\frac{2}{n}} \\cos\\left(\\frac{\\pi (j - 1)(i - 0.5)}{n}\\right) & \\mbox{otherwise}\n\\end{cases}</math>\n\n===Periodic boundary conditions===\n\n:<math>\n\\lambda_j = \\begin{cases}\n-\\frac{4}{h^2} \\sin^2\\left(\\frac{\\pi (j-1)}{2n}\\right) & \\mbox{ if j is odd.}\\\\\n-\\frac{4}{h^2} \\sin^2\\left(\\frac{\\pi j}{2n}\\right) & \\mbox{ if j is even.}\n\\end{cases}\n</math>\n\n(Note that eigenvalues are repeated except for 0 and the largest one if n is even.)\n:<math> v_{i,j} = \\begin{cases}\nn^{-\\frac{1}{2}} & \\mbox{if } j = 1.\\\\\nn^{-\\frac{1}{2}} (-1)^i & \\mbox{if } j = n \\mbox{ and n is even.}\\\\\n\\sqrt{\\frac{2}{n}} \\sin\\left(\\frac{\\pi (i-0.5) j}{n} \\right) & \\mbox{ otherwise if j is even.}\\\\\n\\sqrt{\\frac{2}{n}} \\cos\\left(\\frac{\\pi (i-0.5) (j - 1)}{n}\\right) & \\mbox{ otherwise if j is odd.}\n\\end{cases} </math>\n\n===Mixed Dirichlet-Neumann boundary conditions===\n:<math>\\lambda_j = -\\frac{4}{h^2} \\sin^2\\left(\\frac{\\pi (j-\\frac{1}{2})}{2n + 1}\\right)</math>\n\n:<math>v_{i,j} = \\sqrt{\\frac{2}{n+0.5}} \\sin\\left(\\frac{\\pi i (2j - 1)}{2n + 1}\\right)</math>\n\n===Mixed Neumann-Dirichlet boundary conditions===\n:<math>\\lambda_j = -\\frac{4}{h^2} \\sin^2\\left(\\frac{\\pi (j-\\frac{1}{2})}{2n + 1}\\right)</math>\n\n:<math>v_{i,j} = \\sqrt{\\frac{2}{n+0.5}} \\cos\\left(\\frac{\\pi (i-0.5) (2j - 1)}{2n + 1}\\right)</math>\n\n== Derivation of Eigenvalues and Eigenvectors in the Discrete Case ==\n===Dirichlet case===\nIn the 1D discrete case with Dirichlet boundary conditions, we are solving\n\n:<math> \\frac{v_{k+1} -2v_k + v_{k-1}}{h^2} = \\lambda v_{k}, \\ k=1,...,n, \\ v_0 = v_{n+1} = 0.</math>\n\nRearranging terms, we get\n\n:<math> v_{k+1} = (2 + h^2 \\lambda)v_k - v_{k-1}. \\!</math>\n\nNow let <math> 2 \\alpha = (2 + h^2 \\lambda) </math>. Also, assuming <math> v_1 \\neq 0 </math>, we can scale eigenvectors by any nonzero scalar, so scale <math>v</math> so that <math>v_1 = 1 </math>.\n\nThen we find the recurrence\n\n:<math>\nv_0 = 0\n\\,\\!</math>\n\n:<math>\nv_1 = 1.\n\\,\\!</math>\n\n:<math> \nv_{k+1} = 2 \\alpha v_{k} - v_{k-1}\n\\,\\!</math>\n\nConsidering <math> \\alpha </math> as an indeterminate,\n\n:<math> v_{k+1} = U_k (\\alpha) \\,\\!</math> \nwhere <math> U_k </math> is the kth [[Chebyshev polynomials|Chebyshev polynomial]] of the 2nd kind.\n\nSince <math> v_{n+1} = 0 </math>, we get that\n\n:<math> U_n (\\alpha) = 0 \\,\\!</math>.\n\nIt is clear that the eigenvalues of our problem will be the zeros of the nth Chebyshev polynomial of the second kind, with the relation <math> 2 \\alpha = (2 + h^2 \\lambda) </math>.\n\nThese zeros are well known and are:\n\n:<math>\n\\alpha_k = \\cos\\left(\\frac{k \\pi}{n+1}\\right).\n\\,\\!</math>\n\nPlugging these into the formula for <math> \\lambda </math>,\n\n:<math>\n2 \\cos\\left(\\frac{k \\pi}{n+1}\\right) = h^2 \\lambda_k + 2\n\\,\\!</math>\n:<math>\n\\lambda_k = -\\frac{2}{h^2}\\left[1 - \\cos\\left(\\frac{k \\pi}{n+1}\\right)\\right].\n\\,\\!</math>\n\nAnd using a trig formula to simplify, we find\n\n:<math>\n\\lambda_k = -\\frac{4}{h^2}\\sin^2\\left(\\frac{k \\pi}{2(n+1)}\\right).\n\\,\\!</math>\n\n===Neumann case===\nIn the Neumann case, we are solving\n\n:<math> \\frac{v_{k+1} -2v_k + v_{k-1}}{h^2} = \\lambda v_{k}, \\ k = 1,...,n, \\ v'_{0.5} = v'_{n+0.5} = 0.\n\\,\\!</math>\n\nIn the standard discretization, we introduce <math>v_{0}\\,\\!</math> and <math>v_{n+1}\\,\\!</math> and define\n\n:<math>\nv'_{0.5} := \\frac{v_1 - v_0}{h}, \\ v'_{n+0.5} := \\frac{v_{n+1} - v_n}{h}\n\\,\\!</math>\n\nThe boundary conditions are then equivalent to\n:<math>\nv_1 - v_0 = 0, \\ v_{n+1} - v_n = 0.\n</math>\n\nIf we make a change of variables,\n:<math>\nw_k = v_{k+1} - v_k, \\ k = 0,...,n\n\\,\\!</math>\n\nwe can derive the following:\n:<math>\n\\begin{alignat}{2}\n\\frac{v_{k+1} -2v_k + v_{k-1}}{h^2} & = \\lambda v_{k} \\\\\nv_{k+1} -2v_k + v_{k-1} & = h^2 \\lambda v_{k} \\\\\n(v_{k+1} - v_k) - (v_k - v_{k-1}) & = h^2 \\lambda v_{k} \\\\\nw_k - w_{k-1} & = h^2 \\lambda v_{k}  \\\\\n& = h^2 \\lambda w_{k-1} + h^2 \\lambda v_{k-1} \\\\\n& =  h^2 \\lambda w_{k-1} + w_{k-1} - w_{k-2} \\\\\nw_{k} & = (2 + h^2 \\lambda) w_{k-1} - w_{k-2} \\\\\nw_{k+1} & = (2 + h^2 \\lambda) w_{k} - w_{k-1} \\\\\n& = 2 \\alpha w_k - w_{k-1}.\n\\end{alignat}\n</math>\n\nwith <math>w_{n} = w_{0} = 0</math> being the boundary conditions.\n\nThis is precisely the Dirichlet formula with <math>n-1</math> interior grid points and grid spacing <math>h</math>. Similar to what we saw in the above, assuming <math> w_{1} \\neq 0 </math>, we get\n\n:<math>\n\\lambda_k = -\\frac{4}{h^2}\\sin^2\\left(\\frac{k \\pi}{2n}\\right), \\ k = 1,...,n-1.\n</math>\n\nThis gives us <math>n-1</math> eigenvalues and there are <math>n</math>. If we drop the assumption that <math> w_{1} \\neq 0 </math>, we find there is also a solution with <math> v_{k} = \\mathrm{constant} \\  \\forall \\ k=0,...,n+1, </math> and this corresponds to eigenvalue <math>0</math>.\n\nRelabeling the indices in the formula above and combining with the zero eigenvalue, we obtain,\n\n:<math>\n\\lambda_k = -\\frac{4}{h^2}\\sin^2\\left(\\frac{(k-1) \\pi}{2n}\\right), \\ k = 1,...,n.\n</math>\n\n===Dirichlet-Neumann Case===\n\nFor the Dirichlet-Neumann case, we are solving\n\n:<math> \\frac{v_{k+1} -2v_k + v_{k-1}}{h^2} = \\lambda v_{k}, \\ k=1,...,n, \\ v_0 = v'_{n+0.5} = 0.</math>,\n\nwhere <math> v'_{n+0.5} := \\frac{v_{n+1} - v_n}{h}. </math>\n\nWe need to introduce auxiliary variables <math> v_{j + 0.5}, \\ j = 0,...,n. </math>\n\nConsider the recurrence\n\n:<math> v_{k+0.5} = 2 \\beta v_{k} - v_{k-0.5}, \\text{ for some }\\beta \\,\\!</math>.\n\nAlso, we know <math>v_0 = 0</math> and assuming <math>v_{0.5} \\neq 0 </math>, we can scale <math>v_{0.5}</math> so that <math>v_{0.5} = 1.</math>\n\nWe can also write\n:<math> \nv_{k} = 2 \\beta v_{k-0.5} - v_{k-1}\n\\,\\!</math>\n:<math>\nv_{k+1} = 2 \\beta v_{k+0.5} - v_{k}. \n\\,\\!</math>\n\nTaking the correct combination of these three equations, we can obtain\n\n:<math> v_{k+1} = (4 \\beta^2 - 2) v_{k} - v_{k-1}. \\,\\!</math>\n\nAnd thus our new recurrence will solve our eigenvalue problem when\n\n:<math> h^2 \\lambda + 2 = (4 \\beta^2 - 2). \\,\\!</math>\n\nSolving for <math> \\lambda </math> we get\n\n:<math> \\lambda = \\frac{4 (\\beta^2 - 1)}{h^2}. </math>\n\nOur new recurrence gives\n\n:<math>v_{n+1} = U_{2n + 1}(\\beta), \\ v_{n} = U_{2n - 1}(\\beta), \\,\\!</math>\n\nwhere <math>U_{k}(\\beta) </math> again is the kth [[Chebyshev polynomials|Chebyshev polynomial]] of the 2nd kind.\n\nAnd combining with our Neumann boundary condition, we have\n\n:<math> U_{2n + 1}(\\beta) - U_{2n - 1}(\\beta) = 0. \\,\\!</math>\n\nA well-known formula relates the [[Chebyshev polynomials]] of the first kind, <math>T_k(\\beta)</math>, to those of the second kind by\n\n:<math>\nU_{k}(\\beta) - U_{k - 2}(\\beta) = T_k (\\beta). \n\\,\\!</math>\n\nThus our eigenvalues solve\n\n:<math> T_{2n + 1} (\\beta) = 0, \\ \\lambda = \\frac{4 (\\beta^2 - 1)}{h^2}. \\,\\!</math>\n\nThe zeros of this polynomial are also known to be\n\n:<math> \\beta_{k} = \\cos\\left(\\frac{\\pi (k - 0.5)}{2n + 1}\\right), \\ k=1,...,2n + 1 \\,\\!</math>\n\nAnd thus\n\n:<math>\n\\begin{alignat}{2}\n\\lambda_{k} & = \\frac{4}{h^2}\\left[\\cos^2\\left(\\frac{\\pi (k - 0.5)}{2n + 1}\\right) - 1\\right] \\\\\n& = -\\frac{4}{h^2}\\sin^2\\left(\\frac{\\pi (k - 0.5)}{2n + 1}\\right).\n\\end{alignat}\n</math>\n\nNote that there are 2n + 1 of these values, but only the first n + 1 are unique. The (n + 1)th value gives us the zero vector as an eigenvector with eigenvalue 0, which is trivial. This can be seen by returning to the original recurrence. So we consider only the first n of these values to be the n eigenvalues of the Dirichlet - Neumann problem.\n\n:<math>\n\\lambda_{k} = -\\frac{4}{h^2}\\sin^2\\left(\\frac{\\pi (k - 0.5)}{2n + 1}\\right), \\ k = 1,...,n.\n</math>\n\n==References==\n{{Reflist}}\n\n[[Category:Operator theory]]\n[[Category:Matrix theory]]\n[[Category:Numerical differential equations]]\n[[Category:Finite differences]]"
    },
    {
      "title": "Faulhaber's formula",
      "url": "https://en.wikipedia.org/wiki/Faulhaber%27s_formula",
      "text": "{{Use American English|date = March 2019}}\n{{Short description|Expression for sums of powers}}\nIn [[mathematics]], '''Faulhaber's formula''', named after [[Johann Faulhaber]], expresses the sum of the ''p''-th powers of the first ''n'' positive integers\n\n:<math>\\sum_{k=1}^n k^p = 1^p + 2^p + 3^p + \\cdots + n^p</math>\n \t\nas a (''p''&nbsp;+&nbsp;1)th-degree [[polynomial]] function of&nbsp;''n'', the coefficients involving [[Bernoulli number]]s ''B<sub>j</sub>'', in the form submitted by [[Jacob Bernoulli]] and published in 1713:\n \t\n: <math> \\sum_{k=1}^n k^{p} = \\frac{n^{p+1}}{p+1}+\\frac{1}{2}n^p+\\sum_{k=2}^p \\frac{B_{k}}{k!}p^\\underline{k-1}n^{p-k+1},</math>\nwhere <math>p^\\underline{k-1}=(p)_{k-1}=\\dfrac{p!}{(p-k+1)!}</math> is a [[falling factorial]].\n\n== History ==\nFaulhaber's formula is also called '''Bernoulli's formula'''. Faulhaber did not know the properties of the coefficients discovered by Bernoulli. Rather, he knew at least the first 17 cases, as well as the existence of the Faulhaber polynomials for odd powers described below.<ref name=\"Knuth1993\">{{cite journal|author=[[Donald E. Knuth]] |title=Johann Faulhaber and sums of powers |journal=Mathematics of Computation |year=1993 |volume=61 |pages=277&ndash;294 |arxiv=math.CA/9207222|doi=10.2307/2152953|issue=203|ref=harv|postscript=.|jstor=2152953}} The arxiv.org paper has a misprint in the formula for the sum of 11th powers, which was corrected in the printed version. [http://www-cs-faculty.stanford.edu/~knuth/papers/jfsp.tex.gz Correct version.]</ref>\n\nA rigorous proof of these formulas and his assertion that such formulas would exist for all odd powers took until {{harvs|authorlink=Carl Gustav Jacob Jacobi|first=Carl|last=Jacobi|year=1834|txt}}.\n\n==Faulhaber polynomials==\nThe term ''Faulhaber polynomials'' is used by some authors to refer to something other than the polynomial sequence given above.  Faulhaber observed that '''if ''p'' is odd''', then\n\n:<math>1^p + 2^p + 3^p + \\cdots + n^p </math>\n\nis a polynomial function of\n\n:<math>a=1+2+3+\\cdots+n= \\frac{n(n+1)}{2}. </math>\n\nIn particular:\n\n:<math>1^3 + 2^3 + 3^3 + \\cdots + n^3 = a^2;</math> {{OEIS2C|A000537}}\n\n<!-- spacing for easier legibility -->\n\n:<math>1^5 + 2^5 + 3^5 + \\cdots + n^5 = {4a^3 - a^2 \\over 3};</math> {{OEIS2C|A000539}}\n\n<!-- spacing for easier legibility -->\n\n:<math>1^7 + 2^7 + 3^7 + \\cdots + n^7 = {6a^4 -4a^3 + a^2 \\over 3};</math> {{OEIS2C|A000541}}\n\n<!-- spacing for easier legibility -->\n\n:<math>1^9 + 2^9 + 3^9 + \\cdots + n^9 = {16a^5 - 20a^4 +12a^3 - 3a^2 \\over 5};</math> {{OEIS2C|A007487}}\n\n<!-- spacing for easier legibility -->\n\n:<math>1^{11} + 2^{11} + 3^{11} + \\cdots + n^{11} = {16a^6 - 32a^5 + 34a^4 - 20a^3 + 5a^2 \\over 3}.</math> {{OEIS2C|A123095}}\n\nThe first of these [[Identity (mathematics)|identities]] (the case ''p'' = 3) is known as [[Squared triangular number|Nicomachus's theorem]].\n\nMore generally, {{citation needed|date=January 2017}}\n\n:<math>\n\\begin{align}\n 1^{2m+1} + 2^{2m+1} &+ 3^{2m+1} + \\cdots + n^{2m+1}\\\\ &= \\frac{1}{2^{2m+2}(2m+2)} \\sum_{q=0}^m \\binom{2m+2}{2q}\n(2-2^{2q})~ B_{2q} ~\\left[(8a+1)^{m+1-q}-1\\right].\n\\end{align}\n</math>\n\nSome authors call the polynomials in ''a'' on the right-hand sides of these identities '''Faulhaber polynomials'''.  These polynomials are divisible by {{math|''a''<sup>2</sup>}} because the [[Bernoulli number]] {{math|''B''<sub>''j''</sub>}} is 0 for {{math|''j'' > 1}} odd.\n\nFaulhaber also knew that if a sum for an odd power is given by\n\n:<math>\\sum_{k=1}^n k^{2m+1} = c_1 a^2 + c_2 a^3 + \\cdots + c_m a^{m+1}</math>\n\nthen the sum for the even power just below is given by\n\n:<math>\\sum_{k=1}^n k^{2m} = \\frac{n+1/2}{2m+1}(2 c_1 a + 3 c_2 a^2+\\cdots + (m+1) c_m a^m).</math>\n\nNote that the polynomial in parentheses is the derivative of the polynomial above with respect to ''a''.\n\nSince ''a''&nbsp;=&nbsp;''n''(''n''&nbsp;+&nbsp;1)/2, these formulae show that for an odd power (greater than&nbsp;1), the sum is a polynomial in ''n'' having factors ''n''<sup>2</sup> and (''n''&nbsp;+&nbsp;1)<sup>2</sup>, while for an even power the polynomial has factors ''n'', ''n''&nbsp;+&nbsp;½ and ''n''&nbsp;+&nbsp;1.\n\n==''Summae Potestatum''==\n[[File:JakobBernoulliSummaePotestatum.png|thumb|right|480px|Jakob Bernoulli's ''Summae Potestatum'', [[Ars Conjectandi]], 1713]]\n\nIn 1713, [[Jacob Bernoulli]] published under the title ''Summae Potestatum'' an expression of the sum of the {{mvar|p}} powers of the {{math|''n''}} first integers as a ({{math|''p'' + 1}})th-degree [[polynomial function]] of&nbsp;{{math|''n''}}, with coefficients involving numbers {{math|''B<sub>j</sub>''}}, now called [[Bernoulli number]]s:{{clarify|reason=Notation p^\\underline{k-1} undefined. Also, confusing change of the summation index between this formula and the next ones.|date=April 2019}}\n: <math> \\sum_{k=1}^n k^{p} = \\frac{n^{p+1}}{p+1}+\\frac{1}{2}n^p+\\sum_{k=2}^p \\frac{B_{k}}{k!}p^{\\underline{k-1}}n^{p-k+1}.</math>\nIntroducing also the first two Bernoulli numbers (which Bernoulli did not), the previous formula becomes\n\n:<math>\\sum_{k=1}^n k^p = {1 \\over p+1} \\sum_{j=0}^p {p+1 \\choose j} B_j n^{p+1-j},</math>\nusing the Bernoulli number of the second kind for which <math>B_1=\\frac{1}{2}</math>, or\n:<math>\\sum_{k=1}^n k^p = {1 \\over p+1} \\sum_{j=0}^p (-1)^j{p+1 \\choose j} B_j n^{p+1-j},</math>\nusing the Bernoulli number of the first kind for which <math>B_1=-\\frac{1}{2}.</math>\n\nFor example, as\n<math>B_0=1,~B_1=1/2,~B_2=1/6,~B_3=0,~B_4=-1/30,</math> one has for {{math|1=''p'' = 4}},\n:<math>\\begin{align}1^4 + 2^4 + 3^4 + \\cdots + n^4 &= {1 \\over 5} \\sum_{j=0}^4 {5 \\choose j} B_j n^{5-j}\\\\\n&= {1 \\over 5} \\left(B_0 n^5+5B_1n^4+10B_2n^3+10B_3n^2+5B_4n\\right)\\\\\n&= \\frac{1}{5}n^5 + \\frac{1}{2}n^4+ \\frac{1}{3}n^3- \\frac{1}{30}n.\\end{align}</math>\n\nFaulhaber himself did not know the formula in this form, but only computed the first seventeen polynomials; the general form was established with the discovery of the [[Bernoulli numbers]] (see [[#History|History section]]).  The derivation of Faulhaber's formula is available in ''The Book of Numbers'' by [[John Horton Conway]] and [[Richard K. Guy]].<ref>{{cite book |author=[[John H. Conway]], [[Richard K. Guy|Richard Guy]] |title=The Book of Numbers |publisher=Springer |year=1996 |isbn=0-387-97993-X |page=107}}</ref>\n\nThere is also a similar (but somehow simpler) expression: using the idea of [[Telescoping series|telescoping]] and the [[binomial theorem]], one gets ''[[Blaise Pascal|Pascal]]'s identity'':<ref>{{cite journal|author=Kieren MacMillan, Jonathan Sondow|title=Proofs of power sum and binomial coefficient congruences via Pascal's identity |journal=[[American Mathematical Monthly]] |year=2011 |volume=118 |pages=549&ndash;551 |doi=10.4169/amer.math.monthly.118.06.549|arxiv=1011.0076}}</ref>\n:<math>\\begin{align}(n+1)^{k+1} - 1 &= \\sum_{m = 1}^n \\left((m+1)^{k+1} - m^{k+1}\\right)\\\\\n&= \\sum_{p = 0}^k \\binom{k+1}{p} (1^p+2^p+ \\dots + n^p).\\end{align}</math> \nThis<!-- due to Pascal--> in particular yields the examples below – e.g., take {{math|1=''k'' = 1}} to get the first example.\n\n==Examples==\n\n:<math>1 + 2 + 3 + \\cdots + n = \\frac{n(n+1)}{2} = \\frac{n^2 + n}{2}</math> (the [[triangular number]]s)\n\n:<math>1^2 + 2^2 + 3^2 + \\cdots + n^2 = \\frac{n(n+1)(2n+1)}{6} = \\frac{2n^3 + 3n^2 + n}{6}</math> (the [[square pyramidal number]]s)\n\n:<math>1^3 + 2^3 + 3^3 + \\cdots + n^3 = \\left[\\frac{n(n+1)}{2}\\right]^2 = \\frac{n^4 + 2n^3 + n^2}{4}</math> (the [[triangular number]]s squared)\n\n:<math>\n\\begin{align}\n1^4 + 2^4 + 3^4 + \\cdots + n^4 & = \\frac{n(n+1)(2n+1)(3n^2+3n-1)}{30} \\\\\n& = \\frac{6n^5 + 15n^4 + 10n^3 - n}{30}\n\\end{align}\n</math>\n\n:<math>\n\\begin{align}\n1^5 + 2^5 + 3^5 + \\cdots + n^5 & = \\frac{[n(n+1)]^2(2n^2+2n-1)}{12} \\\\\n& = \\frac{2n^6 + 6n^5 + 5n^4 - n^2}{12}\n\\end{align}\n</math>\n\n:<math>\n\\begin{align}\n1^6 + 2^6 + 3^6 + \\cdots + n^6 & = \\frac{n(n+1)(2n+1)(3n^4+6n^3-3n+1)}{42} \\\\\n& = \\frac{6n^7 + 21n^6 + 21n^5 -7n^3 + n}{42}\n\\end{align}\n</math>\n\n==From examples to matrix theorem==\nFrom the previous examples we get:\n:<math>\\sum_{i=1}^n i^0 = n  </math>\n:<math>\\sum_{i=1}^n i^1 = {1\\over 2}n+{1\\over 2}n^2 </math>\n:<math>\\sum_{i=1}^n i^2 = {1\\over 6}n+{1\\over 2}n^2+{1\\over 3}n^3 </math>\n:<math>\\sum_{i=1}^n i^3 = {1\\over 4}n^2+{1\\over 2}n^3+{1\\over 4}n^4 </math>\n:<math>\\sum_{i=1}^n i^4 = -{1\\over 30}n+{1\\over 3}n^3+{1\\over 2}n^4+{1\\over 5}n^5 </math>\n:<math>\\sum_{i=1}^n i^5 = -{1\\over 12}n^2+{5\\over 12}n^4+{1\\over 2}n^5+{1\\over 6}n^6 </math>\n:<math>\\sum_{i=1}^n i^6 = {1\\over 42}n-{1\\over 6}n^3+{1\\over 2}n^5+{1\\over 2}n^6+{1\\over 7}n^7 </math>\n:Writing these polynomials as a product between matrices gives\n:<math>\\begin{pmatrix}\n\\sum_{i=1}^{n} i^0 \\\\\n\\sum_{i=1}^{n} i^1 \\\\\n\\sum_{i=1}^{n} i^2 \\\\\n\\sum_{i=1}^{n} i^3 \\\\\n\\sum_{i=1}^{n} i^4 \\\\\n\\sum_{i=1}^{n} i^5 \\\\\n\\sum_{i=1}^{n} i^6 \\\\\n\n\\end{pmatrix}=G_7\\cdot\\begin{pmatrix}\nn \\\\\nn^2 \\\\\nn^3 \\\\\nn^4\\\\\nn^5 \\\\\nn^6 \\\\\nn^7 \\\\\n\n\\end{pmatrix}\n\\qquad \\text{where}\\qquad\n\nG_7=\\begin{pmatrix}\n1& 0& 0& 0& 0&0& 0\\\\\n{1\\over 2}& {1\\over 2}& 0& 0& 0& 0& 0\\\\\n{1\\over 6}& {1\\over 2}&{1\\over 3}& 0& 0& 0& 0\\\\\n0& {1\\over 4}& {1\\over 2}& {1\\over 4}& 0&0& 0\\\\\n-{1\\over 30}& 0& {1\\over 3}& {1\\over 2}& {1\\over 5}&0& 0\\\\\n0& -{1\\over 12}& 0& {5\\over 12}& {1\\over 2}& {1\\over 6}& 0\\\\\n{1\\over 42}& 0& -{1\\over 6}& 0& {1\\over 2}&{1\\over 2}& {1\\over 7}\\\\\n\n\\end{pmatrix}</math>\n\n:\nSurprisingly, [[invertible matrix|inverting the matrix]] of polynomial coefficients yields something more familiar:\n:<math>\nG_7^{-1}=\\begin{pmatrix}\n 1&  0&   0&   0&   0&   0& 0\\\\\n-1&  2&   0&   0&   0&   0& 0\\\\\n 1& -3&   3&   0&   0&   0& 0\\\\\n-1&  4&  -6&   4&   0&   0& 0\\\\\n 1& -5&  10& -10&   5&   0& 0\\\\\n-1&  6& -15&  20& -15&   6& 0\\\\\n 1& -7&  21& -35&  35& -21& 7\\\\\n\n\\end{pmatrix}\n</math>\n\nIn the inverted matrix, [[Pascal's triangle]] can be recognized, without the last element of each line, and with alternate signs. More precisely, let <math>A_7</math>  be the matrix obtained from Pascal's triangle by removing the last element of each row, and filling the rows by zeros on the right: \n:<math>\nA_7=\\begin{pmatrix}\n1& 0&  0&  0&  0&  0& 0\\\\\n1& 2&  0&  0&  0&  0& 0\\\\\n1& 3&  3&  0&  0&  0& 0\\\\\n1& 4&  6&  4&  0&  0& 0\\\\\n1& 5& 10& 10&  5&  0& 0\\\\\n1& 6& 15& 20& 15&  6& 0\\\\\n1& 7& 21& 35& 35& 21& 7\\\\\n\\end{pmatrix}\n</math>\nLet <math>\\overline{A}_7</math>  be the matrix obtained from <math>A_7</math>  by changing the signs of the entries in odd diagonals, that is by replacing <math>a_{i,j}</math> by <math>(-1)^{i+j}a_{i,j}</math>. Then\n\n:<math>G_7^{-1}=\\overline{A}_7.</math>\n\nThis is true for every order,<ref>{{citation |first=Giorgio|last=Pietrocola|title=On polynomials for the calculation of sums of powers of successive integers and Bernoulli numbers deduced from the Pascal’s triangle|year=2017 |url=http://www.pietrocola.eu/EN/Theoremsonthesumofpowersofsuccessiveintegersbygiorgiopietrocola%20.pdf}}.\n</ref>  that is, for each positive integer {{mvar|m}}, one has <math>G_m^{-1}=\\overline{A}_m.</math>\nThus, it is possible to obtain the coefficients of the polynomials of the sums of powers of successive integers without resorting to the numbers of Bernoulli but by inverting the matrix easily obtained from the triangle of Pascal.\n\nOne has also<ref>{{citation |first=Nigel|last=Derby|title=A search for sums of powers |journal=The Mathematical Gazette| year=2015 |url=https://search.proquest.com/openview/f8786728002514b2de4eaa379d175640/1?pq-origsite=gscholar&cbl=2035960}}.\n</ref> \n:<math>A_7^{-1}=\\overline G_7=\\begin{pmatrix}\n1& 0& 0& 0& 0&0& 0\\\\\n{1\\over 2}& {1\\over 2}& 0& 0& 0& 0& 0\\\\\n{1\\over 6}& {1\\over 2}&{1\\over 3}& 0& 0& 0& 0\\\\\n0& {1\\over 4}& {1\\over 2}& {1\\over 4}& 0&0& 0\\\\\n{1\\over 30}& 0& {1\\over 3}& {1\\over 2}& {1\\over 5}&0& 0\\\\\n0& {1\\over 12}& 0& {5\\over 12}& {1\\over 2}& {1\\over 6}& 0\\\\\n{1\\over 42}& 0& {1\\over 6}& 0& {1\\over 2}&{1\\over 2}& {1\\over 7}\n\\end{pmatrix},</math>\nwhere <math>\\overline G_7</math> is obtained  from <math>G_7</math> by removing the minus signs.\n\n==Proof with complex numbers==\nLet \n:<math>\nS_{p}(n)=\\sum_{k=1}^{n} k^p,\n</math>\ndenote the sum under consideration for integer <math>p\\ge 0.</math>\n\nDefine the following exponential [[generating function]] with (initially) indeterminate <math>z</math>\n:<math>\nG(z,n)=\\sum_{p=0}^{\\infty} S_{p}(n) \\frac{1}{p!}z^p.\n</math>\nWe find \n:<math>\n\\begin{align}\nG(z,n) =& \\sum_{p=0}^{\\infty} \\sum_{k=1}^{n}  \\frac{1}{p!}(kz)^p\n=\\sum_{k=1}^{n}e^{kz}=e^{z}.\\frac{1-e^{nz}}{1-e^{z}},\\\\\n=& \\frac{1-e^{nz}}{e^{-z}-1}.\n\\end{align}\n</math>\nThis is an entire function in <math>z</math> so that <math>z</math> can be taken to be any complex number.\n\nWe next recall the exponential generating function for the [[Bernoulli polynomials]] <math>B_j(x)</math> \n:<math>\n\\frac{ze^{zx}}{e^{z}-1}=\\sum_{j=0}^{\\infty} B_j(x) \\frac{z^j}{j!},\n</math>\nwhere <math>B_j=B_j(0)</math> denotes the Bernoulli number (with the convention <math>B_{1}=-\\frac{1}{2}</math>).\nWe obtain the Faulhaber formula by expanding the generating function as follows:\n:<math>\n\\begin{align}\nG(z,n) =& \\sum_{j=0}^{\\infty} B_j \\frac{(-z)^{j-1}}{j!}\n\\left(-\\sum_{l=1}^{\\infty} \\frac{(nz)^{l}}{l!}\\right)\\\\\n=& \\sum_{p=0}^{\\infty}z^p \n\\sum_{j=0}^p (-1)^j \\frac{1}{j!(p+1-j)!}B_j n^{p+1-j}\\\\\n=& \\sum_{p=0}^{\\infty}\\frac{z^p}{p!}  {1 \\over p+1} \\sum_{j=0}^p (-1)^j{p+1 \\choose j} B_j n^{p+1-j},\\\\\n\\mbox{i.e.}\\quad \\sum_{k=1}^nk^p=&{1 \\over p+1} \\sum_{j=0}^p (-1)^j{p+1 \\choose j} B_j n^{p+1-j}.\n\\end{align}\n</math>\nNote that <math>B_j =0</math> for all odd <math>j>1</math>. Hence some authors define <math>B_{1}=\\frac{1}{2}</math> so that the alternating factor <math>(-1)^j</math> is absent.\n\n==Alternate expressions==\nBy relabelling we find the alternative expression\n\n: <math>\n\\sum_{k=1}^nk^p= \\sum_{k=0}^p {(-1)^{p-k} \\over k+1}{p \\choose k} B_{p-k} n^{k+1}.\n</math>\n\nWe may also expand  <math>G(z,n)</math> in terms of the Bernoulli polynomials to find\n:<math>\n\\begin{align}\nG(z,n) =& \\frac{e^{(n+1)z}}{e^{z}-1}-\\frac{e^z}{e^{z}-1}\\\\\n=& \\sum_{j=0}^{\\infty} \\left(B_j(n+1)-(-1)^jB_j\\right) \\frac{z^{j-1}}{j!},\n\\end{align}\n</math>\nwhich implies\n:<math>\n\\sum_{k=1}^nk^p=\\frac{1}{p+1}\\left(B_{p+1}(n+1)-(-1)^{p+1}B_{p+1}\\right)=\\frac{1}{p+1}\\left(B_{p+1}(n+1)-B_{p+1}(1)\\right).\n</math>\nSince <math>B_n = 0</math> whenever <math> n > 1</math> is odd, the factor <math> (-1)^{p+1} </math> may be removed when <math>p > 0</math>.\n\n==Relationship to Riemann zeta function==\n\nUsing <math>B_k=-k\\zeta(1-k)</math>, one can write\n:<math>\n\\sum\\limits_{k=1}^n k^p = \\frac{n^{p+1}}{p+1} - \\sum\\limits_{j=0}^{p-1}{p \\choose j}\\zeta(-j)n^{p-j}.\n</math>\n\nIf we consider the generating function <math>G(z,n)</math> in the large <math>n</math>  limit for <math>\\Re (z)<0</math>, then we find\n:<math>\n\\lim_{n\\rightarrow \\infty}G(z,n) = \\frac{1}{e^{-z}-1}=\\sum_{j=0}^{\\infty} (-1)^{j-1}B_j \\frac{z^{j-1}}{j!}\n</math>\nHeuristically, this suggests that \n:<math>\n\\sum_{k=1}^{\\infty} k^p=\\frac{(-1)^{p} B_{p+1}}{p+1}.\n</math>\nThis result agrees with the value of the [[Riemann zeta function]] <math>\\zeta(s)=\\sum_{n=1}^{\\infty}\\frac{1}{n^s}</math> for negative integers <math>s=-p<0</math> on appropriately analytically continuing <math>\\zeta(s)</math>.\n\n==Umbral form==\n\nIn the classical [[umbral calculus]] one formally treats the indices ''j'' in a sequence ''B''<sub>''j''</sub> as if they were exponents, so that, in this case we can apply the [[binomial theorem]] and say\n\n:<math>\\sum_{k=1}^n k^p = {1 \\over p+1} \\sum_{j=0}^p {p+1 \\choose j} B_j n^{p+1-j}\n= {1 \\over p+1} \\sum_{j=0}^p {p+1 \\choose j} B^j n^{p+1-j} </math>\n\n<!-- extra blank line for easier legibility -->\n\n:::<math>= {(B+n)^{p+1} - B^{p+1} \\over p+1}. </math>\n\nIn the ''modern'' umbral calculus, one considers the [[linear functional]] ''T'' on the [[vector space]] of polynomials in a variable ''b'' given by\n\n:<math>T(b^j) = B_j.\\,</math>\n\nThen one can say\n\n:<math>\\sum_{k=1}^n k^p = {1 \\over p+1} \\sum_{j=0}^p {p+1 \\choose j} B_j n^{p+1-j}\n= {1 \\over p+1} \\sum_{j=0}^p {p+1 \\choose j} T(b^j) n^{p+1-j} </math>\n\n<!-- extra blank line for easier legibility -->\n\n:::<math> = {1 \\over p+1} T\\left(\\sum_{j=0}^p {p+1 \\choose j} b^j n^{p+1-j} \\right)\n= T\\left({(b+n)^{p+1} - b^{p+1} \\over p+1}\\right). </math>\n\n==Notes==\n\n{{reflist}}\n\n==External links==\n*{{Cite news\n | last=Jacobi\n | first=Carl\n | year=1834\n | title=De usu legitimo formulae summatoriae Maclaurinianae\n | periodical =[[Journal für die reine und angewandte Mathematik]]\n | volume =12\n | pages =263–72\n | ref=harv\n | postscript=.\n}}\n* {{MathWorld|urlname=FaulhabersFormula|title=Faulhaber's formula}}\n* {{cite book |author=Johann Faulhaber|title=Academia Algebrae - Darinnen die miraculosische Inventiones zu den höchsten Cossen weiters ''continuirt'' und ''profitiert'' werden |year=1631}} A very rare book, but Knuth has placed a photocopy in the Stanford library, call number QA154.8 F3 1631a f MATH. ({{Google books|0pw_AAAAcAAJ|online copy|Page=}})\n* {{cite web|last=Beardon|first=A. F.|title=Sums of Powers of Integers|url=http://www.maa.org/sites/default/files/pdf/upload_library/22/Ford/Beardon201-213.pdf|journal=American Mathematical Monthly|volume=103|year=1996|pages=201–213|accessdate=2011-10-23}} (Winner of a [http://www.maa.org/programs/maa-awards/writing-awards/sums-of-powers-of-integers Lester R. Ford Award])\n*{{Cite news\n | last=Schumacher\n | first=Raphael\n | year=2016\n | title=An Extended Version of Faulhaber’s Formula\n | url=https://cs.uwaterloo.ca/journals/JIS/VOL19/Schumacher/schu3.pdf\n | periodical = Journal of Integer Sequences\n | volume =19\n}}\n\n*{{Cite news\n | last=Orosi\n | first=Greg\n | year=2018\n | title=A Simple Derivation Of Faulhaber's Formula\n | url=http://www.math.nthu.edu.tw/~amen/2018/AMEN-170803.pdf\n | periodical = Applied Mathematics E-Notes\n | volume =18\n | pages =124–126\n}}\n\n[[Category:Finite differences]]"
    },
    {
      "title": "Finite difference coefficient",
      "url": "https://en.wikipedia.org/wiki/Finite_difference_coefficient",
      "text": "In mathematics, to approximate a derivative to an arbitrary order of accuracy, it is possible to use the [[finite difference]].  A finite difference can be '''central''',  '''forward''' or '''backward'''.\n\n==Central finite difference==\n\nThis table contains the coefficients of the central differences, for several orders of accuracy and with uniform grid spacing:<ref name=fornberg>{{Citation | last1=Fornberg | first1=Bengt | title=Generation of Finite Difference Formulas on Arbitrarily Spaced Grids | doi=10.1090/S0025-5718-1988-0935077-0  | year=1988 | journal=[[Mathematics of Computation]] | issn=0025-5718 | volume=51 | issue=184 | pages=699–706}}.</ref>\n <!-- replaces Image:Coeff_der_cent_eng.jpg -->\n\n{| class=\"wikitable\" style=\"text-align:center\"\n|-\n! Derivative\n! Accuracy\n! −5 || −4 || −3 || −2 || −1 || 0 || 1 || 2 || 3 || 4 || 5\n|-\n| rowspan=\"4\" | 1\n|| 2 \n| || || || || −1/2 || 0|| 1/2 || || ||\n|\n|-\n| 4 \n| || || || 1/12 || −2/3 || 0|| 2/3|| −1/12 || ||\n|\n|-\n| 6 \n| || || −1/60 || 3/20   || −3/4 || 0 || 3/4 || −3/20 || 1/60 ||\n|\n|- style=\"border-bottom: 2px solid #aaa;\"\n|| 8 \n|||1/280 || −4/105 || 1/5 || −4/5 || 0 || 4/5 || −1/5 || 4/105 || −1/280\n|\n|- \n| rowspan=\"4\" | 2\n| 2 \n| || || || || 1 || −2|| 1 || || ||\n|\n|-\n| 4 \n| || || || −1/12 || 4/3 || −5/2|| 4/3|| −1/12 || ||\n|\n|-\n| 6 \n| || || 1/90 || −3/20   || 3/2 || −49/18 || 3/2 || −3/20 || 1/90 ||\n|\n|- style=\"border-bottom: 2px solid #aaa;\"\n| 8 \n|||−1/560 || 8/315 || −1/5 || 8/5 || −205/72 || 8/5 || −1/5 || 8/315 || −1/560\n|\n|- \n| rowspan=\"3\" | 3\n| 2 \n| || || || −1/2 || 1 || 0|| −1|| 1/2 || ||\n|\n|-\n| 4 \n| || || 1/8 || −1 || 13/8 || 0|| −13/8 || 1 || −1/8 ||\n|\n|- style=\"border-bottom: 2px solid #aaa;\"\n|| 6 \n| || −7/240 || 3/10 || −169/120  || 61/30 ||0 || −61/30|| 169/120 || −3/10 || 7/240\n|\n|- \n| rowspan=\"3\" | 4\n| 2 \n| || || || 1 || −4 || 6|| −4|| 1 || ||\n|\n|-\n| 4 \n| || || −1/6 || 2 || −13/2 || 28/3|| −13/2 || 2 || −1/6 ||\n|\n|- style=\"border-bottom: 2px solid #aaa;\"\n|| 6 \n| || 7/240 || −2/5 || 169/60  || −122/15 || 91/8 || −122/15 || 169/60 || −2/5 || 7/240\n|\n|-\n| rowspan=\"3\" |5\n| 2\n| || || −1/2 || 2 || −5/2 || 0 || 5/2 || −2 || 1/2 ||\n|\n|-\n| 4\n| || 1/6 || −3/2 || 13/3 || −29/6 || 0 || 29/6 || −13/3 || 3/2 || −1/6\n|\n|- style=\"border-bottom: 2px solid #aaa;\"\n| 6\n| −13/288 || 19/36 || −87/32 || 13/2 || −323/48 || 0 || 323/48 || −13/2 || 87/32 || −19/36 || 13/288\n|- \n| rowspan=\"3\" | 6\n|| 2 \n| || || 1 || −6 || 15 || −20 || 15 || −6 || 1 ||\n|\n|-\n| 4\n| || −1/4 || 3 || −13 || 29 || −75/2 || 29 || −13 || 3 || −1/4\n|\n|-\n| 6\n| 13/240 || −19/24 || 87/16 || −39/2 || 323/8 || −1023/20 || 323/8 || −39/2 || 87/16 || −19/24 || 13/240\n|}\n\nFor example, the third derivative with a second-order accuracy is\n\n: <math>f'''(x_{0}) \\approx \\frac{-\\frac{1}{2}f(x_{-2}) + f(x_{-1}) -f(x_{+1}) + \\frac{1}{2}f(x_{+2})}{h^3_x} + O\\left(h_x^2  \\right),</math>\n\nwhere <math> h_x </math> represents a uniform grid spacing between each finite difference interval, and <math>x_n = x_0 + n h_x</math>.\n\nFor the <math>m</math>-th derivative with accuracy <math>n</math>, there are <math>2p + 1 = 2 \\left\\lfloor \\frac{m+1}{2} \\right\\rfloor - 1 + n</math> central coefficients <math>a_{-p}, a_{-p+1}, ..., a_{p-1}, a_p</math>. These are given by the solution of the linear equation system\n\n: <math>\n\\begin{pmatrix} \n 1 & 1 & ... & 1 & 1 \\\\\n -p & -p+1 & ... & p-1 & p \\\\\n (-p)^2 & (-p+1)^2 &... & (p-1)^2 & p^2 \\\\\n ... & ... &...&...&... \\\\\n ... & ... &...&...&... \\\\\n ... & ... &...&...&... \\\\\n (-p)^{2p} & (-p+1)^{2p} & ... & (p-1)^{2p} & p^{2p}\n\\end{pmatrix}\n\\begin{pmatrix}\n a_{-p} \\\\\n a_{-p+1} \\\\\n a_{-p+2} \\\\\n ... \\\\\n ... \\\\\n ... \\\\\n a_p\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n 0 \\\\\n 0 \\\\\n 0 \\\\\n ... \\\\\n m! \\\\\n ...\\\\\n 0\n\\end{pmatrix},\n</math>\n\nwhere the only non-zero value on the right hand side is in the <math>(m+1)</math>-th row.\n\nAn open source implementation for calculating finite difference coefficients of arbitrary derivate and accuracy order is available<ref>{{Cite web|url=https://github.com/maroba/findiff|title=A Python package for finite difference numerical derivatives in arbitrary number of dimensions.|last=|first=|date=|website=|archive-url=|archive-date=|dead-url=|access-date=}}</ref>.\n\n==Forward finite difference==\n\nThis table contains the coefficients of the forward differences, for several orders of accuracy and with uniform grid spacing:<ref name=fornberg/>\n\n{| class=\"wikitable\" style=\"text-align:center\"\n|-\n! Derivative\n! Accuracy\n! 0\n! 1\n! 2\n! 3\n! 4\n! 5\n! 6\n! 7\n! 8\n|-\n| rowspan=\"6\" | 1\n|| 1 || &minus;1 || 1 || &nbsp; || &nbsp; || &nbsp;|| &nbsp;|| &nbsp; || &nbsp; || &nbsp;\n|-\n|| 2 || &minus;3/2 || 2 || &minus;1/2 || &nbsp; || &nbsp;|| &nbsp;|| &nbsp; || &nbsp; || &nbsp;\n|-\n|| 3 || &minus;11/6 || 3 || &minus;3/2|| 1/3 || &nbsp; || &nbsp; || &nbsp; || &nbsp; || &nbsp;\n|-\n|| 4 || &minus;25/12 || 4 || &minus;3 || 4/3 || &minus;1/4|| &nbsp;|| &nbsp; || &nbsp; || &nbsp;\n|-\n|| 5 || &minus;137/60 || 5 || &minus;5 || 10/3 || &minus;5/4 || 1/5 || &nbsp; || &nbsp; || &nbsp;\n|- style=\"border-bottom: 2px solid #aaa;\"\n|| 6 || &minus;49/20 || 6 || &minus;15/2 || 20/3 || &minus;15/4 || 6/5 || &minus;1/6 || &nbsp; || &nbsp;\n|- \n| rowspan=\"6\" | 2\n|| 1 || 1 || &minus;2 || 1 || &nbsp; || &nbsp;|| &nbsp;|| &nbsp; || &nbsp; || &nbsp;\n|-\n|| 2 || 2 || &minus;5 || 4 || &minus;1 || &nbsp;|| &nbsp;|| &nbsp; || &nbsp; || &nbsp;\n|-\n|| 3 || 35/12 || &minus;26/3 || 19/2 || &minus;14/3 || 11/12 || &nbsp; || &nbsp; || &nbsp; || &nbsp;\n|-\n|| 4 || 15/4 || &minus;77/6 || 107/6 || &minus;13 || 61/12 || &minus;5/6|| &nbsp; || &nbsp; || &nbsp;\n|-\n|| 5 || 203/45 || &minus;87/5 || 117/4 || &minus;254/9 || 33/2 || &minus;27/5 || 137/180 || &nbsp; || &nbsp;\n|- style=\"border-bottom: 2px solid #aaa;\"\n|| 6 || 469/90 || &minus;223/10 || 879/20 || &minus;949/18 || 41 || &minus;201/10 || 1019/180 || &minus;7/10 || &nbsp;\n|- \n| rowspan=\"6\" | 3\n|| 1 || &minus;1 || 3 || &minus;3 || 1 || &nbsp;|| &nbsp;|| &nbsp; || &nbsp; || &nbsp;\n|-\n|| 2 || &minus;5/2 || 9 || &minus;12 || 7 || &minus;3/2|| &nbsp;|| &nbsp; || &nbsp; || &nbsp;\n|-\n|| 3 || &minus;17/4 || 71/4 || &minus;59/2 || 49/2 || &minus;41/4 || 7/4 || &nbsp; || &nbsp; || &nbsp;\n|-\n|| 4 || &minus;49/8 || 29 || &minus;461/8 || 62 || &minus;307/8 || 13 || &minus;15/8 || &nbsp; || &nbsp;\n|-\n|| 5 || &minus;967/120 || 638/15 || &minus;3929/40 || 389/3 || &minus;2545/24 || 268/5 || &minus;1849/120 || 29/15 || &nbsp;\n|- style=\"border-bottom: 2px solid #aaa;\"\n|| 6 || &minus;801/80 || 349/6 || &minus;18353/120 || 2391/10 || &minus;1457/6 || 4891/30 || &minus;561/8 || 527/30 || &minus;469/240\n|- \n| rowspan=\"5\" | 4\n|| 1 || 1 || &minus;4 || 6 || &minus;4 || 1|| &nbsp;|| &nbsp; || &nbsp; || &nbsp;\n|-\n|| 2 || 3 || &minus;14 || 26 || &minus;24 || 11 || &minus;2 || &nbsp; || &nbsp; || &nbsp;\n|-\n|| 3 || 35/6 || &minus;31 || 137/2 || &minus;242/3 || 107/2 || &minus;19 || 17/6 || &nbsp; || &nbsp;\n|-\n|| 4 || 28/3 || &minus;111/2 || 142 || &minus;1219/6 || 176 || &minus;185/2 || 82/3 || &minus;7/2 || &nbsp;\n|- style=\"border-bottom: 2px solid #aaa;\"\n|| 5 || 1069/80 || &minus;1316/15 || 15289/60 || &minus;2144/5 || 10993/24 || &minus;4772/15 || 2803/20 || &minus;536/15 || 967/240\n|}\n\nFor example, the first derivative with a third-order accuracy and the second derivative with a second-order accuracy are\n\n: <math>\\displaystyle f'(x_{0}) \\approx \\displaystyle \\frac{-\\frac{11}{6}f(x_{0}) + 3f(x_{+1}) -\\frac{3}{2}f(x_{+2}) +\\frac{1}{3}f(x_{+3}) }{h_{x}} + O\\left(h_{x}^3  \\right), </math>\n\n: <math>\\displaystyle f''(x_{0}) \\approx \\displaystyle \\frac{2f(x_{0}) - 5f(x_{+1}) + 4f(x_{+2}) - f(x_{+3}) }{h_{x}^2} + O\\left(h_{x}^2  \\right), </math>\n\nwhile the corresponding backward approximations are given by\n\n: <math>\\displaystyle f'(x_{0}) \\approx \\displaystyle \\frac{\\frac{11}{6}f(x_{0}) - 3f(x_{-1}) +\\frac{3}{2}f(x_{-2}) -\\frac{1}{3}f(x_{-3}) }{h_{x}} + O\\left(h_{x}^3  \\right), </math>\n\n: <math>\\displaystyle f''(x_{0}) \\approx \\displaystyle \\frac{2f(x_{0}) - 5f(x_{-1}) + 4f(x_{-2}) - f(x_{-3}) }{h_{x}^2} + O\\left(h_{x}^2  \\right), </math>\n\n==Backward finite difference==\n\nIn general, to get the coefficients of the backward approximations, give all odd derivatives listed in the table the opposite sign, whereas for even derivatives the signs stay the same.\nThe following table illustrates this:\n{| class=\"wikitable\" style=\"text-align:center\"\n|-\n! Derivative\n! Accuracy\n! &minus;8\n! &minus;7\n! &minus;6\n! &minus;5\n! &minus;4\n! &minus;3\n! &minus;2\n! &minus;1\n! 0\n|-\n| rowspan=\"2\" | 1\n|| 1 || &nbsp; || &nbsp; || &nbsp;|| &nbsp;|| &nbsp; || &nbsp; || &nbsp; || &minus;1 || 1\n|-\n|| 2 || &nbsp; || &nbsp;|| &nbsp;|| &nbsp; || &nbsp; || &nbsp; || 1/2 || &minus;2 || 3/2\n|-\n| rowspan=\"2\" | 2\n|| 1 || &nbsp; || &nbsp;|| &nbsp;|| &nbsp; || &nbsp; || &nbsp; || 1 || &minus;2 || 1\n|-\n|| 2 || &nbsp;|| &nbsp;|| &nbsp; || &nbsp; || &nbsp; || &minus;1 || 4 || &minus;5 || 2\n|-\n| rowspan=\"2\" | 3\n|| 1 || &nbsp;|| &nbsp;|| &nbsp; || &nbsp; || &nbsp; || &minus;1 || 3 || &minus;3 || 1\n|-\n|| 2|| &nbsp;|| &nbsp; || &nbsp; || &nbsp; || 3/2 || &minus;7 || 12 || &minus;9 || 5/2\n|-\n| rowspan=\"2\" | 4\n|| 1 || &nbsp;|| &nbsp; || &nbsp; || &nbsp; || 1 || &minus;4 || 6 || &minus;4 || 1\n|-\n|| 2 || &nbsp; || &nbsp; || &nbsp; || &minus;2 || 11 || &minus;24 || 26 || &minus;14 || 3\n|}\n\n==Arbitrary stencil points==\n\nFor a given arbitrary stencil points  <math>\\displaystyle s  </math> of length <math>\\displaystyle N  </math> with the order of derivatives <math>\\displaystyle d < N </math>, the finite difference coefficients can be obtained by solving the linear equations <ref>http://web.media.mit.edu/~crtaylor/calculator.html</ref>\n\n: <math>\n\\begin{pmatrix} \n s_1^0 & \\cdots & s_N^0 \\\\\n \\vdots & \\ddots & \\vdots \\\\\n s_1^{N-1} & \\cdots & s_N^{N-1} \n\\end{pmatrix}\n\\begin{pmatrix}\n a_1 \\\\\n \\vdots \\\\\n a_N\n\\end{pmatrix}\n= \nd!\n\\begin{pmatrix}\n \\delta_{0,d} \\\\\n \\vdots\\\\\n \\delta_{i,d}\\\\\n \\vdots\\\\\n \\delta_{N-1,d}\n\\end{pmatrix},\n</math>\n\nwhere the <math>\\delta_{i,j}</math> are the [[Kronecker delta]]. \n\nExample, for <math>s = [-3, -2, -1, 0, 1]</math>, order of differentiation <math>d = 4</math>:\n\n: <math>\n\\begin{pmatrix}\n a_{1} \\\\\n a_{2} \\\\\n a_{3} \\\\\n a_4 \\\\\n a_5\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n   1 &  1 &  1 & 1 & 1 \\\\\n  -3 & -2 & -1 & 0 & 1 \\\\\n   9 &  4 &  1 & 0 & 1 \\\\\n -27 & -8 & -1 & 0 & 1 \\\\\n  81 & 16 &  1 & 0 & 1 \\\\\n\\end{pmatrix}^{-1}\n\\begin{pmatrix}\n 0 \\\\\n 0 \\\\\n 0 \\\\\n 0 \\\\\n 24\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n  1 \\\\\n -4 \\\\\n  6 \\\\\n -4\\\\\n  1\n\\end{pmatrix}.\n</math>\n\nThe order of accuracy of the approximation takes the usual form <math>O\\left(h^{(N-d)}\\right)</math>.\n\n==See also==\n* [[Finite difference method]]\n* [[Finite difference]]\n* [[Five-point stencil]]\n* [[Numerical differentiation]]\n\n== References ==\n{{reflist}}\n\n{{Numerical PDE}}\n\n\n{{DEFAULTSORT:Finite Difference Coefficient}}\n[[Category:Finite differences]]\n[[Category:Numerical differential equations]]"
    },
    {
      "title": "Finite difference method",
      "url": "https://en.wikipedia.org/wiki/Finite_difference_method",
      "text": "{{distinguish|text=\"finite difference method based on variation principle\", the first name of [[finite element method]]{{citation needed|date=August 2013}}}}\n{{Multiple issues|\n{{lead too short|date=April 2015}}\n{{technical|date=April 2015}}\n}}\n{{Differential equations}}\n\nIn [[mathematics]], '''finite-difference methods''' (FDM) are [[numerical methods]] for solving [[differential equations]] by approximating them with [[Recurrence relation#Relationship to difference equations narrowly defined|difference equations]], in which [[Finite difference approximation|finite differences approximate]] the [[derivative]]s. FDMs are thus [[discretization]] methods. FDMs convert a linear (non-linear) ODE (Ordinary Differential Equations) /PDE (Partial differential equations) into a system of linear (non-linear) equations, which can then be solved by matrix algebra techniques. The reduction of the differential equation to a system of algebraic equations makes the problem of finding the solution to a given ODE ideally suited to modern computers, hence the widespread use of FDMs in modern numerical analysis<ref name=\"GrossmannRoos2007\" />.\n\nToday, FDMs are the dominant approach to numerical solutions of [[partial differential equation]]s.<ref name=\"GrossmannRoos2007\">{{cite book|author1=Christian Grossmann|author2=Hans-G. Roos|author3=Martin Stynes|title=Numerical Treatment of Partial Differential Equations|year=2007|publisher=Springer Science & Business Media|isbn=978-3-540-71584-9|page=23}}</ref>\n\n== Derivation from Taylor's polynomial ==\n\nFirst, assuming the function whose derivatives are to be approximated is properly-behaved, by [[Taylor's theorem]], we can create a [[Taylor series]] expansion\n\n:<math>f(x_0 + h) = f(x_0) + \\frac{f'(x_0)}{1!}h + \\frac{f^{(2)}(x_0)}{2!}h^2 + \\cdots + \\frac{f^{(n)}(x_0)}{n!}h^n + R_n(x),</math>\n\nwhere ''n''! denotes the [[factorial]] of ''n'', and ''R''<sub>''n''</sub>(''x'') is a remainder term, denoting the difference between the Taylor polynomial of degree ''n'' and the original function. We will derive an approximation for the first derivative of the function \"f\" by first truncating the Taylor polynomial:\n\n:<math>f(x_0 + h) = f(x_0) + f'(x_0)h + R_1(x),</math>\n\nSetting, x<sub>0</sub>=a we have,\n\n:<math>f(a+h) = f(a) + f'(a)h + R_1(x),</math>\n\nDividing across by ''h'' gives:\n\n:<math>{f(a+h)\\over h} = {f(a)\\over h} + f'(a)+{R_1(x)\\over h} </math>\n\nSolving for f'(a):\n\n:<math>f'(a) = {f(a+h)-f(a)\\over h} - {R_1(x)\\over h}</math>\n\nAssuming that <math>R_1(x)</math> is sufficiently small, the approximation of the first derivative of \"f\" is:\n\n:<math>f'(a)\\approx {f(a+h)-f(a)\\over h}.</math>\n\n== Accuracy and order ==\n{{see also|Finite difference coefficient}}\n\nThe error in a method's solution is defined as the difference between the approximation and the exact analytical solution. The two sources of error in finite difference methods are [[round-off error]], the loss of precision due to computer rounding of decimal quantities, and [[truncation error]] or [[discretization error]], the difference between the exact solution of the original differential equation and the exact quantity assuming perfect arithmetic (that is, assuming no round-off).\n\n[[File:Finite Differences.svg|right|thumb|The finite difference method relies on discretizing a function on a grid.]]\nTo use a finite difference method to approximate the solution to a problem, one must first discretize the problem's domain. This is usually done by dividing the domain into a uniform grid (see image to the right). Note that this means that finite-difference methods produce sets of discrete numerical approximations to the derivative, often in a \"time-stepping\" manner.\n\nAn expression of general interest is the [[local truncation error]] of a method. Typically expressed using [[Big-O notation]], local truncation error refers to the error from a single application of a method. That is, it is the quantity <math>f'(x_i) - f'_i</math> if <math>f'(x_i)</math> refers to the exact value and <math>f'_i</math> to the numerical approximation. The remainder term of a Taylor polynomial is convenient for analyzing the local truncation error. Using the Lagrange form of the remainder from the Taylor polynomial for <math>f(x_0 + h)</math>, which is\n\n<math>\n  R_n(x_0 + h) = \\frac{f^{(n+1)}(\\xi)}{(n+1)!} (h)^{n+1}\n</math>, where <math> x_0 < \\xi < x_0 + h</math>,\n\nthe dominant term of the local truncation error can be discovered. For example, again using the forward-difference formula for the first derivative, knowing that <math>f(x_i)=f(x_0+i h)</math>,\n\n:<math> f(x_0 + i h) = f(x_0) + f'(x_0)i h + \\frac{f''(\\xi)}{2!} (i h)^{2}, </math>\n\nand with some algebraic manipulation, this leads to\n\n:<math> \\frac{f(x_0 + i h) - f(x_0)}{i h} = f'(x_0) + \\frac{f''(\\xi)}{2!} i h, </math>\n\nand further noting that the quantity on the left is the approximation from the finite difference method and that the quantity on the right is the exact quantity of interest plus a remainder, clearly that remainder is the local truncation error. A final expression of this example and its order is:\n\n:<math> \\frac{f(x_0 + i h) - f(x_0)}{i h} = f'(x_0) + O(h). </math>\n\nThis means that, in this case, the local truncation error is proportional to the step sizes. The quality and duration of simulated FDM solution depends on the discretization equation selection and the step sizes (time and space steps). The data quality and simulation duration increase significantly with smaller step size.<ref name=IserlesA_1996>{{cite book|author1=Arieh Iserles|title= A first course in the numerical analysis of differential equations |year=2008|publisher= Cambridge University Press |isbn=9780521734905|page=23}}</ref> Therefore, a reasonable balance between data quality and simulation duration is necessary for practical usage. Large time steps are useful for increasing simulation speed in practice. However, time steps which are too large may create instabilities and affect the data quality.<ref name=HoffmanJD_2001>{{cite book|author1=Hoffman JD|author2=Frankel S|year=2001|title=Numerical methods for engineers and scientists|publisher=CRC Press, Boca Raton}}</ref><ref name=Jaluria_CM_1994>{{cite journal|author1=Jaluria Y|author2=Atluri S|year=1994|title=Computational heat transfer|journal= Computational Mechanics|volume=14|pages=385–386|doi=10.1007/BF00377593}}</ref>\n\nThe [[von Neumann stability analysis|von Neumann]] and [[Courant–Friedrichs–Lewy condition|Courant-Friedrichs-Lewy]] criteria are often evaluated to determine the numerical model stability.<ref name=HoffmanJD_2001 /><ref name=Jaluria_CM_1994 /><ref>{{cite book|author1=Majumdar P|year=2005|title=Computational methods for heat and mass transfer|edition=1st|publisher=Taylor and Francis, New York}}</ref><ref name=\"SmithGD_1985\">{{cite book|author1=Smith GD|year=1985|title=Numerical solution of partial differential equations: finite difference methods|publisher=Oxford University Press|edition=3rd}}</ref>\n\n== Example: ordinary differential equation ==\nFor example, consider the ordinary differential equation\n:<math> u'(x) = 3u(x) + 2. \\, </math>\nThe [[Euler method]] for solving this equation uses the finite difference quotient\n:<math>\\frac{u(x+h) - u(x)}{h} \\approx u'(x)</math>\nto approximate the differential equation by first substituting it for u'(x) then applying a little algebra (multiplying both sides by h, and then adding u(x) to both sides) to get\n:<math> u(x+h) = u(x) + h(3u(x)+2). \\, </math>\nThe last equation is a finite-difference equation, and solving this equation gives an approximate solution to the differential equation.\n\n==  Example: The heat equation ==\n\nConsider the normalized [[heat equation]] in one dimension, with homogeneous [[Dirichlet boundary condition]]s\n\n:<math> U_t=U_{xx} \\, </math>\n:<math> U(0,t)=U(1,t)=0 \\, </math>  (boundary condition)\n:<math> U(x,0) =U_0(x) \\, </math>   (initial condition)\n\nOne way to numerically solve this equation is to approximate all the derivatives by finite differences. We partition the domain in space using a mesh <math> x_0, ..., x_J </math> and in time using a mesh <math> t_0, ...., t_N </math>. We assume a uniform partition both in space and in time, so the difference between two consecutive space points will be ''h'' and between two consecutive time points will be ''k''. The points\n\n:<math> u(x_j,t_n) = u_{j}^n </math>\n\nwill represent the numerical approximation of <math> u(x_j, t_n). </math>\n\n===Explicit method===\n[[File:Explicit method-stencil.svg|right|thumb|The [[Stencil (numerical analysis)|stencil]] for the most common explicit method for the heat equation.]]\nUsing a [[forward difference]] '''at time <math> t_n </math>''' and a second-order [[central difference]] for the space derivative at position <math> x_j </math> ([[FTCS scheme|FTCS]]) we get the recurrence equation:\n\n:<math> \\frac{u_{j}^{n+1} - u_{j}^{n}}{k} = \\frac{u_{j+1}^n - 2u_{j}^n + u_{j-1}^n}{h^2}. \\, </math>\n\nThis is an [[explicit method]] for solving the one-dimensional [[heat equation]].\n\nWe can obtain <math> u_j^{n+1} </math> from the other values this way:\n\n:<math> u_{j}^{n+1} = (1-2r)u_{j}^{n} + ru_{j-1}^{n} + ru_{j+1}^{n}  </math>\n\nwhere <math> r=k/h^2. </math>\n\nSo, with this recurrence relation, and knowing the values at time ''n'', one can obtain the corresponding values at time ''n''+1. <math> u_0^n </math> and <math> u_J^n </math> must be replaced by the boundary conditions, in this example they are both 0.\n\nThis explicit method is known to be [[numerically stable]] and [[limit of a sequence|convergent]] whenever <math> r\\le 1/2 </math>.<ref>Crank, J. ''The Mathematics of Diffusion''. 2nd Edition, Oxford, 1975, p. 143.</ref> The numerical errors are proportional to the time step and the square of the space step:\n:<math> \\Delta u = O(k)+O(h^2)  \\, </math>\n\n===Implicit method===\n[[File:Implicit method-stencil.svg|right|thumb|The implicit method stencil.]]\nIf we use the [[backward difference]] '''at time <math> t_{n+1} </math>''' and a second-order central difference for the space derivative at position <math> x_j </math> (The Backward Time, Centered Space Method \"BTCS\") we get the recurrence equation:\n\n:<math> \\frac{u_{j}^{n+1} - u_{j}^{n}}{k} =\\frac{u_{j+1}^{n+1} - 2u_{j}^{n+1} + u_{j-1}^{n+1}}{h^2}. \\, </math>\n\nThis is an [[implicit method]] for solving the one-dimensional [[heat equation]].\n\nWe can obtain <math> u_j^{n+1} </math> from solving a system of linear equations:\n\n:<math> (1+2r)u_j^{n+1} - ru_{j-1}^{n+1} - ru_{j+1}^{n+1}= u_{j}^{n} </math>\n\nThe scheme is always [[numerically stable]] and convergent but usually more numerically intensive than the explicit method as it requires solving a system of numerical equations on each time step. The errors are  linear over the time step and quadratic over the space step:\n:<math> \\Delta u = O(k)+O(h^2).  \\, </math>\n\n===Crank&ndash;Nicolson method===\nFinally if we use the central difference at time <math> t_{n+1/2} </math> and a second-order central difference for the space derivative at position <math> x_j </math> (\"CTCS\") we get the recurrence equation:\n\n:<math> \\frac{u_j^{n+1} - u_j^{n}}{k} = \\frac{1}{2} \\left(\\frac{u_{j+1}^{n+1} - 2u_j^{n+1} + u_{j-1}^{n+1}}{h^2}+\\frac{u_{j+1}^{n} - 2u_j^{n} + u_{j-1}^{n}}{h^2}\\right).\\, </math>\n\nThis formula is known as the [[Crank&ndash;Nicolson method]].\n[[File:Crank-Nicolson-stencil.svg|right|thumb|The Crank&ndash;Nicolson stencil.]]\n\nWe can obtain <math> u_j^{n+1} </math> from solving a system of linear equations:\n\n:<math> (2+2r)u_j^{n+1} - ru_{j-1}^{n+1} - ru_{j+1}^{n+1}= (2-2r)u_j^n + ru_{j-1}^n + ru_{j+1}^n </math>\n\nThe scheme is always [[numerically stable]] and convergent but usually more numerically intensive as it requires solving a system of numerical equations on each time step. The errors are quadratic over both the time step and the space step:\n:<math> \\Delta u = O(k^2)+O(h^2).  \\, </math>\n\nUsually the Crank&ndash;Nicolson scheme is the most accurate scheme for small time steps. The explicit scheme is the least accurate and can be unstable, but is also the easiest to implement and the least numerically intensive. The implicit scheme works the best for large time steps.\n\n===Comparison===\nThe figures below present the solutions given by the above methods to approximate the heat equation \n\n: <math>U_t = \\alpha U_{xx}, \\quad \\alpha = \\frac{1}{\\pi^2},</math>\n\nwith the boundary condition\n\n: <math>U(0, t) = U(1, t) = 0.</math>\n\nThe exact solution is\n\n:<math>U(x, t) = \\frac{1}{\\pi^2}e^{-t}\\sin(\\pi x).</math>\n\n{{multiple image\n<!-- Essential parameters -->\n| perrow = 3\n| align     = center\n| direction = horizontal\n| width     = 260\n<!-- Extra parameters -->\n| header = Comparison of Finite Difference Methods\n| header_align = center\n| header_background = \n| footer = \n| footer_align = \n| footer_background = \n| background color =\n\n|image1=HeatEquationExplicitApproximate.svg\n|width1=260\n|caption1=<div class=\"center\" style=\"width:auto; margin-left:auto; margin-right:auto;\">Explicit method (''not'' stable)</div>\n|alt1= ''c'' = 4\n\n|image2=HeatEquationImplicitApproximate.svg\n|width2=260\n|caption2=<div class=\"center\" style=\"width:auto; margin-left:auto; margin-right:auto;\">Implicit method (stable)</div>\n|alt2= ''c'' = 6\n\n|image3=HeatEquationCNApproximate.svg\n|width3=260\n|caption3=<div class=\"center\" style=\"width:auto; margin-left:auto; margin-right:auto;\">Crank-Nicolson method (stable)</div>\n|alt3= ''c'' = 8.5\n}}\n\n== Example: The Laplace operator ==\nThe (continuous) [[Laplace operator]] in <math> n </math>-dimensions is given by <math> \\Delta u(x) = \\sum_{i=1}^n \\partial_i^2 u(x) </math>.\nThe discrete Laplace operator <math> \\Delta_h u </math> depends on the dimension <math> n </math>.\n\nIn 1D the Laplace operator is approximated as \n:<math>\n  \\Delta u(x) = u''(x)\n    \\approx \\frac{u(x-h)-2u(x)+u(x+h)}{h^2 }\n    =: \\Delta_h u(x) \\,.\n</math>\nThis approximation is usually expressed via the following [[Stencil_(numerical_analysis)|stencil]]\n:<math>\n  \\frac{1}{h^2} \n  \\begin{bmatrix}\n    1 & -2 & 1\n  \\end{bmatrix}\n  \\,.\n</math>\n\nThe 2D case shows all the characteristics of the more general nD case. Each second partial derivative needs to be approximated similar to the 1D case \n:<math>\n  \\begin{align}\n  \\Delta u(x,y) &= u_{xx}(x,y)+u_{yy}(x,y) \\\\\n    &\\approx \\frac{u(x-h,y)-2u(x,y)+u(x+h,y) }{h^2}\n      + \\frac{u(x,y-h) -2u(x,y) +u(x,y+h)}{h^2} \\\\\n    &= \\frac{u(x-h,y)+u(x+h,y) -4u(x,y)+u(x,y-h)+u(x,y+h)}{h^2} \\\\\n    &=: \\Delta_h u(x, y) \\,,\n  \\end{align}\n</math>\nwhich is usually given by the following [[Stencil_(numerical_analysis)|stencil]]\n:<math>\n  \\frac{1}{h^2} \n  \\begin{bmatrix}\n      & 1 \\\\\n    1 & -4 & 1 \\\\\n      & 1 \n  \\end{bmatrix}\n  \\,.\n</math>\n\n=== Consistency ===\nConsistency of the above-mentioned approximation can be shown for highly regular functions, such as <math> u \\in C^4(\\Omega) </math>.\nThe statement is\n:<math>\n  \\Delta u - \\Delta_h u = \\mathcal{O}(h^2) \\,.\n</math>\n\nTo proof this one needs to substitute [[Taylor Series]] expansions up to order 3 into the discrete Laplace operator.\n\n=== Properties ===\n==== Subharmonic ====\nSimilar to [[Harmonic_function|continuous subharmonic functions]] one can define ''subharmonic functions'' for finite-difference approximations <math>u_h</math>\n:<math>\n  -\\Delta_h u_h \\leq 0 \\,.\n</math>\n\n==== Mean value ====\nOne can define a general [[Stencil_(numerical_analysis)|stencil]] of ''positive type'' via\n:<math>  \n  \\begin{bmatrix}\n      & \\alpha_N \\\\\n    \\alpha_W & -\\alpha_C & \\alpha_E \\\\\n      & \\alpha_S\n  \\end{bmatrix}\n  \\,, \\quad \\alpha_i >0\\,, \\quad \\alpha_C = \\sum_{i\\in \\{N,E,S,W\\}} \\alpha_i \\,.\n</math>\n\nIf <math> u_h </math> is (discrete) subharmonic then the following'' mean value property'' holds\n:<math>\n  u_h(x_C) \\leq \\frac{ \\sum_{i\\in \\{N,E,S,W\\}} \\alpha_i u_h(x_i) }{ \\sum_{i\\in \\{N,E,S,W\\}} \\alpha_i } \\,,\n</math>\nwhere the approximation is evaluated on points of the grid, and the stencil is assumed to be of positive type.\n\nA similar [[Harmonic_function#The_mean_value_property|mean value property]] also holds for the continuous case.\n\n==== Maximum principle ====\nFor a (discrete) subharmonic function <math> u_h </math> the following holds\n:<math>\n  \\max_{\\Omega_h} u_h \\leq \\max_{\\partial \\Omega_h} u_h \\,,\n</math>\nwhere <math> \\Omega_h, \\partial\\Omega_h </math> are discretizations of the continuous domain <math> \\Omega </math>, respectively the boundary <math> \\partial \\Omega </math>.\n\nA similar [[Harmonic_function#Maximum_principle|maximum principle]] also holds for the continuous case.\n\n==See also==\n{{Div col|colwidth=20em}}\n* [[Finite element method]]\n* [[Finite difference]]\n* [[Finite difference time domain]]\n* [[Infinite difference method]]\n* [[Stencil (numerical analysis)]]\n* [[Finite difference coefficients]]\n* [[Five-point stencil]]\n* [[Lax–Richtmyer theorem]]\n* [[Finite difference methods for option pricing]]\n* [[Upwind differencing scheme for convection]]\n* [[Central differencing scheme]]\n* [[Discrete Poisson equation]]\n* [[Discrete Laplace operator]]\n{{div col end}}\n\n==References==\n{{Reflist}}\n* K.W. Morton and D.F. Mayers, ''Numerical Solution of Partial Differential Equations, An Introduction''. Cambridge University Press, 2005.\n* Autar Kaw and E. Eric Kalu, ''Numerical Methods with Applications'', (2008) [http://www.autarkaw.com/books/numericalmethods/index.html]. Contains a brief, engineering-oriented introduction to FDM (for ODEs) in [http://numericalmethods.eng.usf.edu/topics/finite_difference_method.html Chapter 08.07].\n* {{cite book|author=John Strikwerda|title=Finite Difference Schemes and Partial Differential Equations|year=2004|publisher=SIAM|isbn=978-0-89871-639-9|edition=2nd}}\n* {{Citation\n | last = Smith\n | first = G. D.\n | title = Numerical Solution of Partial Differential Equations: Finite Difference Methods, 3rd ed.\n | publisher = Oxford University Press\n | year = 1985\n}}\n* {{cite book|author=Peter Olver|author-link=Peter J. Olver|title=Introduction to Partial Differential Equations|year=2013|publisher=Springer|isbn=978-3-319-02099-0|at=Chapter 5: Finite differences|url=http://www.math.umn.edu/~olver/pde.html}}.\n* [[Randall J. LeVeque]], ''[http://faculty.washington.edu/rjl/fdmbook/ Finite Difference Methods for Ordinary and Partial Differential Equations]'', SIAM, 2007.\n\n=== Various lectures and lecture notes ===\n* [https://web.archive.org/web/20140302111344/http://emlab.utep.edu/ee5390cem.htm Finite-Difference Method in Electromagnetics (see and listen to lecture 9)]\n*[https://web.archive.org/web/20090206071754/http://ltl.iams.sinica.edu.tw/document/training_lectures/2006/SH_Chen/Finite_Difference_Methods.pdf Lecture Notes] Shih-Hung Chen, [[National Central University]]\n*[http://jwhaverkort.net23.net/documents/NumMethPDEs.pdf Numerical Methods for time-dependent Partial Differential Equations]\n\n{{Numerical PDE}}\n\n{{Authority control}}\n{{DEFAULTSORT:Finite Difference Method}}\n[[Category:Finite differences]]\n[[Category:Numerical differential equations]]"
    },
    {
      "title": "Geodesic grid",
      "url": "https://en.wikipedia.org/wiki/Geodesic_grid",
      "text": "{{expert needed|date=July 2017}}\n[[File:Geodesic Grid (ISEA3H) illustrated.png|thumb|Screenshot of PYXIS WorldView showing an ISEA geodesic grid.]]\nA '''geodesic grid''' is a [[spatial grid]] based on a [[geodesic polyhedron]] or [[Goldberg polyhedron]].\n\n==Construction==\n[[File:Geodesic grid used in atmosphere simulation.png|alt=|thumb|Volume rendering of Geodesic grid<ref>{{Cite journal|last=Xie|first=Jinrong|last2=Yu|first2=Hongfeng|last3=Ma|first3=Kwan-Liu|date=2013-06-01|title=Interactive Ray Casting of Geodesic Grids|journal=Computer Graphics Forum|language=en|volume=32|issue=3pt4|pages=481–490|doi=10.1111/cgf.12135|issn=1467-8659|citeseerx=10.1.1.361.7299}}</ref> applied in atmosphere simulation using Global Cloud Resolving Model (GCRM)<ref>{{Cite journal|last=Khairoutdinov|first=Marat F.|last2=Randall|first2=David A.|date=2001-09-15|title=A cloud resolving model as a cloud parameterization in the NCAR Community Climate System Model: Preliminary results|journal=Geophysical Research Letters|language=en|volume=28|issue=18|pages=3617–3620|doi=10.1029/2001gl013552|issn=1944-8007|bibcode=2001GeoRL..28.3617K}}</ref>. The combination of grid illustration and volume rendering of vorticity (yellow tubes) . Note that for the purpose of clear illustration in the image, the grid is coarser than the actual one used to generate the vorticity.]][[File:Icosahedron.svg|thumb|The icosahedron]]\n[[File:Conway_dccccD.png|thumb|A highly divided [[geodesic polyhedron]] based on the icosahedron]]\n[[File:Conway_ccccD.png|thumb|A highly divided [[Goldberg polyhedron]]: the dual of the above image.]]\n\nA geodesic grid is a global Earth reference that uses triangular tiles based on the subdivision of a polyhedron (usually the [[icosahedron]], and usually a Class I subdivision) to subdivide the surface of the Earth. Such a grid does not have a straightforward relationship to latitude and longitude, but conforms to many of the main criteria for a statistically valid discrete global grid.<ref>{{cite web |last=Clarke |first=Keith C |title=Criteria and Measures for the Comparison of Global Geocoding Systems |work=Discrete Global Grids: Goodchild, M. F. and A. J. Kimerling, Eds |url=http://www.geog.ucsb.edu/~kclarke/Papers/GlobalGrids.html |year= 2000}}</ref> Primarily, the cells' area and shape are generally similar, especially near the poles where many other spatial grids have singularities or heavy distortion. The popular Quaternary Triangular Mesh (QTM) falls into this category.<ref>{{cite web|url=http://www.spatial-effects.com/SE-papers1.html |first = Geoffrey| last=Dutton|title=Spatial Effects: Research Papers}}</ref>\n\nGeodesic grids may use the [[dual polyhedron]] of the geodesic polyhedron, which is the [[Goldberg polyhedron]]. Goldberg polyhedra are made up of hexagons and (if based on the icosahedron) 12 pentagons. One implementation that uses an [[icosahedron]] as the base polyhedron, hexagonal cells, and the [[Snyder equal-area projection]] is known as the Icosahedron Snyder Equal Area (ISEA) grid.<ref>{{cite journal |last=Mahdavi-Amiri |first=Ali |author2=Harrison.E |author3=Samavati.F |title=hexagonal connectivity maps for digital earth |journal=International Journal of Digital Earth |volume=8 |issue=9 |pages=750 |year=2014 |doi=10.1080/17538947.2014.927597 |bibcode= 2015IJDE....8..750M}}</ref>\n\n==Applications==\nIn biodiversity science, geodesic grids are a global extension of local discrete grids that are staked out in field studies to ensure appropriate statistical sampling and larger multi-use grids deployed at regional and national levels to develop an aggregated understanding of biodiversity. These grids translate environmental and ecological monitoring data from multiple spatial and temporal scales into assessments of current ecological condition and forecasts of risks to our natural resources. A geodesic grid allows local to global assimilation of ecologically significant information at its own level of granularity.<ref>{{cite journal |doi=10.1559/152304092783786636 |last=White |first=D |author2=Kimerling AJ |author3=Overton WS |title=Cartographic and geometric components of a global sampling design for environmental monitoring. |journal=Cartography and Geographic Information Systems |volume=19 |pages=5–22 |year=1992 |issue=1}}</ref>\n\nWhen modeling the [[weather prediction|weather]], ocean circulation, or the [[climatology|climate]], [[partial differential equation]]s are used to describe the evolution of these systems over time. Because computer programs are used to build and work with these complex models, approximations need to be formulated into easily computable forms. Some of these [[numerical analysis]] techniques (such as [[finite differences]]) require the area of interest to be subdivided into a grid — in this case, over the [[geodesy|shape of the Earth]].\n\nGeodesic grids can be used in [[video game development]] to model fictional worlds instead of the Earth. They are a natural analog of the [[hex map]] to a spherical surface.<ref>{{cite web|url=http://simblob.blogspot.com/2016/10/hexagon-tiling-of-sphere.html|last=Patel |first=Amit |title=Hexagon tiling of a sphere |year= 2016}}</ref>\n\n==Pros and cons==\nPros:\n*Largely [[isotropic]].\n*Resolution can be easily increased by binary division.\n*Does not suffer from over sampling near the poles like more traditional rectangular longitude–latitude square grids.\n*Does not result in dense linear systems like [[spectral method]]s do (see also [[Gaussian grid]]).\n*No single points of contact between neighboring grid cells. [[Square grid]]s and isometric grids suffer from the ambiguous problem of how to handle neighbors that only touch at a single point.\n*Cells can be both minimally distorted and near-equal-area. In contrast, square grids are not equal area, while equal-area rectangular grids vary in shape from equator to poles.\n\nCons:\n*More complicated to implement than rectangular longitude–latitude grids in computers\n\n==History==\nThe earliest use of the (icosahedral) geodesic grid in geophysical modeling dates back to 1968 and the work by Sadourny, Arakawa, and Mintz<ref>{{cite journal |last=Sadourny |first=R. |author2=A. Arakawa |author3= Y. Mintz |title=Integration of the non-divergent barotropic vorticity equation with an icosahedral-hexagonal grid for the sphere |journal=Monthly Weather Review |volume=96 |pages=351–356 |year=1968 |doi=10.1175/1520-0493(1968)096<0351:IOTNBV>2.0.CO;2|bibcode= 1968MWRv...96..351S |issue=6|citeseerx=10.1.1.395.2717 }}</ref> and Williamson.<ref>{{cite journal |last=Williamson |first=D. L. |title=Integration of the barotropic vorticity equation on a spherical geodesic grid |journal=Tellus |volume=20 |pages=642–653 |year=1968 |doi=10.1111/j.2153-3490.1968.tb00406.x |issue=4|bibcode=1968TellA..20..642W }}</ref><ref>Williamson, 1969</ref> Later work expanded on this base.<ref>{{cite journal |last=Cullen |first=M. J. P. |title=Integrations of the primitive equations on a sphere using the finite-element method |journal=Quarterly Journal of the Royal Meteorological Society |volume=100 |pages=555–562 |year=1974 |doi=10.1002/qj.49710042605|bibcode= 1974QJRMS.100..555C |issue=426}}</ref><ref>Cullen and Hall, 1979.</ref><ref>{{cite conference |last=Masuda |first=Y. Girard1 |title=An integration scheme of the primitive equation model with an icosahedral-hexagonal grid system and its application to the shallow-water equations |booktitle=Short- and Medium-Range Numerical Weather Prediction |publisher=Japan Meteorological Society |pages=317–326 |year=1987}}</ref><ref>{{cite journal |last=Heikes |first=Ross |author2=David A. Randall |title=Numerical integration of the shallow-water equations on a twisted icosahedral grid. Part I: Basic design and results of tests |journal=Monthly Weather Review |volume=123 |pages=1862–1880 |year=1995 |doi=10.1175/1520-0493(1995)123<1862:NIOTSW>2.0.CO;2|bibcode= 1995MWRv..123.1862H |issue=6}}{{cite journal |last=Heikes |first=Ross |author2=David A. Randall |title=Numerical integration of the shallow-water equations on a twisted icosahedral grid. Part II: A detailed description of the grid and an analysis of numerical accuracy |journal=Monthly Weather Review |volume=123 |pages=1881–1887 |year=1995 |doi=10.1175/1520-0493(1995)123<1881:NIOTSW>2.0.CO;2|bibcode= 1995MWRv..123.1881H |issue=6}}</ref><ref>Randall ''et al.'', 2000; Randall ''et al.'', 2002.</ref>\n==See also==\n*[[Grid reference]]\n* [[Discrete Global Grid]]\n* [[Spherical design]], generalization to more than three dimensions\n* The [[quadrilateralized spherical cube]], a grid over the earth based on the cube and made of quadrilaterals instead of triangles\n[[File:Geodesic grid gcrm atmosphere vorticity 4096x4096.png|thumb|High quality volume rendering<ref>{{Cite journal|last=Xie|first=Jinrong|last2=Yu|first2=Hongfeng|last3=Ma|first3=Kwan-Liu|date=2013-06-01|title=Interactive Ray Casting of Geodesic Grids|journal=Computer Graphics Forum|language=en|volume=32|issue=3pt4|pages=481–490|doi=10.1111/cgf.12135|issn=1467-8659|citeseerx=10.1.1.361.7299}}</ref> of atmosphere simulation at global scale based on Geodesic grid. The colored strips indicate the simulated atmosphere vorticity strength based on GCRM model<ref>{{Cite journal|last=Khairoutdinov|first=Marat F.|last2=Randall|first2=David A.|date=2001-09-15|title=A cloud resolving model as a cloud parameterization in the NCAR Community Climate System Model: Preliminary results|journal=Geophysical Research Letters|language=en|volume=28|issue=18|pages=3617–3620|doi=10.1029/2001gl013552|issn=1944-8007|bibcode=2001GeoRL..28.3617K}}</ref>.]]\n[[File:Geodesic grid adaptive mesh refinement ocean simulation mpas model.png|thumb|A variation of geodesic grid with adaptive mesh refinement which devotes higher resolution mesh at regions of interests increasing the simulation precision while keeping the memory footage at manageable size<ref>{{Cite book|last=Xie|first=J.|last2=Yu|first2=H.|last3=Maz|first3=K. L.|date=November 2014|title=Visualizing large 3D geodesic grid data with massively distributed GPUs|journal=2014 IEEE 4th Symposium on Large Data Analysis and Visualization (LDAV)|pages=3–10|doi=10.1109/ldav.2014.7013198|isbn=978-1-4799-5215-1}}</ref>.]]\n[[File:Geodesic grid MPAS ocean simulation vorticity 4096x4096.png|thumb|High quality volume rendering<ref>{{Cite book|last=Xie|first=J.|last2=Yu|first2=H.|last3=Maz|first3=K. L.|date=November 2014|title=Visualizing large 3D geodesic grid data with massively distributed GPUs|journal=2014 IEEE 4th Symposium on Large Data Analysis and Visualization (LDAV)|pages=3–10|doi=10.1109/ldav.2014.7013198|isbn=978-1-4799-5215-1}}</ref> of ocean simulation at global scale based on Geodesic grid. The colored strip indicate the simulated ocean vorticity strength based on MPAS model<ref>{{Cite journal|last=Ringler|first=Todd|last2=Petersen|first2=Mark|last3=Higdon|first3=Robert L.|last4=Jacobsen|first4=Doug|last5=Jones|first5=Philip W.|last6=Maltrud|first6=Mathew|title=A multi-resolution approach to global ocean modeling|journal=Ocean Modelling|volume=69|pages=211–232|doi=10.1016/j.ocemod.2013.04.010|bibcode=2013OcMod..69..211R|year=2013}}</ref>.]]\n\n==References==\n{{Reflist}}\n\n==External links==\n{{sisterlinks|d=Q1898473|b=no|n=no|v=no|voy=no|s=no|m=no|mw=no|species=no|q=no|c=Category:Geodesic diagrams|wikt=no}}\n*[http://kiwi.atmos.colostate.edu/BUGS/geodesic/ BUGS climate model] page on geodesic grids\n*[http://www.discreteglobalgrids.org/ Discrete Global Grids] page at the Computer Science department at Southern Oregon University\n*[http://www.pyxisinnovation.com/pyxwiki/index.php?title=How_PYXIS_Works the PYXIS innovation [[Digital Earth Reference Model]] ].\n*[http://www.sciencedirect.com/science/article/pii/S0377042706006522 Interpolation on spherical geodesic grids: A comparative study]\n\n[[Category:Finite differences]]\n[[Category:Geodesy]]\n[[Category:Geometric data structures]]\n[[Category:Numerical climate and weather models]]"
    },
    {
      "title": "Hermite interpolation",
      "url": "https://en.wikipedia.org/wiki/Hermite_interpolation",
      "text": "In [[numerical analysis]], '''Hermite interpolation''', named after [[Charles Hermite]], is a method of [[interpolation|interpolating data points]] as a [[polynomial function]].  The generated Hermite interpolating polynomial is closely related to the [[Newton polynomial]], in that both are derived from the calculation of [[divided differences]]. However, the Hermite interpolating polynomial may also be computed without using divided differences, see {{slink|Chinese remainder theorem|Hermite interpolation}}.\n\nUnlike Newton interpolation, Hermite interpolation matches an unknown function both in observed value, and the observed value of its first ''m'' derivatives. This means that ''n''(''m''&nbsp;+&nbsp;1) values\n:<math>\n\\begin{matrix}\n(x_0, y_0), &(x_1, y_1), &\\ldots, &(x_{n-1}, y_{n-1}), \\\\\n(x_0, y_0'), &(x_1, y_1'), &\\ldots, &(x_{n-1}, y_{n-1}'), \\\\\n\\vdots & \\vdots & &\\vdots  \\\\\n(x_0, y_0^{(m)}), &(x_1, y_1^{(m)}), &\\ldots, &(x_{n-1}, y_{n-1}^{(m)})\n\\end{matrix}\n</math>\nmust be known, rather than just the first ''n'' values required for Newton interpolation. The resulting polynomial may have degree at most ''n''(''m''&nbsp;+&nbsp;1)&nbsp;&minus;&nbsp;1, whereas the Newton polynomial has maximum degree ''n''&nbsp;&minus;&nbsp;1. (In the general case, there is no need for ''m'' to be a fixed value; that is, some points may have more known derivatives than others. In this case the resulting polynomial may have degree ''N''&nbsp;&minus;&nbsp;1, with ''N'' the number of data points.)\n\n== Usage ==\n\n=== Simple case ===\nWhen using divided differences to calculate the Hermite polynomial of a function ''f'', the first step is to copy each point ''m'' times. (Here we will consider the simplest case <math>m = 1</math> for all points.) Therefore, given <math>n + 1</math> data points <math>x_0, x_1, x_2, \\ldots, x_n</math>, and values <math>f(x_0), f(x_1), \\ldots, f(x_n)</math> and <math>f'(x_0), f'(x_1), \\ldots, f'(x_n)</math> for a function <math>f</math> that we want to interpolate, we create a new dataset\n:<math>z_0, z_1, \\ldots, z_{2n+1}</math>\nsuch that\n:<math>z_{2i}=z_{2i+1}=x_i.</math>\n\nNow, we create a [[Divided differences|divided differences table]] for the points <math>z_0, z_1, \\ldots, z_{2n+1}</math>. However, for some divided differences,\n:<math>z_i = z_{i + 1}\\implies f[z_i, z_{i+1}] = \\frac{f(z_{i+1})-f(z_{i})}{z_{i+1}-z_{i}} = \\frac{0}{0}</math>\nwhich is undefined.\nIn this case, the divided difference is replaced by <math>f'(z_i)</math>. All others are calculated normally.\n\n=== General case ===\nIn the general case, suppose a given point <math>x_i</math> has ''k'' derivatives. Then the dataset <math>z_0, z_1, \\ldots, z_{N}</math> contains ''k'' identical copies of <math>x_i</math>. When creating the table, [[divided differences]] of <math>j = 2, 3, \\ldots, k</math> identical values will be calculated as\n\n:<math>\\frac{f^{(j)}(x_i)}{j!}.</math>\n\nFor example,\n:<math>f[x_i, x_i, x_i]=\\frac{f''(x_i)}{2}</math>\n:<math>f[x_i, x_i, x_i, x_i]=\\frac{f^{(3)}(x_i)}{6}</math>\netc.\n\n=== Example ===\nConsider the function <math>f(x) = x^8 + 1</math>. Evaluating the function and its first two derivatives at <math>x \\in \\{-1, 0, 1\\}</math>, we obtain the following data:\n:{| class=\"wikitable\" style=\"text-align: right; padding: 1em;\"\n|-\n! ''x''  || ''&fnof;''(''x'') || ''&fnof;''<nowiki>'</nowiki>(''x'')  || ''&fnof;''<nowiki>''</nowiki>(''x'')\n|-\n| &minus;1 ||  2   ||  &minus;8     || 56\n|-\n| 0  ||  1   ||  0     || 0\n|-\n| 1  ||  2   ||  8    || 56\n|}\n\nSince we have two derivatives to work with, we construct the set <math>\\{z_i\\} = \\{-1, -1, -1, 0, 0, 0, 1, 1, 1\\}</math>. Our divided difference table is then:\n:<math>\n\\begin{array}{llcclrrrrr}\nz_0 = -1  &  f[z_0] = 2  &                          &                         &                           &      &     &   &    & \\\\\n          &              &  \\frac{f'(z_0)}{1} = -8  &                         &                           &      &     &   &    & \\\\\nz_1 = -1  &  f[z_1] = 2  &                          & \\frac{f''(z_1)}{2} = 28 &                           &      &     &   &    & \\\\\n          &              &  \\frac{f'(z_1)}{1} = -8  &                         &  f[z_3,z_2,z_1,z_0] = -21 &      &     &   &    & \\\\\nz_2 = -1  &  f[z_2] = 2  &                          & f[z_3,z_2,z_1] = 7      &                           &  15  &     &   &    & \\\\\n          &              &  f[z_3,z_2] = -1         &                         &  f[z_4,z_3,z_2,z_1] = -6  &      & -10 &   &    & \\\\\nz_3 =  0  &  f[z_3] = 1  &                          & f[z_4,z_3,z_2] = 1      &                           &   5  &     & 4 &    & \\\\\n          &              &  \\frac{f'(z_3)}{1} = 0   &                         &  f[z_5,z_4,z_3,z_2] = -1  &      &  -2 &   & -1 & \\\\\nz_4 =  0  &  f[z_4] = 1  &                          & \\frac{f''(z_4)}{2} = 0  &                           &   1  &     & 2 &    & 1 \\\\\n          &              &  \\frac{f'(z_4)}{1} = 0   &                         &  f[z_6,z_5,z_4,z_3] =  1  &      &   2 &   &  1 & \\\\\nz_5 =  0  &  f[z_5] = 1  &                          & f[z_6,z_5,z_4] = 1      &                           &   5  &     & 4 &    & \\\\\n          &              &  f[z_6,z_5] = 1          &                         &  f[z_7,z_6,z_5,z_4] =  6  &      &  10 &   &    & \\\\\nz_6 =  1  &  f[z_6] = 2  &                          & f[z_7,z_6,z_5] = 7      &                           &  15  &     &   &    & \\\\\n          &              &  \\frac{f'(z_6)}{1} = 8   &                         &  f[z_8,z_7,z_6,z_5] =  21 &      &     &   &    & \\\\\nz_7 =  1  &  f[z_7] = 2  &                          & \\frac{f''(z_7)}{2} = 28 &                           &      &     &   &    & \\\\\n          &              &  \\frac{f'(z_7)}{1} = 8   &                         &                           &      &     &   &    & \\\\\nz_8 =  1  &  f[z_8] = 2  &                          &                         &                           &      &     &   &    & \\\\\n\\end{array}\n</math>\nand the generated polynomial is\n:<math>\n\\begin{align}\nP(x) &= 2 - 8(x+1) + 28(x+1) ^2 - 21 (x+1)^3 + 15x(x+1)^3 - 10x^2(x+1)^3 \\\\\n&\\quad{} + 4x^3(x+1)^3 -1x^3(x+1)^3(x-1)+x^3(x+1)^3(x-1)^2 \\\\\n&=2 - 8 + 28 - 21 - 8x + 56x - 63x + 15x + 28x^2 - 63x^2 + 45x^2 - 10x^2 - 21x^3 \\\\\n&\\quad {}+ 45x^3 - 30x^3 + 4x^3 + x^3 + x^3 + 15x^4 - 30x^4 + 12x^4 + 2x^4 + x^4 \\\\\n&\\quad {}- 10x^5 + 12x^5 - 2x^5 + 4x^5 - 2x^5 - 2x^5 - x^6 + x^6 - x^7 + x^7 + x^8 \\\\\n&= x^8 + 1.\n\\end{align}\n</math>\nby taking the coefficients from the diagonal of the divided difference table, and multiplying the ''k''th coefficient by <math>\\prod_{i=0}^{k-1} (x - z_i)</math>, as we would when generating a Newton polynomial.\n\n==Error==\nCall the calculated polynomial ''H'' and original function ''f''. Evaluating a point <math>x \\in [x_0, x_n]</math>, the error function is\n:<math>f(x) - H(x) = \\frac{f^{(K)}(c)}{K!}\\prod_{i}(x - x_i)^{k_i}</math>\nwhere ''c'' is an unknown within the range <math>[x_0, x_N]</math>, ''K'' is the total number of data-points, and <math>k_i</math> is the number of derivatives known at each <math>x_i</math> plus one.\n\n==See also==\n*[[Cubic Hermite spline]]\n*[[Newton series]], also known as [[finite differences]]\n*[[Neville's schema]]\n*[[Polynomial interpolation]]\n*[[Lagrange polynomial|Lagrange form]] of the interpolation polynomial\n*[[Bernstein polynomial|Bernstein form]] of the interpolation polynomial\n*[[Chinese remainder theorem#Applications|Chinese remainder theorem - Applications]]\n\n==References==\n* {{ cite book|last1=Burden|first1=Richard L.|first2= J. Douglas |last2=Faires|title=Numerical Analysis|publisher= Belmont: Brooks/Cole|year= 2004}}\n* {{Citation |last=Spitzbart |first=A. |title=A Generalization of Hermite's Interpolation Formula |journal=[[American Mathematical Monthly]] |volume=67 |issue=1 |pages=42&ndash;46 |date=January 1960 |jstor=2308924 |doi= 10.2307/2308924}}\n\n==External links==\n*[http://mathworld.wolfram.com/HermitesInterpolatingPolynomial.html Hermites Interpolating Polynomial] at Mathworld\n\n[[Category:Interpolation]]\n[[Category:Finite differences]]\n[[Category:Factorial and binomial topics]]"
    },
    {
      "title": "Higher-order compact finite difference scheme",
      "url": "https://en.wikipedia.org/wiki/Higher-order_compact_finite_difference_scheme",
      "text": "{{Multiple issues|{{Original research|date=June 2013}}{{Cleanup-weighted|date=August 2013}}{{Orphan|date=June 2013}}}}\n\n[[File:HOC stencil.jpg|thumb|The nine-point HOC stencil]]\n'''High-order compact finite difference schemes''' are used for solving third-order [[differential equations]] created during the study of [[Obstacle problem|obstacle boundary value problems]]. They have been shown to be highly accurate and efficient. They are constructed by modifying the second-order scheme that was developed by Noor and Al-Said in 2002. The [[convergence rate]] of the high-order compact scheme is third order, the second-order scheme is fourth order.<ref>{{Cite journal | last1 = Xie | first1 = S. | last2 = Li | first2 = P. | last3 = Gao | first3 = Z. | last4 = Wang | first4 = H. | title = High order compact finite difference schemes for a system of third order boundary value problem | doi = 10.1016/j.amc.2012.08.091 | journal = Applied Mathematics and Computation | volume = 219 | issue = 5 | pages = 2564 | year = 2012 | pmid =  | pmc = }}</ref>\n\n[[Differential equation]]s are essential tools in [[mathematical modelling]]. Most [[physical system]]s are described in terms of mathematical models that include convective and diffusive transport of some variables. [[Finite difference method]]s are amongst the most popular methods that have been applied most frequently in solving such differential equations.  A finite difference scheme is compact in the sense that the discretised formula comprises at most nine point [[Stencil (numerical analysis)|stencils]] which includes a [[Node (autonomous system)|node]] in the middle about which differences are taken. In addition, greater order of accuracy (more than two) justifies the terminology 'higher-order compact finite difference scheme' (HOC). This can be achieved in several ways. The higher-order compact scheme considered here <ref name=\"Kalita 2002\">Kalita JC, Dalal DC and Dass AK.,  A class of higher-order compact schemes for the unsteady two-dimensional convection-diffusion equations with variable convection coefficients., Int. J. Numer. Meth. Fluids, Vol. 101,(2002), pp. 1111–1131</ref> is by using the original differential equation to substitute for the leading [[Truncation error (numerical integration)|truncation error]] terms in the finite difference equation. Overall, the scheme is found to be robust, efficient and accurate for most [[computational fluid dynamics]] (CFD) applications discussed here further.\n\nThe simplest problem for the validation of the numerical algorithms is the Lid Driven cavity problem. Computed results in form of tables, graphs and figures for a fluid with Prandtl number = 0.71 with Rayleigh number (Ra) ranging from 10<sup>3</sup> to 10<sup>7</sup> are available in the literature.<ref name=\"Kalita 2002\"/> The efficacy of the scheme is proved when it very clearly captures the secondary and tertiary vortices at the sides of the cavity at high values of Ra.\n\nAnother milestone was the development of these schemes for solving two dimensional steady/unsteady convection diffusion equations. A comprehensive study of flow past an impulsively started circular cylinder was made.<ref>J. C. Kalita, and R. K. Ray., A transformation-free HOC scheme for incompressible viscous flows past an impulsively started circular cylinder, Int. J. Numer. Meth. Fluids, Vol. 228,(2009), pp. 5207–5236</ref> The problem of flow past a circular cylinder has continued to generate tremendous interest{{Clarify|reason=vague|date=January 2016}} amongst researchers working in CFD mainly because it displays almost all the fluid mechanical phenomena for incompressible, viscous flows in the simplest of geometrical settings. It was able to analyze and visualize the flow patterns more accurately for Reynold's number (Re) ranging from 10 to 9500 compared to the existing numerical results. This was followed by its extension to rotating counterpart of the cylinder surface for Re ranging from 200 to 1000.<ref>R. K. Ray., A transformation free HOC scheme for incompressible viscous flow past a rotating and translating circular cylinder, J. Sci. Comput., Vol. 46,(2011), pp. 265–293</ref> More complex phenomenon that involves a circular cylinder undergoing rotational oscillations while translating in a fluid is studied for Re as high as 500.<ref>Rajendra K. Ray and H. V. R. Mittal, A transform-free HOC scheme for incompressible viscous flow past a rotationally oscillating circular cylinder, Proc. World Academy of Science Engineering and Technology, Bangkok, Issue 72,(December 2012), pp. 1069–1075</ref> <ref>H. V. R. Mittal, Rajendra K. Ray and Qasem M. Al-Mdallal, A numerical study of initial flow past an impulsively started rotationally oscillating circular cylinder using a transformation-free HOC scheme, Physics of Fluids, vol. 29, no. 9 (2017), pp. 093603</ref> <ref>H. V. R. Mittal, Qasem M. Al-Mdallal and Rajendra K. Ray, Locked-on vortex shedding modes from a rotationally oscillating circular cylinder, Ocean Engineering, vol. 146 (2017), pp. 324-338</ref> \n\nAnother benchmark in the history is its extension to multiphase flow phenomena. Natural processes such as gas bubble in oil, ice melting, wet steam  are observed everywhere in nature.  Such processes also play an important role with the practical applications in the area of biology, medicine, [[environmental remediation]]. The scheme has been successively implemented to solve one and two dimensional elliptic and parabolic equation with discontinuous coefficients and singular source terms.<ref>Rajendra K. Ray, J. C. Kalita, and A. K. Dass, An efficient HOC scheme for transient convection–diffusion reaction equations with discontinuous coefficients and singular source terms, Proc. Appl. Math. Mech., vol. 7, no. 1(2007), pp. 1025603–1025604</ref> These type of problems hold importance numerically because they usually lead to non-smooth or discontinuous solutions across the interfaces.  Expansion of this idea from fixed to moving interfaces with both regular and irregular geometries is currently going on <ref>H. V. R. Mittal, Jiten C. Kalita and Rajendra K. Ray, A class of finite difference schemes for interface problems with an HOC approach, International Journal for Numerical Methods in Fluids, vol. 82, no. 9 (2016), pp. 567-606</ref>, <ref> H. V. R. Mittal, Ray, Rajendra K. Ray, Solving Immersed Interface Problems Using a New Interfacial Points-Based Finite Difference Approach, SIAM Journal on Scientific Computing, vol. 40, no. 3 (2018), pp. A1860-A1883 </ref>.\n\n==References==\n{{reflist}}\n\n[[Category:Finite differences]]\n[[Category:Numerical differential equations]]"
    },
    {
      "title": "Indefinite sum",
      "url": "https://en.wikipedia.org/wiki/Indefinite_sum",
      "text": "In [[mathematics]] the '''indefinite sum''' operator (also known as the '''antidifference''' operator), denoted by <math>\\sum _x </math> or <math>\\Delta^{-1} </math>,<ref>{{PlanetMath|urlname=IndefiniteSum|title=Indefinite Sum}}</ref><ref>[http://hostel6.ru/books/_Papers/Computer_algebra/Summation/Man.%20Closed%20forms%20for%20symbolic%20summation.%20JSC%201993%20(22s).pdf On Computing Closed Forms for Indefinite Summations. Yiu-Kwong Man. J. Symbolic Computation (1993), 16, 355-376]</ref><ref>\"If ''Y'' is a function whose first difference is the function ''y'', then ''Y'' is called an indefinite sum of ''y'' and denoted Δ<sup>−1</sup>''y''\" [https://books.google.com/books?id=5rFOeE0zvY4C&pg=PA41&dq=%22indefinite+sum%22 ''Introduction to Difference Equations''], Samuel Goldberg</ref> is the [[linear operator]], inverse of the [[difference operator|forward difference operator]] <math>\\Delta </math>. It relates to the [[difference operator|forward difference operator]] as the [[indefinite integral]] relates to the [[derivative]]. Thus\n\n:<math>\\Delta \\sum_x f(x) = f(x) \\, .</math>\n\nMore explicitly, if <math>\\sum_x f(x) = F(x) </math>, then\n\n:<math>F(x+1) - F(x) = f(x) \\, .</math>\n\nIf ''F''(''x'') is a solution of this functional equation for a given ''f''(''x''), then so is ''F''(''x'')+''C(x)'' for any periodic function ''C(x)'' with period 1. Therefore, each indefinite sum actually represents a family of functions. However the solution equal to its [[Finite difference#Newton's series|Newton series]] expansion is unique up to an additive constant C. This unique solution can be represented by formal power series form of the antidifference operator: <math>\\Delta^{-1}=\\frac1{e^D-1}</math>\n\n==Fundamental theorem of discrete calculus==\n\nIndefinite sums can be used to calculate definite sums with the formula:<ref>\"Handbook of discrete and combinatorial mathematics\", Kenneth H. Rosen, John G. Michaels, CRC Press, 1999, {{ISBN|0-8493-0149-1}}</ref>\n \n:<math>\\sum_{k=a}^b f(k)=\\Delta^{-1}f(b+1)-\\Delta^{-1}f(a)</math>\n\n==Definitions==\n\n===Laplace summation formula===\n\n:<math>\\sum _x f(x)=\\int_0^x f(t) dt +\\sum_{k=1}^\\infty \\frac{c_k\\Delta^{k-1}f(x)}{k!} + C </math>\n\n:where <math>c_k=\\int_0^1 \\frac{\\Gamma(x+1)}{\\Gamma(x-k+1)}dx</math> are the Cauchy numbers of the first kind.<ref>[http://mathworld.wolfram.com/BernoulliNumberoftheSecondKind.html Bernoulli numbers of the second kind on Mathworld]</ref>\n{{Citation needed|reason=While Cauchy numbers are cited, equation for Laplace summation formula is not|date=September 2018}}\n\n===Newton's formula===\n:<math>\\sum_x f(x)=\\sum_{k=1}^\\infty \\binom{x}k \\Delta^{k-1} [f]\\left (0\\right)+C=\\sum_{k=1}^{\\infty}\\frac{\\Delta^{k-1}[f](0)}{k!}(x)_k+C</math>\n\n:where <math>(x)_k=\\frac{\\Gamma(x+1)}{\\Gamma(x-k+1)}</math> is the [[falling factorial]].\n\n===Faulhaber's formula===\n\n:<math>\\sum _x f(x)= \\sum_{n=1}^{\\infty} \\frac{f^{(n-1)} (0)}{n!} B_n(x) + C \\, ,</math>\n\nprovided that the right-hand side of the equation converges.\n\n===Mueller's formula===\nIf <math>\\lim_{x\\to{+\\infty}}f(x)=0,</math> then<ref>[http://www.math.tu-berlin.de/~mueller/HowToAdd.pdf Markus Müller. How to Add a Non-Integer Number of Terms, and How to Produce Unusual Infinite Summations] {{webarchive|url=https://web.archive.org/web/20110617053801/http://www.math.tu-berlin.de/~mueller/HowToAdd.pdf |date=2011-06-17 }} (note that he uses a slightly alternative definition of fractional sum in his work, i.e. inverse to backwards difference, hence 1 as the lower limit in his formula)</ref>\n\n:<math>\\sum _x f(x)=\\sum_{n=0}^\\infty\\left(f(n)-f(n+x)\\right)+ C.</math>\n\n===Euler–Maclaurin formula===\n\n:<math>\\sum _x f(x)= \\int_0^x f(t) dt - \\frac12 f(x)+\\sum_{k=1}^{\\infty}\\frac{B_{2k}}{(2k)!}f^{(2k-1)}(x) + C</math>\n\n==Choice of the constant term==\nOften the constant C in indefinite sum is fixed from the following condition.\n\nLet\n\n:<math>F(x)=\\sum _x f(x)+C</math>\n\nThen the constant C is fixed from the condition\n\n: <math>\\int_0^1 F(x) dx=0 </math>\n\nor\n\n: <math>\\int_1^2 F(x) dx=0 </math>\n\nAlternatively, Ramanujan's sum can be used:\n\n: <math>\\sum_{x \\ge 1}^{\\Re}f(x)=-f(0)-F(0)</math>\n\nor at 1\n\n: <math>\\sum_{x \\ge 1}^{\\Re}f(x)=-F(1)</math>\n\nrespectively<ref>Bruce C. Berndt, [http://www.comms.scitech.susx.ac.uk/fft/math/RamanujanNotebooks1.pdf Ramanujan's Notebooks] {{webarchive|url=https://web.archive.org/web/20061012064851/http://www.comms.scitech.susx.ac.uk/fft/math/RamanujanNotebooks1.pdf |date=2006-10-12 }}, ''Ramanujan's Theory of Divergent Series'', Chapter 6, Springer-Verlag (ed.), (1939), pp. 133&ndash;149.</ref><ref>Éric Delabaere, [http://algo.inria.fr/seminars/sem01-02/delabaere2.pdf Ramanujan's Summation], ''Algorithms Seminar 2001–2002'', F. Chyzak (ed.), INRIA, (2003), pp. 83–88.</ref>\n\n==Summation by parts==\n\n{{main|Summation by parts}}\n\nIndefinite summation by parts:\n:<math>\\sum_x f(x)\\Delta g(x)=f(x)g(x)-\\sum_x (g(x)+\\Delta g(x)) \\Delta f(x) </math>\n\n:<math>\\sum_x f(x)\\Delta g(x)+\\sum_x g(x)\\Delta f(x)=f(x)g(x)-\\sum_x \\Delta f(x)\\Delta g(x) </math>\n\nDefinite summation by parts:\n:<math>\\sum_{i=a}^b f(i)\\Delta g(i)=f(b+1)g(b+1)-f(a)g(a)-\\sum_{i=a}^b g(i+1)\\Delta f(i)</math>\n\n==Period rules==\n\nIf <math>T </math> is a period of function <math>f(x)</math> then\n\n:<math>\\sum _x f(Tx)=x f(Tx) + C</math>\n\nIf <math>T </math> is an antiperiod of function <math>f(x)</math>, that is <math>f(x+T)=-f(x)</math> then\n\n:<math>\\sum _x f(Tx)=-\\frac12 f(Tx) + C</math>\n\n==Alternative usage==\n\nSome authors use the phrase \"indefinite sum\" to describe a sum in which the numerical value of the upper limit is not given:\n\n:<math>\\sum_{k=1}^n f(k).</math>\n\nIn this case a closed form expression ''F''(''k'') for the sum is a solution of\n\n:<math>F(x+1) - F(x) = f(x+1) </math>\n\nwhich is called the telescoping equation.<ref>[http://www.risc.uni-linz.ac.at/people/mkauers/publications/kauers05c.pdf Algorithms for Nonlinear Higher Order Difference Equations], Manuel Kauers</ref> It is the inverse of the [[backward difference]] <math>\\nabla</math> operator.\nIt is related to the forward antidifference operator using the fundamental theorem of discrete calculus described earlier.\n\n==List of indefinite sums==\n\nThis is a list of indefinite sums of various functions. Not every function has an indefinite sum that can be expressed in terms of elementary functions.\n\n===Antidifferences of rational functions===\n\n:<math>\\sum _x a = ax + C </math>\n\n:<math>\\sum _x x = \\frac{x^2}{2}-\\frac{x}{2} + C</math>\n\n:<math>\\sum _x x^a = \\frac{B_{a+1}(x)}{a+1} + C,\\,a\\notin \\mathbb{Z}^-</math>\n:where <math>B_a(x)=-a\\zeta(-a+1,x)</math>, the generalized to real order [[Bernoulli polynomials]].\n\n:<math>\\sum _x x^a = \\frac{(-1)^{a-1}\\psi^{(-a-1)}(x)}{\\Gamma(-a)}+ C,\\,a\\in\\mathbb{Z}^-</math>\n:where <math>\\psi^{(n)}(x)</math> is the [[polygamma function]].\n\n:<math>\\sum _x \\frac1x = \\psi(x) + C </math>\n:where <math>\\psi(x)</math> is the [[digamma function]].\n\n===Antidifferences of exponential functions===\n\n:<math>\\sum _x a^x = \\frac{a^x}{a-1} + C </math>\n\nParticularly,\n\n:<math>\\sum _x 2^x = 2^x + C </math>\n\n===Antidifferences of logarithmic functions===\n\n:<math>\\sum _x \\log_b x = \\log_b \\Gamma (x) + C </math>\n\n:<math>\\sum _x \\log_b ax = \\log_b (a^{x-1}\\Gamma (x)) + C </math>\n\n===Antidifferences of hyperbolic functions===\n\n:<math>\\sum _x \\sinh ax = \\frac{1}{2} \\operatorname{csch} \\left(\\frac{a}{2}\\right) \\cosh \\left(\\frac{a}{2} - a x\\right) + C  </math>\n\n:<math>\\sum _x \\cosh ax = \\frac{1}{2} \\coth \\left(\\frac{a}{2}\\right) \\sinh ax -\\frac{1}{2} \\cosh ax + C  </math>\n\n:<math>\\sum _x \\tanh ax = \\frac1a \\psi _{e^a}\\left(x-\\frac{i \\pi }{2 a}\\right)+\\frac1a \\psi _{e^a}\\left(x+\\frac{i \\pi }{2 a}\\right)-x + C</math>\n\n:where <math>\\psi_q(x)</math> is the [[q-analog|q-digamma]] function.\n\n===Antidifferences of trigonometric functions===\n\n:<math>\\sum _x \\sin ax = -\\frac{1}{2} \\csc \\left(\\frac{a}{2}\\right) \\cos \\left(\\frac{a}{2}- a x \\right) + C \\,,\\,\\,a\\ne n \\pi </math>\n\n:<math>\\sum _x \\cos ax = \\frac{1}{2} \\cot \\left(\\frac{a}{2}\\right) \\sin ax -\\frac{1}{2} \\cos ax + C \\,,\\,\\,a\\ne n \\pi</math>\n\n:<math>\\sum _x \\sin^2 ax = \\frac{x}{2} + \\frac{1}{4} \\csc (a) \\sin (a-2 a x) + C \\, \\,,\\,\\,a\\ne \\frac{n\\pi}2</math>\n\n:<math>\\sum _x \\cos^2 ax = \\frac{x}{2}-\\frac{1}{4} \\csc (a) \\sin (a-2 a x) + C  \\,\\,,\\,\\,a\\ne \\frac{n\\pi}2</math>\n\n:<math>\\sum_x \\tan ax = i x-\\frac1a \\psi _{e^{2 i a}}\\left(x-\\frac{\\pi }{2 a}\\right) + C \\,,\\,\\,a\\ne \\frac{n\\pi}2</math>\n\n:where <math>\\psi_q(x)</math> is the [[q-analog|q-digamma]] function.\n\n:<math>\\sum_x \\tan x=ix-\\psi _{e^{2 i}}\\left(x+\\frac{\\pi }{2}\\right) + C = -\\sum _{k=1}^{\\infty } \\left(\\psi \\left(k \\pi -\\frac{\\pi }{2}+1-z\\right)+\\psi \\left(k \\pi -\\frac{\\pi }{2}+z\\right)-\\psi \\left(k \\pi -\\frac{\\pi }{2}+1\\right)-\\psi \\left(k \\pi -\\frac{\\pi }{2}\\right)\\right) + C</math>\n\n:<math>\\sum_x \\cot ax =-i x-\\frac{i \\psi _{e^{2 i a}}(x)}{a} + C \\,,\\,\\,a\\ne \\frac{n\\pi}2</math>\n\n===Antidifferences of inverse hyperbolic functions===\n\n:<math>\\sum_x \\operatorname{artanh}\\, a x =\\frac{1}{2} \\ln \\left(\\frac{\\Gamma \\left(x+\\frac{1}{a}\\right)}{\\Gamma \\left(x-\\frac{1}{a}\\right)}\\right) + C</math>\n\n===Antidifferences of inverse trigonometric functions===\n\n:<math>\\sum_x \\arctan a x = \\frac{i}{2} \\ln \\left(\\frac{\\Gamma (x+\\frac ia)}{ \\Gamma (x-\\frac ia)}\\right)+C</math>\n\n===Antidifferences of special functions===\n\n:<math>\\sum _x \\psi(x)=(x-1) \\psi(x)-x+C </math>\n\n:<math>\\sum _x \\Gamma(x)=(-1)^{x+1}\\Gamma(x)\\frac{\\Gamma(1-x,-1)}e+C</math>\n\n:where <math>\\Gamma(s,x)</math> is the [[incomplete gamma function]].\n\n:<math>\\sum _x (x)_a = \\frac{(x)_{a+1}}{a+1}+C</math>\n\n:where <math>(x)_a</math> is the [[falling factorial]].\n\n:<math>\\sum _x \\operatorname{sexp}_a (x) = \\ln_a \\frac{(\\operatorname{sexp}_a (x))'}{(\\ln a)^x} + C </math>\n:(see [[super-exponential function]])\n\n==See also==\n*[[Indefinite product]]\n*[[Time scale calculus]]\n*[[List of derivatives and integrals in alternative calculi]]\n\n==References==\n{{reflist}}\n\n==Further reading==\n* \"Difference Equations: An Introduction with Applications\", Walter G. Kelley, Allan C. Peterson, Academic Press, 2001, {{ISBN|0-12-403330-X}}\n* [https://web.archive.org/web/20110617053801/http://www.math.tu-berlin.de/~mueller/HowToAdd.pdf Markus Müller. How to Add a Non-Integer Number of Terms, and How to Produce Unusual Infinite Summations]\n* [https://arxiv.org/abs/math/0502109 Markus Mueller, Dierk Schleicher. Fractional Sums and Euler-like Identities]\n* [http://www.springerlink.com/content/kj0jx24240756457/ S. P. Polyakov. Indefinite summation of rational functions with additional minimization of the summable part. Programmirovanie, 2008, Vol. 34, No. 2.]\n* \"Finite-Difference Equations And Simulations\", Francis B. Hildebrand, Prenctice-Hall, 1968\n\n{{DEFAULTSORT:Indefinite Sum}}\n[[Category:Mathematical analysis]]\n[[Category:Mathematical tables|Indefinite sums]]\n[[Category:Finite differences]]\n[[Category:Linear operators in calculus]]"
    },
    {
      "title": "Infinite difference method",
      "url": "https://en.wikipedia.org/wiki/Infinite_difference_method",
      "text": "In [[mathematics]], '''infinite difference methods''' are [[numerical methods]] for solving [[differential equations]] by approximating them with [[Recurrence relation#Relationship to difference equations narrowly defined|difference equations]], in which [[Finite difference#Generalizations|infinite differences approximate]] the [[derivative]]s.\n\n==See also==\n* [[Infinite element method]]\n* [[Finite difference]]\n* [[Finite difference time domain]]\n\n==References==\n{{Reflist}}\n* [http://www.sciencedirect.com/science/article/pii/S0022072803001037 Simulation of ion transfer under conditions of natural convection by the finite difference method]\n* {{cite book|author1=Han, Houde|author2=Wu, Xiaonan|title=Artificial Boundary Method|year=2013|publisher=Springer|isbn=978-3-642-35464-9|at=Chapter 6: Discrete Artificial Boundary Conditions|url=https://www.springer.com/la/book/9783642354632}}.\n* [http://www.khuisf.ac.ir/prof/Images/Uploaded_Files/3445-8593-4-PB%5B6699598%5D.PDF Genetic Algorithm and Numerical Solution]\n\n{{Authority control}}\n\n{{DEFAULTSORT:Infinite Difference Method}}\n[[Category:Finite differences]]\n[[Category:Numerical differential equations]]\n\n{{Numerical PDE}}\n\n{{Applied-math-stub}}"
    },
    {
      "title": "Kronecker sum of discrete Laplacians",
      "url": "https://en.wikipedia.org/wiki/Kronecker_sum_of_discrete_Laplacians",
      "text": "In mathematics, the '''Kronecker sum of discrete Laplacians''', named after [[Leopold Kronecker]], is a discrete version of the [[separation of variables]] for the continuous [[Laplacian]] in a [[Cuboid#Rectangular_cuboid|rectangular cuboid]] domain.\n\n==General form of the Kronecker sum of discrete Laplacians==\n\nIn a general situation of the [[separation of variables]] in the discrete case, the  multidimensional [[Discrete Laplace operator|discrete Laplacian]] is a [[Kronecker_product#Kronecker_sum_and_exponentiation|Kronecker sum]] of 1D discrete Laplacians.\n\n===Example: 2D discrete Laplacian on a regular grid with the homogeneous Dirichlet boundary condition===\n\nMathematically, using the [[Kronecker_product#Kronecker_sum|Kronecker sum]]:\n\n:<math>L = \\mathbf{D_{xx}}\\oplus\\mathbf{D_{yy}}=\\mathbf{D_{xx}}\\otimes\\mathbf{I}+\\mathbf{I}\\otimes\\mathbf{D_{yy}}, \\,</math>\n\nwhere <math>\\mathbf{D_{xx}} </math> and <math>\\mathbf{D_{yy}} </math> are 1D discrete Laplacians in the ''x''- and ''y''-directions, correspondingly, and <math>\\mathbf{I} </math> are the identities of appropriate sizes. Both <math>\\mathbf{D_{xx}} </math> and <math>\\mathbf{D_{yy}} </math> must correspond to the case of the homogeneous [[Dirichlet boundary condition]] at end points of the ''x''- and ''y''-intervals, in order to generate the 2D discrete Laplacian ''L'' corresponding to the homogeneous [[Dirichlet boundary condition]] everywhere on the boundary of the rectangular domain. \n\nHere is a sample [[GNU_Octave|OCTAVE]]/[[MATLAB]] code to compute ''L'' on the regular 10&times;15 2D grid:\n<source lang=\"matlab\">\nnx = 10; % number of grid points in the x-direction;\nny = 15; % number of grid points in the y-direction;\nex = ones(nx,1);\nDxx = spdiags([ex -2*ex ex], [-1 0 1], nx, nx); %1D discrete Laplacian in the x-direction ;\ney = ones(ny,1);\nDyy = spdiags([ey, -2*ey ey], [-1 0 1], ny, ny); %1D discrete Laplacian in the y-direction ;\nL = kron(Dyy, speye(nx)) + kron(speye(ny), Dxx) ;\n</source>\n\n==Eigenvalues and eigenvectors of multidimensional discrete Laplacian on a regular grid==\n\nKnowing all [[eigenvalue]]s and [[eigenvector]]s of the factors, all [[eigenvalue]]s and [[eigenvector]]s of the [[Kronecker product]] can be [[Kronecker_product#Spectrum|explicitly calculated]]. Based on this, [[eigenvalue]]s and [[eigenvector]]s of the [[Kronecker_product#Kronecker_sum_and_exponentiation|Kronecker sum]]\ncan also be explicitly calculated. \n\nThe [[eigenvalue]]s and [[eigenvector]]s of the standard [[Central_difference#Higher-order_differences|central difference approximation of the second derivative]] on an interval for traditional combinations of boundary conditions at the interval end points are [[Eigenvalues and eigenvectors of the second derivative|well known]]. Combining these expressions with the formulas of [[eigenvalue]]s and [[eigenvector]]s for the [[Kronecker_product#Kronecker_sum_and_exponentiation|Kronecker sum]], one can easily obtain the required answer.\n\n===Example: 3D discrete Laplacian on a regular grid with the homogeneous Dirichlet boundary condition===\n\n:<math>L = \\mathbf{D_{xx}}\\otimes\\mathbf{I}\\otimes\\mathbf{I}+\\mathbf{I}\\otimes\\mathbf{D_{yy}}\\otimes\\mathbf{I}+\\mathbf{I}\\otimes\\mathbf{I}\\otimes\\mathbf{D_{zz}}, \\,</math>\n\nwhere <math>\\mathbf{D_{xx}},\\,\\mathbf{D_{yy}}</math> and <math>\\mathbf{D_{zz}} </math> are 1D discrete Laplacians in every of the 3 directions, and <math>\\mathbf{I} </math> are the identities of appropriate sizes. Each 1D discrete Laplacian must correspond to the case of the homogeneous [[Dirichlet boundary condition]], in order to generate the 3D discrete Laplacian ''L'' corresponding to the homogeneous [[Dirichlet boundary condition]] everywhere on the boundary. The eigenvalues are\n\n:<math> \\lambda_{jx,jy,jz} = \n-\\frac{4}{h_x^2} \\sin\\left(\\frac{\\pi j_x}{2(n_x + 1)}\\right)^2\n-\\frac{4}{h_y^2} \\sin\\left(\\frac{\\pi j_y}{2(n_y + 1)}\\right)^2\n-\\frac{4}{h_z^2} \\sin\\left(\\frac{\\pi j_z}{2(n_z + 1)}\\right)^2\n</math>\n\nwhere <math> j_x=1,\\ldots,n_x,\\,j_y=1,\\ldots,n_y,\\,j_z=1,\\ldots,n_z,\\,</math>, and the corresponding eigenvectors are\n\n:<math>v_{ix,iy,iz,jx,jy,jz} = \n\\sqrt{\\frac{2}{n_x+1}} \\sin\\left(\\frac{i_x j_x \\pi}{n_x+1}\\right)\n\\sqrt{\\frac{2}{n_y+1}} \\sin\\left(\\frac{i_y j_y \\pi}{n_y+1}\\right)\n\\sqrt{\\frac{2}{n_z+1}} \\sin\\left(\\frac{i_z j_z \\pi}{n_z+1}\\right)\n</math>\n\nwhere the multi-index <math>{jx,jy,jz}</math> pairs the eigenvalues and the eigenvectors, while the multi-index\n<math>{ix,iy,iz}</math> determines the location of the value of every eigenvector at the [[regular grid]]. The boundary points, where \nthe homogeneous [[Dirichlet boundary condition]] are imposed, are just outside the grid.\n\n==Available software==\nAn [[GNU_Octave|OCTAVE]]/[[MATLAB]] code http://www.mathworks.com/matlabcentral/fileexchange/27279-laplacian-in-1d-2d-or-3d is available under a [[BSD License]], which computes the sparse matrix of the 1, 2D, and 3D negative Laplacians on a rectangular grid for combinations of Dirichlet, Neumann, and Periodic boundary conditions using [[Kronecker sum]]s of discrete 1D Laplacians. The code also provides the exact [[eigenvalue]]s and [[eigenvector]]s using the explicit formulas given above.\n\n[[Category:Operator theory]]\n[[Category:Matrix theory]]\n[[Category:Numerical differential equations]]\n[[Category:Finite differences]]\n[[Category:Articles with example MATLAB/Octave code]]"
    },
    {
      "title": "Mean value theorem (divided differences)",
      "url": "https://en.wikipedia.org/wiki/Mean_value_theorem_%28divided_differences%29",
      "text": "In [[mathematical analysis]], the '''mean value theorem for divided differences''' generalizes the [[mean value theorem]] to higher derivatives.<ref>{{cite journal|last=de Boor|first=C.|title=Divided differences|journal=Surv. Approx. Theory|year=2005|volume=1|pages=46&ndash;69|authorlink=Carl R. de Boor|mr=2221566}}</ref>\n\n==Statement of the theorem==\n\nFor any ''n''&nbsp;+&nbsp;1 pairwise distinct points ''x''<sub>0</sub>,&nbsp;...,&nbsp;''x''<sub>''n''</sub> in the domain of an ''n''-times differentiable function ''f'' there exists an interior point\n\n: <math> \\xi \\in (\\min\\{x_0,\\dots,x_n\\},\\max\\{x_0,\\dots,x_n\\}) \\,</math>\n\nwhere the ''n''th derivative of ''f'' equals ''n''&nbsp;<nowiki>!</nowiki> times the ''n''th [[divided difference]] at these points:\n\n: <math> f[x_0,\\dots,x_n] = \\frac{f^{(n)}(\\xi)}{n!}.</math>\n\nFor ''n''&nbsp;=&nbsp;1, that is two function points, one obtains the simple [[mean value theorem]].\n\n==Proof==\n\nLet <math>P</math> be the [[Lagrange interpolation polynomial]] for ''f'' at ''x''<sub>0</sub>,&nbsp;...,&nbsp;''x''<sub>''n''</sub>.\nThen it follows from the [[Newton polynomial|Newton form]] of <math>P</math> that the highest term of <math>P</math> is <math>f[x_0,\\dots,x_n](x-x_{n-1})\\dots(x-x_1)(x-x_0)</math>.\n\nLet <math>g</math> be the remainder of the interpolation, defined by <math>g = f - P</math>.  Then <math>g</math> has <math>n+1</math> zeros: ''x''<sub>0</sub>,&nbsp;...,&nbsp;''x''<sub>''n''</sub>.\nBy applying [[Rolle's theorem]] first to <math>g</math>, then to <math>g'</math>, and so on until <math>g^{(n-1)}</math>, we find that <math>g^{(n)}</math> has a zero <math>\\xi</math>.  This means that\n\n: <math> 0 = g^{(n)}(\\xi) = f^{(n)}(\\xi) - f[x_0,\\dots,x_n] n!</math>,\n: <math> f[x_0,\\dots,x_n] = \\frac{f^{(n)}(\\xi)}{n!}.</math>\n\n==Applications==\nThe theorem can be used to generalise the [[Stolarsky mean]] to more than two variables.\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Mean Value Theorem (Divided Differences)}}\n[[Category:Finite differences]]"
    },
    {
      "title": "Newton polynomial",
      "url": "https://en.wikipedia.org/wiki/Newton_polynomial",
      "text": "{{more citations needed|date=September 2014}}\n\nIn the [[mathematical]] field of [[numerical analysis]], a '''Newton polynomial''', named after its inventor [[Isaac Newton]],{{Citation needed|date=October 2017}} is an [[polynomial interpolation|interpolation]] [[polynomial]] for a given set of data points. The Newton polynomial is sometimes called '''Newton's divided differences interpolation polynomial''' because the coefficients of the polynomial are calculated using Newton's [[divided differences]] method.\n\n==Definition==\nGiven a set of ''k''&nbsp;+&nbsp;1 data points\n\n:<math>(x_0, y_0),\\ldots,(x_j, y_j),\\ldots,(x_k, y_k)</math>\n\nwhere no two ''x''<sub>''j''</sub> are the same, the Newton interpolation polynomial is a [[linear combination]] of '''Newton basis polynomials'''\n\n:<math>N(x) := \\sum_{j=0}^{k} a_{j} n_{j}(x)</math>\n\nwith the Newton basis polynomials defined as\n\n:<math>n_j(x) := \\prod_{i=0}^{j-1} (x - x_i)</math>\n\nfor ''j'' > 0 and <math>n_0(x) \\equiv 1</math>.\n\nThe coefficients are defined as\n\n:<math>a_j := [y_0,\\ldots,y_j]</math>\n\nwhere\n\n:<math>[y_0,\\ldots,y_j]</math>\n\nis the notation for [[divided differences]].\n\nThus the Newton polynomial can be written as\n\n:<math>N(x) = [y_0] + [y_0,y_1](x-x_0) + \\cdots + [y_0,\\ldots,y_k](x-x_0)(x-x_1)\\cdots(x-x_{k-1}).</math>\n\n===Newton forward divided difference formula=== \nThe Newton polynomial can be expressed in a simplified form when\n <math>x_0, x_1, \\dots, x_k</math> \nare arranged consecutively with equal spacing. \nIntroducing the notation \n<math>h = x_{i+1}-x_i</math> for each <math>i=0,1,\\dots,k-1</math>\n and <math>x=x_0+sh</math>, \nthe difference <math>x-x_i</math> can be written as <math>(s-i)h</math>. \nSo the Newton polynomial becomes\n\n:<math>\\begin{align}\nN(x) &= [y_0] + [y_0,y_1]sh + \\cdots + [y_0,\\ldots,y_k] s (s-1) \\cdots (s-k+1){h}^{k} \\\\\n&= \\sum_{i=0}^{k}s(s-1) \\cdots (s-i+1){h}^{i}[y_0,\\ldots,y_i] \\\\\n&= \\sum_{i=0}^{k}{s \\choose i}i!{h}^{i}[y_0,\\ldots,y_i].\n\\end{align}</math>\n\nThis is called the '''Newton forward divided difference formula'''{{citation needed|date=October 2017}}.\n\n===Newton backward divided difference formula===\nIf the nodes are reordered as <math>{x}_{k},{x}_{k-1},\\dots,{x}_{0}</math>, the Newton polynomial becomes\n\n:<math>N(x)=[y_k]+[{y}_{k}, {y}_{k-1}](x-{x}_{k})+\\cdots+[{y}_{k},\\ldots,{y}_{0}](x-{x}_{k})(x-{x}_{k-1})\\cdots(x-{x}_{1}).</math>\n\nIf <math>{x}_{k},\\;{x}_{k-1},\\;\\dots,\\;{x}_{0}</math> are equally spaced with <math>x={x}_{k}+sh</math> and <math>{x}_{i}={x}_{k}-(k-i)h</math> for ''i'' = 0, 1, ..., ''k'', then,\n\n:<math>\\begin{align}\nN(x) &= [{y}_{k}]+ [{y}_{k}, {y}_{k-1}]sh+\\cdots+[{y}_{k},\\ldots,{y}_{0}]s(s+1)\\cdots(s+k-1){h}^{k} \\\\\n&=\\sum_{i=0}^{k}{(-1)}^{i}{-s \\choose i}i!{h}^{i}[{y}_{k},\\ldots,{y}_{k-i}].\n\\end{align}</math>\n\nis called the '''Newton backward divided difference formula''' {{citation needed|date=October 2017}}.\n\n==Significance==\nNewton's formula is of interest because it is the straightforward and natural differences-version of Taylor's polynomial. Taylor's polynomial tells where a function will go, based on its ''y'' value, and its derivatives (its rate of change, and the rate of change of its rate of change, etc.) at one particular ''x'' value. Newton's formula is Taylor's polynomial based on finite differences instead of instantaneous rates of change.\n\n==Addition of new points==\nAs with other difference formulas, the degree of a Newton interpolating polynomial can be increased by adding more terms and points without discarding existing ones. Newton's form has the simplicity that the new points are always added at one end: Newton's forward formula can add new points to the right, and Newton's backward formula can add new points to the left.\n\nThe accuracy of polynomial interpolation depends on how close the interpolated point is to the middle of the ''x'' values of the set of points used. Obviously, as new points are added at one end, that middle becomes farther and farther from the first data point. Therefore, if it isn't known how many points will be needed for the desired accuracy, the middle of the x-values might be far from where the interpolation is done.\n\nGauss, Stirling, and Bessel all developed formulae to remedy that problem.<ref>[http://alvand.basu.ac.ir/~dezfoulian/files/Numericals/Numerical.Methods.For.Scientists.And.Engineers_2ed_Hamming_0486652416.pdf Numerical Methods for Scientists and Engineers, R.W. Hamming]</ref>\n\nGauss's formula alternately adds new points at the left and right ends, thereby keeping the set of points centered near the same place (near the evaluated point). When so doing, it uses terms from Newton's formula, with data points and ''x'' values renamed in keeping with one's choice of what data point is designated as the ''x''<sub>0</sub> data point.\n\nStirling's formula remains centered about a particular data point, for use when the evaluated point is nearer to a data point than to a middle of two data points.\n\nBessel's formula remains centered about a particular middle between two data points, for use when the evaluated point is nearer to a middle than to a data point.\n\nBessel and Stirling achieve that by sometimes using the average of two differences, and sometimes using the average of two products of binomials in ''x'', where Newton's or Gauss's would use just one difference or product. Stirling's uses an average difference in odd-degree terms (whose difference uses an even number of data points); Bessel's uses an average difference in even-degree terms (whose difference uses an odd number of data points).\n\n==Strengths and weaknesses of various formulae==\nFor any given finite set of data points, there is only one polynomial of least possible degree that passes through all of them. Thus, it is appropriate to speak of the \"Newton form\", or [[Lagrange polynomial|Lagrange form]], etc., of the interpolation polynomial.  However, the way the polynomial is obtained matters.  There are several similar methods, such as those of Gauss, Bessel and Stirling. (They can be derived from Newton's by renaming the ''x''-values of the data points, but in practice they are important in their own right.)\n\n===Gauss vs. Bessel & Stirling===\nBessel and Stirling require a bit more work than Gauss does. The desirability of using Bessel or Stirling depends on whether or not their small improvement in accuracy is needed.\n\n===Bessel vs. Stirling===\nThe choice between Bessel and Stirling depends on whether the interpolated point is closer to a data point, or closer to a middle between two data points.\n\nBut it should be pointed out that a polynomial interpolation's error approaches zero, as the interpolation point approaches a data-point. Therefore, Stirling's formula brings its accuracy improvement where it's least needed and Bessel brings its accuracy improvement where it's most needed.\n\nSo, Bessel's formula could be said to be the most consistently accurate difference formula, and, in general, the most consistently accurate of the familiar polynomial interpolation formulas.\n\n===Divided-Difference Methods vs. Lagrange===\nLagrange is sometimes said to require less work, and is sometimes recommended for problems in which it's known, in advance, from previous experience, how many terms are needed for sufficient accuracy.\n\nThe divided difference method have the advantage that more data points can be added, for improved accuracy, without re-doing the whole problem. The terms based on the previous data points can continue to be used. With the ordinary Lagrange formula, to do the problem with more data points would require re-doing the whole problem.\n\nThere is a \"barycentric\" version of Lagrange that avoids the need to re-do the entire calculation when adding a new data point. But it requires that the values of each term be recorded.\n\nBut the ability, of Gauss, Bessel and Stirling, to keep the data points centered close to the interpolated point gives them an advantage over Lagrange, when it isn't known, in advance, how many data points will be needed.\n\nAdditionally, suppose that one wants to find out if, for some particular type of problem, linear interpolation is sufficiently accurate. That can be determined by evaluating the quadratic term of a divided difference formula. If the quadratic term is negligible—meaning that the linear term is sufficiently accurate without adding the quadratic term—then linear interpolation is sufficiently accurate. (If the problem is sufficiently important, or if the quadratic term is nearly big enough to matter, then one might want to determine whether the _sum_ of the quadratic and cubic terms is large enough to matter in the problem.)\n\nOf course only a divided-difference method can be used for such a determination.\n\nFor that purpose, the divided-difference formula and/or its ''x''<sub>0</sub> point should be chosen so that the formula will use, for its linear term, the two data points between which the linear interpolation of interest would be done.\n\nThe divided difference formulas are more versatile, useful in more kinds of problems.\n\nThe Lagrange formula is at its best when all the interpolation will be done at one ''x'' value, with only the data points' ''y'' values varying from one problem to another, and when it's known, from past experience, how many terms are needed for sufficient accuracy.\n\nWith the Newton form of the interpolating polynomial a compact and effective algorithm exists for combining the terms to find the coefficients of the polynomial.<ref>{{cite web|last1=Stetekluh|first1=Jeff|title=Algorithm for the Newton Form of the Interpolating Polynomial|url=http://stetekluh.com/NewtonPoly.html}}</ref>\n\n===Accuracy===\n\nWhen, with Stirling's or Bessel's, the last term used includes the average of two differences, then one more point is being used than Newton's or other polynomial interpolations would use for the same polynomial degree. So, in that instance, Stirling's or Bessel's is not putting an ''N''−1 degree polynomial through ''N'' points, but is, instead, trading equivalence with Newton's for better centering and accuracy, giving those methods sometimes potentially greater accuracy, for a given polynomial degree, than other polynomial interpolations.\n\n==General case==\nFor the special case of ''x<sub>i</sub>'' = ''i'', there is a closely related set of polynomials, also called the Newton polynomials, that are simply the [[binomial coefficient]]s for general argument. That is, one also has the Newton polynomials <math>p_n(z)</math> given by\n\n:<math>p_n(z)={z \\choose n}= \\frac{z(z-1)\\cdots(z-n+1)}{n!}</math>\n\nIn this form, the Newton polynomials generate the [[Newton series]]. These are in turn a special case of the general [[difference polynomials]] which allow the representation of [[analytic function]]s through generalized difference equations.\n\n==Main idea==\nSolving an interpolation problem leads to a problem in linear algebra where we have to solve a system of linear equations. Using a standard [[monomial basis]] for our interpolation polynomial we get the very complicated [[Vandermonde matrix]]. By choosing another basis, the Newton basis, we get a system of linear equations with a much simpler [[lower triangular matrix]] which can be solved faster.\n\nFor ''k''&nbsp;+&nbsp;1 data points we construct the Newton basis as\n\n:<math>n_j(x) := \\prod_{i=0}^{j-1} (x - x_i) \\qquad j=1,\\ldots,k.</math>\n\nUsing these polynomials as a basis for <math>\\Pi_k</math> we have to solve\n\n:<math>\\begin{bmatrix}\n      1 &         & \\ldots &        & 0  \\\\\n      1 & x_1-x_0 &        &        &    \\\\\n      1 & x_2-x_0 & (x_2-x_0)(x_2-x_1) &        & \\vdots   \\\\\n \\vdots & \\vdots  &        & \\ddots &    \\\\\n      1 & x_k-x_0 & \\ldots & \\ldots & \\prod_{j=0}^{k-1}(x_k - x_j)\n\\end{bmatrix}\n\\begin{bmatrix}     a_0 \\\\     \\\\     \\vdots \\\\     \\\\     a_{k} \\end{bmatrix} =\n\\begin{bmatrix}      y_0 \\\\  \\\\  \\vdots \\\\ \\\\    y_{k} \\end{bmatrix}</math>\n\nto solve the polynomial interpolation problem.\n\nThis system of equations can be solved iteratively by solving\n\n:<math> \\sum_{i=0}^{j} a_{i} n_{i}(x_j) = y_j \\qquad j = 0,\\dots,k.</math>\n\n==Taylor polynomial==\n\nThe limit of the Newton polynomial if all nodes coincide is a [[Taylor polynomial]], because the divided differences become derivatives.\n:<math>\\lim_{(x_0,\\dots,x_n)\\to(z,\\dots,z)} f[x_0] + f[x_0,x_1]\\cdot(\\xi-x_0) + \\dots + f[x_0,\\dots,x_n]\\cdot(\\xi-x_0)\\cdot\\dots\\cdot(\\xi-x_{n-1}) = </math>\n:::<math>=  f(z) + f'(z)\\cdot(\\xi-z) + \\dots + \\frac{f^{(n)}(z)}{n!}\\cdot(\\xi-z)^n</math>\n\n==Application==\nAs can be seen from the definition of the divided differences new data points can be added to the data set to create a new interpolation polynomial without recalculating the old coefficients. And when a data point changes we usually do not have to recalculate all coefficients. Furthermore, if the ''x''<sub>''i''</sub> are distributed equidistantly the calculation of the divided differences becomes significantly easier. Therefore, the divided-difference formulas are usually preferred over the [[Lagrange polynomial|Lagrange form]] for practical purposes.\n\n===Examples===\nThe divided differences can be written in the form of a table. For example, for a function ''f'' is to be interpolated on points <math>x_0, \\ldots, x_n</math>. Write\n\n:<math>\\begin{matrix}\n   x_0 & f(x_0) &                                 & \\\\\n       &        & {f(x_1)-f(x_0)\\over x_1 - x_0}  & \\\\\n   x_1 & f(x_1) &                                 & {{f(x_2)-f(x_1)\\over x_2 - x_1}-{f(x_1)-f(x_0)\\over x_1 - x_0} \\over x_2 - x_0} \\\\\n       &        & {f(x_2)-f(x_1)\\over x_2 - x_1}  & \\\\\n   x_2 & f(x_2) &                                 & \\vdots \\\\\n       &        & \\vdots                          & \\\\\n\\vdots &        &                                 & \\vdots \\\\\n       &        & \\vdots                          & \\\\\n   x_n & f(x_n) &                                 & \\\\\n\\end{matrix}</math>\nThen the interpolating polynomial is formed as above using the topmost entries in each column as coefficients.\n\nFor example, suppose we are to construct the interpolating polynomial to ''f''(''x'') = tan(''x'') using divided differences, at the points\n{| cellpadding=10px\n|-\n| <math>x_0=-\\tfrac{3}{2}</math>|| <math>x_1=-\\tfrac{3}{4}</math>        || <math>x_2=0</math>    || <math>x_3=\\tfrac{3}{4}</math>        || <math>x_4=\\tfrac{3}{2}</math>\n|-\n| <math>f(x_0)=-14.1014</math> || <math>f(x_1)=-0.931596</math> || <math>f(x_2)=0</math> || <math>f(x_3)=0.931596</math> || <math>f(x_4)=14.1014</math>\n|}\n\nUsing six digits of accuracy, we construct the table\n: <math>\\begin{matrix}\n-\\tfrac{3}{2} & -14.1014  &         &          &          &\\\\\n      &           & 17.5597 &          &          &\\\\\n-\\tfrac{3}{4} & -0.931596 &         & -10.8784 &          &\\\\\n      &           & 1.24213 &          & 4.83484  &  \\\\\n0     & 0       &               & 0        &          & 0\\\\\n      &           & 1.24213 &          & 4.83484  &\\\\\n\\tfrac{3}{4}  & 0.931596  &         & 10.8784  &          &\\\\\n      &          & 17.5597 &          &          &\\\\\n\\tfrac{3}{2} & 14.1014   &         &          &          &\\\\\n\\end{matrix}</math>\nThus, the interpolating polynomial is\n:<math>-14.1014+17.5597(x+\\tfrac{3}{2})-10.8784(x+\\tfrac{3}{2})(x+\\tfrac{3}{4}) +4.83484(x+\\tfrac{3}{2})(x+\\tfrac{3}{4})(x)+0(x+\\tfrac{3}{2})(x+\\tfrac{3}{4})(x)(x-\\tfrac{3}{4}) =</math>\n:::<math>=-0.00005-1.4775x-0.00001x^2+4.83484x^3</math>\nGiven more digits of accuracy in the table, the first and third coefficients will be found to be zero.\n\nAnother example:\n\nThe sequence <math>f_0</math> such that <math>f_0(1) = 6, f_0(2) = 9, f_0(3) = 2</math> and <math>f_0(4) = 5</math>, i.e., they are <math>6, 9, 2, 5</math> from <math>x_0 = 1</math> to <math>x_3 = 4</math>.\n\nYou obtain the slope of order <math>1</math> in the following way:\n* <math>f_1(x_0, x_1) = \\frac{f_0(x_1) - f_0(x_0)}{x_1 - x_0} = \\frac{9 - 6}{2 - 1} = 3</math>\n* <math>f_1(x_1, x_2) = \\frac{f_0(x_2) - f_0(x_1)}{x_2 - x_1} = \\frac{2 - 9}{3 - 2} = -7</math>\n* <math>f_1(x_2, x_3) = \\frac{f_0(x_3) - f_0(x_2)}{x_3 - x_2} = \\frac{5 - 2}{4 - 3} = 3</math>\n\nAs we have the slopes of order <math>1</math>, it's possible to obtain the next order:\n* <math>f_2(x_0, x_1, x_2) = \\frac{f_1(x_1, x_2) - f_1(x_0, x_1)}{x_2 - x_0} = \\frac{-7 - 3}{3 - 1} = -5</math>\n* <math>f_2(x_1, x_2, x_3) = \\frac{f_1(x_2, x_3) - f_1(x_1, x_2)}{x_3 - x_1} = \\frac{3 - (-7)}{4 - 2} = 5</math>\n\nFinally, we define the slope of order <math>3</math>:\n* <math>f_3(x_0, x_1, x_2, x_3) = \\frac{f_2(x_1, x_2, x_3) - f_2(x_0, x_1, x_2)}{x_3 - x_0} = \\frac{5 - (-5)}{4 - 1} = \\frac{10}{3}</math>\n\nOnce we have the slope, we can define the consequent polynomials:\n* <math>p_0(x) = 6</math>.\n* <math>p_1(x) = 6 + 3(x - 1)</math>\n* <math>p_2(x) = 6 + 3(x - 1) - 5(x - 1)(x - 2)</math>.\n* <math>p_3(x) = 6 + 3(x - 1) - 5(x - 1)(x - 2) + \\frac{10}{3} (x - 1)(x - 2)(x - 3)</math>\n\n==See also==\n*[[Newton series]]\n*[[Neville's schema]]\n*[[Polynomial interpolation]]\n*[[Lagrange polynomial|Lagrange form]] of the interpolation polynomial\n*[[Bernstein polynomial|Bernstein form]] of the interpolation polynomial\n*[[Hermite interpolation]]\n*[[Carlson's theorem]]\n*[[Table of Newtonian series]]\n\n==References==\n{{reflist}}\n\n==External links==\n*[https://web.archive.org/web/20120213001949/http://math.fullerton.edu/mathews/n2003/NewtonPolyMod.html Module for the Newton Polynomial by John H. Mathews]\n\n{{Isaac Newton}}\n\n[[Category:Interpolation]]\n[[Category:Finite differences]]\n[[Category:Factorial and binomial topics]]\n[[Category:Polynomials]]"
    },
    {
      "title": "Nørlund–Rice integral",
      "url": "https://en.wikipedia.org/wiki/N%C3%B8rlund%E2%80%93Rice_integral",
      "text": "In [[mathematics]], the '''Nørlund–Rice integral''', sometimes called '''Rice's method''', relates the ''n''th [[forward difference]] of a function to a [[line integral]] on the [[complex plane]].  As such, it commonly appears in the theory of [[finite differences]], and also has been applied in [[computer science]] and [[graph theory]] to estimate [[binary tree]] lengths. It is named in honour of [[Niels Erik Nørlund]] and [[Stephen O. Rice]]. Nørlund's contribution was to define the integral; Rice's contribution was to demonstrate its utility by applying [[Method of steepest descent|saddle-point technique]]s to its evaluation.\n\n==Definition==\nThe ''n''th [[forward difference]] of a function ''f''(''x'') is given by\n\n:<math>\\Delta^n[f](x)= \\sum_{k=0}^n {n \\choose k} (-1)^{n-k} f(x+k)</math>\n\nwhere <math>{n \\choose k}</math> is the [[binomial coefficient]].\n\nThe Nörlund–Rice integral is given by\n\n:<math>\\sum_{k=\\alpha}^n {n \\choose k} (-1)^{n-k} f(k) = \n\\frac{n!}{2\\pi i}\n\\oint_\\gamma \\frac{f(z)}{z(z-1)(z-2)\\cdots(z-n)}\\, dz</math>\n\nwhere ''f'' is understood to be [[meromorphic]], α is an integer, <math>0\\leq \\alpha \\leq n</math>, and the contour of integration is understood to circle the [[pole (complex analysis)|poles]] located at the integers α, ..., ''n'', but encircles neither integers 0, ..., <math>\\alpha-1</math> nor any of the poles of ''f''.  The integral may also be written as\n\n:<math>\\sum_{k=\\alpha}^n {n \\choose k} (-1)^{k} f(k) = \n-\\frac{1}{2\\pi i}\n\\oint_\\gamma B(n+1, -z) f(z)\\, dz</math>\n\nwhere ''B''(''a'',''b'') is the Euler [[beta function]].  If the function <math>f(z)</math> is [[polynomially bounded]] on the right hand side of the complex plane, then the contour may be extended to infinity on the right hand side, allowing the transform to be written as\n\n:<math>\\sum_{k=\\alpha}^n {n \\choose k} (-1)^{n-k} f(k) = \n\\frac{-n!}{2\\pi i}\n\\int_{c-i\\infty}^{c+i\\infty} \\frac{f(z)}{z(z-1)(z-2)\\cdots(z-n)}\\, dz</math>\n\nwhere the constant ''c'' is to the left of α.\n\n==Poisson–Mellin–Newton cycle==\nThe Poisson–Mellin–Newton cycle, noted by Flajolet et al. in 1985, is the observation that the resemblance of the Nørlund–Rice integral to the [[Mellin transform]] is not accidental, but is related by means of the [[binomial transform]] and the [[Newton series]].  In this cycle, let <math>\\{f_n\\}</math> be a [[sequence]], and let ''g''(''t'') be the corresponding [[Poisson generating function]], that is, let\n\n:<math>g(t) = e^{-t} \\sum_{n=0}^\\infty f_n t^n.</math>\n\nTaking its Mellin transform\n\n:<math>\\phi(s)=\\int_0^\\infty g(t) t^{s-1}\\, dt,</math>\n\none can then regain the original sequence by means of the Nörlund–Rice integral:\n\n:<math>f_n = \\frac{(-1)^n }{2\\pi i}\n\\int_\\gamma\n\\frac {\\phi(s)}{\\Gamma(-s)} \\frac{n!}{s(s-1)\\cdots (s-n)}\\, ds</math>\n\nwhere Γ is the [[gamma function]].\n\n==Riesz mean==\nA closely related integral frequently occurs in the discussion of [[Riesz mean]]s. Very roughly, it can be said to be related to the Nörlund–Rice integral in the same way that [[Perron's formula]] is related to the Mellin transform: rather than dealing with infinite series, it deals with finite series.\n\n==Utility==\nThe integral representation for these types of series is interesting because the integral can often be evaluated using [[asymptotic expansion]] or [[Method of steepest descent|saddle-point]] techniques; by contrast, the forward difference series can be extremely hard to evaluate numerically, because the binomial coefficients grow rapidly for large ''n''.\n\n==See also==\n* [[Table of Newtonian series]]\n* [[List of factorial and binomial topics]]\n\n==References==\n* Niels Erik Nørlund, ''Vorlesungen uber Differenzenrechnung'', (1954) Chelsea Publishing Company, New York.\n* Donald E. Knuth, ''[[The Art of Computer Programming]]'', (1973), Vol. 3 Addison-Wesley.\n* Philippe Flajolet and Robert Sedgewick, \"[https://www.sciencedirect.com/science/article/pii/030439759400281M Mellin transforms and asymptotics: Finite differences and Rice's integrals]\",  ''Theoretical Computer Science'' '''144''' (1995) pp 101–124.\n* Peter Kirschenhofer, \"[http://www.combinatorics.org/ojs/index.php/eljc/article/view/v3i2r7/pdf]\", ''[http://www.combinatorics.org The Electronic Journal of Combinatorics]'', Volume '''3''' (1996) Issue 2 Article 7.\n\n{{DEFAULTSORT:Norlund-Rice integral}}\n[[Category:Factorial and binomial topics]]\n[[Category:Complex analysis]]\n[[Category:Integral transforms]]\n[[Category:Finite differences]]"
    },
    {
      "title": "Reciprocal difference",
      "url": "https://en.wikipedia.org/wiki/Reciprocal_difference",
      "text": "In [[mathematics]], the '''reciprocal difference''' of a [[finite sequence]] of numbers <math>(x_0, x_1, ..., x_n)</math> on a function <math>f(x)</math> is defined inductively by the following formulas:\n\n:<math>\\rho_1(x_0, x_1) = \\frac{x_0 - x_1}{f(x_0) - f(x_1)}</math>\n\n:<math>\\rho_2(x_0, x_1, x_2) = \\frac{x_0 - x_2}{\\rho_1(x_0, x_1) - \\rho_1(x_1, x_2)} + f(x_1)</math>\n\n:<math>\\rho_n(x_0,x_1,\\ldots,x_n)=\\frac{x_0-x_n}{\\rho_{n-1}(x_0,x_1,\\ldots,x_{n-1})-\\rho_{n-1}(x_1,x_2,\\ldots,x_n)}+\\rho_{n-2}(x_1,\\ldots,x_{n-1})</math>\n\n==See also==\n*[[Divided differences]]\n\n==References==\n\n*{{MathWorld|title=Reciprocal Difference|urlname=ReciprocalDifference}}\n*{{citebook|last=Abramowitz|first=Milton|author2=Irene A. Stegun |title=[[Abramowitz and Stegun|Handbook of Mathematical Functions]]|origyear=1964|year=1972|publisher=Dover|edition=ninth Dover printing, tenth GPO printing|isbn=0-486-61272-4|page=878}}\n\n[[Category:Finite differences]]"
    },
    {
      "title": "Table of Newtonian series",
      "url": "https://en.wikipedia.org/wiki/Table_of_Newtonian_series",
      "text": "In [[mathematics]], a [[Newtonian series]], named after [[Isaac Newton]], is a sum over a [[sequence]] <math>a_n</math> written in the form\n\n:<math>f(s) = \\sum_{n=0}^\\infty (-1)^n {s\\choose n} a_n = \\sum_{n=0}^\\infty \\frac{(-s)_n}{n!} a_n</math>\n\nwhere \n\n:<math>{s \\choose n}</math>\n\nis the [[binomial coefficient]] and <math>(s)_n</math> is the [[rising factorial]].  Newtonian series often appear in relations of the form seen in [[umbral calculus]].\n\n==List==\n\nThe generalized [[binomial theorem]] gives\n\n:<math> (1+z)^s = \\sum_{n = 0}^{\\infty}{s \\choose n}z^n = 1+{s \\choose 1}z+{s \\choose 2}z^2+\\cdots.</math>\n\nA proof for this identity can be obtained by showing that it satisfies the differential equation\n\n: <math> (1+z) \\frac{d(1+z)^s}{dz} = s (1+z)^s.</math>\n\nThe [[digamma function]]: \n\n:<math>\\psi(s+1)=-\\gamma-\\sum_{n=1}^\\infty \\frac{(-1)^n}{n} {s \\choose n}.</math>\n\nThe [[Stirling numbers of the second kind]] are given by the finite sum\n\n:<math>\\left\\{\\begin{matrix} n \\\\ k \\end{matrix}\\right\\}\n=\\frac{1}{k!}\\sum_{j=0}^{k}(-1)^{k-j}{k \\choose j} j^n.</math>\n\nThis formula is a special case of the ''k''th [[forward difference]] of the [[monomial]] ''x''<sup>''n''</sup> evaluated at&nbsp;''x''&nbsp;=&nbsp;0:\n\n:<math> \\Delta^k x^n = \\sum_{j=0}^{k}(-1)^{k-j}{k \\choose j} (x+j)^n.</math>\n\nA related identity forms the basis of the [[Nörlund–Rice integral]]:\n\n:<math>\\sum_{k=0}^n {n \\choose k}\\frac {(-1)^{n-k}}{s-k} = \n\\frac{n!}{s(s-1)(s-2)\\cdots(s-n)} = \n\\frac{\\Gamma(n+1)\\Gamma(s-n)}{\\Gamma(s+1)}= \nB(n+1,s-n),s \\notin \\{0,\\ldots,n\\}</math>\n\nwhere <math>\\Gamma(x)</math> is the [[Gamma function]] and <math>B(x,y)</math> is the [[Beta function]].\n\nThe [[trigonometric function]]s have [[umbral calculus|umbral]] identities:\n\n:<math>\\sum_{n=0}^\\infty (-1)^n {s \\choose 2n} = 2^{s/2} \\cos \\frac{\\pi s}{4}</math>\n\nand \n:<math>\\sum_{n=0}^\\infty (-1)^n {s \\choose 2n+1} = 2^{s/2} \\sin \\frac{\\pi s}{4}</math>\n\nThe umbral nature of these identities is a bit more clear by writing them in terms of the [[falling factorial]] <math>(s)_n</math>. The first few terms of the sin series are\n\n:<math>s - \\frac{(s)_3}{3!} +  \\frac{(s)_5}{5!} - \\frac{(s)_7}{7!} + \\cdots</math>\n\nwhich can be recognized as resembling the [[Taylor series]] for sin&nbsp;''x'', with (''s'')<sub>''n''</sub> standing in the place of&nbsp;''x''<sup>''n''</sup>.\n\nIn [[analytic number theory]] it is of interest to sum\n:<math>\\!\\sum_{k=0}B_k z^k,</math>\nwhere ''B'' are the [[Bernoulli numbers]]. Employing the generating function its Borel sum can be evaluated as \n:<math>\\sum_{k=0}B_k z^k= \\int_0^\\infty e^{-t} \\frac{t z}{e^{t z}-1}d t= \\sum_{k=1}\\frac z{(k z+1)^2}.</math>\nThe general relation gives the Newton series\n:<math>\\sum_{k=0}\\frac{B_k(x)}{z^k}\\frac{{1-s\\choose k}}{s-1}= z^{s-1}\\zeta(s,x+z),</math>{{Citation needed|date=February 2012}}\nwhere <math>\\zeta</math> is the [[Hurwitz zeta function]] and <math>B_k(x)</math> the [[Bernoulli polynomials|Bernoulli polynomial]]. The series does not converge, the identity holds formally.\n\nAnother identity is \n<math>\\frac 1{\\Gamma(x)}= \\sum_{k=0}^\\infty {x-a\\choose k}\\sum_{j=0}^k \\frac{(-1)^{k-j}}{\\Gamma(a+j)}{k\\choose j},</math>\nwhich converges for <math>x>a</math>.  This follows from the general form of a Newton series for equidistant nodes (when it exists, i.e. is convergent)\n:<math>f(x)=\\sum_{k=0}{\\frac{x-a}h \\choose k} \\sum_{j=0}^k (-1)^{k-j}{k\\choose j}f(a+j h).</math>\n\n==See also==\n* [[Binomial transform]]\n* [[List of factorial and binomial topics]]\n* [[Nörlund–Rice integral]]\n* [[Carlson's theorem]]\n\n==References==\n* Philippe Flajolet and Robert Sedgewick, \"[http://www-rocq.inria.fr/algo/flajolet/Publications/mellin-rice.ps.gz Mellin transforms and asymptotics: Finite differences and Rice's integrals]{{Dead link|date=June 2018 |bot=InternetArchiveBot |fix-attempted=no }}\",  ''Theoretical Computer Science'' ''144'' (1995) pp 101–124.\n\n{{Isaac Newton}}\n\n[[Category:Finite differences]]\n[[Category:Factorial and binomial topics]]\n[[Category:Mathematics-related lists|Newton series]]"
    },
    {
      "title": "Thiele's interpolation formula",
      "url": "https://en.wikipedia.org/wiki/Thiele%27s_interpolation_formula",
      "text": "In [[mathematics]], '''Thiele's interpolation formula''' is a formula that defines a [[rational function]] <math>f(x)</math> from a [[finite set]] of inputs <math>x_i</math> and their function values <math>f(x_i)</math>. The problem of generating a function whose graph passes through a given set of function values is called [[interpolation]]. This interpolation formula is named after the [[Danish people|Danish]] mathematician [[Thorvald N. Thiele]]. It is expressed as a [[continued fraction]], where ρ represents the [[reciprocal difference]]:\n\n:<math> f(x) = f(x_1) + \\cfrac{x-x_1}{\\rho(x_1,x_2) + \\cfrac{x-x_2}{\\rho_2(x_1,x_2,x_3) - f(x_1) + \\cfrac{x-x_3}{\\rho_3(x_1,x_2,x_3,x_4) - \\rho(x_1,x_2) + \\cdots}}} </math>\n\n==References==\n* {{mathworld|urlname=ThielesInterpolationFormula|title=Thiele's Interpolation Formula}}\n\n[[Category:Finite differences]]\n[[Category:Articles with example ALGOL 68 code]]\n[[Category:Interpolation]]\n\n\n{{mathanalysis-stub}}"
    },
    {
      "title": "Umbral calculus",
      "url": "https://en.wikipedia.org/wiki/Umbral_calculus",
      "text": "{{Use American English|date = March 2019}}\n{{Short description|Historical term in mathematics}}\nIn [[mathematics]] before the 1970s, the term '''''umbral calculus''''' referred to the surprising similarity between seemingly unrelated [[polynomial equation]]s and certain shadowy techniques used to 'prove' them. These techniques were introduced by {{harvs|txt|authorlink=John Blissard|first=John|last=Blissard|year=1861}} and are sometimes called '''Blissard's symbolic method'''. They are often attributed to [[Édouard Lucas]] (or [[James Joseph Sylvester]]), who used the technique extensively.<ref>E. T. Bell, \"The History of Blissard's Symbolic Method, with a Sketch of its Inventor's Life\", ''The American Mathematical Monthly'' '''45''':7 (1938), pp. 414–421.</ref>\n\n== Short History ==\nIn the 1930s and 1940s, [[Eric Temple Bell]] attempted to set the umbral calculus on a rigorous footing.\n\nIn the 1970s, [[Steven Roman]], [[Gian-Carlo Rota]], and others developed the umbral calculus by means of [[linear functional]]s on spaces of polynomials. Currently, ''umbral calculus'' refers to the study of [[Sheffer sequence]]s, including polynomial sequences of [[binomial type]] and [[Appell sequence]]s, but may encompass systematic correspondence techniques of the [[calculus of finite differences]].\n\n==The 19th-century umbral calculus==\nThe method is a notational procedure used for deriving identities involving indexed sequences of numbers by ''pretending that the indices are exponents''.  Construed literally, it is absurd, and yet it is successful: identities derived via the umbral calculus can also be properly derived by more complicated methods that can be taken literally without logical difficulty.\n\nAn example involves the [[Bernoulli polynomials]].  Consider, for example, the ordinary [[binomial expansion]] (which contains a [[binomial coefficient]]):\n\n:<math>(y+x)^n=\\sum_{k=0}^n{n\\choose k}y^{n-k} x^k</math>\n\nand the remarkably similar-looking relation on the [[Bernoulli polynomials]]:\n\n:<math>B_n(y+x)=\\sum_{k=0}^n{n\\choose k}B_{n-k}(y) x^k.</math>\n\nCompare also the ordinary derivative\n\n:<math> \\frac{d}{dx} x^n = nx^{n-1} </math>\n\nto a very similar-looking relation on the Bernoulli polynomials: \n  \n:<math> \\frac{d}{dx} B_n(x) = nB_{n-1}(x).</math>\n\nThese similarities allow one to construct ''umbral'' proofs, which, on the surface, cannot be correct, but seem to work anyway.  Thus, for example,  by pretending that the subscript ''n''&nbsp;&minus;&nbsp;''k'' is an exponent:\n\n:<math>B_n(x)=\\sum_{k=0}^n {n\\choose k}b^{n-k}x^k=(b+x)^n,</math>\n\nand then differentiating, one gets the desired result:\n\n:<math>B_n'(x)=n(b+x)^{n-1}=nB_{n-1}(x).</math>\n\nIn the above, the variable ''b'' is an \"umbra\" ([[Latin]] for ''shadow'').\n\nSee also [[Faulhaber's formula]].\n\n==Umbral Taylor series==\nSimilar relationships were also observed in the theory of [[finite differences]]. The umbral version of the [[Taylor series]] is given by a similar expression involving the ''k''-th [[forward difference]]s <math>\\Delta^k [f]</math> of a [[polynomial]] function ''f'',\n\n:<math>f(x)=\\sum_{k=0}^\\infty\\frac{\\Delta^k [f](0)}{k!}(x)_k</math>\n\nwhere\n\n:<math>(x)_k=x(x-1)(x-2)\\cdots(x-k+1)</math>\n\nis the [[Pochhammer symbol]] used here for the falling sequential product.  A similar relationship holds for the backward differences and rising factorial.\n\nThis series is also known as the [[Finite difference#Newton's_series|''Newton series'']] or '''Newton's forward difference expansion'''.\nThe analogy to Taylor's expansion is utilized in the [[calculus of finite differences]].\n\n==Bell and Riordan==\n\nIn the 1930s and 1940s, [[Eric Temple Bell]] tried unsuccessfully to make this kind of argument logically rigorous. The [[combinatorics|combinatorialist]] [[John Riordan (mathematician)|John Riordan]] in his book ''Combinatorial Identities'' published in the 1960s, used techniques of this sort extensively.\n\n==The modern umbral calculus==\n\nAnother combinatorialist, [[Gian-Carlo Rota]], pointed out that the mystery vanishes if one considers the [[linear functional]] ''L'' on polynomials in ''z'' defined by\n\n:<math>L(z^n)= B_n(0)= B_n.</math>\n\nThen, using the definition of the Bernoulli polynomials and the definition and linearity of ''L'', one can write\n\n:<math>\\begin{align}\nB_n(x) &= \\sum_{k=0}^n{n\\choose k}B_{n-k}x^k \\\\\n       &= \\sum_{k=0}^n{n\\choose k}L\\left(z^{n-k}\\right)x^k \\\\\n       &= L\\left(\\sum_{k=0}^n{n\\choose k}z^{n-k}x^k\\right) \\\\\n       &= L\\left((z+x)^n\\right)\n\\end{align}</math>\n\nThis enables one to replace occurrences of <math>B_n(x)</math> by <math>L((z+x)^n)</math>, that is, move the ''n'' from a subscript to a superscript (the key operation of umbral calculus). For instance, we can now prove that:\n\n:<math>\\begin{align}\nB_n(y+x) &= \\sum_{k=0}^n{n\\choose k}B_{n-k}(y) x^k \\\\\n&= \\sum_{k=0}^n{n\\choose k}L\\left((z+y)^{n-k}\\right) x^k \\\\\n&= L\\left(\\sum_{k=0}^n {n\\choose k} (z+y)^{n-k} x^k \\right) \\\\\n&= L\\left((z+x+y)^n\\right) \\\\\n&= B_n(x+y).\n\\end{align}</math>\n\nRota later stated that much confusion resulted from the failure to distinguish between three [[equivalence relation]]s that occur frequently in this topic, all of which were denoted by \"=\". <!-- Details need to be added here. -->\n\nIn a paper published in 1964, Rota used umbral methods to establish the [[recursion]] formula satisfied by the [[Bell numbers]], which enumerate [[partition of a set|partitions]] of finite sets.\n\nIn the paper of Roman and Rota cited below, the umbral calculus is characterized as the study of the '''umbral algebra''', defined as the [[algebra over a field|algebra]] of linear functionals on the [[vector space]] of polynomials in a variable ''x'', with a product ''L''<sub>1</sub>''L''<sub>2</sub> of linear functionals defined by\n\n:<math>\\left \\langle L_1 L_2 | x^n \\right \\rangle = \\sum_{k=0}^n {n \\choose k} \\left \\langle L_1 | x^k \\right \\rangle \\left \\langle L_2 | x^{n-k} \\right \\rangle.</math>\n\nWhen [[polynomial sequence]]s replace sequences of numbers as images of ''y<sup>n</sup>'' under the linear mapping ''L'', then the umbral method is seen to be an essential component of Rota's general theory of special polynomials, and that theory is the '''umbral calculus''' by some more modern definitions of the term.<ref>{{Cite journal | last1 = Rota | first1 = G. C. | last2 = Kahaner | first2 = D. | last3 = Odlyzko | first3 = A. | doi = 10.1016/0022-247X(73)90172-8 | title = On the foundations of combinatorial theory. VIII. Finite operator calculus | journal = Journal of Mathematical Analysis and Applications | volume = 42 | issue = 3 | pages = 684 | year = 1973 | pmid = | pmc = }}</ref> A small sample of that theory can be found in the article on [[binomial type|polynomial sequences of binomial type]].  Another is the article titled [[Sheffer sequence]].\n\nRota later applied umbral calculus extensively in his paper with Shen to study the various combinatorial properties of the [[cumulant]]s.<ref>G.-C. Rota and J. Shen, [http://www.sciencedirect.com/science/article/pii/S0097316599930170 \"On the Combinatorics of Cumulants\"], Journal of Combinatorial Theory, Series A, 91:283–304, 2000.</ref>\n\n==See also==\n*[[Binomial type#Umbral composition of polynomial sequences|Umbral composition of polynomial sequences]]\n*Calculus of [[finite difference]]s\n*[[Pidduck polynomials]]\n*[[Symbolic method]] in invariant theory\n\n==Notes==\n<references />\n\n==References==\n*{{Citation | authorlink=E. T. Bell | last1=Bell | first1=E. T. | title=The History of Blissard's Symbolic Method, with a Sketch of its Inventor's Life | jstor=2304144 | publisher=[[Mathematical Association of America]] | year=1938 | journal=[[American Mathematical Monthly|The American Mathematical Monthly]] | issn=0002-9890 | volume=45 | issue=7 | pages=414–421| doi=10.1080/00029890.1938.11990829 }}\n*{{Citation | last1=Blissard | first1=John | title=Theory of generic equations | url=http://resolver.sub.uni-goettingen.de/purl?PPN600494829_0004 | year=1861 | journal=The Quarterly Journal of Pure and Applied Mathematics | volume=4 | pages=279–305}}\n*{{Citation | last1=Roman | first1=Steven M. | last2=Rota | first2=Gian-Carlo | author2-link=Gian-Carlo Rota | title=The umbral calculus | doi=10.1016/0001-8708(78)90087-7 | mr=0485417 | year=1978 | journal=Advances in Mathematics | issn=0001-8708 | volume=27 | issue=2 | pages=95–188}}\n* G.-C. Rota, D. Kahaner, and [[Andrew Odlyzko|A. Odlyzko]], ''\"Finite Operator Calculus,\"'' Journal of Mathematical Analysis and its Applications, vol. 42, no. 3, June 1973.  Reprinted in the book with the same title, Academic Press, New York, 1975.\n*{{Citation | last1=Roman | first1=Steven | title=The umbral calculus | url=https://books.google.com/books?id=JpHjkhFLfpgC | publisher=Academic Press Inc. [Harcourt Brace Jovanovich Publishers] | location=London | series=Pure and Applied Mathematics | isbn=978-0-12-594380-2 | mr=741185  | year=1984 | volume=111}}. Reprinted by Dover, 2005.\n*{{eom|id=U/u095050|first=S. |last=Roman|title=Umbral calculus}}\n\n==External links==\n* {{MathWorld|urlname=UmbralCalculus|title=Umbral Calculus}}\n* {{cite journal |author=A. Di Bucchianico, D. Loeb |title=A Selected Survey of Umbral Calculus |journal=[[Electronic Journal of Combinatorics]] |series=Dynamic Surveys |volume=DS3 |year=2000 |url=http://www1.combinatorics.org/Surveys/ds3.pdf |deadurl=yes |archiveurl=https://web.archive.org/web/20120224193628/http://www.combinatorics.org/Surveys/ds3.pdf |archivedate=2012-02-24 |df= }}\n* Roman, S. (1982), [http://www.romanpress.com/MathArticles/TheoryI.pdf The Theory of the Umbral Calculus, I]\n\n{{DEFAULTSORT:Umbral Calculus}}\n[[Category:Combinatorics]]\n[[Category:Polynomials]]\n[[Category:Finite differences]]"
    },
    {
      "title": "Euler method",
      "url": "https://en.wikipedia.org/wiki/Euler_method",
      "text": "{{about||integrating with respect to the Euler characteristic|Euler calculus|Euler's method for factorizing an integer|Euler's factorization method}}\n\n[[Image:Euler method.svg|right|thumb|Illustration of the Euler method. The unknown curve is in blue, and its polygonal approximation is in red.]]\n\nEuler's method is a [[numerical method]] to solve first order first degree [[differential equation]] with a given initial value. It is the most basic [[explicit and implicit methods|explicit method]] for [[numerical ordinary differential equations|numerical integration of ordinary differential equations]] and is the simplest [[Runge–Kutta method]]. The Euler method is named after [[Leonhard Euler]], who treated it in his book ''[[Institutionum calculi integralis]]'' (published 1768–1870).<ref>{{harvnb|Butcher|2003|p=45}}; {{harvnb|Hairer|Nørsett|Wanner|1993|p=35}}</ref>\n\nThe Euler method is a first-order method, which means that the local error (error per step) is proportional to the square of the step size, and the global error (error at a given time) is proportional to the step size. \nThe Euler method often serves as the basis to construct more complex methods, e.g., [[predictor–corrector method]].\n\n==Informal geometrical description==\n\nConsider the problem of calculating the shape of an unknown curve which starts at a given point and satisfies a given differential equation. Here, a differential equation can be thought of as a formula by which the [[slope]] of the [[tangent line]] to the curve can be computed at any point on the curve, once the position of that point has been calculated.\n\nThe idea is that while the curve is initially unknown, its starting point, which we denote by <math>A_0,</math> is known (see the picture on top right). Then, from the differential equation, the slope to the curve at <math>A_0</math> can be computed, and so, the tangent line.\n\nTake a small step along that tangent line  up to a point <math>A_1.</math> Along this small step, the slope does not change too much, so <math>A_1</math> will be close to the curve. If we pretend that  <math>A_1</math> is still on the curve, the same reasoning as for the point <math>A_0</math> above can be used. After several steps, a [[polygonal curve]] <math>A_0A_1A_2A_3\\dots</math> is computed. In general, this curve does not diverge too far from the original unknown curve, and the error between the two curves can be made small if the step size is small enough and the interval of computation is finite:<ref>{{harvnb|Atkinson|1989|p=342}}; {{harvnb|Butcher|2003|p=60}}</ref>\n\n:<math>y'(t) = f(t,y(t)), \\qquad y(t_0)=y_0. </math>\n\nChoose a value <math>h</math> for the size of every step and set <math>t_n = t_0 + nh</math>. Now, one step of the Euler method from <math>t_n</math> to <math>t_{n+1} = t_n + h</math> is:<ref>{{harvnb|Butcher|2003|p=45}}; {{harvnb|Hairer|Nørsett|Wanner|1993|p=36}}</ref>\n\n:<math> y_{n+1} = y_n + hf(t_n,y_n).</math>\n\nThe value of <math>y_n</math> is an approximation of the solution to the ODE at time <math>t_n</math>: <math>y_n \\approx y(t_n)</math>. The Euler method is [[explicit and implicit methods|explicit]], i.e. the solution <math>y_{n+1}</math> is an explicit function of <math>y_i</math> for <math>i \\leq n</math>.\n\nWhile the Euler method integrates a first-order ODE, any ODE of order ''N'' can be represented as a first-order ODE:\nto treat the equation\n\n:<math> y^{(N)}(t) = f(t, y(t), y'(t), \\ldots, y^{(N-1)}(t)) </math>,\n\nwe introduce auxiliary variables <math>z_1(t)=y(t), z_2(t)=y'(t),\\ldots, z_N(t)=y^{(N-1)}(t)</math> and obtain\nthe equivalent equation:\n\n:<math> \\mathbf{z}'(t)\n  = \\begin{pmatrix} z_1'(t)\\\\ \\vdots\\\\ z_{N-1}'(t)\\\\ z_N'(t) \\end{pmatrix}\n  = \\begin{pmatrix} y'(t)\\\\ \\vdots\\\\ y^{(N-1)}(t)\\\\ y^{(N)}(t) \\end{pmatrix}\n  = \\begin{pmatrix} z_2(t)\\\\ \\vdots\\\\ z_N(t)\\\\ f(t,z_1(t),\\ldots,z_N(t)) \\end{pmatrix} </math>\n\nThis is a first-order system in the variable <math>\\mathbf{z}(t)</math> and can be handled by Euler's method or, in fact, by any other scheme for first-order systems.<ref>{{harvnb|Butcher|2003|p=3}}; {{harvnb|Hairer|Nørsett|Wanner|1993|p=2}}</ref>\n\n==Example==\nGiven the initial value problem\n\n:<math>y'=y, \\quad y(0)=1, </math>\n\nwe would like to use the Euler method to approximate <math>y(4)</math>.<ref>See also {{harvnb|Atkinson|1989|p=344}}</ref>\n\n=== Using step size equal to 1 (''h'' = 1) ===\n\n[[Image:Numerical integration illustration, step=1.svg|right|thumb|Illustration of numerical integration for the equation <math>y'=y, y(0)=1.</math> Blue is the Euler method; green, the [[midpoint method]]; red, the exact solution, <math>y=e^t.</math> The step size is ''h''&nbsp;=&nbsp;1.0&nbsp;.]]\nThe Euler method is\n\n:<math> y_{n+1} = y_n + hf(t_n,y_n).  \\qquad \\qquad</math>\n\nso first we must compute <math>f(t_0, y_0)</math>.  In this simple differential equation, the function <math>f</math> is defined by <math>f(t,y) = y</math>. We have\n\n:<math> f(t_0,y_0) = f(0,1) = 1. \\qquad \\qquad</math>\n\nBy doing the above step, we have found the slope of the line that is tangent to the solution curve at the point <math>(0,1)</math>.  Recall that the slope is defined as the change in <math>y</math> divided by the change in <math>t</math>, or <math> \\Delta y/ \\Delta t</math>.\n\nThe next step is to multiply the above value by the step size <math>h</math>, which we take equal to one here:\n\n:<math> h \\cdot f(y_0) = 1 \\cdot 1 = 1. \\qquad \\qquad</math>\n\nSince the step size is the change in <math>t</math>, when we multiply the step size and the slope of the tangent, we get a change in <math>y</math> value.  This value is then added to the initial <math>y</math> value to obtain the next value to be used for computations.\n\n:<math> y_0 + hf(y_0) = y_1 = 1 + 1 \\cdot 1 = 2. \\qquad \\qquad</math>\n \nThe above steps should be repeated to find <math> y_2</math>, <math>y_3 </math> and <math>y_4</math>.\n\n:<math> \\begin{align}\ny_2 &= y_1 + hf(y_1) = 2 + 1 \\cdot 2 = 4, \\\\\ny_3 &= y_2 + hf(y_2) = 4 + 1 \\cdot 4 = 8, \\\\\ny_4 &= y_3 + hf(y_3) = 8 + 1 \\cdot 8 = 16.\n\\end{align} </math>\n\nDue to the repetitive nature of this algorithm, it can be helpful to organize computations in a chart form, as seen below, to avoid making errors.\n\n:{| class=\"wikitable\"\n|-\n! <math>n</math> !! <math>y_n</math> !! <math>t_n</math> !!<math>f(t_n,y_n)</math> !! <math>h</math> !! <math>\\Delta y</math> !! <math>y_{n+1}</math>\n|-\n| 0 || 1 || 0 || 1 || 1 || 1 || 2\n|-\n| 1 || 2 || 1 || 2 || 1 || 2 || 4\n|-\n| 2 || 4 || 2 || 4 || 1 || 4 || 8\n|-\n| 3 || 8 || 3 || 8 || 1 || 8 || 16\n|}\n\nThe conclusion of this computation is that <math> y_4 = 16 </math>. The exact solution of the differential equation is <math> y(t) = e^t </math>, so <math> y(4) = e^4 \\approx 54.598 </math>. Although the approximation of the Euler method was not very precise in this specific case, particularly due to a large value step size <math>h</math>, its behaviour is qualitatively correct as the figure shows.\n\n=== MATLAB code example ===\n<source lang=\"matlab\">\nclear; clc; close('all');\ny0 = 1;\nt0 = 0;\nh = 1; % try: h = 0.01\ntn = 4; % equal to: t0 + h*n, with n the number of steps\n[t, y] = Euler(t0, y0, h, tn);\nplot(t, y, 'b');\n\n    % exact solution (y = e^t):\n    tt = (t0:0.001:tn)';\n    yy = exp(tt);\n    hold('on');\n    plot(tt, yy, 'r');\n    hold('off');\n    legend('Euler', 'Exact');\n\nfunction [t, y] = Euler(t0, y0, h, tn)\n    fprintf('%10s%10s%10s%15s\\n', 'i', 'yi', 'ti', 'f(yi,ti)');\n    fprintf('%10d%+10.2f%+10.2f%+15.2f\\n', 0, y0, t0, f(y0,t0));\n    t = (t0:h:tn)';\n    y = zeros(size(t));\n    y(1) = y0;\n    for i = 1:1:length(t)-1\n        y(i+1) = y(i) + h*f(y(i),t(i));\n        fprintf('%10d%+10.2f%+10.2f%+15.2f\\n', i, y(i+1), t(i+1), f(y(i+1),t(i+1)));\n    end\nend\n\n% in this case, f(y,t) = f(y)\nfunction dydt = f(y,t)\n    dydt = y;\nend\n\n% OUTPUT:\n%         i        yi        ti       f(yi,ti)\n%         0     +1.00     +0.00          +1.00\n%         1     +2.00     +1.00          +2.00\n%         2     +4.00     +2.00          +4.00\n%         3     +8.00     +3.00          +8.00\n%         4    +16.00     +4.00         +16.00\n% NOTE: Code also outputs a comparison plot\n</source>\n\n=== R code example ===\n[[File:Rplot.svg|thumb|Graphical output of the R programming language code for the posed example]]\nBelow is the code of the example in the [[R (programming language)|R programming language]].\n<source lang=\"R\">\n# ============\n# SOLUTION to\n#   y' = y,   where y' = f(t,y)\n# then:\nf <- function(ti,y) y\n\n# INITIAL VALUES:\nt0 <- 0\ny0 <- 1\nh  <- 1\ntn <- 4\n\n# Euler's method: function definition\nEuler <- function(t0, y0, h, tn, dy.dt) {\n  # dy.dt: derivative function\n  \n  # t sequence:\n  tt <- seq(t0, tn, by=h)\n  # table with as many rows as tt elements:\n  tbl <- data.frame(ti=tt)\n  tbl$yi <- y0 # Initializes yi with y0\n  tbl$Dy.dt[1] <- dy.dt(tbl$ti[1],y0) # derivative\n  for (i in 2:nrow(tbl)) {\n    tbl$yi[i] <- tbl$yi[i-1] + h*tbl$Dy.dt[i-1]\n    # For next iteration:\n    tbl$Dy.dt[i] <- dy.dt(tbl$ti[i],tbl$yi[i])\n  }\n  return(tbl)\n}\n\n# Euler's method: function application\nr <- Euler(t0, y0, h, tn, f)\nrownames(r) <- 0:(nrow(r)-1) # to coincide with index n\n\n# Exact solution for this case: y = exp(t)\n#       added as an additional column to r\nr$y <- exp(r$ti)\n\n# TABLE with results:\nprint(r)\n\nplot(r$ti, r$y, type=\"l\", col=\"red\", lwd=2)\nlines(r$ti, r$yi, col=\"blue\", lwd=2)\ngrid(col=\"black\")\nlegend(\"top\",  legend = c(\"Exact\", \"Euler\"), lwd=2,  col = c(\"red\", \"blue\"))\n\n# OUTPUT:\n#\n#   ti yi Dy.dt         y\n# 0  0  1     1  1.000000\n# 1  1  2     2  2.718282\n# 2  2  4     4  7.389056\n# 3  3  8     8 20.085537\n# 4  4 16    16 54.598150\n# NOTE: Code also outputs a comparison plot\n</source>\n\n=== Using other step sizes ===\n[[Image:Numerical integration illustration step=0.25.svg|right|thumb|The same illustration for ''h''&nbsp;=&nbsp;0.25.]]\nAs suggested in the introduction, the Euler method is more accurate if the step size <math>h</math> is smaller. The table below shows the result with different step sizes. The top row corresponds to the example in the previous section, and the second row is illustrated in the figure.\n\n:{| class=\"wikitable\"\n|-\n! step size !! result of Euler's method !! error\n|-\n| 1 || 16.00 || 38.60\n|-\n| 0.25 || 35.53 || 19.07\n|-\n| 0.1 || 45.26 || 9.34\n|-\n| 0.05 || 49.56 || 5.04\n|-\n| 0.025 || 51.98 || 2.62\n|-\n| 0.0125 || 53.26 || 1.34\n|}\n\nThe error recorded in the last column of the table is the difference between the exact solution at <math> t = 4 </math> and the Euler approximation. In the bottom of the table, the step size is half the step size in the previous row, and the error is also approximately half the error in the previous row. This suggests that the error is roughly proportional to the step size, at least for fairly small values of the step size. This is true in general, also for other equations; see the section [[#Global truncation error|''Global truncation error'']] for more details.\n\nOther methods, such as the [[midpoint method]] also illustrated in the figures, behave more favourably: the global error of the midpoint method is roughly proportional to the ''square'' of the step size. For this reason, the Euler method is said to be a first-order method, while the midpoint method is second order.\n\nWe can extrapolate from the above table that the step size needed to get an answer that is correct to three decimal places is approximately 0.00001, meaning that we need 400,000 steps. This large number of steps entails a high computational cost. For this reason, people usually employ alternative, higher-order methods such as [[Runge–Kutta method]]s or [[linear multistep method]]s, especially if a high accuracy is desired.<ref>{{harvnb|Hairer|Nørsett|Wanner|1993|p=40}}</ref>\n\n==Derivation==\n\nThe Euler method can be derived in a number of ways. Firstly, there is the geometrical description above.\n\nAnother possibility is to consider the [[Taylor expansion]] of the function <math>y</math> around <math>t_0</math>:\n\n:<math> y(t_0 + h) = y(t_0) + h y'(t_0) + \\frac{1}{2}h^2 y''(t_0) + O(h^3). </math>\n\nThe differential equation states that <math>y'=f(t,y)</math>. If this is substituted in the Taylor expansion and the quadratic and higher-order terms are ignored, the Euler method arises.<ref>{{harvnb|Atkinson|1989|p=342}}; {{harvnb|Hairer|Nørsett|Wanner|1993|p=36}}</ref> The Taylor expansion is used below to analyze the error committed by the Euler method, and it can be extended to produce [[Runge–Kutta methods]].\n\nA closely related derivation is to substitute the forward [[finite difference]] formula for the derivative,\n\n:<math> y'(t_0) \\approx \\frac{y(t_0+h) - y(t_0)}{h} </math>\n\nin the differential equation <math>y' = f(t,y)</math>. Again, this yields the Euler method.<ref>{{harvnb|Atkinson|1989|p=342}}</ref> A similar computation leads to the [[midpoint method]] and the [[backward Euler method]].\n\nFinally, one can integrate the differential equation from <math>t_0</math> to <math>t_0 + h</math> and apply the [[fundamental theorem of calculus]] to get:\n\n:<math> y(t_0+h) - y(t_0) = \\int_{t_0}^{t_0+h} f(t,y(t)) \\,\\mathrm{d}t. </math>\n\nNow approximate the integral by the left-hand [[rectangle method]] (with only one rectangle):\n\n:<math> \\int_{t_0}^{t_0+h} f(t,y(t)) \\,\\mathrm{d}t \\approx h f(t_0, y(t_0)). </math>\n\nCombining both equations, one finds again the Euler method.<ref>{{harvnb|Atkinson|1989|p=343}}</ref> This line of thought can be continued to arrive at various [[linear multistep method]]s.\n\n==Local truncation error==\nThe [[local truncation error]] of the Euler method is the error made in a single step. It is the difference between the numerical solution after one step, <math>y_1</math>, and the exact solution at time <math>t_1 = t_0+h</math>. The numerical solution is given by\n\n:<math> y_1 = y_0 + h f(t_0, y_0). \\quad</math>\n\nFor the exact solution, we use the Taylor expansion mentioned in the section [[#Derivation|''Derivation'']] above:\n\n:<math> y(t_0 + h) = y(t_0) + h y'(t_0) + \\frac{1}{2}h^2 y''(t_0) + O(h^3). </math>\n\nThe local truncation error (LTE) introduced by the Euler method is given by the difference between these equations:\n\n:<math> \\mathrm{LTE} = y(t_0 + h) - y_1 = \\frac{1}{2} h^2 y''(t_0) + O(h^3). </math>\n\nThis result is valid if <math>y</math> has a bounded third derivative.<ref>{{harvnb|Butcher|2003|p=60}}</ref>\n\nThis shows that for small <math>h</math>, the local truncation error is approximately proportional to <math>h^2</math>. This makes the Euler method less accurate (for small <math>h</math>) than other higher-order techniques such as [[Runge-Kutta method]]s and [[linear multistep method]]s, for which the local truncation error is proportional to a higher power of the step size.\n\nA slightly different formulation for the local truncation error can be obtained by using the Lagrange form for the remainder term in [[Taylor's theorem]]. If <math>y</math> has a continuous second derivative, then there exists a <math>\\xi \\in [t_0,t_0+h]</math> such that\n\n:<math> \\mathrm{LTE} = y(t_0 + h) - y_1 = \\frac{1}{2} h^2 y''(\\xi). </math><ref>{{harvnb|Atkinson|1989|p=342}}</ref>\n\nIn the above expressions for the error, the second derivative of the unknown exact solution <math>y</math> can be replaced by an expression involving the right-hand side of the differential equation. Indeed, it follows from the equation <math>y'=f(t,y)</math> that\n\n:<math>y''(t_0) = {\\partial f\\over\\partial t}(t_0, y(t_0)) + {\\partial f\\over\\partial y}(t_0, y(t_0)) \\, f(t_0, y(t_0)).</math><ref>{{harvnb|Stoer|Bulirsch|2002|p=474}}</ref>\n\n==Global truncation error==\nThe [[global truncation error]] is the error at a fixed time <math>t</math>, after however many steps the methods needs to take to reach that time from the initial time. The global truncation error is the cumulative effect of the local truncation errors committed in each step.<ref>{{harvnb|Atkinson|1989|p=344}}</ref> The number of steps is easily determined to be <math>(t-t_0)/h</math>, which is proportional to <math>1/h</math>, and the error committed in each step is proportional to <math>h^2</math> (see the previous section). Thus, it is to be expected that the global truncation error will be proportional to <math>h</math>.<ref>{{harvnb|Butcher|2003|p=49}}</ref>\n\nThis intuitive reasoning can be made precise. If the solution <math>y</math> has a bounded second derivative and <math>f</math> is [[Lipschitz continuity|Lipschitz continuous]] in its second argument, then the global truncation error (GTE) is bounded by\n\n:<math> |\\text{GTE}| \\le \\frac{hM}{2L}(e^{L(t-t_0)}-1) \\qquad \\qquad </math>\n\nwhere <math>M</math> is an upper bound on the second derivative of <math>y</math> on the given interval and <math>L</math> is the Lipschitz constant of <math>f</math>.<ref>{{harvnb|Atkinson|1989|p=346}}; {{harvnb|Lakoba|2012|loc=equation (1.16)}}</ref>\n\nThe precise form of this bound is of little practical importance, as in most cases the bound vastly overestimates the actual error committed by the Euler method.<ref>{{harvnb|Iserles|1996|p=7}}</ref> What is important is that it shows that the global truncation error is (approximately) proportional to <math>h</math>. For this reason, the Euler method is said to be first order.<ref>{{harvnb|Butcher|2003|p=63}}</ref>\n\n==Numerical stability==\n[[Image:Instability of Euler's method.svg|thumb|Solution of <math>y' = -2.3y</math> computed with the Euler method with step size <math>h=1</math> (blue squares) and <math>h=0.7</math> (red circles). The black curve shows the exact solution.]]\nThe Euler method can also be numerically [[numerical stability|unstable]], especially for [[stiff equation]]s, meaning that the numerical solution grows very large for equations where the exact solution does not. This can be illustrated using the linear equation\n:<math> y' = -2.3y, \\qquad y(0) = 1. </math>\nThe exact solution is <math>y(t) = e^{-2.3t}</math>, which decays to zero as <math>t \\to \\infty</math>. However, if the Euler method is applied to this equation with step size <math>h=1</math>, then the numerical solution is qualitatively wrong: It oscillates and grows (see the figure). This is what it means to be unstable. If a smaller step size is used, for instance <math>h = 0.7</math>, then the numerical solution does decay to zero.\n\n[[Image:Stability region for Euler method.svg|thumb|The pink disk shows the stability region for the Euler method.]]\nIf the Euler method is applied to the linear equation <math>y' = k y</math>, then the numerical solution is unstable if the product <math>hk</math> is outside the region\n:<math> \\{ z \\in \\mathbf{C} \\mid |z+1| \\le 1 \\}, </math>\nillustrated on the right. This region is called the (linear) ''stability region''.<ref>{{harvnb|Butcher|2003|p=70}}; {{harvnb|Iserles|1996|p=57}}</ref> In the example, <math>k</math> is −2.3, so if <math>h = 1</math> then <math>hk = -2.3</math> which is outside the stability region, and thus the numerical solution is unstable.\n\nThis limitation —along with its slow convergence of error with ''h''— means that the Euler method is not often used, except as a simple example of numerical integration.\n\n==Rounding errors==\nThe discussion up to now has ignored the consequences of [[rounding error]]. In step ''n'' of the Euler method, the rounding error is roughly of the magnitude ε''y''<sub>''n''</sub> where ε is the [[machine epsilon]]. Assuming that the rounding errors are all of approximately the same size, the combined rounding error in ''N'' steps is roughly ''N''ε''y''<sub>0</sub> if all errors points in the same direction. Since the number of steps is inversely proportional to the step size ''h'', the total rounding error is proportional to ε / ''h''. In reality, however, it is extremely unlikely that all rounding errors point in the same direction. If instead it is assumed that the rounding errors are independent random variables, then the expected total rounding error is proportional to <math> \\varepsilon / \\sqrt{h} </math>.<ref>{{harvnb|Butcher|2003|pp=74–75}}</ref>\n\nThus, for extremely small values of the step size, the truncation error will be small but the effect of rounding error may be big. Most of the effect of rounding error can be easily avoided if [[compensated summation]] is used in the formula for the Euler method.<ref>{{harvnb|Butcher|2003|pp=75–78}}</ref>\n\n==Modifications and extensions==\nA simple modification of the Euler method which eliminates the stability problems noted in the previous section is the [[backward Euler method]]:\n:<math> y_{n+1} = y_n + h f(t_{n+1}, y_{n+1}). </math>\nThis differs from the (standard, or forward) Euler method in that the function <math>f</math> is evaluated at the end point of the step, instead of the starting point. The backward Euler method is an [[explicit and implicit methods|implicit method]], meaning that the formula for the backward Euler method has <math> y_{n+1} </math> on both sides, so when applying the backward Euler method we have to solve an equation. This makes the implementation more costly.\n\nOther modifications of the Euler method that help with stability yield the [[exponential Euler method]] or the [[semi-implicit Euler method]].\n\nMore complicated methods can achieve a higher order (and more accuracy). One possibility is to use more function evaluations. This is illustrated by the [[midpoint method]] which is already mentioned in this article:\n:<math> y_{n+1} = y_n + h f \\Big( t_n + \\tfrac12 h, y_n + \\tfrac12 h f(t_n, y_n) \\Big)</math>.\nThis leads to the family of [[Runge–Kutta methods]].\n\nThe other possibility is to use more past values, as illustrated by the two-step Adams–Bashforth method:\n:<math> y_{n+1} = y_n + \\tfrac32 h f(t_{n}, y_{n}) - \\tfrac12 h f(t_{n-1}, y_{n-1}). </math>\nThis leads to the family of [[linear multistep method]]s. There are other modifications which uses techniques from compressive sensing to minimize memory usage<ref>{{cite journal |last=Unni |first=M. P. |last2=Chandra|first2=M. G. |last3=Kumar |first3=A. A. |date=March 2017 |title=Memory reduction for numerical solution of differential equations using compressive sensing |url=http://ieeexplore.ieee.org/document/8064928/ |journal=2017 IEEE 13th&nbsp;International Colloquium on Signal Processing its Applications (CSPA) |pages=79–84 |doi=10.1109/CSPA.2017.8064928}}</ref>\n\n==In popular culture==\nIn the film ''[[Hidden Figures]]'', [[Katherine Goble]] resorts to the Euler method in calculating the re-entry of astronaut [[John Glenn]] from Earth orbit.<ref>{{cite web |last1=Khan |first1=Amina |title=Meet the ‘Hidden Figures’ mathematician who helped send Americans into space |url=http://www.latimes.com/science/sciencenow/la-sci-sn-hidden-figures-katherine-johnson-20170109-story.html |website=Los Angeles Times |accessdate=12 February 2017}}</ref>\n\n==See also==\n*[[Crank–Nicolson method]]\n*[[Dynamic errors of numerical methods of ODE discretization]]\n*[[Gradient descent]] similarly uses finite steps, here to find minima of functions\n*[[List of Runge-Kutta methods]]\n*[[Linear multistep method]]\n*[[Numerical integration]] (for calculating definite integrals)\n*[[Numerical methods for ordinary differential equations]]\n\n== Notes ==\n{{reflist|2}}\n\n== References ==\n* {{cite book |last1=Atkinson |first1=Kendall A. |title=An Introduction to Numerical Analysis |publisher=[[John Wiley & Sons]] |location=New York |edition=2nd |isbn=978-0-471-50023-0 |year=1989}}\n* {{cite book |last1=Ascher |first1=Uri M. |last2=Petzold |first2=Linda R. |author2-link=Linda Petzold |title=Computer Methods for Ordinary Differential Equations and Differential-Algebraic Equations |publisher=[[Society for Industrial and Applied Mathematics]] |location=Philadelphia |isbn=978-0-89871-412-8 |year=1998}}\n* {{cite book |last1=Butcher |first1=John C. |author1-link=John C. Butcher |title=Numerical Methods for Ordinary Differential Equations |publisher=[[John Wiley & Sons]] |location=New York |isbn=978-0-471-96758-3 |year=2003}}\n* {{cite book |last1=Hairer |first1=Ernst |last2=Nørsett |first2=Syvert Paul |last3=Wanner |first3=Gerhard |title=Solving ordinary differential equations I: Nonstiff problems |publisher=[[Springer-Verlag]] |location=Berlin, New York |isbn=978-3-540-56670-0 |year=1993}}\n* {{cite book |last1=Iserles |first1=Arieh |author1-link=Arieh Iserles |title=A First Course in the Numerical Analysis of Differential Equations |publisher=[[Cambridge University Press]] |isbn=978-0-521-55655-2 |year=1996}}\n* {{cite book |last1=Stoer |first1=Josef |last2=Bulirsch |first2=Roland |title=Introduction to Numerical Analysis |publisher=[[Springer-Verlag]] |location=Berlin, New York |edition=3rd |isbn=978-0-387-95452-3 |year=2002}}\n* {{citation |last1=Lakoba |first1=Taras I. |title=Simple Euler method and its modifications |type=Lecture notes for MATH334 |publisher=University of Vermont |year=2012 |url=http://www.cems.uvm.edu/~lakobati/math337/notes_1.pdf |accessdate=29 February 2012}}\n* {{cite book |last1=Unni |first1=M P. |title=Memory reduction for numerical solution of differential equations using compressive sensing |publisher=[[IEEE CSPA]] |url=http://ieeexplore.ieee.org/document/8064928/ |isbn=978-1-5090-1184-1 |year=2017}}\n\n== External links ==\n{{wikibooks|1=Calculus|2=Euler's Method}}\n* {{commons category inline}}\n* [http://rosettacode.org/wiki/Euler_method Euler method implementations in different languages] by [[Rosetta Code]]\n{{Numerical integrators}}\n\n[[Category:Numerical differential equations]]\n[[Category:Runge–Kutta methods]]\n[[Category:First order methods]]\n[[Category:Leonhard Euler]]"
    },
    {
      "title": "Proximal gradient methods for learning",
      "url": "https://en.wikipedia.org/wiki/Proximal_gradient_methods_for_learning",
      "text": "'''Proximal gradient''' (forward backward splitting) '''methods for learning''' is an area of research in [[optimization]] and [[statistical learning theory]] which studies algorithms for a general class of [[Convex function#Definition|convex]] [[Regularization (mathematics)|regularization]] problems where the regularization penalty may not be [[Differentiable function|differentiable]]. One such example is <math>\\ell_1</math> regularization (also known as Lasso) of the form\n:<math>\\min_{w\\in\\mathbb{R}^d} \\frac{1}{n}\\sum_{i=1}^n (y_i- \\langle w,x_i\\rangle)^2+ \\lambda \\|w\\|_1, \\quad \\text{ where } x_i\\in \\mathbb{R}^d\\text{ and } y_i\\in\\mathbb{R}.</math>\n\nProximal gradient methods offer a general framework for solving regularization problems from statistical learning theory with penalties that are tailored to a specific problem application.<ref name=combettes>{{cite journal|last=Combettes|first=Patrick L.|author2=Wajs, Valérie R. |title=Signal Recovering by Proximal Forward-Backward Splitting|journal=Multiscale Model. Simul.|year=2005|volume=4|issue=4|pages=1168–1200|url=http://epubs.siam.org/doi/abs/10.1137/050626090|doi=10.1137/050626090}}</ref><ref name=structSparse>{{cite journal|last=Mosci|first=S.|author2=Rosasco, L. |author3=Matteo, S. |author4=Verri, A. |author5=Villa, S. |title=Solving Structured Sparsity Regularization with Proximal Methods|journal=Machine Learning and Knowledge Discovery in Databases|year=2010|volume=6322|pages=418–433 |doi=10.1007/978-3-642-15883-4_27}}</ref> Such customized penalties can help to induce certain structure in problem solutions, such as ''sparsity'' (in the case of [[Lasso (statistics)|lasso]]) or ''group structure'' (in the case of  [[Lasso (statistics)#Group LASSO|group lasso]]).\n\n== Relevant background ==\n\n[[Proximal gradient method]]s are applicable in a wide variety of scenarios for solving [[convex optimization]] problems of the form\n:<math> \\min_{x\\in \\mathcal{H}} F(x)+R(x),</math>\nwhere <math>F</math> is [[Convex function|convex]] and differentiable with [[Lipschitz continuity|Lipschitz continuous]] [[gradient]], <math> R</math> is a [[Convex function|convex]], [[Semicontinuous function|lower semicontinuous]] function which is possibly nondifferentiable, and <math>\\mathcal{H}</math> is some set, typically a [[Hilbert space]]. The usual criterion of <math> x</math> minimizes <math> F(x)+R(x)</math> if and only if <math> \\nabla (F+R)(x) = 0</math> in the convex, differentiable setting is now replaced by\n:<math> 0\\in \\partial (F+R)(x), </math>\nwhere <math>\\partial \\varphi</math> denotes the [[subdifferential]] of a real-valued, convex function <math> \\varphi</math>.\n\nGiven a convex function <math>\\varphi:\\mathcal{H} \\to \\mathbb{R}</math> an important operator to consider is its '''proximity operator''' <math>\\operatorname{prox}_{\\varphi}:\\mathcal{H}\\to\\mathcal{H} </math> defined by\n:<math> \\operatorname{prox}_{\\varphi}(u) = \\operatorname{arg}\\min_{x\\in\\mathcal{H}} \\varphi(x)+\\frac{1}{2}\\|u-x\\|_2^2,</math>\nwhich is well-defined because of the strict convexity of the <math> \\ell_2</math> norm. The proximity operator can be seen as a generalization of a [[Projection (linear algebra)|projection]].<ref name=combettes /><ref name=moreau /><ref name=bauschke>{{cite book|last=Bauschke|first=H.H., and Combettes, P.L.|title=Convex analysis and monotone operator theory in Hilbert spaces|year=2011|publisher=Springer}}</ref>\nWe see that the proximity operator is important because <math> x^* </math> is a minimizer to the problem <math> \\min_{x\\in\\mathcal{H}} F(x)+R(x)</math> if and only if\n:<math>x^* = \\operatorname{prox}_{\\gamma R}\\left(x^*-\\gamma\\nabla F(x^*)\\right),</math> where <math>\\gamma>0</math> is any positive real number.<ref name=combettes />\n\n=== Moreau decomposition ===\n\nOne important technique related to proximal gradient methods is the '''Moreau decomposition,''' which decomposes the identity operator as the sum of two proximity operators.<ref name=combettes /> Namely, let <math>\\varphi:\\mathcal{X}\\to\\mathbb{R}</math> be a [[Semi-continuity|lower semicontinuous]], convex function on a vector space <math>\\mathcal{X}</math>. We define its [[Convex conjugate|Fenchel conjugate]] <math>\\varphi^*:\\mathcal{X}\\to\\mathbb{R}</math> to be the function\n:<math>\\varphi^*(u) := \\sup_{x\\in\\mathcal{X}} \\langle x,u\\rangle - \\varphi(x).</math>\nThe general form of Moreau's decomposition states that for any <math>x\\in\\mathcal{X}</math> and any <math>\\gamma>0</math> that\n:<math>x = \\operatorname{prox}_{\\gamma \\varphi}(x) + \\gamma\\operatorname{prox}_{\\varphi^*/\\gamma}(x/\\gamma),</math>\nwhich for <math>\\gamma=1</math> implies that <math>x = \\operatorname{prox}_{\\varphi}(x)+\\operatorname{prox}_{\\varphi^*}(x)</math>.<ref name=combettes /><ref name=moreau>{{cite journal|last=Moreau|first=J.-J.|title=Fonctions convexes duales et points proximaux dans un espace hilbertien|journal=Comptes Rendus de l'Académie des Sciences, Série A|year=1962|volume=255|pages=2897–2899|mr=144188|zbl=0118.10502}}</ref> The Moreau decomposition can be seen to be a generalization of the usual orthogonal decomposition of a vector space, analogous with the fact that proximity operators are generalizations of projections.<ref name=combettes />\n\nIn certain situations it may be easier to compute the proximity operator for the conjugate <math>\\varphi^*</math> instead of the function <math>\\varphi</math>, and therefore the Moreau decomposition can be applied. This is the case for  [[Lasso (statistics)#Group LASSO|group lasso]].\n\n== Lasso regularization ==\n\nConsider the [[Regularization (mathematics)|regularized]] [[empirical risk minimization]] problem with square loss and with the [[L1-norm|<math>\\ell_1</math> norm]] as the regularization penalty:\n:<math>\\min_{w\\in\\mathbb{R}^d} \\frac{1}{n}\\sum_{i=1}^n (y_i- \\langle w,x_i\\rangle)^2+ \\lambda \\|w\\|_1, </math>\nwhere <math>x_i\\in \\mathbb{R}^d\\text{ and } y_i\\in\\mathbb{R}.</math> The <math>\\ell_1</math> regularization problem is sometimes referred to as ''lasso'' ([[Lasso (statistics)|least absolute shrinkage and selection operator]]).<ref name=tibshirani /> Such <math>\\ell_1</math> regularization problems are interesting because they induce '' sparse'' solutions, that is, solutions <math>w</math> to the minimization problem have relatively few nonzero components. Lasso can be seen to be a convex relaxation of the non-convex problem\n:<math>\\min_{w\\in\\mathbb{R}^d} \\frac{1}{n}\\sum_{i=1}^n (y_i- \\langle w,x_i\\rangle)^2+ \\lambda \\|w\\|_0, </math>\nwhere <math>\\|w\\|_0</math> denotes the <math>\\ell_0</math> \"norm\", which is the number of nonzero entries of the vector <math>w</math>. Sparse solutions are of particular interest in learning theory for interpretability of results: a sparse solution can identify a small number of important factors.<ref name=tibshirani>{{cite journal|last=Tibshirani|first=R.|title=Regression shrinkage and selection via the lasso|journal=J. R. Stat. Soc. Ser. B|year=1996|volume=58|series=1|issue=1|pages=267–288}}</ref>\n\n=== Solving for <math>\\ell_1</math> proximity operator ===\n\nFor simplicity we restrict our attention to the problem where <math>\\lambda=1</math>. To solve the problem\n:<math>\\min_{w\\in\\mathbb{R}^d} \\frac{1}{n}\\sum_{i=1}^n (y_i- \\langle w,x_i\\rangle)^2+  \\|w\\|_1, </math>\nwe consider our objective function in two parts: a convex, differentiable term <math>F(w) = \\frac{1}{n}\\sum_{i=1}^n (y_i- \\langle w,x_i\\rangle)^2</math> and a convex function <math>R(w) = \\|w\\|_1</math>. Note that <math>R</math> is not strictly convex.\n\nLet us compute the proximity operator for <math>R(w)</math>. First we find an alternative characterization of the proximity operator <math>\\operatorname{prox}_{R}(x)</math> as follows:\n\n<math>\n\\begin{align}\nu = \\operatorname{prox}_R(x) \\iff & 0\\in \\partial \\left(R(u)+\\frac{1}{2}\\|u-x\\|_2^2\\right)\\\\\n\\iff & 0\\in \\partial R(u) + u-x\\\\\n\\iff & x-u\\in \\partial R(u).\n\\end{align}\n</math>\n\nFor <math>R(w) = \\|w\\|_1</math> it is easy to compute <math>\\partial R(w)</math>: the <math>i</math>th entry of <math>\\partial R(w)</math> is precisely\n\n:<math> \\partial |w_i| = \\begin{cases}\n1,&w_i>0\\\\\n-1,&w_i<0\\\\\n\\left[-1,1\\right],&w_i = 0.\n\\end{cases}</math>\n\nUsing the recharacterization of the proximity operator given above, for the choice of <math>R(w) = \\|w\\|_1</math> and <math>\\gamma>0</math> we have that <math>\\operatorname{prox}_{\\gamma R}(x)</math> is defined entrywise by\n\n::<math>\\left(\\operatorname{prox}_{\\gamma R}(x)\\right)_i = \\begin{cases}\nx_i-\\gamma,&x_i>\\gamma\\\\\n0,&|x_i|\\leq\\gamma\\\\\nx_i+\\gamma,&x_i<-\\gamma,\n\\end{cases}</math>\n\nwhich is known as the [[Thresholding (image processing)|soft thresholding]] operator <math>S_{\\gamma}(x)=\\operatorname{prox}_{\\gamma \\|\\cdot\\|_1}(x)</math>.<ref name=combettes /><ref name=daubechies>{{cite journal|last=Daubechies|first=I. |author2=Defrise, M. |author3=De Mol, C.|title=An iterative thresholding algorithm for linear inverse problem with a sparsity constraint|journal=Comm. Pure Appl. Math.|year=2004|volume=57|issue=11|pages=1413–1457|doi=10.1002/cpa.20042|arxiv=math/0307152}}</ref>\n\n=== Fixed point iterative schemes ===\n\nTo finally solve the lasso problem we consider the fixed point equation shown earlier:\n:<math>x^* = \\operatorname{prox}_{\\gamma R}\\left(x^*-\\gamma\\nabla F(x^*)\\right).</math>\n\nGiven that we have computed the form of the proximity operator explicitly, then we can define a standard fixed point iteration procedure. Namely, fix some initial <math>w^0\\in\\mathbb{R}^d</math>, and for <math>k=1,2,\\ldots</math> define\n:<math>w^{k+1} = S_{\\gamma}\\left(w^k - \\gamma \\nabla F\\left(w^k\\right)\\right).</math>\nNote here the effective trade-off between the empirical error term <math>F(w) </math> and the regularization penalty <math>R(w)</math>. This  fixed point method has decoupled the effect of the two different convex functions which comprise the objective function into a gradient descent step (<math> w^k - \\gamma \\nabla F\\left(w^k\\right)</math>) and a soft thresholding step (via <math>S_\\gamma</math>).\n\nConvergence of this fixed point scheme is well-studied in the literature<ref name=combettes /><ref name=daubechies /> and is guaranteed under appropriate choice of step size <math>\\gamma</math> and loss function (such as the square loss taken here). [[Gradient descent#Extensions|Accelerated methods]] were introduced by Nesterov in 1983 which improve the rate of convergence under certain regularity assumptions on <math>F</math>.<ref name=nesterov>{{cite journal|last=Nesterov|first=Yurii|title=A method of solving a convex programming problem with convergence rate <math>O(1/k^2)</math>|journal=Soviet Mathematics - Doklady|year=1983|volume=27|issue=2|pages=372–376}}</ref> Such methods have been studied extensively in previous years.<ref>{{cite book|last=Nesterov|first=Yurii|title=Introductory Lectures on Convex Optimization|year=2004|publisher=Kluwer Academic Publisher}}</ref>\nFor more general learning problems where the proximity operator cannot be computed explicitly for some regularization term <math>R</math>, such fixed point schemes can still be carried out using approximations to both the gradient and the proximity operator.<ref name=bauschke /><ref>{{cite journal|last=Villa|first=S.|author2=Salzo, S. |author3=Baldassarre, L. |author4=Verri, A. |title=Accelerated and inexact forward-backward algorithms|journal=SIAM J. Optim.|year=2013|volume=23|issue=3|pages=1607–1633|doi=10.1137/110844805}}</ref>\n\n== Practical considerations ==\n\nThere have been numerous developments within the past decade in [[convex optimization]] techniques which have influenced the application of proximal gradient methods in statistical learning theory. Here we survey a few important topics which can greatly improve practical algorithmic performance of these methods.<ref name=structSparse /><ref name=bach>{{cite journal|last=Bach|first=F.|author2=Jenatton, R. |author3=Mairal, J. |author4=Obozinski, Gl. |title=Optimization with sparsity-inducing penalties|journal=Found. & Trends Mach. Learn.|year=2011|volume=4|issue=1|pages=1–106|doi=10.1561/2200000015|arxiv=1108.0775}}</ref>\n\n=== Adaptive step size ===\n\nIn the fixed point iteration scheme\n:<math>w^{k+1} = \\operatorname{prox}_{\\gamma R}\\left(w^k-\\gamma \\nabla F\\left(w^k\\right)\\right),</math>\none can allow variable step size <math>\\gamma_k</math> instead of a constant <math>\\gamma</math>. Numerous adaptive step size schemes have been proposed throughout the literature.<ref name=combettes /><ref name=bauschke /><ref>{{cite journal|last=Loris|first=I. |author2=Bertero, M. |author3=De Mol, C. |author4=Zanella, R. |author5=Zanni, L. |title=Accelerating gradient projection methods for <math>\\ell_1</math>-constrained signal recovery by steplength selection rules|journal=Applied & Comp. Harmonic Analysis|volume=27|issue=2|pages=247–254|year=2009|doi=10.1016/j.acha.2009.02.003}}</ref><ref>{{cite journal|last=Wright|first=S.J.|author2=Nowak, R.D. |author3=Figueiredo, M.A.T. |title=Sparse reconstruction by separable approximation|journal=IEEE Trans. Image Process.|year=2009|volume=57|issue=7|pages=2479–2493|doi=10.1109/TSP.2009.2016892|bibcode=2009ITSP...57.2479W}}</ref> Applications of these schemes<ref name=structSparse /><ref>{{cite journal|last=Loris|first=Ignace|title=On the performance of algorithms for the minimization of <math>\\ell_1</math>-penalized functionals|journal=Inverse Problems|year=2009|volume=25|issue=3|doi=10.1088/0266-5611/25/3/035008|page=035008|arxiv=0710.4082|bibcode=2009InvPr..25c5008L}}</ref>  suggest that these can offer substantial improvement in number of iterations required for fixed point convergence.\n\n=== Elastic net (mixed norm regularization) ===\n\n[[Elastic net regularization]] offers an alternative to pure <math>\\ell_1</math> regularization. The problem of lasso (<math>\\ell_1</math>) regularization involves the penalty term <math>R(w) = \\|w\\|_1</math>, which is not strictly convex. Hence, solutions to <math>\\min_w F(w) + R(w),</math> where <math>F</math> is some empirical loss function, need not be unique. This is often avoided by the inclusion of an additional strictly convex term, such as an <math>\\ell_2</math> norm regularization penalty. For example, one can consider the problem\n:<math>\\min_{w\\in\\mathbb{R}^d} \\frac{1}{n}\\sum_{i=1}^n (y_i- \\langle w,x_i\\rangle)^2+ \\lambda \\left((1-\\mu)\\|w\\|_1+\\mu \\|w\\|_2^2\\right), </math>\nwhere <math>x_i\\in \\mathbb{R}^d\\text{ and } y_i\\in\\mathbb{R}.</math>\nFor <math>0<\\mu\\leq 1</math> the penalty term <math>\\lambda \\left((1-\\mu)\\|w\\|_1+\\mu \\|w\\|_2^2\\right)</math> is now strictly convex, and hence the minimization problem now admits a unique solution. It has been observed that for sufficiently small <math>\\mu > 0</math>, the additional penalty term <math>\\mu \\|w\\|_2^2</math> acts as a preconditioner and can substantially improve convergence while not adversely affecting the sparsity of solutions.<ref name=structSparse /><ref name=deMolElasticNet>{{cite journal|last=De Mol|first=C. |author2=De Vito, E. |author3=Rosasco, L.|title=Elastic-net regularization in learning theory|journal=J. Complexity|year=2009|volume=25|issue=2|pages=201–230|doi=10.1016/j.jco.2009.01.002}}</ref>\n\n== Exploiting group structure ==\n\nProximal gradient methods provide a general framework which is applicable to a wide variety of problems in [[statistical learning theory]]. Certain problems in learning can often involve data which has additional structure that is known '' a priori''. In the past several years there have been new developments which incorporate information about group structure to provide methods which are tailored to different applications. Here we survey a few such methods.\n\n=== Group lasso ===\n\nGroup lasso is a generalization of the [[Lasso (statistics)|lasso method]] when features are grouped into disjoint blocks.<ref name=groupLasso>{{cite journal|last=Yuan|first=M.|author2=Lin, Y. |title=Model selection and estimation in regression with grouped variables|journal=J. R. Stat. Soc. B|year=2006|volume=68|issue=1|pages=49–67|doi=10.1111/j.1467-9868.2005.00532.x}}</ref> Suppose the features are grouped into blocks <math>\\{w_1,\\ldots,w_G\\}</math>. Here we take as a regularization penalty\n\n:<math>R(w) =\\sum_{g=1}^G \\|w_g\\|_2,</math>\n\nwhich is the sum of the <math>\\ell_2</math> norm on corresponding feature vectors for the different groups. A similar proximity operator analysis as above can be used to compute the proximity operator for this penalty. Where the lasso penalty has a proximity operator which is soft thresholding on each individual component, the proximity operator for the group lasso is soft thresholding on each group. For the group <math>w_g</math> we have that proximity operator of <math>\\lambda\\gamma\\left(\\sum_{g=1}^G \\|w_g\\|_2\\right) </math> is given by\n\n:<math>\\widetilde{S}_{\\lambda\\gamma }(w_g) =  \\begin{cases}\nw_g-\\lambda\\gamma \\frac{w_g}{\\|w_g\\|_2}, & \\|w_g\\|_2>\\lambda\\gamma \\\\\n0, & \\|w_g\\|_2\\leq \\lambda\\gamma\n\\end{cases}</math>\n\nwhere <math>w_g</math> is the <math>g</math>th group.\n\nIn contrast to lasso, the derivation of the proximity operator for group lasso relies on the [[#Moreau decomposition|Moreau decomposition]]. Here the proximity operator of the conjugate of the group lasso penalty becomes a projection onto the [[Ball (mathematics)|ball]] of a [[dual norm]].<ref name=structSparse />\n\n=== Other group structures ===\n\nIn contrast to the group lasso problem, where features are grouped into disjoint blocks, it may be the case that grouped features are overlapping or have a nested structure.  Such generalizations of group lasso have been considered in a variety of contexts.<ref>{{cite journal|last=Chen|first=X.|author2=Lin, Q. |author3=Kim, S. |author4=Carbonell, J.G. |author5=Xing, E.P. |title=Smoothing proximal gradient method for general structured sparse regression|journal=Ann. Appl. Stat.|year=2012|volume=6|issue=2|pages=719–752|doi=10.1214/11-AOAS514|arxiv=1005.4717}}</ref><ref>{{cite journal|last=Mosci|first=S.|author2=Villa, S. |author3=Verri, A. |author4=Rosasco, L. |title=A primal-dual algorithm for group sparse regularization with overlapping groups|journal=NIPS|year=2010|volume=23|pages=2604–2612}}</ref><ref name=nest>{{cite journal|last=Jenatton|first=R. |author2=Audibert, J.-Y. |author3=Bach, F. |title=Structured variable selection with sparsity-inducing norms|journal=J. Mach. Learn. Res.|year=2011|volume=12|pages=2777–2824}}</ref><ref>{{cite journal|last=Zhao|first=P.|author2=Rocha, G. |author3=Yu, B. |title=The composite absolute penalties family for grouped and hierarchical variable selection|journal=Ann. Stat.|year=2009|volume=37|issue=6A|pages=3468–3497|doi=10.1214/07-AOS584|arxiv=0909.0411}}</ref> For overlapping groups one common approach is known as ''latent group lasso'' which introduces latent variables to account for overlap.<ref>{{cite journal|last=Obozinski|first=G. |author2=Laurent, J. |author3=Vert, J.-P. |title=Group lasso with overlaps: the latent group lasso approach|journal=INRIA Technical Report|year=2011|url=http://hal.inria.fr/inria-00628498/en/}}</ref><ref>{{cite journal|last=Villa|first=S.|author2=Rosasco, L. |author3=Mosci, S. |author4=Verri, A. |title=Proximal methods for the latent group lasso penalty|journal=preprint|year=2012|arxiv=1209.0368|bibcode=2012arXiv1209.0368V}}</ref> Nested group structures are studied in ''hierarchical structure prediction'' and with [[directed acyclic graph]]s.<ref name=nest />\n\n== See also ==\n* [[Convex analysis]]\n* [[Proximal gradient method]]\n* [[Regularization (mathematics)#Regularization in statistics and machine learning|Regularization]]\n* [[Statistical learning theory]]\n\n== References ==\n\n{{reflist}}\n\n[[Category:First order methods|First order methods]]\n[[Category:Convex optimization]]\n[[Category:Machine learning]]"
    },
    {
      "title": "Structured sparsity regularization",
      "url": "https://en.wikipedia.org/wiki/Structured_sparsity_regularization",
      "text": "{{Orphan|date=December 2015}}\n\n'''Structured sparsity regularization''' is a class of methods, and an area of research in [[statistical learning theory]], that extend and generalize sparsity regularization learning methods.<ref name=\"rosPoggio\">{{cite document|last = Rosasco|first = Lorenzo|author2 = Poggio, Tomasso|title = A Regularization Tour of Machine Learning |work=MIT-9.520 Lectures Notes|date=December 2014}}</ref> Both sparsity and structured sparsity regularization methods seek to exploit the assumption that the output variable <math> Y </math> (i.e., response, or [[Dependent and independent variables#Dependent variable|dependent variable]]) to be learned can be described by a reduced number of variables in the input space <math> X </math> (i.e., the [[Domain of a function|domain]], space of [[Feature (machine learning)|features]] or [[Dependent and independent variables#Independent variable|explanatory variables]]). ''Sparsity regularization methods'' focus on selecting the input variables that best describe the output. ''Structured sparsity regularization methods'' generalize and extend sparsity regularization methods, by allowing for optimal selection over structures like groups or networks of input variables in <math> X </math>.<ref name=\"groupLasso\" /><ref name=\"latentLasso\" />\n\nCommon motivation for the use of structured sparsity methods are model interpretability, [[Curse of dimensionality|high-dimensional learning]] (where dimensionality of <math> X </math> may be higher than the number of observations <math> n </math>), and reduction of [[Time complexity|computational complexity]].<ref name=\"LR18\" /> Moreover, structured sparsity methods allow to incorporate prior assumptions on the structure of the input variables, such as overlapping groups,<ref name=\"groupLasso\" /> non-overlapping groups, and acyclic graphs.<ref name=\"latentLasso\" /> Examples of uses of structured sparsity methods include face recognition,<ref name=\"face_recognition\">{{cite journal|last = Jia|first = Kui|title = Robust and Practical Face Recognition via Structured Sparsity|year = 2012|display-authors=etal}}</ref> [[Magnetic resonance imaging|magnetic resonance image (MRI)]] processing,<ref name=\"MRI\">{{cite journal|last = Chen|first = Chen|title = Compressive Sensing MRI with Wavelet Tree Sparsity|journal = Proc. Of the 26th Annual Conference on Neural Information Processing Systems (NIPS)|year = 2012|display-authors=etal}}</ref> socio-linguistic analysis in natural language processing,<ref name=\"sociolinguistic\">{{cite journal|last = Eisenstein|first = Jacob|title = Discovering Sociolinguistic Associations with Structured Sparsity|journal = Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics|year = 2011|display-authors=etal}}</ref> and analysis of genetic expression in breast cancer.<ref name=\"genetic\">{{cite journal|last = Jacob|first = Laurent|title = Group Lasso with Overlap and Graph Lasso|journal = Proceedings of the 26th International Conference on Machine Learning|year = 2009|display-authors=etal}}</ref>\n\n== Definition and related concepts ==\n\n=== Sparsity regularization ===\nConsider the linear kernel [[Regularization (mathematics)|regularized]] [[empirical risk minimization]] problem with a loss function <math> V(y_i, f(x)) </math>  and the <math>\\ell_0</math> \"norm\" as the regularization penalty:\n: <math>\\min_{w\\in\\mathbb{R}^d} \\frac{1}{n}\\sum_{i=1}^n V(y_i, \\langle w,x_i\\rangle)  + \\lambda \\|w\\|_0, </math>\nwhere <math> x, w \\in \\mathbb{R^d} </math>, and <math>\\|w\\|_0</math> denotes the <math>\\ell_0</math> \"norm\", defined as the number of nonzero entries of the vector <math>w</math>. <math>f(x) = \\langle w,x_i\\rangle</math>  is said to be '''sparse if'''  <math>\\|w\\|_0 = s < d</math>. Which means that the output <math>Y</math> can be described by a small subset of input variables.\n\nMore generally, assume a dictionary <math> \\phi_j : X \\rightarrow \\mathbb{R} </math> with <math>j = 1,...,p </math>  is given, such that the target function <math>f(x)</math> of a learning problem can be written as:\n: <math>f(x) = \\sum_{j=1}^p \\phi_j(x) w_j</math>, <math> \\forall x \\in X </math> \nThe <math>\\ell_0</math> norm <math>\\|f\\|_0 = \\|w\\|_0</math>  as the number of non-zero components of <math>w</math> is defined as \n: <math>\\|w\\|_0 = | \\{ j | w_j \\neq 0, j \\in\\{ 1,...,p \\}\\} |</math>, where <math>|A|</math> is the cardinality of set <math>A</math>.\n<math>f</math> is said to be sparse if <math>\\|f\\|_0 = \\|w\\|_0 = s < d</math>.\n\nHowever, while using the <math>\\ell_0</math> norm for regularization favors sparser solutions, it is computationally difficult to use and additionally is not convex. A computationally more feasible norm that favors sparser solutions is the <math>\\ell_1</math> norm; this has been shown to still favor sparser solutions and is additionally convex.<ref name=\"LR18\" />\n\n=== Structured sparsity regularization ===\nStructured sparsity regularization extends and generalizes the variable selection problem that characterizes sparsity regularization.<ref name=\"groupLasso\">{{cite journal|last = Yuan|first = M.|author2 = Lin, Y.|title = Model selection and estimation in regression with grouped variables|journal = J. R. Stat. Soc. B|year = 2006|volume = 68|issue = 1|pages = 49–67|doi = 10.1111/j.1467-9868.2005.00532.x|citeseerx = 10.1.1.79.2062}}</ref><ref name=\"latentLasso\" /> Consider the above [[Regularization (mathematics)|regularized]] [[empirical risk minimization]] problem with a general kernel and associated feature map <math> \\phi_j : X \\rightarrow \\mathbb{R} </math> with <math>j = 1,...,p </math>.\n: <math>\\min_{w\\in\\mathbb{R}^d} \\frac{1}{n}\\sum_{i=1}^n V(y_i, \\langle w,\\Phi(x_i)\\rangle)  + \\lambda \\|w\\|_0, </math>\nThe regularization term <math>\\lambda \\|w\\|_0 </math> penalizes each <math>w_j</math> component independently, which means that the algorithm will suppress input variables independently from each other.\n\nIn several situations we may want to impose more structure in the regularization process, so that, for example, input variables are suppressed according to predefined groups. '''Structured sparsity regularization methods''' allow to impose such structure by adding structure to the norms defining the regularization term.\n\n== Structures and norms ==\n\n=== Non-overlapping groups: group Lasso ===\nThe non-overlapping group case is the most basic instance of structured sparsity. In it, an ''a priori'' partition of the coefficient vector <math>w</math> in <math>G</math> non-overlapping groups is assumed. Let <math>w_g</math> be the vector of coefficients in group <math>g</math>, we can define a regularization term and its group norm as\n: <math>\\lambda R(w)=\\lambda\\sum _{{g=1}}^{G}\\|w_{g}\\|_{g} </math>,\nwhere <math> \\|w_{g}\\|_{g}</math> is the group <math>\\ell_2</math> norm <math> \\|w_{g}\\|_{g}= \\sqrt{ \\sum _{{j=1}}^{{|G_{g}|}}(w_{g}^{j})^{2}} </math> ,    <math>G_g</math> is group <math>g</math>, and <math>w_{g}^{j}</math> is the ''j-th'' component of group <math>G_g</math>.\n\nThe above norm is also referred to as '''group Lasso'''.<ref name=\"groupLasso\" /> This regularizer will force entire coefficient groups towards zero, rather than individual coefficients. As the groups are non-overlapping, the set of non-zero coefficients can be obtained as the union of the groups that were not set to zero, and conversely for the set of zero coefficients.\n\n=== Overlapping groups ===\nOverlapping groups is the structure sparsity case where a variable can belong to more than one group <math>g</math>. This case is often of interest as it can represent a more general class of  relationships among variables than non-overlapping groups can, such as tree structures or other type of graphs.<ref name=\"latentLasso\">{{cite arxiv |last1 = Obozinski|first1 = G.|last2 = Laurent | first2= J. | last3= Vert | first3= J.-P.|title = Group lasso with overlaps: the latent group lasso approach |year = 2011 |eprint = 1110.0413|class = stat.ML}}</ref><ref name=\"genetic\" />\n\nThere are two types of overlapping group sparsity regularization approaches, which are used to model different types of input variable relationships:\n\n==== Intersection of complements: group Lasso  ====\nThe ''intersection of complements'' approach is used in cases when we want to select only those input variables that have positive coefficients in all groups they belong to.  Consider again the '''group Lasso''' for a [[Regularization (mathematics)|regularized]] [[empirical risk minimization]] problem:\n: <math>\\lambda R(w)=\\lambda\\sum _{{g=1}}^{G}\\|w_{g}\\|_{g} </math>,\nwhere <math> \\|w_{g}\\|_{g}</math> is the group <math>\\ell_2</math> norm,    <math>G_g</math> is group <math>g</math>, and <math>w_{g}^{j}</math> is the ''j-th'' component of group <math>G_g</math>.\n\nAs in the non-overlapping groups case, the ''group Lasso'' regularizer will potentially set entire groups of coefficients to zero. Selected variables are those with coefficients <math>w_j > 0</math>. However, as in this case groups may overlap, we take the '''intersection of the complements''' of those groups that are not set to zero.\n\nThis ''intersection of complements'' selection criteria implies the modeling choice that we allow some coefficients within a particular group <math>g</math> to be set to zero, while others within the same group <math>g</math> may remain positive. In other words, coefficients within a group may differ depending on the several group memberships that each variable within the group may have.\n\n==== Union of groups: latent group Lasso  ====\nA different approach is to consider union of groups for variable selection. This approach captures the modeling situation where variables can be selected as long as they belong at least to one group with positive coefficients. This modeling perspective implies that we want to preserve group structure.\n\nThe formulation of the union of groups approach is also referred to as '''latent group Lasso''', and requires to modify the group <math>\\ell_2</math> norm considered above and introduce the following regularizer <ref name=\"latentLasso\" />\n: <math>R(w)=inf\\left\\{\\sum _{g}\\|w_{{g}}\\|_{{g}}:w=\\sum _{{g=1}}^{G}{\\bar  {w}}_{g}\\right\\}</math>\nwhere <math>w\\in {\\mathbb  {R^{d}}}</math>,  <math>w_{{g}}\\in G_{g}</math> is the vector of coefficients of group g, and <math>{\\bar  {w}}_{g}\\in {\\mathbb  {R^{d}}}</math> is a vector with coefficients <math>w_{g}^{j}</math> for all variables  <math>j</math>  in group  <math>g</math> , and  <math>0</math>  in all others, i.e., <math>{\\bar  w}_{g}^{j}=w_{g}^{j}</math> if  <math>j</math>  in group  <math>g</math>  and <math>{\\bar  w}_{g}^{j}=0</math> otherwise.\n\nThis regularizer can be interpreted as effectively replicating variables that belong to more than one group, therefore conserving group structure. As intended by the union of groups approach, requiring <math>w=\\sum _{{g=1}}^{G}{\\bar  {w}}_{g}</math> produces a vector of weights w that effectively sums up the weights of all variables across all groups they belong to.\n\n=== Issues with Group Lasso regularization and alternative approaches ===\nThe objective function using group lasso consists of an error function, which is generally required to be convex but not necessarily strongly convex, and a group <math>\\ell_1</math> regularization term.  An issue with this objective function is that it is convex but not necessarily strongly convex, and thus generally does not lead to unique solutions.<ref name=\":0\" />\n\nAn example of a way to fix this is to introduce the squared <math>\\ell_2</math> norm of the weight vector as an additional regularization term while keeping the <math>\\ell_1</math> regularization term from the group lasso approach.<ref name=\":0\" /> If the coefficient of the squared  <math>\\ell_2</math> norm term is greater than <math>0</math>, then because the squared  <math>\\ell_2</math> norm term is strongly convex, the resulting objective function will also be strongly convex.<ref name=\":0\" /> Provided that the  <math>\\ell_2</math> coefficient is suitably small but still positive, the weight vector minimizing the resulting objective function is generally very close to a weight vector that minimizes the objective function that would result from removing the group  <math>\\ell_2</math> regularization term altogether from the original objective function; the latter scenario corresponds to the group Lasso approach.<ref name=\":0\" /> Thus this approach allows for simpler optimization while maintaining sparsity.<ref name=\":0\" />\n\n=== Norms based on the structure over Input variables ===\n''See: [[Submodular set function]]''\n\nBesides the norms discussed above, other norms used in structured sparsity methods include hierarchical norms and norms defined on grids. These norms arise from submodular functions and allow the incorporation of prior assumptions on the structure of the input variables. In the context of hierarchical norms, this structure can be represented as a [[directed acyclic graph]] over the variables while in the context of grid-based norms, the structure can be represented using a grid.<ref name=\":2\" /><ref name=\":3\" /><ref name=\":4\" /><ref name=\":1\" /><ref name=\":5\" /><ref name=\":6\" />\n\n==== Hierarchical Norms ====\n''See:'' [[Unsupervised learning]]\n\nUnsupervised learning methods are often used to learn the parameters of [[latent variable model]]s. Latent variable models are statistical models where in addition to the observed variables, a set of latent variables also exists which is not observed. Often in such models, \"hierarchies\" are assumed between the variables of the system; this system of hierarchies can be represented using directed acyclic graphs.\n\nHierarchies of latent variables have emerged as a natural structure in several applications, notably to model text documents.<ref name=\":3\">Bengio, Y. \"Learning deep architectures for AI\". Foundations and Trends in Machine Learning, 2(1), 2009.</ref>  Hierarchical models using Bayesian non-parametric methods have been used to learn [[topic model]]s,<ref name=\":2\">Blei, D., Ng, A., and Jordan, M. Latent dirichlet allocation. J. Mach. Learn. Res., 3:993–1022, 2003.</ref> which are statistical models for discovering the abstract \"topics\" that occur in a collection of documents. Hierarchies have also been considered in the context of kernel methods.<ref name=\":1\">{{Cite journal|arxiv=0904.3523|last1=Jenatton|first1=Rodolphe|title=Structured Variable Selection with Sparsity-Inducing Norms|journal=Journal of Machine Learning Research |volume=12|issue=2011|pages=2777–2824|last2=Audibert|first2=Jean-Yves|last3=Bach|first3=Francis|year=2011|bibcode=2009arXiv0904.3523J}}</ref> Hierarchical norms have been applied to bioinformatics,<ref name=\":4\">S. Kim and E. Xing. Tree-guided group Lasso for multi-task regression with structured sparsity. In Proc. ICML, 2010.</ref> computer vision and topic models.<ref name=\":5\">R. Jenatton, J. Mairal, G. Obozinski, and F. Bach. Proximal methods for sparse hierarchical dictionary learning. In Proc. ICML, 2010.</ref>\n\n==== Norms defined on grids ====\nIf the structure assumed over variables is in the form of a 1D, 2D or 3D grid, then submodular functions based on overlapping groups can be considered as norms, leading to stable sets equal to rectangular or convex shapes.<ref name=\":1\" /> Such methods have applications in computer vision<ref name=\":6\">R. Jenatton, G. Obozinski, and F. Bach. Structured sparse principal component analysis. In ''Proc. AISTATS'', 2009.</ref>\n\n== Algorithms for computation ==\n\n=== Best subset selection problem ===\nThe problem of choosing the best subset of input variables can be naturally formulated under a penalization framework as:<ref name=\"LR18\">L. Rosasco. Lecture 10 of the Lecture Notes for 9.520: Statistical Learning Theory and Applications. Massachusetts Institute of Technology, Fall 2014. Available at http://www.mit.edu/~9.520/fall14/slides/class18/class18_sparsity.pdf</ref>\n: <math>\\min_{w\\in\\mathbb{R}^d} \\frac{1}{n}\\sum_{i=1}^n V(y_i, w, x_i)  + \\lambda \\|w\\|_0, </math>\nWhere <math>\\|w\\|_0</math> denotes the <math>\\ell_0</math> \"norm\", defined as the number of nonzero entries of the vector <math>w</math>.\n\nAlthough this formulation makes sense from a modeling perspective, it is computationally unfeasible, as it is equivalent to an exhaustive search evaluating all possible subsets of variables.<ref name=\"LR18\" />\n\nTwo main approaches for solving the optimization problem are: 1) greedy methods, such as [[Stepwise regression|step-wise regression]] in statistics, or [[matching pursuit]] in [[signal processing]]; and 2) convex relaxation formulation approaches and [[Proximal gradient methods for learning|proximal gradient]] optimization methods.\n\n=== Convex relaxation ===\nA natural approximation for the best subset selection problem is the <math>\\ell_1</math> norm regularization:<ref name=\"LR18\" />\n: <math> \\min_{w\\in\\mathbb{R}^d}  \\frac{1}{n}\\sum_{i=1}^n V(y_i, w, x_i) + \\lambda \\|w\\|_1</math>\nSuch as scheme is called [[basis pursuit]] or the [[Lasso (statistics)|Lasso]], which substitutes the <math>\\ell_0</math> \"norm\" for the convex, non-differentiable <math>\\ell_1</math> norm.\n\n=== Proximal gradient methods ===\n{{Main article|Proximal gradient methods for learning|l1=Proximal gradient methods}}\n\n[[Proximal gradient methods for learning|Proximal gradient methods]], also called forward-backward splitting, are optimization methods useful for minimizing functions with a [[Convex function|convex]] and [[Differentiable function|differentiable]] component, and a convex potentially non-differentiable component.\n\nAs such, proximal gradient methods are useful for solving sparsity and structured sparsity regularization problems<ref name=\":0\">{{cite arXiv|last = Villa|first = S.|author2 = Rosasco, L.|author3 = Mosci, S.|author4 = Verri, A.|title = Proximal methods for the latent group lasso penalty|year = 2012|eprint = 1209.0368|class = math.OC}}</ref> of the following form: \n: <math> \\min_{w\\in\\mathbb{R}^d}  \\frac{1}{n}\\sum_{i=1}^n V(y_i, w, x_i) + R(w)</math>\nWhere <math>V(y_i, w, x_i) </math> is a convex and differentiable [[loss function]] like the [[Loss function#Quadratic loss function|quadratic loss]], and <math>R(w) </math> is a convex potentially non-differentiable regularizer such as the <math>\\ell_1</math> norm.\n\n== Connections to Other Areas of Machine Learning ==\n\n=== Connection to Multiple Kernel Learning ===\n{{main article|Multiple kernel learning}}\n\nStructured Sparsity regularization can be applied in the context of [[multiple kernel learning]].<ref name=\":7\">{{Cite journal|url = |title = MIT 9.520 course notes Fall 2015, chapter 6|last = Rosasco|first = Lorenzo|date = Fall 2015|journal = |doi = |pmid = |access-date = |last2 = Poggio, Tomaso}}</ref> Multiple kernel learning refers to a set of machine learning methods that use a predefined set of kernels and learn an optimal linear or non-linear combination of kernels as part of the algorithm.\n\nIn the algorithms mentioned above, a whole space was taken into consideration at once and was partitioned into groups, i.e. subspaces. A complementary point of view is to consider the case in which distinct spaces are combined to obtain a new one. It is useful to discuss this idea considering finite dictionaries. Finite dictionaries with linearly independent elements - these elements are also known as atoms - refer to finite sets of linearly independent basis functions, the linear combinations of which define hypothesis spaces. Finite dictionaries can be used to define specific kernels, as will be shown.<ref name=\":7\" /> Assume for this example that rather than only one dictionary, several finite dictionaries are considered.\n\nFor simplicity, the case in which there are only two dictionaries <math>A = \\{a_j: X \\rightarrow \\R, j=1,...,p\\} </math> and <math>B = \\{b_t: X \\rightarrow \\R, t=1,...,q\\} </math> where <math>q</math> and <math>p</math> are integers, will be considered. The atoms in <math>A</math> as well as the atoms in <math>B</math> are assumed to be linearly independent. Let <math>D = \\{d_k: X \\rightarrow \\R, k=1,...,p+q\\} = A \\cup B </math> be the union of the two dictionaries. Consider the linear space of functions <math>H </math> given by linear combinations of the form\n\n<math>f(x) = \\sum_{i=1}^{p+q}{w^j d_j(x)} = \\sum_{j=1}^{p}{w_A^j a_j(x)} + \\sum_{t=1}^{q}{w_B^t b_t(x)}, x \\in X </math>\n\nfor some coefficient vectors <math>w_A \\in \\R^p, w_B \\in \\R^q </math>, where <math>w=(w_A,w_B) </math>. Assume the atoms in <math>D </math> to still be linearly independent, or equivalently, that the map <math>w = (w_A, w_B) \\mapsto f </math> is one to one. The functions in the space <math>H </math> can be seen as the sums of two components, one in the space <math>H_A </math>, the linear combinations of atoms in  <math>A</math> and one in <math>H_B </math>, the linear combinations of the atoms in <math>B</math>.\n\nOne choice of norm on this space is <math>||f|| = ||w_A|| + ||w_B|| </math>. Note that we can now view <math>H </math> as a function space in which  <math>H_A </math>,  <math>H_B </math> are subspaces. In view of the linear independence assumption, <math>H </math> can be identified with <math>\\R^{p+q} </math> and <math>H_A, H_B </math> with <math>\\R^p, \\R^q </math> respectively. The norm mentioned above can be seen as the group norm in  <math>H </math>associated to the subspaces  <math>H_A </math>,  <math>H_B </math>, providing a connection to structured sparsity regularization.\n\nHere, <math>H_A </math>,  <math>H_B </math> and <math>H </math> can be seen to be the reproducing kernel Hilbert spaces with corresponding feature maps <math>\\Phi_A : X \\rightarrow \\R^p </math>, given by <math>\\Phi_A(x) = (a_1(x),...,a_p(x)) </math>, <math>\\Phi_B : X \\rightarrow \\R^q </math>, given by <math>\\Phi_B(x) = (b_1(x),...,b_q(x)) </math>, and <math>\\Phi: X \\rightarrow \\R^{p+q} </math>, given by the concatenation of <math>\\Phi_A, \\Phi_B </math>, respectively.\n\nIn the structured sparsity regularization approach to this scenario, the relevant groups of variables which the group norms consider correspond to the subspaces <math>H_A </math> and <math>H_B </math>. This approach promotes setting the groups of coefficients corresponding to these subspaces to zero as opposed to only individual coefficients, promoting sparse multiple kernel learning.\n\nThe above reasoning directly generalizes to any finite number of dictionaries, or feature maps. It can be extended to feature maps inducing infinite dimensional hypothesis\n\nspaces.<ref name=\":7\" />\n\n==== When Sparse Multiple Kernel Learning is useful ====\nConsidering sparse multiple kernel learning is useful in several situations including the following:\n\n• Data fusion: When each kernel corresponds to a different kind of modality/feature.\n\n• Nonlinear variable selection: Consider kernels <math>K_g</math> depending only one dimension of the input.\n\nGenerally sparse multiple kernel learning is particularly useful when there are many kernels and model selection and interpretability are important.<ref name=\":7\" />\n\n== Additional uses and applications ==\nStructured sparsity regularization methods have been used in a number of settings where it is desired to impose an ''a priori'' input variable structure to the regularization process. Some such applications are:\n* [[Compressed sensing|Compressive sensing]] in [[magnetic resonance imaging]] (MRI), reconstructing MR images from a small number of measurements, potentially yielding significant reductions in MR scanning time<ref name=\"MRI\" />\n* Robust [[Facial recognition system|face recognition]] in the presence of misalignment, occlusion and illumination variation<ref name=\"face_recognition\" />\n* Uncovering [[Sociolinguistics|socio-linguistic]] associations between lexical frequencies used by Twitter authors, and the socio-demographic variables of their geographic communities<ref name=\"sociolinguistic\" />\n* Gene selection analysis of breast cancer data using priors of overlapping groups, e.g., biologically meaningful gene sets<ref name=\"genetic\" />\n\n== See also ==\n* [[Statistical learning theory]]\n* [[Regularization (mathematics)#Regularization in statistics and machine learning|Regularization]]\n* [[Sparse approximation]]\n* [[Proximal gradient method]]s\n* [[Convex analysis]]\n* [[Feature selection]]\n\n== References ==\n{{reflist}}\n\n[[Category:Machine learning]]\n[[Category:First order methods|First order methods]]\n[[Category:Convex optimization]]"
    },
    {
      "title": "Frank–Wolfe algorithm",
      "url": "https://en.wikipedia.org/wiki/Frank%E2%80%93Wolfe_algorithm",
      "text": "The '''Frank–Wolfe algorithm''' is an [[iterative method|iterative]] [[First-order approximation|first-order]] [[Mathematical optimization|optimization]] [[algorithm]] for [[constrained optimization|constrained]] [[convex optimization]]. Also known as the '''conditional gradient method''',<ref>{{Cite journal | last1 = Levitin | first1 = E. S. | last2 = Polyak | first2 = B. T. | doi = 10.1016/0041-5553(66)90114-5 | title = Constrained minimization methods | journal = USSR Computational Mathematics and Mathematical Physics | volume = 6 | issue = 5 | pages = 1 | year = 1966 | pmid =  | pmc = }}</ref> '''reduced gradient algorithm''' and the '''convex combination algorithm''', the method was originally proposed by [[Marguerite Frank]] and [[Philip Wolfe (mathematician)|Philip Wolfe]] in&nbsp;1956.<ref>{{Cite journal | last1 = Frank | first1 = M. | last2 = Wolfe | first2 = P. | doi = 10.1002/nav.3800030109 | title = An algorithm for quadratic programming | journal = Naval Research Logistics Quarterly | volume = 3 | issue = 1–2 | pages = 95–110 | year = 1956 | pmid =  | pmc = }}</ref> In each iteration, the Frank–Wolfe algorithm considers a [[linear approximation]] of the objective function, and moves towards a minimizer of this linear function (taken over the same domain).\n\n==Problem statement==\n\nSuppose <math>\\mathcal{D}</math> is a [[compact space|compact]] [[convex set]] in a [[vector space]] and <math>f \\colon \\mathcal{D} \\to \\mathbb{R}</math> is a [[convex function|convex]] [[differentiable function|differentiable]] [[real-valued function]]. The Frank–Wolfe algorithm solves the [[optimization problem]]\n:Minimize <math> f(\\mathbf{x})</math>\n:subject to <math> \\mathbf{x} \\in \\mathcal{D}</math>.\n\n==Algorithm==\n[[File:Frank-Wolfe Algorithm.png|thumbnail|right|A step of the Frank–Wolfe algorithm]]\n\n:''Initialization:'' Let <math>k \\leftarrow 0</math>, and let <math>\\mathbf{x}_0 \\!</math> be any point in <math>\\mathcal{D}</math>.\n\n:'''Step 1.'''  ''Direction-finding subproblem:'' Find <math>\\mathbf{s}_k</math> solving\n::Minimize <math>  \\mathbf{s}^T \\nabla f(\\mathbf{x}_k)</math>\n::Subject to <math>\\mathbf{s} \\in \\mathcal{D}</math>\n:''(Interpretation: Minimize the linear approximation of the problem given by the first-order [[Taylor series|Taylor approximation]] of <math>f</math> around <math>\\mathbf{x}_k \\!</math>.)''\n\n:'''Step 2.'''  ''Step size determination:'' Set <math>\\gamma \\leftarrow \\frac{2}{k+2}</math>, or alternatively find <math>\\gamma</math> that minimizes <math> f(\\mathbf{x}_k+\\gamma(\\mathbf{s}_k -\\mathbf{x}_k))</math> subject to <math>0 \\le \\gamma \\le 1</math> .\n\n:'''Step 3.'''  ''Update:''  Let <math>\\mathbf{x}_{k+1}\\leftarrow \\mathbf{x}_k+\\gamma(\\mathbf{s}_k-\\mathbf{x}_k)</math>, let <math>k \\leftarrow k+1</math> and go to Step 1.\n\n==Properties==\nWhile competing methods such as [[gradient descent]] for constrained optimization require a [[Projection (mathematics)|projection step]] back to the feasible set in each iteration, the Frank–Wolfe algorithm only needs the solution of a linear problem over the same set in each iteration, and automatically stays in the feasible set.\n\nThe convergence of the Frank–Wolfe algorithm is sublinear in general: the error in the objective function to the optimum is <math>O(1/k)</math> after ''k'' iterations, so long as the gradient is [[Lipschitz continuity|Lipschitz continuous]] with respect to some norm. The same convergence rate can also be shown if the sub-problems are only solved approximately.<ref>{{Cite journal | last1 = Dunn | first1 = J. C. | last2 = Harshbarger | first2 = S. | doi = 10.1016/0022-247X(78)90137-3 | title = Conditional gradient algorithms with open loop step size rules | journal = Journal of Mathematical Analysis and Applications | volume = 62 | issue = 2 | pages = 432 | year = 1978 | pmid =  | pmc = }}</ref>\n\nThe iterates of the algorithm can always be represented as a sparse convex combination of the extreme points of the feasible set, which has helped to the popularity of the algorithm for sparse greedy optimization in [[machine learning]] and [[signal processing]] problems,<ref>{{Cite journal | last1 = Clarkson | first1 = K. L. | title = Coresets, sparse greedy approximation, and the Frank-Wolfe algorithm | doi = 10.1145/1824777.1824783 | journal = ACM Transactions on Algorithms | volume = 6 | issue = 4 | pages = 1–30 | year = 2010 | pmid =  | pmc = | citeseerx = 10.1.1.145.9299 }}</ref> as well as for example the optimization of [[flow network|minimum–cost flow]]s in [[Transport network|transportation network]]s.<ref>{{Cite journal | last1 = Fukushima | first1 = M. | title = A modified Frank-Wolfe algorithm for solving the traffic assignment problem | doi = 10.1016/0191-2615(84)90029-8 | journal = Transportation Research Part B: Methodological | volume = 18 | issue = 2 | pages = 169–177| year = 1984 | pmid =  | pmc = }}</ref>\n\nIf the feasible set is given by a set of linear constraints, then the subproblem to be solved in each iteration becomes a [[linear programming|linear program]].\n\nWhile the worst-case convergence rate with <math>O(1/k)</math> can not be improved in general, faster convergence can be obtained for special problem classes, such as some strongly convex problems.<ref>{{Cite book|title=Nonlinear Programming|first= Dimitri |last=Bertsekas|year= 1999|page= 215|publisher=  Athena Scientific| isbn =978-1-886529-00-7}}</ref>\n\n==Lower bounds on the solution value, and primal-dual analysis==\n\nSince <math>f</math> is [[Convex function|convex]], for any two points <math>\\mathbf{x}, \\mathbf{y} \\in \\mathcal{D}</math> we have:\n\n:<math>\n  f(\\mathbf{y}) \\geq f(\\mathbf{x}) +  (\\mathbf{y} - \\mathbf{x})^T \\nabla f(\\mathbf{x})\n</math>\n\nThis also holds for the (unknown) optimal solution <math>\\mathbf{x}^*</math>. That is, <math>f(\\mathbf{x}^*) \\ge f(\\mathbf{x}) +  (\\mathbf{x}^* - \\mathbf{x})^T \\nabla f(\\mathbf{x})</math>. The best lower bound with respect to a given point <math>\\mathbf{x}</math> is given by\n\n:<math>\n  \\begin{align}\nf(\\mathbf{x}^*) \n& \\ge f(\\mathbf{x}) +  (\\mathbf{x}^* - \\mathbf{x})^T \\nabla f(\\mathbf{x}) \\\\ \n&\\geq \\min_{\\mathbf{y} \\in D} \\left\\{ f(\\mathbf{x}) +  (\\mathbf{y} - \\mathbf{x})^T \\nabla f(\\mathbf{x}) \\right\\} \\\\\n&= f(\\mathbf{x}) - \\mathbf{x}^T \\nabla f(\\mathbf{x}) + \\min_{\\mathbf{y} \\in D} \\mathbf{y}^T \\nabla f(\\mathbf{x})\n\\end{align}\n</math>\n\nThe latter optimization problem is solved in every iteration of the Frank–Wolfe algorithm, therefore the solution <math>\\mathbf{s}_k</math> of the direction-finding subproblem of the <math>k</math>-th iteration can be used to determine increasing lower bounds <math>l_k</math> during each iteration by setting <math>l_0 = - \\infty</math> and\n\n:<math>\n  l_k := \\max (l_{k - 1}, f(\\mathbf{x}_k) +  (\\mathbf{s}_k - \\mathbf{x}_k)^T \\nabla f(\\mathbf{x}_k))\n</math>\nSuch lower bounds on the unknown optimal value are important in practice because they can be used as a stopping criterion, and give an efficient certificate of the approximation quality in every iteration, since always <math>l_k \\leq f(\\mathbf{x}^*) \\leq f(\\mathbf{x}_k)</math>.\n\nIt has been shown that this corresponding [[duality gap]], that is the difference between <math>f(\\mathbf{x}_k)</math> and the lower bound <math>l_k</math>, decreases with the same convergence rate, i.e.\n<math>\n  f(\\mathbf{x}_k) - l_k = O(1/k) .\n</math>\n\n==Notes==\n{{Reflist}}\n\n==Bibliography==\n*{{cite journal|last=Jaggi|first=Martin|title=Revisiting Frank–Wolfe: Projection-Free Sparse Convex Optimization|journal=Journal of Machine Learning Research: Workshop and Conference Proceedings |volume=28|issue=1|pages=427–435|year= 2013 |url=http://jmlr.csail.mit.edu/proceedings/papers/v28/jaggi13.html}} (Overview paper)\n*[http://www.math.chalmers.se/Math/Grundutb/CTH/tma946/0203/fw_eng.pdf The Frank–Wolfe algorithm] description\n* {{Cite book | last1=Nocedal | first1=Jorge | last2=Wright | first2=Stephen J. | title=Numerical Optimization | publisher=[[Springer-Verlag]] | location=Berlin, New York | edition=2nd | isbn=978-0-387-30303-1 | year=2006 | ref=harv | postscript=<!--None-->}}.\n\n==External links==\n*[https://www.youtube.com/watch?v=24e08AX9Eww Marguerite Frank giving a personal account of the history of the algorithm]\n\n== See also ==\n* [[Proximal gradient methods]]\n\n{{Optimization algorithms|convex}}\n\n{{DEFAULTSORT:Frank-Wolfe algorithm}}\n[[Category:Optimization algorithms and methods]]\n[[Category:Iterative methods]]\n[[Category:First order methods]]\n[[Category:Gradient methods]]"
    },
    {
      "title": "Gradient descent",
      "url": "https://en.wikipedia.org/wiki/Gradient_descent",
      "text": "{{About||the [[mathematical analysis|analytical]] method called \"steepest descent\"|Method of steepest descent}}\n\n'''Gradient descent''' is a [[:Category:First order methods|first-order]] [[Iterative algorithm|iterative]] [[Mathematical optimization|optimization]] [[algorithm]] for finding the minimum of a function. To find a [[local minimum]] of a function using gradient descent, one takes steps proportional to the ''negative'' of the [[gradient]] (or approximate gradient) of the function at the current point. If, instead, one takes steps proportional to the ''positive'' of the gradient, one approaches a [[local maximum]] of that function; the procedure is then known as '''gradient ascent'''.\n\nGradient descent is also known as '''steepest descent'''. However, gradient descent should not be confused with the [[method of steepest descent]] for approximating integrals.\n\n==Description==\n[[File:gradient descent.svg|thumb|350px|Illustration of gradient descent on a series of [[level set]]s.]]\n\nGradient descent is based on the observation that if the [[multi-variable function]] <math>F(\\mathbf{x})</math> is [[Defined and undefined|defined]] and [[Differentiable function|differentiable]] in a neighborhood of a point <math>\\mathbf{a}</math>, then <math>F(\\mathbf{x})</math> decreases ''fastest'' if one goes from <math>\\mathbf{a}</math> in the direction of the negative gradient of <math>F</math> at <math>\\mathbf{a}, -\\nabla F(\\mathbf{a})</math>. It follows that, if\n\n:<math> \\mathbf{a}_{n+1} = \\mathbf{a}_n-\\gamma\\nabla F(\\mathbf{a}_n)</math>\n\nfor <math>\\gamma \\in \\R_{+}</math> small enough, then  <math>F(\\mathbf{a_n})\\geq F(\\mathbf{a_{n+1}})</math>. In other words, the term <math>\\gamma\\nabla F(\\mathbf{a})</math> is subtracted from <math>\\mathbf{a}</math> because we want to move against the gradient, toward the minimum. With this observation in mind, one starts with a guess <math>\\mathbf{x}_0</math> for a local minimum of <math>F</math>, and considers the sequence <math>\\mathbf{x}_0, \\mathbf{x}_1, \\mathbf{x}_2, \\ldots</math> such that\n\n:<math>\\mathbf{x}_{n+1}=\\mathbf{x}_n-\\gamma_n \\nabla F(\\mathbf{x}_n),\\ n \\ge 0.</math>\n\nWe have a [[Monotonic function|monotonic]] sequence\n\n:<math>F(\\mathbf{x}_0)\\ge F(\\mathbf{x}_1)\\ge F(\\mathbf{x}_2)\\ge \\cdots,</math>\n\nso hopefully the sequence <math>(\\mathbf{x}_n)</math> converges to the desired local minimum. Note that the value of the ''step size'' <math>\\gamma</math> is allowed to change at every iteration.  With certain assumptions on the function <math>F</math> (for example, <math>F</math> [[Convex function|convex]] and <math>\\nabla F</math> [[Lipschitz continuity|Lipschitz]]) and particular choices of <math>\\gamma</math> (e.g., chosen either via a [[line search]] that satisfies the [[Wolfe conditions]] or the Barzilai-Borwein<ref>{{cite journal |first=Jonathan |last=Barzilai |first2=Jonathan M. |last2=Borwein |title=Two-Point Step Size Gradient Methods |journal=IMA Journal of Numerical Analysis |volume=8 |issue=1 |year=1988 |pages=141–148 |doi=10.1093/imanum/8.1.141 }}</ref> method shown as following), \n\n:<math>\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}</math>\n\n[[convergent series|convergence]] to a local minimum can be guaranteed. When the function <math>F</math> is [[Convex function|convex]], all local minima are also global minima, so in this case gradient descent can converge to the global solution.\n\nThis process is illustrated in the adjacent picture. Here <math>F</math> is assumed to be defined on the plane, and that its graph has a [[Bowl (vessel)|bowl]] shape.  The blue curves are the [[contour line]]s, that is, the regions on which the value of <math>F</math> is constant. A red arrow originating at a point shows the direction of the negative gradient at that point. Note that the (negative) gradient at a point is [[orthogonal]] to the contour line going through that point. We see that gradient ''descent'' leads us to the bottom of the bowl, that is, to the point where the value of the function <math>F</math> is minimal.\n\n===Examples===\nGradient descent has problems with pathological functions such as the [[Rosenbrock function]] shown here. \n\n: <math>f(x_1, x_2) = (1-x_1)^2 + 100(x_2-{x_1}^2)^2.</math>\n\nThe Rosenbrock function has a narrow curved valley which contains the minimum. The bottom of the valley is very flat. Because of the curved flat valley the optimization is zigzagging slowly with small step sizes towards the minimum.\n\n[[File:Banana-SteepDesc.gif|400px]]\n\nThe zigzagging nature of the method is also evident below, where the gradient descent method is applied to \n\n:<math>F(x,y)=\\sin\\left(\\frac{1}{2} x^2 - \\frac{1}{4} y^2 + 3 \\right)\\cos\\left(2 x+1-e^y\\right).</math>\n\n{|\n[[File:gradient ascent (contour).png|350px|The gradient descent algorithm in action. (1: contour)]]\n[[File:gradient ascent (surface).png|450px|The gradient descent algorithm in action. (2: surface)]]\n|}\n\n===Limitations===\nFor some of the above examples, gradient descent is relatively slow close to the minimum: technically, its asymptotic rate of convergence is inferior to many other methods. For poorly conditioned convex problems, gradient descent increasingly 'zigzags' as the gradients point nearly orthogonally to the shortest direction to a minimum point. For more details, see the {{Section link||Comments|nopage=y}} below.\n\nFor non-differentiable functions, gradient methods are ill-defined. For locally [[Lipschitz continuity|Lipschitz]] problems and especially for convex minimization problems, [[subgradient method|bundle methods of descent]] are well-defined. Non-descent methods, like [[Subgradient method|subgradient]] projection methods, may also be used.<ref>{{Cite news|last=Kiwiel|first=Krzysztof C.| title= Convergence and efficiency of subgradient methods for quasiconvex minimization|journal=Mathematical Programming, Series A| publisher =Springer|location=Berlin, Heidelberg|issn=0025-5610|pages=1–25| volume=90 |issue=1 |doi=10.1007/PL00011414 |year=2001 |mr=1819784}}</ref> These methods are typically slower than gradient descent. Another alternative for non-differentiable functions is to \"smooth\" the function, or bound the function by a smooth function. In this approach, the smooth problem is solved in the hope that the answer is close to the answer for the non-smooth problem (occasionally, this can be made rigorous).\n\n==Solution of a linear system==\n\n[[File:Steepest descent.png|thumb|380px| The Steepest-Descent Algorithm Applied to the [[Wiener filter]]<ref>Haykin, Simon S. Adaptive filter theory. Pearson Education India, 2008. - p. 108-142, 217-242</ref>.]]\n\nGradient descent can be used to solve a system of linear equations, reformulated as a quadratic minimization problem, e.g., using [[linear least squares]]. The solution of \n\n:<math>A\\mathbf{x}-\\mathbf{b}=0</math>\n\nin the sense of linear least squares is defined as minimizing the function \n\n:<math>F(\\mathbf{x})=\\left\\|A\\mathbf{x}-\\mathbf{b}\\right\\|^2.</math>\n\nIn traditional linear least squares for real <math>A</math> and <math>\\mathbf{b}</math> the [[Euclidean norm]] is used, in which case \n\n:<math>\\nabla F(\\mathbf{x})=2A^T(A\\mathbf{x}-\\mathbf{b}).</math>\n\nIn this case, the [[line search]] minimization, finding the locally optimal step size <math>\\gamma</math> on every iteration, can be performed analytically, and explicit formulas for the locally optimal <math>\\gamma</math> are known.<ref>{{cite journal\n | last = Yuan | first = Ya-xiang\n | title = Step-sizes for the gradient method\n | journal = AMS/IP Studies in Advanced Mathematics \n | volume = 42\n | issue = 2\n | pages = 785\n | year = 1999\n | url = ftp://lsec.cc.ac.cn/pub/yyx/papers/p0504.pdf\n}}</ref>\n\nFor solving linear equations, this algorithm is rarely used, with the [[conjugate gradient method]] being one of the most popular alternatives. The speed of convergence of gradient descent depends on the ratio of the maximum to minimum [[eigenvalues]] of <math>A^TA</math>, while the speed of convergence of [[conjugate gradient method|conjugate gradients]] has a more complex dependence on the eigenvalues, and can benefit from [[Preconditioner|preconditioning]]. Gradient descent also benefits from preconditioning, but this is not done as commonly.{{Why?|date=October 2018}}\n\n==Solution of a non-linear system==\n\nGradient descent can also be used to solve a system of [[nonlinear equation]]s. Below is an example that shows how to use the gradient descent to solve for three unknown variables, ''x''<sub>1</sub>, ''x''<sub>2</sub>, and ''x''<sub>3</sub>. This example shows one iteration of the gradient descent.\n\nConsider the nonlinear system of equations\n\n:<math> \\begin{cases}\n3x_1-\\cos(x_2x_3)-\\tfrac{3}{2} =0 \\\\\n4x_1^2-625x_2^2+2x_2-1 = 0  \\\\\n\\exp(-x_1x_2)+20x_3+\\tfrac{10\\pi-3}{3} =0\n\\end{cases}</math>\n\nLet us introduce the associated function\n\n:<math>G(\\mathbf{x}) = \\begin{bmatrix}\n3x_1-\\cos(x_2x_3)-\\tfrac{3}{2} \\\\\n4x_1^2-625x_2^2+2x_2-1 \\\\\n\\exp(-x_1x_2)+20x_3+\\tfrac{10\\pi-3}{3} \\\\\n\\end{bmatrix}, </math>\n\nwhere\n\n:<math> \\mathbf{x} =\\begin{bmatrix}\n  x_1 \\\\\n  x_2 \\\\\n  x_3 \\\\\n\\end{bmatrix}.</math>\n\nOne might now define the objective function\n\n:<math>F(\\mathbf{x}) = \\frac{1}{2} G^\\mathrm{T}(\\mathbf{x}) G(\\mathbf{x}) =\\frac{1}{2} \\left[ \\left (3x_1-\\cos(x_2x_3)-\\frac{3}{2} \\right)^2  + \\left(4x_1^2-625x_2^2+2x_2-1 \\right)^2 + \\left(\\exp(-x_1x_2) + 20x_3 + \\frac{10\\pi-3}{3} \\right)^2 \\right],</math>\n\nwhich we will attempt to minimize. As an initial guess, let us use\n\n:<math> \\mathbf{x}^{(0)}= \\mathbf{0} = \\begin{bmatrix}\n  0 \\\\\n  0 \\\\\n  0 \\\\ \\end{bmatrix}.</math>\n\nWe know that\n\n:<math>\\mathbf{x}^{(1)}=\\mathbf{0}-\\gamma_0 \\nabla F(\\mathbf{0}) = \\mathbf{0}-\\gamma_0 J_G(\\mathbf{0})^\\mathrm{T} G(\\mathbf{0}),</math>\n\nwhere the [[Jacobian matrix]] <math>J_G</math> is given by\n\n:<math>J_G(\\mathbf{x}) = \\begin{bmatrix}\n  3 & \\sin(x_2x_3)x_3 & \\sin(x_2x_3)x_2   \\\\\n  8x_1 & -1250x_2+2 & 0 \\\\\n  -x_2\\exp{(-x_1x_2)} & -x_1\\exp(-x_1x_2) & 20\\\\\n\\end{bmatrix}.</math>\n\nWe calculate:\n\n:<math>J_G(\\mathbf{0}) = \\begin{bmatrix}\n  3 & 0 & 0\\\\\n  0 & 2 & 0\\\\\n  0 & 0 & 20\n\\end{bmatrix}, \\qquad G(\\mathbf{0}) = \\begin{bmatrix}\n  -2.5\\\\\n  -1\\\\\n  10.472\n\\end{bmatrix}.</math>\n\nThus\n\n:<math>\\mathbf{x}^{(1)}= \\mathbf{0}-\\gamma_0 \\begin{bmatrix}\n  -7.5\\\\\n  -2\\\\\n  209.44\n\\end{bmatrix},</math>\n\nand\n\n:<math>F(\\mathbf{0}) = 0.5 \\left( (-2.5)^2 + (-1)^2 + (10.472)^2 \\right) = 58.456.</math>\n\n[[File:Gradient Descent Example Nonlinear Equations.gif|thumb|right|350px|An animation showing the first 83 iterations of gradient descent applied to this example. Surfaces are [[isosurface]]s of <math>F(\\mathbf{x}^{(n)})</math> at current guess <math>\\mathbf{x}^{(n)}</math>, and arrows show the direction of descent. Due to a small and constant step size, the convergence is slow.]]\n\nNow, a suitable <math>\\gamma_0</math> must be found such that \n\n:<math>F\\left (\\mathbf{x}^{(1)}\\right ) \\le F\\left (\\mathbf{x}^{(0)}\\right ) = F(\\mathbf{0}).</math> \n\nThis can be done with any of a variety of [[line search]] algorithms. One might also simply guess <math>\\gamma_0=0.001,</math> which gives\n\n:<math> \\mathbf{x}^{(1)}=\\begin{bmatrix}\n   0.0075  \\\\\n   0.002   \\\\\n  -0.20944 \\\\\n\\end{bmatrix}.</math>\n\nEvaluating the objective function at this value, yields\n\n:<math>F \\left (\\mathbf{x}^{(1)}\\right ) = 0.5 \\left ((-2.48)^2 + (-1.00)^2 + (6.28)^2 \\right ) = 23.306.</math>\n\nThe decrease from <math>F(\\mathbf{0})=58.456</math> to the next step's value of \n\n:<math> F\\left (\\mathbf{x}^{(1)}\\right ) =23.306 </math> \n\nis a sizable decrease in the objective function. Further steps would reduce its value further, until an approximate solution to the system was found.\n\n==Comments==\n\nGradient descent works in spaces of any number of dimensions, even in infinite-dimensional ones. In the latter case, the search space is typically a [[function space]], and one calculates the [[Fréchet derivative]] of the functional to be minimized to determine the descent direction.<ref>{{cite book |first=G. P. |last=Akilov |first2=L. V. |last2=Kantorovich |authorlink2=Leonid Kantorovich |title=Functional Analysis |publisher=Pergamon Press |edition=2nd |isbn=0-08-023036-9 |year=1982 }}</ref>\n\nThe gradient descent can take many iterations to compute a local minimum with a required [[accuracy]], if the [[curvature]] in different directions is very different for the given function. For such functions, [[preconditioning]], which changes the geometry of the space to shape the function level sets like [[concentric circles]], cures the slow convergence. Constructing and applying preconditioning can be computationally expensive, however.\n\nThe gradient descent can be combined with a [[line search]], finding the locally optimal step size <math>\\gamma</math> on every iteration. Performing the line search can be time-consuming. Conversely, using a fixed small <math>\\gamma</math> can yield poor convergence.\n\nMethods based on [[Newton's method in optimization|Newton's method]] and inversion of the [[Hessian matrix|Hessian]] using [[conjugate gradient]] techniques can be better alternatives.<ref>{{cite book |first=W. H. |last=Press |first2=S. A. |last2=Teukolsky |first3=W. T. |last3=Vetterling |first4=B. P. |last4=Flannery |title=Numerical Recipes in C: The Art of Scientific Computing |edition=2nd |publisher=Cambridge University Press |location=New York |year=1992 |isbn=0-521-43108-5 }}</ref><ref>{{cite book |first=T. |last=Strutz |title=Data Fitting and Uncertainty: A Practical Introduction to Weighted Least Squares and Beyond |edition=2nd |publisher=Springer Vieweg |year=2016 |isbn=978-3-658-11455-8 }}</ref> Generally, such methods converge in fewer iterations, but the cost of each iteration is higher. An example is the [[Broyden–Fletcher–Goldfarb–Shanno algorithm|BFGS method]] which consists in calculating on every step a matrix by which the gradient vector is multiplied to go into a \"better\" direction, combined with a more sophisticated [[line search]] algorithm, to find the \"best\" value of <math>\\gamma.</math> For extremely large problems, where the computer-memory issues dominate, a limited-memory method such as [[Limited-memory BFGS|L-BFGS]] should be used instead of BFGS or the steepest descent.\n\nGradient descent can be viewed as applying [[Euler's method]] for solving [[ordinary differential equations]] <math>x'(t)=-\\nabla f(x(t))</math> to a [[gradient flow]].\n\n== Computational examples ==\n\nThe following code examples apply the gradient descent algorithm to find the minimum of the function <math>f(x) = x^4-3x^3 + 2,</math> with derivative <math>f'(x) = 4x^{3} - 9x^{2}.</math>\n\nSolving for <math>4x^{3} - 9x^{2} = 0</math> and evaluation of the second derivative at the solutions shows the function has a plateau point at 0 and a global minimum at <math>x = \\tfrac{9}{4}.</math>\n\n=== Python ===\nHere is an implementation in the [[Python (programming language)|Python programming language]].\n\n<source lang=python>\nnext_x = 6  # We start the search at x=6\ngamma = 0.01  # Step size multiplier\nprecision = 0.00001  # Desired precision of result\nmax_iters = 10000  # Maximum number of iterations\n\n# Derivative function\ndf = lambda x: 4 * x**3 - 9 * x**2\n\nfor i in range(max_iters):\n    current_x = next_x\n    next_x = current_x - gamma * df(current_x)\n    step = next_x - current_x\n    if abs(step) <= precision:\n        break\n\nprint(\"Minimum at\", next_x)\n\n# The output for the above will be something like\n# \"Minimum at 2.2499646074278457\"\n</source>\n\n=== Scala ===\nHere is an implementation in the [[Scala (programming language)|Scala programming language]]\n\n<source lang=\"scala\">\nimport math._\n\nval curX = 6\nval gamma = 0.01 \nval precision = 0.00001 \nval previousStepSize = 1 / precision\n\ndef df(x: Double): Double = 4 * pow(x, 3) - 9 * pow(x, 2)\n\ndef gradientDescent(precision: Double, previousStepSize: Double, curX: Double): Double = {\n\tif (previousStepSize > precision) {\n\t\t val newX = curX + -gamma * df(curX)\t\n\t\t println(curX)\n\t\t gradientDescent(precision, abs(newX - curX), newX)\n\t} else curX\t \n}\n\nval ans = gradientDescent(precision, previousStepSize, curX)\nprintln(s\"The local minimum occurs at $ans\")\n\n</source>\n\n=== Clojure ===\nThe following is a [[Clojure (programming language)|Clojure]] implementation:\n\n<source lang=\"clojure\">\n\n(defn df [x] (- (* 4 x x x) (* 9 x x)))\n\n(defn gradient-descent [thresh iters gamma x df]\n  (loop [prev-x x\n         cur-x (- x (* gamma (df x)))\n         iters iters]\n    (if (or (< (Math/abs (- cur-x prev-x)) thresh)\n            (zero? iters))\n      cur-x\n      (recur cur-x\n             (- cur-x (* gamma (df cur-x)))\n             (dec iters)))))\n\n(prn \"The minimum is found at: \" (gradient-descent 0.00001 10000 0.01 6 df))\n\n;; This will output: \"The minimum is found at: \" 2.2499646074278457\n\n</source>\n\none could skip the precision check and then use a more general [[functional programming]] pattern, using reduce:\n\n<source lang=\"clojure\">\n\n(defn gradient-descent [iters gamma x df]\n  (reduce (fn [x _]\n            (- x (* gamma (df x))))\n          x\n          (range iters)))\n\n(prn \"The minimum is found at: \" (gradient-descent 10000 0.01 6 df))\n\n;; This will output: \"The minimum is found at: \" 2.249999999999999\n\n</source>\n\nThe last pattern can also be achieved through infinite lazy sequences, like so:\n\n<source lang=\"clojure\">\n\n(defn gradient-descent [n-iters gamma x df]\n  (nth (iterate (fn [x] (- x (* gamma (df x)))) x)\n       n-iters))\n\n(prn \"The minimum is found at: \" (gradient-descent 10000 0.01 6 df))\n\n;; This will output: \"The minimum is found at: \" 2.249999999999999\n\n</source>\n\n=== Java ===\nSame implementation, to run in [[Java (programming language)|Java]] with [[JShell]] (Java 9 minimum): <code>jshell <scriptfile></code><syntaxhighlight lang=\"java\">\n\nimport java.util.function.Function;\nimport static java.lang.Math.pow;\nimport static java.lang.System.out;\n\ndouble gamma = 0.01;\ndouble precision = 0.00001;\n\nFunction<Double,Double> df = x ->  4 * pow(x, 3) - 9 * pow(x, 2);\n\t\ndouble gradientDescent(Function<Double,Double> f) {\n\n\tdouble curX = 6.0;\n\tdouble previousStepSize = 1.0;\n\n\twhile (previousStepSize > precision) {\n\t    double prevX = curX;\n\t    curX -= gamma * f.apply(prevX);\n\t    previousStepSize = abs(curX - prevX);\n\t}\n\treturn curX;\n}\n\t\ndouble res = gradientDescent(df);\nout.printf(\"The local minimum occurs at %f\", res);\n\n</syntaxhighlight>\n\n=== Julia ===\nHere is the same algorithm implemented in the [[Julia (programming language)|Julia programming language]].\n\n<source lang=julia>\n\ncur_x = 6.0\ngamma = 0.01\nprecision = 0.00001\nprevious_step_size = 1.0\n\ndf(x) = 4x^3 - 9x^2\n\nwhile previous_step_size > precision\n    prev_x = cur_x\n    cur_x -= gamma * df(prev_x)\n    previous_step_size = abs(cur_x - prev_x)\nend\n\nprintln(\"The local minimum occurs at $cur_x\");\n</source>\n\n=== C ===\n\nHere is the same algorithm implemented in the [[C (programming language)|C programming language]].\n\n<source lang=c>\n#include <stdio.h>\n#include <stdlib.h>\n\ndouble dfx(double x) {\n    return 4*(x*x*x) - 9*(x*x);\n}\n\ndouble abs_val(double x) {\n    return x > 0 ? x : -x;\n}\n\ndouble gradient_descent(double dx, double error, double gamma, unsigned int max_iters) {\n    double c_error = error + 1;\n    unsigned int iters = 0;\n    double p_error;\n    while(error < c_error && iters < max_iters) {\n        p_error = dx;\n        dx -= dfx(p_error) * gamma;\n\t    c_error = abs_val(p_error-dx);\n        printf(\"\\nc_error %f\\n\", c_error);\n        iters++;\n    }\n    return dx;\n}\n\nint main() {\n    double dx = 6;\n    double error = 1e-6;\n    double gamma = 0.01;\n    unsigned int max_iters = 10000;\n    printf(\"The local minimum is: %f\\n\", gradient_descent(dx, error, gamma, max_iters));\n    return 0;\n}\n</source>\nThe above piece of code has to be modified with regard to step size according to the system at hand and convergence can be made faster by using an adaptive step size. In the above case the step size is not adaptive. It stays at 0.01 in all the directions which can sometimes cause the method to fail by diverging from the minimum.\n\n=== JavaScript (ES6) ===\nHere is the JavaScript (ES6) version:\n<source lang=javascript>\nconst convergeGradientDescent = (\ngradientFunction, // the gradient Function \nx0 = 0, // starting position\nmaxIters = 1000, // Maximum number of iterations\nprecision = 0.00001,  //  Desired precision of result\ngamma = 0.01  // Step size multiplier\n) => {\n  let step = 2*precision\n  let iter = 0\n  let x = x0\n  while (true){\n    step = gamma * gradientFunction(x)\n    x -= step\n    iter++\n    console.log(iter)\n    if(iter > maxIters) throw Error(\"Exceeded maximum iterations\")\n    if(Math.abs(step) < precision) {\n      console.log(`Minimum at: ${x}`)\n      console.log(`After ${iter} iteration(s)`)\n      return(x)\n    }\n  }\n}\n\n// TEST\n// gradient function, df\nconst df = (x) => (4*x-9)*x*x\nconvergeGradientDescent(df,6)\n//2.2499646074278457\n//Minimum at: 2.2499646074278457\n//after 70 iteration(s)\n\n</source>\n\nAnd here follows an example of gradient descent used to fit price against squareMeter on random values, written in [[JavaScript]], using [[ECMAScript#6th Edition - ECMAScript 2015|ECMAScript 6]] features:\n\n<source lang=javascript>\n// adjust training set size\n\nconst M = 10;\n\n// generate random training set\n\nconst DATA = [];\n\nconst getRandomIntFromInterval = (min, max) =>\n  Math.floor(Math.random() * (max - min + 1) + min);\n\nconst createRandomPortlandHouse = () => ({\n  squareMeter: getRandomIntFromInterval(0, 100),\n  price: getRandomIntFromInterval(0, 100),\n});\n\nfor (let i = 0; i < M; i++) {\n  DATA.push(createRandomPortlandHouse());\n}\n\nconst _x = DATA.map(date => date.squareMeter);\nconst _y = DATA.map(date => date.price);\n\n// linear regression and gradient descent\n\nconst LEARNING_RATE = 0.0003;\n\nlet thetaOne = 0;\nlet thetaZero = 0;\n\nconst hypothesis = x => thetaZero + thetaOne * x;\n\nconst learn = (x, y, alpha) => {\n  let thetaZeroSum = 0;\n  let thetaOneSum = 0;\n\n  for (let i = 0; i < M; i++) {\n    thetaZeroSum += hypothesis(x[i]) - y[i];\n    thetaOneSum += (hypothesis(x[i]) - y[i]) * x[i];\n  }\n\n  thetaZero = thetaZero - (alpha / M) * thetaZeroSum;\n  thetaOne = thetaOne - (alpha / M) * thetaOneSum;\n}\n\nconst MAX_ITER = 1500;\nfor (let i = 0; i < MAX_ITER; i++) {\n  learn(_x, _y, LEARNING_RATE);\n  console.log('thetaZero ', thetaZero, 'thetaOne', thetaOne);\n}\n</source>\n\n=== MATLAB ===\nThe following [[MATLAB]] code demonstrates a concrete solution for solving the non-linear system of equations presented in the [[Gradient descent#Solution of a non-linear system|previous section]]:\n\n:<math> \\begin{cases}\\begin{align}\n3x_1-\\cos(x_2x_3)-\\tfrac{3}{2} \\,\\quad\\qquad &=0 \\\\\n4x_1^2-625x_2^2+2x_2-1 \\ \\qquad &= 0  \\\\\n\\exp(-x_1x_2)+20x_3+\\tfrac{10\\pi-3}{3} &=0.\n\\end{align}\\end{cases}\n</math>\n\n<source lang=matlab>\n% Multi-variate vector-valued function G(x)\nG = @(x) [\n    3*x(1) - cos(x(2)*x(3)) - 3/2           ;\n    4*x(1)^2 - 625*x(2)^2 + 2*x(2) - 1      ;\n    exp(-x(1)*x(2)) + 20*x(3) + (10*pi-3)/3];\n\n% Jacobian of G\nJG = @(x) [\n    3,                     sin(x(2)*x(3))*x(3),   sin(x(2)*x(3))*x(2) ;\n    8*x(1),                -1250*x(2)+2,          0                   ;\n    -x(2)*exp(-x(1)*x(2)), -x(1)*exp(-x(1)*x(2)), 20                 ];\n\n% Objective function F(x) to minimize in order to solve G(x)=0\nF = @(x) 0.5 * sum(G(x).^2);\n\n% Gradient of F (partial derivatives)\ndF = @(x) JG(x).' * G(x);\n\n% Parameters\nGAMMA = 0.001;    % step size (learning rate)\nMAX_ITER = 1000;  % maximum number of iterations\nFUNC_TOL = 0.1;   % termination tolerance for F(x)\n\nfvals = [];       % store F(x) values across iterations\nprogress = @(iter,x) fprintf('iter = %3d: x = %-32s, F(x) = %f\\n', ...\n    iter, mat2str(x,6), F(x));\n\n% Iterate\niter = 1;         % iterations counter\nx = [0; 0; 0];    % initial guess\nfvals(iter) = F(x);\nprogress(iter, x);\nwhile iter < MAX_ITER && fvals(end) > FUNC_TOL\n    iter = iter + 1;\n    x = x - GAMMA * dF(x);  % gradient descent\n    fvals(iter) = F(x);     % evaluate objective function\n    progress(iter, x);      % show progress\nend\n\n% Plot\nplot(1:iter, fvals, 'LineWidth',2); grid on;\ntitle('Objective Function'); xlabel('Iteration'); ylabel('F(x)');\n\n% Evaluate final solution of system of equations G(x)=0\ndisp('G(x) = '); disp(G(x))\n\n% Output:\n%\n% iter =   1: x = [0;0;0]                         , F(x) = 58.456136\n% iter =   2: x = [0.0075;0.002;-0.20944]         , F(x) = 23.306394\n% iter =   3: x = [0.015005;0.0015482;-0.335103]  , F(x) = 10.617030\n% ...\n% iter = 187: x = [0.683335;0.0388258;-0.52231]   , F(x) = 0.101161\n% iter = 188: x = [0.684666;0.0389831;-0.522302]  , F(x) = 0.099372\n%\n% (converged in 188 iterations after exceeding termination tolerance for F(x))\n</source>\n\n=== R ===\nThe following [[R (programming language)|R programming language]] code is an example of implementing gradient descent algorithm to find the minimum of the function <math display=\"inline\"> \\textstyle{f{(x)} = x^{4} -3x^{3} + 2} </math> in previous section. Note that we are looking for <math display=\"inline\"> f </math>'s minimum by solving its derivative being equal to zero.\n\n:<math>\n\\nabla f(x) = 4x^3 - 9x^2 = 0\n</math>\n\nThe <math display=\"inline\"> \\textstyle{x} </math> can be updated with the gradient descent method every iteration in the form of\n:<math>\nx^{(k+1)} = x^{(k)} - \\gamma \\nabla f\\left (x^{(k)}\\right )\n</math>\n\nwhere k = 1, 2, ..., maximum iteration, and <math display=\"inline\"> \\textstyle{\\gamma} </math> is the step size multiplier.\n\n<source lang=\"r\">\n# set up a stepsize multiplier\ngamma = 0.003\n\n# set up a number of iterations\niter = 500\n\n# define the objective function f(x) = x^4 - 3*x^3 + 2\nobjFun = function(x) return(x^4 - 3*x^3 + 2)\n\n# define the gradient of f(x) = x^4 - 3*x^3 + 2\ngradient = function(x) return((4*x^3) - (9*x^2))\n\n# randomly initialize a value to x\nset.seed(100)\nx = floor(runif(1, 0, 10))\n\n# create a vector to contain all xs for all steps\nx.All = numeric(iter)\n\n# gradient descent method to find the minimum\nfor(i in seq_len(iter)){\n        x = x - gamma*gradient(x)\n        x.All[i] = x\n        print(x)\n}\n\n# print result and plot all xs for every iteration\nprint(paste(\"The minimum of f(x) is \", objFun(x), \" at position x = \", x, sep = \"\"))\nplot(x.All, type = \"l\")\n\n</source>\n\n==Extensions==\nGradient descent can be extended to handle [[Constraint (mathematics)|constraints]] by including a [[Projection (linear algebra)|projection]] onto the set of constraints. This method is only feasible when the projection is efficiently computable on a computer. Under suitable assumptions, this method converges.  This method is a specific case of the [[Forward–backward algorithm|forward-backward algorithm]] for monotone inclusions (which includes [[convex programming]] and [[Variational inequality|variational inequalities]]).<ref>P. L. Combettes and J.-C. Pesquet, [https://arxiv.org/pdf/0912.3522v4.pdf \"Proximal splitting methods in signal processing\"], in: ''Fixed-Point Algorithms for Inverse Problems in Science and Engineering'', (H. H. Bauschke, [[Regina S. Burachik|R. S. Burachik]], P. L. Combettes, V. Elser, D. R. Luke, and H. Wolkowicz, Editors), pp. 185-212. Springer, New York, 2011.</ref>\n\n===Fast gradient methods===\n\nAnother extension of gradient descent is due to [[Yurii Nesterov]] from 1983,<ref>{{cite book |first=Yu. |last=Nesterov |title=Introductory Lectures on Convex Optimization : A Basic Course |location= |publisher=Springer |year=2004 |isbn=1-4020-7553-7 }}</ref> and has been subsequently generalized. He provides a simple modification of the algorithm that enables faster convergence for convex problems. For unconstrained smooth problems the method is called the [[Fast Gradient Method]] (FGM)\nor the [[Accelerated Gradient Method]] (AGM). Specifically, if the differentiable function <math>F</math> is convex and <math>\\nabla F</math> is [[Lipschitz continuity|Lipschitz]], and it is not assumed that <math>F</math> is [[Convex function#Strongly convex functions|strongly convex]], then the error in the objective value generated at each step <math>k</math> by the gradient descent method will be [[Big O notation|bounded by]] <math display=\"inline\">\\mathcal{O}\\left(\\tfrac{1}{k}\\right)</math>. Using the Nesterov acceleration technique, the error decreases at <math display=\"inline\">\\mathcal{O}\\left(\\tfrac{1}{k^{2}}\\right)</math>.<ref>{{cite web |url=https://www.seas.ucla.edu/~vandenbe/236C/lectures/fgrad.pdf |title=Fast Gradient Methods |work=Lecture notes for EE236C at UCLA |first=Lieven |last=Vandenberghe |date=2019 }}</ref> It is known that the rate <math>\\mathcal{O}\\left({k^{-2}}\\right)</math> for the decrease of the [[loss function|cost function]] is optimal for first-order optimization methods. Nevertheless, there is the opportunity to improve the algorithm by reducing the constant factor. The [[optimized gradient method]] (OGM)<ref>{{cite journal |first=D. |last=Kim |first2=J. A. |last2=Fessler |title=Optimized First-order Methods for Smooth Convex Minimization |journal=[[Mathematical Programming|Math. Prog.]] |volume=151 |issue=1–2 |pages=81–107 |year=2016 |doi=10.1007/s10107-015-0949-3 }}</ref> reduces that constant by a factor of two and is an optimal first-order method for large-scale problems.<ref>{{cite journal |first=Yoel |last=Drori |date=2017 |title=The Exact Information-based Complexity of Smooth Convex Minimization |journal=Journal of Complexity |volume=39 |issue= |pages=1–16 |doi=10.1016/j.jco.2016.11.001 |arxiv=1606.01424 }}</ref>\n\nFor constrained or non-smooth problems, Nesterov's FGM is called the [[fast proximal gradient method]] (FPGM), an acceleration of the [[proximal gradient method]].\n\n===The momentum method===\n{{main|Stochastic gradient descent#Momentum}}\n\nYet another extension, that reduces the risk of getting stuck in a local minimum, as well as speeds up the convergence considerably in cases where the process would otherwise zig-zag heavily, is the ''momentum method'', which uses a momentum term in analogy to \"the mass of Newtonian particles that move through a viscous medium in a conservative force field\".<ref>{{cite journal|last1=Qian |first1=Ning |title=On the momentum term in gradient descent learning algorithms |journal=[[Neural Networks (journal)|Neural Networks]] |date=January 1999 |volume=12 |issue=1 |pages=145–151 |url=http://brahms.cpmc.columbia.edu/publications/momentum.pdf |accessdate=17 October 2014 |deadurl=yes |archiveurl=https://web.archive.org/web/20140508200053/http://brahms.cpmc.columbia.edu/publications/momentum.pdf |archivedate=8 May 2014 |df= |doi=10.1016/S0893-6080(98)00116-6 |citeseerx=10.1.1.57.5612 }}</ref> This method is often used as an extension to the [[backpropagation]] algorithms used to train [[artificial neural network]]s.<ref>{{cite web|title=Momentum and Learning Rate Adaptation|url=http://www.willamette.edu/~gorr/classes/cs449/momrate.html|publisher=[[Willamette University]]|accessdate=17 October 2014}}</ref><ref>{{cite web|author1=[[Geoffrey Hinton]]|author2=Nitish Srivastava|author3=Kevin Swersky|title=The momentum method|url=https://www.coursera.org/lecture/neural-networks/the-momentum-method-Oya9a|website=[[Coursera]]|accessdate=2 October 2018}} Part of a lecture series for the [[Coursera]] online course [https://www.coursera.org/learn/neural-networks Neural Networks for Machine Learning].</ref>\n\n== An analogy for understanding gradient descent ==\n[[File:Okanogan-Wenatchee National Forest, morning fog shrouds trees (37171636495).jpg|thumb|Fog in the mountains]]\nThe basic intuition behind gradient descent can be illustrated by a hypothetical scenario. A person is stuck in the mountains and is trying to get down (i.e. trying to find the minima). There is heavy fog such that visibility is extremely low. Therefore, the path down the mountain is not visible, so he must use local information to find the minima. He can use the method of gradient descent, which involves looking at the steepness of the hill at his current position, then proceeding in the direction with the steepest descent (i.e. downhill). If he was trying to find the top of the mountain (i.e. the maxima), then he would proceed in the direction steepest ascent (i.e. uphill). Using this method, he would eventually find his way down the mountain. However, assume also that the steepness of the hill is not immediately obvious with simple observation, but rather it requires a sophisticated instrument to measure, which the person happens to have at the moment. It takes quite some time to measure the steepness of the hill with the instrument, thus he should minimize his use of the instrument if he wanted to get down the mountain before sunset. The difficulty then is choosing the frequency at which he should measure the steepness of the hill so not to go off track.\n\nIn this analogy, the person represents the algorithm, and the path taken down the mountain represents the sequence of parameter settings that the algorithm will explore. The steepness of the hill represents the [[slope]] of the error surface at that point. The instrument used to measure steepness is [[Differentiation (mathematics)|differentiation]] (the slope of the error surface can be calculated by taking the [[derivative]] of the squared error function at that point). The direction he chooses to travel in aligns with the [[gradient]] of the error surface at that point. The amount of time he travels before taking another measurement is the learning rate of the algorithm. See {{Section link|Backpropagation|Limitations}} for a discussion of the limitations of this type of \"hill climbing\" algorithm.\n\n==See also==\n{{Div col|colwidth=20em}}\n* [[Conjugate gradient method]]\n* [[Stochastic gradient descent]]\n* [[Rprop]]\n* [[Delta rule]]\n* [[Wolfe conditions]]\n* [[Preconditioning]]\n* [[Broyden–Fletcher–Goldfarb–Shanno algorithm]]\n* [[Davidon–Fletcher–Powell formula]]\n* [[Nelder–Mead method]]\n* [[Gauss–Newton algorithm]]\n* [[Hill climbing]]\n{{Div col end}}\n\n==References==\n{{Reflist|30em}}\n\n== Further reading ==\n*{{cite book |first=Stephen |last=Boyd |authorlink=Stephen P. Boyd |first2=Lieven |last2=Vandenberghe |chapter=Unconstrained Minimization |title=Convex Optimization |location=New York |publisher=Cambridge University Press |year=2004 |isbn=0-521-83378-7 |chapterurl=https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf#page=471 |pages=457–520 }}\n*{{Cite book\n | title = Practical Mathematical Optimization - Basic Optimization Theory and Gradient-Based Algorithms |series=Springer Optimization and Its Applications |volume=133\n | first1 = Jan A.\n | last1 = Snyman\n | first2 = Daniel N.\n | last2 = Wilke\n | publisher = [[Springer International Publishing|Springer]]\n | edition=2\n | year = 2018\n | isbn = 978-3-319-77585-2\n}}. (Python module [http://extras.springer.com/2018/978-3-319-77585-2 pmo.py])\n\n== External links ==\n* [http://codingplayground.blogspot.it/2013/05/learning-linear-regression-with.html Using gradient descent in C++, Boost, Ublas for linear regression]\n* [https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/gradient Series of Khan Academy videos discusses gradient ascent]\n* [http://neuralnetworksanddeeplearning.com/chap1.html#learning_with_gradient_descent Online book teaching gradient descent in deep neural network context]\n* {{cite web |work=3Blue1Brown |title=Gradient Descent, How Neural Networks Learn |date=October 16, 2017 |url=https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=2 |via=[[YouTube]] }}\n\n{{Optimization algorithms}}\n\n{{DEFAULTSORT:Gradient Descent}}\n[[Category:First order methods]]\n[[Category:Optimization algorithms and methods]]\n[[Category:Gradient methods]]\n[[Category:Articles with example Python code]]"
    },
    {
      "title": "Gradient method",
      "url": "https://en.wikipedia.org/wiki/Gradient_method",
      "text": "In [[optimization (mathematics)|optimization]], '''gradient method''' is an [[algorithm]] to solve problems of the form \n\n:<math>\\min_{x\\in\\mathbb R^n}\\; f(x)</math>\n\nwith the search directions defined by the [[gradient]] of the function at the current point. Examples of gradient method are the [[gradient descent]] and the [[conjugate gradient]].\n\n==See also==\n{{col-begin}}\n{{col-break}}\n\n* [[Gradient descent]]\n* [[Stochastic gradient descent]]\n* [[Coordinate descent]]\n* [[Frank–Wolfe algorithm]]\n* [[Landweber iteration]]\n* [[Random coordinate descent]]\n* [[Conjugate gradient method]]\n* [[Derivation of the conjugate gradient method]]\n* [[Nonlinear conjugate gradient method]]\n* [[Biconjugate gradient method]]\n* [[Biconjugate gradient stabilized method]]\n\n==References==\n* {{cite book | year=1997 | title=Optimization : Algorithms and Consistent Approximations\n | publisher=Springer-Verlag | isbn=0-387-94971-2 |author=Elijah Polak}}\n\n{{Optimization algorithms}}\n\n{{DEFAULTSORT:Gradient Method}}\n[[Category:First order methods]]\n[[Category:Optimization algorithms and methods]]\n[[Category:Numerical linear algebra]]\n[[Category:Gradient methods| ]]\n\n[[fr:Algorithme du gradient]]"
    },
    {
      "title": "Schild's ladder",
      "url": "https://en.wikipedia.org/wiki/Schild%27s_ladder",
      "text": "{{short description|A first-order method for approximating parallel transport of a vector along a curve}}\n{{for|the book|Schild's Ladder}}\n[[Image:Schild's ladder step 4.svg|thumb|right|Two rungs of Schild's ladder.  The segments ''A''<sub>1</sub>''X''<sub>1</sub> and ''A''<sub>2</sub>''X''<sub>2</sub> are an approximation to first order of the [[parallel transport]] of ''A''<sub>0</sub>''X''<sub>0</sub> along the curve.]]\nIn the theory of [[general relativity]], and [[differential geometry]] more generally, '''Schild's ladder''' is a [[First-order approximation|first-order]] method for ''approximating'' [[parallel transport]] of a vector along a curve using only [[Torsion of connection#Geodesics and the absorption of torsion|affinely parametrized]] [[geodesic]]s.  The method is named for [[Alfred Schild]], who introduced the method during lectures at [[Princeton University]].\n\n== Construction ==\n\nThe idea is to identify a tangent vector ''x'' at a point <math>A_0</math> with a geodesic segment of unit length <math>A_0X_0</math>, and to construct an approximate parallelogram with approximately parallel sides <math>A_0X_0</math> and <math>A_1X_1</math> as an approximation of the [[Levi-Civita parallelogramoid]]; the new segment <math>A_1X_1</math> thus corresponds to an approximately parallel translated tangent vector at <math>A_1.</math>\n\n{|\n|[[Image:Schild's ladder step 1.svg|thumb|250px|right|A curve in ''M'' with a \"vector\" ''X''<sub>0</sub> at ''A''<sub>0</sub>, identified here as a geodesic segment.]]\n|[[Image:Schild's ladder step 2.svg|thumb|250px|right|Select ''A''<sub>1</sub> on the original curve.  The point ''P''<sub>1</sub> is the midpoint of the geodesic segment ''X''<sub>0</sub>''A''<sub>1</sub>.]]\n|[[Image:Schild's ladder step 3.svg|thumb|250px|right|The point ''X''<sub>1</sub> is obtained by following the geodesic ''A''<sub>0</sub>''P''<sub>1</sub> for twice its parameter length.]]\n|}\n\nFormally, consider a curve &gamma; through a point ''A''<sub>0</sub> in a [[Riemannian manifold]] ''M'', and let ''x'' be a [[tangent vector]] at ''A''<sub>0</sub>.  Then ''x'' can be identified with a geodesic segment ''A''<sub>0</sub>''X''<sub>0</sub> via the [[exponential map (Riemannian geometry)|exponential map]]. This geodesic &sigma; satisfies\n\n:<math>\\sigma(0)=A_0\\,</math> \n:<math>\\sigma'(0) = x.\\,</math>\n\nThe steps of the Schild's ladder construction are:\n* Let ''X''<sub>0</sub> = &sigma;(1), so the geodesic segment <math>A_0X_0</math> has unit length.\n* Now let ''A''<sub>1</sub> be a point on &gamma; close to ''A''<sub>0</sub>, and construct the geodesic ''X''<sub>0</sub>''A''<sub>1</sub>.\n* Let ''P''<sub>1</sub> be the midpoint  of ''X''<sub>0</sub>''A''<sub>1</sub> in the sense that the segments ''X''<sub>0</sub>''P''<sub>1</sub> and ''P''<sub>1</sub>''A''<sub>1</sub> take an equal affine parameter to traverse. \n* Construct the geodesic ''A''<sub>0</sub>''P''<sub>1</sub>, and extend it to a point ''X''<sub>1</sub> so that the parameter length of ''A''<sub>0</sub>''X''<sub>1</sub> is double that of ''A''<sub>0</sub>''P''<sub>1</sub>.\n* Finally construct the geodesic ''A''<sub>1</sub>''X''<sub>1</sub>.  The tangent to this geodesic ''x''<sub>1</sub> is then the parallel transport of ''X''<sub>0</sub> to ''A''<sub>1</sub>, at least to first order.\n\n== Approximation ==\nThis is a discrete approximation of the continuous process of parallel transport. If the ambient space is flat, this is exactly parallel transport, and the steps define [[parallelograms]], which agree with the [[Levi-Civita parallelogramoid]].\n\nIn a curved space, the error is given by [[holonomy]] around the triangle <math>A_1A_0X_0,</math> which is equal to the integral of the [[curvature]] over the interior of the triangle, by the [[Ambrose-Singer theorem]]; this is a form of [[Green's theorem]] (integral around a curve related to integral over interior).\n\n== Notes ==\n# Schild's ladder requires not only geodesics but also relative distance along geodesics. Relative distance may be provided by affine parametrization of geodesics, from which the required midpoints may be determined.\n# The parallel transport which is constructed by Schild's ladder is necessarily [[Torsion of connection|torsion]]-free.\n# A Riemannian metric is not required to generate the geodesics. But if the geodesics are generated from a Riemannian metric, the parallel transport which is constructed in the limit by Schild's ladder is the same as the [[Levi-Civita connection]] because this connection is defined to be torsion-free.\n\n==References==\n*{{citation|first1=Arkady|last1=Kheyfets|first2=Warner A.|last2=Miller|first3=Gregory A.|last3=Newton|title=Schild's ladder parallel transport procedure for an arbitrary connection |journal= [[International Journal of Theoretical Physics]] |year=2000| volume= 39| issue= 12| pages= 2891-2898 }}.\n* {{Citation\n | first1=Charles W.\n | last1=Misner\n | authorlink1=Charles W. Misner\n | first2=Kip S.\n | last2=Thorne\n | authorlink2=Kip S. Thorne\n | first3=John A.\n | last3=Wheeler\n | authorlink3=John Archibald Wheeler\n | title=Gravitation\n | publisher= W. H. Freeman\n | year=1973\n | isbn=0-7167-0344-0\n }}\n\n[[Category:Connection (mathematics)]]\n[[Category:First order methods]]"
    },
    {
      "title": "Biconjugate gradient method",
      "url": "https://en.wikipedia.org/wiki/Biconjugate_gradient_method",
      "text": "{{no footnotes|date=September 2013}}\n\nIn [[mathematics]], more specifically in [[numerical linear algebra]], the '''biconjugate gradient method''' is an [[algorithm]] to solve [[system of linear equations|systems of linear equations]]\n\n:<math>A x= b.\\,</math>\n\nUnlike the [[conjugate gradient method]], this algorithm does not require the [[matrix (mathematics)|matrix]] <math>A</math> to be [[self-adjoint]], but instead one needs to perform multiplications by the [[conjugate transpose]] {{math|<var>A</var><sup>*</sup>}}.\n\n==The algorithm==\n\n# Choose initial guess <math>x_0\\,</math>, two other vectors <math>x_0^*</math> and <math>b^*\\,</math> and a [[preconditioner]] <math>M\\,</math>\n# <math>r_0 \\leftarrow b-A\\, x_0\\,</math>\n# <math>r_0^* \\leftarrow b^*-x_0^*\\, A </math>\n# <math>p_0 \\leftarrow M^{-1} r_0\\,</math>\n# <math>p_0^* \\leftarrow r_0^*M^{-1}\\,</math>\n# for <math>k=0, 1, \\ldots</math> do\n## <math>\\alpha_k \\leftarrow {r_k^* M^{-1} r_k \\over p_k^* A p_k}\\,</math>\n## <math>x_{k+1} \\leftarrow x_k + \\alpha_k \\cdot p_k\\,</math>\n## <math>x_{k+1}^* \\leftarrow x_k^* + \\overline{\\alpha_k}\\cdot p_k^*\\,</math>\n## <math>r_{k+1} \\leftarrow r_k - \\alpha_k \\cdot A p_k\\,</math>\n## <math>r_{k+1}^* \\leftarrow r_k^*- \\overline{\\alpha_k} \\cdot p_k^*\\, A </math>\n## <math>\\beta_k \\leftarrow {r_{k+1}^* M^{-1} r_{k+1} \\over r_k^* M^{-1} r_k}\\,</math>\n## <math>p_{k+1} \\leftarrow M^{-1} r_{k+1} + \\beta_k \\cdot p_k\\,</math>\n## <math>p_{k+1}^* \\leftarrow r_{k+1}^*M^{-1}  + \\overline{\\beta_k}\\cdot p_k^*\\,</math>\n\nIn the above formulation, the computed <math>r_k\\,</math> and <math>r_k^*</math> satisfy\n\n:<math>r_k = b - A x_k,\\,</math>\n:<math>r_k^* = b^* - x_k^*\\, A </math>\n\nand thus are the respective [[Residual (numerical analysis)|residual]]s corresponding to <math>x_k\\,</math> and <math>x_k^*</math>, as approximate solutions to the systems\n\n:<math>A x = b,\\,</math>\n:<math>x^*\\, A = b^*\\,;</math>\n\n<math>x^*</math> is the [[Hermitian adjoint|adjoint]], and <math>\\overline{\\alpha}</math> is the [[complex conjugate]].\n\n=== Unpreconditioned version of the algorithm ===\n# Choose initial guess <math>x_0\\,</math>,\n# <math>r_0 \\leftarrow b-A\\, x_0\\,</math>\n# <math>\\hat{r}_0 \\leftarrow \\hat{b} - \\hat{x}_0A  </math>\n# <math>p_0 \\leftarrow r_0\\,</math>\n# <math>\\hat{p}_0 \\leftarrow \\hat{r}_0\\,</math>\n# for <math>k=0, 1, \\ldots</math> do\n## <math>\\alpha_k \\leftarrow {\\hat{r}_k r_k \\over \\hat{p}_k A p_k}\\,</math>\n## <math>x_{k+1} \\leftarrow x_k + \\alpha_k \\cdot p_k\\,</math>\n## <math>\\hat{x}_{k+1} \\leftarrow \\hat{x}_k + \\alpha_k \\cdot \\hat{p}_k\\,</math>\n## <math>r_{k+1} \\leftarrow r_k - \\alpha_k \\cdot A p_k\\,</math>\n## <math>\\hat{r}_{k+1} \\leftarrow \\hat{r}_k- \\alpha_k \\cdot \\hat{p}_k A  </math>\n## <math>\\beta_k \\leftarrow {\\hat{r}_{k+1} r_{k+1} \\over \\hat{r}_k r_k}\\,</math>\n## <math>p_{k+1} \\leftarrow r_{k+1} + \\beta_k \\cdot p_k\\,</math>\n## <math>\\hat{p}_{k+1} \\leftarrow \\hat{r}_{k+1}  + \\beta_k \\cdot \\hat{p}_k\\,</math>\n\n==Discussion==\nThe biconjugate gradient method is [[numerical stability|numerically unstable]]{{Citation needed|date=September 2009}} (compare to the [[biconjugate gradient stabilized method]]), but very important from a theoretical point of view. Define the iteration steps by\n\n:<math>x_k:=x_j+ P_k A^{-1}\\left(b - A x_j \\right),</math>\n:<math>x_k^*:= x_j^*+\\left(b^*- x_j^* A \\right) P_k A^{-1},</math>\n\nwhere <math>j<k</math> using the related [[projection (linear algebra)|projection]]\n\n:<math>P_k:= \\mathbf{u}_k \\left(\\mathbf{v}_k^* A \\mathbf{u}_k \\right)^{-1} \\mathbf{v}_k^* A,</math>\n\nwith\n\n:<math>\\mathbf{u}_k=\\left[u_0, u_1, \\dots, u_{k-1} \\right],</math>\n:<math>\\mathbf{v}_k=\\left[v_0, v_1, \\dots, v_{k-1} \\right].</math>\n\nThese related projections may be iterated themselves as\n\n:<math>P_{k+1}= P_k+ \\left( 1-P_k\\right) u_k \\otimes {v_k^* A\\left(1-P_k \\right) \\over v_k^* A\\left(1-P_k \\right) u_k}.</math>\n\nA relation to [[Quasi-Newton method]]s is given by <math>P_k= A_k^{-1} A</math> and <math>x_{k+1}= x_k- A_{k+1}^{-1}\\left(A x_k -b \\right)</math>, where\n:<math>A_{k+1}^{-1}= A_k^{-1}+ \\left( 1-A_k^{-1}A\\right) u_k \\otimes {v_k^* \\left(1-A A_k^{-1} \\right) \\over v_k^* A\\left(1-A_k^{-1}A \\right) u_k}.</math>\n\nThe new directions\n\n:<math>p_k = \\left(1-P_k \\right) u_k,</math>\n:<math>p_k^* = v_k^* A \\left(1- P_k \\right) A^{-1}</math>\n\nare then orthogonal to the residuals:\n\n:<math>v_i^* r_k= p_i^* r_k=0,</math>\n:<math>r_k^* u_j = r_k^* p_j= 0,</math>\n\nwhich themselves satisfy\n\n:<math>r_k= A \\left( 1- P_k \\right) A^{-1} r_j,</math>\n:<math>r_k^*= r_j^* \\left( 1- P_k \\right)</math>\n\nwhere <math>i,j<k</math>.\n\nThe biconjugate gradient method now makes a special choice and uses the setting\n:<math>u_k = M^{-1} r_k,\\,</math>\n:<math>v_k^* = r_k^* \\, M^{-1}.\\,</math>\n\nWith this particular choice, explicit evaluations of <math>P_k</math> and {{math|<var>A</var><sup>&minus;1</sup>}} are avoided, and the algorithm takes the form stated above.\n\n==Properties==\n\n* If <math>A= A^*\\,</math> is [[Conjugate transpose|self-adjoint]], <math>x_0^*= x_0</math> and <math>b^*=b</math>, then <math>r_k= r_k^*</math>, <math>p_k= p_k^*</math>, and the [[conjugate gradient method]] produces the same sequence <math>x_k= x_k^*</math> at half the computational cost.\n* The sequences produced by the algorithm are [[Biorthogonal system|biorthogonal]], i.e., <math>p_i^*Ap_j=r_i^*M^{-1}r_j=0</math> for <math>i \\neq j</math>.\n* if <math>P_{j'}\\,</math> is a polynomial with <math>\\mathrm{deg}\\left(P_{j'}\\right)+j<k</math>, then <math>r_k^*P_{j'}\\left(M^{-1}A\\right)u_j=0</math>. The algorithm thus produces projections onto the [[Krylov subspace]].\n* if <math>P_{i'}\\,</math> is a polynomial with <math>i+\\mathrm{deg}\\left(P_{i'}\\right)<k</math>, then <math>v_i^*P_{i'}\\left(AM^{-1}\\right)r_k=0</math>.\n\n==See also==\n* [[Biconjugate gradient stabilized method]]\n* [[Conjugate gradient method]]\n\n==References==\n* {{cite journal|first=R.|last=Fletcher|year=1976|title=Conjugate gradient methods for indefinite systems|journal=Numerical Analysis|volume=506|series=Lecture Notes in Mathematics|publisher=Springer Berlin / Heidelberg|issn=1617-9692|isbn=978-3-540-07610-0|pages=73&ndash;89|url=http://www.springerlink.com/content/974t1l33m84217um/|doi=10.1007/BFb0080109|editor1-last=Watson|editor1-first=G. Alistair}}\n* {{Cite book |last1=Press|first1=WH|last2=Teukolsky|first2=SA|last3=Vetterling|first3=WT|last4=Flannery|first4=BP|year=2007|title=Numerical Recipes: The Art of Scientific Computing|edition=3rd|publisher=Cambridge University Press| publication-place=New York|isbn=978-0-521-88068-8|chapter=Section 2.7.6|chapter-url=http://apps.nrbook.com/empanel/index.html?pg=87 |postscript=<!-- Bot inserted parameter. Either remove it; or change its value to \".\" for the cite to end in a \".\", as necessary. -->{{inconsistent citations}}}}\n\n{{Numerical linear algebra}}\n\n[[Category:Numerical linear algebra]]\n[[Category:Gradient methods]]"
    },
    {
      "title": "Biconjugate gradient stabilized method",
      "url": "https://en.wikipedia.org/wiki/Biconjugate_gradient_stabilized_method",
      "text": "{{Technical|date=May 2015}}\n\nIn [[numerical linear algebra]], the '''biconjugate gradient stabilized method''', often abbreviated as '''BiCGSTAB''', is an [[iterative method]] developed by [[Henk van der Vorst|H. A. van der Vorst]] for the numerical solution of nonsymmetric [[System of linear equations|linear system]]s. It is a variant of the [[biconjugate gradient method]] (BiCG) and has faster and smoother convergence than the original BiCG as well as other variants such as the [[conjugate gradient squared method]] (CGS). It is a [[Krylov subspace]] method.\n\n==Algorithmic steps==\n===Unpreconditioned BiCGSTAB===\nTo solve a linear system {{math|'''<var>Ax</var>''' {{=}} '''<var>b</var>'''}}, BiCGSTAB starts with an initial guess {{math|'''<var>x</var>'''<sub>0</sub>}} and proceeds as follows:\n\n# {{math|'''<var>r</var>'''<sub>0</sub> {{=}} '''<var>b</var>''' − '''<var>Ax</var>'''<sub>0</sub>}}\n# Choose an arbitrary vector {{math|'''<var>r̂</var>'''<sub>0</sub>}} such that {{math|('''<var>r̂</var>'''<sub>0</sub>, '''<var>r</var>'''<sub>0</sub>) ≠ 0}}, e.g., {{math|'''<var>r̂</var>'''<sub>0</sub> {{=}} '''<var>r</var>'''<sub>0</sub>}} .  Note that notation {{math|('''<var>x</var>''','''<var>y</var>''') }} applies for scalar product of vectors {{math|1=('''<var>x</var>''','''<var>y</var>''') = <'''<var>x</var>''','''<var>y</var>'''> = '''<var>x</var>'''·'''<var>y</var>''' = '''<var>x</var>''' ' '''<var>y</var>'''}}\n# {{math|<var>ρ</var><sub>0</sub> {{=}} <var>α</var> {{=}} <var>ω</var><sub>0</sub> {{=}} 1}}\n# {{math|'''<var>v</var>'''<sub>0</sub> {{=}} '''<var>p</var>'''<sub>0</sub> {{=}} '''0'''}}\n# For {{math|<var>i</var> {{=}} 1, 2, 3, …}}\n## {{math|<var>ρ<sub>i</sub></var> {{=}} ('''<var>r̂</var>'''<sub>0</sub>, '''<var>r</var>'''<sub><var>i</var>−1</sub>)}}\n## {{math|<var>β</var> {{=}} (<var>ρ<sub>i</sub></var>/<var>ρ</var><sub><var>i</var>−1</sub>)(<var>α</var>/<var>ω</var><sub><var>i</var>−1</sub>)}}\n## {{math|<var>'''p'''<sub>i</sub></var> {{=}} '''<var>r</var>'''<sub><var>i</var>−1</sub> + <var>β</var>('''<var>p</var>'''<sub><var>i</var>−1</sub> − <var>ω</var><sub><var>i</var>−1</sub>'''<var>v</var>'''<sub><var>i</var>−1</sub>)}}\n## {{math|<var>'''v'''<sub>i</sub></var> {{=}} '''<var>Ap</var>'''<sub><var>i</var></sub>}}\n## {{math|<var>α</var> {{=}} <var>ρ<sub>i</sub></var>/('''<var>r̂</var>'''<sub>0</sub>, <var>'''v'''<sub>i</sub></var>)}}\n## {{math|<var>'''h'''</var> {{=}} '''<var>x</var>'''<sub><var>i</var>−1</sub> + <var>α'''p'''<sub>i</sub></var> }}\n## If {{math|<var>'''h'''</var>}} is accurate enough, then set {{math|<var>'''x'''<sub>i</sub></var> {{=}} <var>'''h'''</var>}} and quit\n## {{math|<var>'''s'''</var> {{=}} <var>'''r'''</var><sub><var>i</var>−1</sub> − <var>α'''v'''<sub>i</sub></var>}}\n## {{math|'''<var>t</var>''' {{=}} '''<var>As</var>'''}}\n## {{math|<var>ω<sub>i</sub></var> {{=}} (<var>'''t'''</var>, <var>'''s'''</var>)/(<var>'''t'''</var>, <var>'''t'''</var>)}}\n## {{math|<var>'''x'''<sub>i</sub></var> {{=}} <var>'''h'''</var> + <var>ω<sub>i</sub>'''s'''</var>}}\n## If {{math|<var>'''x'''<sub>i</sub></var>}} is accurate enough, then quit\n## {{math|<var>'''r'''<sub>i</sub></var> {{=}} <var>'''s'''</var> − <var>ω<sub>i</sub>'''t'''</var>}}\n\n===Preconditioned BiCGSTAB===\n[[Preconditioner]]s are usually used to accelerate convergence of iterative methods. To solve a linear system {{math|'''<var>Ax</var>''' {{=}} '''<var>b</var>'''}} with a preconditioner {{math|'''<var>K</var>''' {{=}} '''<var>K</var>'''<sub>1</sub>'''<var>K</var>'''<sub>2</sub> ≈ '''<var>A</var>'''}}, preconditioned BiCGSTAB starts with an initial guess {{math|'''<var>x</var>'''<sub>0</sub>}} and proceeds as follows:\n\n# {{math|'''<var>r</var>'''<sub>0</sub> {{=}} '''<var>b</var>''' − '''<var>Ax</var>'''<sub>0</sub>}}\n# Choose an arbitrary vector {{math|'''<var>r̂</var>'''<sub>0</sub>}} such that {{math|('''<var>r̂</var>'''<sub>0</sub>, '''<var>r</var>'''<sub>0</sub>) ≠ 0}}, e.g., {{math|'''<var>r̂</var>'''<sub>0</sub> {{=}} '''<var>r</var>'''<sub>0</sub>}}\n# {{math|<var>ρ</var><sub>0</sub> {{=}} <var>α</var> {{=}} <var>ω</var><sub>0</sub> {{=}} 1}}\n# {{math|'''<var>v</var>'''<sub>0</sub> {{=}} '''<var>p</var>'''<sub>0</sub> {{=}} '''0'''}}\n# For {{math|<var>i</var> {{=}} 1, 2, 3, …}}\n## {{math|<var>ρ<sub>i</sub></var> {{=}} ('''<var>r̂</var>'''<sub>0</sub>, '''<var>r</var>'''<sub><var>i</var>−1</sub>)}}\n## {{math|<var>β</var> {{=}} (<var>ρ<sub>i</sub></var>/<var>ρ</var><sub><var>i</var>−1</sub>)(<var>α</var>/<var>ω</var><sub><var>i</var>−1</sub>)}}\n## {{math|<var>'''p'''<sub>i</sub></var> {{=}} '''<var>r</var>'''<sub><var>i</var>−1</sub> + <var>β</var>('''<var>p</var>'''<sub><var>i</var>−1</sub> − <var>ω</var><sub><var>i</var>−1</sub>'''<var>v</var>'''<sub><var>i</var>−1</sub>)}}\n## {{math|'''<var>y</var>''' {{=}} '''<var>K</var>'''<sup>−1</sup>'''<var>p</var>'''<sub><var>i</var></sub>}}\n## {{math|<var>'''v'''<sub>i</sub></var> {{=}} '''<var>Ay</var>'''}}\n## {{math|<var>α</var> {{=}} <var>ρ<sub>i</sub></var>/('''<var>r̂</var>'''<sub>0</sub>, <var>'''v'''<sub>i</sub></var>)}}\n## {{math|<var>'''h'''</var> {{=}} '''<var>x</var>'''<sub><var>i</var>−1</sub> + <var>α'''y'''</var> }}\n## If {{math|<var>'''h'''</var>}} is accurate enough then {{math|<var>'''x'''<sub>i</sub></var> {{=}} <var>'''h'''</var>}} and quit\n## {{math|'''<var>s</var>''' {{=}} '''<var>r</var>'''<sub><var>i</var>−1</sub> − <var>α'''v'''<sub>i</sub></var>}}\n## {{math|'''<var>z</var>''' {{=}} '''<var>K</var>'''<sup>−1</sup>'''<var>s</var>'''}}\n## {{math|'''<var>t</var>''' {{=}} '''<var>Az</var>'''}}\n## {{math|<var>ω<sub>i</sub></var> {{=}} ({{SubSup|'''<var>K</var>'''|1|−1}}'''<var>t</var>''', {{SubSup|'''<var>K</var>'''|1|−1}}'''<var>s</var>''')/({{SubSup|'''<var>K</var>'''|1|−1}}'''<var>t</var>''', {{SubSup|'''<var>K</var>'''|1|−1}}'''<var>t</var>''')}}\n## {{math|<var>'''x'''<sub>i</sub></var> {{=}} <var>'''h'''</var> + <var>ω<sub>i</sub>'''z'''</var>}}\n## If {{math|<var>'''x'''<sub>i</sub></var>}} is accurate enough then quit\n## {{math|<var>'''r'''<sub>i</sub></var> {{=}} '''<var>s</var>''' − <var>ω<sub>i</sub>'''t'''</var>}}\n\nThis formulation is equivalent to applying unpreconditioned BiCGSTAB to the explicitly preconditioned system\n:{{math|'''<var>Ãx̃</var>''' {{=}} '''<var>b̃</var>'''}}\nwith {{math|'''<var>Ã</var>''' {{=}} {{SubSup|'''<var>K</var>'''|1|−1}}'''<var>A</var>'''{{SubSup|'''<var>K</var>'''|2|−1}}}}, {{math|'''<var>x̃</var>''' {{=}} '''<var>K</var>'''<sub>2</sub>'''<var>x</var>'''}} and {{math|'''<var>b̃</var>''' {{=}} {{SubSup|'''<var>K</var>'''|1|−1}}'''<var>b</var>'''}}. In other words, both left- and right-preconditioning are possible with this formulation.\n\n==Derivation==\n===BiCG in polynomial form===\nIn BiCG, the search directions {{math|<var>'''p'''<sub>i</sub></var>}} and {{math|'''<var>p̂</var>'''<sub><var>i</var></sub>}} and the residuals {{math|<var>'''r'''<sub>i</sub></var>}} and {{math|'''<var>r̂</var>'''<sub><var>i</var></sub>}} are updated using the following [[recurrence relation]]s:\n\n:{{math|<var>'''p'''<sub>i</sub></var> {{=}} '''<var>r</var>'''<sub><var>i</var>−1</sub> + <var>β<sub>i</sub></var>'''<var>p</var>'''<sub><var>i</var>−1</sub>}},\n:{{math|'''<var>p̂</var>'''<sub><var>i</var></sub> {{=}} '''<var>r̂</var>'''<sub><var>i</var>−1</sub> + <var>β<sub>i</sub></var>'''<var>p̂</var>'''<sub><var>i</var>−1</sub>}},\n:{{math|<var>'''r'''<sub>i</sub></var> {{=}} '''<var>r</var>'''<sub><var>i</var>−1</sub> − <var>α<sub>i</sub></var>'''<var>Ap</var>'''<sub><var>i</var></sub>}},\n:{{math|'''<var>r̂</var>'''<sub><var>i</var></sub> {{=}} '''<var>r̂</var>'''<sub><var>i</var>−1</sub> − <var>α<sub>i</sub></var>'''<var>A</var>'''<sup>T</sup>'''<var>p̂</var>'''<sub><var>i</var></sub>}}.\n\nThe constants {{math|<var>α<sub>i</sub></var>}} and {{math|<var>β<sub>i</sub></var>}} are chosen to be\n\n:{{math|<var>α<sub>i</sub></var> {{=}} <var>ρ<sub>i</sub></var>/('''<var>p̂</var>'''<sub><var>i</var></sub>, <var>'''Ap'''<sub>i</sub></var>)}},\n:{{math|<var>β<sub>i</sub></var> {{=}} <var>ρ<sub>i</sub></var>/<var>ρ</var><sub><var>i</var>−1</sub>}}\n\nwhere {{math|<var>ρ<sub>i</sub></var> {{=}} ('''<var>r̂</var>'''<sub><var>i</var>−1</sub>, '''<var>r</var>'''<sub><var>i</var>−1</sub>)}} so that the residuals and the search directions satisfy biorthogonality and biconjugacy, respectively, i.e., for {{math|<var>i</var> ≠ <var>j</var>}},\n\n:{{math|('''<var>r̂</var>'''<sub><var>i</var></sub>, <var>'''r'''<sub>j</sub></var>) {{=}} 0}},\n:{{math|('''<var>p̂</var>'''<sub><var>i</var></sub>, <var>'''Ap'''<sub>j</sub></var>) {{=}} 0}}.\n\nIt is straightforward to show that\n\n:{{math|<var>'''r'''<sub>i</sub></var> {{=}} <var>P<sub>i</sub></var>('''<var>A</var>''')'''<var>r</var>'''<sub>0</sub>}},\n:{{math|'''<var>r̂</var>'''<sub><var>i</var></sub> {{=}} <var>P<sub>i</sub></var>('''<var>A</var>'''<sup>T</sup>)'''<var>r̂</var>'''<sub>0</sub>}},\n:{{math|'''<var>p</var>'''<sub><var>i</var>+1</sub> {{=}} <var>T<sub>i</sub></var>('''<var>A</var>''')'''<var>r</var>'''<sub>0</sub>}},\n:{{math|'''<var>p̂</var>'''<sub><var>i</var>+1</sub>  {{=}}  <var>T<sub>i</sub></var>('''<var>A</var>'''<sup>T</sup>)'''<var>r̂</var>'''<sub>0</sub>}}\n\nwhere {{math|<var>P<sub>i</sub></var>('''<var>A</var>''')}} and {{math|<var>T<sub>i</sub></var>('''<var>A</var>''')}} are {{math|<var>i</var>}}th-degree polynomials in {{math|'''<var>A</var>'''}}. These polynomials satisfy the following recurrence relations:\n\n:{{math|<var>P<sub>i</sub></var>('''<var>A</var>''') {{=}} <var>P</var><sub><var>i</var>−1</sub>('''<var>A</var>''') − <var>α<sub>i</sub>'''A'''T</var><sub><var>i</var>−1</sub>('''<var>A</var>''')}},\n:{{math|<var>T<sub>i</sub></var>('''<var>A</var>''') {{=}} <var>P<sub>i</sub></var>('''<var>A</var>''')  + <var>β</var><sub><var>i</var>+1</sub><var>T</var><sub><var>i</var>−1</sub>('''<var>A</var>''')}}.\n\n===Derivation of BiCGSTAB from BiCG===\nIt is unnecessary to explicitly keep track of the residuals and search directions of BiCG. In other words, the BiCG iterations can be performed implicitly. In BiCGSTAB, one wishes to have recurrence relations for\n\n:{{math|'''<var>r̃</var>'''<sub><var>i</var></sub> {{=}} <var>Q<sub>i</sub></var>('''<var>A</var>''')<var>P<sub>i</sub></var>('''<var>A</var>''')'''<var>r</var>'''<sub>0</sub>}}\n\nwhere {{math|<var>Q<sub>i</sub></var>('''<var>A</var>''') {{=}} ('''<var>I</var>''' − <var>ω</var><sub>1</sub>'''<var>A</var>''')('''<var>I</var>''' − <var>ω</var><sub>2</sub>'''<var>A</var>''')⋯('''<var>I</var>''' − <var>ω<sub>i</sub>'''A'''</var>)}} with suitable constants {{math|<var>ω<sub>j</sub></var>}} instead of {{math|<var>'''r'''<sub>i</sub></var> {{=}} <var>P<sub>i</sub></var>('''<var>A</var>''')}} in the hope that {{math|<var>Q<sub>i</sub></var>('''<var>A</var>''')}} will enable faster and smoother convergence in {{math|<var>'''r̃'''<sub>i</sub></var>}} than {{math|<var>'''r'''<sub>i</sub></var>}}.\n\nIt follows from the recurrence relations for {{math|<var>P<sub>i</sub></var>('''<var>A</var>''')}} and {{math|<var>T<sub>i</sub></var>('''<var>A</var>''')}} and the definition of {{math|<var>Q<sub>i</sub></var>('''<var>A</var>''')}} that\n\n:{{math|<var>Q<sub>i</sub></var>('''<var>A</var>''')<var>P<sub>i</sub></var>('''<var>A</var>''')'''<var>r</var>'''<sub>0</sub> {{=}} ('''<var>I</var>''' − <var>ω<sub>i</sub>'''A'''</var>)(<var>Q</var><sub><var>i</var>−1</sub>('''<var>A</var>''')<var>P</var><sub><var>i</var>−1</sub>('''<var>A</var>''')'''<var>r</var>'''<sub>0</sub>   − <var>α<sub>i</sub>'''A'''Q</var><sub><var>i</var>−1</sub>('''<var>A</var>''')<var>T</var><sub><var>i</var>−1</sub>('''<var>A</var>''')'''<var>r</var>'''<sub>0</sub>)}},\n\nwhich entails the necessity of a recurrence relation for {{math|<var>Q<sub>i</sub></var>('''<var>A</var>''')<var>T<sub>i</sub></var>('''<var>A</var>''')'''<var>r</var>'''<sub>0</sub>}}. This can also be derived from the BiCG relations:\n\n:{{math|<var>Q<sub>i</sub></var>('''<var>A</var>''')<var>T<sub>i</sub></var>('''<var>A</var>''')'''<var>r</var>'''<sub>0</sub> {{=}} <var>Q<sub>i</sub></var>('''<var>A</var>''')<var>P<sub>i</sub></var>('''<var>A</var>''')'''<var>r</var>'''<sub>0</sub> + <var>β</var><sub><var>i</var>+1</sub>('''<var>I</var>''' − <var>ω<sub>i</sub>'''A'''</var>)<var>Q</var><sub><var>i</var>−1</sub>('''<var>A</var>''')<var>P</var><sub><var>i</var>−1</sub>('''<var>A</var>''')'''<var>r</var>'''<sub>0</sub>}}.\n\nSimilarly to defining {{math|<var>'''r̃'''<sub>i</sub></var>}}, BiCGSTAB defines\n\n:{{math|'''<var>p̃</var>'''<sub><var>i</var>+1</sub> {{=}} <var>Q<sub>i</sub></var>('''<var>A</var>''')<var>T<sub>i</sub></var>('''<var>A</var>''')'''<var>r</var>'''<sub>0</sub>}}.\n\nWritten in vector form, the recurrence relations for {{math|'''<var>p̃</var>'''<sub><var>i</var></sub>}} and {{math|'''<var>r̃</var>'''<sub><var>i</var></sub>}} are\n\n:{{math|'''<var>p̃</var>'''<sub><var>i</var></sub> {{=}} '''<var>r̃</var>'''<sub><var>i</var>−1</sub> + <var>β<sub>i</sub></var>('''<var>I</var>''' − <var>ω</var><sub><var>i</var>−1</sub>'''<var>A</var>''')'''<var>p̃</var>'''<sub><var>i</var>−1</sub>}},\n:{{math|'''<var>r̃</var>'''<sub><var>i</var></sub> {{=}} ('''<var>I</var>''' − <var>ω<sub>i</sub>'''A'''</var>)('''<var>r̃</var>'''<sub><var>i</var>−1</sub> − <var>α<sub>i</sub>'''A'''</var>'''<var>p̃</var>'''<sub><var>i</var></sub>)}}.\n\nTo derive a recurrence relation for {{math|<var>'''x'''<sub>i</sub></var>}}, define\n\n:{{math|<var>'''s'''<sub>i</sub></var> {{=}} '''<var>r̃</var>'''<sub><var>i</var>−1</sub>  − <var>α<sub>i</sub>'''A'''</var>'''<var>p̃</var>'''<sub><var>i</var></sub>}}.\n\nThe recurrence relation for {{math|'''<var>r̃</var>'''<sub><var>i</var></sub>}} can then be written as\n\n:{{math|'''<var>r̃</var>'''<sub><var>i</var></sub> {{=}} '''<var>r̃</var>'''<sub><var>i</var>−1</sub> − <var>α<sub>i</sub>'''A'''</var>'''<var>p̃</var>'''<sub><var>i</var></sub> − <var>ω<sub>i</sub>'''As'''<sub>i</sub></var>}},\n\nwhich corresponds to\n\n:{{math|'''<var>x</var>'''<sub><var>i</var></sub> {{=}} '''<var>x</var>'''<sub><var>i</var>−1</sub> + <var>α<sub>i</sub></var>'''<var>p̃</var>'''<sub><var>i</var></sub> + <var>ω<sub>i</sub>'''s'''<sub>i</sub></var>}}.\n\n===Determination of BiCGSTAB constants===\nNow it remains to determine the BiCG constants {{math|<var>α<sub>i</sub></var>}}  and {{math|<var>β<sub>i</sub></var>}} and choose a suitable {{math|<var>ω<sub>i</sub></var>}}.\n\nIn BiCG, {{math|<var>β<sub>i</sub></var> {{=}} <var>ρ<sub>i</sub></var>/<var>ρ</var><sub><var>i</var>−1</sub>}} with\n\n:{{math|<var>ρ<sub>i</sub></var> {{=}} ('''<var>r̂</var>'''<sub><var>i</var>−1</sub>, '''<var>r</var>'''<sub><var>i</var>−1</sub>) {{=}} (<var>P</var><sub><var>i</var>−1</sub>('''<var>A</var>'''<sup>T</sup>)'''<var>r̂</var>'''<sub>0</sub>,  <var>P</var><sub><var>i</var>−1</sub>('''<var>A</var>''')'''<var>r</var>'''<sub>0</sub>)}}.\n\nSince BiCGSTAB does not explicitly keep track of {{math|'''<var>r̂</var>'''<sub><var>i</var></sub>}} or {{math|'''<var>r</var>'''<sub><var>i</var></sub>}}, {{math|<var>ρ<sub>i</sub></var>}} is not immediately computable from this formula. However, it can be related to the scalar\n\n:{{math|<var>ρ̃</var><sub><var>i</var></sub> {{=}} (<var>Q</var><sub><var>i</var>−1</sub>('''<var>A</var>'''<sup>T</sup>)'''<var>r̂</var>'''<sub>0</sub>, <var>P</var><sub><var>i</var>−1</sub>('''<var>A</var>''')'''<var>r</var>'''<sub>0</sub>) {{=}} ('''<var>r̂</var>'''<sub>0</sub>, <var>Q</var><sub><var>i</var>−1</sub>('''<var>A</var>''')<var>P</var><sub><var>i</var>−1</sub>('''<var>A</var>''')'''<var>r</var>'''<sub>0</sub>) {{=}} ('''<var>r̂</var>'''<sub>0</sub>, '''<var>r</var>'''<sub><var>i</var>−1</sub>)}}.\n\nDue to biorthogonality, {{math|'''<var>r</var>'''<sub><var>i</var>−1</sub> {{=}} <var>P</var><sub><var>i</var>−1</sub>('''<var>A</var>''')'''<var>r</var>'''<sub>0</sub>}} is orthogonal to {{math|<var>U</var><sub><var>i</var>−2</sub>('''<var>A</var>'''<sup>T</sup>)'''<var>r̂</var>'''<sub>0</sub>}} where {{math|<var>U</var><sub><var>i</var>−2</sub>('''<var>A</var>'''<sup>T</sup>)}} is any polynomial of degree {{math|<var>i</var> − 2}} in {{math|'''<var>A</var>'''<sup>T</sup>}}. Hence, only the highest-order terms of {{math|<var>P</var><sub><var>i</var>−1</sub>('''<var>A</var>'''<sup>T</sup>)}} and {{math|<var>Q</var><sub><var>i</var>−1</sub>('''<var>A</var>'''<sup>T</sup>)}} matter in the dot products {{math|(<var>P</var><sub><var>i</var>−1</sub>('''<var>A</var>'''<sup>T</sup>)'''<var>r̂</var>'''<sub>0</sub>,   <var>P</var><sub><var>i</var>−1</sub>('''<var>A</var>''')'''<var>r</var>'''<sub>0</sub>)}} and {{math| (<var>Q</var><sub><var>i</var>−1</sub>('''<var>A</var>'''<sup>T</sup>)'''<var>r̂</var>'''<sub>0</sub>,  <var>P</var><sub><var>i</var>−1</sub>('''<var>A</var>''')'''<var>r</var>'''<sub>0</sub>)}}. The leading coefficients of {{math|<var>P</var><sub><var>i</var>−1</sub>('''<var>A</var>'''<sup>T</sup>)}} and {{math|<var>Q</var><sub><var>i</var>−1</sub>('''<var>A</var>'''<sup>T</sup>)}} are {{math|(−1)<sup><var>i</var>−1</sup><var>α</var><sub>1</sub><var>α</var><sub>2</sub>⋯<var>α</var><sub><var>i</var>−1</sub>}} and {{math|(−1)<sup><var>i</var>−1</sup><var>ω</var><sub>1</sub><var>ω</var><sub>2</sub>⋯<var>ω</var><sub><var>i</var>−1</sub>}}, respectively. It follows that\n\n:{{math|<var>ρ<sub>i</sub></var> {{=}} (<var>α</var><sub>1</sub>/<var>ω</var><sub>1</sub>)(<var>α</var><sub>2</sub>/<var>ω</var><sub>2</sub>)⋯(<var>α</var><sub><var>i</var>−1</sub>/<var>ω</var><sub><var>i</var>−1</sub>)<var>ρ̃</var><sub><var>i</var></sub>}},\n\nand thus\n\n:{{math|<var>β<sub>i</sub></var> {{=}} <var>ρ<sub>i</sub></var>/<var>ρ</var><sub><var>i</var>−1</sub> {{=}} (<var>ρ̃</var><sub><var>i</var></sub>/<var>ρ̃</var><sub><var>i</var>−1</sub>)(<var>α</var><sub><var>i</var>−1</sub>/<var>ω</var><sub><var>i</var>−1</sub>)}}.\n\nA simple formula for {{math|<var>α<sub>i</sub></var>}} can be similarly derived. In BiCG,\n\n:{{math|<var>α<sub>i</sub></var> {{=}} <var>ρ<sub>i</sub></var>/('''<var>p̂</var>'''<sub><var>i</var></sub>, <var>'''Ap'''<sub>i</sub></var>) {{=}} (<var>P</var><sub><var>i</var>−1</sub>('''<var>A</var>'''<sup>T</sup>)'''<var>r̂</var>'''<sub>0</sub>,  <var>P</var><sub><var>i</var>−1</sub>('''<var>A</var>''')'''<var>r</var>'''<sub>0</sub>)/(<var>T</var><sub><var>i</var>−1</sub>('''<var>A</var>'''<sup>T</sup>)'''<var>r̂</var>'''<sub>0</sub>,  <var>'''A'''T</var><sub><var>i</var>−1</sub>('''<var>A</var>''')'''<var>r</var>'''<sub>0</sub>)}}.\n\nSimilarly to the case above, only the highest-order terms of {{math|<var>P</var><sub><var>i</var>−1</sub>('''<var>A</var>'''<sup>T</sup>)}} and {{math|<var>T</var><sub><var>i</var>−1</sub>('''<var>A</var>'''<sup>T</sup>)}} matter in the dot products thanks to biorthogonality and biconjugacy. It happens that {{math|<var>P</var><sub><var>i</var>−1</sub>('''<var>A</var>'''<sup>T</sup>)}} and {{math|<var>T</var><sub><var>i</var>−1</sub>('''<var>A</var>'''<sup>T</sup>)}} have the same leading coefficient. Thus, they can be replaced simultaneously with {{math|<var>Q</var><sub><var>i</var>−1</sub>('''<var>A</var>'''<sup>T</sup>)}} in the formula, which leads to\n\n:{{math|<var>α<sub>i</sub></var> {{=}} (<var>Q</var><sub><var>i</var>−1</sub>('''<var>A</var>'''<sup>T</sup>)'''<var>r̂</var>'''<sub>0</sub>,  <var>P</var><sub><var>i</var>−1</sub>('''<var>A</var>''')'''<var>r</var>'''<sub>0</sub>)/(<var>Q</var><sub><var>i</var>−1</sub>('''<var>A</var>'''<sup>T</sup>)'''<var>r̂</var>'''<sub>0</sub>,  <var>'''A'''T</var><sub><var>i</var>−1</sub>('''<var>A</var>''')'''<var>r</var>'''<sub>0</sub>) {{=}} <var>ρ̃</var><sub><var>i</var></sub>/('''<var>r̂</var>'''<sub>0</sub>,  '''<var>A</var>'''<var>Q</var><sub><var>i</var>−1</sub>('''<var>A</var>''')<var>T</var><sub><var>i</var>−1</sub>('''<var>A</var>''')'''<var>r</var>'''<sub>0</sub>) {{=}} <var>ρ̃</var><sub><var>i</var></sub>/('''<var>r̂</var>'''<sub>0</sub>, '''<var>Ap̃</var>'''<sub><var>i</var></sub>)}}.\n\nFinally, BiCGSTAB selects {{math|<var>ω<sub>i</sub></var>}} to minimize {{math|'''<var>r̃</var>'''<sub><var>i</var></sub> {{=}} ('''I''' − <var>ω<sub>i</sub>'''A'''</var>)<var>'''s'''<sub>i</sub></var>}} in {{math|2}}-norm as a function of {{math|<var>ω<sub>i</sub></var>}}. This is achieved when\n\n:{{math|(('''I''' − <var>ω<sub>i</sub>'''A'''</var>)<var>'''s'''<sub>i</sub></var>, <var>'''As'''<sub>i</sub></var>) {{=}} 0}},\n\ngiving the optimal value\n\n:{{math|<var>ω<sub>i</sub></var> {{=}} (<var>'''As'''<sub>i</sub></var>, <var>'''s'''<sub>i</sub></var>)/(<var>'''As'''<sub>i</sub></var>, <var>'''As'''<sub>i</sub></var>)}}.\n\n==Generalization==\nBiCGSTAB can be viewed as a combination of BiCG and [[GMRES]] where each BiCG step is followed by a GMRES({{math|1}}) (i.e., GMRES restarted at each step) step to repair the irregular convergence behavior of CGS, as an improvement of which BiCGSTAB was developed. However, due to the use of degree-one minimum residual polynomials, such repair may not be effective if the matrix {{math|'''<var>A</var>'''}} has large complex eigenpairs. In such cases, BiCGSTAB is likely to stagnate, as confirmed by numerical experiments.\n\nOne may expect that higher-degree minimum residual polynomials may better handle this situation. This gives rise to algorithms including BiCGSTAB2{{ref|bicgstab2}} and the more general BiCGSTAB({{math|<var>l</var>}}){{ref|bicgstab(l)}}. In BiCGSTAB({{math|<var>l</var>}}), a GMRES({{math|<var>l</var>}}) step follows every {{math|<var>l</var>}} BiCG steps. BiCGSTAB2 is equivalent to BiCGSTAB({{math|<var>l</var>}}) with {{math|<var>l</var> {{=}} 2}}.\n\n==See also==\n* [[Biconjugate gradient method]]\n* [[Conjugate gradient squared method]]\n* [[Conjugate gradient method]]\n\n==References==\n* {{Cite journal | doi = 10.1137/0913035 | title = Bi-CGSTAB: A Fast and Smoothly Converging Variant of Bi-CG for the Solution of Nonsymmetric Linear Systems | year = 1992 | last1 = Van der Vorst | first1 = H. A. | journal = [[SIAM Journal on Scientific Computing|SIAM J. Sci. Stat. Comput.]] | volume = 13 | issue = 2 | pages = 631–644 }}\n* {{cite book\n|last = Saad\n|first = Y.\n|title = Iterative Methods for Sparse Linear Systems\n|edition = 2nd\n|chapter = §7.4.2 BICGSTAB\n|pages = 231–234\n|year = 2003\n|publisher = SIAM\n|isbn = 978-0-89871-534-7\n|doi = 10.2277/0898715342\n}}\n* {{note|bicgstab2}}{{Cite journal| doi = 10.1137/0914062| title = Variants of BICGSTAB for Matrices with Complex Spectrum| year = 1993| last1 = Gutknecht | first1 = M. H.| journal = [[SIAM Journal on Scientific Computing|SIAM J. Sci. Comput.]]| volume = 14| issue = 5| pages = 1020–1033 }}\n* {{note|bicgstab(l)}}{{cite journal\n| last1 = Sleijpen | first1 = G. L. G.\n| last2 = Fokkema | first2 = D. R.\n|date=November 1993\n| title = BiCGstab(''l'') for linear equations involving unsymmetric matrices with complex spectrum\n| journal = [[Electronic Transactions on Numerical Analysis]]\n| volume = 1\n| pages = 11–32\n| publisher = Kent State University\n| location = Kent, OH\n| issn = 1068-9613\n| url = http://www.emis.ams.org/journals/ETNA/vol.1.1993/pp11-32.dir/pp11-32.pdf\n| format = PDF\n}}\n\n{{Numerical linear algebra}}\n\n[[Category:Numerical linear algebra]]\n[[Category:Gradient methods]]\n[[Category:Articles with example pseudocode]]"
    },
    {
      "title": "Conjugate gradient method",
      "url": "https://en.wikipedia.org/wiki/Conjugate_gradient_method",
      "text": "[[File:Conjugate gradient illustration.svg|right|thumb|A comparison of the convergence of [[gradient descent]] with optimal step size (in green) and conjugate vector (in red) for minimizing a quadratic function associated with a given linear system. Conjugate gradient, assuming exact arithmetic, converges in at most ''n'' steps, where ''n'' is the size of the matrix of the system (here ''n''&nbsp;=&nbsp;2).]]\n\nIn [[mathematics]], the '''conjugate gradient method''' is an [[algorithm]] for the [[numerical solution]] of particular [[system of linear equations|systems of linear equations]], namely those whose matrix is [[symmetric matrix|symmetric]] and [[positive-definite matrix|positive-definite]]. The conjugate gradient method is often implemented as an [[iterative method|iterative algorithm]], applicable to [[sparse matrix|sparse]] systems that are too large to be handled by a direct implementation or other direct methods such as the [[Cholesky decomposition]]. Large sparse systems often arise when numerically solving [[partial differential equation]]s or optimization problems.\n\nThe conjugate gradient method can also be used to solve unconstrained [[Mathematical optimization|optimization]] problems such as [[energy minimization]]. It was mainly developed by [[Magnus Hestenes]] and [[Eduard Stiefel]]<ref>{{cite document |last=Straeter |first=T. A. |title=On the Extension of the Davidon-Broyden Class of Rank One, Quasi-Newton Minimization Methods to an Infinite Dimensional Hilbert Space with Applications to Optimal Control Problems |work=NASA Technical Reports Server |publisher=NASA |hdl=2060/19710026200 }}</ref> who programmed it on the [[Z4 (computer)|Z4]].<ref>[[Ambros Speiser|Speiser, Ambros]] \"''Konrad Zuse and the ERMETH: A worldwide comparison of architectures''\" (\"Konrad Zuse und die ERMETH: Ein weltweiter Architektur-Vergleich\"), in: Hans Dieter Hellige (ed.): Geschichten der Informatik. Visionen, Paradigmen, Leitmotive. Berlin, Springer 2004, {{ISBN|3-540-00217-0}}. p. 185.</ref>\n\nThe [[biconjugate gradient method]] provides a generalization to non-symmetric matrices. Various [[nonlinear conjugate gradient method]]s seek minima of nonlinear equations.\n<!--\n{|class=\"infobox bordered\" style=\"width: 22em; text-align: left; font-size: 95%;\"\n|colspan=\"2\" style=\"text-align:center;\" | [[File:Banana-ConjGrad.gif|420px| ]]<br /><br />'''Conjugate gradient search over Rosenbrock banana function'''<br />\n|-\n|}-->\n\n==Description of the problem addressed by conjugate gradients==\n\nSuppose we want to solve the [[system of linear equations]]\n\n:<math>\\mathbf{A}\\mathbf{x} = \\mathbf{b}</math>\n\nfor the vector '''x''', where the known ''n'' × ''n'' matrix '''A''' is [[Symmetric matrix|symmetric]] (i.e., '''A'''<sup>T</sup> = '''A'''), [[positive-definite matrix|positive-definite]] (i.e. '''x'''<sup>T</sup>'''Ax''' > 0 for all non-zero vectors '''x''' in '''R'''<sup>''n''</sup>), and [[real number|real]], and '''b''' is known as well. We denote the unique solution of this system by <math>\\mathbf{x}_*</math>.\n\n==As a direct method==\n\nWe say that two non-zero vectors '''u''' and '''v''' are [[Inner automorphism|conjugate]] (with respect to '''A''') if\n\n:<math> \\mathbf{u}^\\mathsf{T} \\mathbf{A} \\mathbf{v} = 0. </math>\n\nSince '''A''' is symmetric and positive-definite, the left-hand side defines an [[inner product space|inner product]]\n\n:<math>\n \\mathbf{u}^\\mathsf{T} \\mathbf{A} \\mathbf{v} =\n \\langle \\mathbf{u}, \\mathbf{v} \\rangle_\\mathbf{A} :=\n \\langle \\mathbf{A} \\mathbf{u}, \\mathbf{v}\\rangle =\n \\langle \\mathbf{u}, \\mathbf{A}^\\mathsf{T} \\mathbf{v}\\rangle =\n \\langle \\mathbf{u}, \\mathbf{A}\\mathbf{v}\\rangle.\n</math>\n\nTwo vectors are conjugate if and only if they are orthogonal with respect to this inner product. Being conjugate is a symmetric relation: if '''u''' is conjugate to '''v''', then '''v''' is conjugate to '''u'''. Suppose that\n\n:<math>P = \\{ \\mathbf{p}_1, \\dots, \\mathbf{p}_n \\}</math>\n\nis a set of ''n'' mutually conjugate vectors (with respect to '''A'''). Then {{mvar|P}} forms a [[basis (linear algebra)|basis]] for <math>\\mathbb{R}^n</math>, and we may express the solution {{math|'''x'''<sub>∗</sub>}} of <math>\\mathbf{Ax} = \\mathbf{b}</math> in this basis:\n\n:<math>\\mathbf{x}_* = \\sum^{n}_{i=1} \\alpha_i \\mathbf{p}_i.</math>\n\nBased on this expansion we calculate:\n\n:<math>\\mathbf{A} \\mathbf{x}_* = \\sum^{n}_{i=1} \\alpha_i \\mathbf{A} \\mathbf{p}_i.</math>\n\nLeft-multiplying by <math>\\mathbf{p}_k^\\mathsf{T}</math>:\n\n:<math>\\mathbf{p}_k^\\mathsf{T} \\mathbf{A} \\mathbf{x}_* = \\sum^{n}_{i=1} \\alpha_i \\mathbf{p}_k^\\mathsf{T} \\mathbf{A} \\mathbf{p}_i,</math>\n\nsubstituting <math>\\mathbf{Ax_*} = \\mathbf{b}</math> and <math>\\mathbf{u}^\\mathsf{T} \\mathbf{A} \\mathbf{v} = \\langle \\mathbf{u}, \\mathbf{v} \\rangle_\\mathbf{A}</math>:\n\n:<math>\\mathbf{p}_k^\\mathsf{T} \\mathbf{b} = \\sum^{n}_{i=1} \\alpha_i \\left \\langle \\mathbf{p}_k, \\mathbf{p}_i \\right \\rangle_{\\mathbf{A}},</math>\n\nthen <math>\\mathbf{u}^\\mathsf{T} \\mathbf{v} = \\langle \\mathbf{u}, \\mathbf{v} \\rangle</math> and using <math>\\forall i \\neq k: \\langle \\mathbf{p}_k, \\mathbf{p}_i \\rangle_{\\mathbf{A}} = 0</math> yields\n\n:<math>\\langle \\mathbf{p}_k, \\mathbf{b} \\rangle = \\alpha_k \\langle \\mathbf{p}_k, \\mathbf{p}_k \\rangle_{\\mathbf{A}},\n</math>\n\nwhich implies\n\n:<math>\\alpha_k = \\frac{\\langle \\mathbf{p}_k, \\mathbf{b} \\rangle}{\\langle \\mathbf{p}_k, \\mathbf{p}_k \\rangle_\\mathbf{A}}.</math>\n\nThis gives the following method for solving the equation {{math|'''Ax''' {{=}} '''b'''}}: find a sequence of ''n'' conjugate directions, and then compute the coefficients {{mvar|α<sub>k</sub>}}.\n\n==As an iterative method==\n\nIf we choose the conjugate vectors '''p'''<sub>''k''</sub> carefully, then we may not need all of them to obtain a good approximation to the solution {{math|'''x'''<sub>∗</sub>}}. So, we want to regard the conjugate gradient method as an iterative method. This also allows us to approximately solve systems where ''n'' is so large that the direct method would take too much time.\n \nWe denote the initial guess for {{math|'''x'''<sub>∗</sub>}} by {{math|'''x'''<sub>0</sub>}} (we can assume without loss of generality that {{math|'''x'''<sub>0</sub> {{=}} '''0'''}}, otherwise consider the system '''Az''' = '''b''' − '''Ax'''<sub>0</sub> instead). Starting with '''x'''<sub>0</sub> we search for the solution and in each iteration we need a metric to tell us whether we are closer to the solution {{math|'''x'''<sub>∗</sub>}} (that is unknown to us). This metric comes from the fact that the solution {{math|'''x'''<sub>∗</sub>}} is also the unique minimizer of the following [[quadratic function]]\n\n:<math> \n  f(\\mathbf{x}) = \\tfrac12 \\mathbf{x}^\\mathsf{T} \\mathbf{A}\\mathbf{x} - \\mathbf{x}^\\mathsf{T} \\mathbf{b}, \\qquad \\mathbf{x}\\in\\mathbf{R}^n \\,. \n</math>\nThe existence of a unique minimizer is apparent as its second derivative is given by a symmetric positive-definite matrix\n:<math>\n  \\nabla^2 f(\\mathbf{x}) = \\mathbf{A} \\,,\n</math>\nand that the minimizer (use D''f''('''x''')=0) solves the initial problem is obvious from its first derivative\n:<math>\n  \\nabla f(\\mathbf{x}) = \\mathbf{A} \\mathbf{x} - \\mathbf{b} \\,.\n</math>\n\n<!--  so if f('''x''') becomes smaller in an iteration it means that we are closer to {{math|'''x'''<sub>∗</sub>}}.Note that the +c has been added to the equation above. Without the c the trivial solution of x=0 is the minimum and the solution of the original equation. If x0 is the minimum, then the quadratic can be described as (x^T-x0^T)*A*(x-x0)-c0=0 (c0 is f at the minimum while x0 is the solution at the minimum). Expanding this you get x^T*X*x-x^T*b+c where b=2*A*x0 and c=x0^T*A*x0-c0 ... wait a minute... for the purposes of minimization c0 does not matter... if this is the case then c0 could be selected such that c0=x0^T*A*x0 in which case the original form x^T*X*x-x^T*b is still valid... -->\nThis suggests taking the first basis vector '''p'''<sub>0</sub> to be the negative  of the gradient of ''f'' at '''x''' = '''x'''<sub>0</sub>. The gradient of ''f'' equals {{math|'''Ax''' − '''b'''}}. Starting with an initial guess '''x'''<sub>0</sub>, this means we take '''p'''<sub>0</sub> = '''b''' − '''Ax'''<sub>0</sub>. The other vectors in the basis will be conjugate to the gradient, hence the name ''conjugate gradient method''.  Note that '''p'''<sub>0</sub> is also the [[residual (numerical analysis)|residual]] provided by this initial step of the algorithm.\n\nLet '''r'''<sub>''k''</sub> be the [[residual (numerical analysis)|residual]] at the ''k''th step:\n:<math> \\mathbf{r}_k = \\mathbf{b} - \\mathbf{Ax}_k.</math>\nAs observed above, '''r'''<sub>''k''</sub> is the negative gradient of ''f'' at '''x''' = '''x'''<sub>''k''</sub>, so the [[gradient descent]] method would require to move in the direction '''r'''<sub>''k''</sub>. Here, however, we insist that the directions '''p'''<sub>''k''</sub> be conjugate to each other. A practical way to enforce this, is by requiring that the next search direction be built out of the current residual and all previous search directions.<ref>The conjugation constraint is an orthonormal-type constraint and hence the algorithm bears resemblance to [[Gram–Schmidt process|Gram-Schmidt orthonormalization]].</ref> This gives the following expression:\n:<math>\\mathbf{p}_{k} = \\mathbf{r}_{k} - \\sum_{i < k}\\frac{\\mathbf{p}_i^\\mathsf{T} \\mathbf{A} \\mathbf{r}_{k}}{\\mathbf{p}_i^\\mathsf{T}\\mathbf{A} \\mathbf{p}_i} \\mathbf{p}_i</math>\n(see the picture at the top of the article for the effect of the conjugacy constraint on convergence). Following this direction, the next optimal location is given by\n:<math> \\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k </math>\nwith \n:<math> \\alpha_{k} = \\frac{\\mathbf{p}_k^\\mathsf{T} (\\mathbf{b} - \\mathbf{Ax}_k )}{\\mathbf{p}_k^\\mathsf{T} \\mathbf{A} \\mathbf{p}_k} = \\frac{\\mathbf{p}_{k}^\\mathsf{T} \\mathbf{r}_{k}}{\\mathbf{p}_{k}^\\mathsf{T} \\mathbf{A} \\mathbf{p}_{k}},  </math>\n\nwhere the last equality follows from the definition of '''r'''<sub>''k''</sub> .\nThe expression for <math> \\alpha_k </math> can be derived if one substitutes the expression for '''x'''<sub>''k''+1</sub> into ''f'' and minimizing it w.r.t. <math> \\alpha_k </math>\n:<math>\n\\begin{align}\nf(\\mathbf{x}_{k+1}) &= f(\\mathbf{x}_k + \\alpha_k \\mathbf{p}_k) =: g(\\alpha_k)\n  \\\\\ng'(\\alpha_k) &\\overset{!}{=} 0\n  \\quad \\Rightarrow \\quad\n  \\alpha_{k} = \\frac{\\mathbf{p}_k^\\mathsf{T} (\\mathbf{b} - \\mathbf{Ax}_k)}{\\mathbf{p}_k^\\mathsf{T} \\mathbf{A} \\mathbf{p}_k} \\,.\n\\end{align}\n</math>\n\n===The resulting algorithm===\nThe above algorithm gives the most straightforward explanation of the conjugate gradient method.  Seemingly, the algorithm as stated requires storage of all previous searching directions and residue vectors, as well as many matrix-vector multiplications, and thus can be computationally expensive. However, a closer analysis of the algorithm shows that '''r'''<sub>''i''</sub> is orthogonal to '''r'''<sub>''j''</sub> ,   i.e. <math>\\mathbf{r}_i^\\mathsf{T} \\mathbf{r}_j=0 </math> , for i ≠ j. And '''p'''<sub>''i''</sub> is A-orthogonal to '''p'''<sub>''j''</sub> ,   i.e. <math>\\mathbf{p}_i^\\mathsf{T}A \\mathbf{p}_j=0 </math> , for i ≠ j. This can be regarded that as the algorithm progresses, '''p'''<sub>''i''</sub> and '''r'''<sub>''i''</sub> span the same Krylov subspace. Where '''r'''<sub>''i''</sub> form the orthogonal basis with respect to standard inner product, and '''p'''<sub>''i''</sub> form the orthogonal basis with respect to inner product induced by A. Therefore, '''x'''<sub>''k''</sub> can be regarded as the projection of '''x''' on the Krylov subspace.\n\nThe algorithm is detailed below for solving '''Ax''' = '''b''' where '''A''' is a real, symmetric, positive-definite matrix. The input vector '''x'''<sub>0</sub> can be an approximate initial solution or '''0'''. It is a different formulation of the exact procedure described above.\n\n:<math>\\begin{align}\n& \\mathbf{r}_0 := \\mathbf{b} - \\mathbf{A x}_0 \\\\\n& \\hbox{if } \\mathbf{r}_{0} \\text{ is sufficiently small, then return } \\mathbf{x}_{0} \\text{ as the result}\\\\\n& \\mathbf{p}_0 := \\mathbf{r}_0 \\\\\n& k := 0 \\\\\n& \\text{repeat} \\\\\n& \\qquad \\alpha_k := \\frac{\\mathbf{r}_k^\\mathsf{T} \\mathbf{r}_k}{\\mathbf{p}_k^\\mathsf{T} \\mathbf{A p}_k}  \\\\\n& \\qquad \\mathbf{x}_{k+1} := \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k \\\\\n& \\qquad \\mathbf{r}_{k+1} := \\mathbf{r}_k - \\alpha_k \\mathbf{A p}_k \\\\\n& \\qquad \\hbox{if } \\mathbf{r}_{k+1} \\text{ is sufficiently small, then exit loop} \\\\\n& \\qquad \\beta_k := \\frac{\\mathbf{r}_{k+1}^\\mathsf{T} \\mathbf{r}_{k+1}}{\\mathbf{r}_k^\\mathsf{T} \\mathbf{r}_k} \\\\\n& \\qquad \\mathbf{p}_{k+1} := \\mathbf{r}_{k+1} + \\beta_k \\mathbf{p}_k \\\\\n& \\qquad k := k + 1 \\\\\n& \\text{end repeat} \\\\\n& \\text{return } \\mathbf{x}_{k+1} \\text{ as the result}\n\\end{align}</math>\n\nThis is the most commonly used algorithm. The same formula for {{mvar|β<sub>k</sub>}} is also used in the Fletcher–Reeves [[nonlinear conjugate gradient method]].\n\n====Computation of alpha and beta====\nIn the algorithm, {{mvar|α<sub>k</sub>}} is chosen such that <math>\\mathbf{r}_{k+1}</math> is orthogonal to '''r'''<sub>''k''</sub>. The denominator is simplified from\n\n:<math>\\alpha_k = \\frac{\\mathbf{r}_{k}^\\mathsf{T} \\mathbf{r}_{k}}{\\mathbf{r}_{k}^\\mathsf{T} \\mathbf{A} \\mathbf{p}_k} = \\frac{\\mathbf{r}_k^\\mathsf{T} \\mathbf{r}_k}{\\mathbf{p}_k^\\mathsf{T} \\mathbf{A p}_k} </math>\n\nsince <math>\\mathbf{r}_{k+1} = \\mathbf{p}_{k+1}-\\mathbf{\\beta}_{k}\\mathbf{p}_{k}</math>. The {{mvar|β<sub>k</sub>}} is chosen such that <math>\\mathbf{p}_{k+1}</math> is conjugated to '''p'''<sub>''k''</sub>. Initially, {{mvar|β<sub>k</sub>}} is\n\n:<math>\\beta_k = - \\frac{\\mathbf{r}_{k+1}^\\mathsf{T} \\mathbf{A} \\mathbf{p}_k}{\\mathbf{p}_k^\\mathsf{T} \\mathbf{A} \\mathbf{p}_k}</math>\n\nusing\n\n:<math>\\mathbf{r}_{k+1} = \\mathbf{r}_{k} - \\alpha_{k} \\mathbf{A} \\mathbf{p}_{k}</math>\n\nand equivalently\n\n<math> \\mathbf{A} \\mathbf{p}_{k} = \\frac{1}{\\alpha_{k}} (\\mathbf{r}_{k} - \\mathbf{r}_{k+1}), </math>\n\nthe numerator of {{mvar|β<sub>k</sub>}} is rewritten as\n\n:<math> \\mathbf{r}_{k+1}^\\mathsf{T} \\mathbf{A} \\mathbf{p}_k = \\frac{1}{\\alpha_k} \\mathbf{r}_{k+1}^\\mathsf{T} (\\mathbf{r}_k - \\mathbf{r}_{k+1}) = - \\frac{1}{\\alpha_k} \\mathbf{r}_{k+1}^\\mathsf{T} \\mathbf{r}_{k+1} </math>\n\nbecause <math>\\mathbf{r}_{k+1}</math> and '''r'''<sub>''k''</sub> are orthogonal by design. The denominator is rewritten as\n \n:<math> \\mathbf{p}_k^\\mathsf{T} \\mathbf{A} \\mathbf{p}_k = (\\mathbf{r}_k + \\beta_{k-1} \\mathbf{p}_{k-1})^\\mathsf{T} \\mathbf{A} \\mathbf{p}_k = \\frac{1}{\\alpha_k} \\mathbf{r}_k^\\mathsf{T} (\\mathbf{r}_k - \\mathbf{r}_{k+1}) = \\frac{1}{\\alpha_k} \\mathbf{r}_k^\\mathsf{T} \\mathbf{r}_k </math>\n\nusing that the search directions '''p'''<sub>''k''</sub> are conjugated and again that the residuals are orthogonal. This gives the {{mvar|β}} in the algorithm after cancelling {{mvar|α<sub>k</sub>}}.\n\n====Example code in [[MATLAB]] / [[GNU Octave]]====\n<source lang=\"matlab\">\nfunction x = conjgrad(A, b, x)\n    r = b - A * x;\n    p = r;\n    rsold = r' * r;\n\n    for i = 1:length(b)\n        Ap = A * p;\n        alpha = rsold / (p' * Ap);\n        x = x + alpha * p;\n        r = r - alpha * Ap;\n        rsnew = r' * r;\n        if sqrt(rsnew) < 1e-10\n              break;\n        end\n        p = r + (rsnew / rsold) * p;\n        rsold = rsnew;\n    end\nend\n</source>\n\n===Numerical example===\n\nConsider the linear system '''Ax''' = '''b''' given by\n\n:<math>\\mathbf{A} \\mathbf{x}= \\begin{bmatrix} 4 & 1 \\\\ 1 & 3 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix},</math>\n\nwe will perform two steps of the conjugate gradient method beginning with the initial guess\n\n:<math>\\mathbf{x}_0 = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}</math>\n\nin order to find an approximate solution to the system.\n\n====Solution====\nFor reference, the exact solution is \n:<math> \\mathbf{x} = \\begin{bmatrix} \\frac{1}{11} \\\\\\\\ \\frac{7}{11} \\end{bmatrix} \\approx \\begin{bmatrix} 0.0909 \\\\\\\\ 0.6364 \\end{bmatrix}</math>\nOur first step is to calculate the residual vector '''r'''<sub>0</sub> associated with '''x'''<sub>0</sub>.  This residual is computed from the formula '''r'''<sub>0</sub> = '''b''' - '''Ax'''<sub>0</sub>, and in our case is equal to\n\n:<math>\\mathbf{r}_0 = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} - \n\\begin{bmatrix} 4 & 1 \\\\ 1 & 3 \\end{bmatrix}\n\\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} = \n\\begin{bmatrix}-8 \\\\ -3 \\end{bmatrix}.</math>\n\nSince this is the first iteration, we will use the residual vector '''r'''<sub>0</sub> as our initial search direction '''p'''<sub>0</sub>; the method of selecting '''p'''<sub>''k''</sub> will change in further iterations.\n\nWe now compute the scalar {{math|''α''<sub>0</sub>}} using the relationship\n\n:<math> \\alpha_0 = \\frac{\\mathbf{r}_0^\\mathsf{T} \\mathbf{r}_0}{\\mathbf{p}_0^\\mathsf{T} \\mathbf{A p}_0} = \\frac{\\begin{bmatrix} -8 & -3 \\end{bmatrix} \\begin{bmatrix} -8 \\\\ -3 \\end{bmatrix}}{  \\begin{bmatrix} -8 & -3 \\end{bmatrix} \\begin{bmatrix} 4 & 1 \\\\ 1 & 3 \\end{bmatrix} \\begin{bmatrix} -8 \\\\ -3 \\end{bmatrix}  } =\\frac{73}{331}.</math>\n\nWe can now compute '''x'''<sub>1</sub> using the formula\n\n:<math>\\mathbf{x}_1 = \\mathbf{x}_0 + \\alpha_0\\mathbf{p}_0 = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} + \\frac{73}{331} \\begin{bmatrix} -8 \\\\ -3 \\end{bmatrix} = \\begin{bmatrix} 0.2356 \\\\ 0.3384 \\end{bmatrix}.</math>\n\nThis result completes the first iteration, the result being an \"improved\" approximate solution to the system, '''x'''<sub>1</sub>. We may now move on and compute the next residual vector '''r'''<sub>1</sub> using the formula\n\n:<math>\\mathbf{r}_1 = \\mathbf{r}_0 - \\alpha_0 \\mathbf{A} \\mathbf{p}_0 = \\begin{bmatrix} -8 \\\\ -3 \\end{bmatrix} - \\frac{73}{331} \\begin{bmatrix} 4 & 1 \\\\ 1 & 3 \\end{bmatrix} \\begin{bmatrix} -8 \\\\ -3 \\end{bmatrix} = \\begin{bmatrix} -0.2810 \\\\ 0.7492 \\end{bmatrix}.</math>\n\nOur next step in the process is to compute the scalar {{math|''β''<sub>0</sub>}} that will eventually be used to determine the next search direction '''p'''<sub>1</sub>.\n\n:<math>\\beta_0 = \\frac{\\mathbf{r}_1^\\mathsf{T} \\mathbf{r}_1}{\\mathbf{r}_0^\\mathsf{T} \\mathbf{r}_0} = \\frac{\\begin{bmatrix} -0.2810 & 0.7492 \\end{bmatrix} \\begin{bmatrix} -0.2810 \\\\ 0.7492 \\end{bmatrix}}{\\begin{bmatrix} -8 & -3 \\end{bmatrix} \\begin{bmatrix} -8 \\\\ -3 \\end{bmatrix}} = 0.0088.</math>\n\nNow, using this scalar {{math|''β''<sub>0</sub>}}, we can compute the next search direction '''p'''<sub>1</sub> using the relationship\n\n:<math>\\mathbf{p}_1 = \\mathbf{r}_1 + \\beta_0 \\mathbf{p}_0 = \\begin{bmatrix} -0.2810 \\\\ 0.7492 \\end{bmatrix} + 0.0088 \\begin{bmatrix} -8 \\\\ -3 \\end{bmatrix} = \\begin{bmatrix} -0.3511 \\\\ 0.7229 \\end{bmatrix}.</math>\n\nWe now compute the scalar {{math|''α''<sub>1</sub>}} using our newly acquired '''p'''<sub>1</sub> using the same method as that used for {{math|''α''<sub>0</sub>}}.\n\n:<math> \\alpha_1 = \\frac{\\mathbf{r}_1^\\mathsf{T} \\mathbf{r}_1}{\\mathbf{p}_1^\\mathsf{T} \\mathbf{A p}_1} = \\frac{\\begin{bmatrix} -0.2810 & 0.7492 \\end{bmatrix} \\begin{bmatrix} -0.2810 \\\\ 0.7492 \\end{bmatrix}}{  \\begin{bmatrix} -0.3511 & 0.7229 \\end{bmatrix} \\begin{bmatrix} 4 & 1 \\\\ 1 & 3 \\end{bmatrix} \\begin{bmatrix} -0.3511 \\\\ 0.7229 \\end{bmatrix}  } = 0.4122.</math>\n\nFinally, we find '''x'''<sub>2</sub> using the same method as that used to find '''x'''<sub>1</sub>.\n\n:<math>\\mathbf{x}_2 = \\mathbf{x}_1 + \\alpha_1 \\mathbf{p}_1 = \\begin{bmatrix} 0.2356 \\\\ 0.3384 \\end{bmatrix} + 0.4122 \\begin{bmatrix} -0.3511 \\\\ 0.7229 \\end{bmatrix} = \\begin{bmatrix} 0.0909 \\\\ 0.6364 \\end{bmatrix}.</math>\n\nThe result, '''x'''<sub>2</sub>, is a \"better\" approximation to the system's solution than '''x'''<sub>1</sub> and '''x'''<sub>0</sub>.  If exact arithmetic were to be used in this example instead of limited-precision, then the exact solution would theoretically have been reached after ''n'' = 2 iterations (''n'' being the order of the system).\n\n==Convergence properties==\nThe conjugate gradient method can theoretically be viewed as a direct method, as it produces the exact solution after a finite number of iterations, which is not larger than the size of the matrix, in the absence of [[round-off error]]. However, the conjugate gradient method is unstable with respect to even small perturbations, e.g., most directions are not in practice conjugate, and the exact solution is never obtained. Fortunately, the conjugate gradient method can be used as an [[iterative method]] as it provides monotonically improving approximations <math>\\mathbf{x}_{k}</math> to the exact solution, which may reach the required tolerance after a relatively small (compared to the problem size) number of iterations. The improvement is typically linear and its speed is determined by the [[condition number]] <math>\\kappa(A)</math> of the system matrix <math>A</math>: the larger <math>\\kappa(A)</math> is, the slower the improvement.<ref name=saad1996iterative>{{cite book|last=Saad|first=Yousef|title=Iterative methods for sparse linear systems|year=2003|publisher=Society for Industrial and Applied Mathematics|location=Philadelphia, Pa.|isbn=978-0-89871-534-7|pages=195|edition=2nd}}</ref>\n\nIf <math>\\kappa(A)</math> is large,  [[preconditioning]] is used to replace the original system <math>\\mathbf{A x}-\\mathbf{b} = 0</math> with <math>\\mathbf{M}^{-1}(\\mathbf{A x}-\\mathbf{b}) = 0</math> such that <math>\\kappa(\\mathbf{M}^{-1}\\mathbf{A})</math> is smaller than <math>\\kappa(\\mathbf{A})</math>, see below.\n\n=== Convergence theorem ===\n\nDefine a subset of polynomials as\n:<math>\n  \\Pi_k^* := \\left\\lbrace \\ p \\in \\Pi_k \\ : \\ p(0)=1 \\ \\right\\rbrace \\,,\n</math>\nwhere <math> \\Pi_k </math> is the set of [[Polynomial ring|polynomials]] of maximal degree <math> k </math>.\n\nLet <math> \\left( \\mathbf{x}_k \\right)_k </math> be the iterative approximations of the exact solution <math> \\mathbf{x}_* </math>, and define the errors as <math> \\mathbf{e}_k := \\mathbf{x}_k - \\mathbf{x}_* </math>.\nNow, the rate of convergence can be approximated as <ref>{{Cite book |title=Iterative solution of large sparse systems of equations |last=Hackbusch |first=W. |isbn=9783319284835 |edition=2nd |location=Switzerland |publisher=Springer |oclc=952572240|date=2016-06-21 }}</ref>\n:<math>\n\\begin{align}\n  \\left\\| \\mathbf{e}_k \\right\\|_\\mathbf{A}\n  &= \\min_{p \\in \\Pi_k^*}  \\left\\| p(\\mathbf{A}) \\mathbf{e}_0 \\right\\|_\\mathbf{A}\n  \\\\\n  &\\leq \\min_{p \\in \\Pi_k^*} \\,  \\max_{ \\lambda \\in \\sigma(\\mathbf{A})} | p(\\lambda) | \\  \\left\\|  \\mathbf{e}_0 \\right\\|_\\mathbf{A}\n  \\\\\n  &\\leq 2 \\left( \\frac{ \\sqrt{\\kappa(\\mathbf{A})}-1 }{ \\sqrt{\\kappa(\\mathbf{A})}+1 } \\right)^k \\ \\left\\|  \\mathbf{e}_0 \\right\\|_\\mathbf{A}\n  \\,,\n\\end{align}\n</math>\nwhere <math> \\sigma(\\mathbf{A}) </math> denotes the [[Spectrum of a matrix|spectrum]], and <math> \\kappa(\\mathbf{A}) </math> denotes the [[condition number]].\n\nNote, the important limit when <math> \\kappa(\\mathbf{A}) </math> tends to <math> \\infty </math>\n:<math>\n  \\frac{ \\sqrt{\\kappa(\\mathbf{A})}-1 }{ \\sqrt{\\kappa(\\mathbf{A})}+1 }\n  \\approx 1 - \\frac{2}{\\sqrt{\\kappa(\\mathbf{A})}}\n  \\quad \\text{for} \\quad\n  \\kappa(\\mathbf{A}) \\gg 1\n  \\,.\n</math>\nThis limit shows a faster convergence rate compared to the iterative methods of [[Jacobi method|Jacobi]] or [[Gauss–Seidel method|Gauss-Seidel]] which scale as <math> \\approx 1 - \\frac{2}{\\kappa(\\mathbf{A})} </math>.\n\n==The preconditioned conjugate gradient method==\n{{See also|Preconditioner}}\nIn most cases, [[preconditioning]] is necessary to ensure fast convergence of the conjugate gradient method. The preconditioned conjugate gradient method takes the following form:\n\n:<math>\\mathbf{r}_0 := \\mathbf{b} - \\mathbf{A x}_0</math>\n:<math>\\mathbf{z}_0 := \\mathbf{M}^{-1} \\mathbf{r}_0</math>\n:<math>\\mathbf{p}_0 := \\mathbf{z}_0</math>\n:<math>k := 0 \\, </math>\n:'''repeat'''\n::<math>\\alpha_k := \\frac{\\mathbf{r}_k^\\mathsf{T} \\mathbf{z}_k}{\\mathbf{p}_k^\\mathsf{T} \\mathbf{A p}_k}</math>\n::<math>\\mathbf{x}_{k+1} := \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k</math>\n::<math>\\mathbf{r}_{k+1} := \\mathbf{r}_k - \\alpha_k \\mathbf{A p}_k</math>\n::'''if''' '''r'''<sub>''k''+1</sub> is sufficiently small '''then''' exit loop '''end if'''\n::<math>\\mathbf{z}_{k+1} := \\mathbf{M}^{-1} \\mathbf{r}_{k+1}</math>\n::<math>\\beta_k := \\frac{\\mathbf{z}_{k+1}^\\mathsf{T} \\mathbf{r}_{k+1}}{\\mathbf{z}_k^\\mathsf{T} \\mathbf{r}_k}</math>\n::<math>\\mathbf{p}_{k+1} := \\mathbf{z}_{k+1} + \\beta_k \\mathbf{p}_k</math>\n::<math>k := k + 1 \\, </math>\n:'''end repeat'''\n:The result is '''x'''<sub>''k''+1</sub>\n\nThe above formulation is equivalent to applying the conjugate gradient method without preconditioning to the system{{ref label|agonizing_pain|1|^}}\n:<math>\\mathbf{E}^{-1}\\mathbf{A}(\\mathbf{E}^{-1})^\\mathsf{T}\\mathbf{\\hat{x}}=\\mathbf{E}^{-1}\\mathbf{b}</math>\nwhere \n:<math>\\mathbf{EE}^\\mathsf{T}=\\mathbf{M}, \\qquad \\mathbf{\\hat{x}}=\\mathbf{E}^\\mathsf{T}\\mathbf{x}.</math>\n\nThe preconditioner matrix '''M''' has to be symmetric positive-definite and fixed, i.e., cannot change from iteration to iteration. \nIf any of these assumptions on the preconditioner is violated, the behavior of the preconditioned conjugate gradient method may become unpredictable.\n\nAn example of a commonly used [[preconditioner]] is the [[incomplete Cholesky factorization]].\n\n==The flexible preconditioned conjugate gradient method==\n\nIn numerically challenging applications, sophisticated preconditioners are used, which may lead to variable preconditioning, changing between iterations. Even if the preconditioner is symmetric positive-definite on every iteration, the fact that it may change makes the arguments above invalid, and in practical tests leads to a significant slow down of the convergence of the algorithm presented above. Using the [[nonlinear conjugate gradient method|Polak–Ribière]] formula\n\n:<math>\\beta_k := \\frac{\\mathbf{z}_{k+1}^\\mathsf{T} \\left(\\mathbf{r}_{k+1}-\\mathbf{r}_{k}\\right)}{\\mathbf{z}_k^\\mathsf{T} \\mathbf{r}_k}</math>\n\ninstead of the [[nonlinear conjugate gradient method|Fletcher–Reeves]] formula\n\n:<math>\\beta_k := \\frac{\\mathbf{z}_{k+1}^\\mathsf{T} \\mathbf{r}_{k+1}}{\\mathbf{z}_k^\\mathsf{T} \\mathbf{r}_k}</math>\n\nmay dramatically improve the convergence in this case.<ref>{{cite journal |doi=10.1137/S1064827597323415 |title=Inexact Preconditioned Conjugate Gradient Method with Inner-Outer Iteration |year=1999 |last1=Golub |first1=Gene H. |last2=Ye |first2=Qiang |journal=SIAM Journal on Scientific Computing |volume=21 |issue=4 |pages=1305|citeseerx=10.1.1.56.1755 }}</ref> This version of the preconditioned conjugate gradient method can be  called<ref>{{cite journal|doi=10.1137/S1064827599362314|title=Flexible Conjugate Gradients|year=2000|last1=Notay|first1=Yvan|journal=SIAM Journal on Scientific Computing|volume=22|issue=4|pages=1444–1460|citeseerx=10.1.1.35.7473}}</ref> '''flexible,''' as it allows for variable preconditioning. \nThe flexible version is also shown<ref>Henricus Bouwmeester, Andrew Dougherty, Andrew V Knyazev. Nonsymmetric Preconditioning for Conjugate Gradient and Steepest Descent Methods. Procedia Computer Science, Volume 51, Pages 276-285, Elsevier, 2015. https://doi.org/10.1016/j.procs.2015.05.241</ref> to be robust even if the preconditioner is not symmetric positive definite (SPD).\n\nThe implementation of the flexible version requires storing an extra vector. For a fixed SPD preconditioner, <math>\\mathbf{z}_{k+1}^\\mathsf{T} \\mathbf{r}_{k}=0,</math> so both formulas for {{mvar|β<sub>k</sub>}} are equivalent in exact arithmetic, i.e., without the [[round-off error]].\n\nThe mathematical explanation of the better convergence behavior of the method with the [[nonlinear conjugate gradient method|Polak–Ribière]] formula is that the method is '''locally optimal''' in this case, in particular, it does not converge slower than the locally optimal steepest descent method.<ref>{{cite journal|doi=10.1137/060675290|title=Steepest Descent and Conjugate Gradient Methods with Variable Preconditioning| year=2008| last1=Knyazev|first1=Andrew V.|last2=Lashuk|first2=Ilya|journal=SIAM Journal on Matrix Analysis and Applications|volume=29|issue=4|pages=1267|arxiv=math/0605767}}</ref>\n\n===Example code in MATLAB / GNU Octave===\n<source lang=\"matlab\">\nfunction [x, k] = cgp(x0, A, C, b, mit, stol, bbA, bbC)\n% Synopsis:\n% x0: initial point\n% A: Matrix A of the system Ax=b\n% C: Preconditioning Matrix can be left or right\n% mit: Maximum number of iterations\n% stol: residue norm tolerance\n% bbA: Black Box that computes the matrix-vector product for A * u\n% bbC: Black Box that computes:\n%      for left-side preconditioner : ha = C \\ ra\n%      for right-side preconditioner: ha = C * ra\n% x: Estimated solution point\n% k: Number of iterations done \n%\n% Example:\n% tic;[x, t] = cgp(x0, S, speye(1), b, 3000, 10^-8, @(Z, o) Z*o, @(Z, o) o);toc\n% Elapsed time is 0.550190 seconds.\n%\n% Reference:\n%  Métodos iterativos tipo Krylov para sistema lineales\n%  B. Molina y M. Raydan - {{ISBN|908-261-078-X}}\n        if nargin < 8, error('Not enough input arguments. Try help.'); end;\n        if isempty(A), error('Input matrix A must not be empty.'); end;\n        if isempty(C), error('Input preconditioner matrix C must not be empty.'); end;\n        x = x0;\n        ha = 0;\n        hp = 0;\n        hpp = 0;\n        ra = 0;\n        rp = 0;\n        rpp = 0;\n        u = 0;\n        k = 0;\n\n        ra = b - bbA(A, x0); % <--- ra = b - A * x0;\n        while norm(ra, inf) > stol\n                ha = bbC(C, ra); % <--- ha = C \\ ra;\n                k = k + 1;\n                if (k == mit), warning('GCP:MAXIT', 'mit reached, no conversion.'); return; end;\n                hpp = hp;\n                rpp = rp;\n                hp = ha;\n                rp = ra;\n                t = rp' * hp;\n                if k == 1\n                        u = hp;\n                else\n                        u = hp + (t / (rpp' * hpp)) * u;\n                end;\n                Au = bbA(A, u); % <--- Au = A * u;\n                a = t / (u' * Au);\n                x = x + a * u;\n                ra = rp - a * Au;\n        end;\n</source>\n\n==Vs. the locally optimal steepest descent method==\n\nIn both the original and the preconditioned conjugate gradient methods one only needs to set  <math>\\beta_k := 0</math> in order to make them locally optimal, using the [[line search]], [[steepest descent]] methods. With this substitution, vectors {{math|'''p'''}} are always the same as vectors {{math|'''z'''}}, so there is no need to store vectors {{math|'''p'''}}. Thus, every iteration of these [[steepest descent]] methods is a bit cheaper compared to that for the conjugate gradient methods. However, the latter converge faster, unless a (highly) variable and/or non-SPD [[preconditioner]] is used, see above.\n\n==Derivation of the method==\n{{main|Derivation of the conjugate gradient method}}\n\nThe conjugate gradient method can be derived from several different perspectives, including specialization of the conjugate direction method for optimization, and variation of the [[Arnoldi iteration|Arnoldi]]/[[Lanczos iteration|Lanczos]] iteration for [[eigenvalue]] problems. Despite differences in their approaches, these derivations share a common topic—proving the orthogonality of the residuals and conjugacy of the search directions. These two properties are crucial to developing the well-known succinct formulation of the method.\n\n==Conjugate gradient on the normal equations==\nThe conjugate gradient method can be applied to an arbitrary ''n''-by-''m'' matrix by applying it to [[normal equations]] '''A'''<sup>T</sup>'''A''' and right-hand side vector '''A'''<sup>T</sup>'''b''', since  '''A'''<sup>T</sup>'''A''' is a symmetric [[Positive-definite matrix#Negative-definite.2C semidefinite and indefinite matrices|positive-semidefinite]] matrix for any '''A'''. The result is conjugate gradient on the normal equations (CGNR).\n\n: '''A'''<sup>T</sup>'''Ax''' =  '''A'''<sup>T</sup>'''b'''\n\nAs an iterative method, it is not necessary to form  '''A'''<sup>T</sup>'''A''' explicitly in memory but only to perform the matrix-vector and transpose matrix-vector multiplications. Therefore, CGNR is particularly useful when ''A'' is a [[sparse matrix]] since these operations are usually extremely efficient. However the downside of forming the normal equations is that the [[condition number]] κ('''A'''<sup>T</sup>'''A''') is equal to κ<sup>2</sup>('''A''') and so the rate of convergence of CGNR may be slow and the quality of the approximate solution may be sensitive to roundoff errors. Finding a good [[preconditioner]] is often an important part of using the CGNR method.\n\nSeveral algorithms have been proposed (e.g., CGLS, LSQR). The LSQR algorithm purportedly has the best numerical stability when '''A''' is ill-conditioned, i.e., '''A''' has a large [[condition number]].\n\n==See also==\n* [[Biconjugate gradient method]] (BiCG)\n* [[Conjugate residual method]]\n* [[Belief propagation#Gaussian belief propagation .28GaBP.29|Gaussian belief propagation]]\n* [[Iterative method#Linear systems|Iterative method. Linear systems]]\n* [[Krylov subspace]]\n* [[Nonlinear conjugate gradient]] method\n* [[Preconditioning]]\n* [[Sparse matrix-vector multiplication]]\n\n==Notes==\n{{Reflist}}\n\n==References==\nThe conjugate gradient method was originally proposed in\n*{{cite journal|last = [[Magnus Hestenes|Hestenes]]|first = [[Magnus Hestenes|Magnus R.]]|author2=Stiefel, Eduard |authorlink2=Eduard Stiefel |title = Methods of Conjugate Gradients for Solving Linear Systems|journal = Journal of Research of the National Bureau of Standards|volume = 49|issue = 6|pages = 409|date=December 1952|doi=10.6028/jres.049.044}}\nDescriptions of the method can be found in the following text books:\n* {{cite book|first=Kendell A. |last=Atkinson |year=1988|title=An introduction to numerical analysis|edition=2nd |chapter= Section 8.9|publisher= John Wiley and Sons| isbn= 978-0-471-50023-0}}\n* {{cite book|first=Mordecai|last= Avriel |year=2003|title=Nonlinear Programming: Analysis and Methods|publisher= Dover Publishing| isbn= 978-0-486-43227-4}}\n* {{cite book|first1=Gene H. |last1=Golub |first2= Charles F.|last2= Van Loan|title=Matrix computations|edition=3rd|chapter= Chapter 10|publisher= Johns Hopkins University Press| isbn =978-0-8018-5414-9|date=1996-10-15 }}\n* {{cite book|first=Yousef|last= Saad|title=Iterative methods for sparse linear systems|edition=2nd |chapter=Chapter 6\n|publisher= SIAM| isbn= 978-0-89871-534-7|date= 2003-04-01}}\n\n==External links==\n* {{springer|title=Conjugate gradients, method of|id=p/c025030}}\n\n{{Numerical linear algebra}}\n\n{{DEFAULTSORT:Conjugate Gradient Method}}\n[[Category:Numerical linear algebra]]\n[[Category:Gradient methods]]\n[[Category:Articles with example MATLAB/Octave code]]"
    },
    {
      "title": "Coordinate descent",
      "url": "https://en.wikipedia.org/wiki/Coordinate_descent",
      "text": "'''Coordinate descent''' is an [[optimization algorithm]] that successively minimizes along coordinate directions to find the minimum of a function. At each iteration, the algorithm determines a [[Coordinate system|coordinate]] or coordinate block via a coordinate selection rule, then exactly or inexactly minimizes over the corresponding coordinate hyperplane while fixing all other coordinates or coordinate blocks. A [[line search]] along the [[Coordinate system|coordinate]] direction can be performed at the current iterate to determine the appropriate step size. Coordinate descent is applicable in both differentiable and derivative-free contexts.\n\n==Description==\n\nCoordinate descent is based on the idea that the minimization of a multivariable function <math>F(\\mathbf{x})</math> can be achieved by minimizing it along one direction at a time, i.e., solving univariate (or at least much simpler) optimization problems in a loop.<ref name=\"wright\">{{Cite journal |last=Wright |first=Stephen J. |title=Coordinate descent algorithms |journal=Mathematical Programming |volume=151 |issue=1 |year=2015 |pages=3–34 |arxiv=1502.04759 |doi=10.1007/s10107-015-0892-3}}</ref> In the simplest case of ''cyclic coordinate descent'', one cyclically iterates through the directions, one at a time, minimizing the objective function with respect to each coordinate direction at a time. That is, starting with initial variable values\n\n: <math>\\mathbf{x}^0 = (x^0_1, \\ldots, x^0_n)</math>,\n\nround <math>k+1</math> defines <math>\\mathbf{x}^{k+1}</math> from <math>\\mathbf{x}^k</math> by iteratively solving the single variable optimization problems\n\n:<math>x^{k+1}_i = \\underset{y\\in\\mathbb R}{\\operatorname{arg\\,min}}\\; f(x^{k+1}_1, \\dots, x^{k+1}_{i-1}, y, x^k_{i+1}, \\dots, x^k_n)</math><ref>https://www.cs.cmu.edu/~ggordon/10725-F12/slides/25-coord-desc.pdf</ref>\n\nfor each variable <math>x_i</math> of <math>\\mathbf{x}</math>, for <math>i</math> from 1 to <math>n</math>.\n\nThus, one begins with an initial guess <math>\\mathbf{x}^0</math> for a local minimum of <math>F</math>, and gets a sequence\n<math>\\mathbf{x}^0, \\mathbf{x}^1, \\mathbf{x}^2, \\dots</math> iteratively.\n\nBy doing [[line search]] in each iteration, one automatically has\n\n:<math>F(\\mathbf{x}^0)\\ge F(\\mathbf{x}^1)\\ge F(\\mathbf{x}^2)\\ge \\dots.</math>\n\nIt can be shown that this sequence has similar convergence properties as steepest descent. No improvement after one cycle of [[line search]] along coordinate directions implies a stationary point is reached.\n\nThis process is illustrated below.\n\n[[File:Coordinate descent.svg|center|500px]]\n\n===Differentiable case===\nIn the case of a [[continuously differentiable]] function {{mvar|F}}, a coordinate descent algorithm can be [[pseudocode|sketched]] as:{{r|wright}}\n\n<div style=\"margin-left: 35px; width: 600px\">\n{{framebox|blue}}\n* Choose an initial parameter vector {{math|'''x'''}}.\n* Until convergence is reached, or for some fixed number of iterations:\n** Choose an index {{mvar|i}} from {{math|1}} to {{mvar|n}}.\n** Choose a step size {{mvar|α}}. \n** Update {{mvar|x<sub>i</sub>}} to {{math|''x<sub>i</sub>'' − {{mvar|α}}{{sfrac|∂''F''|∂''x<sub>i</sub>''}}('''x''')}}.\n{{frame-footer}}\n</div>\n\nThe step size can be chosen in various ways, e.g., by solving for the exact minimizer of {{math|''f''(''x<sub>i</sub>'') {{=}} ''F''('''x''')}} (i.e., {{mvar|F}} with all variables but {{mvar|x<sub>i</sub>}} fixed), or by traditional line search criteria.{{r|wright}}\n\n==Limitations==\nCoordinate descent has two problems. One of them is having a non-[[Smoothness|smooth]] multivariable function. The following picture shows that coordinate descent iteration may get stuck at a non-[[stationary point]] if the level curves of a function are not smooth. Suppose that the algorithm is at the point {{math|(-2, -2)}}; then there are two axis-aligned directions it can consider for taking a step, indicated by the red arrows. However, every step along these two directions will increase the objective function's value (assuming a minimization problem), so the algorithm will not take any step, even though both steps together would bring the algorithm closer to the optimum. While this example shows that coordinate descent is not necessarily convergent to the optimum, it is possible to show formal convergence under reasonable conditions.<ref>Spall, J. C. (2012), “Cyclic Seesaw Process for Optimization and Identification,” Journal of Optimization Theory and Applications, vol. 154(1), pp. 187–208. http://dx.doi.org/10.1007/s10957-012-0001-1</ref> \n\n[[File:Nonsmooth coordinate descent.svg|center|500px]]The other problem is difficulty in parallelism. Since the nature of coordinate descent is to cycle through the directions and minimize the objective function with respect to each coordinate direction, coordinate descent is not an obvious candidate for massive parallelism. Recent research works have shown that massive parallelism is applicable to coordinate descent by relaxing the change of the objective function with respect to each coordinate direction.<ref>{{Cite journal|last=Zheng|first=J.|last2=Saquib|first2=S. S.|last3=Sauer|first3=K.|last4=Bouman|first4=C. A.|date=2000-10-01|title=Parallelizable Bayesian tomography algorithms with rapid, guaranteed convergence|journal=IEEE Transactions on Image Processing|volume=9|issue=10|pages=1745–1759|doi=10.1109/83.869186|pmid=18262913|issn=1057-7149|bibcode=2000ITIP....9.1745Z|citeseerx=10.1.1.34.4282}}</ref><ref>{{Cite journal|last=Fessler|first=J. A.|last2=Ficaro|first2=E. P.|last3=Clinthorne|first3=N. H.|last4=Lange|first4=K.|date=1997-04-01|title=Grouped-coordinate ascent algorithms for penalized-likelihood transmission image reconstruction|journal=IEEE Transactions on Medical Imaging|volume=16|issue=2|pages=166–175|doi=10.1109/42.563662|pmid=9101326|issn=0278-0062}}</ref><ref>{{Cite book|last=Wang|first=Xiao|last2=Sabne|first2=Amit|last3=Kisner|first3=Sherman|last4=Raghunathan|first4=Anand|last5=Bouman|first5=Charles|last6=Midkiff|first6=Samuel|date=2016-01-01|title=High Performance Model Based Image Reconstruction|journal=Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming|series=PPoPP '16|location=New York, NY, USA|publisher=ACM|pages=2:1–2:12|doi=10.1145/2851141.2851163|isbn=9781450340922}}</ref>\n\n\n\n==Applications==\nCoordinate descent algorithms are popular with practitioners owing to their simplicity, but the same property has led optimization researchers to largely ignore them in favor of more interesting (complicated) methods.{{r|wright}} An early application of coordinate descent optimization was in the area of computed tomography<ref>{{cite journal|last1=Sauer|first1=Ken|last2=Bouman|first2=Charles|title=A Local Update Strategy for Iterative Reconstruction from Projections|journal=IEEE Trans. On Sig. Proc.|date=February 1993|volume=41|issue=2|pages=534–548|doi=10.1109/78.193196|url=https://engineering.purdue.edu/~bouman/publications/orig-pdf/sp2.pdf|bibcode=1993ITSP...41..534S|citeseerx=10.1.1.135.6045}}</ref> where it has been found to have rapid convergence<ref>{{cite journal|last1=Yu|first1=Zhou|last2=Thibault|first2=Jean-Baptiste|last3=Bouman|first3=Charles|last4=Sauer|first4=Ken|last5=Hsieh|first5=Jiang|title=Fast Model-Based X-ray CT Reconstruction Using Spatially Non-Homogeneous ICD Optimization|journal=IEEE Transactions on Image Processing|date=January 2011|volume=20|issue=1|pages=161–175|doi=10.1109/TIP.2010.2058811|pmid=20643609|url=https://engineering.purdue.edu/~bouman/publications/orig-pdf/tip28.pdf|bibcode=2011ITIP...20..161Y}}</ref> and was subsequently used for clinical multi-slice helical scan CT reconstruction.<ref>{{cite journal|last1=Thibault|first1=Jean-Baptiste|last2=Sauer|first2=Ken|last3=Bouman|first3=Charles|last4=Hsieh|first4=Jiang|title=A Three-Dimensional Statistical Approach to Improved Image Quality for Multi-Slice Helical CT|journal=Medical Physics|date=November 2007|volume=34|issue=11|pages=4526–4544|doi=10.1118/1.2789499|pmid=18072519|url=https://engineering.purdue.edu/~bouman/publications/orig-pdf/medphys1.pdf|bibcode=2007MedPh..34.4526T}}</ref> Moreover, there has been increased interest in the use of coordinate descent with the advent of large-scale problems in [[machine learning]], where coordinate descent has been shown competitive to other methods when applied to such problems as training linear [[support vector machine]]s<ref>{{Cite book | last1 = Hsieh | first1 = C. J. | last2 = Chang | first2 = K. W. | last3 = Lin | first3 = C. J. | last4 = Keerthi | first4 = S. S. | last5 = Sundararajan | first5 = S. | doi = 10.1145/1390156.1390208 | chapter = A dual coordinate descent method for large-scale linear SVM | title = Proceedings of the 25th international conference on Machine learning - ICML '08 | pages = 408 | year = 2008 | isbn = 9781605582054 | pmid =  | pmc = | chapter-url = http://ntu.csie.org/~cjlin/papers/cddual.pdf}}</ref> (see [[LIBLINEAR]]) and [[non-negative matrix factorization]].<ref>{{Cite conference | last1 = Hsieh | first1 = C. J. | last2 = Dhillon | first2 = I. S. | doi = 10.1145/2020408.2020577 | title = Fast coordinate descent methods with variable selection for non-negative matrix factorization | conference = Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '11 | pages = \n1064| year = 2011 | isbn = 9781450308137 | pmid =  | pmc = | url = http://www.cs.utexas.edu/~cjhsieh/nmf_kdd11.pdf}}</ref> They are attractive for problems where computing gradients is infeasible, perhaps because the data required to do so are distributed across computer networks.<ref>{{cite journal |last=Nesterov |first=Yurii |authorlink=Yurii Nesterov |title=Efficiency of coordinate descent methods on huge-scale optimization problems |journal=SIAM J. Optim. |volume=22 |issue=2 |year=2012 |pages=341–362 |url=http://www.ulouvain.be/cps/ucl/doc/core/documents/coredp2010_2web.pdf |doi=10.1137/100802001|citeseerx=10.1.1.332.3336 }}</ref>\n\n==See also==\n* [[Adaptive coordinate descent]]\n* [[Conjugate gradient]]\n* [[Gradient descent]]\n* [[Line search]]\n* [[Mathematical optimization]]\n* [[Newton's method in optimization|Newton's method]]\n* [[Stochastic gradient descent]] – uses one example at a time, rather than one coordinate\n\n==References==\n{{reflist}}\n<!--- After listing your sources please cite them using inline citations and place them after the information they cite. Please see http://en.wikipedia.org/wiki/Wikipedia:REFB for instructions on how to add citations. --->\n\n*{{Citation\n | last=Bezdek\n | first=J. C.\n | last2=Hathaway\n | first2=R. J.\n | last3=Howard\n | first3=R. E.\n | last4=Wilson\n | first4=C. A.\n | last5=Windham\n | first5=M. P.\n | year=1987\n | title=Local convergence analysis of a grouped variable version of coordinate descent\n | periodical=Journal of Optimization Theory and Applications\n | volume=54\n | issue=3\n | pages=471–477\n | doi=10.1007/BF00940196\n | publisher=Kluwer Academic/Plenum Publishers\n}}\n* Bertsekas, Dimitri P. (1999). ''Nonlinear Programming, Second Edition'' Athena Scientific, Belmont, Massachusetts. {{ISBN|1-886529-00-0}}.\n* {{Citation\n | last=Canutescu\n | first=AA\n | last2=Dunbrack\n | first2=RL\n | title=Cyclic coordinate descent: A robotics algorithm for protein loop closure.\n | periodical=Protein Science\n | year=2003\n | volume=12\n | issue=5\n | pages=963–72\n | pmid=12717019\n | doi=10.1110/ps.0242703\n | pmc=2323867\n}}. \n*{{Citation\n | last=Luo\n | first=Zhiquan\n | last2=Tseng\n | first2=P.\n | year=1992\n | title=On the convergence of the coordinate descent method for convex differentiable minimization\n | periodical=Journal of Optimization Theory and Applications\n | volume=72\n | issue=1\n | pages=7–35\n | doi=10.1007/BF00939948\n | publisher=Kluwer Academic/Plenum Publishers\n| hdl=1721.1/3164\n }}.\n*{{Citation\n | last=Wu\n | first=TongTong\n | last2=Lange\n | first2=Kenneth\n | year=2008\n | title=Coordinate descent algorithms for Lasso penalized regression\n | periodical=The Annals of Applied Statistics\n | volume=2\n | issue=1\n | pages=224–244\n | doi=10.1214/07-AOAS147\n | publisher=Institute of Mathematical Statistics\n| arxiv=0803.3876\n }}.\n*{{Citation\n | last=Richtarik\n | first=Peter\n | last2=Takac\n | first2=Martin\n | year=April 2011\n | title=Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function\n | periodical=Mathematical Programming\n | doi=10.1007/s10107-012-0614-z\n | publisher=Springer\n | volume=144\n | issue=1–2\n | pages=1–38\n| arxiv=1107.2848\n }}.\n*{{Citation\n | last=Richtarik\n | first=Peter\n | last2=Takac\n | first2=Martin\n | year=December 2012\n | title=Parallel coordinate descent methods for big data optimization\n | periodical=ArXiv:1212.0873\n}}.\n\n{{Optimization algorithms}}\n\n[[Category:Gradient methods]]"
    },
    {
      "title": "Derivation of the conjugate gradient method",
      "url": "https://en.wikipedia.org/wiki/Derivation_of_the_conjugate_gradient_method",
      "text": "In [[numerical linear algebra]], the [[conjugate gradient method]] is an [[iterative method]] for numerically solving the [[System of linear equations|linear system]]\n\n:<math>\\boldsymbol{Ax}=\\boldsymbol{b}</math>\n\nwhere <math>\\boldsymbol{A}</math> is [[Symmetric matrix|symmetric]] [[Positive-definite matrix|positive-definite]]. The conjugate gradient method can be derived from several different perspectives, including specialization of the [[conjugate direction method]] for [[Optimization (mathematics)|optimization]], and variation of the [[Arnoldi iteration|Arnoldi]]/[[Lanczos iteration|Lanczos]] iteration for [[eigenvalue]] problems.\n\nThe intent of this article is to document the important steps in these derivations.\n\n==Derivation from the conjugate direction method==\n{{Expand section|date=April 2010}}\nThe conjugate gradient method can be seen as a special case of the conjugate direction method applied to minimization of the quadratic function\n\n:<math>f(\\boldsymbol{x})=\\boldsymbol{x}^\\mathrm{T}\\boldsymbol{A}\\boldsymbol{x}-2\\boldsymbol{b}^\\mathrm{T}\\boldsymbol{x}\\text{.}</math>\n\n===The conjugate direction method===\nIn the conjugate direction method for minimizing\n\n:<math>f(\\boldsymbol{x})=\\boldsymbol{x}^\\mathrm{T}\\boldsymbol{A}\\boldsymbol{x}-2\\boldsymbol{b}^\\mathrm{T}\\boldsymbol{x}\\text{.}</math>\n\none starts with an initial guess <math>\\boldsymbol{x}_0</math> and the corresponding residual <math>\\boldsymbol{r}_0=\\boldsymbol{b}-\\boldsymbol{Ax}_0</math>, and computes the iterate and residual by the formulae\n\n:<math>\\begin{align}\n\\alpha_i&=\\frac{\\boldsymbol{p}_i^\\mathrm{T}\\boldsymbol{r}_i}{\\boldsymbol{p}_i^\\mathrm{T}\\boldsymbol{Ap}_i}\\text{,}\\\\\n\\boldsymbol{x}_{i+1}&=\\boldsymbol{x}_i+\\alpha_i\\boldsymbol{p}_i\\text{,}\\\\\n\\boldsymbol{r}_{i+1}&=\\boldsymbol{r}_i-\\alpha_i\\boldsymbol{Ap}_i\n\\end{align}</math>\n\nwhere <math>\\boldsymbol{p}_0,\\boldsymbol{p}_1,\\boldsymbol{p}_2,\\ldots</math> are a series of mutually conjugate directions, i.e.,\n\n:<math>\\boldsymbol{p}_i^\\mathrm{T}\\boldsymbol{Ap}_j=0</math>\n\nfor any <math>i\\neq j</math>.\n\nThe conjugate direction method is imprecise in the sense that no formulae are given for selection of the directions <math>\\boldsymbol{p}_0,\\boldsymbol{p}_1,\\boldsymbol{p}_2,\\ldots</math>. Specific choices lead to various methods including the conjugate gradient method and [[Gaussian elimination]].\n\n==Derivation from the Arnoldi/Lanczos iteration==\n{{see|Arnoldi iteration|Lanczos iteration}}\nThe conjugate gradient method can also be seen as a variant of the Arnoldi/Lanczos iteration applied to solving linear systems.\n\n===The general Arnoldi method===\nIn the Arnoldi iteration, one starts with a vector <math>\\boldsymbol{r}_0</math> and gradually builds an [[orthonormal]] basis <math>\\{\\boldsymbol{v}_1,\\boldsymbol{v}_2,\\boldsymbol{v}_3,\\ldots\\}</math> of the [[Krylov subspace]]\n\n:<math>\\mathcal{K}(\\boldsymbol{A},\\boldsymbol{r}_0)=\\mathrm{span}\\{\\boldsymbol{r}_0,\\boldsymbol{Ar}_0,\\boldsymbol{A}^2\\boldsymbol{r}_0,\\ldots\\}</math>\n\nby defining <math>\\boldsymbol{v}_i=\\boldsymbol{w}_i/\\lVert\\boldsymbol{w}_i\\rVert_2</math> where\n\n:<math>\\boldsymbol{w}_i=\\begin{cases}\n\\boldsymbol{r}_0 & \\text{if }i=1\\text{,}\\\\\n\\boldsymbol{Av}_{i-1}-\\sum_{j=1}^{i-1}(\\boldsymbol{v}_j^\\mathrm{T}\\boldsymbol{Av}_{i-1})\\boldsymbol{v}_j & \\text{if }i>1\\text{.}\n\\end{cases}</math>\n\nIn other words, for <math>i>1</math>, <math>\\boldsymbol{v}_i</math> is found by [[Gram-Schmidt orthogonalization|Gram-Schmidt orthogonalizing]] <math>\\boldsymbol{Av}_{i-1}</math> against <math>\\{\\boldsymbol{v}_1,\\boldsymbol{v}_2,\\ldots,\\boldsymbol{v}_{i-1}\\}</math> followed by normalization.\n\nPut in matrix form, the iteration is captured by the equation\n\n:<math>\\boldsymbol{AV}_i=\\boldsymbol{V}_{i+1}\\boldsymbol{\\tilde{H}}_i</math>\n\nwhere\n\n:<math>\\begin{align}\n\\boldsymbol{V}_i&=\\begin{bmatrix}\n\\boldsymbol{v}_1 & \\boldsymbol{v}_2 & \\cdots & \\boldsymbol{v}_i\n\\end{bmatrix}\\text{,}\\\\\n\\boldsymbol{\\tilde{H}}_i&=\\begin{bmatrix}\nh_{11} & h_{12} & h_{13} & \\cdots & h_{1,i}\\\\\nh_{21} & h_{22} & h_{23} & \\cdots & h_{2,i}\\\\\n& h_{32} & h_{33} & \\cdots & h_{3,i}\\\\\n& & \\ddots & \\ddots & \\vdots\\\\\n& & & h_{i,i-1} & h_{i,i}\\\\\n& & & & h_{i+1,i}\n\\end{bmatrix}=\\begin{bmatrix}\n\\boldsymbol{H}_i\\\\\nh_{i+1,i}\\boldsymbol{e}_i^\\mathrm{T}\n\\end{bmatrix}\n\\end{align}</math>\n\nwith\n\n:<math>h_{ji}=\\begin{cases}\n\\boldsymbol{v}_j^\\mathrm{T}\\boldsymbol{Av}_i & \\text{if }j\\leq i\\text{,}\\\\\n\\lVert\\boldsymbol{w}_{i+1}\\rVert_2 & \\text{if }j=i+1\\text{,}\\\\\n0 & \\text{if }j>i+1\\text{.}\n\\end{cases}</math>\n\nWhen applying the Arnoldi iteration to solving linear systems, one starts with <math>\\boldsymbol{r}_0=\\boldsymbol{b}-\\boldsymbol{Ax}_0</math>, the residual corresponding to an initial guess <math>\\boldsymbol{x}_0</math>. After each step of iteration, one computes <math>\\boldsymbol{y}_i=\\boldsymbol{H}_i^{-1}(\\lVert\\boldsymbol{r}_0\\rVert_2\\boldsymbol{e}_1)</math> and the new iterate <math>\\boldsymbol{x}_i=\\boldsymbol{x}_0+\\boldsymbol{V}_i\\boldsymbol{y}_i</math>.\n\n===The direct Lanczos method===\nFor the rest of discussion, we assume that <math>\\boldsymbol{A}</math> is symmetric positive-definite. With symmetry of <math>\\boldsymbol{A}</math>, the [[upper Hessenberg matrix]] <math>\\boldsymbol{H}_i=\\boldsymbol{V}_i^\\mathrm{T}\\boldsymbol{AV}_i</math> becomes symmetric and thus tridiagonal. It then can be more clearly denoted by\n\n:<math>\\boldsymbol{H}_i=\\begin{bmatrix}\na_1 & b_2\\\\\nb_2 & a_2 & b_3\\\\\n& \\ddots & \\ddots & \\ddots\\\\\n& & b_{i-1} & a_{i-1} & b_i\\\\\n& & & b_i & a_i\n\\end{bmatrix}\\text{.}</math>\n\nThis enables a short three-term recurrence for <math>\\boldsymbol{v}_i</math> in the iteration, and the Arnoldi iteration is reduced to the Lanczos iteration.\n\nSince <math>\\boldsymbol{A}</math> is symmetric positive-definite, so is <math>\\boldsymbol{H}_i</math>. Hence, <math>\\boldsymbol{H}_i</math> can be [[LU factorization|LU factorized]] without [[partial pivoting]] into\n\n:<math>\\boldsymbol{H}_i=\\boldsymbol{L}_i\\boldsymbol{U}_i=\\begin{bmatrix}\n1\\\\\nc_2 & 1\\\\\n& \\ddots & \\ddots\\\\\n& & c_{i-1} & 1\\\\\n& & & c_i & 1\n\\end{bmatrix}\\begin{bmatrix}\nd_1 & b_2\\\\\n& d_2 & b_3\\\\\n& & \\ddots & \\ddots\\\\\n& & & d_{i-1} & b_i\\\\\n& & & & d_i\n\\end{bmatrix}</math>\n\nwith convenient recurrences for <math>c_i</math> and <math>d_i</math>:\n\n:<math>\\begin{align}\nc_i&=b_i/d_{i-1}\\text{,}\\\\\nd_i&=\\begin{cases}\na_1 & \\text{if }i=1\\text{,}\\\\\na_i-c_ib_i & \\text{if }i>1\\text{.}\n\\end{cases}\n\\end{align}</math>\n\nRewrite <math>\\boldsymbol{x}_i=\\boldsymbol{x}_0+\\boldsymbol{V}_i\\boldsymbol{y}_i</math> as\n\n:<math>\\begin{align}\n\\boldsymbol{x}_i&=\\boldsymbol{x}_0+\\boldsymbol{V}_i\\boldsymbol{H}_i^{-1}(\\lVert\\boldsymbol{r}_0\\rVert_2\\boldsymbol{e}_1)\\\\\n&=\\boldsymbol{x}_0+\\boldsymbol{V}_i\\boldsymbol{U}_i^{-1}\\boldsymbol{L}_i^{-1}(\\lVert\\boldsymbol{r}_0\\rVert_2\\boldsymbol{e}_1)\\\\\n&=\\boldsymbol{x}_0+\\boldsymbol{P}_i\\boldsymbol{z}_i\n\\end{align}</math>\n\nwith\n\n:<math>\\begin{align}\n\\boldsymbol{P}_i&=\\boldsymbol{V}_{i}\\boldsymbol{U}_i^{-1}\\text{,}\\\\\n\\boldsymbol{z}_i&=\\boldsymbol{L}_i^{-1}(\\lVert\\boldsymbol{r}_0\\rVert_2\\boldsymbol{e}_1)\\text{.}\n\\end{align}</math>\n\nIt is now important to observe that\n\n:<math>\\begin{align}\n\\boldsymbol{P}_i&=\\begin{bmatrix}\n\\boldsymbol{P}_{i-1} & \\boldsymbol{p}_i\n\\end{bmatrix}\\text{,}\\\\\n\\boldsymbol{z}_i&=\\begin{bmatrix}\n\\boldsymbol{z}_{i-1}\\\\\n\\zeta_i\n\\end{bmatrix}\\text{.}\n\\end{align}</math>\n\nIn fact, there are short recurrences for <math>\\boldsymbol{p}_i</math> and <math>\\zeta_i</math> as well:\n\n:<math>\\begin{align}\n\\boldsymbol{p}_i&=\\frac{1}{d_i}(\\boldsymbol{v}_i-b_i\\boldsymbol{p}_{i-1})\\text{,}\\\\\n\\zeta_i&=-c_i\\zeta_{i-1}\\text{.}\n\\end{align}</math>\n\nWith this formulation, we arrive at a simple recurrence for <math>\\boldsymbol{x}_i</math>:\n\n:<math>\\begin{align}\n\\boldsymbol{x}_i&=\\boldsymbol{x}_0+\\boldsymbol{P}_i\\boldsymbol{z}_i\\\\\n&=\\boldsymbol{x}_0+\\boldsymbol{P}_{i-1}\\boldsymbol{z}_{i-1}+\\zeta_i\\boldsymbol{p}_i\\\\\n&=\\boldsymbol{x}_{i-1}+\\zeta_i\\boldsymbol{p}_i\\text{.}\n\\end{align}</math>\n\nThe relations above straightforwardly lead to the direct Lanczos method, which turns out to be slightly more complex.\n\n===The conjugate gradient method from imposing orthogonality and conjugacy===\nIf we allow <math>\\boldsymbol{p}_i</math> to scale and compensate for the scaling in the constant factor, we potentially can have simpler recurrences of the form:\n\n:<math>\\begin{align}\n\\boldsymbol{x}_i&=\\boldsymbol{x}_{i-1}+\\alpha_{i-1}\\boldsymbol{p}_{i-1}\\text{,}\\\\\n\\boldsymbol{r}_i&=\\boldsymbol{r}_{i-1}-\\alpha_{i-1}\\boldsymbol{Ap}_{i-1}\\text{,}\\\\\n\\boldsymbol{p}_i&=\\boldsymbol{r}_i+\\beta_{i-1}\\boldsymbol{p}_{i-1}\\text{.}\n\\end{align}</math>\n\nAs premises for the simplification, we now derive the orthogonality of <math>\\boldsymbol{r}_i</math> and conjugacy of <math>\\boldsymbol{p}_i</math>, i.e., for <math>i\\neq j</math>,\n\n:<math>\\begin{align}\n\\boldsymbol{r}_i^\\mathrm{T}\\boldsymbol{r}_j&=0\\text{,}\\\\\n\\boldsymbol{p}_i^\\mathrm{T}\\boldsymbol{Ap}_j&=0\\text{.}\n\\end{align}</math>\n\nThe residuals are mutually orthogonal because <math>\\boldsymbol{r}_i</math> is essentially a multiple of <math>\\boldsymbol{v}_{i+1}</math> since for <math>i=0</math>, <math>\\boldsymbol{r}_0=\\lVert\\boldsymbol{r}_0\\rVert_2\\boldsymbol{v}_1</math>, for <math>i>0</math>,\n\n:<math>\\begin{align}\n\\boldsymbol{r}_i&=\\boldsymbol{b}-\\boldsymbol{Ax}_i\\\\\n&=\\boldsymbol{b}-\\boldsymbol{A}(\\boldsymbol{x}_0+\\boldsymbol{V}_i\\boldsymbol{y}_i)\\\\\n&=\\boldsymbol{r}_0-\\boldsymbol{AV}_i\\boldsymbol{y}_i\\\\\n&=\\boldsymbol{r}_0-\\boldsymbol{V}_{i+1}\\boldsymbol{\\tilde{H}}_i\\boldsymbol{y}_i\\\\\n&=\\boldsymbol{r}_0-\\boldsymbol{V}_i\\boldsymbol{H}_i\\boldsymbol{y}_i-h_{i+1,i}(\\boldsymbol{e}_i^\\mathrm{T}\\boldsymbol{y}_i)\\boldsymbol{v}_{i+1}\\\\\n&=\\lVert\\boldsymbol{r}_0\\rVert_2\\boldsymbol{v}_1-\\boldsymbol{V}_i(\\lVert\\boldsymbol{r}_0\\rVert_2\\boldsymbol{e}_1)-h_{i+1,i}(\\boldsymbol{e}_i^\\mathrm{T}\\boldsymbol{y}_i)\\boldsymbol{v}_{i+1}\\\\\n&=-h_{i+1,i}(\\boldsymbol{e}_i^\\mathrm{T}\\boldsymbol{y}_i)\\boldsymbol{v}_{i+1}\\text{.}\\end{align}</math>\n\nTo see the conjugacy of <math>\\boldsymbol{p}_i</math>, it suffices to show that <math>\\boldsymbol{P}_i^\\mathrm{T}\\boldsymbol{AP}_i</math> is diagonal:\n\n:<math>\\begin{align}\n\\boldsymbol{P}_i^\\mathrm{T}\\boldsymbol{AP}_i&=\\boldsymbol{U}_i^{-\\mathrm{T}}\\boldsymbol{V}_i^\\mathrm{T}\\boldsymbol{AV}_i\\boldsymbol{U}_i^{-1}\\\\\n&=\\boldsymbol{U}_i^{-\\mathrm{T}}\\boldsymbol{H}_i\\boldsymbol{U}_i^{-1}\\\\\n&=\\boldsymbol{U}_i^{-\\mathrm{T}}\\boldsymbol{L}_i\\boldsymbol{U}_i\\boldsymbol{U}_i^{-1}\\\\\n&=\\boldsymbol{U}_i^{-\\mathrm{T}}\\boldsymbol{L}_i\n\\end{align}</math>\n\nis symmetric and lower triangular simultaneously and thus must be diagonal.\n\nNow we can derive the constant factors <math>\\alpha_i</math> and <math>\\beta_i</math> with respect to the scaled <math>\\boldsymbol{p}_i</math> by solely imposing the orthogonality of <math>\\boldsymbol{r}_i</math> and conjugacy of <math>\\boldsymbol{p}_i</math>.\n\nDue to the orthogonality of <math>\\boldsymbol{r}_i</math>, it is necessary that <math>\\boldsymbol{r}_{i+1}^\\mathrm{T}\\boldsymbol{r}_i=(\\boldsymbol{r}_i-\\alpha_i\\boldsymbol{Ap}_i)^\\mathrm{T}\\boldsymbol{r}_i=0</math>. As a result,\n\n:<math>\\begin{align}\n\\alpha_i&=\\frac{\\boldsymbol{r}_i^\\mathrm{T}\\boldsymbol{r}_i}{\\boldsymbol{r}_i^\\mathrm{T}\\boldsymbol{Ap}_i}\\\\\n&=\\frac{\\boldsymbol{r}_i^\\mathrm{T}\\boldsymbol{r}_i}{(\\boldsymbol{p}_i-\\beta_{i-1}\\boldsymbol{p}_{i-1})^\\mathrm{T}\\boldsymbol{Ap}_i}\\\\\n&=\\frac{\\boldsymbol{r}_i^\\mathrm{T}\\boldsymbol{r}_i}{\\boldsymbol{p}_i^\\mathrm{T}\\boldsymbol{Ap}_i}\\text{.}\n\\end{align}</math>\n\nSimilarly, due to the conjugacy of <math>\\boldsymbol{p}_i</math>, it is necessary that <math>\\boldsymbol{p}_{i+1}^\\mathrm{T}\\boldsymbol{Ap}_i=(\\boldsymbol{r}_{i+1}+\\beta_i\\boldsymbol{p}_i)^\\mathrm{T}\\boldsymbol{Ap}_i=0</math>. As a result,\n\n:<math>\\begin{align}\n\\beta_i&=-\\frac{\\boldsymbol{r}_{i+1}^\\mathrm{T}\\boldsymbol{Ap}_i}{\\boldsymbol{p}_i^\\mathrm{T}\\boldsymbol{Ap}_i}\\\\\n&=-\\frac{\\boldsymbol{r}_{i+1}^\\mathrm{T}(\\boldsymbol{r}_i-\\boldsymbol{r}_{i+1})}{\\alpha_i\\boldsymbol{p}_i^\\mathrm{T}\\boldsymbol{Ap}_i}\\\\\n&=\\frac{\\boldsymbol{r}_{i+1}^\\mathrm{T}\\boldsymbol{r}_{i+1}}{\\boldsymbol{r}_i^\\mathrm{T}\\boldsymbol{r}_i}\\text{.}\n\\end{align}</math>\n\nThis completes the derivation.\n\n==References==\n#{{cite journal|last1 = Hestenes|first1 = M. R.|authorlink1 = David Hestenes|last2 = Stiefel|first2 = E.|authorlink2 = Eduard Stiefel|title = Methods of conjugate gradients for solving linear systems|journal = Journal of Research of the National Bureau of Standards|volume = 49|issue = 6|date=December 1952|url = http://nvlpubs.nist.gov/nistpubs/jres/049/6/V49.N06.A08.pdf|format=PDF}}\n#{{cite book|last = Saad|first = Y.|title = Iterative methods for sparse linear systems|edition = 2nd|chapter = Chapter 6: Krylov Subspace Methods, Part I|publisher = SIAM|year = 2003|isbn = 978-0-89871-534-7}}\n\n{{Numerical linear algebra}}\n\n[[Category:Numerical linear algebra]]\n[[Category:Optimization algorithms and methods]]\n[[Category:Gradient methods]]\n[[Category:Articles containing proofs]]"
    },
    {
      "title": "Landweber iteration",
      "url": "https://en.wikipedia.org/wiki/Landweber_iteration",
      "text": "The '''Landweber iteration''' or '''Landweber algorithm''' is an algorithm to solve [[ill-posed]] linear [[inverse problems]], and it has been extended to solve non-linear problems that involve constraints. The method was first proposed in the 1950s by [[Louis Landweber]],<ref name=\"Landweber\"/> and it can be now viewed as a special case of many other more general methods.<ref name=\"Combettes\"/>\n\n== Basic algorithm ==\nThe original Landweber algorithm <ref name=\"Landweber\"/> attempts to recover a signal ''x'' from (noisy) measurements ''y''. The linear version assumes that <math>y = Ax </math> for a [[linear operator]] ''A''. When the problem is in finite [[dimensions]], ''A'' is just a matrix.\n\nWhen ''A'' is [[nonsingular]], then an explicit solution is <math> x = A^{-1} y</math>. However, if ''A'' is [[ill-conditioned]], the explicit solution is a poor choice since it is sensitive to any noise in the data ''y''. If ''A'' is [[Mathematical singularity|singular]], this explicit solution doesn't even exist. The Landweber algorithm is an attempt to [[Regularization (mathematics)|regularize]] the problem, and is one of the alternatives to [[Tikhonov regularization]].  We may view the Landweber algorithm as solving:\n\n: <math> \\min_x  \\|Ax-y\\|_2^2/2 </math>\n\nusing an iterative method. The algorithm is given by the update\n\n: <math> x_{k+1} = x_{k} - \\omega A^*(Ax_k - y). </math>\n\nwhere the relaxation factor <math>\\omega</math> satisfies <math> 0 < \\omega < 2/\\sigma_1^2</math>. Here <math>\\sigma_1</math> is the largest [[singular value decomposition|singular value]] of <math>A</math>. If we write <math> f(x) = \\|Ax-y\\|_2^2 /2</math>, then the update can be written in terms of the [[gradient]]\n\n: <math> x_{k+1} = x_k - \\omega \\nabla f(x_k) </math>\n\nand hence the algorithm is a special case of [[gradient descent]].\n\nFor [[ill-posed]] problems, the iterative method needs to be stopped at a suitable iteration index, because it semi-converges. This means that the iterates approach a regularized solution during the first iterations, but become unstable in further iterations. The reciprocal of the iteration index <math> 1/k </math> acts as a regularization parameter. A suitable parameter is found, when the mismatch <math> \\|Ax_k-y\\|_2^2 </math> approaches the noise level.\n\nUsing the Landweber iteration as a [[regularization (mathematics)|regularization]] algorithm has been discussed in the literature.<ref>Louis, A.K. (1989): Inverse und schlecht gestellte Probleme. Stuttgart, Teubner</ref><ref>Vainikko, G.M., Veretennikov, A.Y. (1986): Iteration Procedures in Ill-Posed Problems. Moscow, Nauka (in Russian)</ref>\n\n== Nonlinear extension ==\nIn general, the updates generated by \n<math> x_{k+1} = x_{k} - \\tau \\nabla f(x_k) </math>\nwill generate a sequence <math>f(x_k)</math> that [[convergence (mathematics)|converges]] to a minimizer of ''f'' whenever ''f'' is [[convex function|convex]]\nand the stepsize <math>\\tau</math> is chosen such that <math> 0 < \\tau < 2/( \\|\\nabla f\\|^2 ) </math> where <math> \\|\\cdot \\| </math> is the [[spectral norm]].\n\nSince this is special type of gradient descent, there currently is not much benefit to analyzing it on its own as the nonlinear Landweber, but such analysis was performed historically by many communities not aware of unifying frameworks.\n\nThe nonlinear Landweber problem has been studied in many papers in many communities; see, for example,.<ref>A convergence analysis of the Landweber iteration for nonlinear ill-posed problems\nMartin Hanke, Andreas Neubauer and Otmar Scherzer. NUMERISCHE MATHEMATIK\nVolume 72, Number 1 (1995), 21-37, DOI: 10.1007/s002110050158</ref>\n\n== Extension to constrained problems ==\nIf ''f'' is a [[convex function]] and ''C'' is a [[convex set]], then the problem\n\n: <math> \\min_{x \\in C} f(x) </math>\n\ncan be solved by the constrained, nonlinear Landweber iteration, given by:\n\n: <math> x_{k+1} = \\mathcal{P}_C( x_{k} - \\tau \\nabla f(x_k) )</math>\n\nwhere <math>\\mathcal{P}</math> is the [[projection (mathematics)|projection]] onto the set ''C''. Convergence is guaranteed when <math> 0 < \\tau < 2/( \\|A\\|^2 ) </math>.<ref>Eicke, B.: Iteration methods for convexly constrained ill-posed problems in Hilbert space. Numer. Funct. Anal. Optim. 13, 413–429 (1992)</ref> This is again a special case of [[projected gradient descent]] (which is a special case of the [[Forward–backward algorithm (operator splitting)|forward–backward algorithm]]) as discussed in.<ref name=\"Combettes\"/>\n\n== Applications ==\nSince the method has been around since the 1950s, it has been adopted and rediscovered by many scientific communities, especially those studying ill-posed problems. In [[X-ray computed tomography]] it is called SIRT - simultaneous iterative reconstruction technique. It has also been used in the [[computer vision]] community<ref>\nJohansson, B., Elfving, T., Kozlovc, V., Censor, Y., Forssen, P.E., Granlund, G.; \"The application of an oblique-projected Landweber method to a model of supervised learning\", Math. Comput. Modelling, vol 43, pp 892–909 (2006)</ref> and the signal restoration community.<ref>Trussell, H.J., Civanlar, M.R.: The Landweber iteration and projection onto convex sets. IEEE Trans. Acoust., Speech, Signal Process. 33, 1632–1634 (1985)</ref> It is also used in [[image processing]], since many image problems, such as [[deconvolution]], are ill-posed. Variants of this method have been used also in sparse approximation problems and compressed sensing settings.<ref>{{cite web\n|url         = http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6136024&tag=1\n|title       = Recipes for hard thresholding methods\n|author      = Anastasios Kyrillidis\n|author2      = Volkan Cevher\n|last-author-amp      = yes\n}}</ref>\n\n== References ==\n<references>\n<ref name=\"Landweber\">Landweber, L. (1951): An iteration formula for Fredholm integral equations of the first kind.\nAmer. J. Math. 73, 615–624</ref>\n<ref name=\"Combettes\">P. L. Combettes and J.-C. Pesquet, \"Proximal splitting methods in signal processing,\" in: Fixed-Point Algorithms for Inverse Problems in Science and Engineering, (H. H. Bauschke, [[Regina S. Burachik|R. S. Burachik]], P. L. Combettes, V. Elser, D. R. Luke, and H. Wolkowicz, Editors), pp. 185–212. Springer, New York, 2011. [http://www4.ncsu.edu/~pcombet/prox.pdf PDF]</ref>\n</references>\n\n[[Category:Image processing]]\n[[Category:Inverse problems]]\n[[Category:Gradient methods]]"
    },
    {
      "title": "Nonlinear conjugate gradient method",
      "url": "https://en.wikipedia.org/wiki/Nonlinear_conjugate_gradient_method",
      "text": "In [[numerical optimization]], the '''nonlinear conjugate gradient method''' generalizes the [[conjugate gradient method]] to [[nonlinear optimization]]. For a quadratic function <math>\\displaystyle f(x)</math>\n:: <math>\\displaystyle f(x)=\\|Ax-b\\|^2,</math>\nthe minimum of <math>f</math> is obtained when the [[gradient]] is 0:\n:: <math>\\nabla_x f=2 A^T(Ax-b)=0</math>.\n \nWhereas linear conjugate gradient seeks a solution to the linear equation \n<math>\\displaystyle A^T Ax=A^T b</math>, the nonlinear conjugate gradient method is generally \nused to find the [[maxima and minima|local minimum]] of a nonlinear function \nusing its [[gradient]] <math>\\nabla_x f</math> alone. It works when the function is approximately quadratic near the minimum, which is the case when the function is twice differentiable at the minimum and the second derivative is non-singular there.\n\nGiven a function <math>\\displaystyle f(x)</math> of <math>N</math> variables to minimize, its gradient <math>\\nabla_x f</math> indicates the direction of maximum increase.\nOne simply starts in the opposite ([[steepest descent]]) direction:\n:: <math>\\Delta x_0=-\\nabla_x f (x_0) </math>\n\nwith an adjustable step length <math>\\displaystyle \\alpha</math> and performs a [[line search]] in this direction until it reaches the minimum of <math>\\displaystyle f</math>:\n:: <math>\\displaystyle \\alpha_0:= \\arg \\min_\\alpha f(x_0+\\alpha \\Delta x_0)</math>,\n:: <math>\\displaystyle x_1=x_0+\\alpha_0 \\Delta x_0</math>\n\nAfter this first iteration in the steepest direction <math>\\displaystyle \\Delta x_0</math>, the following steps constitute one iteration of moving along a subsequent conjugate direction <math>\\displaystyle s_n</math>, where <math>\\displaystyle s_0=\\Delta x_0</math>:\n# Calculate the steepest direction: <math>\\Delta x_n=-\\nabla_x f (x_n) </math>,\n# Compute <math>\\displaystyle \\beta_n</math> according to one of the formulas below,\n# Update the conjugate direction: <math>\\displaystyle s_n=\\Delta x_n+\\beta_n s_{n-1}</math>\n# Perform a line search: optimize <math>\\displaystyle \\alpha_n=\\arg \\min_{\\alpha} f(x_n+\\alpha s_n)</math>, \n# Update the position: <math>\\displaystyle x_{n+1}=x_{n}+\\alpha_{n} s_{n}</math>,\n \nWith a pure quadratic function the minimum is reached within ''N'' iterations (excepting roundoff error), but a non-quadratic function will make slower progress.  Subsequent search directions lose conjugacy requiring the search direction to be reset to the steepest descent direction at least every ''N'' iterations, or sooner if progress stops. However, resetting every iteration turns the method into [[steepest descent]].  The algorithm stops when it finds the minimum, determined when no progress is made after a direction reset (i.e. in the steepest descent direction), or when some tolerance criterion is reached.\n\nWithin a linear approximation, the parameters <math>\\displaystyle \\alpha</math> and <math>\\displaystyle \\beta</math> are the same as in the\nlinear conjugate gradient method but have been obtained with line searches.\nThe conjugate gradient method can follow narrow ([[ill-conditioned]]) valleys, where the [[steepest descent]] method slows down and follows a criss-cross pattern.\n\nFour of the best known formulas for <math>\\displaystyle \\beta_n</math> are named after their developers:\n* Fletcher–Reeves:<ref>R. Fletcher and C. M. Reeves, \"Function minimization by conjugate gradients\", Comput. J. 7\n(1964), 149–154.</ref>\n:: <math>\\beta_{n}^{FR} = \\frac{\\Delta x_n^T \\Delta x_n}\n{\\Delta x_{n-1}^T \\Delta x_{n-1}}.\n</math>\n* Polak–Ribière:<ref>E. Polak and G. Ribière, \"Note sur la convergence de directions conjugu´ee\", Rev. Francaise\nInformat Recherche Operationelle, 3e Ann´ee 16 (1969), 35–43.</ref>\n:: <math>\\beta_{n}^{PR} = \\frac{\\Delta x_n^T (\\Delta x_n-\\Delta x_{n-1})}\n{\\Delta x_{n-1}^T \\Delta x_{n-1}}.\n</math>\n* Hestenes-Stiefel:<ref>M. R. Hestenes and E. Stiefel, \"Methods of conjugate gradients for solving linear systems\", J.\nResearch Nat. Bur. Standards 49 (1952), 409–436 (1953).</ref>\n:: <math>\\beta_n^{HS} = -\\frac{\\Delta x_n^T (\\Delta x_n-\\Delta x_{n-1})}\n{s_{n-1}^T (\\Delta x_n-\\Delta x_{n-1})}.\n</math>\n* Dai–Yuan:<ref>Y.-H. Dai and Y. Yuan, \"A nonlinear conjugate gradient method with a strong global convergence\nproperty\", SIAM J. Optim. 10 (1999), no. 1, 177–182.</ref>\n:: <math>\\beta_{n}^{DY} = -\\frac{\\Delta x_n^T \\Delta x_n}\n{s_{n-1}^T (\\Delta x_n-\\Delta x_{n-1})}.\n</math>.\n\nThese formulas are equivalent for a quadratic function, but for nonlinear optimization the preferred formula is a matter of heuristics or taste. A popular choice is <math>\\displaystyle \\beta=\\max\\{0, \\beta^{PR}\\}</math>, which provides a direction reset automatically.<ref>J. R. Shewchuk, \"[https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf An Introduction to the Conjugate Gradient Method Without the Agonizing Pain]\", August 1994.</ref>\n\nAlgorithms based on [[Newton's method]] potentially converge much faster. There, both step direction and length are computed from the gradient as the solution of a linear system of equations, with the coefficient matrix being the exact [[Hessian matrix]] (for Newton's method proper) or an estimate thereof (in the [[quasi-Newton method]]s, where the observed change in the gradient during the iterations is used to update the Hessian estimate). For high-dimensional problems, the exact computation of the Hessian is usually prohibitively expensive, and even its storage can be problematic, requiring <math>O(N^2)</math> memory (but see the limited-memory [[L-BFGS]] quasi-Newton method).\n\n==See also==\n* [[Broyden–Fletcher–Goldfarb–Shanno algorithm]]\n* [[Conjugate gradient method]]\n* [[L-BFGS]] (limited memory BFGS)\n* [[Nelder–Mead method]]\n* [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.700.5917&rep=rep1&type=pdf MODIFIED FLETCHER-REEVES-TYPE ]\n\n==References==\n{{reflist}}\n\n==External links==\n* [https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf An Introduction to the Conjugate Gradient Method Without the Agonizing Pain] by Jonathan Richard Shewchuk.\n* [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.46.3325&rep=rep1&type=pdf A NONLINEAR CONJUGATE GRADIENT METHOD WITH A STRONG GLOBAL CONVERGENCE PROPERTY] by Y. H. DAI and Y. YUAN.\n\n{{Optimization algorithms}}\n\n[[Category:Optimization algorithms and methods]]\n[[Category:Gradient methods]]"
    },
    {
      "title": "Proximal gradient method",
      "url": "https://en.wikipedia.org/wiki/Proximal_gradient_method",
      "text": "{{more footnotes|date=November 2013}}\n\n'''Proximal gradient methods''' are a generalized form of projection used to solve non-differentiable [[convex optimization]] problems. \n\nMany interesting problems can be formulated as convex optimization problems of form:\n\n<math>\n\\operatorname{min}\\limits_{x \\in \\mathbb{R}^N} \\sum_{i=1}^n f_i(x) \n</math>\n\nwhere <math>f_i,\\ i = 1, \\dots, n</math> are [[convex functions]] defined from <math>f: \\mathbb{R}^N  \\rightarrow \\mathbb{R} </math>\nwhere some of the functions are non-differentiable, this rules out our conventional smooth optimization techniques like\n[[Gradient descent|Steepest descent method]], [[conjugate gradient method]] etc. Proximal gradient methods can be used instead. These methods proceed by splitting, in that the functions <math>f_1, . . . , f_n</math> are used individually so as to yield an easily [[wikt:implementable|implementable]] algorithm.\nThey are called [[proximal]] because each non [[smooth function]] among <math>f_1, . . . , f_n</math> is involved via its proximity\noperator. Iterative Shrinkage thresholding algorithm,<ref>\n{{cite news | last1=\"Daubechies | first1=I | last2=Defrise | first2 = M | last3 = De Mol| first3 = C| title=An iterative thresholding algorithm for linear inverse problems with a sparsity constraint\n |journal=A Journal Issued by the Courant Institute of Mathematical Sciences|volume=57 |year=2004|pages=1413–1457}}</ref> [[Landweber iteration|projected Landweber]], projected\ngradient, [[alternating projection]]s, [[Alternating direction method of multipliers#Alternating direction method of multipliers|alternating-direction method of multipliers]], alternating\nsplit [[Bregman method|Bregman]] are special instances of proximal algorithms. Details of proximal methods are discussed in Combettes and Pesquet.<ref>\n{{cite arXiv |last1=Combettes |first1=Patrick L. |last2= Pesquet |first2=Jean-Christophe |title=Proximal Splitting Methods in Signal Processing|page=|year=2009 |arxiv=0912.3522}}</ref> For the theory of proximal gradient methods from the perspective of and with applications to [[statistical learning theory]], see [[proximal gradient methods for learning]].\n\n== Notations and terminology ==\nLet <math>\\mathbb{R}^N</math>, the <math>N</math>-dimensional [[euclidean space]], be the domain of the function\n<math> f: \\mathbb{R}^N  \\rightarrow (-\\infty,+\\infty]</math>. Suppose <math>C</math> is a non-empty\nconvex subset of <math>\\mathbb{R}^N</math>. Then, the indicator function of <math>C</math> is defined as\n\n: <math> \\iota_C : x \\mapsto\n\\begin{cases}\n0        &  \\text{if } x \\in C \\\\\n+ \\infty &  \\text{if } x \\notin C \n\\end{cases}\n</math>\n\n: <math>p</math>-norm is defined as ( <math>\\| \\cdot \\|_p</math> )\n\n: <math>\n\\|x\\|_p = ( |x_1|^p + |x_2|^p + \\cdots + |x_N|^p )^{1/p}\n</math>\n\nThe distance from <math>x \\in \\mathbb{R}^N</math> to <math>C</math> is defined as\n\n: <math>\nD_C(x) =  \\min_{y \\in C} \\|x - y\\|_2\n</math>\n\nIf <math>C</math> is closed and convex, the projection of <math>x \\in \\mathbb{R}^N</math> onto <math>C</math> is the unique point\n<math>P_Cx \\in C</math> such that <math>D_C(x) = \\| x - P_Cx \\|_2 </math>.\n\nThe [[subdifferential]] of <math>f</math> is given by\n\n: <math>\n \\partial f = \\{ u \\in \\mathbb{R}^N \\mid \\forall y \\in \\mathbb{R}^N, (y-x)^\\mathrm{T}u+f(x) \\leq f(y).\\}\n</math>\n\n== Projection onto convex sets (POCS) ==\n\nOne of the widely used convex optimization algorithms is POCS ([[Projections onto convex sets|Projection Onto Convex Sets]]).\nThis algorithm is employed to recover/synthesize a signal satisfying simultaneously several convex constraints.\nLet <math>f_i</math> be the indicator function of non-empty closed convex set <math>C_i</math> modeling a constraint.\nThis reduces to convex feasibility problem, which require us to find a solution such that it lies in the intersection\nof all convex sets <math>C_i</math>. In POCS method each set <math>C_i</math> is incorporated by its [[projection operator]]\n<math>P_{C_i}</math>. So in each [[iteration]] <math>x</math> is updated as\n\n: <math>\nx_{k+1} = P_{C_1}  P_{C_2} \\cdots P_{C_n} x_k\n</math>\n\nHowever beyond such problems [[projection operator]]s are not appropriate and more general operators\nare required to tackle them. Among the various generalizations of the notion of a convex projection\noperator that exist, proximity operators are best suited for other purposes.\n\n== Definition ==\nThe [[proximal operator|proximity operator]] of a convex function <math>f</math> at <math>x</math> is defined as the unique solution to\n:<math>\n\\operatorname{argmin}\\limits_y \\bigg( f(y) + \\frac{1}{2} \\left\\| x-y \\right\\|_2^2 \\bigg)\n</math>\nand is denoted <math>\\operatorname{prox}_f(x)</math>.\n\n: <math>\n \\operatorname{prox}_f(x) :\\mathbb{R}^N \\rightarrow \\mathbb{R}^N\n</math>\n\nNote that in the specific case where <math>f</math> is the indicator function <math>\\iota_C</math> of some convex set <math>C</math>\n\n: <math> \n\\begin{align}\n\\operatorname{prox}_{\\iota_C}(x)\n&= \\operatorname{argmin}\\limits_y\n\\begin{cases}\n\\frac{1}{2} \\left\\| x-y \\right\\|_2^2 &  \\text{if } y \\in C \\\\\n+ \\infty                             &  \\text{if } y \\notin C \n\\end{cases} \\\\\n&=\\operatorname{argmin}\\limits_{y \\in C} \\frac{1}{2} \\left\\| x-y \\right\\|_2^2 \\\\\n&= P_C(x)\n\\end{align}\n</math>\n\nshowing that the proximity operator is indeed a generalisation of the projection operator.\n\nThe proximity operator of <math>f</math> is characterized by inclusion\n\n: <math>\np=\\operatorname{prox}_f(x) \\Leftrightarrow x-p \\in \\partial f(p) \\qquad (\\forall(x,p) \\in \\mathbb{R}^N \\times \\mathbb{R}^N)\n</math>\n\nIf <math>f</math> is differentiable then above equation reduces to\n\n: <math>\np=\\operatorname{prox}_f(x) \\Leftrightarrow x-p = \\nabla f(p) \\quad (\\forall(x,p) \\in \\mathbb{R}^N \\times \\mathbb{R}^N)\n</math>\n\n== Examples ==\nSpecial instances of Proximal Gradient Methods are\n*[[Landweber iteration|Projected Landweber]]\n*[[Alternating projection]]\n*[[Alternating direction method of multipliers#Alternating direction method of multipliers|Alternating-direction method of multipliers]]\n*Fast Iterative Shrinkage Thresholding Algorithm (FISTA)<ref>\n{{cite news | last1=\"Beck | first1=A | last2=Teboulle | first2 = M | title=A fast iterative shrinkage-thresholding algorithm for linear inverse problems\n |journal=SIAM Journal on Imaging Sciences|volume=2 |year=2009|pages=183–202}}</ref>\n\n== See also ==\n* [[Alternating projection]]\n* [[Convex optimization]]\n* [[Frank–Wolfe algorithm]]\n* [[Proximal operator]]\n* [[Proximal gradient methods for learning]]\n\n== Notes ==\n<references/>\n\n== References ==\n* {{cite book\n  | last = Rockafellar\n  | first = R. T.\n  | authorlink = R. Tyrrell Rockafellar\n  | title = Convex analysis\n  | publisher = Princeton University Press\n  | year = 1970\n  | location = Princeton\n}}\n*{{ cite book\n | last1 = Combettes\n | first1 = Patrick L.\n | last2 = Pesquet\n | first2 = Jean-Christophe\n | title = Springer's Fixed-Point Algorithms for Inverse Problems in Science and Engineering\n | volume = 49\n | year = 2011\n | pages = 185–212\n}}\n\n==External links==\n* Stephen Boyd and Lieven Vandenberghe Book, [http://www.stanford.edu/~boyd/cvxbook/ ''Convex optimization'']\n* [http://www.stanford.edu/class/ee364a/ EE364a: Convex Optimization I] and [http://www.stanford.edu/class/ee364b/ EE364b: Convex Optimization II], Stanford course homepages\n* [http://www.eecs.berkeley.edu/~elghaoui/Teaching/EE227A/lecture18.pdf EE227A: Lieven Vandenberghe Notes] Lecture 18\n* [https://github.com/kul-forbes/ProximalOperators.jl ProximalOperators.jl]: a [[Julia (programming language)|Julia]] package implementing proximal operators.\n* [https://github.com/kul-forbes/ProximalAlgorithms.jl ProximalAlgorithms.jl]: a [[Julia (programming language)|Julia]] package implementing algorithms based on the proximal operator, including the proximal gradient method.\n* [http://proximity-operator.net/ Proximity Operator repository]: a collection of proximity operators implemented in [[Matlab]] and [[Python (programming language)|Python]].\n\n[[Category:Gradient methods]]"
    },
    {
      "title": "Random coordinate descent",
      "url": "https://en.wikipedia.org/wiki/Random_coordinate_descent",
      "text": "Randomized (Block) Coordinate Descent Method is an optimization algorithm popularized by Nesterov (2010) and Richtárik and Takáč (2011). The first analysis of this method, when applied to the problem of minimizing a smooth convex function, was performed by Nesterov (2010).<ref>{{Citation\n | last=Nesterov\n | first=Yurii\n | year=2010\n | title=Efficiency of coordinate descent methods on huge-scale optimization problems\n | doi=10.1137/100802001\n | volume=22\n | issue=2\n | journal=SIAM Journal on Optimization\n | pages=341–362\n| citeseerx=10.1.1.332.3336\n }}</ref> In Nesterov's analysis the method needs to be applied to a quadratic perturbation of the original function with an unknown scaling factor. Richtárik and Takáč (2011) give iteration complexity bounds which do not require this, i.e., the method is applied to the objective function directly. Furthermore, they generalize the setting to the problem of minimizing a composite function, i.e., sum of a smooth convex and a (possibly nonsmooth) convex block-separable function:\n\n<math> F(x) = f(x) + \\Psi(x), </math>\n\nwhere <math> \\Psi(x) = \\sum_{i=1}^n \\Psi_i(x^{(i)}),</math> <math>x\\in R^N</math> is decomposed into <math> n </math> blocks of variables/coordinates: <math> x = (x^{(1)},\\dots,x^{(n)})</math> and <math> \\Psi_1,\\dots, \\Psi_n </math> are (simple) convex functions.\n\n'''Example (block decomposition):''' If <math> x = (x_1,x_2,\\dots,x_5) \\in R^5 </math> and <math> n = 3 </math>, one may choose <math> x^{(1)} = (x_1,x_3), x^{(2)} = (x_2,x_5) </math> and <math> x^{(3)} = x_4 </math>.\n\n'''Example (block-separable regularizers):''' \n# <math> n=N; \\Psi(x) = \\|x\\|_1 = \\sum_{i=1}^n |x_i| </math>\n# <math> N = N_1 + N_2 + \\dots + N_n; \\Psi(x) = \\sum_{i=1}^n \\|x^{(i)}\\|_2 </math>, where <math> x^{(i)}\\in R^{N_i}</math> and <math>\\|\\cdot\\|_2</math> is the standard Euclidean norm.\n\n==Algorithm==\nConsider the optimization problem\n\n:<math> \\min_{x \\in R^n} f(x), </math>\n\nwhere <math>f</math> is a [[convex function|convex]] and smooth function.\n\n'''Smoothness:''' By smoothness we mean the following: we assume\nthe gradient of <math>f</math> is coordinate-wise Lipschitz continuous \nwith constants <math>L_1, L_2, \\dots , L_n</math>. That is, we assume that\n\n:<math>|\\nabla_i f(x + h e_i) - \\nabla_i f(x)| \\leq L_i |h|, </math>\n\nfor all <math> x \\in R^n </math> and <math> h \\in R </math>, where <math> \\nabla_i </math> denotes the partial derivative with respect to variable <math>x^{(i)}</math>.\n\nNesterov, and Richtarik and Takac showed that the following algorithm converges to the optimal point:\n{{algorithm-begin|name=Random Coordinate Descent Method}}\n   Input: <math>x_0 \\in R^n</math> //starting point\n   Output: <math>x</math>\n   set x=x_0\n   for k=1,... do\n      choose coordinate <math>i\\in \\{1,2,\\dots,n\\}</math>, uniformly at random\n      update <math>x^{(i)} = x^{(i)} - \\frac1{L_i} \\nabla f_i(x)</math> \n   endfor;\n{{algorithm-end}}\n\n==Convergence rate==\nSince the iterates of this algorithm are random vectors, a complexity result would give a bound on the number of iterations needed for the method to output an approximate solution with high probability. It was shown in <ref>{{Citation\n | last=Richtárik\n | first=Peter\n | last2=Takáč\n | first2=Martin\n | year=2011\n | title=Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function\n | journal=Mathematical Programming, Series A\n | doi=10.1007/s10107-012-0614-z\n | volume=144\n | issue=1–2\n | pages=1–38| arxiv=1107.2848\n }}</ref> that if \n<math>k\\geq \\frac{2n R_L(x_0)}{\\epsilon} \\log \\left(\\frac{f(x_0)-f^*}{\\epsilon \\rho}\\right)</math>, \nwhere <math>R_L(x)=\\max_{y}  \\max_{x^* \\in X^*} \\{ \\|y-x^*\\|_L : f(y)\\leq f(x) \\}</math>, \n<math>f^*</math> is an optimal solution (<math> f^* = \\min_{x\\in R^n}\\{f(x)\\}</math>),\n<math>\\rho\\in(0,1)</math> is a confidence level and <math>\\epsilon>0</math> is  target accuracy,\nthen <math>Prob(f(x_k)-f^*> \\epsilon) \\leq \\rho</math>.\n\n==Example on particular function==\nThe following Figure shows\nhow <math>x_k</math> develops during iterations, in principle.\nThe problem is\n\n: <math> f(x) = \\tfrac{1}{2} x^T \\left(\\begin{array}{cc}\n                             1 & 0.5 \\\\ 0.5 & 1\n                            \\end{array}\n                            \\right)\n                            x -\\left(\\begin{array}{cc}\n                             1.5 & 1.5\n                            \\end{array}\n                            \\right)x,\\quad    x_0=\\left(\\begin{array}{ cc}\n                             0   & 0\n                            \\end{array}\n                            \\right)^T </math>\n\n[[File:Convergence on small problem.jpg]]\n\n==Extension to Block Coordinate Setting==\n[[File:BlockStructure.jpg|thumb|Blocking coordinate directions into Block coordinate directions]]\nOne can naturally extend this algorithm not only just to coordinates, but to blocks of coordinates.\nAssume that we have space <math>R^5</math>. This space has 5 coordinate directions, concretely\n<math> e_1 = (1,0,0,0,0)^T,\ne_2 = (0,1,0,0,0)^T,\ne_3 = (0,0,1,0,0)^T,\ne_4 = (0,0,0,1,0)^T,\ne_5 = (0,0,0,0,1)^T</math>\nin which Random Coordinate Descent Method can move.\nHowever, one can group some coordinate directions into blocks and we can have instead of those 5 coordinate directions\n3 block coordinate directions (see image).\n\n==See also==\n* [[Coordinate descent]]\n* [[Gradient descent]]\n* [[Mathematical optimization]]\n\n==References==\n{{reflist}}\n\n[[Category:Gradient methods]]"
    },
    {
      "title": "Stochastic gradient descent",
      "url": "https://en.wikipedia.org/wiki/Stochastic_gradient_descent",
      "text": "'''Stochastic gradient descent''' (often abbreviated '''SGD''') is an [[iterative method]] for [[Mathematical optimization|optimizing]] an [[objective function]] with suitable smoothness properties (e.g. [[Differentiable function|differentiable]] or [[Subgradient method|subdifferentiable]]). It is called '''stochastic''' because the method uses randomly selected (or shuffled) samples to evaluate the gradients, hence SGD can be regarded as a [[stochastic approximation]] of [[gradient descent]] optimization. The ideas can be traced back<ref>{{cite journal | last = Mei | first = Song | title = A mean field view of the landscape of two-layer neural networks | journal = Proceedings of the National Academy of Sciences | volume =  115| issue =  33| year = 2018 | pages =  E7665–E7671| jstor =  | doi = 10.1073/pnas.1806579115 | pmid = 30054315 | pmc = 6099898 }}</ref> at least to the 1951 article titled \"A Stochastic Approximation Method\" by [[Herbert Robbins]] and [[Sutton Monro]], who proposed with detailed analysis a root-finding method now called the [[stochastic approximation|Robbins–Monro algorithm]].\n\n== Background ==\n{{Main|M-estimation}}\n{{See also|Estimating equation}}\nBoth [[statistics|statistical]] [[M-estimation|estimation]] and [[machine learning]] consider the problem of [[Mathematical optimization|minimizing]] an [[objective function]] that has the form of a sum:\n: <math>Q(w) = \\frac{1}{n}\\sum_{i=1}^n Q_i(w),</math>\nwhere the [[parametric statistics|parameter]] <math>w</math> that minimizes <math>Q(w)</math> is to be [[estimator|estimated]]. Each summand function <math>Q_i</math> is typically associated with the <math>i</math>-th [[Observation (statistics)|observation]] in the [[data set]] (used for training).\n\nIn classical statistics, sum-minimization problems arise in [[least squares]] and in [[maximum-likelihood estimation]] (for independent observations).  The general class of estimators that arise as minimizers of sums are called [[M-estimator]]s. However, in statistics, it has been long recognized that requiring even local minimization is too restrictive for some problems of maximum-likelihood estimation.<ref>{{cite journal | last = Ferguson | first = Thomas S. | title = An inconsistent maximum likelihood estimate | journal = Journal of the American Statistical Association | volume = 77 | issue = 380 | year = 1982 | pages = 831–834 | jstor = 2287314 | doi = 10.1080/01621459.1982.10477894 }}</ref> Therefore, contemporary statistical theorists often consider [[stationary point]]s of the [[likelihood function]] (or zeros of its derivative, the [[Score (statistics)|score function]], and other [[estimating equations]]).\n\nThe sum-minimization problem also arises for [[empirical risk minimization]]. In this case, <math>Q_i(w)</math> is the value of the [[loss function]] at <math>i</math>-th example, and <math>Q(w)</math> is the empirical risk.\n\nWhen used to minimize the above function, a standard (or \"batch\") [[gradient descent]] method would perform the following iterations :\n: <math>w := w - \\eta \\nabla Q(w) = w - \\eta \\sum_{i=1}^n \\nabla Q_i(w)/n,</math>\nwhere <math>\\eta</math> is a step size (sometimes called the ''[[learning rate]]'' in machine learning).\n\nIn many cases, the summand functions have a simple form that enables inexpensive evaluations of the sum-function and the sum gradient. For example, in statistics, [[exponential families|one-parameter exponential families]] allow economical function-evaluations and gradient-evaluations.\n\nHowever, in other cases, evaluating the sum-gradient may require expensive evaluations of the gradients from all summand functions. When the training set is enormous and no simple formulas exist, evaluating the sums of gradients becomes very expensive, because evaluating the gradient requires evaluating all the summand functions' gradients. To economize on the computational cost at every iteration, stochastic gradient descent [[sampling (statistics)|samples]] a subset of summand functions at every step. This is very\neffective in the case of large-scale machine learning problems.<ref>{{Cite conference |first1=Léon |last1=Bottou |author1-link=Léon Bottou |last2=Bousquet |first2=Olivier |title=The Tradeoffs of Large Scale Learning |url=http://leon.bottou.org/papers/bottou-bousquet-2008 |conference=[[Advances in Neural Information Processing Systems]] |volume=20 |pages=161–168 |year=2008}}</ref>\n\n== Iterative method ==\n[[Image:stogra.png|thumb|right|Fluctuations in the total objective function as gradient steps with respect to mini-batches are taken.]]\n\nIn stochastic (or \"on-line\") gradient descent, the true gradient of <math>Q(w)</math> is approximated by a gradient at a single example:\n: <math>w := w - \\eta \\nabla Q_i(w).</math>\nAs the algorithm sweeps through the training set, it performs the above update for each training example. Several passes can be made over the training set until the algorithm converges. If this is done, the data can be shuffled for each pass to prevent cycles. Typical implementations may use an [[adaptive learning rate]] so that the algorithm converges.\n\nIn pseudocode, stochastic gradient descent can be presented as follows:\n<div style=\"margin-left: 35px; width: 600px\">\n{{framebox|blue}}\n* Choose an initial vector of parameters <math>w</math> and learning rate <math>\\eta</math>.\n* Repeat until an approximate minimum is obtained:\n** Randomly shuffle examples in the training set.\n** For <math> i=1, 2, ..., n</math>, do:\n*** <math>\\! w := w - \\eta \\nabla Q_i(w).</math>\n{{frame-footer}}\n</div>\n\nA compromise between computing the true gradient and the gradient at a single example is to compute the gradient against more than one training example (called a \"mini-batch\") at each step.  This can perform significantly better than \"true\" stochastic gradient descent described, because the code can make use of [[Vectorization (mathematics)|vectorization]] libraries rather than computing each step separately.  It may also result in smoother convergence, as the gradient computed at each step is averaged over more training examples.\n\nThe convergence of stochastic gradient descent has been analyzed using the theories of [[convex optimization|convex minimization]] and of [[stochastic approximation]]. Briefly, when the [[learning rate]]s <math>\\eta</math> decrease with an appropriate rate,\nand subject to relatively mild assumptions, stochastic gradient descent converges [[almost surely]] to a global minimum \nwhen the objective function is [[convex function|convex]] or [[pseudoconvex function|pseudoconvex]], \nand otherwise converges almost surely to a local minimum.<ref>{{Cite book\n     |last=Bottou\n     |first=Léon\n     |authorlink=Léon Bottou\n     |contribution=Online Algorithms and Stochastic Approximations\n     |year=1998\n     |title=Online Learning and Neural Networks\n     |publisher=Cambridge University Press\n     |url=http://leon.bottou.org/papers/bottou-98x\n     |isbn=978-0-521-65263-6\n     |postscript=<!-- Bot inserted parameter. Either remove it; or change its value to \".\" for the cite to end in a \".\", as necessary. -->{{inconsistent citations}}\n}}</ref><ref>{{cite news\n     |last=Kiwiel\n     |first=Krzysztof C.\n     |title=Convergence and efficiency of subgradient methods for quasiconvex minimization\n     |journal=Mathematical Programming, Series A\n     |publisher=Springer|location=Berlin, Heidelberg\n     |issn=0025-5610|pages=1–25|volume=90|issue=1\n     |doi=10.1007/PL00011414|year=2001 |mr=1819784}}</ref>\nThis is in fact a consequence of the [[Robbins-Siegmund theorem]].<ref>{{Cite book\n     |last1=Robbins\n     |first1=Herbert\n     |author1-link=Herbert Robbins\n     |last2=Siegmund\n     |first2=David O.\n     |author2-link=David O. Siegmund\n     |contribution=A convergence theorem for non negative almost supermartingales and some applications\n     |title=Optimizing Methods in Statistics\n     |publisher=Academic Press\n     |year=1971\n     |editor-last=Rustagi\n     |editor-first=Jagdish S.\n     |postscript=<!-- Bot inserted parameter. Either remove it; or change its value to \".\" for the cite to end in a \".\", as necessary. -->{{inconsistent citations}}\n}}\n</ref>\n\n== Example ==\nLet's suppose we want to fit a straight line <math>y = \\! w_1 + w_2 x</math> to a training set with observations<math> (x_1, x_2, \\ldots, x_n)</math> and corresponding estimated responses <math> (\\hat{y_1}, \\hat{y_2}, \\ldots, \\hat{y_n})</math> using [[least squares]]. The objective function to be minimized is:\n: <math>Q(w) = \\sum_{i=1}^n Q_i(w) = \\sum_{i=1}^n \\left(\\hat{y_i}-y_i\\right)^2 = \\sum_{i=1}^n \\left(w_1 + w_2 x_i - y_i\\right)^2.</math>\n\nThe last line in the above pseudocode for this specific problem will become:\n: <math>\\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix} :=\n    \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix}\n    - \\eta \\begin{bmatrix} \\frac{\\partial}{\\partial w_1} (w_1 + w_2 x_i - y_i)^2 \\\\\n            \\frac{\\partial}{\\partial w_2} (w_1 + w_2 x_i - y_i)^2 \\end{bmatrix} =\n    \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix}\n    -  \\eta  \\begin{bmatrix} 2 (w_1 + w_2 x_i - y_i) \\\\ 2 x_i(w_1 + w_2 x_i - y_i) \\end{bmatrix}.</math>\n\nNote that in each iteration (also called update), only the gradient evaluated at a single point <math> x_i </math> instead of evaluating at the set of all samples.\n\nThe key difference compared to standard (Batch) Gradient Descent is that only one piece of data from the dataset is used to calculate the step, and the piece of data is picked randomly at each step.\n\n== Notable applications ==\nStochastic gradient descent is a popular algorithm for training a wide range of models in [[machine learning]], including (linear) [[support vector machine]]s, [[logistic regression]] (see, e.g., [[Vowpal Wabbit]]) and [[graphical model]]s.<ref>Jenny Rose Finkel, Alex Kleeman, Christopher D. Manning (2008). [http://www.aclweb.org/anthology/P08-1109 Efficient, Feature-based, Conditional Random Field Parsing]. Proc. Annual Meeting of the ACL.</ref> When combined with the [[backpropagation]] algorithm, it is the ''de facto'' standard algorithm for training [[artificial neural network]]s.<ref>[http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf LeCun, Yann A., et al. \"Efficient backprop.\" Neural networks: Tricks of the trade. Springer Berlin Heidelberg, 2012. 9-48]</ref> Its use has been also reported in the [[Geophysics]] community, specifically to applications of Full Waveform Inversion (FWI).<ref>[http://library.seg.org/doi/abs/10.1190/1.3627777 Díaz, Esteban and Guitton, Antoine. \"Fast full waveform inversion with random shot decimation\". SEG Technical Program Expanded Abstracts, 2011. 2804-2808]</ref>\n\nStochastic gradient descent competes with the [[limited-memory BFGS|L-BFGS]] algorithm,{{Citation needed|date=July 2015}} which is also widely used. Stochastic gradient descent has been used since at least 1960 for training [[linear regression]] models, originally under the name [[ADALINE]].<ref>{{cite web |author=Avi Pfeffer |title=CS181 Lecture 5 — Perceptrons |url=http://www.seas.harvard.edu/courses/cs181/files/lecture05-notes.pdf |publisher=Harvard University }}{{Dead link|date=June 2018 |bot=InternetArchiveBot |fix-attempted=no }}</ref>\n\nAnother stochastic gradient descent algorithm is the [[Least mean squares filter|least mean squares (LMS)]] adaptive filter.\n\n==Extensions and variants==\nMany improvements on the basic stochastic gradient descent algorithm have been proposed and used. In particular, in machine learning, the need to set a [[learning rate]] (step size) has been recognized as problematic. Setting this parameter too high can cause the algorithm to diverge{{citation needed|date=October 2017}}; setting it too low makes it slow to converge{{citation needed|date=October 2017}}. A conceptually simple extension of stochastic gradient descent makes the learning rate a decreasing function {{mvar|η<sub>t</sub>}} of the iteration number {{mvar|t}}, giving a ''learning rate schedule'', so that the first iterations cause large changes in the parameters, while the later ones do only fine-tuning. Such schedules have been known since the work of MacQueen on [[K-means clustering|{{mvar|k}}-means clustering]].<ref>Cited by {{cite conference |last1=Darken |first1=Christian |first2=John |last2=Moody |title=Fast adaptive k-means clustering: some empirical results |year=1990 |conference=Int'l Joint Conf. on Neural Networks (IJCNN) |publisher=IEEE|url=http://ieeexplore.ieee.org/abstract/document/5726679/}}</ref> Some practical guidance on choosing the step size in several variants of SGD is given in Sects. 4.4, 6.6, and 7.5 of <ref>Spall, J. C. (2003), Introduction to Stochastic Search and Optimization: Estimation, Simulation, and Control, Wiley, Hoboken, NJ</ref>. \n\n===Implicit updates (ISGD)===\nAs mentioned earlier, classical stochastic gradient descent is generally sensitive to [[learning rate]] {{mvar|η}}. Fast convergence requires large learning rates but this may induce numerical instability. The problem can be largely solved{{citation needed|date=May 2019}} by considering ''implicit updates'' whereby the stochastic gradient is evaluated at the next iterate rather than the current one:\n:<math>w^{new} := w^{old} - \\eta \\nabla Q_i(w^{new}).</math>\n\nThis equation is implicit since <math>w^{new}</math> appears on both sides of the equation. It is a stochastic form of the [[proximal gradient method]] since the update\ncan also be written as:\n:<math>w^{new} := \\arg\\min_w \\{ Q_i(w) + \\frac{1}{2\\eta} ||w - w^{old}||^2 \\}.</math>\n\nAs an example, \nconsider least squares with features <math>x_1, \\ldots, x_n \\in\\mathbb{R}^p</math> and observations\n<math>y_1, \\ldots, y_n\\in\\mathbb{R}</math>. We wish to solve:\n:<math>\\min_w \\sum_{j=1}^n (y_j - x_j'w)^2.</math>\nNote that <math>x</math> could have \"1\" as the first element to include an intercept. Classical stochastic gradient descent proceeds as follows:\n:<math>w^{new} = w^{old} + \\eta (y_i - x_i'w^{old}) x_i</math>\n\nwhere <math>i</math> is uniformly sampled between 1 and <math>n</math>. Although theoretical convergence of this procedure happens under relatively mild assumptions, in practice the procedure can be quite unstable. In particular, when <math>\\eta</math> is misspecified so that <math>I - \\eta x_i x_i'</math> has large absolute eigenvalues with high probability, the procedure may diverge numerically within a few iterations. In contrast, ''implicit stochastic gradient descent'' (shortened as ISGD) can be solved in closed-form as:\n:<math>w^{new} = w^{old} + \\frac{\\eta}{1 + \\eta ||x_i||^2} (y_i - x_i'w^{old}) x_i.</math>\n\nThis procedure will remain numerically stable virtually for all <math>\\eta</math> as the [[learning rate]] is now normalized. Such comparison between classical and implicit stochastic gradient descent in the least squares problem is very similar to the comparison between [[Least mean squares filter|least mean squares (LMS)]] and \n[[Least mean squares filter#Normalized least mean squares filter (NLMS)|normalized least mean squares filter (NLMS)]].\n\nEven though a closed-form solution for ISGD is only possible in least squares, the procedure can be efficiently implemented in a wide range of models. Specifically, suppose that <math>Q_i(w)</math> depends on <math>w</math> only through a linear combination with features <math>x_i</math>, so that we can write <math>\\nabla_w Q_i(w) = -q(x_i'w) x_i</math>, where \n<math>q() \\in\\mathbb{R}</math> may depend on <math>x_i, y_i</math> as well but not on <math>w</math> except through <math>x_i'w</math>. Least squares obeys this rule, and so does [[logistic regression]], and most [[generalized linear model]]s. For instance, in least squares, <math>q(x_i'w) = y_i - x_i'w</math>, and in logistic regression <math>q(x_i'w) = y_i - S(x_i'w)</math>, where <math>S(u) = e^u/(1+e^u)</math> is the [[logistic function]]. In [[Poisson regression]], <math>q(x_i'w) = y_i - e^{x_i'w}</math>, and so on.\n\nIn such settings, ISGD is simply implemented as follows:\n:<math>w^{new} = w^{old} + \\xi x_i,\\quad\\xi = \\eta q(x_i'w^{old} + \\xi ||x_i||^2).</math>\n\nThe scaling factor <math>\\xi\\in\\mathbb{R}</math> can be found through [[bisection method]] since \nin most regular models, such as the aforementioned generalized linear models, function <math>q()</math> is decreasing, \nand thus the search bounds for <math>\\xi</math> are \n<math>[\\min(0, b_i), \\max(0, b_i)]</math>, where <math>b_i = \\eta q(x_i'w^{old})</math>.\n\n===Momentum===\nFurther proposals include the ''momentum method'', which appeared in [[David Rumelhart|Rumelhart]], [[Geoffrey Hinton|Hinton]] and [[Ronald J. Williams|Williams]]' seminal paper on backpropagation learning.<ref name=\"Rumelhart1986\">{{cite journal|last=Rumelhart|first=David E.|author2=Hinton, Geoffrey E.|author3=Williams, Ronald J.|title=Learning representations by back-propagating errors|journal=Nature|date=8 October 1986|volume=323|issue=6088|pages=533–536|doi=10.1038/323533a0|bibcode=1986Natur.323..533R}}</ref> Stochastic gradient descent with momentum remembers the update {{math|Δ ''w''}} at each iteration, and determines the next update as a [[linear combination]] of the gradient and the previous update:<ref name=\"Sutskever2013\">{{cite conference|last=Sutskever|first=Ilya|author2=Martens, James|author3=Dahl, George|author4=Hinton, Geoffrey E.|editor=Sanjoy Dasgupta and David Mcallester|title=On the importance of initialization and momentum in deep learning|conference=In Proceedings of the 30th international conference on machine learning (ICML-13)|date=June 2013|volume=28|location=Atlanta, GA|pages=1139–1147|url=http://www.cs.utoronto.ca/~ilya/pubs/2013/1051_2.pdf|access-date=14 January 2016}}</ref><ref name=\"SutskeverPhD\">{{cite thesis|last=Sutskever|first=Ilya|title=Training recurrent neural networks|date=2013|publisher=University of Toronto|url=http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf|type=Ph.D.|page=74}}</ref>\n:<math>\\Delta w := \\alpha \\Delta w - \\eta \\nabla Q_i(w)</math>\n:<math>w := w + \\Delta w </math>\nthat leads to:\n:<math>w := w - \\eta \\nabla Q_i(w) + \\alpha \\Delta w </math>\n\nwhere the [[parametric statistics|parameter]] <math>w</math> which minimizes <math>Q(w)</math> is to be [[estimator|estimated]], and <math>\\eta</math> is a step size (sometimes called the ''[[learning rate]]'' in machine learning){{clarify|reason=What is alpha in this case??|date=October 2017}}.\n\nThe name momentum stems from an analogy to [[momentum]] in physics: the weight vector <math>w</math>, thought of as a particle traveling through parameter space{{r|Rumelhart1986}}, incurs acceleration from the gradient of the loss (\"[[force]]\"). Unlike in classical stochastic gradient descent, it tends to keep traveling in the same direction, preventing oscillations. Momentum has been used successfully by computer scientists in the training of [[artificial neural networks]] for several decades.<ref name=\"Zeiler 2012\">{{cite arXiv |last=Zeiler |first=Matthew D. |eprint=1212.5701 |title=ADADELTA: An adaptive learning rate method |year=2012|class=cs.LG }}</ref>\n\n===Averaging===\n''Averaged stochastic gradient descent'', invented independently by Ruppert and Polyak in the late 1980s, is ordinary stochastic gradient descent that records an average of its parameter vector over time. That is, the update is the same as for ordinary stochastic gradient descent, but the algorithm also keeps track of<ref>{{cite journal |last1=Polyak |first1=Boris T. |first2=Anatoli B. |last2=Juditsky |title=Acceleration of stochastic approximation by averaging |journal=SIAM J. Control Optim. |volume=30 |issue=4 |year=1992 |pages=838–855|url=http://www.meyn.ece.ufl.edu/archive/spm_files/Courses/ECE555-2011/555media/poljud92.pdf|doi=10.1137/0330046 }}</ref>\n\n:<math>\\bar{w} = \\frac{1}{t} \\sum_{i=0}^{t-1} w_i</math>.\n\nWhen optimization is done, this averaged parameter vector takes the place of {{mvar|w}}.\n\n===AdaGrad===\n''AdaGrad'' (for adaptive [[Gradient descent|gradient]] algorithm) is a modified stochastic gradient descent with per-parameter [[learning rate]], first published in 2011.<ref name=\"duchi\">{{cite journal |last1=Duchi |first1=John |first2=Elad |last2=Hazan |first3=Yoram |last3=Singer |title=Adaptive subgradient methods for online learning and stochastic optimization |journal=[[Journal of Machine Learning Research|JMLR]] |volume=12 |year=2011 |pages=2121–2159 |url=http://jmlr.org/papers/volume12/duchi11a/duchi11a.pdf}}</ref><ref>{{cite web |first=Joseph |last=Perla |year=2014 |title=Notes on AdaGrad |url=http://seed.ucsd.edu/mediawiki/images/6/6a/Adagrad.pdf |deadurl=yes |archiveurl=https://web.archive.org/web/20150330033637/http://seed.ucsd.edu/mediawiki/images/6/6a/Adagrad.pdf |archivedate=2015-03-30 |df= }}</ref> Informally, this increases the learning rate for sparser parameters and decreases the learning rate for ones that are less sparse. This strategy often improves convergence performance over standard stochastic gradient descent in settings where data is sparse and sparse parameters are more informative. Examples of such applications include natural language processing and image recognition.<ref name=\"duchi\"/>  It still has a base learning rate {{mvar|η}}, but this is multiplied with the elements of a vector {{math|{''G''<sub>''j'',''j''</sub>} }} which is the diagonal of the [[outer product]] matrix\n\n:<math>G = \\sum_{\\tau=1}^t g_\\tau g_\\tau^\\mathsf{T}</math>\n\nwhere <math>g_\\tau = \\nabla Q_i(w)</math>, the gradient, at iteration {{mvar|τ}}. The diagonal is given by\n\n:<math>G_{j,j} = \\sum_{\\tau=1}^t g_{\\tau,j}^2</math>.\n\nThis vector is updated after every iteration. The formula for an update is now\n\n:<math>w := w - \\eta\\, \\mathrm{diag}(G)^{-\\frac{1}{2}} \\circ g</math>{{efn|<math>\\circ</math> is the [[Hadamard product (matrices)|element-wise product]].}}\n\nor, written as per-parameter updates,\n\n:<math>w_j := w_j - \\frac{\\eta}{\\sqrt{G_{j,j}}} g_j.</math>\n\nEach {{math|{''G''<sub>(''i'',''i'')</sub>} }} gives rise to a scaling factor for the learning rate that applies to a single parameter {{math|''w''<sub>''i''</sub>}}. Since the denominator in this factor, <math>\\sqrt{G_i} = \\sqrt{\\sum_{\\tau=1}^t g_\\tau^2}</math> is the [[Norm (mathematics)#Euclidean norm|''ℓ''<sub>2</sub> norm]] of previous derivatives, extreme parameter updates get dampened, while parameters that get few or small updates receive higher learning rates.<ref name=\"Zeiler 2012\"/>\n\nWhile designed for [[convex optimization|convex problems]], AdaGrad has been successfully applied to non-convex optimization.<ref>{{cite journal |last1=Gupta |first1=Maya R. |first2=Samy |last2=Bengio |first3=Jason |last3=Weston |title=Training highly multiclass classifiers |journal=JMLR |volume=15 |issue=1 |year=2014 |pages=1461–1492 |url=http://jmlr.org/papers/volume15/gupta14a/gupta14a.pdf}}</ref>\n\n===RMSProp===\n''RMSProp'' (for Root Mean Square Propagation) is also a method in which the [[learning rate]] is adapted for each of the parameters. The idea is to divide the learning rate for a weight by a running average of the magnitudes of recent gradients for that weight.<ref>Tieleman, Tijmen and Hinton, Geoffrey (2012). Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning</ref> So, first the running average is calculated in terms of means square,\n\n:<math>v(w,t):=\\gamma v(w,t-1)+(1-\\gamma)(\\nabla Q_i(w))^2</math>\n\nwhere, <math>\\gamma</math> is the forgetting factor.\n\nAnd the parameters are updated as,\n\n:<math>w:=w-\\frac{\\eta}{\\sqrt{v(w,t)}}\\nabla Q_i(w)</math>\n\nRMSProp has shown excellent adaptation of learning rate in different applications. RMSProp can be seen as a generalization of [[Rprop]] and is capable to work with mini-batches as well opposed to only full-batches.<ref>{{Cite web|url=http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf|title=Overview of mini-batch gradient descent|last=Hinton|first=Geoffrey|authorlink=Geoffrey Hinton|date=|website=|publisher=|pages=27–29|access-date=27 September 2016}}</ref>\n\n===Adam===\n''Adam''<ref name=\"Adam2014\">{{cite arXiv |last1=Diederik |first1=Kingma |first2=Jimmy |last2=Ba |eprint=1412.6980 |title=Adam: A method for stochastic optimization |year=2014 |class=cs.LG }}</ref> (short for Adaptive Moment Estimation) is an update to the ''RMSProp'' optimizer. In this optimization algorithm, running averages of both the gradients and the second moments of the gradients are used. Given parameters <math> w^ {(t)} </math> and a loss function <math> L ^ {(t)} </math>, where <math> t </math> indexes the current training iteration (indexed at <math> 0 </math>), Adam's parameter update is given by:\n\n:<math>m_w ^ {(t+1)} \\leftarrow \\beta_1 m_w ^ {(t)} + (1 - \\beta_1) \\nabla _w L ^ {(t)} </math>\n:<math>v_w ^ {(t+1)} \\leftarrow \\beta_2 v_w ^ {(t)} + (1 - \\beta_2) (\\nabla _w L ^ {(t)} )^2 </math>\n\n:<math>\\hat{m}_w = \\frac{m_w ^ {(t+1)}}{1 - (\\beta_1) ^{t+1}} </math>\n:<math>\\hat{v}_w = \\frac{ v_w ^ {(t+1)}}{1 - (\\beta_2) ^{t+1}} </math>\n\n:<math>w ^ {(t+1)} \\leftarrow w ^ {(t)} - \\eta \\frac{\\hat{m}_w}{\\sqrt{\\hat{v}_w} + \\epsilon} </math>\n\nwhere <math>\\epsilon</math> is a small scalar used to prevent division by 0, and <math>\\beta_1</math> and <math>\\beta_2</math> are the forgetting factors for gradients and second moments of gradients, respectively. Squaring and square-rooting is done elementwise.\n\n===Natural Gradient Descent and kSGD===\nKalman-based Stochastic Gradient Descent (kSGD)<ref name=\":0\">{{Cite journal|last=Patel|first=V.|date=2016-01-01|title=Kalman-Based Stochastic Gradient Method with Stop Condition and Insensitivity to Conditioning|journal=SIAM Journal on Optimization|volume=26|issue=4|pages=2620–2648|doi=10.1137/15M1048239|issn=1052-6234|arxiv=1512.01139}}</ref> is an online and offline algorithm for learning parameters from statistical problems from [[quasi-likelihood]] models, which include [[Linear regression|linear models]], [[Nonlinear regression|non-linear models]], [[generalized linear model]]s, and [[Artificial neural network|neural networks]] with [[Mean squared error|squared error loss]] as special cases. For online learning problems, kSGD is a special case of the [[Kalman filter|Kalman Filter]] for linear regression problems, a special case of the [[Extended Kalman filter|Extended Kalman Filter]] for non-linear regression problems, and can be viewed as an incremental [[Gauss–Newton algorithm|Gauss-Newton]] method. Moreover, because of kSGD's relationship to the Kalman Filter and natural gradient descent's<ref>{{cite journal|last1=Cichocki|first1=A|last2=Chen|first2=T|last3=Amari|first3=S|title=Stability Analysis of Learning Algorithms for Blind Source Separation.|journal=Neural Networks|date=November 1997|volume=10|issue=8|pages=1345–1351|pmid=12662478|doi=10.1016/S0893-6080(97)00039-7}}</ref> relationship to the Kalman Filter,<ref>{{cite arxiv|last1=Yann|first1=Ollivier|title=Online Natural Gradient as a Kalman Filter|eprint=1703.00209|language=en|date=1 March 2017|class=stat.ML}}</ref> kSGD is a rigorous improvement over the popular natural gradient descent method.\n\nThe benefits of kSGD, in comparison to other methods, are (1) it is not sensitive to the condition number of the problem ,{{efn|For the linear regression problem, kSGD's objective function discrepancy (i.e. the total of bias and variance) at iteration <math>k</math>  is <math>{\\frac {1+\\epsilon }{k}}p\\sigma ^{2}</math>  with probability converging to 1 at a rate depending on <math>\\epsilon \\in (0,1)</math>, where <math>\\sigma ^{2}</math>  is the variance of the residuals. Moreover, for specific choices of <math>\\gamma _{1},\\gamma _{2}</math>, kSGD's objective function bias at iteration <math>k</math>  can be shown to be <math>\\frac {(1+\\epsilon )^{2}}{2k^{2}}\\Vert w(0)-w_{*}\\Vert _{2}^{2}</math>  with probability converging to 1 at a rate depending on <math>\\epsilon \\in (0,1)</math>, where <math>w_{*}</math> is the optimal parameter.\n}} (2) it has a robust choice of hyperparameters, and (3) it has a stopping condition. The drawbacks of kSGD is that the algorithm requires storing a dense covariance matrix between iterations, and requires a matrix-vector product at each iteration.\n\nTo describe the algorithm, suppose <math>Q_i(w)</math>, where <math>w \\in \\mathbb{R}^p</math> is defined by an example <math>(Y_i,X_i)\\in \\mathbb{R} \\times \\mathbb{R}^d</math> such that\n\n:<math>\\nabla_w Q_i(w) = \\frac{Y_i - \\mu(X_i,w)}{V(\\mu(X_i,w))} \\nabla_w \\mu(X_i,w)</math>\n\nwhere <math>\\mu(X_i,w)</math> is mean function (i.e. the expected value of <math>Y_i</math> given <math>X_i</math>), and <math>V(\\mu(X_i,w))</math> is the variance function (i.e. the variance of <math>Y_i</math> given <math>X_i</math>). Then, the parameter update, <math> w(t+1) </math>, and covariance matrix update, <math> M(t+1) </math> are given by the following\n\n:<math> p = \\nabla_w \\mu(X_{t+1},w(t)) </math>\n:<math> m = \\mu(X_{t+1},w(t)) </math>\n:<math> v = M(t) p </math>\n:<math> s = \\min\\lbrace \\gamma_1, \\max\\lbrace \\gamma_2, V(m)\\rbrace \\rbrace + v^\\mathsf{T} p </math>\n:<math> w(t+1) = w(t) + \\frac{Y_{t+1} - m}{s}v </math>\n:<math> M(t+1) = M(t) - \\frac{1}{s} v v^\\mathsf{T} </math>\n\nwhere <math>\\gamma_1,\\gamma_2</math> are hyperparameters. The <math>M(t)</math> update can result in the covariance matrix becoming indefinite, which can be avoided at the cost of a matrix-matrix multiplication. <math>M(0)</math> can be any positive definite symmetric matrix, but is typically taken to be the identity. As noted by Patel,<ref name=\":0\" /> for all problems besides linear regression, restarts are required to ensure convergence of the algorithm, but no theoretical or implementation details were given. In a closely related, off-line, mini-batch method for non-linear regression analyzed by Bertsekas,<ref>{{Cite journal|last=Bertsekas|first=D.|date=1996-08-01|title=Incremental Least Squares Methods and the Extended Kalman Filter|journal=SIAM Journal on Optimization|volume=6|issue=3|pages=807–822|doi=10.1137/S1052623494268522|issn=1052-6234|hdl=1721.1/3362}}</ref> a forgetting factor was used in the covariance matrix update to prove convergence.\n\n===Second-Order Methods===\nIt is known that a stochastic analogue of the standard (deterministic) Newton-Raphson algorithm (a “second-order” method) provides an asymptotically optimal or near-optimal form of iterative optimization in the setting of stochastic approximation. A method that uses direct measurements of the Hessian matrices of the summands in the empirical risk function is given in <ref>R. H. Byrd, S. L. Hansen, J. Nocedal, and Y. Singer, “A Stochastic Quasi-Newton method for Large-Scale Optimization,” SIAM Journal on Optimization, vol. 26, no. 2, pp. 1008–1031, 2016</ref>. However, directly determining the required Hessian matrices for optimization may not be possible in practice. Practical and theoretically sound methods for second-order versions of SGD that do not require direct Hessian information are given in <ref>J. C. Spall (2000), “Adaptive Stochastic Approximation by the Simultaneous Perturbation Method,” IEEE Transactions on Automatic Control, vol. 45, pp. 1839−1853. http://dx.doi.org/10.1109/TAC.2000.880982 </ref> <ref>J. C. Spall (2009), “Feedback and Weighting Mechanisms for Improving Jacobian Estimates in the Adaptive Simultaneous Perturbation Algorithm,” IEEE Transactions on Automatic Control, vol. 54(6), pp. 1216–1229. http://dx.doi.org/10.1109/TAC.2009.2019793 </ref> <ref>S. Bhatnagar, H. L. Prasad, and L. A. Prashanth (2013), Stochastic Recursive Algorithms for Optimization: Simultaneous Perturbation Methods. Springer.</ref>. (A less efficient method based on finite differences, instead of simultaneous perturbations, is given in <ref>D. Ruppert, “A Newton-Raphson Version of the Multivariate Robbins-Monro Procedure,” The Annals of Statistics, vol. 13, no. 1, pp. 236–245, 1985</ref>.) These methods not requiring direct Hessian information are based on either values of the summands in the above empirical risk function or values of the gradients of the summands (i.e., the SGD inputs). In particular, second-order optimality is asymptotically achievable without direct calculation of the Hessian matrices of the summands in the empirical risk function.   \n\n== Notes ==\n{{notelist}}\n\n== See also ==\n* [[Coordinate descent]] – changes one coordinate at a time, rather than one example\n* [[Linear classifier]]\n* [[Online machine learning]]\n\n== References ==\n{{reflist|30em}}\n\n== Further reading ==\n*{{Citation\n | last = Bertsekas\n | first = Dimitri P.\n | author-link = Dimitri P. Bertsekas\n | title = Nonlinear Programming\n | publisher = Athena Scientific\n | year = 1999\n | edition = 2nd\n | location = Cambridge, MA.\n | isbn = 978-1-886529-00-7 \n | ref = none\n}}.\n\n*{{Citation\n | last = Bertsekas\n | first = Dimitri\n | author-link = Dimitri P. Bertsekas\n | title = Convex Analysis and Optimization\n | publisher = Athena Scientific\n | year = 2003\n | ref = none\n}}.\n\n*{{Citation\n | last = Bottou\n | first = Léon\n | author-link = Léon Bottou\n | contribution = Stochastic Learning\n | year = 2004\n | title = Advanced Lectures on Machine Learning\n | pages = 146–168\n | publisher = Springer\n | series = LNAI\n | volume = 3176\n | url = http://leon.bottou.org/papers/bottou-mlss-2004\n | isbn = 978-3-540-23122-6\n | ref = none\n}}.\n\n*{{Citation\n | last = Davidon\n | first = W.C.\n | author-link = William C. Davidon\n | title = New least-square algorithms\n | journal = Journal of Optimization Theory and Applications\n | volume = 18\n | year = 1976\n | number = 2\n | pages = 187–197\n | doi = 10.1007/BF00935703\n | mr=418461\n | ref = none\n}}.\n\n*{{Citation\n | title = Pattern Classification\n | first1 = Richard O.\n | last1 = Duda\n | first2 = Peter E.\n | last2 = Hart\n | first3 = David G.\n | last3 = Stork\n | publisher = [[John Wiley & Sons|Wiley]]\n | year = 2000\n | edition = 2nd\n | isbn = 978-0-471-05669-0\n | ref = none\n}}.\n\n*{{Citation\n | last = Kiwiel\n | first = Krzysztof C.\n | title = Convergence of approximate and incremental subgradient methods for convex optimization\n | journal = SIAM Journal on Optimization\n | volume = 14\n | year = 2004\n | number = 3\n | pages = 807–840\n | doi = 10.1137/S1052623400376366\n | mr = 2085944\n | ref = none\n}}. (Extensive list of references)\n\n*{{Citation\n | title = Practical Mathematical Optimization - Basic Optimization Theory and Gradient-Based Algorithms|series=Springer Optimization and Its Applications Vol. 133\n | first1 = Jan A.\n | last1 = Snyman\n | first2 = Daniel N.\n | last2 = Wilke\n | publisher = [[Springer International Publishing|Springer]]\n | edition=2\n | year = 2018\n | pages=xxvi+372\n | isbn = 978-3-319-77585-2\n | url = https://www.springer.com/gp/book/9783319775852\n | ref = none\n}}. (Python module [http://extras.springer.com/2018/978-3-319-77585-2 pmo.py])\n\n*{{Citation\n | title = Introduction to Stochastic Search and Optimization\n | first = James C.\n | last = Spall\n | publisher = [[John Wiley & Sons|Wiley]]\n | year = 2003\n | isbn = 978-0-471-33052-3\n | ref = none\n}}.\n\n== External links ==\n* [http://codingplayground.blogspot.it/2013/05/stocastic-gradient-descent.html Using stochastic gradient descent in C++, Boost, Ublas for linear regression]\n* [http://studyofai.com/machine-learning-algorithms/ Machine Learning Algorithms]\n* {{cite web |work=3Blue1Brown |title=Gradient Descent, How Neural Networks Learn |date=October 16, 2017 |url=https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=2 |via=[[YouTube]] }}\n\n[[Category:Stochastic optimization]]\n[[Category:Computational statistics]]\n[[Category:Gradient methods]]\n[[Category:M-estimators]]\n[[Category:Machine learning algorithms]]\n[[Category:Convex optimization]]\n[[Category:Statistical approximations]]"
    },
    {
      "title": "Interpolation",
      "url": "https://en.wikipedia.org/wiki/Interpolation",
      "text": "{{Other uses}}\n\n{{more footnotes|date=October 2016}}\nIn the [[mathematics|mathematical]] field of [[numerical analysis]], '''interpolation''' is a method of constructing new [[data points]] within the range of a [[discrete set]] of known data points.\n\nIn [[engineering]] and [[science]], one often has a number of data points, obtained by [[sampling (statistics)|sampling]] or [[experimentation]], which represent the values of a function for a limited number of values of the [[Dependent and independent variables|independent variable]]. It is often required to '''interpolate''', i.e., estimate the value of that function for an intermediate value of the independent variable. \n\nA closely related problem is the [[function approximation|approximation]] of a complicated function by a simple function. Suppose the formula for some given function is known, but too complicated to evaluate efficiently. A few data points from the original function can be interpolated to produce a simpler function which is still fairly close to the original. The resulting gain in simplicity may outweigh the loss from interpolation error.\n\n[[File:Splined epitrochoid.svg|300px|thumb|An interpolation of a finite set of points on an [[epitrochoid]]. The points in red are connected by blue interpolated [[spline (mathematics)|spline curves]] deduced only from the red points. The interpolated curves have polynomial formulas much simpler than that of the original epitrochoid curve.]]\n\n==Example==\nThis table gives some values of an unknown function <math>f(x)</math>.\n[[File:Interpolation Data.svg|right|thumb|230px|Plot of the data points as given in the table.]]\n{| cellpadding=0 cellspacing=0\n|width=\"20px\"|\n! ''x''\n|width=\"10px\"|\n!colspan=3 align=center| ''f''(''x'')\n|-\n| || 0 || ||align=right| 0\n|-\n| || 1 || ||align=right| 0 || . || 8415\n|-\n| || 2 || ||align=right| 0 || . || 9093\n|-\n| || 3 || ||align=right| 0 || . ||1411\n|-\n| || 4 || ||align=right| &minus;0 || . || 7568\n|-\n| || 5 || ||align=right| &minus;0 || . || 9589\n|-\n| || 6 || ||align=right| &minus;0 || . || 2794\n|}\nInterpolation provides a means of estimating the function at intermediate points, such as <math>x=2.5</math>.\n\nWe describe some [[algorithm|methods]] of interpolation, differing in such properties as: accuracy, cost, number of data points needed, and [[smooth function|smoothness]] of the resulting [[interpolant]] function.\n{{clear}}\n\n===Piecewise constant interpolation===\n[[File:Piecewise constant.svg|thumb|right|Piecewise constant interpolation, or [[nearest-neighbor interpolation]].]]\n{{details|Nearest-neighbor interpolation}}\nThe simplest interpolation method is to locate the nearest data value, and assign the same value. In simple problems, this method is unlikely to be used, as linear interpolation (see below) is almost as easy, but in higher-dimensional [[multivariate interpolation]], this could be a favourable choice for its speed and simplicity.\n{{clear}}\n\n===Linear interpolation===\n[[File:Interpolation example linear.svg|right|thumb|230px|Plot of the data with linear interpolation superimposed]]\n{{Main|Linear interpolation}}\nOne of the simplest methods is [[linear]] interpolation (sometimes known as lerp). Consider the above example of estimating ''f''(2.5). Since 2.5 is midway between 2 and 3, it is reasonable to take ''f''(2.5) midway between ''f''(2) = 0.9093 and ''f''(3) = 0.1411, which yields 0.5252.\n\nGenerally, linear interpolation takes two data points, say (''x''<sub>''a''</sub>,''y''<sub>''a''</sub>) and (''x''<sub>''b''</sub>,''y''<sub>''b''</sub>), and the interpolant is given by:\n:<math> y = y_a + \\left( y_b-y_a \\right) \\frac{x-x_a}{x_b-x_a} \\text{ at the point } \\left( x,y \\right) </math>\n\n\n:<math> \\frac{y-y_a}{y_b-y_a} = \\frac{x-x_a}{x_b-x_a} </math>\n\n\n:<math> \\frac{y-y_a}{x-x_a} = \\frac{y_b-y_a}{x_b-x_a} </math>\n\n\nThis previous equation states that the slope of the new line between <math> (x_a,y_a) </math> and <math> (x,y) </math> is the same as the slope of the line between <math> (x_a,y_a) </math> and <math> (x_b,y_b) </math>\n\nLinear interpolation is quick and easy, but it is not very precise. Another disadvantage is that the interpolant is not [[derivative|differentiable]] at the point ''x''<sub>''k''</sub>.\n\nThe following error estimate shows that linear interpolation is not very precise. Denote the function which we want to interpolate by ''g'', and suppose that ''x'' lies between ''x''<sub>''a''</sub> and ''x''<sub>''b''</sub> and that ''g'' is twice continuously differentiable. Then the linear interpolation error is\n\n:<math> |f(x)-g(x)| \\le C(x_b-x_a)^2 \\quad\\text{where}\\quad C = \\frac18 \\max_{r\\in[x_a,x_b]} |g''(r)|. </math>\n\nIn words, the error is proportional to the square of the distance between the data points. The error in some other methods, including polynomial interpolation and spline interpolation (described below), is proportional to higher powers of the distance between the data points. These methods also produce smoother interpolants.\n{{clear}}\n\n===Polynomial interpolation===\n[[File:Interpolation example polynomial.svg|right|thumb|230px|Plot of the data with polynomial interpolation applied]]\n{{Main|Polynomial interpolation}}\nPolynomial interpolation is a generalization of linear interpolation. Note that the linear interpolant is a [[linear function]]. We now replace this interpolant with a [[polynomial]] of higher [[degree of a polynomial|degree]].\n\nConsider again the problem given above. The following sixth degree polynomial goes through all the seven points:\n:<math> f(x) = -0.0001521 x^6 - 0.003130 x^5 + 0.07321 x^4 - 0.3577 x^3 + 0.2255 x^2 + 0.9038 x. </math>\n<!-- Coefficients are 0, 0.903803333333334, 0.22549749999997, -0.35772291666664, 0.07321458333332, -0.00313041666667, -0.00015208333333. -->\nSubstituting ''x'' = 2.5, we find that ''f''(2.5) = 0.5965.\n\nGenerally, if we have ''n'' data points, there is exactly one polynomial of degree at most ''n''&minus;1 going through all the data points. The interpolation error is proportional to the distance between the data points to the power ''n''. Furthermore, the interpolant is a polynomial and thus infinitely differentiable. So, we see that polynomial interpolation overcomes most of the problems of linear interpolation.\n\nHowever, polynomial interpolation also has some disadvantages. Calculating the interpolating polynomial is computationally expensive (see [[Computational complexity theory|computational complexity]]) compared to linear interpolation. Furthermore, polynomial interpolation may exhibit oscillatory artifacts, especially at the end points (see [[Runge's phenomenon]]).\n\nPolynomial interpolation can estimate local maxima and minima that are outside the range of the samples, unlike linear interpolation. For example, the interpolant above has a local maximum at ''x'' ≈ 1.566, ''f''(''x'') ≈ 1.003 and a local minimum at ''x'' ≈ 4.708, ''f''(''x'') ≈ −1.003. However, these maxima and minima may exceed the theoretical range of the function—for example, a function that is always positive may have an interpolant with negative values, and whose inverse therefore contains false [[division by zero|vertical asymptotes]].\n\nMore generally, the shape of the resulting curve, especially for very high or low values of the independent variable, may be contrary to commonsense, i.e. to what is known about the experimental system which has generated the data points. These disadvantages can be reduced by using spline interpolation or restricting attention to [[Chebyshev polynomials]].\n{{clear}}\n\n===Spline interpolation===\n[[File:Interpolation example spline.svg|right|thumb|230px|Plot of the data with spline interpolation applied]]\n{{Main|Spline interpolation}}\n\nRemember that linear interpolation uses a linear function for each of intervals [''x''<sub>''k''</sub>,''x''<sub>''k+1''</sub>]. Spline interpolation uses low-degree polynomials in each of the intervals, and chooses the polynomial pieces such that they fit smoothly together. The resulting function is called a [[spline (mathematics)|spline]].\n\nFor instance, the [[natural cubic spline]] is [[piecewise]] cubic and twice continuously differentiable. Furthermore, its second derivative is zero at the end points. The natural cubic spline interpolating the points in the table above is given by\n\n: <math> f(x) = \\begin{cases}\n-0.1522 x^3 + 0.9937 x, & \\text{if } x \\in [0,1], \\\\\n-0.01258 x^3 - 0.4189 x^2 + 1.4126 x - 0.1396, & \\text{if } x \\in [1,2], \\\\\n0.1403 x^3 - 1.3359 x^2 + 3.2467 x - 1.3623, & \\text{if } x \\in [2,3], \\\\\n0.1579 x^3 - 1.4945 x^2 + 3.7225 x - 1.8381, & \\text{if } x \\in [3,4], \\\\\n0.05375 x^3 -0.2450 x^2 - 1.2756 x + 4.8259, & \\text{if } x \\in [4,5], \\\\\n-0.1871 x^3 + 3.3673 x^2 - 19.3370 x + 34.9282, & \\text{if } x \\in [5,6].\n\\end{cases} </math>\n\nIn this case we get ''f''(2.5)&nbsp;=&nbsp;0.5972.\n\nLike polynomial interpolation, spline interpolation incurs a smaller error than linear interpolation and the interpolant is smoother. However, the interpolant is easier to evaluate than the high-degree polynomials used in polynomial interpolation. However, the global nature of the basis functions leads to ill-conditioning. This is completely mitigated by using splines of compact support, such as are implemented in Boost.Math and discussed in Kress.<ref>{{cite book|last1=Kress|first1=Rainer|title=Numerical Analysis|year=1998}}</ref>\n{{Clear}}\n\n==Function approximation==\nInterpolation is a common way to approximate functions. Given a function <math>f:[a,b] \\to \\mathbb{R}</math> with a set of points <math>x_1, x_2, \\dots, x_n \\in [a, b]</math> one can form a function <math>s: [a,b] \\to \\mathbb{R}</math> such that <math>f(x_i)=s(x_i)</math> for <math>i=1, 2, \\dots, n</math> (that is that <math>s</math> interpolates <math>f</math> at these points). In general, an interpolant need not be a good approximation, but there are well known and often reasonable conditions where it will. For example, if <math>f\\in C^4([a,b])</math> (four times continuously differentiable) then [[spline interpolation|cubic spline interpolation]] has an error bound given by <math>\\|f-s\\|_\\infty \\leq C \\|f^{(4)}\\|_\\infty h^4</math> where <math>h \\max_{i=1,2, \\dots, n-1} |x_{i+1}-x_i|</math> and <math>C</math> is a constant.<ref>{{cite journal |last1=Hall |first1=Charles A. |last2=Meyer |first2=Weston W. |title=Optimal Error Bounds for Cubic Spline Interpolation |journal=Journal of Approximation Theory |date=1976 |volume=16 |issue=2 |pages=105-122 |url=http://www.sciencedirect.com/science/article/pii/002190457690040X |accessdate=4 April 2019}}</ref>\n\n==Via Gaussian processes==\n[[Gaussian process]] is a powerful non-linear interpolation tool. Many popular interpolation tools are actually equivalent to particular Gaussian processes. Gaussian processes can be used not only for fitting an interpolant that passes exactly through the given data points but also for regression, i.e., for fitting a curve through noisy data. In the geostatistics community Gaussian process regression is also known as [[Kriging]].\n\n==Other forms==\nOther forms of interpolation can be constructed by picking a different class of interpolants. For instance, rational interpolation is '''interpolation''' by [[rational function]]s using [[Padé approximant]], and [[trigonometric interpolation]] is interpolation by [[trigonometric polynomial]]s using [[Fourier series]]. Another possibility is to use [[wavelet]]s.\n\nThe [[Whittaker–Shannon interpolation formula]] can be used if the number of data points is infinite.\n\nSometimes, we know not only the value of the function that we want to interpolate, at some points, but also its derivative. This leads to [[Hermite interpolation]] problems.\n\nWhen each data point is itself a function, it can be useful to see the interpolation problem as a partial [[advection]] problem between each data point. This idea leads to the [[displacement interpolation]] problem used in [[Transportation theory (mathematics)|transportation theory]].\n\n==In higher dimensions==\n{{comparison_of_1D_and_2D_interpolation.svg|250px|}}\n{{main|Multivariate interpolation}}\nMultivariate interpolation is the interpolation of functions of more than one variable. \nMethods include [[bilinear interpolation]] and [[bicubic interpolation]] in two dimensions, and [[trilinear interpolation]] in three dimensions.\nThey can be applied to gridded or scattered data.\n\n<gallery>\nImage:Nearest2DInterpolExample.png|Nearest neighbor\nImage:BilinearInterpolExample.png|Bilinear\nImage:BicubicInterpolationExample.png|Bicubic\n</gallery>\n\n==In digital signal processing==\nIn the domain of digital signal processing, the term interpolation refers to the process of converting a sampled digital signal (such as a sampled audio signal) to that of a higher sampling rate ([[Upsampling]]) using various digital filtering techniques (e.g., convolution with a frequency-limited impulse signal). In this application there is a specific requirement that the harmonic content of the original signal be preserved without creating aliased harmonic content of the original signal above the original Nyquist limit of the signal (i.e., above fs/2 of the original signal sample rate). An early and fairly elementary discussion on this subject can be found in Rabiner and Crochiere's book ''Multirate Digital Signal Processing''.<ref>[https://www.amazon.com/Multirate-Digital-Signal-Processing-Crochiere/dp/0136051626 R.E. Crochiere and L.R. Rabiner. (1983). Multirate Digital Signal Processing. Englewood Cliffs, NJ: Prentice–Hall.]</ref>\n\n==Related concepts==\nThe term ''[[extrapolation]]'' is used to find data points outside the range of known data points.\n\nIn [[curve fitting]] problems, the constraint that the interpolant has to go exactly through the data points is relaxed. It is only required to approach the data points as closely as possible (within some other constraints).  This requires parameterizing the potential interpolants and having some way of measuring the error.  In the simplest case this leads to [[least squares]] approximation.\n\n[[Approximation theory]] studies how to find the best approximation to a given function by another function from some predetermined class, and how good this approximation is. This clearly yields a bound on how well the interpolant can approximate the unknown function.\n\n==Generalization==\nIf we consider <math>x</math> as a variable in a [[topological space]], with and the function <math>f(x)</math> mapping to a [[Banach space]], then the problem is treated as \"interpolation of operators\".<ref>Colin Bennett, Robert C. Sharpley, ''Interpolation of Operators'', Academic Press 1988</ref> The classical results about interpolation of operators are the [[Riesz–Thorin theorem]] and the [[Marcinkiewicz theorem]]. There are also many other subsequent results.\n\n==See also==\n{{Div col|colwidth=30em}}\n* [[Barycentric coordinate system (mathematics)|Barycentric coordinates – for interpolating within on a triangle or tetrahedron]]\n* [[Bilinear interpolation]]\n* [[Brahmagupta's interpolation formula]]\n* [[Extrapolation]]\n* [[Fractal_compression#Fractal_interpolation|Fractal interpolation]]\n* [[Imputation (statistics)]]\n* [[Lagrange polynomial|Lagrange interpolation]]\n* [[Missing data]]\n* [[Multivariate interpolation]]\n* [[Newton–Cotes formulas]]\n* [[Polynomial interpolation]]\n* [[Radial basis function interpolation]]\n* [[Simple rational approximation]]\n{{div col end}}\n\n==References==\n<references />\n\n==External links==\n* Online tools for [http://tools.timodenk.com/linear-interpolation linear], [http://tools.timodenk.com/quadratic-interpolation quadratic], [http://tools.timodenk.com/cubic-spline-interpolation cubic spline], and [http://tools.timodenk.com/polynomial-interpolation polynomial] interpolation with visualisation and [[JavaScript]] source code.\n* [http://sol.gfxile.net/interpolation/index.html Sol Tutorials - Interpolation Tricks]\n* [http://www.boost.org/doc/libs/release/libs/math/doc/html/math_toolkit/cubic_b.html Compactly Supported Cubic B-Spline interpolation in Boost.Math]\n* [http://www.boost.org/doc/libs/release/libs/math/doc/html/math_toolkit/barycentric.html Barycentric rational interpolation in Boost.Math]\n* [http://www.boost.org/doc/libs/release/libs/math/doc/html/math_toolkit/sf_poly/chebyshev.html Interpolation via the Chebyshev transform in Boost.Math]\n[[Category:Interpolation| ]]\n[[Category:Video]]\n[[Category:Video signal]]"
    }
  ]
}